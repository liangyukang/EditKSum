nohup: ignoring input
2022-08-14 09:38:28 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:15235
2022-08-14 09:38:29 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15235
2022-08-14 09:38:29 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15235
2022-08-14 09:38:29 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:15235
2022-08-14 09:38:29 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-08-14 09:38:29 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-08-14 09:38:30 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-08-14 09:38:30 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-08-14 09:38:30 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-14 09:38:30 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-14 09:38:30 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-08-14 09:38:30 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-08-14 09:38:30 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-14 09:38:30 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-14 09:38:30 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-08-14 09:38:30 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-08-14 09:38:34 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=64, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=64, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:15235', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_transformer_transformer_uncased_Ggw', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='6,6,6', layers_num='6,6,6', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-uncased-Ggw', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-uncased', encoder_adapter_dimention=2048, decoder_input='target', kpe=False, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-uncased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-uncased-decoder', share_decoder_input_output_embed=False, encoder='transformer', decoder='transformer', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-uncased')
2022-08-14 09:38:34 | INFO | fairseq.tasks.translation | [source] dictionary: 30522 types
2022-08-14 09:38:34 | INFO | fairseq.tasks.translation | [target] dictionary: 30522 types
2022-08-14 09:38:34 | INFO | fairseq.data.data_utils | loaded 189651 examples from: ../data-bin-bert-uncased-Ggw/valid.source-target.source
2022-08-14 09:38:34 | INFO | fairseq.data.data_utils | loaded 189651 examples from: ../data-bin-bert-uncased-Ggw/valid.source-target.target
2022-08-14 09:38:34 | INFO | fairseq.tasks.translation | ../data-bin-bert-uncased-Ggw valid source-target 189651 examples
2022-08-14 09:38:34 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-uncased/config.json
2022-08-14 09:38:34 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
2022-08-14 09:38:37 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): EditorTransformerEncoder(
    (embed_tokens): Embedding(30522, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): EditorTransformerDecoder(
    (embed_tokens): Embedding(30522, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=30522, bias=False)
    (embed_mask_ins): Embedding(256, 1536)
    (layers_reposition): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-08-14 09:38:37 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-08-14 09:38:37 | INFO | fairseq_cli.train | num. model params: 152240640 (num. trained: 152240640)
2022-08-14 09:38:37 | INFO | fairseq_cli.train | num. Encoder model params: 56918784 (Encoder num. trained: 56918784)
2022-08-14 09:38:37 | INFO | fairseq_cli.train | num. Decoder model params: 118762752 (Decoder num. trained: 118762752)
Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']2022-08-14 09:38:37 | INFO | fairseq_cli.train | training on 4 GPUs
2022-08-14 09:38:37 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 64
2022-08-14 09:38:37 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_transformer_transformer_uncased_Ggw/checkpoint_last.pt
2022-08-14 09:38:37 | INFO | fairseq.trainer | loading train data for epoch 1
Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']2022-08-14 09:38:37 | INFO | fairseq.data.data_utils | loaded 3803957 examples from: ../data-bin-bert-uncased-Ggw/train.source-target.source
2022-08-14 09:38:37 | INFO | fairseq.data.data_utils | loaded 3803957 examples from: ../data-bin-bert-uncased-Ggw/train.source-target.target
2022-08-14 09:38:37 | INFO | fairseq.tasks.translation | ../data-bin-bert-uncased-Ggw train source-target 3803957 examples
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-08-14 09:41:30 | INFO | train_inner | epoch 001:    100 / 1858 loss=23.57, nll_loss=14.343, mask_ins=7.524, word_ins_ml=14.42, word_reposition=1.628, ppl=1.24524e+07, wps=17057.2, ups=0.69, wpb=24678.3, bsz=2048, num_updates=100, lr=1.0098e-05, gnorm=17.505, clip=0, loss_scale=128, train_wall=142, wall=174
2022-08-14 09:43:54 | INFO | train_inner | epoch 001:    200 / 1858 loss=18.054, nll_loss=12.839, mask_ins=4.305, word_ins_ml=13.08, word_reposition=0.669, ppl=272090, wps=17106.2, ups=0.69, wpb=24618.4, bsz=2048, num_updates=200, lr=2.0096e-05, gnorm=17.611, clip=0, loss_scale=128, train_wall=141, wall=318
2022-08-14 09:46:19 | INFO | train_inner | epoch 001:    300 / 1858 loss=14.849, nll_loss=11.574, mask_ins=2.31, word_ins_ml=11.994, word_reposition=0.546, ppl=29514.5, wps=16969.6, ups=0.69, wpb=24468.6, bsz=2047.9, num_updates=300, lr=3.0094e-05, gnorm=3.637, clip=0, loss_scale=128, train_wall=141, wall=462
2022-08-14 09:48:39 | INFO | train_inner | epoch 001:    400 / 1858 loss=14.072, nll_loss=11.009, mask_ins=1.989, word_ins_ml=11.558, word_reposition=0.525, ppl=17224.1, wps=17514.9, ups=0.71, wpb=24682.7, bsz=2048, num_updates=400, lr=4.0092e-05, gnorm=1.614, clip=0, loss_scale=128, train_wall=138, wall=603
2022-08-14 09:51:03 | INFO | train_inner | epoch 001:    500 / 1858 loss=13.747, nll_loss=10.719, mask_ins=1.897, word_ins_ml=11.316, word_reposition=0.533, ppl=13747.2, wps=17055.4, ups=0.7, wpb=24482.6, bsz=2048, num_updates=500, lr=5.009e-05, gnorm=1.492, clip=0, loss_scale=128, train_wall=140, wall=746
2022-08-14 09:53:28 | INFO | train_inner | epoch 001:    600 / 1858 loss=13.451, nll_loss=10.382, mask_ins=1.878, word_ins_ml=11.02, word_reposition=0.553, ppl=11199.4, wps=16950.2, ups=0.69, wpb=24519.3, bsz=2048, num_updates=600, lr=6.0088e-05, gnorm=1.43, clip=0, loss_scale=242, train_wall=141, wall=891
2022-08-14 09:55:52 | INFO | train_inner | epoch 001:    700 / 1858 loss=13.138, nll_loss=10.013, mask_ins=1.879, word_ins_ml=10.695, word_reposition=0.565, ppl=9017.25, wps=17050.1, ups=0.69, wpb=24612.6, bsz=2048, num_updates=700, lr=7.0086e-05, gnorm=1.488, clip=0, loss_scale=256, train_wall=141, wall=1035
2022-08-14 09:58:15 | INFO | train_inner | epoch 001:    800 / 1858 loss=12.851, nll_loss=9.697, mask_ins=1.87, word_ins_ml=10.417, word_reposition=0.564, ppl=7386.3, wps=17109.4, ups=0.7, wpb=24532.8, bsz=2048, num_updates=800, lr=8.0084e-05, gnorm=1.366, clip=0, loss_scale=256, train_wall=140, wall=1179
2022-08-14 10:00:40 | INFO | train_inner | epoch 001:    900 / 1858 loss=12.606, nll_loss=9.426, mask_ins=1.859, word_ins_ml=10.179, word_reposition=0.566, ppl=6232.3, wps=17001.3, ups=0.69, wpb=24570.7, bsz=2048, num_updates=900, lr=9.0082e-05, gnorm=1.385, clip=0, loss_scale=256, train_wall=141, wall=1323
2022-08-14 10:03:04 | INFO | train_inner | epoch 001:   1000 / 1858 loss=12.385, nll_loss=9.171, mask_ins=1.856, word_ins_ml=9.957, word_reposition=0.572, ppl=5347.66, wps=17156.1, ups=0.7, wpb=24669.4, bsz=2048, num_updates=1000, lr=0.00010008, gnorm=1.329, clip=0, loss_scale=256, train_wall=141, wall=1467
2022-08-14 10:05:28 | INFO | train_inner | epoch 001:   1100 / 1858 loss=12.184, nll_loss=8.928, mask_ins=1.851, word_ins_ml=9.744, word_reposition=0.589, ppl=4651.94, wps=17030.6, ups=0.69, wpb=24643.8, bsz=2048, num_updates=1100, lr=0.000110078, gnorm=1.412, clip=0, loss_scale=453, train_wall=142, wall=1612
2022-08-14 10:07:53 | INFO | train_inner | epoch 001:   1200 / 1858 loss=11.954, nll_loss=8.652, mask_ins=1.848, word_ins_ml=9.502, word_reposition=0.605, ppl=3968.81, wps=17104.7, ups=0.69, wpb=24789.3, bsz=2048, num_updates=1200, lr=0.000120076, gnorm=1.508, clip=0, loss_scale=512, train_wall=142, wall=1757
2022-08-14 10:10:18 | INFO | train_inner | epoch 001:   1300 / 1858 loss=11.75, nll_loss=8.423, mask_ins=1.835, word_ins_ml=9.303, word_reposition=0.612, ppl=3443.85, wps=16978.9, ups=0.69, wpb=24491.4, bsz=2048, num_updates=1300, lr=0.000130074, gnorm=1.432, clip=0, loss_scale=512, train_wall=141, wall=1901
2022-08-14 10:12:42 | INFO | train_inner | epoch 001:   1400 / 1858 loss=11.561, nll_loss=8.224, mask_ins=1.817, word_ins_ml=9.13, word_reposition=0.615, ppl=3022.2, wps=17157.5, ups=0.69, wpb=24774.4, bsz=2048, num_updates=1400, lr=0.000140072, gnorm=1.447, clip=0, loss_scale=512, train_wall=141, wall=2045
2022-08-14 10:15:06 | INFO | train_inner | epoch 001:   1500 / 1858 loss=11.372, nll_loss=8, mask_ins=1.809, word_ins_ml=8.934, word_reposition=0.628, ppl=2649.53, wps=17034.9, ups=0.69, wpb=24538, bsz=2048, num_updates=1500, lr=0.00015007, gnorm=1.556, clip=0, loss_scale=512, train_wall=141, wall=2189
2022-08-14 10:17:29 | INFO | train_inner | epoch 001:   1600 / 1858 loss=11.2, nll_loss=7.807, mask_ins=1.788, word_ins_ml=8.767, word_reposition=0.645, ppl=2353.3, wps=17379.3, ups=0.7, wpb=24890.4, bsz=2048, num_updates=1600, lr=0.000160068, gnorm=1.472, clip=0, loss_scale=845, train_wall=140, wall=2333
2022-08-14 10:19:53 | INFO | train_inner | epoch 001:   1700 / 1858 loss=11.042, nll_loss=7.63, mask_ins=1.771, word_ins_ml=8.614, word_reposition=0.657, ppl=2108.56, wps=17334, ups=0.7, wpb=24839.1, bsz=2048, num_updates=1700, lr=0.000170066, gnorm=1.549, clip=0, loss_scale=1024, train_wall=140, wall=2476
2022-08-14 10:22:15 | INFO | train_inner | epoch 001:   1800 / 1858 loss=10.848, nll_loss=7.422, mask_ins=1.753, word_ins_ml=8.434, word_reposition=0.662, ppl=1843.83, wps=17349.1, ups=0.7, wpb=24756.7, bsz=2048, num_updates=1800, lr=0.000180064, gnorm=1.666, clip=0, loss_scale=1024, train_wall=140, wall=2619
2022-08-14 10:23:38 | INFO | train | epoch 001 | loss 13.287 | nll_loss 9.607 | mask_ins 2.306 | word_ins_ml 10.328 | word_reposition 0.652 | ppl 9991.73 | wps 17124.6 | ups 0.7 | wpb 24631.8 | bsz 2047 | num_updates 1858 | lr 0.000185863 | gnorm 3.333 | clip 0 | loss_scale 425 | train_wall 2614 | wall 2702
2022-08-14 10:24:31 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.559 | nll_loss 6.93 | mask_ins 1.75 | word_ins_ml 8.077 | word_reposition 0.732 | ppl 1508.6 | wps 45056.9 | wpb 3111.7 | bsz 255.9 | num_updates 1858
2022-08-14 10:24:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_uncased_Ggw/checkpoint_best.pt (epoch 1 @ 1858 updates, score 10.559) (writing took 2.864066444337368 seconds)
2022-08-14 10:25:35 | INFO | train_inner | epoch 002:     42 / 1858 loss=10.674, nll_loss=7.241, mask_ins=1.738, word_ins_ml=8.276, word_reposition=0.66, ppl=1633.89, wps=12326.2, ups=0.5, wpb=24564, bsz=2030.1, num_updates=1900, lr=0.000190062, gnorm=1.842, clip=0, loss_scale=1024, train_wall=141, wall=2818
2022-08-14 10:27:58 | INFO | train_inner | epoch 002:    142 / 1858 loss=10.451, nll_loss=7.015, mask_ins=1.719, word_ins_ml=8.08, word_reposition=0.652, ppl=1399.89, wps=17063.9, ups=0.7, wpb=24469.8, bsz=2048, num_updates=2000, lr=0.00020006, gnorm=1.607, clip=0, loss_scale=1024, train_wall=140, wall=2961
2022-08-14 10:30:19 | INFO | train_inner | epoch 002:    242 / 1858 loss=10.296, nll_loss=6.848, mask_ins=1.703, word_ins_ml=7.937, word_reposition=0.656, ppl=1257.32, wps=17574.7, ups=0.71, wpb=24824.2, bsz=2048, num_updates=2100, lr=0.000210058, gnorm=1.608, clip=0, loss_scale=1567, train_wall=138, wall=3102
2022-08-14 10:32:43 | INFO | train_inner | epoch 002:    342 / 1858 loss=10.169, nll_loss=6.712, mask_ins=1.694, word_ins_ml=7.82, word_reposition=0.655, ppl=1151.29, wps=17232, ups=0.69, wpb=24801.8, bsz=2048, num_updates=2200, lr=0.000220056, gnorm=1.675, clip=0, loss_scale=2048, train_wall=141, wall=3246
2022-08-14 10:35:07 | INFO | train_inner | epoch 002:    442 / 1858 loss=9.972, nll_loss=6.53, mask_ins=1.669, word_ins_ml=7.664, word_reposition=0.639, ppl=1004.01, wps=17092.9, ups=0.69, wpb=24628.9, bsz=2048, num_updates=2300, lr=0.000230054, gnorm=1.534, clip=0, loss_scale=2048, train_wall=141, wall=3390
2022-08-14 10:37:32 | INFO | train_inner | epoch 002:    542 / 1858 loss=9.871, nll_loss=6.428, mask_ins=1.653, word_ins_ml=7.577, word_reposition=0.641, ppl=936.53, wps=17099.8, ups=0.69, wpb=24676, bsz=2048, num_updates=2400, lr=0.000240052, gnorm=1.533, clip=0, loss_scale=2048, train_wall=141, wall=3535
2022-08-14 10:39:56 | INFO | train_inner | epoch 002:    642 / 1858 loss=9.717, nll_loss=6.277, mask_ins=1.635, word_ins_ml=7.448, word_reposition=0.635, ppl=841.58, wps=17068.8, ups=0.69, wpb=24620.3, bsz=2048, num_updates=2500, lr=0.00025005, gnorm=1.608, clip=0, loss_scale=2048, train_wall=141, wall=3679
2022-08-14 10:42:20 | INFO | train_inner | epoch 002:    742 / 1858 loss=9.598, nll_loss=6.171, mask_ins=1.62, word_ins_ml=7.357, word_reposition=0.621, ppl=774.92, wps=17062, ups=0.69, wpb=24635.4, bsz=2048, num_updates=2600, lr=0.000260048, gnorm=1.527, clip=0, loss_scale=2888, train_wall=141, wall=3823
2022-08-14 10:44:44 | INFO | train_inner | epoch 002:    842 / 1858 loss=9.462, nll_loss=6.051, mask_ins=1.605, word_ins_ml=7.254, word_reposition=0.603, ppl=705.01, wps=17106.9, ups=0.69, wpb=24661.7, bsz=2048, num_updates=2700, lr=0.000270046, gnorm=1.508, clip=0, loss_scale=4096, train_wall=141, wall=3968
2022-08-14 10:47:09 | INFO | train_inner | epoch 002:    942 / 1858 loss=9.366, nll_loss=5.96, mask_ins=1.588, word_ins_ml=7.177, word_reposition=0.6, ppl=659.77, wps=17129.6, ups=0.69, wpb=24730.6, bsz=2048, num_updates=2800, lr=0.000280044, gnorm=1.565, clip=0, loss_scale=4096, train_wall=141, wall=4112
2022-08-14 10:49:33 | INFO | train_inner | epoch 002:   1042 / 1858 loss=9.252, nll_loss=5.85, mask_ins=1.574, word_ins_ml=7.082, word_reposition=0.596, ppl=609.92, wps=16914.6, ups=0.69, wpb=24413.3, bsz=2048, num_updates=2900, lr=0.000290042, gnorm=1.551, clip=0, loss_scale=4096, train_wall=141, wall=4256
2022-08-14 10:51:57 | INFO | train_inner | epoch 002:   1142 / 1858 loss=9.184, nll_loss=5.787, mask_ins=1.564, word_ins_ml=7.029, word_reposition=0.591, ppl=581.64, wps=17243.5, ups=0.7, wpb=24791.6, bsz=2048, num_updates=3000, lr=0.00030004, gnorm=1.439, clip=0, loss_scale=4096, train_wall=141, wall=4400
2022-08-14 10:54:20 | INFO | train_inner | epoch 002:   1242 / 1858 loss=9.077, nll_loss=5.692, mask_ins=1.551, word_ins_ml=6.948, word_reposition=0.578, ppl=540.08, wps=17056.5, ups=0.7, wpb=24499.2, bsz=2048, num_updates=3100, lr=0.000310038, gnorm=1.424, clip=0, loss_scale=5284, train_wall=140, wall=4544
2022-08-14 10:56:44 | INFO | train_inner | epoch 002:   1342 / 1858 loss=9.015, nll_loss=5.642, mask_ins=1.543, word_ins_ml=6.906, word_reposition=0.566, ppl=517.39, wps=17072.1, ups=0.7, wpb=24523.8, bsz=2048, num_updates=3200, lr=0.000320036, gnorm=1.472, clip=0, loss_scale=8192, train_wall=140, wall=4687
2022-08-14 10:59:09 | INFO | train_inner | epoch 002:   1442 / 1858 loss=8.948, nll_loss=5.571, mask_ins=1.536, word_ins_ml=6.845, word_reposition=0.567, ppl=493.91, wps=17076.8, ups=0.69, wpb=24713, bsz=2048, num_updates=3300, lr=0.000330034, gnorm=1.494, clip=0, loss_scale=8192, train_wall=142, wall=4832
2022-08-14 11:01:33 | INFO | train_inner | epoch 002:   1542 / 1858 loss=8.885, nll_loss=5.518, mask_ins=1.526, word_ins_ml=6.8, word_reposition=0.559, ppl=472.71, wps=17119.6, ups=0.7, wpb=24620.9, bsz=2048, num_updates=3400, lr=0.000340032, gnorm=1.491, clip=0, loss_scale=8192, train_wall=141, wall=4976
2022-08-14 11:03:56 | INFO | train_inner | epoch 002:   1642 / 1858 loss=8.824, nll_loss=5.475, mask_ins=1.518, word_ins_ml=6.763, word_reposition=0.544, ppl=453.32, wps=17183.5, ups=0.7, wpb=24676.9, bsz=2048, num_updates=3500, lr=0.00035003, gnorm=1.602, clip=0, loss_scale=8192, train_wall=140, wall=5119
2022-08-14 11:06:20 | INFO | train_inner | epoch 002:   1742 / 1858 loss=8.775, nll_loss=5.416, mask_ins=1.51, word_ins_ml=6.712, word_reposition=0.553, ppl=438.16, wps=17252.5, ups=0.7, wpb=24765.4, bsz=2048, num_updates=3600, lr=0.000360028, gnorm=1.47, clip=0, loss_scale=9585, train_wall=140, wall=5263
2022-08-14 11:08:44 | INFO | train_inner | epoch 002:   1842 / 1858 loss=8.716, nll_loss=5.379, mask_ins=1.5, word_ins_ml=6.68, word_reposition=0.535, ppl=420.55, wps=16961.1, ups=0.69, wpb=24449.5, bsz=2047.9, num_updates=3700, lr=0.000370026, gnorm=1.431, clip=0, loss_scale=16384, train_wall=141, wall=5407
2022-08-14 11:09:06 | INFO | train | epoch 002 | loss 9.443 | nll_loss 6.039 | mask_ins 1.597 | word_ins_ml 7.245 | word_reposition 0.6 | ppl 695.85 | wps 16776.4 | ups 0.68 | wpb 24631.7 | bsz 2047 | num_updates 3716 | lr 0.000371626 | gnorm 1.539 | clip 0 | loss_scale 5227 | train_wall 2613 | wall 5429
2022-08-14 11:09:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.62 | nll_loss 5.124 | mask_ins 1.491 | word_ins_ml 6.577 | word_reposition 0.552 | ppl 393.49 | wps 44836.5 | wpb 3111.7 | bsz 255.9 | num_updates 3716 | best_loss 8.62
2022-08-14 11:10:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_uncased_Ggw/checkpoint_best.pt (epoch 2 @ 3716 updates, score 8.62) (writing took 18.21942267753184 seconds)
2022-08-14 11:12:16 | INFO | train_inner | epoch 003:     84 / 1858 loss=8.669, nll_loss=5.331, mask_ins=1.497, word_ins_ml=6.639, word_reposition=0.533, ppl=406.96, wps=11430.7, ups=0.47, wpb=24267.8, bsz=2030.1, num_updates=3800, lr=0.000380024, gnorm=1.355, clip=0, loss_scale=16384, train_wall=139, wall=5619
2022-08-14 11:14:40 | INFO | train_inner | epoch 003:    184 / 1858 loss=8.572, nll_loss=5.234, mask_ins=1.486, word_ins_ml=6.555, word_reposition=0.53, ppl=380.46, wps=17069.7, ups=0.69, wpb=24583, bsz=2048, num_updates=3900, lr=0.000390022, gnorm=1.573, clip=0, loss_scale=16384, train_wall=141, wall=5763
2022-08-14 11:17:05 | INFO | train_inner | epoch 003:    284 / 1858 loss=8.537, nll_loss=5.207, mask_ins=1.481, word_ins_ml=6.532, word_reposition=0.523, ppl=371.43, wps=17041.9, ups=0.69, wpb=24584.4, bsz=2048, num_updates=4000, lr=0.00040002, gnorm=1.469, clip=0, loss_scale=16384, train_wall=141, wall=5908
2022-08-14 11:19:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16384.0
2022-08-14 11:19:30 | INFO | train_inner | epoch 003:    385 / 1858 loss=8.538, nll_loss=5.208, mask_ins=1.476, word_ins_ml=6.533, word_reposition=0.528, ppl=371.7, wps=17097.7, ups=0.69, wpb=24845.8, bsz=2047.9, num_updates=4100, lr=0.000410018, gnorm=1.481, clip=0, loss_scale=16546, train_wall=142, wall=6053
2022-08-14 11:21:55 | INFO | train_inner | epoch 003:    485 / 1858 loss=8.463, nll_loss=5.155, mask_ins=1.465, word_ins_ml=6.487, word_reposition=0.511, ppl=352.89, wps=16905.3, ups=0.69, wpb=24494, bsz=2048, num_updates=4200, lr=0.000420016, gnorm=1.425, clip=0, loss_scale=16384, train_wall=142, wall=6198
2022-08-14 11:24:18 | INFO | train_inner | epoch 003:    585 / 1858 loss=8.425, nll_loss=5.117, mask_ins=1.459, word_ins_ml=6.454, word_reposition=0.512, ppl=343.71, wps=17084.7, ups=0.7, wpb=24554, bsz=2048, num_updates=4300, lr=0.000430014, gnorm=1.464, clip=0, loss_scale=16384, train_wall=141, wall=6342
2022-08-14 11:26:42 | INFO | train_inner | epoch 003:    685 / 1858 loss=8.406, nll_loss=5.103, mask_ins=1.455, word_ins_ml=6.442, word_reposition=0.509, ppl=339.29, wps=17186, ups=0.7, wpb=24623.6, bsz=2048, num_updates=4400, lr=0.000440012, gnorm=1.515, clip=0, loss_scale=16384, train_wall=140, wall=6485
2022-08-14 11:29:05 | INFO | train_inner | epoch 003:    785 / 1858 loss=8.375, nll_loss=5.077, mask_ins=1.445, word_ins_ml=6.42, word_reposition=0.51, ppl=332.05, wps=17248.6, ups=0.7, wpb=24698.3, bsz=2048, num_updates=4500, lr=0.00045001, gnorm=1.395, clip=0, loss_scale=16384, train_wall=140, wall=6628
2022-08-14 11:31:29 | INFO | train_inner | epoch 003:    885 / 1858 loss=8.322, nll_loss=5.035, mask_ins=1.441, word_ins_ml=6.383, word_reposition=0.499, ppl=319.94, wps=17187.6, ups=0.7, wpb=24679.7, bsz=2048, num_updates=4600, lr=0.000460008, gnorm=1.464, clip=0, loss_scale=16384, train_wall=140, wall=6772
2022-08-14 11:31:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16384.0
2022-08-14 11:33:54 | INFO | train_inner | epoch 003:    986 / 1858 loss=8.338, nll_loss=5.045, mask_ins=1.441, word_ins_ml=6.392, word_reposition=0.505, ppl=323.54, wps=16905.4, ups=0.69, wpb=24557.3, bsz=2048, num_updates=4700, lr=0.000470006, gnorm=1.397, clip=0, loss_scale=16546, train_wall=142, wall=6917
2022-08-14 11:36:17 | INFO | train_inner | epoch 003:   1086 / 1858 loss=8.312, nll_loss=5.028, mask_ins=1.434, word_ins_ml=6.377, word_reposition=0.501, ppl=317.75, wps=17218.3, ups=0.7, wpb=24636.3, bsz=2048, num_updates=4800, lr=0.000480004, gnorm=1.401, clip=0, loss_scale=16384, train_wall=140, wall=7060
2022-08-14 11:38:40 | INFO | train_inner | epoch 003:   1186 / 1858 loss=8.295, nll_loss=5.02, mask_ins=1.432, word_ins_ml=6.37, word_reposition=0.494, ppl=314.18, wps=17440, ups=0.7, wpb=24900.4, bsz=2048, num_updates=4900, lr=0.000490002, gnorm=1.329, clip=0, loss_scale=16384, train_wall=140, wall=7203
2022-08-14 11:41:03 | INFO | train_inner | epoch 003:   1286 / 1858 loss=8.251, nll_loss=4.976, mask_ins=1.423, word_ins_ml=6.331, word_reposition=0.496, ppl=304.63, wps=17103, ups=0.7, wpb=24538.5, bsz=2048, num_updates=5000, lr=0.0005, gnorm=1.365, clip=0, loss_scale=16384, train_wall=140, wall=7346
2022-08-14 11:43:27 | INFO | train_inner | epoch 003:   1386 / 1858 loss=8.251, nll_loss=4.973, mask_ins=1.426, word_ins_ml=6.328, word_reposition=0.497, ppl=304.65, wps=17208.6, ups=0.7, wpb=24688.8, bsz=2048, num_updates=5100, lr=0.000495074, gnorm=1.388, clip=0, loss_scale=16384, train_wall=140, wall=7490
2022-08-14 11:44:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16384.0
2022-08-14 11:45:52 | INFO | train_inner | epoch 003:   1487 / 1858 loss=8.2, nll_loss=4.932, mask_ins=1.42, word_ins_ml=6.292, word_reposition=0.488, ppl=294.13, wps=16985.7, ups=0.69, wpb=24685.8, bsz=2048, num_updates=5200, lr=0.00049029, gnorm=1.353, clip=0, loss_scale=17520, train_wall=142, wall=7635
2022-08-14 11:48:15 | INFO | train_inner | epoch 003:   1587 / 1858 loss=8.17, nll_loss=4.906, mask_ins=1.408, word_ins_ml=6.269, word_reposition=0.492, ppl=287.97, wps=17150.2, ups=0.7, wpb=24565.9, bsz=2048, num_updates=5300, lr=0.000485643, gnorm=1.27, clip=0, loss_scale=16384, train_wall=140, wall=7778
2022-08-14 11:49:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-14 11:50:40 | INFO | train_inner | epoch 003:   1688 / 1858 loss=8.161, nll_loss=4.895, mask_ins=1.41, word_ins_ml=6.259, word_reposition=0.493, ppl=286.32, wps=16967.4, ups=0.69, wpb=24586.9, bsz=2048, num_updates=5400, lr=0.000481125, gnorm=1.33, clip=0, loss_scale=12491, train_wall=142, wall=7923
2022-08-14 11:52:59 | INFO | train_inner | epoch 003:   1788 / 1858 loss=8.118, nll_loss=4.861, mask_ins=1.403, word_ins_ml=6.229, word_reposition=0.487, ppl=277.82, wps=17844.7, ups=0.72, wpb=24805, bsz=2048, num_updates=5500, lr=0.000476731, gnorm=1.298, clip=0, loss_scale=8192, train_wall=136, wall=8062
2022-08-14 11:54:41 | INFO | train | epoch 003 | loss 8.343 | nll_loss 5.05 | mask_ins 1.442 | word_ins_ml 6.395 | word_reposition 0.505 | ppl 324.62 | wps 16700.8 | ups 0.68 | wpb 24633.1 | bsz 2047 | num_updates 5570 | lr 0.000473726 | gnorm 1.401 | clip 0 | loss_scale 15502 | train_wall 2606 | wall 8164
2022-08-14 11:55:32 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.034 | nll_loss 4.6 | mask_ins 1.391 | word_ins_ml 6.114 | word_reposition 0.529 | ppl 262.09 | wps 44958 | wpb 3111.7 | bsz 255.9 | num_updates 5570 | best_loss 8.034
2022-08-14 11:55:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_uncased_Ggw/checkpoint_best.pt (epoch 3 @ 5570 updates, score 8.034) (writing took 20.756283907219768 seconds)
2022-08-14 11:56:36 | INFO | train_inner | epoch 004:     30 / 1858 loss=8.081, nll_loss=4.831, mask_ins=1.401, word_ins_ml=6.202, word_reposition=0.478, ppl=270.72, wps=11286.3, ups=0.46, wpb=24496.8, bsz=2030.1, num_updates=5600, lr=0.000472456, gnorm=1.317, clip=0, loss_scale=8192, train_wall=142, wall=8279
2022-08-14 11:59:00 | INFO | train_inner | epoch 004:    130 / 1858 loss=8.003, nll_loss=4.762, mask_ins=1.388, word_ins_ml=6.142, word_reposition=0.474, ppl=256.57, wps=17102.1, ups=0.7, wpb=24590.2, bsz=2048, num_updates=5700, lr=0.000468293, gnorm=1.224, clip=0, loss_scale=8192, train_wall=141, wall=8423
2022-08-14 12:01:25 | INFO | train_inner | epoch 004:    230 / 1858 loss=8, nll_loss=4.746, mask_ins=1.382, word_ins_ml=6.127, word_reposition=0.491, ppl=255.99, wps=17087.3, ups=0.69, wpb=24730.1, bsz=2048, num_updates=5800, lr=0.000464238, gnorm=1.22, clip=0, loss_scale=8192, train_wall=141, wall=8568
2022-08-14 12:03:49 | INFO | train_inner | epoch 004:    330 / 1858 loss=7.955, nll_loss=4.719, mask_ins=1.378, word_ins_ml=6.103, word_reposition=0.474, ppl=248.07, wps=17076.8, ups=0.69, wpb=24631.4, bsz=2048, num_updates=5900, lr=0.000460287, gnorm=1.262, clip=0, loss_scale=11141, train_wall=141, wall=8712
2022-08-14 12:06:13 | INFO | train_inner | epoch 004:    430 / 1858 loss=7.941, nll_loss=4.713, mask_ins=1.374, word_ins_ml=6.098, word_reposition=0.468, ppl=245.73, wps=17042.7, ups=0.69, wpb=24562.4, bsz=2048, num_updates=6000, lr=0.000456435, gnorm=1.245, clip=0, loss_scale=16384, train_wall=141, wall=8856
2022-08-14 12:08:37 | INFO | train_inner | epoch 004:    530 / 1858 loss=7.907, nll_loss=4.678, mask_ins=1.371, word_ins_ml=6.066, word_reposition=0.471, ppl=240.02, wps=17036.6, ups=0.69, wpb=24594.7, bsz=2048, num_updates=6100, lr=0.000452679, gnorm=1.184, clip=0, loss_scale=16384, train_wall=141, wall=9001
2022-08-14 12:11:02 | INFO | train_inner | epoch 004:    630 / 1858 loss=7.905, nll_loss=4.675, mask_ins=1.369, word_ins_ml=6.063, word_reposition=0.473, ppl=239.69, wps=17114.2, ups=0.69, wpb=24736.4, bsz=2048, num_updates=6200, lr=0.000449013, gnorm=1.2, clip=0, loss_scale=16384, train_wall=141, wall=9145
2022-08-14 12:13:27 | INFO | train_inner | epoch 004:    730 / 1858 loss=7.897, nll_loss=4.668, mask_ins=1.367, word_ins_ml=6.057, word_reposition=0.473, ppl=238.33, wps=17081.6, ups=0.69, wpb=24731.5, bsz=2048, num_updates=6300, lr=0.000445435, gnorm=1.144, clip=0, loss_scale=16384, train_wall=142, wall=9290
2022-08-14 12:15:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16384.0
2022-08-14 12:15:53 | INFO | train_inner | epoch 004:    831 / 1858 loss=7.87, nll_loss=4.649, mask_ins=1.365, word_ins_ml=6.039, word_reposition=0.465, ppl=233.86, wps=16933.2, ups=0.68, wpb=24751.8, bsz=2048, num_updates=6400, lr=0.000441942, gnorm=1.178, clip=0, loss_scale=17033, train_wall=143, wall=9436
2022-08-14 12:18:16 | INFO | train_inner | epoch 004:    931 / 1858 loss=7.848, nll_loss=4.628, mask_ins=1.36, word_ins_ml=6.021, word_reposition=0.467, ppl=230.41, wps=17154, ups=0.7, wpb=24594.6, bsz=2048, num_updates=6500, lr=0.000438529, gnorm=1.203, clip=0, loss_scale=16384, train_wall=140, wall=9579
2022-08-14 12:20:39 | INFO | train_inner | epoch 004:   1031 / 1858 loss=7.814, nll_loss=4.606, mask_ins=1.355, word_ins_ml=6.001, word_reposition=0.458, ppl=224.99, wps=17124.1, ups=0.7, wpb=24474.5, bsz=2048, num_updates=6600, lr=0.000435194, gnorm=1.148, clip=0, loss_scale=16384, train_wall=140, wall=9722
2022-08-14 12:23:03 | INFO | train_inner | epoch 004:   1131 / 1858 loss=7.816, nll_loss=4.606, mask_ins=1.353, word_ins_ml=6.001, word_reposition=0.461, ppl=225.32, wps=17140.7, ups=0.7, wpb=24628, bsz=2047.9, num_updates=6700, lr=0.000431934, gnorm=1.155, clip=0, loss_scale=16384, train_wall=141, wall=9866
2022-08-14 12:25:26 | INFO | train_inner | epoch 004:   1231 / 1858 loss=7.792, nll_loss=4.591, mask_ins=1.352, word_ins_ml=5.987, word_reposition=0.453, ppl=221.65, wps=17205, ups=0.7, wpb=24628.7, bsz=2048, num_updates=6800, lr=0.000428746, gnorm=1.149, clip=0, loss_scale=16384, train_wall=140, wall=10009
2022-08-14 12:27:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16384.0
2022-08-14 12:27:50 | INFO | train_inner | epoch 004:   1332 / 1858 loss=7.797, nll_loss=4.588, mask_ins=1.35, word_ins_ml=5.984, word_reposition=0.463, ppl=222.42, wps=17010.3, ups=0.69, wpb=24574.1, bsz=2048, num_updates=6900, lr=0.000425628, gnorm=1.155, clip=0, loss_scale=17520, train_wall=141, wall=10154
2022-08-14 12:30:14 | INFO | train_inner | epoch 004:   1432 / 1858 loss=7.765, nll_loss=4.568, mask_ins=1.344, word_ins_ml=5.967, word_reposition=0.454, ppl=217.47, wps=17077, ups=0.7, wpb=24546.1, bsz=2048, num_updates=7000, lr=0.000422577, gnorm=1.093, clip=0, loss_scale=16384, train_wall=141, wall=10297
2022-08-14 12:32:39 | INFO | train_inner | epoch 004:   1532 / 1858 loss=7.746, nll_loss=4.539, mask_ins=1.343, word_ins_ml=5.941, word_reposition=0.463, ppl=214.66, wps=17050.3, ups=0.69, wpb=24736.2, bsz=2048, num_updates=7100, lr=0.000419591, gnorm=1.075, clip=0, loss_scale=16384, train_wall=142, wall=10442
2022-08-14 12:35:02 | INFO | train_inner | epoch 004:   1632 / 1858 loss=7.736, nll_loss=4.543, mask_ins=1.337, word_ins_ml=5.943, word_reposition=0.457, ppl=213.24, wps=17358.9, ups=0.7, wpb=24704.5, bsz=2048, num_updates=7200, lr=0.000416667, gnorm=1.098, clip=0, loss_scale=16384, train_wall=139, wall=10585
2022-08-14 12:37:26 | INFO | train_inner | epoch 004:   1732 / 1858 loss=7.72, nll_loss=4.525, mask_ins=1.338, word_ins_ml=5.927, word_reposition=0.455, ppl=210.81, wps=17082.7, ups=0.69, wpb=24598.6, bsz=2048, num_updates=7300, lr=0.000413803, gnorm=1.16, clip=0, loss_scale=16384, train_wall=141, wall=10729
2022-08-14 12:39:49 | INFO | train_inner | epoch 004:   1832 / 1858 loss=7.715, nll_loss=4.527, mask_ins=1.332, word_ins_ml=5.929, word_reposition=0.454, ppl=210.15, wps=17171.1, ups=0.7, wpb=24653.3, bsz=2048, num_updates=7400, lr=0.000410997, gnorm=1.072, clip=0, loss_scale=16384, train_wall=140, wall=10872
2022-08-14 12:40:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16384.0
2022-08-14 12:40:26 | INFO | train | epoch 004 | loss 7.848 | nll_loss 4.631 | mask_ins 1.359 | word_ins_ml 6.023 | word_reposition 0.465 | ppl 230.39 | wps 16643.4 | ups 0.68 | wpb 24631.5 | bsz 2047 | num_updates 7425 | lr 0.000410305 | gnorm 1.169 | clip 0 | loss_scale 15194 | train_wall 2614 | wall 10909
2022-08-14 12:41:18 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.648 | nll_loss 4.315 | mask_ins 1.352 | word_ins_ml 5.85 | word_reposition 0.446 | ppl 200.54 | wps 44678.9 | wpb 3111.7 | bsz 255.9 | num_updates 7425 | best_loss 7.648
2022-08-14 12:41:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_uncased_Ggw/checkpoint_best.pt (epoch 4 @ 7425 updates, score 7.648) (writing took 15.532436730340123 seconds)
2022-08-14 12:43:21 | INFO | train_inner | epoch 005:     75 / 1858 loss=7.674, nll_loss=4.477, mask_ins=1.336, word_ins_ml=5.885, word_reposition=0.453, ppl=204.16, wps=11506.8, ups=0.47, wpb=24399, bsz=2030.1, num_updates=7500, lr=0.000408248, gnorm=1.175, clip=0, loss_scale=16546, train_wall=142, wall=11084
2022-08-14 12:45:45 | INFO | train_inner | epoch 005:    175 / 1858 loss=7.614, nll_loss=4.434, mask_ins=1.32, word_ins_ml=5.847, word_reposition=0.446, ppl=195.84, wps=17016.8, ups=0.69, wpb=24511.1, bsz=2048, num_updates=7600, lr=0.000405554, gnorm=1.084, clip=0, loss_scale=16384, train_wall=141, wall=11228
2022-08-14 12:48:09 | INFO | train_inner | epoch 005:    275 / 1858 loss=7.618, nll_loss=4.436, mask_ins=1.322, word_ins_ml=5.849, word_reposition=0.446, ppl=196.38, wps=16960.3, ups=0.69, wpb=24446.2, bsz=2047.9, num_updates=7700, lr=0.000402911, gnorm=1.116, clip=0, loss_scale=16384, train_wall=141, wall=11373
2022-08-14 12:50:33 | INFO | train_inner | epoch 005:    375 / 1858 loss=7.619, nll_loss=4.44, mask_ins=1.32, word_ins_ml=5.852, word_reposition=0.447, ppl=196.65, wps=17157.7, ups=0.69, wpb=24702.5, bsz=2048, num_updates=7800, lr=0.00040032, gnorm=1.081, clip=0, loss_scale=16384, train_wall=141, wall=11517
2022-08-14 12:52:57 | INFO | train_inner | epoch 005:    475 / 1858 loss=7.601, nll_loss=4.417, mask_ins=1.322, word_ins_ml=5.832, word_reposition=0.447, ppl=194.19, wps=17048.7, ups=0.69, wpb=24558.7, bsz=2048, num_updates=7900, lr=0.000397779, gnorm=1.156, clip=0, loss_scale=16384, train_wall=141, wall=11661
2022-08-14 12:54:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16384.0
2022-08-14 12:55:23 | INFO | train_inner | epoch 005:    576 / 1858 loss=7.587, nll_loss=4.404, mask_ins=1.313, word_ins_ml=5.819, word_reposition=0.455, ppl=192.32, wps=17086.2, ups=0.69, wpb=24810.3, bsz=2048, num_updates=8000, lr=0.000395285, gnorm=1.026, clip=0, loss_scale=20602, train_wall=142, wall=11806
2022-08-14 12:57:46 | INFO | train_inner | epoch 005:    676 / 1858 loss=7.584, nll_loss=4.417, mask_ins=1.314, word_ins_ml=5.831, word_reposition=0.44, ppl=191.88, wps=17285.2, ups=0.7, wpb=24781.7, bsz=2048, num_updates=8100, lr=0.000392837, gnorm=1.032, clip=0, loss_scale=16384, train_wall=140, wall=11949
2022-08-14 13:00:10 | INFO | train_inner | epoch 005:    776 / 1858 loss=7.566, nll_loss=4.392, mask_ins=1.306, word_ins_ml=5.808, word_reposition=0.452, ppl=189.48, wps=17090.8, ups=0.7, wpb=24569.6, bsz=2048, num_updates=8200, lr=0.000390434, gnorm=1.112, clip=0, loss_scale=16384, train_wall=141, wall=12093
2022-08-14 13:02:33 | INFO | train_inner | epoch 005:    876 / 1858 loss=7.574, nll_loss=4.399, mask_ins=1.312, word_ins_ml=5.815, word_reposition=0.447, ppl=190.52, wps=17159.4, ups=0.7, wpb=24623, bsz=2048, num_updates=8300, lr=0.000388075, gnorm=1.02, clip=0, loss_scale=16384, train_wall=140, wall=12236
2022-08-14 13:04:57 | INFO | train_inner | epoch 005:    976 / 1858 loss=7.562, nll_loss=4.395, mask_ins=1.307, word_ins_ml=5.811, word_reposition=0.445, ppl=188.98, wps=17192.8, ups=0.69, wpb=24799, bsz=2048, num_updates=8400, lr=0.000385758, gnorm=1.1, clip=0, loss_scale=16384, train_wall=141, wall=12381
2022-08-14 13:06:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16384.0
2022-08-14 13:07:22 | INFO | train_inner | epoch 005:   1077 / 1858 loss=7.542, nll_loss=4.377, mask_ins=1.305, word_ins_ml=5.795, word_reposition=0.442, ppl=186.42, wps=16898.3, ups=0.69, wpb=24409.4, bsz=2048, num_updates=8500, lr=0.000383482, gnorm=1.031, clip=0, loss_scale=17357, train_wall=141, wall=12525
2022-08-14 13:09:45 | INFO | train_inner | epoch 005:   1177 / 1858 loss=7.52, nll_loss=4.36, mask_ins=1.307, word_ins_ml=5.779, word_reposition=0.434, ppl=183.54, wps=17083.3, ups=0.7, wpb=24497.5, bsz=2048, num_updates=8600, lr=0.000381246, gnorm=1.059, clip=0, loss_scale=16384, train_wall=140, wall=12669
2022-08-14 13:12:10 | INFO | train_inner | epoch 005:   1277 / 1858 loss=7.516, nll_loss=4.354, mask_ins=1.303, word_ins_ml=5.774, word_reposition=0.438, ppl=182.98, wps=17095.8, ups=0.69, wpb=24673.6, bsz=2048, num_updates=8700, lr=0.000379049, gnorm=0.995, clip=0, loss_scale=16384, train_wall=141, wall=12813
2022-08-14 13:14:34 | INFO | train_inner | epoch 005:   1377 / 1858 loss=7.517, nll_loss=4.359, mask_ins=1.303, word_ins_ml=5.777, word_reposition=0.436, ppl=183.13, wps=17192.1, ups=0.69, wpb=24883.1, bsz=2048, num_updates=8800, lr=0.000376889, gnorm=1.055, clip=0, loss_scale=16384, train_wall=142, wall=12958
2022-08-14 13:16:56 | INFO | train_inner | epoch 005:   1477 / 1858 loss=7.522, nll_loss=4.352, mask_ins=1.301, word_ins_ml=5.772, word_reposition=0.449, ppl=183.77, wps=17458, ups=0.71, wpb=24637.6, bsz=2048, num_updates=8900, lr=0.000374766, gnorm=1.047, clip=0, loss_scale=16384, train_wall=138, wall=13099
2022-08-14 13:19:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16384.0
2022-08-14 13:19:21 | INFO | train_inner | epoch 005:   1578 / 1858 loss=7.481, nll_loss=4.326, mask_ins=1.297, word_ins_ml=5.749, word_reposition=0.436, ppl=178.67, wps=16987.2, ups=0.69, wpb=24763.4, bsz=2048, num_updates=9000, lr=0.000372678, gnorm=0.979, clip=0, loss_scale=18006, train_wall=143, wall=13245
2022-08-14 13:20:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-14 13:21:47 | INFO | train_inner | epoch 005:   1679 / 1858 loss=7.495, nll_loss=4.334, mask_ins=1.299, word_ins_ml=5.755, word_reposition=0.441, ppl=180.41, wps=17079.9, ups=0.69, wpb=24807.3, bsz=2048, num_updates=9100, lr=0.000370625, gnorm=1.048, clip=0, loss_scale=11517, train_wall=142, wall=13390
2022-08-14 13:24:09 | INFO | train_inner | epoch 005:   1779 / 1858 loss=7.482, nll_loss=4.323, mask_ins=1.296, word_ins_ml=5.745, word_reposition=0.441, ppl=178.72, wps=17209.2, ups=0.7, wpb=24581.3, bsz=2048, num_updates=9200, lr=0.000368605, gnorm=0.996, clip=0, loss_scale=8192, train_wall=140, wall=13533
2022-08-14 13:26:02 | INFO | train | epoch 005 | loss 7.554 | nll_loss 4.384 | mask_ins 1.309 | word_ins_ml 5.801 | word_reposition 0.444 | ppl 187.91 | wps 16691.1 | ups 0.68 | wpb 24630 | bsz 2047 | num_updates 9279 | lr 0.000367032 | gnorm 1.061 | clip 0 | loss_scale 15701 | train_wall 2609 | wall 13645
2022-08-14 13:26:53 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.504 | nll_loss 4.201 | mask_ins 1.285 | word_ins_ml 5.747 | word_reposition 0.473 | ppl 181.57 | wps 45576.8 | wpb 3111.7 | bsz 255.9 | num_updates 9279 | best_loss 7.504
2022-08-14 13:27:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_uncased_Ggw/checkpoint_best.pt (epoch 5 @ 9279 updates, score 7.504) (writing took 9.85740303620696 seconds)
2022-08-14 13:27:33 | INFO | train_inner | epoch 006:     21 / 1858 loss=7.464, nll_loss=4.301, mask_ins=1.297, word_ins_ml=5.726, word_reposition=0.442, ppl=176.55, wps=11983.8, ups=0.49, wpb=24390.7, bsz=2030.1, num_updates=9300, lr=0.000366618, gnorm=1.14, clip=0, loss_scale=8192, train_wall=140, wall=13736
2022-08-14 13:29:56 | INFO | train_inner | epoch 006:    121 / 1858 loss=7.416, nll_loss=4.269, mask_ins=1.283, word_ins_ml=5.698, word_reposition=0.436, ppl=170.8, wps=17239.4, ups=0.7, wpb=24683.1, bsz=2048, num_updates=9400, lr=0.000364662, gnorm=1.008, clip=0, loss_scale=8192, train_wall=140, wall=13879
2022-08-14 13:32:20 | INFO | train_inner | epoch 006:    221 / 1858 loss=7.399, nll_loss=4.25, mask_ins=1.289, word_ins_ml=5.681, word_reposition=0.429, ppl=168.82, wps=17073.6, ups=0.69, wpb=24569, bsz=2048, num_updates=9500, lr=0.000362738, gnorm=1, clip=0, loss_scale=8192, train_wall=141, wall=14023
2022-08-14 13:34:44 | INFO | train_inner | epoch 006:    321 / 1858 loss=7.411, nll_loss=4.253, mask_ins=1.293, word_ins_ml=5.684, word_reposition=0.433, ppl=170.14, wps=17098.3, ups=0.69, wpb=24642.3, bsz=2048, num_updates=9600, lr=0.000360844, gnorm=1.094, clip=0, loss_scale=12124, train_wall=141, wall=14167
2022-08-14 13:37:08 | INFO | train_inner | epoch 006:    421 / 1858 loss=7.379, nll_loss=4.234, mask_ins=1.281, word_ins_ml=5.667, word_reposition=0.432, ppl=166.49, wps=17008.5, ups=0.69, wpb=24472.7, bsz=2048, num_updates=9700, lr=0.000358979, gnorm=0.986, clip=0, loss_scale=16384, train_wall=141, wall=14311
2022-08-14 13:39:32 | INFO | train_inner | epoch 006:    521 / 1858 loss=7.387, nll_loss=4.244, mask_ins=1.283, word_ins_ml=5.675, word_reposition=0.429, ppl=167.39, wps=16980.8, ups=0.69, wpb=24472.9, bsz=2048, num_updates=9800, lr=0.000357143, gnorm=0.998, clip=0, loss_scale=16384, train_wall=141, wall=14455
2022-08-14 13:41:56 | INFO | train_inner | epoch 006:    621 / 1858 loss=7.393, nll_loss=4.249, mask_ins=1.279, word_ins_ml=5.68, word_reposition=0.434, ppl=168.13, wps=17225.9, ups=0.7, wpb=24736.1, bsz=2048, num_updates=9900, lr=0.000355335, gnorm=0.934, clip=0, loss_scale=16384, train_wall=140, wall=14599
2022-08-14 13:44:20 | INFO | train_inner | epoch 006:    721 / 1858 loss=7.357, nll_loss=4.223, mask_ins=1.275, word_ins_ml=5.657, word_reposition=0.424, ppl=163.93, wps=17017.8, ups=0.7, wpb=24470.4, bsz=2048, num_updates=10000, lr=0.000353553, gnorm=0.982, clip=0, loss_scale=16384, train_wall=141, wall=14743
2022-08-14 13:46:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16384.0
2022-08-14 13:46:45 | INFO | train_inner | epoch 006:    822 / 1858 loss=7.381, nll_loss=4.237, mask_ins=1.281, word_ins_ml=5.668, word_reposition=0.432, ppl=166.71, wps=17069.4, ups=0.69, wpb=24747.5, bsz=2048, num_updates=10100, lr=0.000351799, gnorm=0.968, clip=0, loss_scale=19304, train_wall=142, wall=14888
2022-08-14 13:49:08 | INFO | train_inner | epoch 006:    922 / 1858 loss=7.372, nll_loss=4.221, mask_ins=1.279, word_ins_ml=5.654, word_reposition=0.44, ppl=165.7, wps=17350.4, ups=0.7, wpb=24865.6, bsz=2048, num_updates=10200, lr=0.00035007, gnorm=0.958, clip=0, loss_scale=16384, train_wall=140, wall=15031
2022-08-14 13:51:31 | INFO | train_inner | epoch 006:   1022 / 1858 loss=7.382, nll_loss=4.237, mask_ins=1.281, word_ins_ml=5.668, word_reposition=0.432, ppl=166.76, wps=17199.2, ups=0.7, wpb=24705.5, bsz=2048, num_updates=10300, lr=0.000348367, gnorm=0.957, clip=0, loss_scale=16384, train_wall=140, wall=15175
2022-08-14 13:53:55 | INFO | train_inner | epoch 006:   1122 / 1858 loss=7.358, nll_loss=4.222, mask_ins=1.28, word_ins_ml=5.655, word_reposition=0.423, ppl=164.07, wps=17030.9, ups=0.7, wpb=24501.4, bsz=2048, num_updates=10400, lr=0.000346688, gnorm=1.015, clip=0, loss_scale=16384, train_wall=141, wall=15319
2022-08-14 13:56:18 | INFO | train_inner | epoch 006:   1222 / 1858 loss=7.338, nll_loss=4.211, mask_ins=1.27, word_ins_ml=5.644, word_reposition=0.423, ppl=161.75, wps=17138, ups=0.7, wpb=24446.9, bsz=2048, num_updates=10500, lr=0.000345033, gnorm=0.92, clip=0, loss_scale=16384, train_wall=139, wall=15461
2022-08-14 13:58:37 | INFO | train_inner | epoch 006:   1322 / 1858 loss=7.34, nll_loss=4.202, mask_ins=1.271, word_ins_ml=5.637, word_reposition=0.431, ppl=162, wps=17606.3, ups=0.72, wpb=24444.5, bsz=2047.9, num_updates=10600, lr=0.000343401, gnorm=1.055, clip=0, loss_scale=17531, train_wall=136, wall=15600
2022-08-14 13:59:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16384.0
2022-08-14 14:01:06 | INFO | train_inner | epoch 006:   1423 / 1858 loss=7.353, nll_loss=4.216, mask_ins=1.276, word_ins_ml=5.649, word_reposition=0.428, ppl=163.5, wps=16709.3, ups=0.67, wpb=24859.2, bsz=2048, num_updates=10700, lr=0.000341793, gnorm=0.966, clip=0, loss_scale=24819, train_wall=146, wall=15749
2022-08-14 14:03:29 | INFO | train_inner | epoch 006:   1523 / 1858 loss=7.347, nll_loss=4.217, mask_ins=1.269, word_ins_ml=5.65, word_reposition=0.429, ppl=162.82, wps=17313.8, ups=0.7, wpb=24771.3, bsz=2048, num_updates=10800, lr=0.000340207, gnorm=0.933, clip=0, loss_scale=16384, train_wall=140, wall=15892
2022-08-14 14:05:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-14 14:05:54 | INFO | train_inner | epoch 006:   1624 / 1858 loss=7.353, nll_loss=4.22, mask_ins=1.269, word_ins_ml=5.652, word_reposition=0.432, ppl=163.52, wps=17105.5, ups=0.69, wpb=24834.3, bsz=2048, num_updates=10900, lr=0.000338643, gnorm=0.965, clip=0, loss_scale=14762, train_wall=142, wall=16037
2022-08-14 14:08:18 | INFO | train_inner | epoch 006:   1724 / 1858 loss=7.323, nll_loss=4.197, mask_ins=1.269, word_ins_ml=5.631, word_reposition=0.423, ppl=160.16, wps=17063.1, ups=0.69, wpb=24628.8, bsz=2048, num_updates=11000, lr=0.0003371, gnorm=0.979, clip=0, loss_scale=8192, train_wall=141, wall=16181
2022-08-14 14:10:41 | INFO | train_inner | epoch 006:   1824 / 1858 loss=7.308, nll_loss=4.187, mask_ins=1.266, word_ins_ml=5.622, word_reposition=0.419, ppl=158.42, wps=17204.1, ups=0.7, wpb=24646.8, bsz=2048, num_updates=11100, lr=0.000335578, gnorm=0.944, clip=0, loss_scale=8192, train_wall=140, wall=16325
2022-08-14 14:11:30 | INFO | train | epoch 006 | loss 7.366 | nll_loss 4.227 | mask_ins 1.277 | word_ins_ml 5.659 | word_reposition 0.43 | ppl 164.99 | wps 16748.6 | ups 0.68 | wpb 24631.8 | bsz 2047 | num_updates 11134 | lr 0.000335065 | gnorm 0.983 | clip 0 | loss_scale 14739 | train_wall 2608 | wall 16373
2022-08-14 14:12:22 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.365 | nll_loss 4.081 | mask_ins 1.273 | word_ins_ml 5.635 | word_reposition 0.457 | ppl 164.87 | wps 44590.4 | wpb 3111.7 | bsz 255.9 | num_updates 11134 | best_loss 7.365
2022-08-14 14:12:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_uncased_Ggw/checkpoint_best.pt (epoch 6 @ 11134 updates, score 7.365) (writing took 5.782470319420099 seconds)
2022-08-14 14:14:03 | INFO | train_inner | epoch 007:     66 / 1858 loss=7.288, nll_loss=4.154, mask_ins=1.264, word_ins_ml=5.594, word_reposition=0.43, ppl=156.25, wps=12202.6, ups=0.5, wpb=24615, bsz=2030.1, num_updates=11200, lr=0.000334077, gnorm=0.97, clip=0, loss_scale=8192, train_wall=141, wall=16526
2022-08-14 14:16:27 | INFO | train_inner | epoch 007:    166 / 1858 loss=7.257, nll_loss=4.127, mask_ins=1.258, word_ins_ml=5.57, word_reposition=0.428, ppl=152.94, wps=17247.9, ups=0.7, wpb=24759.9, bsz=2048, num_updates=11300, lr=0.000332595, gnorm=0.999, clip=0, loss_scale=8192, train_wall=140, wall=16670
2022-08-14 14:18:51 | INFO | train_inner | epoch 007:    266 / 1858 loss=7.25, nll_loss=4.128, mask_ins=1.257, word_ins_ml=5.571, word_reposition=0.422, ppl=152.24, wps=17124.8, ups=0.69, wpb=24648.8, bsz=2047.9, num_updates=11400, lr=0.000331133, gnorm=0.909, clip=0, loss_scale=8847, train_wall=141, wall=16814
2022-08-14 14:21:14 | INFO | train_inner | epoch 007:    366 / 1858 loss=7.243, nll_loss=4.123, mask_ins=1.261, word_ins_ml=5.566, word_reposition=0.416, ppl=151.48, wps=17164.4, ups=0.7, wpb=24547.7, bsz=2048, num_updates=11500, lr=0.00032969, gnorm=0.932, clip=0, loss_scale=16384, train_wall=140, wall=16957
2022-08-14 14:23:37 | INFO | train_inner | epoch 007:    466 / 1858 loss=7.262, nll_loss=4.142, mask_ins=1.259, word_ins_ml=5.583, word_reposition=0.42, ppl=153.51, wps=17136.7, ups=0.7, wpb=24544.3, bsz=2048, num_updates=11600, lr=0.000328266, gnorm=0.938, clip=0, loss_scale=16384, train_wall=140, wall=17100
2022-08-14 14:24:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-14 14:26:02 | INFO | train_inner | epoch 007:    567 / 1858 loss=7.253, nll_loss=4.127, mask_ins=1.261, word_ins_ml=5.57, word_reposition=0.422, ppl=152.51, wps=17064.7, ups=0.69, wpb=24741.9, bsz=2048, num_updates=11700, lr=0.00032686, gnorm=0.959, clip=0, loss_scale=12734, train_wall=142, wall=17245
2022-08-14 14:27:53 | INFO | train_inner | epoch 007:    667 / 1858 loss=7.26, nll_loss=4.133, mask_ins=1.257, word_ins_ml=5.575, word_reposition=0.427, ppl=153.23, wps=22255.4, ups=0.9, wpb=24683.6, bsz=2048, num_updates=11800, lr=0.000325472, gnorm=0.982, clip=0, loss_scale=8192, train_wall=108, wall=17356
2022-08-14 14:29:39 | INFO | train_inner | epoch 007:    767 / 1858 loss=7.251, nll_loss=4.128, mask_ins=1.255, word_ins_ml=5.57, word_reposition=0.425, ppl=152.3, wps=23196.3, ups=0.94, wpb=24605.1, bsz=2048, num_updates=11900, lr=0.000324102, gnorm=0.93, clip=0, loss_scale=8192, train_wall=103, wall=17462
2022-08-14 14:31:25 | INFO | train_inner | epoch 007:    867 / 1858 loss=7.224, nll_loss=4.109, mask_ins=1.257, word_ins_ml=5.554, word_reposition=0.413, ppl=149.54, wps=23172.6, ups=0.94, wpb=24562.7, bsz=2048, num_updates=12000, lr=0.000322749, gnorm=0.953, clip=0, loss_scale=8192, train_wall=103, wall=17568
2022-08-14 14:33:11 | INFO | train_inner | epoch 007:    967 / 1858 loss=7.223, nll_loss=4.111, mask_ins=1.25, word_ins_ml=5.555, word_reposition=0.418, ppl=149.44, wps=23419.8, ups=0.95, wpb=24782.3, bsz=2048, num_updates=12100, lr=0.000321412, gnorm=0.923, clip=0, loss_scale=8192, train_wall=103, wall=17674
2022-08-14 14:34:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-14 14:34:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-14 14:34:59 | INFO | train_inner | epoch 007:   1069 / 1858 loss=7.221, nll_loss=4.106, mask_ins=1.252, word_ins_ml=5.551, word_reposition=0.419, ppl=149.21, wps=22749.6, ups=0.93, wpb=24532.7, bsz=2048, num_updates=12200, lr=0.000320092, gnorm=1.083, clip=0, loss_scale=5562, train_wall=105, wall=17782
2022-08-14 14:36:44 | INFO | train_inner | epoch 007:   1169 / 1858 loss=7.241, nll_loss=4.124, mask_ins=1.254, word_ins_ml=5.566, word_reposition=0.42, ppl=151.31, wps=23126.8, ups=0.95, wpb=24468.5, bsz=2048, num_updates=12300, lr=0.000318788, gnorm=1.023, clip=0, loss_scale=2048, train_wall=103, wall=17888
2022-08-14 14:38:30 | INFO | train_inner | epoch 007:   1269 / 1858 loss=7.215, nll_loss=4.094, mask_ins=1.254, word_ins_ml=5.54, word_reposition=0.421, ppl=148.57, wps=23431.2, ups=0.95, wpb=24746.5, bsz=2048, num_updates=12400, lr=0.0003175, gnorm=0.958, clip=0, loss_scale=2048, train_wall=102, wall=17993
2022-08-14 14:40:15 | INFO | train_inner | epoch 007:   1369 / 1858 loss=7.222, nll_loss=4.105, mask_ins=1.252, word_ins_ml=5.55, word_reposition=0.421, ppl=149.32, wps=23274.4, ups=0.95, wpb=24552.1, bsz=2048, num_updates=12500, lr=0.000316228, gnorm=0.913, clip=0, loss_scale=2048, train_wall=102, wall=18099
2022-08-14 14:42:01 | INFO | train_inner | epoch 007:   1469 / 1858 loss=7.216, nll_loss=4.109, mask_ins=1.249, word_ins_ml=5.552, word_reposition=0.414, ppl=148.64, wps=23267.1, ups=0.95, wpb=24592.4, bsz=2048, num_updates=12600, lr=0.00031497, gnorm=0.961, clip=0, loss_scale=2048, train_wall=102, wall=18204
2022-08-14 14:43:47 | INFO | train_inner | epoch 007:   1569 / 1858 loss=7.211, nll_loss=4.105, mask_ins=1.251, word_ins_ml=5.548, word_reposition=0.412, ppl=148.21, wps=23432.7, ups=0.95, wpb=24729.9, bsz=2048, num_updates=12700, lr=0.000313728, gnorm=0.909, clip=0, loss_scale=2683, train_wall=102, wall=18310
2022-08-14 14:45:32 | INFO | train_inner | epoch 007:   1669 / 1858 loss=7.216, nll_loss=4.108, mask_ins=1.25, word_ins_ml=5.551, word_reposition=0.414, ppl=148.62, wps=23344.1, ups=0.95, wpb=24652.4, bsz=2048, num_updates=12800, lr=0.0003125, gnorm=0.97, clip=0, loss_scale=4096, train_wall=102, wall=18416
2022-08-14 14:47:19 | INFO | train_inner | epoch 007:   1769 / 1858 loss=7.209, nll_loss=4.101, mask_ins=1.246, word_ins_ml=5.545, word_reposition=0.419, ppl=147.99, wps=23039.5, ups=0.94, wpb=24519.5, bsz=2048, num_updates=12900, lr=0.000311286, gnorm=0.941, clip=0, loss_scale=4096, train_wall=103, wall=18522
2022-08-14 14:48:54 | INFO | train | epoch 007 | loss 7.234 | nll_loss 4.116 | mask_ins 1.255 | word_ins_ml 5.56 | word_reposition 0.419 | ppl 150.5 | wps 20360.3 | ups 0.83 | wpb 24631.8 | bsz 2047 | num_updates 12989 | lr 0.000310218 | gnorm 0.958 | clip 0 | loss_scale 6955 | train_wall 2126 | wall 18618
2022-08-14 14:49:30 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.224 | nll_loss 4.001 | mask_ins 1.249 | word_ins_ml 5.552 | word_reposition 0.424 | ppl 149.55 | wps 64936.7 | wpb 3111.7 | bsz 255.9 | num_updates 12989 | best_loss 7.224
2022-08-14 14:49:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_uncased_Ggw/checkpoint_best.pt (epoch 7 @ 12989 updates, score 7.224) (writing took 23.244977790862322 seconds)
2022-08-14 14:50:05 | INFO | train_inner | epoch 008:     11 / 1858 loss=7.197, nll_loss=4.091, mask_ins=1.251, word_ins_ml=5.536, word_reposition=0.411, ppl=146.73, wps=14807.6, ups=0.6, wpb=24629.1, bsz=2030.1, num_updates=13000, lr=0.000310087, gnorm=0.974, clip=0, loss_scale=4096, train_wall=104, wall=18688
2022-08-14 14:51:51 | INFO | train_inner | epoch 008:    111 / 1858 loss=7.143, nll_loss=4.024, mask_ins=1.246, word_ins_ml=5.478, word_reposition=0.418, ppl=141.33, wps=23229, ups=0.94, wpb=24669.9, bsz=2048, num_updates=13100, lr=0.000308901, gnorm=0.96, clip=0, loss_scale=4096, train_wall=103, wall=18794
2022-08-14 14:53:38 | INFO | train_inner | epoch 008:    211 / 1858 loss=7.147, nll_loss=4.035, mask_ins=1.241, word_ins_ml=5.487, word_reposition=0.419, ppl=141.74, wps=23310.9, ups=0.94, wpb=24787.9, bsz=2048, num_updates=13200, lr=0.000307729, gnorm=0.893, clip=0, loss_scale=4874, train_wall=103, wall=18901
2022-08-14 14:55:24 | INFO | train_inner | epoch 008:    311 / 1858 loss=7.14, nll_loss=4.028, mask_ins=1.238, word_ins_ml=5.481, word_reposition=0.421, ppl=141.02, wps=23265.5, ups=0.94, wpb=24675.1, bsz=2048, num_updates=13300, lr=0.00030657, gnorm=0.92, clip=0, loss_scale=8192, train_wall=103, wall=19007
2022-08-14 14:57:11 | INFO | train_inner | epoch 008:    411 / 1858 loss=7.16, nll_loss=4.049, mask_ins=1.24, word_ins_ml=5.499, word_reposition=0.421, ppl=143.01, wps=23075.1, ups=0.93, wpb=24773, bsz=2048, num_updates=13400, lr=0.000305424, gnorm=0.913, clip=0, loss_scale=8192, train_wall=104, wall=19114
2022-08-14 14:58:57 | INFO | train_inner | epoch 008:    511 / 1858 loss=7.133, nll_loss=4.022, mask_ins=1.238, word_ins_ml=5.476, word_reposition=0.42, ppl=140.35, wps=23161.8, ups=0.94, wpb=24545.5, bsz=2048, num_updates=13500, lr=0.00030429, gnorm=0.891, clip=0, loss_scale=8192, train_wall=103, wall=19220
2022-08-14 15:00:42 | INFO | train_inner | epoch 008:    611 / 1858 loss=7.131, nll_loss=4.035, mask_ins=1.24, word_ins_ml=5.487, word_reposition=0.405, ppl=140.2, wps=23509.9, ups=0.95, wpb=24747, bsz=2048, num_updates=13600, lr=0.00030317, gnorm=0.889, clip=0, loss_scale=8192, train_wall=102, wall=19325
2022-08-14 15:02:28 | INFO | train_inner | epoch 008:    711 / 1858 loss=7.116, nll_loss=4.018, mask_ins=1.239, word_ins_ml=5.472, word_reposition=0.405, ppl=138.7, wps=23396.1, ups=0.95, wpb=24656.5, bsz=2048, num_updates=13700, lr=0.000302061, gnorm=0.909, clip=0, loss_scale=8765, train_wall=102, wall=19431
2022-08-14 15:04:14 | INFO | train_inner | epoch 008:    811 / 1858 loss=7.128, nll_loss=4.033, mask_ins=1.239, word_ins_ml=5.484, word_reposition=0.405, ppl=139.92, wps=23180.1, ups=0.94, wpb=24579.8, bsz=2048, num_updates=13800, lr=0.000300965, gnorm=0.928, clip=0, loss_scale=16384, train_wall=103, wall=19537
2022-08-14 15:06:00 | INFO | train_inner | epoch 008:    911 / 1858 loss=7.118, nll_loss=4.029, mask_ins=1.234, word_ins_ml=5.481, word_reposition=0.402, ppl=138.89, wps=23074.5, ups=0.94, wpb=24483.7, bsz=2048, num_updates=13900, lr=0.00029988, gnorm=0.874, clip=0, loss_scale=16384, train_wall=103, wall=19643
2022-08-14 15:07:46 | INFO | train_inner | epoch 008:   1011 / 1858 loss=7.114, nll_loss=4.026, mask_ins=1.238, word_ins_ml=5.478, word_reposition=0.399, ppl=138.56, wps=23260.6, ups=0.95, wpb=24599.5, bsz=2048, num_updates=14000, lr=0.000298807, gnorm=0.909, clip=0, loss_scale=16384, train_wall=103, wall=19749
2022-08-14 15:09:33 | INFO | train_inner | epoch 008:   1111 / 1858 loss=7.136, nll_loss=4.034, mask_ins=1.242, word_ins_ml=5.485, word_reposition=0.409, ppl=140.67, wps=23232.5, ups=0.93, wpb=24857.5, bsz=2048, num_updates=14100, lr=0.000297746, gnorm=0.901, clip=0, loss_scale=16384, train_wall=104, wall=19856
2022-08-14 15:11:20 | INFO | train_inner | epoch 008:   1211 / 1858 loss=7.131, nll_loss=4.025, mask_ins=1.236, word_ins_ml=5.477, word_reposition=0.418, ppl=140.18, wps=23101.2, ups=0.93, wpb=24868.1, bsz=2048, num_updates=14200, lr=0.000296695, gnorm=0.933, clip=0, loss_scale=16384, train_wall=104, wall=19963
2022-08-14 15:13:08 | INFO | train_inner | epoch 008:   1311 / 1858 loss=7.14, nll_loss=4.04, mask_ins=1.238, word_ins_ml=5.49, word_reposition=0.411, ppl=141.02, wps=22677.8, ups=0.93, wpb=24506.3, bsz=2048, num_updates=14300, lr=0.000295656, gnorm=0.877, clip=0, loss_scale=31949, train_wall=105, wall=20071
2022-08-14 15:14:54 | INFO | train_inner | epoch 008:   1411 / 1858 loss=7.123, nll_loss=4.027, mask_ins=1.233, word_ins_ml=5.479, word_reposition=0.411, ppl=139.37, wps=23225.2, ups=0.94, wpb=24673.7, bsz=2048, num_updates=14400, lr=0.000294628, gnorm=0.933, clip=0, loss_scale=32768, train_wall=103, wall=20178
2022-08-14 15:16:40 | INFO | train_inner | epoch 008:   1511 / 1858 loss=7.127, nll_loss=4.031, mask_ins=1.236, word_ins_ml=5.482, word_reposition=0.409, ppl=139.79, wps=23367, ups=0.94, wpb=24751, bsz=2048, num_updates=14500, lr=0.00029361, gnorm=0.865, clip=0, loss_scale=32768, train_wall=103, wall=20284
2022-08-14 15:18:26 | INFO | train_inner | epoch 008:   1611 / 1858 loss=7.109, nll_loss=4.017, mask_ins=1.229, word_ins_ml=5.469, word_reposition=0.411, ppl=138.06, wps=23244.7, ups=0.95, wpb=24556.1, bsz=2047.9, num_updates=14600, lr=0.000292603, gnorm=0.868, clip=0, loss_scale=32768, train_wall=102, wall=20389
2022-08-14 15:18:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16384.0
2022-08-14 15:20:13 | INFO | train_inner | epoch 008:   1712 / 1858 loss=7.099, nll_loss=4.01, mask_ins=1.229, word_ins_ml=5.464, word_reposition=0.406, ppl=137.07, wps=22844.9, ups=0.94, wpb=24393.8, bsz=2048, num_updates=14700, lr=0.000291606, gnorm=0.87, clip=0, loss_scale=18168, train_wall=104, wall=20496
2022-08-14 15:21:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-14 15:21:59 | INFO | train_inner | epoch 008:   1813 / 1858 loss=7.109, nll_loss=4.022, mask_ins=1.232, word_ins_ml=5.474, word_reposition=0.404, ppl=138.08, wps=23044.8, ups=0.94, wpb=24564.8, bsz=2048, num_updates=14800, lr=0.000290619, gnorm=0.927, clip=0, loss_scale=11842, train_wall=103, wall=20603
2022-08-14 15:22:47 | INFO | train | epoch 008 | loss 7.128 | nll_loss 4.029 | mask_ins 1.237 | word_ins_ml 5.481 | word_reposition 0.411 | ppl 139.92 | wps 22490.8 | ups 0.91 | wpb 24632.3 | bsz 2047 | num_updates 14845 | lr 0.000290178 | gnorm 0.906 | clip 0 | loss_scale 15992 | train_wall 1914 | wall 20650
2022-08-14 15:23:22 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.153 | nll_loss 3.95 | mask_ins 1.231 | word_ins_ml 5.505 | word_reposition 0.417 | ppl 142.34 | wps 65928.6 | wpb 3111.7 | bsz 255.9 | num_updates 14845 | best_loss 7.153
2022-08-14 15:23:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_uncased_Ggw/checkpoint_best.pt (epoch 8 @ 14845 updates, score 7.153) (writing took 9.219250770285726 seconds)
2022-08-14 15:24:30 | INFO | train_inner | epoch 009:     55 / 1858 loss=7.097, nll_loss=4.002, mask_ins=1.227, word_ins_ml=5.457, word_reposition=0.414, ppl=136.92, wps=16064.3, ups=0.66, wpb=24253.5, bsz=2030.1, num_updates=14900, lr=0.000289642, gnorm=0.915, clip=0, loss_scale=8192, train_wall=104, wall=20754
2022-08-14 15:26:18 | INFO | train_inner | epoch 009:    155 / 1858 loss=7.034, nll_loss=3.945, mask_ins=1.224, word_ins_ml=5.407, word_reposition=0.403, ppl=131.05, wps=22904.8, ups=0.93, wpb=24564.9, bsz=2048, num_updates=15000, lr=0.000288675, gnorm=0.919, clip=0, loss_scale=8192, train_wall=104, wall=20861
2022-08-14 15:28:04 | INFO | train_inner | epoch 009:    255 / 1858 loss=7.051, nll_loss=3.953, mask_ins=1.222, word_ins_ml=5.414, word_reposition=0.415, ppl=132.59, wps=23274.9, ups=0.94, wpb=24773.1, bsz=2048, num_updates=15100, lr=0.000287718, gnorm=0.843, clip=0, loss_scale=8192, train_wall=103, wall=20967
2022-08-14 15:29:50 | INFO | train_inner | epoch 009:    355 / 1858 loss=7.064, nll_loss=3.969, mask_ins=1.225, word_ins_ml=5.427, word_reposition=0.412, ppl=133.8, wps=23392.7, ups=0.95, wpb=24723.7, bsz=2048, num_updates=15200, lr=0.00028677, gnorm=0.929, clip=0, loss_scale=8192, train_wall=102, wall=21073
2022-08-14 15:31:35 | INFO | train_inner | epoch 009:    455 / 1858 loss=7.043, nll_loss=3.955, mask_ins=1.22, word_ins_ml=5.415, word_reposition=0.408, ppl=131.91, wps=23365.1, ups=0.95, wpb=24688.3, bsz=2047.9, num_updates=15300, lr=0.000285831, gnorm=0.919, clip=0, loss_scale=11796, train_wall=102, wall=21179
2022-08-14 15:32:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-14 15:33:22 | INFO | train_inner | epoch 009:    556 / 1858 loss=7.05, nll_loss=3.959, mask_ins=1.226, word_ins_ml=5.419, word_reposition=0.406, ppl=132.51, wps=23287.8, ups=0.94, wpb=24766.8, bsz=2048, num_updates=15400, lr=0.000284901, gnorm=0.861, clip=0, loss_scale=12410, train_wall=103, wall=21285
2022-08-14 15:35:07 | INFO | train_inner | epoch 009:    656 / 1858 loss=7.072, nll_loss=3.979, mask_ins=1.229, word_ins_ml=5.436, word_reposition=0.407, ppl=134.58, wps=23379, ups=0.95, wpb=24654.5, bsz=2048, num_updates=15500, lr=0.000283981, gnorm=0.933, clip=0, loss_scale=8192, train_wall=102, wall=21390
2022-08-14 15:36:52 | INFO | train_inner | epoch 009:    756 / 1858 loss=7.056, nll_loss=3.968, mask_ins=1.225, word_ins_ml=5.427, word_reposition=0.405, ppl=133.09, wps=23468.7, ups=0.95, wpb=24638.9, bsz=2048, num_updates=15600, lr=0.000283069, gnorm=0.914, clip=0, loss_scale=8192, train_wall=102, wall=21495
2022-08-14 15:38:38 | INFO | train_inner | epoch 009:    856 / 1858 loss=7.052, nll_loss=3.963, mask_ins=1.221, word_ins_ml=5.422, word_reposition=0.409, ppl=132.72, wps=23299.1, ups=0.95, wpb=24569.5, bsz=2048, num_updates=15700, lr=0.000282166, gnorm=0.862, clip=0, loss_scale=8192, train_wall=102, wall=21601
2022-08-14 15:40:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-14 15:40:26 | INFO | train_inner | epoch 009:    957 / 1858 loss=7.065, nll_loss=3.969, mask_ins=1.228, word_ins_ml=5.427, word_reposition=0.41, ppl=133.86, wps=22880.5, ups=0.93, wpb=24695.3, bsz=2048, num_updates=15800, lr=0.000281272, gnorm=0.904, clip=0, loss_scale=7219, train_wall=105, wall=21709
2022-08-14 15:42:12 | INFO | train_inner | epoch 009:   1057 / 1858 loss=7.057, nll_loss=3.969, mask_ins=1.22, word_ins_ml=5.426, word_reposition=0.411, ppl=133.17, wps=23109.6, ups=0.94, wpb=24566.7, bsz=2048, num_updates=15900, lr=0.000280386, gnorm=0.874, clip=0, loss_scale=4096, train_wall=103, wall=21815
2022-08-14 15:43:59 | INFO | train_inner | epoch 009:   1157 / 1858 loss=7.031, nll_loss=3.939, mask_ins=1.218, word_ins_ml=5.401, word_reposition=0.413, ppl=130.8, wps=22867.1, ups=0.93, wpb=24536, bsz=2048, num_updates=16000, lr=0.000279508, gnorm=0.861, clip=0, loss_scale=4096, train_wall=104, wall=21922
2022-08-14 15:45:46 | INFO | train_inner | epoch 009:   1257 / 1858 loss=7.041, nll_loss=3.952, mask_ins=1.222, word_ins_ml=5.412, word_reposition=0.407, ppl=131.65, wps=23323.7, ups=0.94, wpb=24842.7, bsz=2048, num_updates=16100, lr=0.000278639, gnorm=0.872, clip=0, loss_scale=4096, train_wall=103, wall=22029
2022-08-14 15:47:33 | INFO | train_inner | epoch 009:   1357 / 1858 loss=7.038, nll_loss=3.954, mask_ins=1.22, word_ins_ml=5.414, word_reposition=0.405, ppl=131.43, wps=22979.8, ups=0.93, wpb=24654, bsz=2048, num_updates=16200, lr=0.000277778, gnorm=0.882, clip=0, loss_scale=4096, train_wall=104, wall=22136
2022-08-14 15:49:19 | INFO | train_inner | epoch 009:   1457 / 1858 loss=7.033, nll_loss=3.954, mask_ins=1.218, word_ins_ml=5.413, word_reposition=0.403, ppl=131, wps=23271, ups=0.94, wpb=24642.6, bsz=2048, num_updates=16300, lr=0.000276924, gnorm=0.877, clip=0, loss_scale=4588, train_wall=103, wall=22242
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
