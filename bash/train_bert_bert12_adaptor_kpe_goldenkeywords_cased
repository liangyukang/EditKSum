nohup: ignoring input
2022-07-25 13:45:57 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:12552
2022-07-25 13:45:58 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:12552
2022-07-25 13:45:58 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:12552
2022-07-25 13:45:58 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:12552
2022-07-25 13:45:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-25 13:45:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-25 13:45:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-25 13:45:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-25 13:45:59 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-25 13:45:59 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-25 13:45:59 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-25 13:45:59 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-25 13:45:59 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-25 13:45:59 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-25 13:45:59 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-25 13:45:59 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-25 13:46:02 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:12552', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='keyword', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-25 13:46:02 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-25 13:46:02 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
2022-07-25 13:46:02 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.source
0it [00:00, ?it/s]2022-07-25 13:46:02 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.target
2022-07-25 13:46:02 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]380it [00:00, 3798.87it/s]384it [00:00, 3834.66it/s]386it [00:00, 3856.19it/s]385it [00:00, 3842.07it/s]760it [00:00, 3375.88it/s]768it [00:00, 3407.75it/s]772it [00:00, 3478.64it/s]770it [00:00, 3472.80it/s]1121it [00:00, 3475.59it/s]1133it [00:00, 3507.83it/s]1135it [00:00, 3542.19it/s]1120it [00:00, 3469.05it/s]1471it [00:00, 3345.97it/s]1487it [00:00, 3390.27it/s]1491it [00:00, 3393.74it/s]1469it [00:00, 3342.19it/s]1842it [00:00, 3468.16it/s]1861it [00:00, 3508.30it/s]1873it [00:00, 3537.10it/s]1855it [00:00, 3517.37it/s]2229it [00:00, 3599.77it/s]2229it [00:00, 3450.57it/s]2238it [00:00, 3448.07it/s]2238it [00:00, 3440.37it/s]2591it [00:00, 3499.47it/s]2625it [00:00, 3608.84it/s]2639it [00:00, 3619.62it/s]2612it [00:00, 3530.66it/s]2985it [00:00, 3632.98it/s]3019it [00:00, 3709.43it/s]3030it [00:00, 3708.62it/s]3008it [00:00, 3653.68it/s]3350it [00:00, 3471.03it/s]3392it [00:00, 3583.86it/s]3403it [00:00, 3554.01it/s]3375it [00:00, 3545.72it/s]3732it [00:01, 3573.11it/s]3780it [00:01, 3670.11it/s]3791it [00:01, 3649.73it/s]3744it [00:01, 3586.50it/s]4092it [00:01, 3487.42it/s]4149it [00:01, 3556.97it/s]4104it [00:01, 3497.62it/s]4158it [00:01, 3520.39it/s]4454it [00:01, 3523.02it/s]4528it [00:01, 3624.52it/s]4481it [00:01, 3575.38it/s]4542it [00:01, 3611.18it/s]4808it [00:01, 3412.87it/s]4892it [00:01, 3497.02it/s]4840it [00:01, 3467.18it/s]4905it [00:01, 3502.30it/s]5183it [00:01, 3508.81it/s]5268it [00:01, 3572.15it/s]5201it [00:01, 3506.87it/s]5266it [00:01, 3533.24it/s]5553it [00:01, 3563.66it/s]5572it [00:01, 3566.14it/s]5627it [00:01, 3438.13it/s]5621it [00:01, 3427.44it/s]5979it [00:01, 3468.86it/s]5973it [00:01, 3413.82it/s]5911it [00:01, 3341.54it/s]5930it [00:01, 3390.84it/s]6327it [00:01, 3448.71it/s]6328it [00:01, 3450.68it/s]6262it [00:01, 3386.58it/s]6284it [00:01, 3430.94it/s]6604it [00:02, 2024.67it/s]6674it [00:02, 2012.09it/s]6673it [00:02, 1991.18it/s]6629it [00:02, 1997.21it/s]6954it [00:02, 2314.07it/s]7029it [00:02, 2312.68it/s]7025it [00:02, 2286.84it/s]6966it [00:02, 2262.46it/s]7305it [00:02, 2574.98it/s]7318it [00:02, 2534.07it/s]7347it [00:02, 2444.87it/s]7331it [00:02, 2427.70it/s]7616it [00:02, 2632.26it/s]7700it [00:02, 2697.81it/s]7665it [00:02, 2640.58it/s]7627it [00:02, 2624.22it/s]7965it [00:02, 2844.24it/s]8014it [00:02, 2853.54it/s]8044it [00:02, 2881.98it/s]7976it [00:02, 2814.02it/s]8281it [00:02, 2855.05it/s]8366it [00:02, 2903.97it/s]8333it [00:02, 2871.76it/s]8290it [00:02, 2783.15it/s]8631it [00:02, 3027.60it/s]8722it [00:02, 3077.98it/s]8668it [00:02, 2999.75it/s]8643it [00:02, 2977.92it/s]8965it [00:02, 3113.34it/s]9049it [00:02, 3029.53it/s]8996it [00:02, 3127.87it/s]9011it [00:02, 2996.02it/s]9290it [00:02, 3051.27it/s]9405it [00:02, 3174.44it/s]9362it [00:02, 3136.43it/s]9323it [00:02, 3034.10it/s]9641it [00:03, 3179.47it/s]9758it [00:03, 3274.77it/s]9718it [00:03, 3253.91it/s]9679it [00:03, 3178.22it/s]10094it [00:03, 3194.42it/s]9967it [00:03, 3009.82it/s]10052it [00:03, 3134.94it/s]10006it [00:03, 3117.25it/s]10450it [00:03, 3296.40it/s]10319it [00:03, 3149.25it/s]10401it [00:03, 3232.70it/s]10358it [00:03, 3221.90it/s]10673it [00:03, 3259.78it/s]10785it [00:03, 3185.18it/s]10697it [00:03, 3268.99it/s]10730it [00:03, 3145.66it/s]11138it [00:03, 3282.99it/s]11004it [00:03, 3165.90it/s]11065it [00:03, 3202.22it/s]11028it [00:03, 3168.69it/s]11494it [00:03, 3361.78it/s]11341it [00:03, 3222.80it/s]11418it [00:03, 3295.09it/s]11378it [00:03, 3262.09it/s]11667it [00:03, 3141.74it/s]11833it [00:03, 3231.66it/s]11750it [00:03, 3186.97it/s]11707it [00:03, 3120.00it/s]12012it [00:03, 3228.97it/s]12190it [00:03, 3326.90it/s]12103it [00:03, 3283.50it/s]12059it [00:03, 3232.88it/s]12366it [00:03, 3318.51it/s]12525it [00:03, 3213.38it/s]12434it [00:03, 3153.90it/s]12388it [00:03, 3147.78it/s]12700it [00:04, 3204.32it/s]12878it [00:04, 3302.88it/s]12788it [00:04, 3263.47it/s]12741it [00:04, 3255.85it/s]13041it [00:04, 3263.03it/s]13224it [00:04, 3348.26it/s]13117it [00:04, 3213.65it/s]13093it [00:04, 3330.46it/s]13368it [00:04, 3193.16it/s]
13368it [00:04, 3162.88it/s]
2022-07-25 13:46:07 | INFO | root | success load 13368 data
2022-07-25 13:46:07 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-25 13:46:07 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-25 13:46:07 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-25 13:46:07 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-25 13:46:07 | INFO | transformer.tokenization_utils | loading file None
2022-07-25 13:46:07 | INFO | transformer.tokenization_utils | loading file None
2022-07-25 13:46:07 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
13368it [00:04, 3152.85it/s]
13368it [00:04, 3156.57it/s]
2022-07-25 13:46:07 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-25 13:46:07 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-25 13:46:07 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-25 13:46:11 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-25 13:46:11 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-25 13:46:11 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-25 13:46:11 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-25 13:46:11 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-25 13:46:13 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-25 13:46:13 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-07-25 13:46:13 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-07-25 13:46:13 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-25 13:46:13 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 142456835)
2022-07-25 13:46:13 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-07-25 13:46:13 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-25 13:46:15 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-25 13:46:15 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-25 13:46:15 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_last.pt
2022-07-25 13:46:15 | INFO | fairseq.trainer | loading train data for epoch 1
2022-07-25 13:46:15 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.source
2022-07-25 13:46:15 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.target
2022-07-25 13:46:15 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]325it [00:00, 3246.89it/s]650it [00:00, 2921.32it/s]983it [00:00, 3095.02it/s]1305it [00:00, 3139.22it/s]1621it [00:00, 2989.75it/s]1956it [00:00, 3106.16it/s]2270it [00:00, 3115.82it/s]2583it [00:00, 2898.53it/s]2914it [00:00, 3018.32it/s]3219it [00:01, 2942.00it/s]3543it [00:01, 3027.96it/s]3866it [00:01, 3086.94it/s]4177it [00:01, 2988.51it/s]4493it [00:01, 3035.42it/s]4820it [00:01, 3102.45it/s]5132it [00:01, 2983.29it/s]5466it [00:01, 3084.70it/s]5777it [00:01, 2972.19it/s]6108it [00:02, 3067.12it/s]6433it [00:02, 3118.57it/s]6747it [00:02, 3115.68it/s]7119it [00:02, 3293.39it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]367it [00:00, 3666.26it/s]352it [00:00, 3519.22it/s]360it [00:00, 3594.56it/s]734it [00:00, 3460.23it/s]704it [00:00, 3366.14it/s]720it [00:00, 3398.09it/s]1098it [00:00, 3537.41it/s]1058it [00:00, 3439.42it/s]1099it [00:00, 3567.67it/s]1461it [00:00, 3572.84it/s]1410it [00:00, 3466.72it/s]1470it [00:00, 3622.42it/s]1819it [00:00, 3421.23it/s]1757it [00:00, 3320.48it/s]1833it [00:00, 3434.87it/s]2194it [00:00, 3528.18it/s]2125it [00:00, 3435.63it/s]2211it [00:00, 3544.79it/s]7450it [00:03, 1105.56it/s]2470it [00:00, 3311.56it/s]2549it [00:00, 3383.60it/s]2568it [00:00, 3401.92it/s]7809it [00:03, 1413.78it/s]2828it [00:00, 3390.80it/s]2914it [00:00, 3463.69it/s]2931it [00:00, 3469.60it/s]8181it [00:03, 1760.75it/s]3262it [00:00, 3335.90it/s]3169it [00:00, 3230.54it/s]8495it [00:03, 2004.30it/s]3280it [00:00, 3365.40it/s]3622it [00:01, 3412.61it/s]3533it [00:01, 3349.26it/s]8866it [00:03, 2348.04it/s]3650it [00:01, 3451.94it/s]3977it [00:01, 3453.05it/s]3879it [00:01, 3380.95it/s]9194it [00:03, 2530.54it/s]4000it [00:01, 3315.47it/s]4324it [00:01, 3359.88it/s]4219it [00:01, 3287.73it/s]9560it [00:03, 2801.11it/s]4373it [00:01, 3432.61it/s]4686it [00:01, 3433.51it/s]4576it [00:01, 3356.57it/s]9936it [00:03, 3045.49it/s]4745it [00:01, 3514.01it/s]4913it [00:01, 3257.03it/s]5031it [00:01, 3282.10it/s]10284it [00:03, 3084.61it/s]5099it [00:01, 3393.55it/s]5264it [00:01, 3327.94it/s]5383it [00:01, 3348.90it/s]10658it [00:03, 3261.82it/s]5463it [00:01, 3463.47it/s]5617it [00:01, 3385.37it/s]5720it [00:01, 3258.34it/s]11008it [00:04, 3238.80it/s]5811it [00:01, 3355.24it/s]5957it [00:01, 3278.28it/s]6090it [00:01, 3364.74it/s]11378it [00:04, 3365.83it/s]6181it [00:01, 3453.97it/s]6310it [00:01, 3339.90it/s]6459it [00:01, 3456.76it/s]11727it [00:04, 3301.47it/s]6528it [00:01, 3329.85it/s]6646it [00:01, 3277.46it/s]6806it [00:02, 3334.43it/s]12109it [00:04, 3446.88it/s]6895it [00:02, 3426.01it/s]6997it [00:02, 3344.80it/s]7164it [00:02, 3403.51it/s]12469it [00:04, 3387.88it/s]7269it [00:02, 3515.71it/s]7350it [00:02, 3398.36it/s]12844it [00:04, 3490.86it/s]13237it [00:04, 3617.77it/s]13603it [00:04, 3519.66it/s]13993it [00:04, 3627.73it/s]14359it [00:05, 3523.79it/s]14734it [00:05, 3587.85it/s]15095it [00:05, 3511.87it/s]7506it [00:02, 1051.41it/s]15473it [00:05, 3588.77it/s]7691it [00:03, 1049.76it/s]7623it [00:02, 1091.95it/s]7868it [00:03, 1343.01it/s]15834it [00:05, 3474.81it/s]8056it [00:03, 1348.01it/s]7991it [00:03, 1388.84it/s]8243it [00:03, 1678.31it/s]16201it [00:05, 3529.19it/s]8368it [00:03, 1597.64it/s]8301it [00:03, 1629.34it/s]8556it [00:03, 1919.52it/s]16584it [00:05, 3614.71it/s]8730it [00:03, 1936.26it/s]8667it [00:03, 1969.09it/s]8924it [00:03, 2256.78it/s]9090it [00:03, 2257.36it/s]9042it [00:03, 2312.09it/s]9253it [00:03, 2435.63it/s]9377it [00:03, 2505.06it/s]9418it [00:03, 2442.17it/s]9630it [00:03, 2743.54it/s]9751it [00:03, 2793.08it/s]9792it [00:03, 2744.73it/s]9968it [00:03, 2863.43it/s]10129it [00:03, 2840.07it/s]10094it [00:03, 2874.06it/s]10329it [00:03, 3056.02it/s]10495it [00:03, 3051.91it/s]10468it [00:03, 3096.56it/s]10704it [00:03, 3241.60it/s]10813it [00:03, 3096.41it/s]10835it [00:03, 3027.19it/s]11055it [00:04, 3184.94it/s]11175it [00:03, 3238.18it/s]11182it [00:04, 3146.73it/s]11427it [00:04, 3331.14it/s]11548it [00:04, 3375.13it/s]11551it [00:04, 3297.72it/s]16947it [00:06, 1108.39it/s]11775it [00:04, 3276.17it/s]17327it [00:06, 1414.53it/s]11900it [00:04, 3310.65it/s]11895it [00:04, 3236.04it/s]12131it [00:04, 3355.48it/s]17640it [00:06, 1654.27it/s]12280it [00:04, 3448.38it/s]12259it [00:04, 3342.47it/s]12475it [00:04, 3324.99it/s]18025it [00:06, 2019.62it/s]12601it [00:04, 3309.07it/s]12633it [00:04, 3349.38it/s]12847it [00:04, 3437.23it/s]18416it [00:06, 2382.40it/s]12959it [00:04, 3382.13it/s]13020it [00:04, 3495.49it/s]13238it [00:04, 3574.31it/s]18763it [00:07, 2572.05it/s]13309it [00:04, 3363.91it/s]13375it [00:04, 3438.76it/s]13599it [00:04, 3455.24it/s]19140it [00:07, 2848.56it/s]13692it [00:04, 3497.98it/s]13747it [00:04, 3519.40it/s]13988it [00:04, 3578.65it/s]14057it [00:04, 3542.20it/s]19491it [00:07, 2919.28it/s]14131it [00:04, 3612.47it/s]14349it [00:04, 3497.03it/s]19868it [00:07, 3137.14it/s]14413it [00:05, 3429.57it/s]14495it [00:04, 3501.30it/s]14718it [00:05, 3550.23it/s]14793it [00:05, 3536.42it/s]20218it [00:07, 3170.65it/s]14869it [00:05, 3568.41it/s]15075it [00:05, 3485.44it/s]20594it [00:07, 3329.32it/s]15149it [00:05, 3426.55it/s]15228it [00:05, 3481.13it/s]15443it [00:05, 3540.41it/s]20947it [00:07, 3289.53it/s]15519it [00:05, 3478.23it/s]15602it [00:05, 3555.47it/s]15823it [00:05, 3614.69it/s]21324it [00:07, 3421.96it/s]15869it [00:05, 3393.14it/s]15959it [00:05, 3436.32it/s]16186it [00:05, 3462.48it/s]21685it [00:07, 3474.61it/s]16223it [00:05, 3432.69it/s]16319it [00:05, 3474.31it/s]16535it [00:05, 3397.79it/s]22041it [00:07, 3401.11it/s]16599it [00:05, 3524.42it/s]16669it [00:05, 3406.12it/s]22421it [00:08, 3513.97it/s]22777it [00:08, 3440.35it/s]23158it [00:08, 3545.60it/s]23516it [00:08, 3381.27it/s]23893it [00:08, 3491.10it/s]24273it [00:08, 3578.07it/s]24634it [00:08, 3462.61it/s]25005it [00:08, 3532.33it/s]25361it [00:08, 3428.45it/s]16877it [00:06, 920.38it/s] 17011it [00:06, 982.96it/s] 25725it [00:09, 3487.78it/s]17252it [00:06, 1202.23it/s]17373it [00:06, 1263.03it/s]16953it [00:06, 863.28it/s] 26076it [00:09, 3375.39it/s]17691it [00:06, 1515.10it/s]17579it [00:06, 1452.25it/s]17329it [00:06, 1132.45it/s]26450it [00:09, 3477.51it/s]18071it [00:06, 1876.91it/s]17964it [00:06, 1812.27it/s]17641it [00:06, 1365.42it/s]18335it [00:07, 2147.57it/s]18419it [00:06, 2141.15it/s]26817it [00:09, 3381.54it/s]18005it [00:07, 1691.23it/s]18672it [00:07, 2375.27it/s]18793it [00:07, 2470.15it/s]27194it [00:09, 3491.03it/s]18388it [00:07, 2054.26it/s]19160it [00:07, 2742.65it/s]27573it [00:09, 3576.11it/s]19049it [00:07, 2667.58it/s]18726it [00:07, 2279.46it/s]19392it [00:07, 2814.09it/s]19506it [00:07, 2858.90it/s]27933it [00:09, 3422.34it/s]19093it [00:07, 2578.46it/s]19767it [00:07, 3050.43it/s]19879it [00:07, 3080.16it/s]28300it [00:09, 3492.71it/s]19435it [00:07, 2718.65it/s]20116it [00:07, 3076.62it/s]20227it [00:07, 3055.81it/s]19810it [00:07, 2973.17it/s]20466it [00:07, 3189.08it/s]20598it [00:07, 3230.72it/s]20156it [00:07, 3019.17it/s]20826it [00:07, 3302.50it/s]20943it [00:07, 3194.32it/s]20516it [00:07, 3173.40it/s]21174it [00:07, 3276.31it/s]21320it [00:07, 3352.35it/s]20876it [00:07, 3290.41it/s]21550it [00:07, 3412.55it/s]21697it [00:07, 3469.79it/s]21225it [00:08, 3256.95it/s]21901it [00:08, 3324.60it/s]22053it [00:07, 3333.48it/s]21590it [00:08, 3367.27it/s]22261it [00:08, 3400.66it/s]22431it [00:08, 3457.42it/s]21937it [00:08, 3302.41it/s]22619it [00:08, 3332.08it/s]22783it [00:08, 3382.88it/s]22300it [00:08, 3393.94it/s]23002it [00:08, 3472.40it/s]23158it [00:08, 3487.00it/s]22645it [00:08, 3316.03it/s]23362it [00:08, 3508.42it/s]23511it [00:08, 3321.46it/s]23012it [00:08, 3416.67it/s]23716it [00:08, 3393.97it/s]23883it [00:08, 3432.66it/s]28652it [00:10, 848.68it/s] 23381it [00:08, 3495.11it/s]24092it [00:08, 3498.12it/s]24259it [00:08, 3524.96it/s]29020it [00:11, 1106.30it/s]23734it [00:08, 3355.75it/s]24444it [00:08, 3373.77it/s]24615it [00:08, 3413.79it/s]29391it [00:11, 1405.47it/s]24111it [00:08, 3472.37it/s]24820it [00:08, 3482.12it/s]24985it [00:08, 3494.62it/s]29705it [00:11, 1646.60it/s]24461it [00:08, 3391.55it/s]25171it [00:09, 3386.40it/s]30078it [00:11, 1995.51it/s]25337it [00:08, 3344.14it/s]24818it [00:09, 3441.73it/s]25531it [00:09, 3446.23it/s]30409it [00:11, 2222.15it/s]25707it [00:09, 3443.16it/s]25164it [00:09, 3348.58it/s]25908it [00:09, 3540.21it/s]30763it [00:11, 2503.78it/s]26054it [00:09, 3340.89it/s]25522it [00:09, 3414.56it/s]26264it [00:09, 3413.81it/s]31097it [00:11, 2646.24it/s]26426it [00:09, 3447.68it/s]25898it [00:09, 3512.72it/s]26640it [00:09, 3511.10it/s]31464it [00:11, 2898.97it/s]26778it [00:09, 3468.28it/s]26251it [00:09, 3399.78it/s]31838it [00:11, 3117.31it/s]26993it [00:09, 3372.02it/s]27127it [00:09, 3382.82it/s]26609it [00:09, 3446.43it/s]27379it [00:09, 3510.74it/s]32187it [00:11, 3143.10it/s]27507it [00:09, 3503.13it/s]26955it [00:09, 3358.97it/s]32556it [00:12, 3291.87it/s]27733it [00:09, 3416.47it/s]27859it [00:09, 3406.29it/s]27341it [00:09, 3500.63it/s]28096it [00:09, 3475.80it/s]32905it [00:12, 3204.90it/s]28231it [00:09, 3495.13it/s]27693it [00:09, 3367.95it/s]28464it [00:09, 3534.59it/s]33270it [00:12, 3326.60it/s]28073it [00:10, 3491.35it/s]33614it [00:12, 3267.75it/s]28428it [00:10, 3506.45it/s]33994it [00:12, 3416.70it/s]34369it [00:12, 3512.24it/s]34725it [00:12, 3353.86it/s]35089it [00:12, 3432.28it/s]35436it [00:12, 3343.64it/s]35804it [00:13, 3437.96it/s]36151it [00:13, 3336.88it/s]36513it [00:13, 3415.48it/s]36863it [00:13, 3438.83it/s]37209it [00:13, 3338.84it/s]37573it [00:13, 3424.25it/s]28582it [00:11, 738.11it/s] 37917it [00:13, 3330.82it/s]28952it [00:11, 977.25it/s]28819it [00:11, 727.37it/s] 38293it [00:13, 3452.11it/s]29326it [00:11, 1262.97it/s]29193it [00:11, 965.81it/s]38642it [00:13, 3460.56it/s]29640it [00:11, 1498.67it/s]29501it [00:11, 1180.60it/s]38990it [00:13, 3343.50it/s]28780it [00:11, 681.26it/s] 29995it [00:11, 1815.55it/s]29879it [00:11, 1509.76it/s]39363it [00:14, 3454.45it/s]29153it [00:11, 910.00it/s]30319it [00:11, 2054.99it/s]30237it [00:11, 1826.65it/s]29464it [00:11, 1122.80it/s]30692it [00:11, 2395.43it/s]39710it [00:14, 3356.55it/s]30569it [00:11, 2065.28it/s]29828it [00:11, 1428.57it/s]31060it [00:11, 2684.71it/s]40078it [00:14, 3449.48it/s]30942it [00:11, 2400.65it/s]30199it [00:12, 1765.96it/s]31405it [00:11, 2798.63it/s]40425it [00:14, 3362.92it/s]31281it [00:12, 2580.98it/s]30532it [00:12, 1979.28it/s]31762it [00:12, 2992.88it/s]40791it [00:14, 3447.64it/s]31635it [00:12, 2809.57it/s]30902it [00:12, 2314.42it/s]41143it [00:14, 3466.60it/s]32104it [00:12, 3038.64it/s]31974it [00:12, 2907.16it/s]31236it [00:12, 2504.11it/s]32475it [00:12, 3219.95it/s]41491it [00:14, 3375.41it/s]32351it [00:12, 3134.42it/s]31612it [00:12, 2798.24it/s]41861it [00:14, 3467.89it/s]32820it [00:12, 3172.03it/s]32710it [00:12, 3257.78it/s]31955it [00:12, 2896.30it/s]42209it [00:14, 3374.47it/s]33171it [00:12, 3263.87it/s]33060it [00:12, 3226.74it/s]32307it [00:12, 3057.93it/s]42580it [00:15, 3468.75it/s]33539it [00:12, 3376.00it/s]33432it [00:12, 3362.43it/s]32672it [00:12, 3218.06it/s]33886it [00:12, 3314.83it/s]42928it [00:15, 3361.02it/s]33781it [00:12, 3313.09it/s]33020it [00:12, 3195.96it/s]34260it [00:12, 3435.53it/s]43276it [00:15, 3394.56it/s]34165it [00:12, 3462.26it/s]33389it [00:12, 3331.85it/s]43641it [00:15, 3467.81it/s]34609it [00:12, 3338.29it/s]34519it [00:13, 3343.69it/s]33736it [00:13, 3253.75it/s]34968it [00:13, 3408.18it/s]34891it [00:13, 3447.93it/s]34110it [00:13, 3390.49it/s]35312it [00:13, 3319.63it/s]35263it [00:13, 3526.23it/s]34457it [00:13, 3339.27it/s]35687it [00:13, 3443.06it/s]35620it [00:13, 3383.32it/s]34815it [00:13, 3406.12it/s]36059it [00:13, 3521.76it/s]35987it [00:13, 3463.40it/s]35189it [00:13, 3502.60it/s]36414it [00:13, 3366.36it/s]36337it [00:13, 3335.30it/s]35543it [00:13, 3371.38it/s]36782it [00:13, 3453.28it/s]36704it [00:13, 3429.34it/s]35917it [00:13, 3476.15it/s]37130it [00:13, 3356.03it/s]37050it [00:13, 3345.13it/s]36268it [00:13, 3372.32it/s]37498it [00:13, 3446.35it/s]37420it [00:13, 3445.11it/s]36623it [00:13, 3421.05it/s]37845it [00:13, 3306.81it/s]37773it [00:13, 3468.15it/s]36970it [00:14, 3336.74it/s]38227it [00:13, 3451.75it/s]38122it [00:14, 3380.57it/s]37328it [00:14, 3405.03it/s]38580it [00:14, 3463.40it/s]38486it [00:14, 3453.21it/s]37693it [00:14, 3473.86it/s]38928it [00:14, 3375.25it/s]38833it [00:14, 3362.87it/s]38042it [00:14, 3383.33it/s]39283it [00:14, 3423.48it/s]39201it [00:14, 3453.68it/s]38399it [00:14, 3436.44it/s]39627it [00:14, 3331.49it/s]39548it [00:14, 3206.36it/s]38744it [00:14, 3330.78it/s]39998it [00:14, 3438.18it/s]39919it [00:14, 3345.73it/s]43989it [00:16, 623.83it/s] 39116it [00:14, 3441.38it/s]40344it [00:14, 3351.61it/s]40296it [00:14, 3466.52it/s]44366it [00:17, 844.37it/s]39478it [00:14, 3493.35it/s]40709it [00:14, 3435.94it/s]40646it [00:14, 3358.51it/s]44678it [00:17, 1051.47it/s]39829it [00:14, 3392.56it/s]41054it [00:14, 3437.04it/s]41006it [00:14, 3426.59it/s]45038it [00:17, 1344.52it/s]40187it [00:14, 3445.72it/s]41399it [00:14, 3348.00it/s]41351it [00:15, 3355.80it/s]45408it [00:17, 1676.87it/s]40533it [00:15, 3346.85it/s]41773it [00:15, 3459.44it/s]41722it [00:15, 3456.53it/s]45739it [00:17, 1933.17it/s]40903it [00:15, 3446.33it/s]42120it [00:15, 3360.29it/s]42070it [00:15, 3373.49it/s]46104it [00:17, 2262.67it/s]41249it [00:15, 3341.51it/s]42498it [00:15, 3479.68it/s]42442it [00:15, 3473.06it/s]46442it [00:17, 2461.12it/s]41620it [00:15, 3447.40it/s]42850it [00:15, 3321.60it/s]42808it [00:15, 3526.01it/s]46812it [00:17, 2747.24it/s]41993it [00:15, 3493.05it/s]43218it [00:15, 3422.03it/s]43162it [00:15, 3408.40it/s]47154it [00:17, 2831.87it/s]42344it [00:15, 3342.84it/s]43584it [00:15, 3490.25it/s]43530it [00:15, 3485.52it/s]47527it [00:18, 3061.89it/s]42715it [00:15, 3447.12it/s]47897it [00:18, 3231.59it/s]43062it [00:15, 3306.05it/s]48249it [00:18, 3207.59it/s]43431it [00:15, 3412.83it/s]48625it [00:18, 3358.32it/s]48976it [00:18, 3311.42it/s]49331it [00:18, 3377.96it/s]49677it [00:18, 3315.41it/s]50049it [00:18, 3429.20it/s]50412it [00:18, 3485.22it/s]50764it [00:18, 3369.74it/s]51140it [00:19, 3481.43it/s]51491it [00:19, 3322.92it/s]51861it [00:19, 3429.58it/s]52207it [00:19, 3330.45it/s]52575it [00:19, 3427.51it/s]52941it [00:19, 3493.77it/s]43880it [00:17, 629.96it/s] 43935it [00:17, 621.51it/s] 53293it [00:19, 3380.67it/s]44245it [00:17, 841.23it/s]44311it [00:17, 838.01it/s]53643it [00:19, 3414.03it/s]44603it [00:17, 1024.73it/s]44599it [00:17, 1073.33it/s]53986it [00:19, 3333.61it/s]44977it [00:17, 1333.06it/s]44971it [00:17, 1373.90it/s]54364it [00:20, 3462.25it/s]45350it [00:17, 1667.28it/s]45344it [00:17, 1702.31it/s]54712it [00:20, 3359.17it/s]43775it [00:17, 560.74it/s] 45683it [00:17, 1916.28it/s]45678it [00:17, 1925.57it/s]55077it [00:20, 3442.13it/s]44153it [00:17, 764.04it/s]46041it [00:17, 2231.05it/s]46025it [00:17, 2215.17it/s]55439it [00:20, 3491.28it/s]44528it [00:18, 1011.05it/s]46376it [00:17, 2351.65it/s]46353it [00:18, 2297.22it/s]55790it [00:20, 3374.73it/s]44840it [00:18, 1233.23it/s]46745it [00:18, 2650.96it/s]46700it [00:18, 2552.75it/s]56162it [00:20, 3472.91it/s]45196it [00:18, 1538.14it/s]47120it [00:18, 2793.60it/s]47070it [00:18, 2827.17it/s]56511it [00:20, 3370.87it/s]45521it [00:18, 1791.02it/s]47447it [00:18, 2869.73it/s]47403it [00:18, 2739.04it/s]56884it [00:20, 3472.46it/s]45841it [00:18, 2041.97it/s]47781it [00:18, 2990.05it/s]47762it [00:18, 2952.32it/s]57233it [00:20, 3385.36it/s]46203it [00:18, 2366.96it/s]48106it [00:18, 3010.96it/s]48086it [00:18, 3002.86it/s]57607it [00:20, 3486.12it/s]46534it [00:18, 2517.71it/s]48441it [00:18, 3103.61it/s]48436it [00:18, 3136.96it/s]57978it [00:21, 3549.91it/s]46902it [00:18, 2796.49it/s]48765it [00:18, 3101.16it/s]48800it [00:18, 3160.35it/s]58335it [00:21, 3407.76it/s]47237it [00:18, 2898.94it/s]49085it [00:18, 3105.32it/s]49171it [00:18, 3312.07it/s]58695it [00:21, 3460.62it/s]47609it [00:18, 3108.09it/s]49414it [00:18, 3157.20it/s]49512it [00:19, 3300.66it/s]59043it [00:21, 3345.94it/s]47951it [00:19, 3083.48it/s]49735it [00:19, 3062.80it/s]49849it [00:19, 3160.66it/s]59409it [00:21, 3434.17it/s]48281it [00:19, 3005.65it/s]50104it [00:19, 3241.05it/s]50214it [00:19, 3296.14it/s]59754it [00:21, 3333.44it/s]48657it [00:19, 3209.23it/s]50472it [00:19, 3367.46it/s]50549it [00:19, 3227.79it/s]60122it [00:21, 3430.50it/s]48991it [00:19, 3171.10it/s]50812it [00:19, 3216.32it/s]50916it [00:19, 3352.82it/s]60488it [00:21, 3488.86it/s]49344it [00:19, 3271.73it/s]51171it [00:19, 3321.13it/s]51290it [00:19, 3463.43it/s]60839it [00:21, 3382.91it/s]49678it [00:19, 3135.97it/s]51506it [00:19, 3071.51it/s]51639it [00:19, 3317.04it/s]61179it [00:22, 3289.05it/s]50013it [00:19, 3192.97it/s]51822it [00:19, 3095.67it/s]52010it [00:19, 3428.68it/s]61510it [00:22, 3228.06it/s]50339it [00:19, 3209.45it/s]52160it [00:19, 3100.92it/s]52356it [00:19, 3351.09it/s]61881it [00:22, 3363.73it/s]50664it [00:19, 3186.12it/s]52481it [00:19, 3130.00it/s]52720it [00:19, 3429.18it/s]50985it [00:19, 3160.44it/s]62238it [00:22, 3266.41it/s]52821it [00:19, 3207.63it/s]53065it [00:20, 3328.47it/s]51319it [00:20, 3143.66it/s]53144it [00:20, 3172.87it/s]53432it [00:20, 3424.38it/s]51670it [00:20, 3246.51it/s]53509it [00:20, 3310.43it/s]53786it [00:20, 3457.47it/s]52008it [00:20, 3284.58it/s]53842it [00:20, 3240.90it/s]54133it [00:20, 3373.25it/s]52338it [00:20, 3238.93it/s]54199it [00:20, 3335.78it/s]54482it [00:20, 3406.28it/s]52676it [00:20, 3278.42it/s]54551it [00:20, 3388.19it/s]54824it [00:20, 3287.78it/s]53005it [00:20, 3100.45it/s]54891it [00:20, 3295.22it/s]55181it [00:20, 3366.48it/s]53374it [00:20, 3268.00it/s]55222it [00:20, 3272.34it/s]53728it [00:20, 3346.07it/s]55519it [00:20, 3292.86it/s]55550it [00:20, 3185.85it/s]55850it [00:20, 3230.76it/s]54065it [00:20, 3295.90it/s]55905it [00:20, 3289.78it/s]56220it [00:21, 3365.76it/s]54426it [00:21, 3386.25it/s]56278it [00:21, 3417.81it/s]56558it [00:21, 3301.17it/s]54766it [00:21, 3198.64it/s]56621it [00:21, 3291.99it/s]56915it [00:21, 3377.84it/s]55121it [00:21, 3296.79it/s]56990it [00:21, 3402.65it/s]57254it [00:21, 3318.50it/s]55480it [00:21, 3380.69it/s]57332it [00:21, 3300.01it/s]57622it [00:21, 3421.62it/s]55821it [00:21, 3296.44it/s]57699it [00:21, 3405.06it/s]57995it [00:21, 3510.32it/s]56194it [00:21, 3421.08it/s]58042it [00:21, 3312.22it/s]58347it [00:21, 3386.38it/s]56538it [00:21, 3340.07it/s]58406it [00:21, 3404.48it/s]58718it [00:21, 3478.40it/s]56912it [00:21, 3453.39it/s]58764it [00:21, 3455.21it/s]59068it [00:21, 3327.73it/s]57259it [00:21, 3378.27it/s]59111it [00:21, 3329.28it/s]62567it [00:24, 525.58it/s] 59435it [00:21, 3425.20it/s]57629it [00:21, 3469.19it/s]59476it [00:21, 3420.20it/s]62937it [00:24, 718.99it/s]59780it [00:22, 3325.42it/s]57991it [00:22, 3512.24it/s]63253it [00:24, 914.39it/s]59820it [00:22, 3307.44it/s]60146it [00:22, 3418.55it/s]58344it [00:22, 3385.36it/s]63621it [00:24, 1198.09it/s]60175it [00:22, 3376.76it/s]60508it [00:22, 3474.51it/s]58715it [00:22, 3476.55it/s]60542it [00:22, 3459.33it/s]63987it [00:24, 1483.10it/s]60857it [00:22, 3374.22it/s]59065it [00:22, 3367.21it/s]64357it [00:24, 1819.39it/s]60890it [00:22, 3339.84it/s]61215it [00:22, 3433.15it/s]59430it [00:22, 3448.32it/s]64728it [00:24, 2155.42it/s]61260it [00:22, 3436.92it/s]61560it [00:22, 3333.39it/s]59777it [00:22, 3301.57it/s]65070it [00:25, 2369.38it/s]61606it [00:22, 3305.83it/s]61933it [00:22, 3446.60it/s]60143it [00:22, 3403.31it/s]61974it [00:22, 3409.84it/s]65404it [00:25, 2530.20it/s]62280it [00:22, 3359.85it/s]60510it [00:22, 3480.04it/s]65729it [00:25, 2667.14it/s]60860it [00:22, 3374.92it/s]66100it [00:25, 2926.37it/s]61233it [00:23, 3475.16it/s]66468it [00:25, 3123.60it/s]61583it [00:23, 3355.98it/s]66813it [00:25, 3132.06it/s]61957it [00:23, 3463.58it/s]67189it [00:25, 3302.91it/s]62306it [00:23, 3340.68it/s]67537it [00:25, 3250.06it/s]67913it [00:25, 3391.13it/s]68262it [00:25, 3312.39it/s]68632it [00:26, 3420.97it/s]69004it [00:26, 3505.28it/s]69359it [00:26, 3404.10it/s]69727it [00:26, 3478.31it/s]70078it [00:26, 3391.64it/s]70442it [00:26, 3462.38it/s]70791it [00:26, 3362.92it/s]71159it [00:26, 3452.71it/s]71531it [00:26, 3529.50it/s]71886it [00:27, 3422.13it/s]72251it [00:27, 3486.15it/s]72601it [00:27, 3383.00it/s]72984it [00:27, 3510.92it/s]62618it [00:24, 480.51it/s] 73337it [00:27, 3396.18it/s]62985it [00:25, 657.56it/s]62317it [00:25, 454.68it/s] 73704it [00:27, 3472.16it/s]63294it [00:25, 836.14it/s]62676it [00:25, 618.06it/s]63659it [00:25, 1103.22it/s]74067it [00:27, 3401.03it/s]63046it [00:25, 831.61it/s]63989it [00:25, 1356.42it/s]74437it [00:27, 3486.30it/s]63359it [00:25, 1038.06it/s]62642it [00:25, 492.44it/s] 64340it [00:25, 1668.30it/s]74803it [00:27, 3536.29it/s]63726it [00:25, 1337.83it/s]63007it [00:25, 670.70it/s]64707it [00:25, 2011.38it/s]75158it [00:27, 3402.01it/s]64052it [00:25, 1596.94it/s]63320it [00:25, 854.32it/s]65044it [00:25, 2238.99it/s]75524it [00:28, 3473.71it/s]64404it [00:25, 1915.53it/s]63672it [00:25, 1111.16it/s]65409it [00:25, 2544.35it/s]75873it [00:28, 3372.74it/s]64773it [00:25, 2254.92it/s]63987it [00:25, 1353.69it/s]65749it [00:25, 2676.39it/s]76235it [00:28, 3442.89it/s]65115it [00:25, 2453.50it/s]64353it [00:25, 1692.67it/s]66120it [00:26, 2932.03it/s]76588it [00:28, 3324.65it/s]65475it [00:25, 2717.30it/s]64719it [00:26, 2033.71it/s]66468it [00:26, 3073.97it/s]76938it [00:28, 3371.44it/s]65816it [00:26, 2823.21it/s]65056it [00:26, 2257.87it/s]66812it [00:26, 3089.07it/s]77308it [00:28, 3464.48it/s]66187it [00:26, 3049.21it/s]65420it [00:26, 2558.02it/s]67184it [00:26, 3259.79it/s]77656it [00:28, 3369.03it/s]66531it [00:26, 3028.88it/s]65759it [00:26, 2667.76it/s]67530it [00:26, 3208.42it/s]78023it [00:28, 3452.91it/s]66902it [00:26, 3211.46it/s]66130it [00:26, 2923.88it/s]67897it [00:26, 3335.41it/s]78370it [00:28, 3354.23it/s]67274it [00:26, 3351.74it/s]66492it [00:26, 3105.12it/s]68241it [00:26, 3186.30it/s]78736it [00:29, 3441.35it/s]67625it [00:26, 3280.31it/s]66839it [00:26, 3076.82it/s]68591it [00:26, 3273.13it/s]79100it [00:29, 3498.42it/s]67996it [00:26, 3399.99it/s]67207it [00:26, 3239.74it/s]68959it [00:26, 3382.37it/s]79451it [00:29, 3387.80it/s]68345it [00:26, 3273.54it/s]67551it [00:26, 3166.06it/s]69303it [00:26, 3303.58it/s]79818it [00:29, 3468.72it/s]68714it [00:26, 3388.70it/s]67921it [00:27, 3313.94it/s]69664it [00:27, 3390.67it/s]80167it [00:29, 3357.20it/s]69059it [00:27, 3298.00it/s]68264it [00:27, 3238.80it/s]70007it [00:27, 3314.92it/s]80525it [00:29, 3420.23it/s]69427it [00:27, 3405.05it/s]68629it [00:27, 3354.50it/s]70372it [00:27, 3410.13it/s]80869it [00:29, 3331.08it/s]69794it [00:27, 3480.27it/s]68994it [00:27, 3438.94it/s]81232it [00:29, 3416.02it/s]70716it [00:27, 3283.40it/s]70145it [00:27, 3334.13it/s]69343it [00:27, 3297.91it/s]81597it [00:29, 3482.62it/s]71075it [00:27, 3370.48it/s]70512it [00:27, 3429.68it/s]69705it [00:27, 3387.97it/s]71441it [00:27, 3451.59it/s]81947it [00:29, 3372.46it/s]70858it [00:27, 3323.89it/s]70048it [00:27, 3302.85it/s]82313it [00:30, 3453.77it/s]71788it [00:27, 3345.79it/s]71224it [00:27, 3417.88it/s]70415it [00:27, 3406.92it/s]72153it [00:27, 3431.05it/s]82660it [00:30, 3353.30it/s]71568it [00:27, 3318.62it/s]70759it [00:27, 3306.28it/s]83027it [00:30, 3442.55it/s]72498it [00:27, 3324.05it/s]71920it [00:27, 3374.95it/s]71118it [00:28, 3385.44it/s]72852it [00:28, 3385.37it/s]83373it [00:30, 3338.45it/s]72283it [00:27, 3448.27it/s]71469it [00:28, 3420.24it/s]73226it [00:28, 3486.32it/s]83728it [00:30, 3391.34it/s]72630it [00:28, 3346.84it/s]71813it [00:28, 3321.98it/s]73576it [00:28, 3383.56it/s]84094it [00:30, 3469.03it/s]73010it [00:28, 3475.04it/s]72176it [00:28, 3408.31it/s]73951it [00:28, 3487.74it/s]84442it [00:30, 3364.44it/s]73359it [00:28, 3355.87it/s]72519it [00:28, 3308.50it/s]84806it [00:30, 3443.44it/s]74302it [00:28, 3376.96it/s]73733it [00:28, 3464.25it/s]72894it [00:28, 3433.81it/s]74672it [00:28, 3468.17it/s]85152it [00:30, 3342.88it/s]74082it [00:28, 3341.62it/s]73239it [00:28, 3329.62it/s]85518it [00:31, 3434.04it/s]75021it [00:28, 3310.75it/s]74451it [00:28, 3439.16it/s]73596it [00:28, 3396.06it/s]75386it [00:28, 3404.81it/s]85863it [00:31, 3327.67it/s]74817it [00:28, 3502.07it/s]73973it [00:28, 3502.35it/s]75750it [00:28, 3308.60it/s]75169it [00:28, 3365.49it/s]74325it [00:28, 3387.96it/s]76113it [00:28, 3398.14it/s]75532it [00:28, 3440.60it/s]74693it [00:29, 3469.63it/s]76477it [00:29, 3466.81it/s]75878it [00:29, 3288.83it/s]75042it [00:29, 3351.29it/s]76826it [00:29, 3346.33it/s]76237it [00:29, 3372.79it/s]75404it [00:29, 3425.79it/s]77174it [00:29, 3383.94it/s]76589it [00:29, 3269.58it/s]75749it [00:29, 3320.73it/s]77514it [00:29, 3301.92it/s]76952it [00:29, 3370.72it/s]76097it [00:29, 3364.28it/s]77879it [00:29, 3399.65it/s]77319it [00:29, 3455.52it/s]76455it [00:29, 3424.47it/s]78243it [00:29, 3468.05it/s]77667it [00:29, 3299.53it/s]76799it [00:29, 3292.48it/s]78591it [00:29, 3334.87it/s]78033it [00:29, 3401.00it/s]77159it [00:29, 3378.48it/s]78937it [00:29, 3368.52it/s]78376it [00:29, 3301.63it/s]77499it [00:29, 3287.33it/s]79276it [00:29, 3254.74it/s]78741it [00:29, 3399.38it/s]77862it [00:29, 3385.26it/s]79640it [00:30, 3363.53it/s]79105it [00:29, 3467.01it/s]78214it [00:30, 3424.36it/s]79978it [00:30, 3284.48it/s]79454it [00:30, 3296.24it/s]78558it [00:30, 3320.06it/s]80346it [00:30, 3397.79it/s]79819it [00:30, 3395.72it/s]78920it [00:30, 3404.58it/s]80713it [00:30, 3474.84it/s]80161it [00:30, 3291.64it/s]79262it [00:30, 3299.81it/s]81062it [00:30, 3317.75it/s]80535it [00:30, 3419.03it/s]79626it [00:30, 3396.73it/s]81420it [00:30, 3392.43it/s]80879it [00:30, 3318.58it/s]79968it [00:30, 3220.96it/s]81762it [00:30, 3284.25it/s]81229it [00:30, 3369.40it/s]80333it [00:30, 3340.34it/s]82122it [00:30, 3371.86it/s]81593it [00:30, 3446.76it/s]80700it [00:30, 3432.62it/s]82470it [00:30, 3279.48it/s]81940it [00:30, 3326.37it/s]81046it [00:30, 3319.31it/s]82819it [00:30, 3337.49it/s]82305it [00:30, 3417.36it/s]81407it [00:31, 3401.27it/s]83178it [00:31, 3410.08it/s]82649it [00:31, 3302.97it/s]81749it [00:31, 3254.68it/s]83521it [00:31, 3306.75it/s]83003it [00:31, 3369.63it/s]86198it [00:33, 426.23it/s] 82115it [00:31, 3367.76it/s]83886it [00:31, 3404.06it/s]86567it [00:33, 588.87it/s]83342it [00:31, 3274.25it/s]82468it [00:31, 3270.62it/s]84228it [00:31, 3308.14it/s]86872it [00:33, 752.38it/s]83708it [00:31, 3384.35it/s]82833it [00:31, 3376.52it/s]84595it [00:31, 3409.75it/s]87239it [00:33, 1005.30it/s]84073it [00:31, 3459.28it/s]83195it [00:31, 3445.80it/s]84956it [00:31, 3467.64it/s]87577it [00:33, 1257.22it/s]84421it [00:31, 3348.74it/s]83542it [00:31, 3336.96it/s]85304it [00:31, 3347.72it/s]87948it [00:34, 1588.53it/s]84775it [00:31, 3402.55it/s]83890it [00:31, 3377.80it/s]85673it [00:31, 3445.81it/s]88319it [00:34, 1931.33it/s]85117it [00:31, 3297.21it/s]84230it [00:31, 3290.57it/s]88661it [00:34, 2182.32it/s]85479it [00:31, 3386.13it/s]84595it [00:32, 3393.35it/s]89030it [00:34, 2497.07it/s]85829it [00:31, 3281.92it/s]84960it [00:32, 3466.29it/s]89376it [00:34, 2651.89it/s]85308it [00:32, 3345.53it/s]89747it [00:34, 2905.88it/s]85675it [00:32, 3438.57it/s]90097it [00:34, 2932.98it/s]90458it [00:34, 3107.39it/s]90816it [00:34, 3235.11it/s]91162it [00:35, 3175.28it/s]91521it [00:35, 3288.40it/s]91862it [00:35, 3206.80it/s]92221it [00:35, 3312.31it/s]92577it [00:35, 3380.62it/s]92921it [00:35, 3271.05it/s]93279it [00:35, 3359.11it/s]93619it [00:35, 3214.14it/s]93977it [00:35, 3317.57it/s]94312it [00:36, 3225.13it/s]94671it [00:36, 3328.62it/s]95030it [00:36, 3401.96it/s]95372it [00:36, 3280.64it/s]95729it [00:36, 3362.66it/s]96068it [00:36, 3256.00it/s]96427it [00:36, 3350.14it/s]96785it [00:36, 3415.62it/s]97128it [00:36, 3288.86it/s]86020it [00:34, 400.40it/s] 97472it [00:36, 3330.60it/s]86389it [00:34, 551.86it/s]97807it [00:37, 3232.70it/s]86739it [00:34, 728.07it/s]98165it [00:37, 3329.98it/s]87101it [00:34, 960.71it/s]86159it [00:34, 377.04it/s] 98500it [00:37, 3230.10it/s]87471it [00:34, 1243.09it/s]86528it [00:34, 524.63it/s]98856it [00:37, 3324.41it/s]87801it [00:35, 1494.80it/s]86829it [00:35, 673.43it/s]99213it [00:37, 3393.37it/s]88170it [00:35, 1832.13it/s]86021it [00:35, 382.28it/s] 87194it [00:35, 908.01it/s]99554it [00:37, 3275.34it/s]88508it [00:35, 2082.15it/s]86387it [00:35, 526.97it/s]87565it [00:35, 1191.32it/s]99909it [00:37, 3352.14it/s]88875it [00:35, 2403.09it/s]86737it [00:35, 698.09it/s]87891it [00:35, 1442.73it/s]100246it [00:37, 3241.21it/s]89217it [00:35, 2559.66it/s]87097it [00:35, 923.13it/s]88241it [00:35, 1754.99it/s]100590it [00:37, 3297.88it/s]89547it [00:35, 2687.26it/s]87466it [00:35, 1199.04it/s]88571it [00:35, 2011.76it/s]100945it [00:37, 3369.52it/s]89910it [00:35, 2922.77it/s]87795it [00:35, 1448.84it/s]88938it [00:35, 2345.70it/s]101284it [00:38, 3235.28it/s]90245it [00:35, 2954.63it/s]88142it [00:35, 1752.35it/s]89276it [00:35, 2523.05it/s]101637it [00:38, 3317.40it/s]90596it [00:35, 3101.85it/s]88472it [00:35, 2010.89it/s]89643it [00:35, 2795.47it/s]101971it [00:38, 3215.63it/s]88837it [00:35, 2336.81it/s]90939it [00:36, 3073.02it/s]90012it [00:35, 3021.95it/s]102327it [00:38, 3313.09it/s]89206it [00:36, 2637.46it/s]91291it [00:36, 3195.10it/s]90361it [00:36, 2996.76it/s]102680it [00:38, 3375.75it/s]91642it [00:36, 3283.22it/s]89553it [00:36, 2758.08it/s]90715it [00:36, 3140.21it/s]103019it [00:38, 3265.68it/s]89916it [00:36, 2975.01it/s]91980it [00:36, 3173.84it/s]91054it [00:36, 3093.85it/s]103374it [00:38, 3345.59it/s]92329it [00:36, 3261.84it/s]90260it [00:36, 2995.61it/s]91409it [00:36, 3216.83it/s]103710it [00:38, 3204.37it/s]90595it [00:36, 3089.45it/s]92661it [00:36, 3174.18it/s]91765it [00:36, 3311.31it/s]104067it [00:38, 3307.72it/s]93015it [00:36, 3277.76it/s]90937it [00:36, 3062.02it/s]92106it [00:36, 3210.32it/s]104400it [00:39, 3209.16it/s]93347it [00:36, 3267.51it/s]91288it [00:36, 3184.45it/s]92446it [00:36, 3262.81it/s]104756it [00:39, 3308.20it/s]91640it [00:36, 3276.97it/s]93677it [00:36, 3184.07it/s]92778it [00:36, 3174.84it/s]105110it [00:39, 3374.51it/s]94017it [00:36, 3244.51it/s]91978it [00:36, 3189.36it/s]93134it [00:36, 3284.11it/s]105449it [00:39, 3253.88it/s]92328it [00:37, 3276.67it/s]94344it [00:37, 3061.11it/s]93466it [00:36, 3189.32it/s]105803it [00:39, 3334.21it/s]94671it [00:37, 3119.26it/s]92662it [00:37, 3143.29it/s]93821it [00:37, 3291.27it/s]106138it [00:39, 3227.05it/s]95025it [00:37, 3238.39it/s]93017it [00:37, 3257.78it/s]94175it [00:37, 3362.81it/s]106492it [00:39, 3315.01it/s]93372it [00:37, 3340.80it/s]95352it [00:37, 3159.90it/s]94514it [00:37, 3205.26it/s]106836it [00:39, 3349.27it/s]95705it [00:37, 3263.82it/s]93710it [00:37, 3237.62it/s]94866it [00:37, 3294.60it/s]107173it [00:39, 3244.63it/s]94062it [00:37, 3316.01it/s]96034it [00:37, 3183.13it/s]95198it [00:37, 3190.15it/s]107528it [00:39, 3322.06it/s]96389it [00:37, 3288.03it/s]94396it [00:37, 3220.84it/s]95551it [00:37, 3285.51it/s]107862it [00:40, 3234.46it/s]96743it [00:37, 3359.43it/s]94734it [00:37, 3265.36it/s]95903it [00:37, 3352.14it/s]108216it [00:40, 3320.89it/s]95085it [00:37, 3336.05it/s]97081it [00:37, 3207.08it/s]96240it [00:37, 3232.24it/s]108574it [00:40, 3394.58it/s]97436it [00:38, 3303.63it/s]95420it [00:37, 3226.18it/s]96577it [00:37, 3270.34it/s]108915it [00:40, 3278.41it/s]95771it [00:38, 3307.40it/s]97769it [00:38, 3206.47it/s]96906it [00:38, 3164.28it/s]109269it [00:40, 3351.16it/s]98119it [00:38, 3290.21it/s]96104it [00:38, 3203.32it/s]97260it [00:38, 3270.73it/s]109606it [00:40, 3246.58it/s]98473it [00:38, 3362.08it/s]96459it [00:38, 3302.08it/s]97614it [00:38, 3348.43it/s]109962it [00:40, 3335.83it/s]96814it [00:38, 3371.73it/s]98811it [00:38, 3253.31it/s]97951it [00:38, 3229.27it/s]110297it [00:40, 3207.10it/s]99159it [00:38, 3316.23it/s]97153it [00:38, 3214.28it/s]98304it [00:38, 3314.81it/s]110654it [00:40, 3310.34it/s]97507it [00:38, 3306.75it/s]99492it [00:38, 3218.09it/s]98638it [00:38, 3162.58it/s]111009it [00:41, 3378.60it/s]99847it [00:38, 3310.95it/s]97840it [00:38, 3213.05it/s]98993it [00:38, 3270.04it/s]111349it [00:41, 3263.10it/s]98193it [00:38, 3303.66it/s]100180it [00:38, 3173.14it/s]99338it [00:38, 3184.85it/s]111701it [00:41, 3334.90it/s]100536it [00:38, 3282.26it/s]98526it [00:38, 3205.72it/s]99692it [00:38, 3283.49it/s]112036it [00:41, 3231.55it/s]100892it [00:39, 3361.32it/s]98881it [00:39, 3302.55it/s]100046it [00:38, 3356.28it/s]112391it [00:41, 3320.58it/s]99219it [00:39, 3323.32it/s]101230it [00:39, 3249.23it/s]100384it [00:39, 3240.54it/s]112744it [00:41, 3379.02it/s]101582it [00:39, 3326.66it/s]99553it [00:39, 3220.87it/s]100722it [00:39, 3280.15it/s]113084it [00:41, 3261.01it/s]99906it [00:39, 3308.08it/s]101917it [00:39, 3227.09it/s]101052it [00:39, 3179.73it/s]113428it [00:41, 3312.12it/s]102273it [00:39, 3320.99it/s]100239it [00:39, 3214.05it/s]101408it [00:39, 3286.77it/s]113761it [00:41, 3215.85it/s]102628it [00:39, 3385.96it/s]100594it [00:39, 3310.44it/s]101760it [00:39, 3353.67it/s]114118it [00:41, 3317.04it/s]100947it [00:39, 3365.78it/s]102968it [00:39, 3271.29it/s]102097it [00:39, 3232.59it/s]114458it [00:42, 3225.81it/s]103323it [00:39, 3351.13it/s]101285it [00:39, 3219.89it/s]102452it [00:39, 3323.26it/s]114812it [00:42, 3313.90it/s]101636it [00:39, 3301.79it/s]103660it [00:39, 3240.73it/s]102786it [00:39, 3176.05it/s]115168it [00:42, 3384.23it/s]104000it [00:40, 3286.20it/s]101968it [00:40, 3205.96it/s]103142it [00:39, 3284.95it/s]104354it [00:40, 3358.44it/s]102323it [00:40, 3302.25it/s]103498it [00:40, 3364.08it/s]102677it [00:40, 3369.55it/s]104691it [00:40, 3250.61it/s]103837it [00:40, 3233.66it/s]105045it [00:40, 3332.28it/s]103016it [00:40, 3258.77it/s]104193it [00:40, 3325.17it/s]103370it [00:40, 3338.47it/s]105380it [00:40, 3229.96it/s]104528it [00:40, 3100.49it/s]105735it [00:40, 3319.72it/s]103706it [00:40, 3191.76it/s]104883it [00:40, 3224.69it/s]104062it [00:40, 3295.75it/s]106069it [00:40, 3220.71it/s]105218it [00:40, 3142.83it/s]106425it [00:40, 3317.93it/s]104394it [00:40, 3197.80it/s]105573it [00:40, 3255.90it/s]106782it [00:40, 3390.57it/s]104749it [00:40, 3296.91it/s]105918it [00:40, 3304.35it/s]105102it [00:40, 3362.86it/s]107123it [00:40, 3273.73it/s]106251it [00:40, 3204.60it/s]107463it [00:41, 3309.87it/s]105440it [00:41, 3244.56it/s]106607it [00:41, 3304.84it/s]107796it [00:41, 3230.16it/s]105779it [00:41, 3284.97it/s]106940it [00:41, 3196.85it/s]108152it [00:41, 3324.30it/s]106109it [00:41, 3200.79it/s]107294it [00:41, 3293.19it/s]108508it [00:41, 3392.35it/s]106464it [00:41, 3299.65it/s]107638it [00:41, 3334.59it/s]106820it [00:41, 3374.83it/s]108849it [00:41, 3283.26it/s]107973it [00:41, 3228.00it/s]109205it [00:41, 3362.60it/s]107159it [00:41, 3258.85it/s]108330it [00:41, 3324.33it/s]107513it [00:41, 3338.58it/s]109543it [00:41, 3251.71it/s]108664it [00:41, 3211.74it/s]109899it [00:41, 3339.19it/s]107849it [00:41, 3199.18it/s]109022it [00:41, 3315.65it/s]110256it [00:41, 3404.35it/s]108204it [00:41, 3299.07it/s]109367it [00:41, 3351.98it/s]110598it [00:41, 3289.17it/s]108560it [00:41, 3373.47it/s]109704it [00:41, 3230.63it/s]110956it [00:42, 3371.52it/s]108899it [00:42, 3266.23it/s]110060it [00:42, 3323.84it/s]109255it [00:42, 3348.90it/s]111295it [00:42, 3228.60it/s]110394it [00:42, 3220.34it/s]111643it [00:42, 3298.73it/s]109592it [00:42, 3244.99it/s]110748it [00:42, 3308.87it/s]109931it [00:42, 3284.56it/s]111975it [00:42, 3213.64it/s]111095it [00:42, 3354.62it/s]112334it [00:42, 3319.33it/s]110261it [00:42, 3192.49it/s]111432it [00:42, 3221.91it/s]112691it [00:42, 3391.28it/s]110613it [00:42, 3285.53it/s]111781it [00:42, 3297.71it/s]110943it [00:42, 3269.89it/s]113032it [00:42, 3276.19it/s]112113it [00:42, 3180.69it/s]113390it [00:42, 3362.08it/s]111271it [00:42, 3028.39it/s]112465it [00:42, 3274.89it/s]113728it [00:42, 3253.50it/s]111623it [00:42, 3165.33it/s]112795it [00:42, 3142.90it/s]114087it [00:43, 3349.29it/s]111944it [00:43, 3112.51it/s]113152it [00:43, 3261.87it/s]114442it [00:43, 3406.65it/s]112287it [00:43, 3195.57it/s]113506it [00:43, 3340.27it/s]114784it [00:43, 3241.80it/s]112640it [00:43, 3290.16it/s]113842it [00:43, 3218.92it/s]115508it [00:45, 319.16it/s] 115139it [00:43, 3327.29it/s]112971it [00:43, 3203.52it/s]114187it [00:43, 3283.52it/s]115862it [00:45, 441.32it/s]113327it [00:43, 3303.91it/s]114518it [00:43, 3193.33it/s]116207it [00:45, 591.66it/s]113659it [00:43, 3206.64it/s]114872it [00:43, 3292.02it/s]116563it [00:46, 793.26it/s]114019it [00:43, 3318.49it/s]115223it [00:43, 3355.27it/s]116918it [00:46, 1036.98it/s]114362it [00:43, 3349.12it/s]117238it [00:46, 1270.38it/s]114699it [00:43, 3245.24it/s]117592it [00:46, 1581.53it/s]115058it [00:43, 3343.74it/s]117918it [00:46, 1829.07it/s]118272it [00:46, 2149.70it/s]118616it [00:46, 2421.52it/s]118949it [00:46, 2560.59it/s]119302it [00:46, 2795.32it/s]119634it [00:46, 2853.23it/s]119986it [00:47, 3029.22it/s]120341it [00:47, 3171.43it/s]120680it [00:47, 3130.57it/s]121031it [00:47, 3235.00it/s]121366it [00:47, 3162.83it/s]121708it [00:47, 3235.24it/s]122060it [00:47, 3315.20it/s]122397it [00:47, 3218.87it/s]122753it [00:47, 3315.08it/s]123088it [00:47, 3216.16it/s]123443it [00:48, 3310.57it/s]123777it [00:48, 3207.63it/s]124132it [00:48, 3304.93it/s]124486it [00:48, 3372.52it/s]124825it [00:48, 3226.25it/s]125176it [00:48, 3306.78it/s]125509it [00:48, 3206.21it/s]125867it [00:48, 3305.27it/s]126223it [00:48, 3377.22it/s]115474it [00:46, 324.20it/s] 126563it [00:49, 3265.82it/s]115835it [00:46, 451.03it/s]126917it [00:49, 3336.80it/s]116195it [00:46, 615.59it/s]127253it [00:49, 3233.02it/s]116496it [00:46, 781.30it/s]127607it [00:49, 3320.69it/s]116856it [00:47, 1035.53it/s]127960it [00:49, 3378.76it/s]117174it [00:47, 1272.40it/s]128300it [00:49, 3224.28it/s]117533it [00:47, 1595.13it/s]128653it [00:49, 3310.91it/s]117888it [00:47, 1870.70it/s]128986it [00:49, 3213.28it/s]118246it [00:47, 2190.98it/s]129342it [00:49, 3311.97it/s]118590it [00:47, 2453.90it/s]115394it [00:47, 297.34it/s] 129675it [00:49, 3214.38it/s]115560it [00:47, 273.88it/s] 118925it [00:47, 2584.08it/s]115753it [00:47, 414.89it/s]130029it [00:50, 3307.37it/s]115916it [00:47, 382.39it/s]119281it [00:47, 2821.26it/s]116112it [00:47, 568.96it/s]130384it [00:50, 3375.68it/s]116207it [00:47, 496.09it/s]116414it [00:47, 727.83it/s]119615it [00:47, 2871.27it/s]130723it [00:50, 3251.85it/s]116563it [00:47, 681.28it/s]116771it [00:48, 968.56it/s]119971it [00:48, 3051.70it/s]131076it [00:50, 3331.22it/s]116907it [00:47, 900.52it/s]117088it [00:48, 1200.60it/s]120329it [00:48, 3194.83it/s]131411it [00:50, 3196.37it/s]117222it [00:48, 1121.74it/s]117446it [00:48, 1516.78it/s]120670it [00:48, 3150.82it/s]131761it [00:50, 3282.67it/s]117572it [00:48, 1421.88it/s]117806it [00:48, 1848.90it/s]121026it [00:48, 3264.13it/s]132115it [00:50, 3354.93it/s]117893it [00:48, 1672.38it/s]118143it [00:48, 2077.32it/s]121364it [00:48, 3195.62it/s]132453it [00:50, 3240.41it/s]118244it [00:48, 1988.51it/s]118502it [00:48, 2387.04it/s]121699it [00:48, 3238.29it/s]132805it [00:50, 3319.37it/s]118598it [00:48, 2299.62it/s]122039it [00:48, 3284.46it/s]118837it [00:48, 2544.74it/s]133139it [00:51, 3218.08it/s]118929it [00:48, 2460.78it/s]119195it [00:48, 2792.32it/s]122372it [00:48, 3207.88it/s]133492it [00:51, 3307.36it/s]119280it [00:48, 2709.23it/s]119549it [00:48, 2981.29it/s]122732it [00:48, 3319.32it/s]133847it [00:51, 3209.30it/s]119610it [00:48, 2768.09it/s]119890it [00:48, 2970.50it/s]123067it [00:48, 3234.91it/s]134201it [00:51, 3301.41it/s]119950it [00:48, 2931.76it/s]120246it [00:49, 3128.29it/s]123421it [00:49, 3322.36it/s]134547it [00:51, 3341.39it/s]120302it [00:49, 3089.61it/s]120582it [00:49, 3101.78it/s]123769it [00:49, 3230.43it/s]134883it [00:51, 3240.05it/s]120635it [00:49, 3045.60it/s]120938it [00:49, 3226.92it/s]124125it [00:49, 3324.41it/s]135240it [00:51, 3334.25it/s]120986it [00:49, 3173.46it/s]124480it [00:49, 3387.25it/s]121273it [00:49, 3120.85it/s]135575it [00:51, 3234.44it/s]121317it [00:49, 3100.54it/s]121629it [00:49, 3242.84it/s]124821it [00:49, 3272.07it/s]135932it [00:51, 3329.20it/s]121660it [00:49, 3191.42it/s]121985it [00:49, 3330.89it/s]125166it [00:49, 3322.13it/s]136287it [00:51, 3392.89it/s]122012it [00:49, 3284.26it/s]122324it [00:49, 3236.83it/s]125500it [00:49, 3218.96it/s]136628it [00:52, 3271.86it/s]122346it [00:49, 3181.48it/s]122683it [00:49, 3337.06it/s]125861it [00:49, 3328.48it/s]136982it [00:52, 3348.25it/s]122701it [00:49, 3284.87it/s]126208it [00:49, 3369.28it/s]123021it [00:49, 3232.30it/s]137319it [00:52, 3236.28it/s]123033it [00:49, 3176.53it/s]123375it [00:50, 3319.43it/s]126547it [00:50, 3233.79it/s]137660it [00:52, 3283.72it/s]123368it [00:49, 3225.86it/s]123731it [00:50, 3386.77it/s]126902it [00:50, 3323.88it/s]138017it [00:52, 3366.70it/s]123720it [00:50, 3309.88it/s]124072it [00:50, 3274.43it/s]127237it [00:50, 3230.43it/s]138355it [00:52, 3257.14it/s]124053it [00:50, 3196.82it/s]124429it [00:50, 3357.79it/s]127597it [00:50, 3333.84it/s]138708it [00:52, 3334.00it/s]124406it [00:50, 3290.26it/s]127955it [00:50, 3403.63it/s]124767it [00:50, 3232.76it/s]139043it [00:52, 3236.41it/s]124737it [00:50, 3189.60it/s]125123it [00:50, 3324.02it/s]128297it [00:50, 3251.55it/s]139400it [00:52, 3331.48it/s]125088it [00:50, 3281.42it/s]128656it [00:50, 3347.45it/s]125458it [00:50, 3229.16it/s]139735it [00:53, 3231.58it/s]125424it [00:50, 3302.02it/s]125818it [00:50, 3333.84it/s]128993it [00:50, 3239.16it/s]140092it [00:53, 3328.68it/s]125756it [00:50, 3193.67it/s]126176it [00:50, 3402.85it/s]129353it [00:50, 3341.43it/s]140446it [00:53, 3389.86it/s]126112it [00:50, 3296.86it/s]126518it [00:50, 3279.07it/s]129689it [00:50, 3238.81it/s]140787it [00:53, 3233.72it/s]126444it [00:50, 3192.09it/s]126874it [00:51, 3357.15it/s]130044it [00:51, 3325.94it/s]141143it [00:53, 3324.51it/s]126796it [00:51, 3283.98it/s]130404it [00:51, 3404.21it/s]127212it [00:51, 3253.09it/s]141478it [00:53, 3223.59it/s]127127it [00:51, 3178.85it/s]127539it [00:51, 3233.83it/s]130746it [00:51, 3260.66it/s]141833it [00:53, 3316.23it/s]127466it [00:51, 3237.81it/s]127894it [00:51, 3324.63it/s]131101it [00:51, 3342.68it/s]142187it [00:53, 3379.05it/s]127816it [00:51, 3312.48it/s]128228it [00:51, 3226.20it/s]131438it [00:51, 3220.16it/s]142527it [00:53, 3263.80it/s]128149it [00:51, 3196.80it/s]128587it [00:51, 3328.97it/s]131780it [00:51, 3276.35it/s]142883it [00:53, 3348.34it/s]128502it [00:51, 3291.10it/s]132136it [00:51, 3355.85it/s]128922it [00:51, 3235.53it/s]143220it [00:54, 3248.96it/s]128833it [00:51, 3187.69it/s]129280it [00:51, 3332.94it/s]132474it [00:51, 3254.31it/s]143577it [00:54, 3331.17it/s]129188it [00:51, 3290.24it/s]129635it [00:51, 3395.34it/s]132832it [00:51, 3347.26it/s]129524it [00:51, 3310.04it/s]143927it [00:54, 3238.16it/s]129976it [00:52, 3280.09it/s]133169it [00:52, 3247.29it/s]144282it [00:54, 3325.72it/s]129857it [00:51, 3196.90it/s]130335it [00:52, 3367.25it/s]133528it [00:52, 3345.39it/s]144627it [00:54, 3360.26it/s]130211it [00:52, 3294.66it/s]130674it [00:52, 3260.12it/s]133864it [00:52, 3250.76it/s]144965it [00:54, 3264.92it/s]130542it [00:52, 3196.45it/s]131032it [00:52, 3351.62it/s]134224it [00:52, 3349.30it/s]145323it [00:54, 3354.49it/s]130895it [00:52, 3291.48it/s]134572it [00:52, 3383.71it/s]131369it [00:52, 3251.69it/s]131248it [00:52, 3359.82it/s]145660it [00:54, 3255.56it/s]131698it [00:52, 3262.22it/s]134912it [00:52, 3274.86it/s]146020it [00:54, 3353.60it/s]131586it [00:52, 3194.72it/s]132050it [00:52, 3336.97it/s]135272it [00:52, 3368.34it/s]146375it [00:55, 3410.24it/s]131936it [00:52, 3280.71it/s]132385it [00:52, 3240.80it/s]135611it [00:52, 3275.99it/s]146718it [00:55, 3288.12it/s]132267it [00:52, 3177.19it/s]132741it [00:52, 3332.55it/s]135971it [00:52, 3367.33it/s]147075it [00:55, 3367.17it/s]132620it [00:52, 3275.90it/s]136321it [00:52, 3404.61it/s]133076it [00:52, 3241.40it/s]147414it [00:55, 3262.65it/s]132972it [00:52, 3344.80it/s]133430it [00:53, 3327.26it/s]136663it [00:53, 3295.55it/s]147771it [00:55, 3349.41it/s]133308it [00:53, 3219.41it/s]133790it [00:53, 3404.35it/s]137023it [00:53, 3383.28it/s]148112it [00:55, 3366.15it/s]133645it [00:53, 3261.71it/s]134132it [00:53, 3291.33it/s]137363it [00:53, 3279.43it/s]148450it [00:55, 3250.85it/s]133973it [00:53, 3167.78it/s]134490it [00:53, 3374.39it/s]137707it [00:53, 3325.13it/s]148805it [00:55, 3335.17it/s]134327it [00:53, 3272.58it/s]134829it [00:53, 3273.49it/s]138048it [00:53, 3347.34it/s]149140it [00:55, 3238.60it/s]134681it [00:53, 3350.05it/s]135177it [00:53, 3325.52it/s]138384it [00:53, 3253.65it/s]149497it [00:55, 3332.35it/s]135018it [00:53, 3231.81it/s]138743it [00:53, 3350.03it/s]135527it [00:53, 3245.00it/s]149832it [00:56, 3233.36it/s]135376it [00:53, 3330.56it/s]135886it [00:53, 3343.05it/s]139080it [00:53, 3219.28it/s]150188it [00:56, 3325.95it/s]135711it [00:53, 3183.32it/s]136245it [00:53, 3414.13it/s]139443it [00:53, 3335.48it/s]150544it [00:56, 3393.77it/s]136067it [00:53, 3289.71it/s]136588it [00:54, 3299.31it/s]139779it [00:54, 3244.89it/s]150885it [00:56, 3281.01it/s]136399it [00:53, 3184.87it/s]136947it [00:54, 3382.79it/s]140140it [00:54, 3347.62it/s]151240it [00:56, 3358.29it/s]136756it [00:54, 3292.89it/s]140499it [00:54, 3409.46it/s]137287it [00:54, 3276.52it/s]151578it [00:56, 3215.03it/s]137111it [00:54, 3365.98it/s]137647it [00:54, 3368.96it/s]140842it [00:54, 3307.73it/s]151935it [00:56, 3314.84it/s]137450it [00:54, 3246.10it/s]137992it [00:54, 3391.02it/s]141192it [00:54, 3362.92it/s]152291it [00:56, 3385.12it/s]137788it [00:54, 3284.21it/s]138333it [00:54, 3283.70it/s]141530it [00:54, 3264.68it/s]138118it [00:54, 3193.20it/s]138691it [00:54, 3366.47it/s]141890it [00:54, 3360.79it/s]138473it [00:54, 3295.01it/s]139029it [00:54, 3270.42it/s]142249it [00:54, 3267.93it/s]138829it [00:54, 3369.56it/s]139389it [00:54, 3365.12it/s]142609it [00:54, 3360.40it/s]139168it [00:54, 3241.35it/s]139727it [00:54, 3261.43it/s]142970it [00:54, 3430.78it/s]139526it [00:54, 3338.13it/s]140087it [00:55, 3358.16it/s]143315it [00:55, 3317.39it/s]139862it [00:55, 3182.81it/s]140445it [00:55, 3420.66it/s]143677it [00:55, 3402.22it/s]140217it [00:55, 3280.05it/s]140789it [00:55, 3303.20it/s]144019it [00:55, 3292.99it/s]140567it [00:55, 3184.38it/s]141149it [00:55, 3385.99it/s]144368it [00:55, 3346.80it/s]140921it [00:55, 3283.07it/s]144733it [00:55, 3433.84it/s]141489it [00:55, 3238.30it/s]141278it [00:55, 3362.79it/s]141848it [00:55, 3336.00it/s]145078it [00:55, 3319.85it/s]141617it [00:55, 3238.09it/s]142208it [00:55, 3411.39it/s]145442it [00:55, 3409.41it/s]141959it [00:55, 3288.38it/s]142551it [00:55, 3297.33it/s]145785it [00:55, 3304.17it/s]142290it [00:55, 3183.66it/s]142910it [00:55, 3380.88it/s]146150it [00:55, 3401.52it/s]142646it [00:55, 3289.03it/s]143250it [00:56, 3276.66it/s]146492it [00:56, 3289.33it/s]143001it [00:55, 3363.89it/s]143610it [00:56, 3369.25it/s]146856it [00:56, 3387.30it/s]143339it [00:56, 3238.08it/s]147217it [00:56, 3450.55it/s]143949it [00:56, 3269.85it/s]143696it [00:56, 3332.04it/s]144309it [00:56, 3362.84it/s]147564it [00:56, 3309.75it/s]144031it [00:56, 3170.31it/s]144674it [00:56, 3443.99it/s]147922it [00:56, 3385.44it/s]144386it [00:56, 3275.81it/s]145020it [00:56, 3284.96it/s]148263it [00:56, 3291.79it/s]144747it [00:56, 3371.34it/s]145381it [00:56, 3376.35it/s]148620it [00:56, 3369.66it/s]145087it [00:56, 3250.34it/s]145721it [00:56, 3286.20it/s]148969it [00:56, 3276.91it/s]145445it [00:56, 3344.46it/s]146085it [00:56, 3384.98it/s]149329it [00:56, 3368.28it/s]145782it [00:56, 3230.56it/s]146443it [00:56, 3439.64it/s]149692it [00:56, 3443.73it/s]146126it [00:56, 3288.92it/s]146789it [00:57, 3324.09it/s]150038it [00:57, 3328.09it/s]146457it [00:57, 3181.10it/s]147147it [00:57, 3392.98it/s]150401it [00:57, 3412.47it/s]146815it [00:57, 3293.91it/s]147488it [00:57, 3305.91it/s]150744it [00:57, 3282.23it/s]147171it [00:57, 3369.30it/s]147847it [00:57, 3384.84it/s]151106it [00:57, 3377.99it/s]147510it [00:57, 3252.33it/s]148187it [00:57, 3288.17it/s]151467it [00:57, 3444.00it/s]147867it [00:57, 3341.70it/s]148529it [00:57, 3325.79it/s]151813it [00:57, 3324.09it/s]148887it [00:57, 3398.76it/s]148203it [00:57, 3175.49it/s]152176it [00:57, 3411.73it/s]148560it [00:57, 3285.17it/s]149228it [00:57, 3295.04it/s]148916it [00:57, 3361.52it/s]149591it [00:57, 3389.56it/s]149255it [00:57, 3240.39it/s]149932it [00:58, 3280.99it/s]149612it [00:58, 3333.87it/s]150294it [00:58, 3375.84it/s]149948it [00:58, 3221.80it/s]150647it [00:58, 3270.16it/s]150292it [00:58, 3283.22it/s]151009it [00:58, 3369.03it/s]151369it [00:58, 3434.16it/s]150647it [00:58, 3191.19it/s]151714it [00:58, 3309.39it/s]151005it [00:58, 3298.39it/s]152061it [00:58, 3354.69it/s]151361it [00:58, 3371.34it/s]152632it [01:01, 258.09it/s] 151700it [00:58, 3250.78it/s]152989it [01:01, 360.01it/s]152059it [00:58, 3345.27it/s]153286it [01:01, 469.94it/s]152396it [00:58, 3211.97it/s]153646it [01:01, 647.56it/s]154006it [01:01, 868.77it/s]154328it [01:01, 1089.88it/s]154689it [01:01, 1394.30it/s]155019it [01:01, 1652.56it/s]155381it [01:01, 1990.07it/s]155742it [01:01, 2308.46it/s]156084it [01:02, 2464.64it/s]156446it [01:02, 2731.69it/s]156784it [01:02, 2815.32it/s]157145it [01:02, 3019.45it/s]157483it [01:02, 3033.67it/s]157844it [01:02, 3188.23it/s]158206it [01:02, 3300.66it/s]158551it [01:02, 3239.26it/s]158913it [01:02, 3344.41it/s]159256it [01:03, 3274.29it/s]159616it [01:03, 3367.11it/s]159958it [01:03, 3233.77it/s]160318it [01:03, 3337.10it/s]160682it [01:03, 3423.28it/s]161028it [01:03, 3310.11it/s]161394it [01:03, 3407.74it/s]161737it [01:03, 3309.38it/s]162103it [01:03, 3408.76it/s]162467it [01:04, 3473.64it/s]162816it [01:04, 3359.74it/s]163180it [01:04, 3440.22it/s]163526it [01:04, 3298.50it/s]163889it [01:04, 3392.75it/s]152519it [01:02, 253.66it/s] 152882it [01:02, 354.67it/s]164231it [01:04, 3304.74it/s]164595it [01:04, 3398.31it/s]153238it [01:02, 482.88it/s]164970it [01:04, 3499.54it/s]153588it [01:02, 648.41it/s]165322it [01:04, 3382.96it/s]153955it [01:02, 868.22it/s]165690it [01:04, 3468.00it/s]154279it [01:02, 1086.12it/s]154639it [01:02, 1382.75it/s]166039it [01:05, 3365.06it/s]152398it [01:02, 262.72it/s] 166407it [01:05, 3454.19it/s]154970it [01:02, 1644.28it/s]152759it [01:02, 367.87it/s]155336it [01:02, 1985.57it/s]166754it [01:05, 3365.67it/s]153122it [01:02, 507.97it/s]155697it [01:03, 2301.62it/s]167122it [01:05, 3454.26it/s]153424it [01:03, 653.07it/s]167476it [01:05, 3479.07it/s]156041it [01:03, 2485.18it/s]153788it [01:03, 880.83it/s]156406it [01:03, 2755.46it/s]167825it [01:05, 3367.07it/s]154108it [01:03, 1102.57it/s]168200it [01:05, 3476.23it/s]156749it [01:03, 2815.62it/s]154472it [01:03, 1413.20it/s]157113it [01:03, 3025.99it/s]168549it [01:05, 3366.91it/s]154835it [01:03, 1742.64it/s]168915it [01:05, 3449.86it/s]157453it [01:03, 3011.94it/s]155175it [01:03, 1995.50it/s]152720it [01:03, 231.26it/s] 157818it [01:03, 3182.61it/s]169262it [01:06, 3345.14it/s]155524it [01:03, 2289.64it/s]153077it [01:03, 325.74it/s]158183it [01:03, 3310.23it/s]169625it [01:06, 3424.84it/s]153355it [01:03, 421.10it/s]155860it [01:03, 2469.59it/s]169988it [01:06, 3482.67it/s]158530it [01:03, 3244.53it/s]153714it [01:03, 587.73it/s]156223it [01:03, 2742.01it/s]170338it [01:06, 3377.42it/s]158898it [01:03, 3364.85it/s]154074it [01:03, 798.14it/s]156585it [01:04, 2962.30it/s]170703it [01:06, 3454.24it/s]159243it [01:04, 3279.48it/s]154390it [01:04, 1007.09it/s]156930it [01:04, 2993.63it/s]159609it [01:04, 3379.78it/s]171050it [01:06, 3308.09it/s]154750it [01:04, 1303.23it/s]157295it [01:04, 3168.29it/s]171416it [01:06, 3406.67it/s]159959it [01:04, 3290.30it/s]155076it [01:04, 1558.20it/s]157638it [01:04, 3143.46it/s]160324it [01:04, 3391.12it/s]171759it [01:06, 3303.01it/s]155436it [01:04, 1895.02it/s]157999it [01:04, 3270.68it/s]160691it [01:04, 3469.33it/s]172129it [01:06, 3415.54it/s]155767it [01:04, 2106.66it/s]158340it [01:04, 3216.63it/s]172497it [01:06, 3491.46it/s]161041it [01:04, 3286.87it/s]156126it [01:04, 2416.97it/s]158692it [01:04, 3301.70it/s]172848it [01:07, 3383.77it/s]161396it [01:04, 3361.00it/s]156487it [01:04, 2690.34it/s]159060it [01:04, 3409.09it/s]173218it [01:07, 3474.37it/s]161735it [01:04, 3283.93it/s]156826it [01:04, 2779.14it/s]159407it [01:04, 3314.16it/s]173567it [01:07, 3359.28it/s]162106it [01:04, 3405.64it/s]157187it [01:04, 2989.79it/s]159775it [01:04, 3417.21it/s]173930it [01:07, 3434.47it/s]162474it [01:05, 3483.72it/s]157525it [01:04, 3004.78it/s]160121it [01:05, 3309.83it/s]174275it [01:07, 3341.62it/s]162825it [01:05, 3376.19it/s]157884it [01:05, 3161.53it/s]160487it [01:05, 3409.40it/s]174650it [01:07, 3458.15it/s]163193it [01:05, 3461.86it/s]158244it [01:05, 3281.86it/s]160831it [01:05, 3306.27it/s]174998it [01:07, 3460.22it/s]163541it [01:05, 3357.65it/s]158588it [01:05, 3169.44it/s]161200it [01:05, 3413.85it/s]175346it [01:07, 3352.58it/s]163895it [01:05, 3407.07it/s]158952it [01:05, 3300.03it/s]161565it [01:05, 3481.72it/s]175713it [01:07, 3441.48it/s]164238it [01:05, 3288.13it/s]159291it [01:05, 3225.35it/s]161915it [01:05, 3342.00it/s]176059it [01:08, 3328.25it/s]164607it [01:05, 3401.03it/s]159654it [01:05, 3337.75it/s]162280it [01:05, 3429.89it/s]176422it [01:08, 3414.02it/s]164967it [01:05, 3458.28it/s]159993it [01:05, 3237.93it/s]162625it [01:05, 3338.51it/s]176765it [01:08, 3328.00it/s]165315it [01:05, 3365.79it/s]160353it [01:05, 3338.32it/s]162992it [01:05, 3432.56it/s]177127it [01:08, 3411.57it/s]165689it [01:05, 3472.48it/s]160710it [01:05, 3369.41it/s]163337it [01:06, 3339.68it/s]177494it [01:08, 3485.90it/s]166038it [01:06, 3383.09it/s]163708it [01:06, 3445.49it/s]161050it [01:06, 3261.06it/s]177844it [01:08, 3374.97it/s]166399it [01:06, 3443.49it/s]164077it [01:06, 3516.18it/s]161412it [01:06, 3362.98it/s]178219it [01:08, 3482.78it/s]166745it [01:06, 3379.11it/s]161751it [01:06, 3267.46it/s]164430it [01:06, 3393.45it/s]167113it [01:06, 3465.77it/s]178569it [01:08, 3320.55it/s]162118it [01:06, 3380.73it/s]164786it [01:06, 3440.71it/s]167487it [01:06, 3544.15it/s]178938it [01:08, 3424.23it/s]165132it [01:06, 3352.42it/s]162477it [01:06, 3272.16it/s]167843it [01:06, 3420.95it/s]179283it [01:08, 3319.09it/s]165501it [01:06, 3449.14it/s]162840it [01:06, 3373.05it/s]168218it [01:06, 3512.51it/s]179653it [01:09, 3426.63it/s]163187it [01:06, 3399.88it/s]165848it [01:06, 3360.04it/s]180019it [01:09, 3492.33it/s]168571it [01:06, 3366.03it/s]166225it [01:06, 3476.27it/s]163529it [01:06, 3297.22it/s]180370it [01:09, 3410.85it/s]168943it [01:06, 3465.75it/s]166599it [01:06, 3551.64it/s]163893it [01:06, 3394.97it/s]180732it [01:09, 3470.83it/s]169292it [01:07, 3364.60it/s]166956it [01:07, 3429.98it/s]164235it [01:06, 3298.64it/s]181081it [01:09, 3379.86it/s]169661it [01:07, 3456.39it/s]167327it [01:07, 3509.11it/s]164597it [01:07, 3390.73it/s]181443it [01:09, 3448.28it/s]170029it [01:07, 3519.04it/s]164970it [01:07, 3489.04it/s]167680it [01:07, 3399.15it/s]181797it [01:09, 3355.09it/s]170383it [01:07, 3415.05it/s]168039it [01:07, 3452.87it/s]165321it [01:07, 3368.26it/s]182169it [01:09, 3458.38it/s]170749it [01:07, 3484.13it/s]165673it [01:07, 3409.74it/s]168386it [01:07, 3361.48it/s]182537it [01:09, 3520.57it/s]171099it [01:07, 3363.10it/s]168756it [01:07, 3458.05it/s]166016it [01:07, 3317.95it/s]182891it [01:09, 3427.36it/s]171468it [01:07, 3456.73it/s]169125it [01:07, 3523.00it/s]166384it [01:07, 3420.21it/s]183263it [01:10, 3511.95it/s]171816it [01:07, 3348.63it/s]169479it [01:07, 3388.54it/s]166728it [01:07, 3339.54it/s]183616it [01:10, 3397.66it/s]172174it [01:07, 3414.84it/s]169846it [01:07, 3467.73it/s]167095it [01:07, 3434.45it/s]183984it [01:10, 3478.38it/s]172549it [01:07, 3509.72it/s]167462it [01:07, 3502.24it/s]170195it [01:08, 3366.32it/s]184334it [01:10, 3388.29it/s]172902it [01:08, 3411.04it/s]170565it [01:08, 3459.95it/s]167814it [01:08, 3377.26it/s]184697it [01:10, 3456.48it/s]173271it [01:08, 3490.96it/s]168171it [01:08, 3430.30it/s]170913it [01:08, 3359.71it/s]185072it [01:10, 3539.82it/s]173622it [01:08, 3378.17it/s]171269it [01:08, 3415.75it/s]168516it [01:08, 3335.66it/s]185428it [01:10, 3397.21it/s]173991it [01:08, 3466.24it/s]171637it [01:08, 3491.73it/s]168882it [01:08, 3428.94it/s]185796it [01:10, 3477.08it/s]174340it [01:08, 3357.91it/s]171988it [01:08, 3379.40it/s]169227it [01:08, 3309.52it/s]186146it [01:10, 3400.20it/s]174721it [01:08, 3486.04it/s]172353it [01:08, 3454.97it/s]169593it [01:08, 3408.57it/s]186515it [01:11, 3482.11it/s]175078it [01:08, 3365.26it/s]169957it [01:08, 3475.00it/s]172700it [01:08, 3360.84it/s]186865it [01:11, 3423.10it/s]175448it [01:08, 3458.74it/s]173074it [01:08, 3468.58it/s]170306it [01:08, 3355.20it/s]187234it [01:11, 3500.31it/s]175821it [01:08, 3535.20it/s]170657it [01:08, 3397.34it/s]173423it [01:08, 3362.34it/s]187605it [01:11, 3561.45it/s]176177it [01:09, 3349.32it/s]173794it [01:09, 3460.17it/s]170999it [01:08, 3293.75it/s]187962it [01:11, 3438.64it/s]176547it [01:09, 3446.56it/s]174167it [01:09, 3532.88it/s]171365it [01:09, 3398.14it/s]188331it [01:11, 3510.37it/s]176895it [01:09, 3361.92it/s]174522it [01:09, 3395.87it/s]171718it [01:09, 3285.93it/s]188684it [01:11, 3417.56it/s]177262it [01:09, 3448.39it/s]174885it [01:09, 3462.53it/s]172087it [01:09, 3400.22it/s]189051it [01:11, 3489.94it/s]177609it [01:09, 3338.25it/s]172459it [01:09, 3490.04it/s]175233it [01:09, 3374.39it/s]189402it [01:11, 3362.67it/s]177987it [01:09, 3462.26it/s]175600it [01:09, 3458.12it/s]172810it [01:09, 3336.82it/s]189772it [01:11, 3457.03it/s]178365it [01:09, 3552.86it/s]175948it [01:09, 3355.39it/s]173184it [01:09, 3450.72it/s]190149it [01:12, 3545.57it/s]178722it [01:09, 3428.21it/s]176316it [01:09, 3447.62it/s]173532it [01:09, 3334.13it/s]190505it [01:12, 3424.62it/s]179091it [01:09, 3503.07it/s]176687it [01:09, 3520.36it/s]173896it [01:09, 3419.75it/s]190874it [01:12, 3499.74it/s]179443it [01:09, 3376.47it/s]177041it [01:10, 3387.69it/s]174240it [01:09, 3325.76it/s]191226it [01:12, 3394.59it/s]179803it [01:10, 3439.37it/s]177398it [01:10, 3437.75it/s]174617it [01:10, 3451.53it/s]191567it [01:12, 3298.65it/s]180149it [01:10, 3351.03it/s]174967it [01:10, 3464.50it/s]177744it [01:10, 3276.26it/s]191899it [01:12, 3264.18it/s]180523it [01:10, 3461.43it/s]178121it [01:10, 3415.79it/s]175315it [01:10, 3352.24it/s]192271it [01:12, 3394.78it/s]180897it [01:10, 3541.65it/s]175685it [01:10, 3450.54it/s]178465it [01:10, 3328.04it/s]192647it [01:12, 3500.50it/s]181253it [01:10, 3412.14it/s]178842it [01:10, 3454.17it/s]176032it [01:10, 3328.71it/s]192999it [01:12, 3400.28it/s]181622it [01:10, 3491.34it/s]179207it [01:10, 3505.48it/s]176396it [01:10, 3417.15it/s]193369it [01:13, 3485.17it/s]181973it [01:10, 3377.78it/s]179560it [01:10, 3397.03it/s]176758it [01:10, 3323.45it/s]193719it [01:13, 3381.22it/s]182349it [01:10, 3486.79it/s]179926it [01:10, 3470.54it/s]177108it [01:10, 3369.17it/s]194099it [01:13, 3500.99it/s]182700it [01:10, 3401.59it/s]180275it [01:10, 3393.32it/s]177476it [01:10, 3457.92it/s]194451it [01:13, 3412.14it/s]183070it [01:11, 3486.74it/s]180628it [01:11, 3432.03it/s]177824it [01:10, 3346.42it/s]194818it [01:13, 3485.22it/s]183446it [01:11, 3566.00it/s]180973it [01:11, 3358.64it/s]178200it [01:11, 3464.74it/s]195209it [01:13, 3606.73it/s]183804it [01:11, 3401.77it/s]181340it [01:11, 3446.30it/s]178549it [01:11, 3344.19it/s]195571it [01:13, 3451.66it/s]184179it [01:11, 3491.72it/s]181709it [01:11, 3514.84it/s]178909it [01:11, 3416.98it/s]195937it [01:13, 3503.69it/s]184531it [01:11, 3384.52it/s]182062it [01:11, 3406.24it/s]179275it [01:11, 3484.82it/s]196289it [01:13, 3391.52it/s]184909it [01:11, 3494.97it/s]182432it [01:11, 3491.11it/s]179625it [01:11, 3373.43it/s]196673it [01:13, 3518.18it/s]185261it [01:11, 3390.69it/s]182783it [01:11, 3412.14it/s]179985it [01:11, 3437.47it/s]197027it [01:14, 3427.38it/s]185626it [01:11, 3463.37it/s]183153it [01:11, 3493.04it/s]180331it [01:11, 3364.59it/s]197406it [01:14, 3529.58it/s]185999it [01:11, 3376.91it/s]183504it [01:11, 3392.20it/s]180695it [01:11, 3442.31it/s]197761it [01:14, 3400.89it/s]186378it [01:11, 3493.09it/s]183863it [01:12, 3447.26it/s]181041it [01:11, 3312.02it/s]198138it [01:14, 3503.70it/s]186768it [01:12, 3608.89it/s]184233it [01:12, 3520.06it/s]181401it [01:12, 3394.30it/s]198512it [01:14, 3570.22it/s]187131it [01:12, 3450.80it/s]184586it [01:12, 3413.28it/s]181771it [01:12, 3481.47it/s]187490it [01:12, 3487.82it/s]184958it [01:12, 3500.85it/s]182121it [01:12, 3378.59it/s]187841it [01:12, 3397.33it/s]185310it [01:12, 3382.26it/s]182494it [01:12, 3479.45it/s]188217it [01:12, 3499.81it/s]185677it [01:12, 3463.91it/s]182844it [01:12, 3387.74it/s]188569it [01:12, 3405.27it/s]186025it [01:12, 3374.19it/s]183198it [01:12, 3422.60it/s]188939it [01:12, 3480.88it/s]186402it [01:12, 3487.99it/s]183542it [01:12, 3335.49it/s]189312it [01:12, 3550.73it/s]186793it [01:12, 3609.33it/s]183910it [01:12, 3433.98it/s]189669it [01:12, 3419.35it/s]187156it [01:12, 3442.15it/s]184285it [01:12, 3523.82it/s]190048it [01:13, 3525.00it/s]187530it [01:13, 3525.40it/s]184639it [01:12, 3399.23it/s]190403it [01:13, 3425.97it/s]187885it [01:13, 3435.94it/s]185017it [01:13, 3507.96it/s]190773it [01:13, 3502.59it/s]188257it [01:13, 3516.27it/s]185370it [01:13, 3358.18it/s]191125it [01:13, 3349.06it/s]188611it [01:13, 3421.31it/s]185726it [01:13, 3414.07it/s]191498it [01:13, 3457.00it/s]188980it [01:13, 3496.39it/s]186070it [01:13, 3329.70it/s]189355it [01:13, 3569.01it/s]191878it [01:13, 3554.65it/s]186443it [01:13, 3442.89it/s]192236it [01:13, 3440.46it/s]189714it [01:13, 3426.37it/s]186835it [01:13, 3580.65it/s]192619it [01:13, 3550.91it/s]190077it [01:13, 3484.52it/s]187195it [01:13, 3425.29it/s]192976it [01:13, 3452.80it/s]190427it [01:13, 3386.24it/s]187567it [01:13, 3509.20it/s]193349it [01:13, 3524.29it/s]190798it [01:13, 3476.66it/s]187920it [01:13, 3369.49it/s]193703it [01:14, 3411.43it/s]191148it [01:14, 3370.98it/s]188292it [01:14, 3467.65it/s]194086it [01:14, 3528.69it/s]191518it [01:14, 3464.86it/s]188641it [01:14, 3369.68it/s]194441it [01:14, 3429.64it/s]191877it [01:14, 3382.57it/s]189008it [01:14, 3452.21it/s]194817it [01:14, 3522.90it/s]192252it [01:14, 3486.38it/s]189358it [01:14, 3349.43it/s]195193it [01:14, 3589.44it/s]192632it [01:14, 3577.08it/s]189726it [01:14, 3441.60it/s]195554it [01:14, 3468.19it/s]192992it [01:14, 3464.69it/s]190090it [01:14, 3498.37it/s]195924it [01:14, 3533.03it/s]193351it [01:14, 3498.20it/s]190442it [01:14, 3344.16it/s]196279it [01:14, 3413.86it/s]193703it [01:14, 3392.64it/s]190814it [01:14, 3450.78it/s]196662it [01:14, 3531.36it/s]194085it [01:14, 3513.48it/s]191162it [01:14, 3342.60it/s]197017it [01:15, 3447.18it/s]194438it [01:15, 3417.07it/s]191531it [01:14, 3441.43it/s]197396it [01:15, 3544.29it/s]194812it [01:15, 3509.26it/s]191878it [01:15, 3332.76it/s]195203it [01:15, 3623.31it/s]197759it [01:15, 3431.55it/s]192253it [01:15, 3450.32it/s]198147it [01:15, 3557.73it/s]195567it [01:15, 3490.75it/s]192633it [01:15, 3549.77it/s]198524it [01:15, 3616.93it/s]195934it [01:15, 3540.82it/s]192990it [01:15, 3431.07it/s]196290it [01:15, 3412.40it/s]193359it [01:15, 3505.27it/s]196664it [01:15, 3503.49it/s]193712it [01:15, 3383.40it/s]197016it [01:15, 3419.80it/s]194084it [01:15, 3478.09it/s]197394it [01:15, 3520.85it/s]194434it [01:15, 3374.78it/s]197757it [01:16, 3404.71it/s]194808it [01:15, 3478.77it/s]198145it [01:16, 3538.32it/s]195195it [01:16, 3592.04it/s]198522it [01:16, 3602.55it/s]195556it [01:16, 3450.05it/s]195924it [01:16, 3515.37it/s]196278it [01:16, 3353.87it/s]196663it [01:16, 3492.21it/s]197015it [01:16, 3403.70it/s]197393it [01:16, 3510.78it/s]197758it [01:16, 3389.86it/s]198146it [01:16, 3526.60it/s]198513it [01:17, 3567.83it/s]198871it [01:19, 224.45it/s] 199242it [01:19, 313.51it/s]199563it [01:19, 415.53it/s]199926it [01:20, 569.45it/s]200294it [01:20, 768.54it/s]200627it [01:20, 977.44it/s]200996it [01:20, 1265.57it/s]201335it [01:20, 1527.36it/s]201694it [01:20, 1851.00it/s]202033it [01:20, 2107.04it/s]202422it [01:20, 2472.84it/s]202801it [01:20, 2770.53it/s]203160it [01:20, 2893.22it/s]203531it [01:21, 3100.51it/s]203886it [01:21, 3114.51it/s]204258it [01:21, 3276.09it/s]204610it [01:21, 3205.04it/s]204983it [01:21, 3347.39it/s]205358it [01:21, 3459.68it/s]205714it [01:21, 3368.96it/s]206091it [01:21, 3481.11it/s]206445it [01:21, 3398.46it/s]206817it [01:22, 3487.84it/s]207170it [01:22, 3360.27it/s]207543it [01:22, 3465.07it/s]207906it [01:22, 3384.96it/s]208293it [01:22, 3520.87it/s]208668it [01:22, 3585.78it/s]209029it [01:22, 3469.32it/s]209404it [01:22, 3548.48it/s]209761it [01:22, 3378.06it/s]210129it [01:22, 3461.99it/s]210478it [01:23, 3359.72it/s]210846it [01:23, 3445.97it/s]211217it [01:23, 3520.27it/s]198888it [01:20, 215.69it/s] 211571it [01:23, 3402.50it/s]199252it [01:21, 298.82it/s]211929it [01:23, 3452.47it/s]199575it [01:21, 397.06it/s]212276it [01:23, 3351.46it/s]199941it [01:21, 546.11it/s]212648it [01:23, 3454.51it/s]200311it [01:21, 739.36it/s]212995it [01:23, 3352.26it/s]200645it [01:21, 943.85it/s]213364it [01:23, 3447.84it/s]201016it [01:21, 1227.72it/s]213727it [01:24, 3499.77it/s]201357it [01:21, 1490.19it/s]214079it [01:24, 3389.63it/s]201737it [01:21, 1843.46it/s]198884it [01:21, 210.23it/s] 214427it [01:24, 3415.15it/s]202084it [01:21, 2106.37it/s]199263it [01:21, 295.22it/s]214770it [01:24, 3315.08it/s]202472it [01:21, 2463.58it/s]199575it [01:22, 387.81it/s]215143it [01:24, 3432.30it/s]202859it [01:22, 2776.50it/s]199941it [01:22, 534.23it/s]215488it [01:24, 3338.16it/s]203223it [01:22, 2872.64it/s]200310it [01:22, 723.78it/s]215861it [01:24, 3450.54it/s]203596it [01:22, 3085.59it/s]200643it [01:22, 925.25it/s]216233it [01:24, 3527.73it/s]203952it [01:22, 3110.60it/s]201013it [01:22, 1205.49it/s]216587it [01:24, 3369.74it/s]204327it [01:22, 3279.51it/s]201353it [01:22, 1465.26it/s]216966it [01:24, 3489.86it/s]204681it [01:22, 3252.08it/s]201728it [01:22, 1810.58it/s]217318it [01:25, 3394.71it/s]205054it [01:22, 3382.65it/s]202073it [01:22, 2074.39it/s]217704it [01:25, 3525.34it/s]205406it [01:22, 3330.75it/s]202460it [01:22, 2433.12it/s]218059it [01:25, 3442.24it/s]205781it [01:22, 3447.16it/s]202834it [01:22, 2722.63it/s]218433it [01:25, 3525.53it/s]206160it [01:23, 3544.73it/s]203193it [01:23, 2860.70it/s]218812it [01:25, 3600.08it/s]206521it [01:23, 3452.20it/s]203565it [01:23, 3070.45it/s]219174it [01:25, 3483.67it/s]198872it [01:23, 191.85it/s] 206879it [01:23, 3488.25it/s]203920it [01:23, 3097.11it/s]219557it [01:25, 3581.22it/s]199241it [01:23, 268.31it/s]207231it [01:23, 3410.43it/s]204293it [01:23, 3265.60it/s]199560it [01:23, 356.78it/s]219917it [01:25, 3480.71it/s]207606it [01:23, 3506.37it/s]204646it [01:23, 3243.03it/s]199924it [01:23, 492.89it/s]220302it [01:25, 3585.83it/s]207959it [01:23, 3422.01it/s]205022it [01:23, 3384.10it/s]200286it [01:23, 667.69it/s]220662it [01:26, 3503.76it/s]208345it [01:23, 3545.97it/s]205385it [01:23, 3326.41it/s]200617it [01:23, 856.27it/s]221035it [01:26, 3569.03it/s]208723it [01:23, 3612.63it/s]205758it [01:23, 3439.37it/s]200981it [01:23, 1120.12it/s]221393it [01:26, 3439.27it/s]209086it [01:23, 3503.16it/s]206126it [01:23, 3508.19it/s]201315it [01:23, 1372.02it/s]221786it [01:26, 3579.60it/s]209457it [01:23, 3561.73it/s]206483it [01:24, 3421.53it/s]201687it [01:23, 1710.64it/s]222155it [01:26, 3609.59it/s]209815it [01:24, 3436.11it/s]206855it [01:24, 3502.76it/s]202027it [01:24, 1972.93it/s]222518it [01:26, 3477.95it/s]210183it [01:24, 3504.95it/s]207209it [01:24, 3415.18it/s]202412it [01:24, 2337.36it/s]222887it [01:26, 3531.02it/s]210535it [01:24, 3354.33it/s]207580it [01:24, 3498.68it/s]202782it [01:24, 2632.61it/s]223242it [01:26, 3415.62it/s]210907it [01:24, 3456.43it/s]207933it [01:24, 3415.15it/s]203137it [01:24, 2782.55it/s]223594it [01:26, 3444.20it/s]211268it [01:24, 3364.19it/s]208322it [01:24, 3549.98it/s]203505it [01:24, 3003.67it/s]223940it [01:26, 3357.00it/s]211639it [01:24, 3461.83it/s]208694it [01:24, 3598.94it/s]203857it [01:24, 3031.68it/s]224314it [01:27, 3465.21it/s]212016it [01:24, 3549.84it/s]209056it [01:24, 3492.50it/s]204228it [01:24, 3210.31it/s]224681it [01:27, 3523.78it/s]212373it [01:24, 3435.75it/s]209419it [01:24, 3530.33it/s]204577it [01:24, 3184.86it/s]225035it [01:27, 3399.55it/s]212744it [01:24, 3513.54it/s]209774it [01:24, 3407.45it/s]204937it [01:24, 3296.90it/s]225402it [01:27, 3476.52it/s]213097it [01:25, 3407.93it/s]210143it [01:25, 3486.46it/s]205312it [01:25, 3424.30it/s]225752it [01:27, 3366.44it/s]213468it [01:25, 3493.76it/s]210494it [01:25, 3387.16it/s]205666it [01:25, 3332.50it/s]226100it [01:27, 3398.91it/s]213819it [01:25, 3391.56it/s]210860it [01:25, 3464.87it/s]206039it [01:25, 3442.47it/s]226442it [01:27, 3295.97it/s]214190it [01:25, 3482.68it/s]211234it [01:25, 3542.22it/s]206390it [01:25, 3363.15it/s]226800it [01:27, 3369.54it/s]214541it [01:25, 3488.37it/s]211590it [01:25, 3422.77it/s]206748it [01:25, 3423.11it/s]227160it [01:27, 3436.34it/s]214891it [01:25, 3379.81it/s]211964it [01:25, 3513.88it/s]207094it [01:25, 3336.49it/s]227505it [01:28, 3327.01it/s]215262it [01:25, 3474.83it/s]212317it [01:25, 3402.18it/s]207464it [01:25, 3440.51it/s]227865it [01:28, 3404.68it/s]215611it [01:25, 3384.04it/s]212679it [01:25, 3462.28it/s]207837it [01:25, 3519.40it/s]228207it [01:28, 3273.25it/s]215981it [01:25, 3473.48it/s]213027it [01:25, 3368.36it/s]208191it [01:25, 3424.40it/s]228571it [01:28, 3377.22it/s]216330it [01:25, 3397.09it/s]213398it [01:26, 3465.48it/s]208568it [01:25, 3523.07it/s]228911it [01:28, 3285.89it/s]216707it [01:26, 3503.89it/s]213768it [01:26, 3532.33it/s]208922it [01:26, 3387.87it/s]229276it [01:28, 3389.09it/s]217090it [01:26, 3596.86it/s]214123it [01:26, 3418.52it/s]209297it [01:26, 3490.56it/s]229651it [01:28, 3492.28it/s]217451it [01:26, 3464.68it/s]214488it [01:26, 3482.92it/s]209648it [01:26, 3359.70it/s]230002it [01:28, 3371.89it/s]217821it [01:26, 3532.07it/s]214838it [01:26, 3374.24it/s]210012it [01:26, 3438.93it/s]230356it [01:28, 3409.26it/s]218176it [01:26, 3421.11it/s]215213it [01:26, 3480.23it/s]210380it [01:26, 3508.51it/s]230699it [01:28, 3302.78it/s]218548it [01:26, 3501.13it/s]215563it [01:26, 3354.09it/s]210733it [01:26, 3379.02it/s]231073it [01:29, 3427.27it/s]218900it [01:26, 3429.19it/s]215931it [01:26, 3444.66it/s]211088it [01:26, 3426.54it/s]219287it [01:26, 3555.44it/s]231427it [01:29, 3326.24it/s]216306it [01:26, 3372.28it/s]211433it [01:26, 3318.86it/s]219644it [01:26, 3558.37it/s]231790it [01:29, 3411.92it/s]216682it [01:26, 3480.20it/s]211801it [01:26, 3422.12it/s]232154it [01:29, 3475.52it/s]220001it [01:27, 3483.46it/s]217068it [01:27, 3588.24it/s]212145it [01:27, 3315.15it/s]220391it [01:27, 3604.09it/s]232503it [01:29, 3362.66it/s]217429it [01:27, 3450.39it/s]212512it [01:27, 3416.50it/s]232852it [01:29, 3398.07it/s]220753it [01:27, 3511.10it/s]217821it [01:27, 3582.35it/s]212878it [01:27, 3485.26it/s]221137it [01:27, 3606.38it/s]233194it [01:29, 3318.95it/s]218182it [01:27, 3496.38it/s]213228it [01:27, 3330.23it/s]233559it [01:29, 3412.42it/s]221499it [01:27, 3509.48it/s]218555it [01:27, 3561.48it/s]213591it [01:27, 3414.34it/s]233925it [01:29, 3484.15it/s]221888it [01:27, 3616.68it/s]218913it [01:27, 3440.72it/s]213935it [01:27, 3316.09it/s]234275it [01:30, 3370.80it/s]222251it [01:27, 3432.07it/s]219304it [01:27, 3574.23it/s]214300it [01:27, 3409.66it/s]234632it [01:30, 3427.07it/s]222624it [01:27, 3516.47it/s]219666it [01:27, 3461.60it/s]214643it [01:27, 3300.57it/s]222998it [01:27, 3579.15it/s]234976it [01:30, 3297.69it/s]220058it [01:27, 3592.01it/s]214996it [01:27, 3364.67it/s]235340it [01:30, 3394.82it/s]223358it [01:27, 3453.81it/s]220451it [01:28, 3688.33it/s]215369it [01:27, 3468.58it/s]223728it [01:28, 3519.55it/s]235682it [01:30, 3286.47it/s]220822it [01:28, 3564.33it/s]215718it [01:28, 3352.01it/s]236047it [01:30, 3384.84it/s]224082it [01:28, 3425.47it/s]221205it [01:28, 3638.53it/s]216091it [01:28, 3458.87it/s]236409it [01:30, 3451.09it/s]224455it [01:28, 3510.41it/s]221571it [01:28, 3538.01it/s]216439it [01:28, 3359.59it/s]236756it [01:30, 3339.68it/s]224808it [01:28, 3404.29it/s]221957it [01:28, 3629.81it/s]216808it [01:28, 3454.37it/s]237118it [01:30, 3418.59it/s]225182it [01:28, 3500.22it/s]222322it [01:28, 3461.11it/s]217155it [01:28, 3347.05it/s]237462it [01:30, 3260.76it/s]225547it [01:28, 3542.33it/s]222696it [01:28, 3537.62it/s]217530it [01:28, 3461.87it/s]237821it [01:31, 3353.09it/s]225903it [01:28, 3359.41it/s]223052it [01:28, 3421.35it/s]217917it [01:28, 3571.97it/s]226269it [01:28, 3443.63it/s]238159it [01:31, 3261.92it/s]223423it [01:28, 3503.35it/s]218276it [01:28, 3454.02it/s]238519it [01:31, 3357.49it/s]226616it [01:28, 3346.75it/s]223794it [01:28, 3562.32it/s]218658it [01:28, 3558.23it/s]238880it [01:31, 3428.33it/s]226975it [01:29, 3416.15it/s]224152it [01:29, 3451.13it/s]219016it [01:29, 3441.85it/s]239225it [01:31, 3304.20it/s]227319it [01:29, 3316.79it/s]224521it [01:29, 3518.49it/s]219387it [01:29, 3514.03it/s]239579it [01:31, 3369.27it/s]227685it [01:29, 3413.07it/s]224875it [01:29, 3403.56it/s]219740it [01:29, 3421.49it/s]228051it [01:29, 3483.47it/s]239918it [01:31, 3281.07it/s]225235it [01:29, 3458.72it/s]220124it [01:29, 3539.05it/s]240280it [01:31, 3376.55it/s]228401it [01:29, 3352.01it/s]225583it [01:29, 3361.92it/s]220507it [01:29, 3476.12it/s]240642it [01:31, 3445.69it/s]228768it [01:29, 3440.16it/s]225947it [01:29, 3441.36it/s]220887it [01:29, 3567.75it/s]240988it [01:32, 3337.08it/s]229114it [01:29, 3350.58it/s]226311it [01:29, 3497.54it/s]221263it [01:29, 3621.58it/s]241358it [01:32, 3441.13it/s]229486it [01:29, 3456.01it/s]226662it [01:29, 3379.12it/s]221627it [01:29, 3498.88it/s]241704it [01:32, 3314.01it/s]229834it [01:29, 3315.09it/s]227017it [01:29, 3426.30it/s]222005it [01:29, 3577.95it/s]242067it [01:32, 3401.78it/s]230199it [01:29, 3408.39it/s]227361it [01:30, 3329.76it/s]222365it [01:29, 3430.71it/s]230563it [01:30, 3473.52it/s]242409it [01:32, 3315.58it/s]227724it [01:30, 3414.25it/s]222739it [01:30, 3517.08it/s]242769it [01:32, 3397.25it/s]230912it [01:30, 3373.47it/s]228067it [01:30, 3315.20it/s]243136it [01:32, 3474.19it/s]223093it [01:30, 3385.80it/s]231288it [01:30, 3477.00it/s]228423it [01:30, 3383.19it/s]223447it [01:30, 3421.58it/s]243485it [01:32, 3359.36it/s]231638it [01:30, 3369.56it/s]228791it [01:30, 3467.27it/s]223815it [01:30, 3493.39it/s]243851it [01:32, 3444.68it/s]232006it [01:30, 3456.39it/s]229139it [01:30, 3355.39it/s]224166it [01:30, 3389.51it/s]244197it [01:32, 3313.52it/s]232354it [01:30, 3352.66it/s]229515it [01:30, 3470.55it/s]224533it [01:30, 3467.51it/s]244556it [01:33, 3391.50it/s]232715it [01:30, 3426.09it/s]229864it [01:30, 3369.82it/s]224882it [01:30, 3349.06it/s]233078it [01:30, 3484.40it/s]244897it [01:33, 3293.55it/s]230228it [01:30, 3446.42it/s]225251it [01:30, 3443.76it/s]245259it [01:33, 3384.81it/s]233428it [01:30, 3353.92it/s]230586it [01:30, 3330.83it/s]245625it [01:33, 3462.82it/s]225597it [01:30, 3303.09it/s]233800it [01:31, 3458.29it/s]230959it [01:31, 3443.16it/s]225957it [01:31, 3387.34it/s]245973it [01:33, 3343.37it/s]234148it [01:31, 3363.81it/s]231334it [01:31, 3529.50it/s]226317it [01:31, 3442.88it/s]246326it [01:33, 3395.50it/s]234513it [01:31, 3444.10it/s]231689it [01:31, 3370.53it/s]226663it [01:31, 3328.32it/s]246667it [01:33, 3297.55it/s]234859it [01:31, 3343.94it/s]232055it [01:31, 3452.01it/s]227016it [01:31, 3386.12it/s]247033it [01:33, 3399.45it/s]235228it [01:31, 3441.52it/s]232403it [01:31, 3353.33it/s]235591it [01:31, 3494.49it/s]227357it [01:31, 3273.23it/s]247387it [01:33, 3302.97it/s]232766it [01:31, 3430.24it/s]227708it [01:31, 3340.50it/s]247753it [01:34, 3404.20it/s]235942it [01:31, 3380.23it/s]233111it [01:31, 3343.56it/s]248118it [01:34, 3473.77it/s]236309it [01:31, 3463.16it/s]228067it [01:31, 3245.35it/s]233483it [01:31, 3450.12it/s]248467it [01:34, 3353.12it/s]228432it [01:31, 3358.49it/s]236657it [01:31, 3332.54it/s]233853it [01:31, 3521.94it/s]228793it [01:31, 3430.35it/s]248822it [01:34, 3407.25it/s]237027it [01:31, 3436.70it/s]234207it [01:32, 3407.68it/s]249165it [01:34, 3318.24it/s]229138it [01:31, 3310.77it/s]237373it [01:32, 3330.13it/s]234562it [01:32, 3429.71it/s]249530it [01:34, 3411.58it/s]229511it [01:32, 3428.09it/s]237735it [01:32, 3412.96it/s]234907it [01:32, 3330.31it/s]249893it [01:34, 3473.68it/s]238103it [01:32, 3489.46it/s]229856it [01:32, 3294.35it/s]235275it [01:32, 3430.09it/s]250242it [01:34, 3360.45it/s]230217it [01:32, 3383.33it/s]238454it [01:32, 3365.27it/s]235626it [01:32, 3318.21it/s]250609it [01:34, 3449.12it/s]230577it [01:32, 3444.36it/s]238818it [01:32, 3443.17it/s]235991it [01:32, 3412.14it/s]250956it [01:34, 3330.54it/s]230924it [01:32, 3330.76it/s]239164it [01:32, 3327.84it/s]236356it [01:32, 3478.90it/s]251317it [01:35, 3409.21it/s]231293it [01:32, 3431.66it/s]239527it [01:32, 3412.76it/s]236706it [01:32, 3360.37it/s]251660it [01:35, 3304.70it/s]239870it [01:32, 3321.17it/s]231638it [01:32, 3289.36it/s]237074it [01:32, 3450.93it/s]252024it [01:35, 3398.40it/s]240236it [01:32, 3416.16it/s]232002it [01:32, 3388.63it/s]237421it [01:32, 3341.00it/s]252390it [01:35, 3474.19it/s]240598it [01:33, 3475.27it/s]232343it [01:32, 3283.65it/s]237786it [01:33, 3427.86it/s]252739it [01:35, 3355.57it/s]240947it [01:33, 3371.08it/s]232708it [01:33, 3387.97it/s]238137it [01:33, 3449.25it/s]253106it [01:35, 3445.19it/s]241316it [01:33, 3462.12it/s]233075it [01:33, 3467.55it/s]238484it [01:33, 3331.96it/s]253452it [01:35, 3323.92it/s]241664it [01:33, 3335.06it/s]233424it [01:33, 3356.40it/s]238846it [01:33, 3412.51it/s]253820it [01:35, 3424.67it/s]242037it [01:33, 3446.84it/s]233781it [01:33, 3417.13it/s]239189it [01:33, 3318.05it/s]254165it [01:35, 3319.72it/s]242384it [01:33, 3351.52it/s]234125it [01:33, 3314.14it/s]239550it [01:33, 3400.90it/s]254529it [01:36, 3409.55it/s]242752it [01:33, 3445.61it/s]234489it [01:33, 3406.33it/s]239892it [01:33, 3319.69it/s]254893it [01:36, 3475.03it/s]243119it [01:33, 3509.89it/s]234832it [01:33, 3288.79it/s]240253it [01:33, 3402.17it/s]255242it [01:36, 3350.59it/s]243472it [01:33, 3397.53it/s]235197it [01:33, 3389.77it/s]240618it [01:33, 3473.27it/s]255603it [01:36, 3422.86it/s]243843it [01:33, 3485.44it/s]235557it [01:33, 3442.33it/s]240967it [01:34, 3358.16it/s]244193it [01:34, 3374.84it/s]235903it [01:34, 3303.72it/s]241341it [01:34, 3466.09it/s]244557it [01:34, 3448.43it/s]236266it [01:34, 3395.15it/s]241689it [01:34, 3316.99it/s]244904it [01:34, 3341.29it/s]236608it [01:34, 3284.71it/s]242055it [01:34, 3412.17it/s]245267it [01:34, 3421.75it/s]236976it [01:34, 3397.43it/s]242399it [01:34, 3323.34it/s]245636it [01:34, 3498.48it/s]237318it [01:34, 3278.97it/s]242766it [01:34, 3413.61it/s]245988it [01:34, 3372.23it/s]237682it [01:34, 3379.55it/s]243135it [01:34, 3491.98it/s]246346it [01:34, 3429.66it/s]238036it [01:34, 3424.88it/s]243486it [01:34, 3380.95it/s]246691it [01:34, 3338.21it/s]238380it [01:34, 3296.26it/s]243855it [01:34, 3469.60it/s]247060it [01:34, 3438.03it/s]238744it [01:34, 3392.62it/s]244204it [01:34, 3361.08it/s]247406it [01:35, 3344.49it/s]239085it [01:34, 3276.75it/s]244566it [01:35, 3435.34it/s]247771it [01:35, 3431.56it/s]239446it [01:35, 3371.15it/s]244911it [01:35, 3331.50it/s]248135it [01:35, 3492.02it/s]239813it [01:35, 3455.47it/s]245259it [01:35, 3373.54it/s]248486it [01:35, 3371.73it/s]240161it [01:35, 3294.17it/s]245628it [01:35, 3463.42it/s]248854it [01:35, 3458.74it/s]240523it [01:35, 3386.11it/s]245976it [01:35, 3352.18it/s]249202it [01:35, 3357.12it/s]240864it [01:35, 3274.29it/s]246339it [01:35, 3431.02it/s]249564it [01:35, 3431.64it/s]241228it [01:35, 3377.76it/s]246684it [01:35, 3341.58it/s]249909it [01:35, 3334.00it/s]241568it [01:35, 3286.28it/s]247050it [01:35, 3430.97it/s]250280it [01:35, 3441.52it/s]241927it [01:35, 3365.25it/s]247395it [01:35, 3334.57it/s]250654it [01:35, 3527.07it/s]242293it [01:35, 3448.92it/s]247761it [01:36, 3426.47it/s]251008it [01:36, 3388.59it/s]242640it [01:36, 3323.63it/s]248113it [01:36, 3452.64it/s]251375it [01:36, 3468.29it/s]243004it [01:36, 3414.24it/s]248460it [01:36, 3340.23it/s]251724it [01:36, 3357.73it/s]243348it [01:36, 3308.83it/s]248812it [01:36, 3389.80it/s]252092it [01:36, 3449.45it/s]243715it [01:36, 3409.87it/s]249153it [01:36, 3304.51it/s]252439it [01:36, 3339.67it/s]244058it [01:36, 3270.29it/s]249518it [01:36, 3402.18it/s]252804it [01:36, 3427.64it/s]244421it [01:36, 3370.79it/s]249882it [01:36, 3471.33it/s]253173it [01:36, 3500.98it/s]244785it [01:36, 3447.51it/s]250231it [01:36, 3367.38it/s]253525it [01:36, 3381.62it/s]245132it [01:36, 3322.10it/s]250604it [01:36, 3470.87it/s]253893it [01:36, 3466.80it/s]245496it [01:36, 3412.32it/s]250953it [01:36, 3368.86it/s]254242it [01:37, 3348.65it/s]245839it [01:36, 3292.22it/s]251318it [01:37, 3448.03it/s]254605it [01:37, 3428.93it/s]246192it [01:37, 3359.82it/s]251665it [01:37, 3338.38it/s]254950it [01:37, 3327.46it/s]246547it [01:37, 3259.15it/s]252032it [01:37, 3432.69it/s]255314it [01:37, 3414.85it/s]246913it [01:37, 3370.73it/s]252384it [01:37, 3457.42it/s]255657it [01:37, 3307.17it/s]247279it [01:37, 3452.49it/s]252731it [01:37, 3339.84it/s]247626it [01:37, 3328.02it/s]253096it [01:37, 3427.22it/s]247982it [01:37, 3394.16it/s]253441it [01:37, 3326.42it/s]248324it [01:37, 3285.87it/s]253809it [01:37, 3426.32it/s]248690it [01:37, 3390.20it/s]254154it [01:37, 3315.14it/s]249060it [01:37, 3477.26it/s]254520it [01:38, 3413.25it/s]249410it [01:38, 3350.69it/s]254881it [01:38, 3468.16it/s]249771it [01:38, 3424.59it/s]255230it [01:38, 3344.80it/s]250116it [01:38, 3293.68it/s]255599it [01:38, 3443.10it/s]250482it [01:38, 3397.70it/s]250824it [01:38, 3301.07it/s]251192it [01:38, 3408.81it/s]251556it [01:38, 3473.51it/s]251905it [01:38, 3329.84it/s]252266it [01:38, 3407.42it/s]252609it [01:38, 3292.23it/s]252978it [01:39, 3405.45it/s]253321it [01:39, 3298.55it/s]253687it [01:39, 3399.73it/s]254054it [01:39, 3477.67it/s]254404it [01:39, 3315.90it/s]254771it [01:39, 3415.86it/s]255115it [01:39, 3309.16it/s]255481it [01:39, 3408.26it/s]255824it [01:39, 3307.64it/s]255947it [01:43, 158.97it/s] 256317it [01:43, 225.90it/s]256675it [01:43, 314.08it/s]256981it [01:43, 413.45it/s]257345it [01:43, 572.07it/s]257668it [01:43, 744.08it/s]258036it [01:44, 994.88it/s]258375it [01:44, 1242.99it/s]258744it [01:44, 1569.05it/s]259105it [01:44, 1894.76it/s]259451it [01:44, 2137.46it/s]259819it [01:44, 2455.41it/s]260164it [01:44, 2619.90it/s]260530it [01:44, 2869.47it/s]260895it [01:44, 2946.09it/s]261256it [01:45, 3117.78it/s]261626it [01:45, 3273.19it/s]261977it [01:45, 3239.50it/s]262346it [01:45, 3362.52it/s]262695it [01:45, 3286.57it/s]263064it [01:45, 3399.56it/s]263416it [01:45, 3301.87it/s]263776it [01:45, 3385.64it/s]264149it [01:45, 3482.96it/s]264501it [01:45, 3378.50it/s]264870it [01:46, 3466.04it/s]265219it [01:46, 3367.46it/s]265586it [01:46, 3454.08it/s]265936it [01:46, 3324.49it/s]255990it [01:44, 168.40it/s] 266306it [01:46, 3428.87it/s]256362it [01:44, 240.71it/s]266676it [01:46, 3507.18it/s]256696it [01:44, 327.56it/s]267029it [01:46, 3385.28it/s]257069it [01:44, 459.30it/s]267402it [01:46, 3482.36it/s]257432it [01:44, 625.49it/s]257759it [01:44, 806.14it/s]267752it [01:46, 3376.25it/s]258132it [01:44, 1069.68it/s]268125it [01:47, 3475.01it/s]258469it [01:44, 1319.92it/s]268475it [01:47, 3343.22it/s]258843it [01:44, 1655.85it/s]268846it [01:47, 3446.24it/s]259215it [01:45, 1997.42it/s]269214it [01:47, 3512.08it/s]259567it [01:45, 2215.93it/s]269567it [01:47, 3385.97it/s]259940it [01:45, 2531.62it/s]269938it [01:47, 3477.20it/s]260287it [01:45, 2687.79it/s]270288it [01:47, 3363.23it/s]255945it [01:45, 162.63it/s] 260647it [01:45, 2908.32it/s]270644it [01:47, 3416.91it/s]256316it [01:45, 230.80it/s]260992it [01:45, 2978.48it/s]270988it [01:47, 3313.34it/s]256687it [01:45, 323.77it/s]261366it [01:45, 3179.19it/s]271356it [01:47, 3417.47it/s]256996it [01:45, 425.44it/s]271722it [01:48, 3485.69it/s]261737it [01:45, 3180.16it/s]257345it [01:45, 578.30it/s]262108it [01:45, 3322.97it/s]272072it [01:48, 3368.63it/s]257666it [01:45, 751.48it/s]262479it [01:45, 3429.14it/s]272435it [01:48, 3443.52it/s]258034it [01:45, 1005.08it/s]262834it [01:46, 3349.08it/s]272781it [01:48, 3298.98it/s]258374it [01:46, 1256.73it/s]263206it [01:46, 3451.72it/s]273147it [01:48, 3399.54it/s]258745it [01:46, 1587.21it/s]263558it [01:46, 3348.97it/s]273496it [01:48, 3305.30it/s]259119it [01:46, 1934.66it/s]263933it [01:46, 3461.96it/s]273865it [01:48, 3413.62it/s]259469it [01:46, 2181.70it/s]264284it [01:46, 3379.97it/s]274232it [01:48, 3485.45it/s]259842it [01:46, 2502.13it/s]264657it [01:46, 3478.92it/s]274583it [01:48, 3363.24it/s]260192it [01:46, 2669.53it/s]265012it [01:46, 3499.11it/s]274948it [01:49, 3444.32it/s]260558it [01:46, 2908.32it/s]265364it [01:46, 3403.96it/s]275295it [01:49, 3316.33it/s]260906it [01:46, 2951.33it/s]265737it [01:46, 3498.04it/s]275662it [01:49, 3415.70it/s]261282it [01:46, 3161.70it/s]266089it [01:47, 3398.31it/s]261657it [01:47, 3319.84it/s]276016it [01:49, 3319.19it/s]266463it [01:47, 3495.58it/s]276382it [01:49, 3413.82it/s]262013it [01:47, 3281.41it/s]266815it [01:47, 3401.27it/s]276745it [01:49, 3474.22it/s]262380it [01:47, 3389.48it/s]267190it [01:47, 3499.66it/s]277094it [01:49, 3367.78it/s]262732it [01:47, 3325.13it/s]267563it [01:47, 3563.89it/s]277447it [01:49, 3412.67it/s]263105it [01:47, 3437.36it/s]267921it [01:47, 3446.58it/s]277790it [01:49, 3319.11it/s]263456it [01:47, 3342.47it/s]268297it [01:47, 3535.83it/s]278156it [01:49, 3414.89it/s]263832it [01:47, 3460.04it/s]256157it [01:47, 146.47it/s] 268652it [01:47, 3431.25it/s]278523it [01:50, 3488.04it/s]264204it [01:47, 3535.05it/s]256525it [01:47, 209.15it/s]269020it [01:47, 3501.22it/s]278873it [01:50, 3365.29it/s]264561it [01:47, 3384.10it/s]256828it [01:47, 278.86it/s]269372it [01:47, 3394.91it/s]279236it [01:50, 3439.95it/s]264933it [01:47, 3478.70it/s]257190it [01:47, 392.77it/s]269746it [01:48, 3492.11it/s]279582it [01:50, 3341.99it/s]265284it [01:48, 3387.45it/s]257535it [01:48, 531.48it/s]270109it [01:48, 3531.04it/s]279933it [01:50, 3388.63it/s]265657it [01:48, 3485.09it/s]257897it [01:48, 722.49it/s]270464it [01:48, 3414.93it/s]280274it [01:50, 3287.61it/s]258247it [01:48, 947.90it/s]266008it [01:48, 3387.81it/s]270836it [01:48, 3500.29it/s]280637it [01:50, 3384.94it/s]266382it [01:48, 3487.89it/s]258578it [01:48, 1184.15it/s]271188it [01:48, 3378.72it/s]281004it [01:50, 3467.55it/s]266756it [01:48, 3560.73it/s]258945it [01:48, 1503.33it/s]271558it [01:48, 3468.68it/s]281352it [01:50, 3349.54it/s]267114it [01:48, 3443.87it/s]259282it [01:48, 1767.40it/s]271907it [01:48, 3376.75it/s]281715it [01:51, 3428.10it/s]267486it [01:48, 3521.28it/s]259646it [01:48, 2102.84it/s]272276it [01:48, 3465.51it/s]260015it [01:48, 2426.00it/s]282060it [01:51, 3294.56it/s]267840it [01:48, 3416.79it/s]272644it [01:48, 3525.31it/s]282424it [01:51, 3390.73it/s]268200it [01:48, 3468.34it/s]260363it [01:48, 2587.45it/s]272998it [01:49, 3407.01it/s]260712it [01:48, 2801.68it/s]282765it [01:51, 3287.34it/s]268549it [01:49, 3379.31it/s]273362it [01:49, 3473.76it/s]283128it [01:51, 3382.90it/s]268922it [01:49, 3479.19it/s]261052it [01:49, 2881.92it/s]273711it [01:49, 3376.72it/s]283489it [01:51, 3446.57it/s]269292it [01:49, 3542.87it/s]261422it [01:49, 3095.13it/s]274086it [01:49, 3481.68it/s]283835it [01:51, 3332.12it/s]269648it [01:49, 3422.49it/s]261765it [01:49, 3098.13it/s]274436it [01:49, 3372.09it/s]284188it [01:51, 3388.28it/s]270022it [01:49, 3513.76it/s]262133it [01:49, 3255.86it/s]274805it [01:49, 3461.93it/s]262500it [01:49, 3372.04it/s]284529it [01:51, 3291.24it/s]270375it [01:49, 3405.55it/s]275177it [01:49, 3367.82it/s]284894it [01:51, 3393.87it/s]270748it [01:49, 3498.12it/s]262851it [01:49, 3289.93it/s]275534it [01:49, 3423.40it/s]263201it [01:49, 3347.54it/s]271100it [01:49, 3384.78it/s]285256it [01:52, 3293.55it/s]275904it [01:49, 3502.63it/s]271466it [01:49, 3463.34it/s]285621it [01:52, 3394.10it/s]263543it [01:49, 3260.67it/s]276256it [01:49, 3390.36it/s]285987it [01:52, 3468.32it/s]263911it [01:49, 3379.94it/s]271815it [01:49, 3320.71it/s]276625it [01:50, 3474.66it/s]264255it [01:49, 3299.49it/s]286336it [01:52, 3356.48it/s]272188it [01:50, 3435.36it/s]276974it [01:50, 3371.62it/s]264623it [01:50, 3406.52it/s]286693it [01:52, 3415.56it/s]272554it [01:50, 3499.93it/s]277346it [01:50, 3469.70it/s]264991it [01:50, 3483.70it/s]287036it [01:52, 3332.91it/s]272906it [01:50, 3384.74it/s]287112it [01:52, 2548.55it/s]
2022-07-25 13:48:08 | INFO | root | success load 287112 data
2022-07-25 13:48:08 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-25 13:48:08 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-25 13:48:08 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-25 13:48:08 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-25 13:48:08 | INFO | transformer.tokenization_utils | loading file None
2022-07-25 13:48:08 | INFO | transformer.tokenization_utils | loading file None
2022-07-25 13:48:08 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
277697it [01:50, 3358.37it/s]265342it [01:50, 3374.09it/s]273273it [01:50, 3465.12it/s]278067it [01:50, 3455.55it/s]265713it [01:50, 3469.86it/s]273622it [01:50, 3369.17it/s]278437it [01:50, 3523.90it/s]266062it [01:50, 3361.37it/s]273990it [01:50, 3457.19it/s]278791it [01:50, 3409.82it/s]266431it [01:50, 3454.39it/s]274338it [01:50, 3356.96it/s]279163it [01:50, 3496.72it/s]266779it [01:50, 3354.25it/s]274709it [01:50, 3457.00it/s]279515it [01:50, 3381.22it/s]267151it [01:50, 3458.64it/s]275080it [01:50, 3527.95it/s]279880it [01:51, 3455.91it/s]267524it [01:50, 3535.77it/s]275434it [01:51, 3368.41it/s]280228it [01:51, 3350.44it/s]267879it [01:51, 3412.80it/s]275802it [01:51, 3456.80it/s]280584it [01:51, 3408.29it/s]268252it [01:51, 3501.98it/s]276150it [01:51, 3358.34it/s]280957it [01:51, 3500.64it/s]268604it [01:51, 3387.88it/s]276516it [01:51, 3442.76it/s]281309it [01:51, 3383.28it/s]268969it [01:51, 3443.73it/s]276862it [01:51, 3345.52it/s]281676it [01:51, 3463.75it/s]269315it [01:51, 3337.56it/s]277234it [01:51, 3452.48it/s]282024it [01:51, 3360.01it/s]269687it [01:51, 3444.95it/s]277601it [01:51, 3513.61it/s]282389it [01:51, 3442.22it/s]270059it [01:51, 3523.12it/s]277954it [01:51, 3399.47it/s]282737it [01:51, 3340.07it/s]270413it [01:51, 3404.76it/s]278319it [01:51, 3470.74it/s]283097it [01:51, 3412.22it/s]270769it [01:51, 3446.92it/s]278668it [01:51, 3370.10it/s]283463it [01:52, 3481.60it/s]271115it [01:51, 3342.22it/s]279023it [01:52, 3419.56it/s]283813it [01:52, 3370.72it/s]271480it [01:52, 3429.72it/s]279375it [01:52, 3317.34it/s]284182it [01:52, 3462.03it/s]271825it [01:52, 3315.87it/s]279744it [01:52, 3423.15it/s]284530it [01:52, 3354.89it/s]272195it [01:52, 3423.72it/s]280110it [01:52, 3489.46it/s]284899it [01:52, 3449.29it/s]272539it [01:52, 3329.95it/s]280461it [01:52, 3371.26it/s]285257it [01:52, 3337.68it/s]280830it [01:52, 3460.53it/s]272874it [01:52, 3170.22it/s]285620it [01:52, 3397.85it/s]281178it [01:52, 3346.73it/s]273235it [01:52, 3291.36it/s]285990it [01:52, 3484.64it/s]281545it [01:52, 3433.29it/s]273567it [01:52, 3218.91it/s]286340it [01:52, 3374.51it/s]273933it [01:52, 3343.57it/s]281895it [01:52, 3328.05it/s]286710it [01:53, 3465.81it/s]274296it [01:52, 3425.86it/s]282259it [01:53, 3416.42it/s]287059it [01:53, 3378.05it/s]287112it [01:53, 2537.88it/s]
282609it [01:53, 3438.20it/s]274641it [01:53, 3309.52it/s]274976it [01:53, 3319.43it/s]282954it [01:53, 3328.12it/s]283318it [01:53, 3417.08it/s]275310it [01:53, 3258.89it/s]275672it [01:53, 3362.57it/s]283662it [01:53, 3319.48it/s]284027it [01:53, 3413.54it/s]276016it [01:53, 3275.75it/s]284396it [01:53, 3493.97it/s]276381it [01:53, 3382.62it/s]276745it [01:53, 3455.15it/s]284747it [01:53, 3376.23it/s]285115it [01:53, 3453.69it/s]277092it [01:53, 3234.00it/s]285462it [01:53, 3363.56it/s]277456it [01:53, 3346.35it/s]285827it [01:54, 3444.58it/s]277794it [01:54, 3253.23it/s]286173it [01:54, 3308.10it/s]278158it [01:54, 3360.88it/s]286544it [01:54, 3420.75it/s]278525it [01:54, 3448.99it/s]286916it [01:54, 3506.88it/s]278872it [01:54, 3329.12it/s]287112it [01:54, 2508.45it/s]
279236it [01:54, 3411.44it/s]279579it [01:54, 3278.99it/s]279940it [01:54, 3371.44it/s]280280it [01:54, 3276.55it/s]280645it [01:54, 3381.57it/s]281013it [01:54, 3467.90it/s]281362it [01:55, 3301.95it/s]281725it [01:55, 3394.37it/s]282067it [01:55, 3289.29it/s]282431it [01:55, 3388.03it/s]282772it [01:55, 3277.30it/s]283134it [01:55, 3372.75it/s]283479it [01:55, 3394.27it/s]283820it [01:55, 3286.41it/s]284186it [01:55, 3391.66it/s]284527it [01:56, 3285.38it/s]284893it [01:56, 3392.04it/s]285256it [01:56, 3286.09it/s]285606it [01:56, 3344.53it/s]285973it [01:56, 3437.12it/s]286319it [01:56, 3321.51it/s]286685it [01:56, 3418.29it/s]287029it [01:56, 3324.31it/s]287112it [01:56, 2458.70it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-25 13:53:11 | INFO | train_inner | epoch 001:    100 / 1122 loss=nan, nll_loss=12.087, mask_ins=7.638, word_ins_ml=12.576, word_reposition=5.202, kpe=nan, ppl=nan, wps=7012.7, ups=0.34, wpb=20527, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=22.821, clip=18, loss_scale=128, train_wall=262, wall=416
2022-07-25 13:58:01 | INFO | train_inner | epoch 001:    200 / 1122 loss=22.518, nll_loss=10.978, mask_ins=5.198, word_ins_ml=11.589, word_reposition=4.312, kpe=1.419, ppl=6.00473e+06, wps=7087.2, ups=0.34, wpb=20583.2, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=16.348, clip=0, loss_scale=128, train_wall=252, wall=706
2022-07-25 14:02:54 | INFO | train_inner | epoch 001:    300 / 1122 loss=19.047, nll_loss=10.79, mask_ins=3.076, word_ins_ml=11.413, word_reposition=3.308, kpe=1.25, ppl=541820, wps=7035.7, ups=0.34, wpb=20561.3, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=5.984, clip=0, loss_scale=128, train_wall=253, wall=999
2022-07-25 14:07:43 | INFO | train_inner | epoch 001:    400 / 1122 loss=17.504, nll_loss=10.548, mask_ins=2.668, word_ins_ml=11.199, word_reposition=2.444, kpe=1.193, ppl=185887, wps=7122.3, ups=0.35, wpb=20576.5, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=3.861, clip=0, loss_scale=128, train_wall=250, wall=1288
2022-07-25 14:12:33 | INFO | train_inner | epoch 001:    500 / 1122 loss=16.997, nll_loss=10.336, mask_ins=2.565, word_ins_ml=11.018, word_reposition=2.263, kpe=1.152, ppl=130796, wps=7075.1, ups=0.34, wpb=20523.5, bsz=256, num_updates=500, lr=5.009e-05, gnorm=3.308, clip=0, loss_scale=128, train_wall=251, wall=1578
2022-07-25 14:17:34 | INFO | train_inner | epoch 001:    600 / 1122 loss=16.707, nll_loss=10.15, mask_ins=2.532, word_ins_ml=10.858, word_reposition=2.196, kpe=1.121, ppl=106970, wps=6788.4, ups=0.33, wpb=20491.4, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=3.093, clip=0, loss_scale=242, train_wall=263, wall=1880
2022-07-25 14:22:25 | INFO | train_inner | epoch 001:    700 / 1122 loss=16.471, nll_loss=9.961, mask_ins=2.506, word_ins_ml=10.699, word_reposition=2.169, kpe=1.098, ppl=90840.3, wps=7079.1, ups=0.34, wpb=20542.5, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=2.809, clip=0, loss_scale=256, train_wall=252, wall=2170
2022-07-25 14:27:14 | INFO | train_inner | epoch 001:    800 / 1122 loss=16.284, nll_loss=9.728, mask_ins=2.49, word_ins_ml=10.501, word_reposition=2.21, kpe=1.083, ppl=79805.2, wps=7111.8, ups=0.35, wpb=20579, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=2.695, clip=0, loss_scale=256, train_wall=251, wall=2459
2022-07-25 14:32:03 | INFO | train_inner | epoch 001:    900 / 1122 loss=16.086, nll_loss=9.489, mask_ins=2.48, word_ins_ml=10.297, word_reposition=2.248, kpe=1.061, ppl=69577.4, wps=7085.8, ups=0.35, wpb=20464, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=2.612, clip=0, loss_scale=256, train_wall=250, wall=2748
2022-07-25 14:36:52 | INFO | train_inner | epoch 001:   1000 / 1122 loss=15.942, nll_loss=9.302, mask_ins=2.474, word_ins_ml=10.137, word_reposition=2.276, kpe=1.055, ppl=62941.3, wps=7115.9, ups=0.35, wpb=20597.8, bsz=256, num_updates=1000, lr=0.00010008, gnorm=2.383, clip=0, loss_scale=256, train_wall=251, wall=3037
2022-07-25 14:41:42 | INFO | train_inner | epoch 001:   1100 / 1122 loss=15.752, nll_loss=9.159, mask_ins=2.46, word_ins_ml=10.014, word_reposition=2.238, kpe=1.04, ppl=55172.5, wps=7068.6, ups=0.35, wpb=20473.7, bsz=256, num_updates=1100, lr=0.000110078, gnorm=2.352, clip=0, loss_scale=453, train_wall=251, wall=3327
2022-07-25 14:42:44 | INFO | train | epoch 001 | loss nan | nll_loss 10.208 | mask_ins 3.265 | word_ins_ml 10.917 | word_reposition 2.796 | kpe nan | ppl nan | wps 7049 | ups 0.34 | wpb 20520.3 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 6.136 | clip 1.6 | loss_scale 220 | train_wall 2842 | wall 3390
2022-07-25 14:44:05 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.032 | nll_loss 9.657 | mask_ins 2.684 | word_ins_ml 10.5 | word_reposition 2.462 | kpe 1.385 | ppl 133973 | wps 12299.9 | wpb 2367.6 | bsz 32 | num_updates 1122
2022-07-25 14:44:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_best.pt (epoch 1 @ 1122 updates, score 17.032) (writing took 3.362299778498709 seconds)
2022-07-25 14:47:54 | INFO | train_inner | epoch 002:     78 / 1122 loss=15.637, nll_loss=9.013, mask_ins=2.448, word_ins_ml=9.889, word_reposition=2.269, kpe=1.031, ppl=50969.3, wps=5469.4, ups=0.27, wpb=20333.3, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=2.334, clip=0, loss_scale=512, train_wall=250, wall=3699
2022-07-25 14:52:42 | INFO | train_inner | epoch 002:    178 / 1122 loss=15.481, nll_loss=8.885, mask_ins=2.441, word_ins_ml=9.778, word_reposition=2.241, kpe=1.021, ppl=45735.6, wps=7133.7, ups=0.35, wpb=20587.3, bsz=256, num_updates=1300, lr=0.000130074, gnorm=2.156, clip=0, loss_scale=512, train_wall=250, wall=3987
2022-07-25 14:57:31 | INFO | train_inner | epoch 002:    278 / 1122 loss=15.328, nll_loss=8.75, mask_ins=2.42, word_ins_ml=9.661, word_reposition=2.235, kpe=1.011, ppl=41120.9, wps=7125.2, ups=0.35, wpb=20599.8, bsz=256, num_updates=1400, lr=0.000140072, gnorm=2.18, clip=0, loss_scale=512, train_wall=250, wall=4277
2022-07-25 15:02:21 | INFO | train_inner | epoch 002:    378 / 1122 loss=15.196, nll_loss=8.611, mask_ins=2.423, word_ins_ml=9.54, word_reposition=2.227, kpe=1.007, ppl=37538.6, wps=7032.3, ups=0.35, wpb=20347.3, bsz=256, num_updates=1500, lr=0.00015007, gnorm=2.13, clip=0, loss_scale=512, train_wall=251, wall=4566
2022-07-25 15:07:10 | INFO | train_inner | epoch 002:    478 / 1122 loss=15.048, nll_loss=8.461, mask_ins=2.412, word_ins_ml=9.408, word_reposition=2.229, kpe=0.999, ppl=33883, wps=7105.4, ups=0.35, wpb=20567.7, bsz=256, num_updates=1600, lr=0.000160068, gnorm=2.067, clip=0, loss_scale=845, train_wall=251, wall=4855
2022-07-25 15:11:59 | INFO | train_inner | epoch 002:    578 / 1122 loss=14.892, nll_loss=8.279, mask_ins=2.409, word_ins_ml=9.25, word_reposition=2.23, kpe=1.003, ppl=30406.2, wps=7111.2, ups=0.35, wpb=20536.9, bsz=256, num_updates=1700, lr=0.000170066, gnorm=2.116, clip=0, loss_scale=1024, train_wall=251, wall=5144
2022-07-25 15:16:48 | INFO | train_inner | epoch 002:    678 / 1122 loss=14.695, nll_loss=8.044, mask_ins=2.415, word_ins_ml=9.044, word_reposition=2.235, kpe=1.002, ppl=26529.9, wps=7090.7, ups=0.35, wpb=20477.4, bsz=256, num_updates=1800, lr=0.000180064, gnorm=2.182, clip=0, loss_scale=1024, train_wall=251, wall=5433
2022-07-25 15:21:59 | INFO | train_inner | epoch 002:    778 / 1122 loss=nan, nll_loss=7.815, mask_ins=2.405, word_ins_ml=8.846, word_reposition=2.232, kpe=nan, ppl=nan, wps=6613, ups=0.32, wpb=20576, bsz=256, num_updates=1900, lr=0.000190062, gnorm=2.318, clip=0, loss_scale=1024, train_wall=273, wall=5744
2022-07-25 15:26:50 | INFO | train_inner | epoch 002:    878 / 1122 loss=14.227, nll_loss=7.556, mask_ins=2.401, word_ins_ml=8.621, word_reposition=2.202, kpe=1.003, ppl=19174.1, wps=7028.2, ups=0.34, wpb=20447.7, bsz=256, num_updates=2000, lr=0.00020006, gnorm=2.458, clip=0, loss_scale=1024, train_wall=253, wall=6035
2022-07-25 15:31:38 | INFO | train_inner | epoch 002:    978 / 1122 loss=13.985, nll_loss=7.309, mask_ins=2.393, word_ins_ml=8.407, word_reposition=2.185, kpe=1, ppl=16213.5, wps=7111.9, ups=0.35, wpb=20513.5, bsz=256, num_updates=2100, lr=0.000210058, gnorm=2.504, clip=0, loss_scale=1567, train_wall=250, wall=6323
2022-07-25 15:36:28 | INFO | train_inner | epoch 002:   1078 / 1122 loss=nan, nll_loss=7.057, mask_ins=2.398, word_ins_ml=8.19, word_reposition=2.179, kpe=nan, ppl=nan, wps=7158.6, ups=0.35, wpb=20708.1, bsz=256, num_updates=2200, lr=0.000220056, gnorm=2.424, clip=0, loss_scale=2048, train_wall=251, wall=6613
2022-07-25 15:38:34 | INFO | train | epoch 002 | loss nan | nll_loss 8.095 | mask_ins 2.414 | word_ins_ml 9.091 | word_reposition 2.22 | kpe nan | ppl nan | wps 6873.9 | ups 0.33 | wpb 20521 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 2.269 | clip 0 | loss_scale 1015 | train_wall 2835 | wall 6739
2022-07-25 15:39:55 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 16.361 | nll_loss 8.795 | mask_ins 2.649 | word_ins_ml 9.836 | word_reposition 2.559 | kpe 1.316 | ppl 84174.6 | wps 12297.9 | wpb 2367.6 | bsz 32 | num_updates 2244 | best_loss 16.361
2022-07-25 15:40:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_best.pt (epoch 2 @ 2244 updates, score 16.361) (writing took 12.069725005887449 seconds)
2022-07-25 15:42:48 | INFO | train_inner | epoch 003:     56 / 1122 loss=nan, nll_loss=6.87, mask_ins=2.397, word_ins_ml=8.028, word_reposition=2.152, kpe=nan, ppl=nan, wps=5358.2, ups=0.26, wpb=20387.7, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=2.526, clip=0, loss_scale=2048, train_wall=250, wall=6993
2022-07-25 15:47:37 | INFO | train_inner | epoch 003:    156 / 1122 loss=13.31, nll_loss=6.624, mask_ins=2.389, word_ins_ml=7.814, word_reposition=2.119, kpe=0.989, ppl=10152.6, wps=7088, ups=0.35, wpb=20466.9, bsz=256, num_updates=2400, lr=0.000240052, gnorm=2.36, clip=0, loss_scale=2048, train_wall=251, wall=7282
2022-07-25 15:52:26 | INFO | train_inner | epoch 003:    256 / 1122 loss=13.172, nll_loss=6.478, mask_ins=2.384, word_ins_ml=7.689, word_reposition=2.108, kpe=0.992, ppl=9230.51, wps=7112.1, ups=0.35, wpb=20590.4, bsz=256, num_updates=2500, lr=0.00025005, gnorm=2.381, clip=0, loss_scale=2048, train_wall=251, wall=7572
2022-07-25 15:57:15 | INFO | train_inner | epoch 003:    356 / 1122 loss=13.002, nll_loss=6.283, mask_ins=2.399, word_ins_ml=7.519, word_reposition=2.097, kpe=0.986, ppl=8201.94, wps=7122.7, ups=0.35, wpb=20552.9, bsz=256, num_updates=2600, lr=0.000260048, gnorm=2.372, clip=0, loss_scale=2888, train_wall=250, wall=7860
2022-07-25 16:02:02 | INFO | train_inner | epoch 003:    456 / 1122 loss=12.814, nll_loss=6.107, mask_ins=2.387, word_ins_ml=7.365, word_reposition=2.072, kpe=0.99, ppl=7202.7, wps=7091.1, ups=0.35, wpb=20384, bsz=256, num_updates=2700, lr=0.000270046, gnorm=2.346, clip=0, loss_scale=4096, train_wall=249, wall=8148
2022-07-25 16:06:52 | INFO | train_inner | epoch 003:    556 / 1122 loss=12.593, nll_loss=5.912, mask_ins=2.373, word_ins_ml=7.195, word_reposition=2.039, kpe=0.987, ppl=6179.69, wps=7084.5, ups=0.35, wpb=20480.9, bsz=256, num_updates=2800, lr=0.000280044, gnorm=2.349, clip=0, loss_scale=4096, train_wall=251, wall=8437
2022-07-25 16:11:41 | INFO | train_inner | epoch 003:    656 / 1122 loss=12.35, nll_loss=5.657, mask_ins=2.372, word_ins_ml=6.971, word_reposition=2.016, kpe=0.991, ppl=5222.31, wps=7120.3, ups=0.35, wpb=20612.3, bsz=256, num_updates=2900, lr=0.000290042, gnorm=2.431, clip=0, loss_scale=4096, train_wall=251, wall=8726
2022-07-25 16:16:30 | INFO | train_inner | epoch 003:    756 / 1122 loss=11.835, nll_loss=5.198, mask_ins=2.346, word_ins_ml=6.569, word_reposition=1.929, kpe=0.992, ppl=3654.51, wps=7134.6, ups=0.35, wpb=20597.8, bsz=256, num_updates=3000, lr=0.00030004, gnorm=2.611, clip=0, loss_scale=4096, train_wall=251, wall=9015
2022-07-25 16:21:19 | INFO | train_inner | epoch 003:    856 / 1122 loss=11.467, nll_loss=4.893, mask_ins=2.29, word_ins_ml=6.3, word_reposition=1.885, kpe=0.993, ppl=2831.5, wps=7133.8, ups=0.35, wpb=20609.8, bsz=256, num_updates=3100, lr=0.000310038, gnorm=2.63, clip=0, loss_scale=5284, train_wall=251, wall=9304
2022-07-25 16:26:31 | INFO | train_inner | epoch 003:    956 / 1122 loss=nan, nll_loss=4.701, mask_ins=2.213, word_ins_ml=6.129, word_reposition=1.856, kpe=nan, ppl=nan, wps=6596.3, ups=0.32, wpb=20572.9, bsz=256, num_updates=3200, lr=0.000320036, gnorm=2.511, clip=0, loss_scale=8192, train_wall=274, wall=9616
2022-07-25 16:31:21 | INFO | train_inner | epoch 003:   1056 / 1122 loss=11.082, nll_loss=4.649, mask_ins=2.173, word_ins_ml=6.082, word_reposition=1.837, kpe=0.989, ppl=2167.08, wps=7067.9, ups=0.34, wpb=20512.4, bsz=256, num_updates=3300, lr=0.000330034, gnorm=2.489, clip=0, loss_scale=8192, train_wall=252, wall=9906
2022-07-25 16:34:30 | INFO | train | epoch 003 | loss nan | nll_loss 5.644 | mask_ins 2.323 | word_ins_ml 6.957 | word_reposition 1.993 | kpe nan | ppl nan | wps 6860.8 | ups 0.33 | wpb 20521.3 | bsz 255.8 | num_updates 3366 | lr 0.000336633 | gnorm 2.455 | clip 0 | loss_scale 4598 | train_wall 2835 | wall 10095
2022-07-25 16:35:50 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.054 | nll_loss 8.085 | mask_ins 2.638 | word_ins_ml 9.23 | word_reposition 2.846 | kpe 1.339 | ppl 68019.9 | wps 12328.2 | wpb 2367.6 | bsz 32 | num_updates 3366 | best_loss 16.054
2022-07-25 16:36:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_best.pt (epoch 3 @ 3366 updates, score 16.054) (writing took 29.812492037191987 seconds)
2022-07-25 16:37:58 | INFO | train_inner | epoch 004:     34 / 1122 loss=10.879, nll_loss=4.515, mask_ins=2.113, word_ins_ml=5.964, word_reposition=1.814, kpe=0.988, ppl=1882.91, wps=5125.7, ups=0.25, wpb=20354.1, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=2.586, clip=0, loss_scale=8192, train_wall=249, wall=10303
2022-07-25 16:42:47 | INFO | train_inner | epoch 004:    134 / 1122 loss=10.77, nll_loss=4.445, mask_ins=2.084, word_ins_ml=5.901, word_reposition=1.805, kpe=0.98, ppl=1746.57, wps=7078.8, ups=0.35, wpb=20472.6, bsz=256, num_updates=3500, lr=0.00035003, gnorm=2.307, clip=0, loss_scale=8192, train_wall=250, wall=10592
2022-07-25 16:43:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-25 16:47:41 | INFO | train_inner | epoch 004:    235 / 1122 loss=10.647, nll_loss=4.366, mask_ins=2.055, word_ins_ml=5.829, word_reposition=1.781, kpe=0.982, ppl=1603.72, wps=7006.4, ups=0.34, wpb=20613.2, bsz=256, num_updates=3600, lr=0.000360028, gnorm=2.353, clip=0, loss_scale=4583, train_wall=254, wall=10886
2022-07-25 16:52:40 | INFO | train_inner | epoch 004:    335 / 1122 loss=10.583, nll_loss=4.346, mask_ins=2.033, word_ins_ml=5.811, word_reposition=1.759, kpe=0.98, ppl=1534.13, wps=6832.1, ups=0.33, wpb=20432.2, bsz=256, num_updates=3700, lr=0.000370026, gnorm=2.249, clip=0, loss_scale=4096, train_wall=255, wall=11186
2022-07-25 16:57:42 | INFO | train_inner | epoch 004:    435 / 1122 loss=10.458, nll_loss=4.24, mask_ins=2.009, word_ins_ml=5.717, word_reposition=1.757, kpe=0.976, ppl=1406.94, wps=6842.2, ups=0.33, wpb=20656.5, bsz=256, num_updates=3800, lr=0.000380024, gnorm=2.267, clip=0, loss_scale=4096, train_wall=256, wall=11487
2022-07-25 17:02:44 | INFO | train_inner | epoch 004:    535 / 1122 loss=10.449, nll_loss=4.226, mask_ins=1.995, word_ins_ml=5.703, word_reposition=1.772, kpe=0.979, ppl=1398.05, wps=6797.1, ups=0.33, wpb=20481.9, bsz=256, num_updates=3900, lr=0.000390022, gnorm=2.213, clip=0, loss_scale=4096, train_wall=257, wall=11789
2022-07-25 17:07:44 | INFO | train_inner | epoch 004:    635 / 1122 loss=nan, nll_loss=4.223, mask_ins=1.984, word_ins_ml=5.7, word_reposition=1.737, kpe=nan, ppl=nan, wps=6828.7, ups=0.33, wpb=20519.3, bsz=256, num_updates=4000, lr=0.00040002, gnorm=2.184, clip=0, loss_scale=4096, train_wall=256, wall=12089
2022-07-25 17:12:45 | INFO | train_inner | epoch 004:    735 / 1122 loss=10.36, nll_loss=4.184, mask_ins=1.975, word_ins_ml=5.664, word_reposition=1.74, kpe=0.981, ppl=1314.48, wps=6850.8, ups=0.33, wpb=20609.6, bsz=256, num_updates=4100, lr=0.000410018, gnorm=2.158, clip=0, loss_scale=7250, train_wall=257, wall=12390
2022-07-25 17:17:45 | INFO | train_inner | epoch 004:    835 / 1122 loss=nan, nll_loss=4.142, mask_ins=1.958, word_ins_ml=5.625, word_reposition=1.707, kpe=nan, ppl=nan, wps=6804.2, ups=0.33, wpb=20446.8, bsz=256, num_updates=4200, lr=0.000420016, gnorm=2.072, clip=0, loss_scale=8192, train_wall=257, wall=12691
2022-07-25 17:22:47 | INFO | train_inner | epoch 004:    935 / 1122 loss=10.286, nll_loss=4.128, mask_ins=1.953, word_ins_ml=5.612, word_reposition=1.744, kpe=0.978, ppl=1248.32, wps=6839.6, ups=0.33, wpb=20633.8, bsz=256, num_updates=4300, lr=0.000430014, gnorm=2.08, clip=0, loss_scale=8192, train_wall=257, wall=12992
2022-07-25 17:27:48 | INFO | train_inner | epoch 004:   1035 / 1122 loss=10.217, nll_loss=4.12, mask_ins=1.93, word_ins_ml=5.604, word_reposition=1.71, kpe=0.974, ppl=1190.4, wps=6795.2, ups=0.33, wpb=20465.7, bsz=256, num_updates=4400, lr=0.000440012, gnorm=2.068, clip=0, loss_scale=8192, train_wall=257, wall=13293
2022-07-25 17:32:48 | INFO | train | epoch 004 | loss nan | nll_loss 4.238 | mask_ins 1.996 | word_ins_ml 5.713 | word_reposition 1.75 | kpe nan | ppl nan | wps 6575.8 | ups 0.32 | wpb 20518.2 | bsz 255.8 | num_updates 4487 | lr 0.00044871 | gnorm 2.198 | clip 0 | loss_scale 6323 | train_wall 2900 | wall 13593
2022-07-25 17:34:14 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 17.14 | nll_loss 8.071 | mask_ins 3.213 | word_ins_ml 9.237 | word_reposition 3.233 | kpe 1.457 | ppl 144456 | wps 11488 | wpb 2367.6 | bsz 32 | num_updates 4487 | best_loss 16.054
2022-07-25 17:34:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_last.pt (epoch 4 @ 4487 updates, score 17.14) (writing took 11.240085436031222 seconds)
2022-07-25 17:35:04 | INFO | train_inner | epoch 005:     13 / 1122 loss=10.222, nll_loss=4.089, mask_ins=1.942, word_ins_ml=5.576, word_reposition=1.721, kpe=0.983, ppl=1194.62, wps=4672.5, ups=0.23, wpb=20374.8, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=2.171, clip=0, loss_scale=8192, train_wall=292, wall=13730
2022-07-25 17:40:05 | INFO | train_inner | epoch 005:    113 / 1122 loss=10.112, nll_loss=4.036, mask_ins=1.92, word_ins_ml=5.529, word_reposition=1.689, kpe=0.974, ppl=1107.03, wps=6855.3, ups=0.33, wpb=20631.6, bsz=256, num_updates=4600, lr=0.000460008, gnorm=2.012, clip=0, loss_scale=13517, train_wall=257, wall=14031
2022-07-25 17:45:11 | INFO | train_inner | epoch 005:    213 / 1122 loss=10.04, nll_loss=3.982, mask_ins=1.91, word_ins_ml=5.48, word_reposition=1.675, kpe=0.974, ppl=1052.97, wps=6722.8, ups=0.33, wpb=20520.3, bsz=256, num_updates=4700, lr=0.000470006, gnorm=2.043, clip=0, loss_scale=16384, train_wall=259, wall=14336
2022-07-25 17:46:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-25 17:50:15 | INFO | train_inner | epoch 005:    314 / 1122 loss=10.076, nll_loss=4.005, mask_ins=1.908, word_ins_ml=5.499, word_reposition=1.694, kpe=0.974, ppl=1079.18, wps=6771, ups=0.33, wpb=20578.5, bsz=256, num_updates=4800, lr=0.000480004, gnorm=2.012, clip=0, loss_scale=9976, train_wall=260, wall=14640
2022-07-25 17:55:16 | INFO | train_inner | epoch 005:    414 / 1122 loss=nan, nll_loss=4.026, mask_ins=1.912, word_ins_ml=5.518, word_reposition=1.687, kpe=nan, ppl=nan, wps=6809.9, ups=0.33, wpb=20499.2, bsz=256, num_updates=4900, lr=0.000490002, gnorm=2.019, clip=0, loss_scale=8192, train_wall=257, wall=14941
2022-07-25 17:57:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-25 17:57:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-25 18:00:24 | INFO | train_inner | epoch 005:    516 / 1122 loss=10.03, nll_loss=3.981, mask_ins=1.898, word_ins_ml=5.477, word_reposition=1.68, kpe=0.974, ppl=1045.17, wps=6614.4, ups=0.32, wpb=20367.7, bsz=256, num_updates=5000, lr=0.0005, gnorm=2.008, clip=0, loss_scale=4739, train_wall=262, wall=15249
2022-07-25 18:05:25 | INFO | train_inner | epoch 005:    616 / 1122 loss=10.052, nll_loss=4.016, mask_ins=1.881, word_ins_ml=5.507, word_reposition=1.687, kpe=0.978, ppl=1061.45, wps=6802.3, ups=0.33, wpb=20492.1, bsz=256, num_updates=5100, lr=0.000495074, gnorm=1.963, clip=0, loss_scale=2048, train_wall=257, wall=15550
2022-07-25 18:10:27 | INFO | train_inner | epoch 005:    716 / 1122 loss=10.007, nll_loss=3.938, mask_ins=1.892, word_ins_ml=5.437, word_reposition=1.704, kpe=0.973, ppl=1028.82, wps=6799.5, ups=0.33, wpb=20528.7, bsz=256, num_updates=5200, lr=0.00049029, gnorm=1.92, clip=0, loss_scale=2048, train_wall=257, wall=15852
2022-07-25 18:15:27 | INFO | train_inner | epoch 005:    816 / 1122 loss=9.963, nll_loss=3.938, mask_ins=1.877, word_ins_ml=5.437, word_reposition=1.678, kpe=0.971, ppl=998.22, wps=6881.4, ups=0.33, wpb=20655.4, bsz=256, num_updates=5300, lr=0.000485643, gnorm=1.877, clip=0, loss_scale=2048, train_wall=257, wall=16152
2022-07-25 18:20:28 | INFO | train_inner | epoch 005:    916 / 1122 loss=9.931, nll_loss=3.901, mask_ins=1.889, word_ins_ml=5.402, word_reposition=1.669, kpe=0.972, ppl=976.51, wps=6826.2, ups=0.33, wpb=20560, bsz=256, num_updates=5400, lr=0.000481125, gnorm=1.886, clip=0, loss_scale=2048, train_wall=256, wall=16453
2022-07-25 18:25:29 | INFO | train_inner | epoch 005:   1016 / 1122 loss=nan, nll_loss=3.875, mask_ins=1.861, word_ins_ml=5.379, word_reposition=1.673, kpe=nan, ppl=nan, wps=6796.7, ups=0.33, wpb=20463.8, bsz=256, num_updates=5500, lr=0.000476731, gnorm=1.788, clip=0, loss_scale=2826, train_wall=257, wall=16754
2022-07-25 18:30:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-25 18:30:33 | INFO | train_inner | epoch 005:   1117 / 1122 loss=9.885, nll_loss=3.878, mask_ins=1.87, word_ins_ml=5.381, word_reposition=1.668, kpe=0.966, ppl=945.87, wps=6774, ups=0.33, wpb=20588, bsz=256, num_updates=5600, lr=0.000472456, gnorm=1.854, clip=0, loss_scale=3893, train_wall=259, wall=17058
2022-07-25 18:30:47 | INFO | train | epoch 005 | loss nan | nll_loss 3.964 | mask_ins 1.894 | word_ins_ml 5.461 | word_reposition 1.683 | kpe nan | ppl nan | wps 6594.3 | ups 0.32 | wpb 20521.7 | bsz 255.8 | num_updates 5605 | lr 0.000472245 | gnorm 1.959 | clip 0 | loss_scale 6160 | train_wall 2884 | wall 17072
2022-07-25 18:32:13 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 15.773 | nll_loss 7.61 | mask_ins 2.663 | word_ins_ml 8.789 | word_reposition 2.886 | kpe 1.436 | ppl 55984.8 | wps 11592.8 | wpb 2367.6 | bsz 32 | num_updates 5605 | best_loss 15.773
2022-07-25 18:32:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_best.pt (epoch 5 @ 5605 updates, score 15.773) (writing took 19.65107957739383 seconds)
2022-07-25 18:37:18 | INFO | train_inner | epoch 006:     95 / 1122 loss=9.791, nll_loss=3.818, mask_ins=1.856, word_ins_ml=5.327, word_reposition=1.648, kpe=0.96, ppl=886.03, wps=5021.8, ups=0.25, wpb=20334.7, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=1.907, clip=0, loss_scale=2048, train_wall=256, wall=17463
2022-07-25 18:43:12 | INFO | train_inner | epoch 006:    195 / 1122 loss=9.764, nll_loss=3.805, mask_ins=1.852, word_ins_ml=5.315, word_reposition=1.633, kpe=0.964, ppl=869.42, wps=5824, ups=0.28, wpb=20602.5, bsz=256, num_updates=5800, lr=0.000464238, gnorm=1.903, clip=0, loss_scale=2048, train_wall=308, wall=17817
2022-07-25 18:48:13 | INFO | train_inner | epoch 006:    295 / 1122 loss=nan, nll_loss=3.755, mask_ins=1.851, word_ins_ml=5.27, word_reposition=1.631, kpe=nan, ppl=nan, wps=6834.8, ups=0.33, wpb=20599, bsz=256, num_updates=5900, lr=0.000460287, gnorm=1.734, clip=0, loss_scale=2048, train_wall=257, wall=18118
2022-07-25 18:53:14 | INFO | train_inner | epoch 006:    395 / 1122 loss=9.666, nll_loss=3.731, mask_ins=1.84, word_ins_ml=5.249, word_reposition=1.621, kpe=0.957, ppl=812.56, wps=6831.2, ups=0.33, wpb=20569.9, bsz=256, num_updates=6000, lr=0.000456435, gnorm=1.727, clip=0, loss_scale=2048, train_wall=256, wall=18419
2022-07-25 18:58:15 | INFO | train_inner | epoch 006:    495 / 1122 loss=9.707, nll_loss=3.772, mask_ins=1.836, word_ins_ml=5.284, word_reposition=1.632, kpe=0.955, ppl=835.7, wps=6828, ups=0.33, wpb=20499.6, bsz=256, num_updates=6100, lr=0.000452679, gnorm=1.826, clip=0, loss_scale=2048, train_wall=256, wall=18720
2022-07-25 19:03:16 | INFO | train_inner | epoch 006:    595 / 1122 loss=9.662, nll_loss=3.741, mask_ins=1.835, word_ins_ml=5.255, word_reposition=1.619, kpe=0.952, ppl=810.13, wps=6747.3, ups=0.33, wpb=20353.6, bsz=256, num_updates=6200, lr=0.000449013, gnorm=1.726, clip=0, loss_scale=4055, train_wall=257, wall=19021
2022-07-25 19:08:17 | INFO | train_inner | epoch 006:    695 / 1122 loss=9.684, nll_loss=3.747, mask_ins=1.843, word_ins_ml=5.261, word_reposition=1.627, kpe=0.953, ppl=822.86, wps=6854.1, ups=0.33, wpb=20646.7, bsz=256, num_updates=6300, lr=0.000445435, gnorm=1.716, clip=0, loss_scale=4096, train_wall=257, wall=19323
2022-07-25 19:13:19 | INFO | train_inner | epoch 006:    795 / 1122 loss=nan, nll_loss=3.686, mask_ins=1.827, word_ins_ml=5.206, word_reposition=1.643, kpe=nan, ppl=nan, wps=6802.8, ups=0.33, wpb=20504.7, bsz=256, num_updates=6400, lr=0.000441942, gnorm=1.73, clip=0, loss_scale=4096, train_wall=257, wall=19624
2022-07-25 19:18:20 | INFO | train_inner | epoch 006:    895 / 1122 loss=9.609, nll_loss=3.701, mask_ins=1.819, word_ins_ml=5.219, word_reposition=1.619, kpe=0.952, ppl=780.98, wps=6850.5, ups=0.33, wpb=20646, bsz=256, num_updates=6500, lr=0.000438529, gnorm=1.709, clip=0, loss_scale=4096, train_wall=257, wall=19925
2022-07-25 19:23:22 | INFO | train_inner | epoch 006:    995 / 1122 loss=9.577, nll_loss=3.674, mask_ins=1.817, word_ins_ml=5.194, word_reposition=1.613, kpe=0.952, ppl=763.73, wps=6792.9, ups=0.33, wpb=20475.3, bsz=256, num_updates=6600, lr=0.000435194, gnorm=1.692, clip=0, loss_scale=4096, train_wall=257, wall=20227
2022-07-25 19:24:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-25 19:28:24 | INFO | train_inner | epoch 006:   1096 / 1122 loss=9.595, nll_loss=3.687, mask_ins=1.821, word_ins_ml=5.205, word_reposition=1.619, kpe=0.95, ppl=773.37, wps=6772.4, ups=0.33, wpb=20507.1, bsz=256, num_updates=6700, lr=0.000431934, gnorm=1.664, clip=0, loss_scale=4785, train_wall=257, wall=20530
2022-07-25 19:29:42 | INFO | train | epoch 006 | loss nan | nll_loss 3.736 | mask_ins 1.835 | word_ins_ml 5.252 | word_reposition 1.627 | kpe nan | ppl nan | wps 6507.7 | ups 0.32 | wpb 20520.5 | bsz 255.8 | num_updates 6726 | lr 0.000431099 | gnorm 1.752 | clip 0 | loss_scale 3251 | train_wall 2929 | wall 20607
2022-07-25 19:31:07 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 15.402 | nll_loss 7.458 | mask_ins 2.671 | word_ins_ml 8.651 | word_reposition 2.713 | kpe 1.367 | ppl 43305.6 | wps 11587 | wpb 2367.6 | bsz 32 | num_updates 6726 | best_loss 15.402
2022-07-25 19:31:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_best.pt (epoch 6 @ 6726 updates, score 15.402) (writing took 26.652437823824584 seconds)
2022-07-25 19:35:21 | INFO | train_inner | epoch 007:     74 / 1122 loss=9.512, nll_loss=3.633, mask_ins=1.811, word_ins_ml=5.158, word_reposition=1.605, kpe=0.939, ppl=730.26, wps=4876.3, ups=0.24, wpb=20322.2, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=1.73, clip=0, loss_scale=4096, train_wall=259, wall=20946
2022-07-25 19:40:21 | INFO | train_inner | epoch 007:    174 / 1122 loss=9.469, nll_loss=3.6, mask_ins=1.81, word_ins_ml=5.128, word_reposition=1.593, kpe=0.938, ppl=708.68, wps=6870.6, ups=0.33, wpb=20588.8, bsz=256, num_updates=6900, lr=0.000425628, gnorm=1.675, clip=0, loss_scale=4096, train_wall=256, wall=21246
2022-07-25 19:43:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-25 19:45:25 | INFO | train_inner | epoch 007:    275 / 1122 loss=9.496, nll_loss=3.653, mask_ins=1.795, word_ins_ml=5.174, word_reposition=1.588, kpe=0.94, ppl=722.29, wps=6789.3, ups=0.33, wpb=20655.5, bsz=256, num_updates=7000, lr=0.000422577, gnorm=1.626, clip=0, loss_scale=3325, train_wall=260, wall=21550
2022-07-25 19:51:14 | INFO | train_inner | epoch 007:    375 / 1122 loss=9.432, nll_loss=3.594, mask_ins=1.781, word_ins_ml=5.121, word_reposition=1.596, kpe=0.935, ppl=690.67, wps=5879.6, ups=0.29, wpb=20522.5, bsz=256, num_updates=7100, lr=0.000419591, gnorm=1.667, clip=0, loss_scale=2048, train_wall=306, wall=21899
2022-07-25 19:56:16 | INFO | train_inner | epoch 007:    475 / 1122 loss=nan, nll_loss=3.594, mask_ins=1.79, word_ins_ml=5.121, word_reposition=1.592, kpe=nan, ppl=nan, wps=6793.1, ups=0.33, wpb=20532.6, bsz=256, num_updates=7200, lr=0.000416667, gnorm=1.689, clip=0, loss_scale=2048, train_wall=258, wall=22202
2022-07-25 20:01:18 | INFO | train_inner | epoch 007:    575 / 1122 loss=nan, nll_loss=3.613, mask_ins=1.801, word_ins_ml=5.138, word_reposition=1.575, kpe=nan, ppl=nan, wps=6777.6, ups=0.33, wpb=20423.6, bsz=256, num_updates=7300, lr=0.000413803, gnorm=1.676, clip=0, loss_scale=2048, train_wall=257, wall=22503
2022-07-25 20:06:20 | INFO | train_inner | epoch 007:    675 / 1122 loss=9.428, nll_loss=3.554, mask_ins=1.799, word_ins_ml=5.085, word_reposition=1.606, kpe=0.937, ppl=688.77, wps=6800.9, ups=0.33, wpb=20543.5, bsz=256, num_updates=7400, lr=0.000410997, gnorm=1.64, clip=0, loss_scale=2048, train_wall=256, wall=22805
2022-07-25 20:11:21 | INFO | train_inner | epoch 007:    775 / 1122 loss=9.387, nll_loss=3.551, mask_ins=1.789, word_ins_ml=5.082, word_reposition=1.582, kpe=0.934, ppl=669.61, wps=6823.9, ups=0.33, wpb=20543.4, bsz=256, num_updates=7500, lr=0.000408248, gnorm=1.667, clip=0, loss_scale=2580, train_wall=256, wall=23106
2022-07-25 20:16:23 | INFO | train_inner | epoch 007:    875 / 1122 loss=9.401, nll_loss=3.569, mask_ins=1.791, word_ins_ml=5.097, word_reposition=1.578, kpe=0.935, ppl=676.23, wps=6815.1, ups=0.33, wpb=20555.9, bsz=256, num_updates=7600, lr=0.000405554, gnorm=1.639, clip=0, loss_scale=4096, train_wall=257, wall=23408
2022-07-25 20:17:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-25 20:21:27 | INFO | train_inner | epoch 007:    976 / 1122 loss=9.321, nll_loss=3.491, mask_ins=1.78, word_ins_ml=5.028, word_reposition=1.578, kpe=0.935, ppl=639.65, wps=6764.1, ups=0.33, wpb=20567.3, bsz=256, num_updates=7700, lr=0.000402911, gnorm=1.624, clip=0, loss_scale=2494, train_wall=260, wall=23712
2022-07-25 20:26:28 | INFO | train_inner | epoch 007:   1076 / 1122 loss=9.392, nll_loss=3.568, mask_ins=1.779, word_ins_ml=5.096, word_reposition=1.582, kpe=0.936, ppl=671.84, wps=6802.4, ups=0.33, wpb=20527.8, bsz=256, num_updates=7800, lr=0.00040032, gnorm=1.617, clip=0, loss_scale=2048, train_wall=257, wall=24014
2022-07-25 20:28:45 | INFO | train | epoch 007 | loss nan | nll_loss 3.58 | mask_ins 1.794 | word_ins_ml 5.108 | word_reposition 1.589 | kpe nan | ppl nan | wps 6486.3 | ups 0.32 | wpb 20520.7 | bsz 255.8 | num_updates 7846 | lr 0.000399145 | gnorm 1.658 | clip 0 | loss_scale 2751 | train_wall 2933 | wall 24150
2022-07-25 20:30:10 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 15.619 | nll_loss 7.524 | mask_ins 2.701 | word_ins_ml 8.725 | word_reposition 2.758 | kpe 1.434 | ppl 50309 | wps 11740.2 | wpb 2367.6 | bsz 32 | num_updates 7846 | best_loss 15.402
2022-07-25 20:30:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_last.pt (epoch 7 @ 7846 updates, score 15.619) (writing took 10.308270000852644 seconds)
2022-07-25 20:33:06 | INFO | train_inner | epoch 008:     54 / 1122 loss=9.372, nll_loss=3.536, mask_ins=1.793, word_ins_ml=5.068, word_reposition=1.581, kpe=0.93, ppl=662.59, wps=5134.4, ups=0.25, wpb=20398.9, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=1.68, clip=0, loss_scale=2048, train_wall=258, wall=24411
2022-07-25 20:38:06 | INFO | train_inner | epoch 008:    154 / 1122 loss=nan, nll_loss=3.481, mask_ins=1.775, word_ins_ml=5.019, word_reposition=1.58, kpe=nan, ppl=nan, wps=6823.4, ups=0.33, wpb=20517.5, bsz=256, num_updates=8000, lr=0.000395285, gnorm=1.553, clip=0, loss_scale=2048, train_wall=257, wall=24712
2022-07-25 20:43:07 | INFO | train_inner | epoch 008:    254 / 1122 loss=9.333, nll_loss=3.508, mask_ins=1.779, word_ins_ml=5.043, word_reposition=1.592, kpe=0.919, ppl=644.85, wps=6819.4, ups=0.33, wpb=20490.6, bsz=256, num_updates=8100, lr=0.000392837, gnorm=1.585, clip=0, loss_scale=2048, train_wall=256, wall=25012
2022-07-25 20:48:08 | INFO | train_inner | epoch 008:    354 / 1122 loss=9.289, nll_loss=3.491, mask_ins=1.755, word_ins_ml=5.026, word_reposition=1.581, kpe=0.927, ppl=625.71, wps=6830.9, ups=0.33, wpb=20563.2, bsz=256, num_updates=8200, lr=0.000390434, gnorm=1.592, clip=0, loss_scale=3420, train_wall=257, wall=25313
2022-07-25 20:53:09 | INFO | train_inner | epoch 008:    454 / 1122 loss=9.235, nll_loss=3.476, mask_ins=1.754, word_ins_ml=5.013, word_reposition=1.55, kpe=0.918, ppl=602.59, wps=6823.7, ups=0.33, wpb=20538.8, bsz=256, num_updates=8300, lr=0.000388075, gnorm=1.562, clip=0, loss_scale=4096, train_wall=256, wall=25614
2022-07-25 20:58:47 | INFO | train_inner | epoch 008:    554 / 1122 loss=9.273, nll_loss=3.478, mask_ins=1.767, word_ins_ml=5.015, word_reposition=1.567, kpe=0.924, ppl=618.83, wps=6075.6, ups=0.3, wpb=20559.9, bsz=256, num_updates=8400, lr=0.000385758, gnorm=1.614, clip=0, loss_scale=4096, train_wall=292, wall=25952
2022-07-25 21:03:49 | INFO | train_inner | epoch 008:    654 / 1122 loss=9.29, nll_loss=3.488, mask_ins=1.769, word_ins_ml=5.023, word_reposition=1.575, kpe=0.923, ppl=625.87, wps=6839.8, ups=0.33, wpb=20632.9, bsz=256, num_updates=8500, lr=0.000383482, gnorm=1.703, clip=0, loss_scale=4096, train_wall=258, wall=26254
2022-07-25 21:08:51 | INFO | train_inner | epoch 008:    754 / 1122 loss=9.225, nll_loss=3.439, mask_ins=1.759, word_ins_ml=4.98, word_reposition=1.565, kpe=0.922, ppl=598.28, wps=6793.3, ups=0.33, wpb=20485.1, bsz=256, num_updates=8600, lr=0.000381246, gnorm=1.608, clip=0, loss_scale=4096, train_wall=256, wall=26556
2022-07-25 21:13:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-25 21:13:56 | INFO | train_inner | epoch 008:    855 / 1122 loss=nan, nll_loss=3.455, mask_ins=1.767, word_ins_ml=4.993, word_reposition=1.568, kpe=nan, ppl=nan, wps=6735.6, ups=0.33, wpb=20540.9, bsz=256, num_updates=8700, lr=0.000379049, gnorm=1.571, clip=0, loss_scale=6245, train_wall=260, wall=26861
2022-07-25 21:18:57 | INFO | train_inner | epoch 008:    955 / 1122 loss=9.188, nll_loss=3.423, mask_ins=1.739, word_ins_ml=4.964, word_reposition=1.563, kpe=0.922, ppl=583.37, wps=6832.6, ups=0.33, wpb=20571.2, bsz=256, num_updates=8800, lr=0.000376889, gnorm=1.571, clip=0, loss_scale=4096, train_wall=256, wall=27162
2022-07-25 21:23:58 | INFO | train_inner | epoch 008:   1055 / 1122 loss=9.263, nll_loss=3.464, mask_ins=1.763, word_ins_ml=5.001, word_reposition=1.577, kpe=0.922, ppl=614.57, wps=6747.7, ups=0.33, wpb=20349.1, bsz=256, num_updates=8900, lr=0.000374766, gnorm=1.605, clip=0, loss_scale=4096, train_wall=257, wall=27463
2022-07-25 21:27:22 | INFO | train | epoch 008 | loss nan | nll_loss 3.474 | mask_ins 1.764 | word_ins_ml 5.011 | word_reposition 1.572 | kpe nan | ppl nan | wps 6541.5 | ups 0.32 | wpb 20520.1 | bsz 255.8 | num_updates 8967 | lr 0.000373363 | gnorm 1.605 | clip 0 | loss_scale 3766 | train_wall 2922 | wall 27667
2022-07-25 21:28:47 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 15.736 | nll_loss 7.598 | mask_ins 2.746 | word_ins_ml 8.799 | word_reposition 2.8 | kpe 1.391 | ppl 54575.9 | wps 11575.1 | wpb 2367.6 | bsz 32 | num_updates 8967 | best_loss 15.402
2022-07-25 21:28:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_last.pt (epoch 8 @ 8967 updates, score 15.736) (writing took 9.875756557099521 seconds)
2022-07-25 21:30:37 | INFO | train_inner | epoch 009:     33 / 1122 loss=9.255, nll_loss=3.473, mask_ins=1.762, word_ins_ml=5.009, word_reposition=1.568, kpe=0.916, ppl=610.8, wps=5095.7, ups=0.25, wpb=20313.4, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=1.642, clip=0, loss_scale=4096, train_wall=259, wall=27862
2022-07-25 21:31:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-25 21:35:41 | INFO | train_inner | epoch 009:    134 / 1122 loss=9.157, nll_loss=3.402, mask_ins=1.74, word_ins_ml=4.945, word_reposition=1.561, kpe=0.911, ppl=571.01, wps=6796.4, ups=0.33, wpb=20646.8, bsz=256, num_updates=9100, lr=0.000370625, gnorm=1.579, clip=0, loss_scale=2190, train_wall=258, wall=28166
2022-07-25 21:40:42 | INFO | train_inner | epoch 009:    234 / 1122 loss=9.094, nll_loss=3.383, mask_ins=1.737, word_ins_ml=4.929, word_reposition=1.519, kpe=0.909, ppl=546.41, wps=6800.8, ups=0.33, wpb=20476.2, bsz=256, num_updates=9200, lr=0.000368605, gnorm=1.603, clip=0, loss_scale=2048, train_wall=256, wall=28467
2022-07-25 21:45:44 | INFO | train_inner | epoch 009:    334 / 1122 loss=9.119, nll_loss=3.383, mask_ins=1.733, word_ins_ml=4.928, word_reposition=1.547, kpe=0.91, ppl=555.97, wps=6777, ups=0.33, wpb=20459.5, bsz=256, num_updates=9300, lr=0.000366618, gnorm=1.575, clip=0, loss_scale=2048, train_wall=257, wall=28769
2022-07-25 21:50:44 | INFO | train_inner | epoch 009:    434 / 1122 loss=nan, nll_loss=3.379, mask_ins=1.726, word_ins_ml=4.924, word_reposition=1.54, kpe=nan, ppl=nan, wps=6806.3, ups=0.33, wpb=20467.1, bsz=256, num_updates=9400, lr=0.000364662, gnorm=1.513, clip=0, loss_scale=2048, train_wall=257, wall=29069
2022-07-25 21:55:45 | INFO | train_inner | epoch 009:    534 / 1122 loss=9.085, nll_loss=3.365, mask_ins=1.726, word_ins_ml=4.912, word_reposition=1.538, kpe=0.91, ppl=543.12, wps=6820.3, ups=0.33, wpb=20531, bsz=256, num_updates=9500, lr=0.000362738, gnorm=1.633, clip=0, loss_scale=2048, train_wall=257, wall=29371
2022-07-25 22:01:01 | INFO | train_inner | epoch 009:    634 / 1122 loss=9.113, nll_loss=3.4, mask_ins=1.727, word_ins_ml=4.943, word_reposition=1.538, kpe=0.906, ppl=553.86, wps=6516.8, ups=0.32, wpb=20558.5, bsz=256, num_updates=9600, lr=0.000360844, gnorm=1.493, clip=0, loss_scale=3727, train_wall=270, wall=29686
2022-07-25 22:06:40 | INFO | train_inner | epoch 009:    734 / 1122 loss=9.085, nll_loss=3.377, mask_ins=1.722, word_ins_ml=4.922, word_reposition=1.533, kpe=0.907, ppl=543.21, wps=6071, ups=0.3, wpb=20567.8, bsz=256, num_updates=9700, lr=0.000358979, gnorm=1.489, clip=0, loss_scale=4096, train_wall=294, wall=30025
2022-07-25 22:11:42 | INFO | train_inner | epoch 009:    834 / 1122 loss=9.15, nll_loss=3.429, mask_ins=1.727, word_ins_ml=4.968, word_reposition=1.546, kpe=0.909, ppl=567.93, wps=6793.9, ups=0.33, wpb=20505.7, bsz=256, num_updates=9800, lr=0.000357143, gnorm=1.509, clip=0, loss_scale=4096, train_wall=258, wall=30327
2022-07-25 22:16:43 | INFO | train_inner | epoch 009:    934 / 1122 loss=9.043, nll_loss=3.349, mask_ins=1.716, word_ins_ml=4.897, word_reposition=1.526, kpe=0.905, ppl=527.64, wps=6813.9, ups=0.33, wpb=20568.6, bsz=256, num_updates=9900, lr=0.000355335, gnorm=1.474, clip=0, loss_scale=4096, train_wall=257, wall=30629
2022-07-25 22:19:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-25 22:21:48 | INFO | train_inner | epoch 009:   1035 / 1122 loss=nan, nll_loss=3.383, mask_ins=1.73, word_ins_ml=4.927, word_reposition=1.55, kpe=nan, ppl=nan, wps=6743, ups=0.33, wpb=20524.3, bsz=256, num_updates=10000, lr=0.000353553, gnorm=1.557, clip=0, loss_scale=3042, train_wall=260, wall=30933
2022-07-25 22:26:12 | INFO | train | epoch 009 | loss nan | nll_loss 3.388 | mask_ins 1.728 | word_ins_ml 4.932 | word_reposition 1.54 | kpe nan | ppl nan | wps 6511.2 | ups 0.32 | wpb 20520.7 | bsz 255.8 | num_updates 10087 | lr 0.000352025 | gnorm 1.547 | clip 0 | loss_scale 2908 | train_wall 2932 | wall 31197
2022-07-25 22:27:37 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 15.551 | nll_loss 7.46 | mask_ins 2.715 | word_ins_ml 8.669 | word_reposition 2.792 | kpe 1.376 | ppl 48017.1 | wps 11544.9 | wpb 2367.6 | bsz 32 | num_updates 10087 | best_loss 15.402
2022-07-25 22:27:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_last.pt (epoch 9 @ 10087 updates, score 15.551) (writing took 9.74710853304714 seconds)
2022-07-25 22:28:26 | INFO | train_inner | epoch 010:     13 / 1122 loss=9.098, nll_loss=3.4, mask_ins=1.717, word_ins_ml=4.942, word_reposition=1.536, kpe=0.904, ppl=547.93, wps=5143.8, ups=0.25, wpb=20466, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=1.64, clip=0, loss_scale=2048, train_wall=258, wall=31331
2022-07-25 22:29:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-25 22:33:30 | INFO | train_inner | epoch 010:    114 / 1122 loss=8.979, nll_loss=3.309, mask_ins=1.707, word_ins_ml=4.861, word_reposition=1.514, kpe=0.896, ppl=504.5, wps=6756.5, ups=0.33, wpb=20585.6, bsz=256, num_updates=10200, lr=0.00035007, gnorm=1.515, clip=0, loss_scale=1288, train_wall=259, wall=31635
2022-07-25 22:38:30 | INFO | train_inner | epoch 010:    214 / 1122 loss=8.977, nll_loss=3.323, mask_ins=1.705, word_ins_ml=4.873, word_reposition=1.506, kpe=0.893, ppl=503.94, wps=6844.3, ups=0.33, wpb=20517.2, bsz=256, num_updates=10300, lr=0.000348367, gnorm=1.466, clip=0, loss_scale=1024, train_wall=256, wall=31935
2022-07-25 22:43:30 | INFO | train_inner | epoch 010:    314 / 1122 loss=9.037, nll_loss=3.339, mask_ins=1.715, word_ins_ml=4.887, word_reposition=1.539, kpe=0.897, ppl=525.36, wps=6880.4, ups=0.33, wpb=20607.2, bsz=256, num_updates=10400, lr=0.000346688, gnorm=1.668, clip=0, loss_scale=1024, train_wall=256, wall=32235
2022-07-25 22:48:31 | INFO | train_inner | epoch 010:    414 / 1122 loss=9.011, nll_loss=3.331, mask_ins=1.712, word_ins_ml=4.88, word_reposition=1.52, kpe=0.899, ppl=515.84, wps=6814.7, ups=0.33, wpb=20562.6, bsz=256, num_updates=10500, lr=0.000345033, gnorm=1.541, clip=0, loss_scale=1024, train_wall=257, wall=32536
2022-07-25 22:53:32 | INFO | train_inner | epoch 010:    514 / 1122 loss=8.977, nll_loss=3.317, mask_ins=1.713, word_ins_ml=4.867, word_reposition=1.507, kpe=0.889, ppl=503.8, wps=6820.5, ups=0.33, wpb=20513.3, bsz=256, num_updates=10600, lr=0.000343401, gnorm=1.597, clip=0, loss_scale=1024, train_wall=257, wall=32837
2022-07-25 22:53:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-25 22:58:36 | INFO | train_inner | epoch 010:    615 / 1122 loss=nan, nll_loss=3.357, mask_ins=1.721, word_ins_ml=4.903, word_reposition=1.537, kpe=nan, ppl=nan, wps=6805.6, ups=0.33, wpb=20671.4, bsz=256, num_updates=10700, lr=0.000341793, gnorm=1.542, clip=0, loss_scale=547, train_wall=259, wall=33141
2022-07-25 23:03:37 | INFO | train_inner | epoch 010:    715 / 1122 loss=9.016, nll_loss=3.33, mask_ins=1.706, word_ins_ml=4.879, word_reposition=1.533, kpe=0.898, ppl=517.64, wps=6774.9, ups=0.33, wpb=20411.9, bsz=256, num_updates=10800, lr=0.000340207, gnorm=1.643, clip=0, loss_scale=512, train_wall=258, wall=33442
2022-07-25 23:08:50 | INFO | train_inner | epoch 010:    815 / 1122 loss=nan, nll_loss=3.29, mask_ins=1.703, word_ins_ml=4.843, word_reposition=1.54, kpe=nan, ppl=nan, wps=6569.8, ups=0.32, wpb=20540.2, bsz=256, num_updates=10900, lr=0.000338643, gnorm=1.501, clip=0, loss_scale=512, train_wall=269, wall=33755
2022-07-25 23:14:27 | INFO | train_inner | epoch 010:    915 / 1122 loss=8.961, nll_loss=3.274, mask_ins=1.711, word_ins_ml=4.829, word_reposition=1.522, kpe=0.899, ppl=498.43, wps=6086.4, ups=0.3, wpb=20530, bsz=256, num_updates=11000, lr=0.0003371, gnorm=1.534, clip=0, loss_scale=512, train_wall=294, wall=34092
2022-07-25 23:19:32 | INFO | train_inner | epoch 010:   1015 / 1122 loss=8.99, nll_loss=3.315, mask_ins=1.7, word_ins_ml=4.865, word_reposition=1.525, kpe=0.9, ppl=508.47, wps=6706.8, ups=0.33, wpb=20464, bsz=256, num_updates=11100, lr=0.000335578, gnorm=1.516, clip=0, loss_scale=512, train_wall=260, wall=34397
2022-07-25 23:24:34 | INFO | train_inner | epoch 010:   1115 / 1122 loss=8.912, nll_loss=3.276, mask_ins=1.688, word_ins_ml=4.83, word_reposition=1.496, kpe=0.899, ppl=481.77, wps=6795.3, ups=0.33, wpb=20477.9, bsz=256, num_updates=11200, lr=0.000334077, gnorm=1.699, clip=0, loss_scale=932, train_wall=257, wall=34699
2022-07-25 23:24:54 | INFO | train | epoch 010 | loss nan | nll_loss 3.315 | mask_ins 1.708 | word_ins_ml 4.866 | word_reposition 1.522 | kpe nan | ppl nan | wps 6525.1 | ups 0.32 | wpb 20519.7 | bsz 255.8 | num_updates 11207 | lr 0.000333972 | gnorm 1.574 | clip 0 | loss_scale 826 | train_wall 2932 | wall 34719
2022-07-25 23:26:19 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 15.448 | nll_loss 7.378 | mask_ins 2.726 | word_ins_ml 8.593 | word_reposition 2.751 | kpe 1.378 | ppl 44689.4 | wps 11594.1 | wpb 2367.6 | bsz 32 | num_updates 11207 | best_loss 15.402
2022-07-25 23:26:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_last.pt (epoch 10 @ 11207 updates, score 15.448) (writing took 14.342969211749732 seconds)
2022-07-25 23:31:13 | INFO | train_inner | epoch 011:     93 / 1122 loss=8.939, nll_loss=3.285, mask_ins=1.696, word_ins_ml=4.839, word_reposition=1.521, kpe=0.883, ppl=490.66, wps=5112.9, ups=0.25, wpb=20402, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=1.624, clip=0, loss_scale=1024, train_wall=256, wall=35098
2022-07-25 23:31:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-25 23:36:18 | INFO | train_inner | epoch 011:    194 / 1122 loss=nan, nll_loss=3.239, mask_ins=1.694, word_ins_ml=4.797, word_reposition=1.517, kpe=nan, ppl=nan, wps=6712.6, ups=0.33, wpb=20461.6, bsz=256, num_updates=11400, lr=0.000331133, gnorm=1.509, clip=0, loss_scale=517, train_wall=260, wall=35403
2022-07-25 23:41:20 | INFO | train_inner | epoch 011:    294 / 1122 loss=nan, nll_loss=3.246, mask_ins=1.676, word_ins_ml=4.803, word_reposition=1.509, kpe=nan, ppl=nan, wps=6800.5, ups=0.33, wpb=20539.4, bsz=256, num_updates=11500, lr=0.00032969, gnorm=1.551, clip=0, loss_scale=512, train_wall=257, wall=35705
2022-07-25 23:46:20 | INFO | train_inner | epoch 011:    394 / 1122 loss=8.871, nll_loss=3.224, mask_ins=1.684, word_ins_ml=4.783, word_reposition=1.517, kpe=0.886, ppl=468.07, wps=6843.7, ups=0.33, wpb=20559.7, bsz=256, num_updates=11600, lr=0.000328266, gnorm=1.61, clip=0, loss_scale=512, train_wall=257, wall=36005
2022-07-25 23:51:21 | INFO | train_inner | epoch 011:    494 / 1122 loss=8.918, nll_loss=3.275, mask_ins=1.678, word_ins_ml=4.829, word_reposition=1.523, kpe=0.889, ppl=483.57, wps=6864.8, ups=0.33, wpb=20651.1, bsz=256, num_updates=11700, lr=0.00032686, gnorm=1.558, clip=0, loss_scale=512, train_wall=257, wall=36306
2022-07-25 23:56:23 | INFO | train_inner | epoch 011:    594 / 1122 loss=8.926, nll_loss=3.281, mask_ins=1.685, word_ins_ml=4.833, word_reposition=1.519, kpe=0.889, ppl=486.41, wps=6805.3, ups=0.33, wpb=20543.4, bsz=256, num_updates=11800, lr=0.000325472, gnorm=1.816, clip=0, loss_scale=512, train_wall=257, wall=36608
2022-07-25 23:57:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-26 00:01:27 | INFO | train_inner | epoch 011:    695 / 1122 loss=8.946, nll_loss=3.297, mask_ins=1.686, word_ins_ml=4.848, word_reposition=1.522, kpe=0.891, ppl=493.35, wps=6743.8, ups=0.33, wpb=20530.7, bsz=256, num_updates=11900, lr=0.000324102, gnorm=1.696, clip=0, loss_scale=517, train_wall=260, wall=36912
2022-07-26 00:06:28 | INFO | train_inner | epoch 011:    795 / 1122 loss=8.905, nll_loss=3.269, mask_ins=1.684, word_ins_ml=4.823, word_reposition=1.503, kpe=0.895, ppl=479.43, wps=6765.6, ups=0.33, wpb=20333, bsz=256, num_updates=12000, lr=0.000322749, gnorm=1.619, clip=0, loss_scale=512, train_wall=256, wall=37213
2022-07-26 00:11:28 | INFO | train_inner | epoch 011:    895 / 1122 loss=8.859, nll_loss=3.246, mask_ins=1.663, word_ins_ml=4.802, word_reposition=1.509, kpe=0.886, ppl=464.38, wps=6856, ups=0.33, wpb=20614.3, bsz=256, num_updates=12100, lr=0.000321412, gnorm=1.558, clip=0, loss_scale=512, train_wall=256, wall=37513
2022-07-26 00:16:46 | INFO | train_inner | epoch 011:    995 / 1122 loss=8.92, nll_loss=3.269, mask_ins=1.685, word_ins_ml=4.823, word_reposition=1.526, kpe=0.886, ppl=484.33, wps=6459.8, ups=0.32, wpb=20487.8, bsz=256, num_updates=12200, lr=0.000320092, gnorm=1.506, clip=0, loss_scale=512, train_wall=272, wall=37831
2022-07-26 00:22:12 | INFO | train_inner | epoch 011:   1095 / 1122 loss=8.887, nll_loss=3.252, mask_ins=1.68, word_ins_ml=4.808, word_reposition=1.515, kpe=0.885, ppl=473.42, wps=6303.3, ups=0.31, wpb=20565.7, bsz=256, num_updates=12300, lr=0.000318788, gnorm=1.544, clip=0, loss_scale=512, train_wall=283, wall=38157
2022-07-26 00:23:33 | INFO | train | epoch 011 | loss nan | nll_loss 3.261 | mask_ins 1.682 | word_ins_ml 4.816 | word_reposition 1.516 | kpe nan | ppl nan | wps 6531.4 | ups 0.32 | wpb 20521 | bsz 255.8 | num_updates 12327 | lr 0.000318439 | gnorm 1.597 | clip 0 | loss_scale 557 | train_wall 2921 | wall 38238
2022-07-26 00:24:59 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 15.153 | nll_loss 7.304 | mask_ins 2.597 | word_ins_ml 8.525 | word_reposition 2.64 | kpe 1.391 | ppl 36424.8 | wps 11517.4 | wpb 2367.6 | bsz 32 | num_updates 12327 | best_loss 15.153
2022-07-26 00:25:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_best.pt (epoch 11 @ 12327 updates, score 15.153) (writing took 27.562194271944463 seconds)
2022-07-26 00:29:06 | INFO | train_inner | epoch 012:     73 / 1122 loss=8.854, nll_loss=3.233, mask_ins=1.676, word_ins_ml=4.791, word_reposition=1.509, kpe=0.878, ppl=462.78, wps=4940, ups=0.24, wpb=20456.1, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=1.563, clip=0, loss_scale=901, train_wall=256, wall=38571
2022-07-26 00:34:07 | INFO | train_inner | epoch 012:    173 / 1122 loss=8.789, nll_loss=3.188, mask_ins=1.671, word_ins_ml=4.751, word_reposition=1.494, kpe=0.872, ppl=442.22, wps=6806.9, ups=0.33, wpb=20488.9, bsz=256, num_updates=12500, lr=0.000316228, gnorm=1.483, clip=0, loss_scale=1024, train_wall=257, wall=38872
2022-07-26 00:35:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-26 00:39:12 | INFO | train_inner | epoch 012:    274 / 1122 loss=8.823, nll_loss=3.213, mask_ins=1.669, word_ins_ml=4.773, word_reposition=1.511, kpe=0.871, ppl=453, wps=6727, ups=0.33, wpb=20500.3, bsz=256, num_updates=12600, lr=0.00031497, gnorm=1.523, clip=0, loss_scale=618, train_wall=260, wall=39177
2022-07-26 00:44:13 | INFO | train_inner | epoch 012:    374 / 1122 loss=8.792, nll_loss=3.184, mask_ins=1.669, word_ins_ml=4.747, word_reposition=1.502, kpe=0.874, ppl=443.32, wps=6850.9, ups=0.33, wpb=20620, bsz=256, num_updates=12700, lr=0.000313728, gnorm=1.523, clip=0, loss_scale=512, train_wall=256, wall=39478
2022-07-26 00:49:12 | INFO | train_inner | epoch 012:    474 / 1122 loss=8.829, nll_loss=3.228, mask_ins=1.674, word_ins_ml=4.785, word_reposition=1.489, kpe=0.881, ppl=454.92, wps=6854.9, ups=0.33, wpb=20488.3, bsz=256, num_updates=12800, lr=0.0003125, gnorm=1.532, clip=0, loss_scale=512, train_wall=255, wall=39777
2022-07-26 00:54:12 | INFO | train_inner | epoch 012:    574 / 1122 loss=8.826, nll_loss=3.209, mask_ins=1.676, word_ins_ml=4.769, word_reposition=1.504, kpe=0.877, ppl=453.75, wps=6851.1, ups=0.33, wpb=20598.6, bsz=256, num_updates=12900, lr=0.000311286, gnorm=1.524, clip=0, loss_scale=512, train_wall=257, wall=40077
2022-07-26 00:59:13 | INFO | train_inner | epoch 012:    674 / 1122 loss=8.732, nll_loss=3.158, mask_ins=1.661, word_ins_ml=4.723, word_reposition=1.473, kpe=0.874, ppl=425.1, wps=6786.2, ups=0.33, wpb=20387.5, bsz=256, num_updates=13000, lr=0.000310087, gnorm=1.492, clip=0, loss_scale=512, train_wall=257, wall=40378
2022-07-26 01:03:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-26 01:04:17 | INFO | train_inner | epoch 012:    775 / 1122 loss=8.757, nll_loss=3.182, mask_ins=1.654, word_ins_ml=4.744, word_reposition=1.482, kpe=0.876, ppl=432.59, wps=6761.6, ups=0.33, wpb=20596.3, bsz=256, num_updates=13100, lr=0.000308901, gnorm=1.505, clip=0, loss_scale=735, train_wall=259, wall=40682
2022-07-26 01:09:20 | INFO | train_inner | epoch 012:    875 / 1122 loss=nan, nll_loss=3.227, mask_ins=1.671, word_ins_ml=4.784, word_reposition=1.512, kpe=nan, ppl=nan, wps=6794.5, ups=0.33, wpb=20573.1, bsz=256, num_updates=13200, lr=0.000307729, gnorm=1.507, clip=0, loss_scale=512, train_wall=257, wall=40985
2022-07-26 01:14:26 | INFO | train_inner | epoch 012:    975 / 1122 loss=8.751, nll_loss=3.167, mask_ins=1.664, word_ins_ml=4.731, word_reposition=1.48, kpe=0.876, ppl=430.84, wps=6713.6, ups=0.33, wpb=20516.7, bsz=256, num_updates=13300, lr=0.00030657, gnorm=1.572, clip=0, loss_scale=512, train_wall=261, wall=41291
2022-07-26 01:19:26 | INFO | train_inner | epoch 012:   1075 / 1122 loss=nan, nll_loss=3.191, mask_ins=1.669, word_ins_ml=4.752, word_reposition=1.51, kpe=nan, ppl=nan, wps=6813.6, ups=0.33, wpb=20485, bsz=256, num_updates=13400, lr=0.000305424, gnorm=1.543, clip=0, loss_scale=512, train_wall=257, wall=41591
2022-07-26 01:21:48 | INFO | train | epoch 012 | loss nan | nll_loss 3.198 | mask_ins 1.669 | word_ins_ml 4.759 | word_reposition 1.498 | kpe nan | ppl nan | wps 6575.9 | ups 0.32 | wpb 20520.4 | bsz 255.8 | num_updates 13447 | lr 0.000304889 | gnorm 1.545 | clip 0 | loss_scale 621 | train_wall 2885 | wall 41733
2022-07-26 01:23:13 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 15.443 | nll_loss 7.403 | mask_ins 2.685 | word_ins_ml 8.625 | word_reposition 2.71 | kpe 1.424 | ppl 44548 | wps 11581.1 | wpb 2367.6 | bsz 32 | num_updates 13447 | best_loss 15.153
2022-07-26 01:23:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_last.pt (epoch 12 @ 13447 updates, score 15.443) (writing took 20.102294387295842 seconds)
2022-07-26 01:26:25 | INFO | train_inner | epoch 013:     53 / 1122 loss=8.791, nll_loss=3.19, mask_ins=1.675, word_ins_ml=4.751, word_reposition=1.494, kpe=0.871, ppl=442.94, wps=4875.1, ups=0.24, wpb=20393.7, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=1.902, clip=0, loss_scale=512, train_wall=270, wall=42010
2022-07-26 01:32:05 | INFO | train_inner | epoch 013:    153 / 1122 loss=8.765, nll_loss=3.182, mask_ins=1.661, word_ins_ml=4.745, word_reposition=1.492, kpe=0.867, ppl=434.97, wps=6050.9, ups=0.29, wpb=20589.9, bsz=256, num_updates=13600, lr=0.00030317, gnorm=1.695, clip=0, loss_scale=579, train_wall=295, wall=42350
2022-07-26 01:37:07 | INFO | train_inner | epoch 013:    253 / 1122 loss=8.764, nll_loss=3.188, mask_ins=1.658, word_ins_ml=4.75, word_reposition=1.492, kpe=0.864, ppl=434.73, wps=6776.8, ups=0.33, wpb=20450.1, bsz=256, num_updates=13700, lr=0.000302061, gnorm=1.559, clip=0, loss_scale=1024, train_wall=257, wall=42652
2022-07-26 01:42:10 | INFO | train_inner | epoch 013:    353 / 1122 loss=nan, nll_loss=3.184, mask_ins=1.647, word_ins_ml=4.746, word_reposition=1.48, kpe=nan, ppl=nan, wps=6780.4, ups=0.33, wpb=20550.4, bsz=256, num_updates=13800, lr=0.000300965, gnorm=1.456, clip=0, loss_scale=1024, train_wall=257, wall=42955
2022-07-26 01:47:12 | INFO | train_inner | epoch 013:    453 / 1122 loss=8.729, nll_loss=3.163, mask_ins=1.654, word_ins_ml=4.728, word_reposition=1.48, kpe=0.867, ppl=424.32, wps=6815.6, ups=0.33, wpb=20578.3, bsz=256, num_updates=13900, lr=0.00029988, gnorm=1.539, clip=0, loss_scale=1024, train_wall=257, wall=43257
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
train.sh: line 42: ran: command not found
