nohup: ignoring input
2022-07-18 16:35:43 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15196
2022-07-18 16:35:43 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:15196
2022-07-18 16:35:43 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-18 16:35:43 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:15196
2022-07-18 16:35:43 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-18 16:35:43 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15196
2022-07-18 16:35:43 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-18 16:35:43 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-18 16:35:43 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-18 16:35:43 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-18 16:35:43 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-18 16:35:43 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-18 16:35:43 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-18 16:35:43 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-18 16:35:43 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-18 16:35:43 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-18 16:35:47 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:15196', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-ACL-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/数据集/preprocess/ACL-bert-cased-510/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-18 16:35:47 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-18 16:35:47 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-07-18 16:35:47 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-ACL-bert-cased-510/valid.source-target.source
2022-07-18 16:35:47 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-ACL-bert-cased-510/valid.source-target.target
2022-07-18 16:35:47 | INFO | fairseq.tasks.translation | ../data-bin-ACL-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]410it [00:00, 4095.95it/s]403it [00:00, 4028.33it/s]392it [00:00, 3912.06it/s]399it [00:00, 3980.67it/s]820it [00:00, 3599.42it/s]806it [00:00, 3598.71it/s]784it [00:00, 3546.48it/s]798it [00:00, 3472.57it/s]1209it [00:00, 3720.11it/s]1184it [00:00, 3660.14it/s]1147it [00:00, 3580.13it/s]1172it [00:00, 3584.94it/s]1552it [00:00, 3605.98it/s]1585it [00:00, 3611.29it/s]1507it [00:00, 3483.33it/s]1534it [00:00, 3438.32it/s]1975it [00:00, 3820.83it/s]2006it [00:00, 3814.60it/s]1904it [00:00, 3649.70it/s]1949it [00:00, 3678.35it/s]2359it [00:00, 3744.92it/s]2271it [00:00, 3604.46it/s]2390it [00:00, 3740.20it/s]2320it [00:00, 3610.99it/s]2790it [00:00, 3922.00it/s]2688it [00:00, 3783.96it/s]2811it [00:00, 3885.12it/s]2724it [00:00, 3737.94it/s]3202it [00:00, 3802.71it/s]3184it [00:00, 3793.39it/s]3074it [00:00, 3674.21it/s]3100it [00:00, 3683.59it/s]3613it [00:00, 3895.43it/s]3608it [00:00, 3925.23it/s]3473it [00:00, 3768.04it/s]3477it [00:00, 3708.09it/s]3865it [00:01, 3812.00it/s]4003it [00:01, 3821.16it/s]3883it [00:01, 3813.95it/s]4004it [00:01, 3751.47it/s]4387it [00:01, 3802.21it/s]4384it [00:01, 3764.50it/s]4248it [00:01, 3606.43it/s]4266it [00:01, 3642.61it/s]4769it [00:01, 3694.41it/s]4632it [00:01, 3658.86it/s]4762it [00:01, 3652.72it/s]4657it [00:01, 3718.87it/s]5172it [00:01, 3790.24it/s]5173it [00:01, 3783.33it/s]5000it [00:01, 3579.26it/s]5031it [00:01, 3587.32it/s]5572it [00:01, 3851.29it/s]5563it [00:01, 3816.77it/s]5398it [00:01, 3693.92it/s]5427it [00:01, 3691.97it/s]5946it [00:01, 3631.32it/s]5959it [00:01, 3640.17it/s]5770it [00:01, 3519.47it/s]5799it [00:01, 3519.79it/s]6320it [00:01, 3661.17it/s]6336it [00:01, 3674.46it/s]6140it [00:01, 3569.13it/s]6165it [00:01, 3559.35it/s]6499it [00:01, 3312.33it/s]6706it [00:02, 2195.59it/s]6688it [00:02, 2127.05it/s]7064it [00:02, 2465.17it/s]7054it [00:02, 2422.80it/s]6523it [00:02, 1766.16it/s]6835it [00:02, 1826.92it/s]7384it [00:02, 2625.49it/s]7369it [00:02, 2580.32it/s]6890it [00:02, 2087.78it/s]7206it [00:02, 2161.92it/s]7764it [00:02, 2903.71it/s]7748it [00:02, 2863.58it/s]7261it [00:02, 2402.71it/s]7509it [00:02, 2337.65it/s]8136it [00:02, 3109.57it/s]8125it [00:02, 3091.54it/s]7584it [00:02, 2556.09it/s]7882it [00:02, 2649.85it/s]8481it [00:02, 3145.42it/s]8471it [00:02, 3124.25it/s]7957it [00:02, 2830.48it/s]8202it [00:02, 2758.70it/s]8856it [00:02, 3309.88it/s]8849it [00:02, 3299.99it/s]8292it [00:02, 2881.70it/s]8560it [00:02, 2967.45it/s]9206it [00:02, 3278.23it/s]9199it [00:02, 3271.09it/s]8650it [00:02, 3061.56it/s]8921it [00:02, 3138.40it/s]9579it [00:02, 3403.84it/s]9568it [00:02, 3385.42it/s]9017it [00:02, 3224.64it/s]9260it [00:02, 3128.91it/s]9930it [00:02, 3294.45it/s]9918it [00:02, 3275.88it/s]9362it [00:02, 3142.89it/s]9612it [00:03, 3237.30it/s]10293it [00:03, 3386.98it/s]10289it [00:03, 3395.89it/s]9724it [00:03, 3270.11it/s]9949it [00:03, 3196.79it/s]10673it [00:03, 3504.17it/s]10665it [00:03, 3498.86it/s]10063it [00:03, 3215.12it/s]10321it [00:03, 3344.44it/s]11028it [00:03, 3423.85it/s]11020it [00:03, 3411.20it/s]10430it [00:03, 3341.90it/s]10683it [00:03, 3421.90it/s]11402it [00:03, 3514.28it/s]11394it [00:03, 3504.26it/s]10771it [00:03, 3277.10it/s]11031it [00:03, 3333.40it/s]11757it [00:03, 3430.63it/s]11748it [00:03, 3419.89it/s]11136it [00:03, 3382.46it/s]11396it [00:03, 3422.30it/s]12139it [00:03, 3541.66it/s]12128it [00:03, 3527.61it/s]11503it [00:03, 3463.92it/s]11742it [00:03, 3235.06it/s]12496it [00:03, 3429.58it/s]12483it [00:03, 3421.17it/s]11853it [00:03, 3226.12it/s]12111it [00:03, 3361.02it/s]12870it [00:03, 3518.57it/s]12854it [00:03, 3501.68it/s]12206it [00:03, 3309.16it/s]12451it [00:03, 3202.36it/s]13206it [00:03, 3493.35it/s]13224it [00:03, 3488.13it/s]13368it [00:03, 3390.20it/s]
12542it [00:03, 3190.55it/s]13368it [00:03, 3370.62it/s]
2022-07-18 16:35:51 | INFO | root | success load 13368 data
2022-07-18 16:35:51 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-18 16:35:51 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-18 16:35:51 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-18 16:35:51 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-18 16:35:51 | INFO | transformer.tokenization_utils | loading file None
2022-07-18 16:35:51 | INFO | transformer.tokenization_utils | loading file None
2022-07-18 16:35:51 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
12814it [00:03, 3320.93it/s]12909it [00:04, 3325.04it/s]13185it [00:04, 3431.49it/s]13368it [00:04, 3216.93it/s]
13245it [00:04, 3228.72it/s]13368it [00:04, 3190.95it/s]
2022-07-18 16:35:51 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-18 16:35:51 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-18 16:35:51 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-18 16:35:55 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-18 16:35:55 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-18 16:35:55 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-18 16:35:55 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-18 16:35:55 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
2022-07-18 16:35:57 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-18 16:35:57 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-07-18 16:35:57 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-07-18 16:35:57 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-18 16:35:57 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 142456835)
2022-07-18 16:35:57 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-07-18 16:35:57 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-18 16:35:57 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-18 16:35:57 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-18 16:35:59 | INFO | fairseq.trainer | loaded checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_last.pt (epoch 4 @ 4486 updates)
2022-07-18 16:35:59 | INFO | fairseq.trainer | loading train data for epoch 4

Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]2022-07-18 16:35:59 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-ACL-bert-cased-510/train.source-target.source
2022-07-18 16:35:59 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-ACL-bert-cased-510/train.source-target.target
2022-07-18 16:35:59 | INFO | fairseq.tasks.translation | ../data-bin-ACL-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]379it [00:00, 3781.51it/s]388it [00:00, 3874.61it/s]775it [00:00, 3881.42it/s]778it [00:00, 3886.20it/s]1164it [00:00, 3645.70it/s]1167it [00:00, 3545.14it/s]1531it [00:00, 3598.61it/s]1549it [00:00, 3645.59it/s]1892it [00:00, 3532.50it/s]1917it [00:00, 3544.33it/s]2290it [00:00, 3677.73it/s]2308it [00:00, 3662.76it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]2670it [00:00, 3568.72it/s]2677it [00:00, 3538.47it/s]357it [00:00, 3569.40it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]3070it [00:00, 3699.08it/s]3074it [00:00, 3668.65it/s]717it [00:00, 3582.63it/s]376it [00:00, 3756.11it/s]3463it [00:00, 3767.36it/s]3443it [00:00, 3564.37it/s]1076it [00:00, 3376.68it/s]763it [00:00, 3817.79it/s]3841it [00:01, 3642.79it/s]3832it [00:01, 3659.58it/s]1443it [00:00, 3486.06it/s]1145it [00:00, 3450.25it/s]4227it [00:01, 3703.98it/s]4217it [00:01, 3711.66it/s]1815it [00:00, 3566.70it/s]1523it [00:00, 3570.25it/s]4599it [00:01, 3601.76it/s]4590it [00:01, 3590.89it/s]2173it [00:00, 3381.96it/s]1884it [00:00, 3380.85it/s]4961it [00:01, 3598.07it/s]4952it [00:01, 3597.35it/s]2525it [00:00, 3424.52it/s]2260it [00:00, 3498.43it/s]5322it [00:01, 3496.03it/s]5313it [00:01, 3481.53it/s]2870it [00:00, 3299.55it/s]2644it [00:00, 3602.72it/s]5711it [00:01, 3609.56it/s]5698it [00:01, 3587.29it/s]3232it [00:00, 3394.16it/s]3007it [00:00, 3496.05it/s]6074it [00:01, 3538.16it/s]6059it [00:01, 3512.57it/s]3574it [00:01, 3310.78it/s]3380it [00:00, 3564.68it/s]6466it [00:01, 3648.68it/s]6439it [00:01, 3593.90it/s]3916it [00:01, 3341.30it/s]3739it [00:01, 3450.86it/s]6854it [00:01, 3714.12it/s]6801it [00:01, 3503.70it/s]4255it [00:01, 3355.42it/s]4125it [00:01, 3568.22it/s]4592it [00:01, 3214.42it/s]4484it [00:01, 3352.50it/s]4970it [00:01, 3376.37it/s]4864it [00:01, 3476.55it/s]5310it [00:01, 3263.68it/s]5215it [00:01, 3319.83it/s]5677it [00:01, 3379.59it/s]5581it [00:01, 3413.12it/s]6030it [00:01, 3300.92it/s]5965it [00:01, 3534.21it/s]6408it [00:01, 3435.90it/s]6322it [00:01, 3430.87it/s]7227it [00:02, 1253.72it/s]6776it [00:01, 3503.91it/s]6708it [00:01, 3551.52it/s]7153it [00:02, 1156.83it/s]7628it [00:02, 1596.02it/s]7549it [00:02, 1492.63it/s]7969it [00:02, 1869.40it/s]7880it [00:02, 1757.90it/s]8366it [00:02, 2241.04it/s]8262it [00:02, 2113.58it/s]8710it [00:03, 2471.40it/s]8593it [00:03, 2348.92it/s]9089it [00:03, 2764.35it/s]8964it [00:03, 2647.51it/s]9460it [00:03, 2899.27it/s]9352it [00:03, 2940.99it/s]9864it [00:03, 3184.30it/s]9708it [00:03, 3043.33it/s]10268it [00:03, 3408.16it/s]10105it [00:03, 3286.92it/s]7128it [00:02, 1054.50it/s]10643it [00:03, 3413.01it/s]10469it [00:03, 3321.17it/s]7066it [00:02, 1033.81it/s]7510it [00:02, 1363.03it/s]11043it [00:03, 3574.10it/s]10855it [00:03, 3469.68it/s]7457it [00:02, 1342.81it/s]7820it [00:03, 1601.81it/s]11419it [00:03, 3552.13it/s]11221it [00:03, 3438.36it/s]8194it [00:03, 1952.57it/s]11818it [00:03, 3673.56it/s]7780it [00:03, 1561.68it/s]11614it [00:03, 3576.71it/s]8561it [00:03, 2278.07it/s]8169it [00:03, 1926.74it/s]12195it [00:03, 3586.51it/s]11982it [00:04, 3441.12it/s]8897it [00:03, 2488.65it/s]8550it [00:03, 2271.43it/s]12578it [00:04, 3655.06it/s]12382it [00:04, 3598.24it/s]9258it [00:03, 2748.64it/s]12949it [00:04, 3593.20it/s]8890it [00:03, 2423.24it/s]9599it [00:03, 2879.89it/s]12750it [00:04, 3537.78it/s]13362it [00:04, 3746.48it/s]9278it [00:03, 2747.66it/s]9975it [00:03, 3108.70it/s]13166it [00:04, 3715.38it/s]13740it [00:04, 3693.43it/s]9623it [00:03, 2878.15it/s]13567it [00:04, 3799.65it/s]10324it [00:03, 3162.03it/s]14154it [00:04, 3821.31it/s]10020it [00:03, 3149.78it/s]10702it [00:03, 3330.34it/s]13951it [00:04, 3710.86it/s]14539it [00:04, 3749.51it/s]10376it [00:03, 3191.21it/s]11084it [00:04, 3467.69it/s]14362it [00:04, 3824.97it/s]14944it [00:04, 3836.13it/s]10755it [00:03, 3352.16it/s]14747it [00:04, 3704.59it/s]11446it [00:04, 3345.03it/s]11135it [00:04, 3476.49it/s]15341it [00:04, 3752.91it/s]15154it [00:04, 3809.29it/s]11831it [00:04, 3486.60it/s]15743it [00:04, 3829.47it/s]11499it [00:04, 3418.69it/s]12189it [00:04, 3424.63it/s]15537it [00:04, 3624.42it/s]11867it [00:04, 3492.30it/s]16128it [00:05, 3761.19it/s]12563it [00:04, 3513.00it/s]15928it [00:05, 3702.78it/s]12225it [00:04, 3381.85it/s]12920it [00:04, 3458.62it/s]12626it [00:04, 3557.34it/s]13314it [00:04, 3597.00it/s]12988it [00:04, 3472.15it/s]13677it [00:04, 3528.28it/s]13394it [00:04, 3639.55it/s]14071it [00:04, 3647.14it/s]13762it [00:04, 3581.44it/s]14472it [00:04, 3751.85it/s]14162it [00:04, 3701.51it/s]14849it [00:05, 3508.90it/s]14535it [00:04, 3538.43it/s]15246it [00:05, 3638.10it/s]14937it [00:05, 3675.36it/s]16506it [00:05, 1163.95it/s]15614it [00:05, 3466.06it/s]15323it [00:05, 3728.43it/s]16301it [00:05, 1161.33it/s]16907it [00:06, 1487.12it/s]15993it [00:05, 3555.17it/s]16709it [00:06, 1494.19it/s]15698it [00:05, 3541.81it/s]17244it [00:06, 1749.37it/s]17059it [00:06, 1774.65it/s]16076it [00:05, 3607.28it/s]17652it [00:06, 2136.10it/s]17458it [00:06, 2143.24it/s]18001it [00:06, 2395.45it/s]17861it [00:06, 2436.03it/s]18414it [00:06, 2767.64it/s]18273it [00:06, 2790.63it/s]18781it [00:06, 2902.50it/s]18673it [00:06, 3069.35it/s]19188it [00:06, 3188.11it/s]19050it [00:06, 3161.49it/s]19592it [00:06, 3408.03it/s]19453it [00:06, 3382.42it/s]19973it [00:06, 3345.64it/s]19830it [00:06, 3295.39it/s]20361it [00:06, 3489.21it/s]20228it [00:06, 3477.27it/s]20732it [00:07, 3463.36it/s]20598it [00:07, 3441.86it/s]21126it [00:07, 3594.11it/s]16352it [00:06, 936.33it/s] 20999it [00:07, 3596.54it/s]21497it [00:07, 3560.48it/s]16753it [00:06, 1233.86it/s]16440it [00:06, 942.88it/s] 21901it [00:07, 3695.87it/s]17091it [00:06, 1494.46it/s]21371it [00:07, 3523.41it/s]16841it [00:06, 1240.07it/s]17473it [00:06, 1838.43it/s]21777it [00:07, 3673.93it/s]22277it [00:07, 3622.32it/s]17172it [00:06, 1491.70it/s]17861it [00:06, 2190.99it/s]22686it [00:07, 3756.19it/s]22152it [00:07, 3587.39it/s]17555it [00:06, 1837.53it/s]18210it [00:06, 2413.71it/s]22552it [00:07, 3703.48it/s]23066it [00:07, 3655.40it/s]17930it [00:06, 2104.34it/s]18603it [00:07, 2745.05it/s]23435it [00:07, 3659.68it/s]22927it [00:07, 3571.63it/s]18318it [00:06, 2449.16it/s]23314it [00:07, 3654.28it/s]18960it [00:07, 2825.53it/s]23811it [00:07, 3542.92it/s]18707it [00:07, 2761.90it/s]23699it [00:07, 3710.38it/s]19356it [00:07, 3104.83it/s]24216it [00:07, 3685.03it/s]19067it [00:07, 2880.28it/s]24618it [00:08, 3781.07it/s]24073it [00:08, 3617.85it/s]19714it [00:07, 3138.44it/s]19465it [00:07, 3151.42it/s]24476it [00:08, 3734.53it/s]20105it [00:07, 3342.44it/s]24999it [00:08, 3672.30it/s]19828it [00:07, 3180.32it/s]25395it [00:08, 3754.09it/s]24852it [00:08, 3635.02it/s]20466it [00:07, 3284.33it/s]20212it [00:07, 3354.83it/s]25245it [00:08, 3718.46it/s]25773it [00:08, 3667.34it/s]20852it [00:07, 3442.03it/s]20574it [00:07, 3235.42it/s]26165it [00:08, 3739.40it/s]21229it [00:07, 3532.74it/s]25619it [00:08, 3615.48it/s]20958it [00:07, 3397.12it/s]26010it [00:08, 3697.61it/s]21593it [00:07, 3429.15it/s]26541it [00:08, 3582.28it/s]21313it [00:07, 3317.34it/s]21976it [00:08, 3541.65it/s]26923it [00:08, 3648.95it/s]26382it [00:08, 3525.73it/s]21681it [00:07, 3416.56it/s]27290it [00:08, 3600.16it/s]26772it [00:08, 3630.12it/s]22337it [00:08, 3468.93it/s]22070it [00:08, 3548.65it/s]27685it [00:08, 3698.76it/s]22734it [00:08, 3608.90it/s]27138it [00:08, 3569.56it/s]22432it [00:08, 3479.25it/s]28057it [00:09, 3635.71it/s]27535it [00:08, 3683.46it/s]23099it [00:08, 3516.60it/s]22825it [00:08, 3608.07it/s]23479it [00:08, 3596.28it/s]27940it [00:09, 3621.96it/s]23190it [00:08, 3507.83it/s]23842it [00:08, 3501.08it/s]23571it [00:08, 3592.86it/s]24223it [00:08, 3588.65it/s]23933it [00:08, 3453.98it/s]24614it [00:08, 3681.67it/s]24328it [00:08, 3594.47it/s]24984it [00:08, 3545.49it/s]24691it [00:08, 3422.99it/s]25362it [00:08, 3612.25it/s]25074it [00:08, 3535.88it/s]25725it [00:09, 3521.67it/s]25451it [00:08, 3600.39it/s]26114it [00:09, 3625.88it/s]25814it [00:09, 3519.57it/s]26479it [00:09, 3517.49it/s]26200it [00:09, 3615.98it/s]26871it [00:09, 3632.14it/s]26564it [00:09, 3488.75it/s]27236it [00:09, 3444.79it/s]26934it [00:09, 3546.82it/s]28422it [00:10, 830.66it/s] 27626it [00:09, 3571.83it/s]27291it [00:09, 3483.74it/s]28304it [00:10, 861.70it/s] 28803it [00:10, 1088.37it/s]28011it [00:09, 3471.70it/s]27663it [00:09, 3551.57it/s]28663it [00:10, 1101.77it/s]29113it [00:10, 1309.62it/s]28020it [00:09, 3407.36it/s]28991it [00:10, 1344.82it/s]29502it [00:10, 1659.80it/s]29391it [00:10, 1709.68it/s]29836it [00:10, 1932.05it/s]29724it [00:10, 1975.31it/s]30235it [00:10, 2318.61it/s]30112it [00:10, 2337.06it/s]30600it [00:10, 2557.77it/s]30511it [00:10, 2689.17it/s]30995it [00:11, 2876.46it/s]30875it [00:11, 2852.13it/s]31382it [00:11, 3120.17it/s]31259it [00:11, 3094.29it/s]31751it [00:11, 3200.52it/s]31623it [00:11, 3173.11it/s]32130it [00:11, 3356.48it/s]31995it [00:11, 3318.69it/s]32496it [00:11, 3350.82it/s]32356it [00:11, 3335.21it/s]32881it [00:11, 3489.22it/s]32738it [00:11, 3469.33it/s]33246it [00:11, 3458.57it/s]33101it [00:11, 3428.69it/s]33640it [00:11, 3594.52it/s]33496it [00:11, 3575.02it/s]34008it [00:11, 3559.32it/s]28361it [00:11, 730.12it/s] 33891it [00:11, 3510.88it/s]34402it [00:11, 3667.49it/s]28748it [00:11, 973.81it/s]34295it [00:11, 3658.54it/s]34798it [00:12, 3751.10it/s]29041it [00:11, 1164.19it/s]34684it [00:12, 3724.82it/s]28363it [00:11, 651.97it/s] 35177it [00:12, 3631.18it/s]29426it [00:11, 1497.66it/s]28734it [00:11, 872.88it/s]35546it [00:12, 3647.53it/s]35061it [00:12, 3527.84it/s]29760it [00:11, 1754.34it/s]29025it [00:11, 1063.50it/s]35452it [00:12, 3634.42it/s]35913it [00:12, 3484.29it/s]30136it [00:11, 2106.04it/s]29409it [00:11, 1390.65it/s]35820it [00:12, 3550.05it/s]36298it [00:12, 3586.86it/s]30524it [00:11, 2460.61it/s]29761it [00:11, 1675.42it/s]36208it [00:12, 3642.68it/s]36660it [00:12, 3514.98it/s]30874it [00:11, 2642.12it/s]30133it [00:11, 2020.48it/s]36575it [00:12, 3522.72it/s]37050it [00:12, 3625.51it/s]31257it [00:12, 2924.17it/s]30522it [00:11, 2383.60it/s]36968it [00:12, 3638.22it/s]37415it [00:12, 3543.78it/s]31611it [00:12, 2996.97it/s]30873it [00:12, 2574.80it/s]37807it [00:12, 3650.20it/s]37335it [00:12, 3539.24it/s]31999it [00:12, 3226.60it/s]31246it [00:12, 2842.88it/s]37713it [00:12, 3607.70it/s]38174it [00:12, 3587.85it/s]32356it [00:12, 3206.45it/s]31597it [00:12, 2933.92it/s]38534it [00:13, 3589.88it/s]38091it [00:13, 3520.98it/s]32725it [00:12, 3337.77it/s]31978it [00:12, 3159.84it/s]38903it [00:13, 3618.08it/s]38452it [00:13, 3545.80it/s]33100it [00:12, 3451.06it/s]32331it [00:12, 3132.71it/s]39266it [00:13, 3563.70it/s]38832it [00:13, 3617.14it/s]33459it [00:12, 3338.92it/s]32710it [00:12, 3309.01it/s]39655it [00:13, 3659.23it/s]39195it [00:13, 3519.18it/s]33847it [00:12, 3488.80it/s]33061it [00:12, 3341.38it/s]40022it [00:13, 3562.87it/s]39595it [00:13, 3656.37it/s]34204it [00:12, 3420.26it/s]33410it [00:12, 3301.29it/s]40419it [00:13, 3679.86it/s]39962it [00:13, 3570.73it/s]34589it [00:12, 3542.26it/s]33799it [00:12, 3465.99it/s]40788it [00:13, 3588.05it/s]40348it [00:13, 3652.03it/s]34948it [00:13, 3439.29it/s]34154it [00:12, 3415.49it/s]41188it [00:13, 3705.37it/s]40715it [00:13, 3544.09it/s]35323it [00:13, 3527.60it/s]34525it [00:13, 3499.00it/s]41560it [00:13, 3584.47it/s]41108it [00:13, 3653.26it/s]35679it [00:13, 3399.61it/s]34880it [00:13, 3413.27it/s]41943it [00:14, 3647.61it/s]41475it [00:14, 3486.66it/s]36061it [00:13, 3509.55it/s]35254it [00:13, 3505.66it/s]42328it [00:14, 3702.59it/s]41864it [00:14, 3599.30it/s]36431it [00:13, 3562.53it/s]35608it [00:13, 3440.73it/s]42700it [00:14, 3622.24it/s]42262it [00:14, 3708.85it/s]36790it [00:13, 3460.34it/s]35955it [00:13, 3381.24it/s]43079it [00:14, 3668.39it/s]42635it [00:14, 3613.39it/s]37164it [00:13, 3538.91it/s]36339it [00:13, 3510.84it/s]43023it [00:14, 3688.95it/s]37520it [00:13, 3437.13it/s]36692it [00:13, 3412.06it/s]37899it [00:13, 3537.56it/s]37076it [00:13, 3534.11it/s]38255it [00:14, 3451.00it/s]37431it [00:13, 3322.71it/s]38602it [00:14, 3431.61it/s]37815it [00:14, 3465.99it/s]38996it [00:14, 3576.72it/s]38165it [00:14, 3298.03it/s]39355it [00:14, 3421.84it/s]38528it [00:14, 3389.89it/s]39736it [00:14, 3530.93it/s]38915it [00:14, 3525.77it/s]40091it [00:14, 3415.21it/s]39271it [00:14, 3451.25it/s]40475it [00:14, 3535.77it/s]39652it [00:14, 3553.50it/s]40831it [00:14, 3451.63it/s]40010it [00:14, 3439.66it/s]41227it [00:14, 3595.11it/s]40397it [00:14, 3561.29it/s]41589it [00:14, 3361.60it/s]40756it [00:14, 3425.57it/s]41979it [00:15, 3510.43it/s]43447it [00:15, 729.30it/s] 41145it [00:15, 3556.85it/s]42361it [00:15, 3390.87it/s]43833it [00:15, 969.60it/s]41521it [00:15, 3370.04it/s]42736it [00:15, 3489.65it/s]44172it [00:16, 1208.56it/s]41908it [00:15, 3506.95it/s]43114it [00:15, 3570.92it/s]44573it [00:16, 1554.97it/s]42299it [00:15, 3619.84it/s]44950it [00:16, 1859.80it/s]43394it [00:16, 629.39it/s] 42665it [00:15, 3510.23it/s]45349it [00:16, 2231.47it/s]43786it [00:16, 847.42it/s]43035it [00:15, 3563.01it/s]45741it [00:16, 2568.14it/s]44118it [00:16, 1061.22it/s]44522it [00:16, 1388.67it/s]46109it [00:16, 2754.14it/s]44880it [00:16, 1669.07it/s]46495it [00:16, 3015.74it/s]45235it [00:16, 1971.94it/s]46862it [00:16, 3061.05it/s]45624it [00:16, 2329.86it/s]47260it [00:16, 3290.05it/s]45981it [00:16, 2549.08it/s]47625it [00:16, 3274.55it/s]46365it [00:17, 2843.86it/s]48024it [00:17, 3466.51it/s]46725it [00:17, 2968.78it/s]48391it [00:17, 3449.80it/s]47120it [00:17, 3218.32it/s]48792it [00:17, 3605.89it/s]47485it [00:17, 3243.45it/s]49164it [00:17, 3523.15it/s]47879it [00:17, 3429.95it/s]49560it [00:17, 3645.61it/s]48245it [00:17, 3389.24it/s]49948it [00:17, 3712.64it/s]48617it [00:17, 3480.05it/s]50324it [00:17, 3546.30it/s]49006it [00:17, 3595.13it/s]50705it [00:17, 3620.69it/s]49375it [00:17, 3508.70it/s]51071it [00:17, 3542.25it/s]43474it [00:17, 584.58it/s] 49770it [00:17, 3633.86it/s]51461it [00:18, 3642.85it/s]43849it [00:17, 784.02it/s]50139it [00:18, 3546.51it/s]51828it [00:18, 3570.73it/s]44177it [00:17, 990.06it/s]50524it [00:18, 3631.06it/s]43394it [00:17, 586.51it/s] 52216it [00:18, 3657.82it/s]44565it [00:17, 1294.91it/s]50891it [00:18, 3549.94it/s]43777it [00:17, 792.54it/s]52584it [00:18, 3567.20it/s]44946it [00:17, 1623.96it/s]51286it [00:18, 3664.92it/s]44109it [00:17, 997.17it/s]52979it [00:18, 3675.35it/s]45290it [00:17, 1858.39it/s]51655it [00:18, 3546.15it/s]44492it [00:17, 1296.40it/s]53350it [00:18, 3571.43it/s]45666it [00:17, 2199.92it/s]52017it [00:18, 3567.29it/s]44870it [00:17, 1621.90it/s]53709it [00:18, 3558.52it/s]46009it [00:18, 2381.56it/s]52405it [00:18, 3655.95it/s]45211it [00:17, 1866.62it/s]54107it [00:18, 3680.61it/s]46388it [00:18, 2692.59it/s]52772it [00:18, 3557.59it/s]45595it [00:18, 2223.21it/s]54477it [00:18, 3601.93it/s]46733it [00:18, 2809.28it/s]53161it [00:18, 3651.42it/s]54869it [00:18, 3694.05it/s]45941it [00:18, 2388.50it/s]47120it [00:18, 3072.06it/s]53528it [00:18, 3529.77it/s]46323it [00:18, 2703.10it/s]55240it [00:19, 3587.79it/s]47471it [00:18, 3128.66it/s]53920it [00:19, 3640.26it/s]55621it [00:19, 3650.30it/s]46669it [00:18, 2830.84it/s]47853it [00:18, 3314.31it/s]54286it [00:19, 3566.04it/s]47040it [00:18, 3050.13it/s]55988it [00:19, 3565.40it/s]48213it [00:18, 3391.61it/s]54676it [00:19, 3662.41it/s]47415it [00:18, 3234.35it/s]56380it [00:19, 3667.77it/s]48570it [00:18, 3356.11it/s]55044it [00:19, 3465.49it/s]47771it [00:18, 3212.05it/s]56748it [00:19, 3525.19it/s]48946it [00:18, 3469.16it/s]55425it [00:19, 3560.29it/s]48149it [00:18, 3367.57it/s]57137it [00:19, 3627.52it/s]49302it [00:18, 3379.42it/s]55801it [00:19, 3477.38it/s]57533it [00:19, 3722.85it/s]48503it [00:18, 3328.01it/s]49677it [00:19, 3483.46it/s]56199it [00:19, 3619.63it/s]48882it [00:18, 3456.14it/s]57907it [00:19, 3624.80it/s]50031it [00:19, 3416.15it/s]56583it [00:19, 3680.61it/s]49237it [00:19, 3395.56it/s]58295it [00:19, 3696.90it/s]50414it [00:19, 3533.44it/s]56953it [00:19, 3581.15it/s]49630it [00:19, 3547.27it/s]58666it [00:19, 3588.40it/s]50800it [00:19, 3627.43it/s]57348it [00:20, 3686.16it/s]59051it [00:20, 3661.86it/s]49991it [00:19, 3361.89it/s]51166it [00:19, 3471.00it/s]57719it [00:20, 3578.09it/s]50360it [00:19, 3453.72it/s]59419it [00:20, 3559.56it/s]51546it [00:19, 3563.96it/s]58104it [00:20, 3654.43it/s]50746it [00:19, 3567.78it/s]59803it [00:20, 3639.98it/s]51905it [00:19, 3452.14it/s]58471it [00:20, 3467.62it/s]51107it [00:19, 3413.45it/s]60169it [00:20, 3470.24it/s]52266it [00:19, 3495.11it/s]58857it [00:20, 3577.46it/s]51486it [00:19, 3519.38it/s]60560it [00:20, 3585.45it/s]52618it [00:19, 3417.08it/s]59218it [00:20, 3488.54it/s]60921it [00:20, 3516.81it/s]51842it [00:19, 3410.09it/s]53002it [00:19, 3537.39it/s]59596it [00:20, 3569.46it/s]61323it [00:20, 3659.30it/s]52228it [00:19, 3537.14it/s]53358it [00:20, 3448.35it/s]59986it [00:20, 3664.49it/s]61701it [00:20, 3694.21it/s]52585it [00:20, 3434.16it/s]53728it [00:20, 3519.04it/s]60355it [00:20, 3529.76it/s]52973it [00:20, 3560.27it/s]54117it [00:20, 3625.79it/s]60748it [00:21, 3642.15it/s]53332it [00:20, 3539.60it/s]54481it [00:20, 3405.79it/s]61115it [00:21, 3532.86it/s]53688it [00:20, 3433.17it/s]54861it [00:20, 3516.46it/s]61478it [00:21, 3560.54it/s]54063it [00:20, 3522.75it/s]55216it [00:20, 3327.57it/s]54417it [00:20, 3381.85it/s]55590it [00:20, 3440.46it/s]54801it [00:20, 3510.73it/s]55938it [00:20, 3371.15it/s]55155it [00:20, 3390.11it/s]56326it [00:20, 3514.84it/s]55529it [00:20, 3488.38it/s]56696it [00:21, 3566.20it/s]55880it [00:20, 3405.79it/s]57055it [00:21, 3491.67it/s]56262it [00:21, 3524.52it/s]57422it [00:21, 3542.53it/s]56628it [00:21, 3562.66it/s]57778it [00:21, 3453.84it/s]56986it [00:21, 3454.00it/s]58131it [00:21, 3475.66it/s]57362it [00:21, 3540.33it/s]58480it [00:21, 3354.06it/s]57718it [00:21, 3439.07it/s]58860it [00:21, 3479.38it/s]58091it [00:21, 3522.37it/s]59230it [00:21, 3392.32it/s]58445it [00:21, 3417.40it/s]59607it [00:21, 3498.64it/s]58827it [00:21, 3531.56it/s]59983it [00:21, 3573.82it/s]59210it [00:21, 3616.00it/s]60342it [00:22, 3442.55it/s]59573it [00:22, 3343.01it/s]60720it [00:22, 3537.35it/s]59956it [00:22, 3477.45it/s]62072it [00:22, 526.17it/s] 61076it [00:22, 3432.55it/s]60308it [00:22, 3313.88it/s]62459it [00:23, 714.28it/s]61449it [00:22, 3517.09it/s]60689it [00:22, 3451.39it/s]62793it [00:23, 909.62it/s]61803it [00:22, 3415.13it/s]63188it [00:23, 1201.60it/s]61038it [00:22, 3344.35it/s]61424it [00:22, 3488.97it/s]63525it [00:23, 1455.75it/s]61836it [00:23, 502.19it/s] 63920it [00:23, 1818.71it/s]61776it [00:22, 3388.50it/s]62234it [00:23, 694.08it/s]64310it [00:23, 2177.90it/s]62590it [00:23, 897.72it/s]64672it [00:23, 2429.77it/s]62979it [00:23, 1178.66it/s]65042it [00:23, 2705.72it/s]63370it [00:23, 1501.33it/s]65402it [00:23, 2829.55it/s]63719it [00:23, 1757.09it/s]65785it [00:24, 3077.10it/s]64101it [00:24, 2104.96it/s]66143it [00:24, 3158.25it/s]64452it [00:24, 2351.29it/s]66523it [00:24, 3328.64it/s]64843it [00:24, 2685.74it/s]66883it [00:24, 3339.10it/s]65202it [00:24, 2840.29it/s]67279it [00:24, 3512.19it/s]65581it [00:24, 3075.11it/s]67668it [00:24, 3619.17it/s]65950it [00:24, 3149.38it/s]68041it [00:24, 3549.37it/s]66331it [00:24, 3324.56it/s]68418it [00:24, 3611.16it/s]66715it [00:24, 3458.92it/s]68785it [00:24, 3480.66it/s]67082it [00:24, 3355.52it/s]69171it [00:24, 3580.48it/s]67470it [00:24, 3500.55it/s]69533it [00:25, 3509.76it/s]67832it [00:25, 3462.69it/s]69929it [00:25, 3636.51it/s]68216it [00:25, 3567.98it/s]70296it [00:25, 3540.09it/s]68579it [00:25, 3494.44it/s]70691it [00:25, 3656.42it/s]68960it [00:25, 3580.68it/s]71061it [00:25, 3541.15it/s]69322it [00:25, 3500.20it/s]62146it [00:24, 459.75it/s] 71454it [00:25, 3651.94it/s]69707it [00:25, 3600.51it/s]62534it [00:24, 638.24it/s]71840it [00:25, 3712.22it/s]70077it [00:25, 3629.22it/s]62843it [00:25, 808.29it/s]72213it [00:25, 3511.72it/s]70442it [00:25, 3503.94it/s]63207it [00:25, 1063.55it/s]72601it [00:25, 3611.51it/s]70826it [00:25, 3599.55it/s]63522it [00:25, 1301.15it/s]72965it [00:26, 3557.65it/s]63910it [00:25, 1662.10it/s]71188it [00:26, 3504.75it/s]73352it [00:26, 3647.14it/s]64291it [00:25, 2019.70it/s]71572it [00:26, 3598.99it/s]73719it [00:26, 3587.82it/s]62118it [00:25, 401.20it/s] 64641it [00:25, 2269.12it/s]71934it [00:26, 3498.46it/s]74082it [00:26, 3598.76it/s]62507it [00:25, 560.87it/s]65021it [00:25, 2593.72it/s]72319it [00:26, 3597.35it/s]74443it [00:26, 3525.06it/s]62794it [00:25, 702.06it/s]72681it [00:26, 3509.36it/s]65374it [00:25, 2675.26it/s]74831it [00:26, 3627.34it/s]63174it [00:25, 951.44it/s]73077it [00:26, 3636.67it/s]65747it [00:25, 2927.47it/s]75195it [00:26, 3630.36it/s]63499it [00:25, 1178.94it/s]73443it [00:26, 3594.50it/s]66093it [00:26, 2943.65it/s]75559it [00:26, 3530.32it/s]63858it [00:25, 1486.21it/s]73804it [00:26, 3523.49it/s]66464it [00:26, 3141.57it/s]75948it [00:26, 3632.70it/s]64237it [00:26, 1841.23it/s]74199it [00:26, 3646.64it/s]66845it [00:26, 3322.37it/s]64578it [00:26, 2099.48it/s]76313it [00:26, 3513.59it/s]74565it [00:26, 3543.21it/s]67200it [00:26, 3300.44it/s]64960it [00:26, 2448.55it/s]76696it [00:27, 3603.08it/s]74950it [00:27, 3629.34it/s]67576it [00:26, 3428.54it/s]65309it [00:26, 2625.14it/s]77058it [00:27, 3508.64it/s]75315it [00:27, 3504.44it/s]67931it [00:26, 3380.20it/s]65666it [00:26, 2850.28it/s]77448it [00:27, 3621.21it/s]75701it [00:27, 3604.18it/s]68302it [00:26, 3473.78it/s]77812it [00:27, 3535.61it/s]66019it [00:26, 2869.32it/s]76063it [00:27, 3493.75it/s]68656it [00:26, 3375.96it/s]78200it [00:27, 3634.10it/s]66399it [00:26, 3106.98it/s]76447it [00:27, 3592.17it/s]68999it [00:26, 3326.51it/s]78565it [00:27, 3618.79it/s]66775it [00:26, 3279.79it/s]76808it [00:27, 3556.45it/s]69372it [00:26, 3439.22it/s]78928it [00:27, 3524.29it/s]67129it [00:26, 3198.29it/s]77165it [00:27, 3458.87it/s]69719it [00:27, 3314.36it/s]79315it [00:27, 3623.65it/s]67507it [00:26, 3356.44it/s]77557it [00:27, 3590.26it/s]70102it [00:27, 3460.72it/s]79679it [00:27, 3524.34it/s]67857it [00:27, 3301.63it/s]77918it [00:27, 3500.23it/s]70451it [00:27, 3392.72it/s]80071it [00:27, 3636.77it/s]68237it [00:27, 3441.02it/s]78293it [00:28, 3571.86it/s]70830it [00:27, 3504.52it/s]80436it [00:28, 3524.45it/s]68589it [00:27, 3362.35it/s]78652it [00:28, 3496.44it/s]71183it [00:27, 3411.59it/s]80827it [00:28, 3634.51it/s]68962it [00:27, 3466.27it/s]79037it [00:28, 3596.17it/s]71549it [00:27, 3475.15it/s]81192it [00:28, 3520.61it/s]69335it [00:27, 3541.51it/s]79398it [00:28, 3486.31it/s]71899it [00:27, 3407.32it/s]81577it [00:28, 3614.92it/s]69693it [00:27, 3238.48it/s]79753it [00:28, 3502.07it/s]72265it [00:27, 3478.54it/s]81940it [00:28, 3617.03it/s]70069it [00:27, 3380.40it/s]80138it [00:28, 3601.19it/s]72638it [00:27, 3550.94it/s]82303it [00:28, 3508.36it/s]70414it [00:27, 3320.49it/s]80500it [00:28, 3520.60it/s]72995it [00:28, 3442.19it/s]82688it [00:28, 3606.96it/s]70793it [00:27, 3452.50it/s]80886it [00:28, 3618.84it/s]73371it [00:28, 3531.40it/s]83050it [00:28, 3522.87it/s]71143it [00:28, 3354.55it/s]81249it [00:28, 3475.47it/s]73726it [00:28, 3458.39it/s]83440it [00:28, 3629.12it/s]71528it [00:28, 3493.69it/s]81630it [00:28, 3569.28it/s]74119it [00:28, 3594.42it/s]83805it [00:29, 3516.47it/s]71899it [00:28, 3349.29it/s]81989it [00:29, 3486.18it/s]74480it [00:28, 3383.24it/s]84191it [00:29, 3615.22it/s]72267it [00:28, 3439.98it/s]82370it [00:29, 3578.74it/s]74855it [00:28, 3485.99it/s]84554it [00:29, 3513.73it/s]72651it [00:28, 3552.36it/s]82741it [00:29, 3614.90it/s]75226it [00:28, 3548.54it/s]84919it [00:29, 3552.80it/s]73009it [00:28, 3358.96it/s]83104it [00:29, 3486.46it/s]75584it [00:28, 3377.31it/s]85297it [00:29, 3616.87it/s]73380it [00:28, 3457.15it/s]83492it [00:29, 3597.45it/s]75953it [00:28, 3464.03it/s]73729it [00:28, 3404.10it/s]83854it [00:29, 3508.30it/s]76302it [00:28, 3356.39it/s]74121it [00:28, 3550.33it/s]84241it [00:29, 3610.31it/s]76678it [00:29, 3470.05it/s]74479it [00:29, 3442.35it/s]84604it [00:29, 3494.22it/s]77028it [00:29, 3378.35it/s]74859it [00:29, 3542.92it/s]84990it [00:29, 3598.11it/s]77411it [00:29, 3505.39it/s]75216it [00:29, 3519.28it/s]77779it [00:29, 3385.08it/s]75570it [00:29, 3401.96it/s]78160it [00:29, 3502.67it/s]75942it [00:29, 3492.93it/s]78521it [00:29, 3531.14it/s]76293it [00:29, 3314.61it/s]78876it [00:29, 3373.16it/s]76672it [00:29, 3446.53it/s]79252it [00:29, 3480.50it/s]77020it [00:29, 3355.29it/s]79603it [00:29, 3396.81it/s]77395it [00:29, 3465.16it/s]79986it [00:30, 3519.55it/s]77776it [00:29, 3563.23it/s]80340it [00:30, 3424.58it/s]78135it [00:30, 3418.02it/s]80709it [00:30, 3499.54it/s]78515it [00:30, 3525.36it/s]81084it [00:30, 3570.24it/s]78870it [00:30, 3403.87it/s]81443it [00:30, 3403.61it/s]79224it [00:30, 3442.61it/s]81811it [00:30, 3480.79it/s]79570it [00:30, 3344.96it/s]82162it [00:30, 3399.13it/s]79952it [00:30, 3480.13it/s]82538it [00:30, 3502.33it/s]80302it [00:30, 3386.65it/s]82890it [00:30, 3407.37it/s]80688it [00:30, 3521.15it/s]83267it [00:30, 3508.95it/s]81052it [00:30, 3553.54it/s]83642it [00:31, 3577.27it/s]81409it [00:31, 3313.48it/s]84001it [00:31, 3360.63it/s]81785it [00:31, 3436.34it/s]84363it [00:31, 3433.12it/s]85660it [00:32, 438.95it/s] 82133it [00:31, 3291.22it/s]84710it [00:31, 3284.96it/s]86039it [00:32, 601.14it/s]82491it [00:31, 3371.53it/s]85084it [00:31, 3410.96it/s]86371it [00:32, 776.90it/s]82832it [00:31, 3306.66it/s]86758it [00:32, 1038.36it/s]83211it [00:31, 3442.09it/s]87091it [00:32, 1282.14it/s]83589it [00:31, 3536.31it/s]87487it [00:32, 1638.55it/s]83945it [00:31, 3421.86it/s]87872it [00:32, 1991.31it/s]84297it [00:31, 3449.00it/s]85352it [00:32, 411.58it/s] 88232it [00:32, 2258.40it/s]84644it [00:31, 3334.78it/s]85737it [00:32, 567.21it/s]88624it [00:32, 2602.90it/s]85017it [00:32, 3447.51it/s]86104it [00:32, 755.68it/s]88988it [00:32, 2739.77it/s]86427it [00:32, 953.15it/s]85364it [00:32, 3268.29it/s]89378it [00:33, 3016.96it/s]86805it [00:33, 1242.32it/s]89740it [00:33, 3098.70it/s]87142it [00:33, 1506.26it/s]90124it [00:33, 3291.27it/s]87532it [00:33, 1873.70it/s]90486it [00:33, 3279.31it/s]87882it [00:33, 2145.58it/s]90864it [00:33, 3414.59it/s]88269it [00:33, 2495.06it/s]91239it [00:33, 3507.90it/s]88656it [00:33, 2803.02it/s]91603it [00:33, 3412.77it/s]89022it [00:33, 2933.77it/s]91976it [00:33, 3501.72it/s]89389it [00:33, 3119.19it/s]92334it [00:33, 3364.12it/s]89748it [00:33, 3167.57it/s]92706it [00:33, 3464.33it/s]90128it [00:34, 3337.89it/s]93058it [00:34, 3370.18it/s]90487it [00:34, 3281.63it/s]93437it [00:34, 3488.55it/s]90862it [00:34, 3409.43it/s]93811it [00:34, 3411.78it/s]91221it [00:34, 3342.54it/s]94190it [00:34, 3517.23it/s]91597it [00:34, 3458.84it/s]94572it [00:34, 3602.37it/s]91973it [00:34, 3544.73it/s]94935it [00:34, 3465.47it/s]92334it [00:34, 3417.47it/s]95314it [00:34, 3557.66it/s]92708it [00:34, 3507.57it/s]95672it [00:34, 3460.47it/s]93063it [00:34, 3414.54it/s]96051it [00:34, 3551.87it/s]85428it [00:34, 397.48it/s] 93443it [00:34, 3523.33it/s]85811it [00:34, 553.38it/s]96408it [00:35, 3460.19it/s]93798it [00:35, 3422.47it/s]86181it [00:34, 745.14it/s]96783it [00:35, 3541.73it/s]94167it [00:35, 3498.58it/s]86492it [00:34, 931.95it/s]97163it [00:35, 3616.17it/s]94549it [00:35, 3591.02it/s]86869it [00:34, 1222.41it/s]97526it [00:35, 3502.27it/s]94910it [00:35, 3474.37it/s]87198it [00:34, 1477.66it/s]97909it [00:35, 3595.01it/s]95286it [00:35, 3555.29it/s]87581it [00:34, 1838.09it/s]98270it [00:35, 3492.19it/s]95644it [00:35, 3445.68it/s]87929it [00:34, 2104.10it/s]98641it [00:35, 3554.40it/s]96017it [00:35, 3525.69it/s]88310it [00:35, 2447.99it/s]98998it [00:35, 3463.62it/s]96372it [00:35, 3428.97it/s]88699it [00:35, 2766.02it/s]99382it [00:35, 3570.30it/s]96751it [00:35, 3530.42it/s]89060it [00:35, 2876.70it/s]99741it [00:36, 3472.87it/s]85694it [00:35, 356.76it/s] 97106it [00:36, 3430.84it/s]89440it [00:35, 3106.84it/s]100116it [00:36, 3551.31it/s]86076it [00:35, 502.13it/s]97481it [00:36, 3520.47it/s]89798it [00:35, 3146.23it/s]100475it [00:36, 3561.74it/s]86392it [00:35, 652.50it/s]97864it [00:36, 3609.43it/s]90175it [00:35, 3313.07it/s]100833it [00:36, 3435.69it/s]86772it [00:35, 887.76it/s]98227it [00:36, 3481.35it/s]90532it [00:35, 3262.53it/s]101215it [00:36, 3544.53it/s]87089it [00:35, 1103.00it/s]98606it [00:36, 3567.73it/s]90902it [00:35, 3382.20it/s]101571it [00:36, 3444.25it/s]87476it [00:35, 1436.74it/s]98965it [00:36, 3462.80it/s]91273it [00:35, 3472.80it/s]101950it [00:36, 3542.53it/s]87860it [00:35, 1790.10it/s]99333it [00:36, 3524.48it/s]91631it [00:36, 3368.08it/s]102306it [00:36, 3424.89it/s]88210it [00:35, 2054.35it/s]99687it [00:36, 3387.61it/s]91989it [00:36, 3419.99it/s]102684it [00:36, 3525.17it/s]88599it [00:36, 2414.44it/s]100065it [00:36, 3498.23it/s]92337it [00:36, 3327.63it/s]103051it [00:36, 3425.56it/s]88955it [00:36, 2588.16it/s]100446it [00:36, 3585.91it/s]92701it [00:36, 3414.05it/s]103431it [00:37, 3529.66it/s]89335it [00:36, 2869.38it/s]100807it [00:37, 3474.90it/s]93046it [00:36, 3317.39it/s]103812it [00:37, 3608.30it/s]89689it [00:36, 2965.28it/s]101178it [00:37, 3540.90it/s]93414it [00:36, 3419.79it/s]90070it [00:36, 3182.47it/s]104175it [00:37, 3466.03it/s]101534it [00:37, 3439.89it/s]93784it [00:36, 3499.03it/s]90443it [00:36, 3329.90it/s]104554it [00:37, 3557.99it/s]101915it [00:37, 3544.01it/s]94136it [00:36, 3383.47it/s]104912it [00:37, 3446.97it/s]90804it [00:36, 3237.11it/s]102271it [00:37, 3427.78it/s]94502it [00:36, 3460.58it/s]105290it [00:37, 3540.38it/s]91175it [00:36, 3365.09it/s]102644it [00:37, 3512.17it/s]94850it [00:36, 3361.02it/s]105646it [00:37, 3412.93it/s]91527it [00:36, 3284.08it/s]102997it [00:37, 3392.28it/s]95218it [00:37, 3451.58it/s]106025it [00:37, 3518.04it/s]91898it [00:36, 3400.42it/s]103377it [00:37, 3506.91it/s]95565it [00:37, 3352.19it/s]106405it [00:37, 3596.86it/s]92247it [00:37, 3282.37it/s]103750it [00:37, 3570.54it/s]95932it [00:37, 3441.97it/s]106767it [00:38, 3463.03it/s]92615it [00:37, 3393.75it/s]104109it [00:38, 3467.91it/s]96304it [00:37, 3521.21it/s]107140it [00:38, 3538.69it/s]92969it [00:37, 3297.69it/s]104486it [00:38, 3554.19it/s]96658it [00:37, 3407.55it/s]107496it [00:38, 3416.40it/s]93338it [00:37, 3407.94it/s]104843it [00:38, 3426.87it/s]97028it [00:37, 3482.97it/s]107878it [00:38, 3529.34it/s]93708it [00:37, 3491.00it/s]105220it [00:38, 3520.67it/s]97378it [00:37, 3354.80it/s]108233it [00:38, 3428.84it/s]94060it [00:37, 3340.59it/s]105574it [00:38, 3416.81it/s]97751it [00:37, 3460.87it/s]108618it [00:38, 3548.34it/s]94434it [00:37, 3453.35it/s]105952it [00:38, 3520.04it/s]98099it [00:37, 3363.54it/s]108975it [00:38, 3451.56it/s]94783it [00:37, 3345.69it/s]106330it [00:38, 3595.36it/s]98468it [00:38, 3455.63it/s]109347it [00:38, 3527.39it/s]95150it [00:37, 3436.87it/s]106691it [00:38, 3443.39it/s]98841it [00:38, 3534.02it/s]109723it [00:38, 3594.08it/s]95496it [00:38, 3333.43it/s]107071it [00:38, 3543.67it/s]99196it [00:38, 3415.61it/s]110084it [00:38, 3472.54it/s]95856it [00:38, 3409.28it/s]107428it [00:38, 3438.56it/s]99566it [00:38, 3494.84it/s]110463it [00:39, 3562.41it/s]96227it [00:38, 3495.80it/s]107809it [00:39, 3543.29it/s]99917it [00:38, 3390.07it/s]110821it [00:39, 3448.56it/s]96579it [00:38, 3379.27it/s]108166it [00:39, 3419.77it/s]100278it [00:38, 3451.00it/s]111196it [00:39, 3533.36it/s]96950it [00:38, 3473.42it/s]108550it [00:39, 3538.93it/s]100625it [00:38, 3362.17it/s]111551it [00:39, 3431.74it/s]97299it [00:38, 3360.51it/s]108906it [00:39, 3444.86it/s]100997it [00:38, 3464.35it/s]111931it [00:39, 3535.19it/s]97659it [00:38, 3420.07it/s]109282it [00:39, 3533.05it/s]101369it [00:38, 3369.80it/s]112291it [00:39, 3433.14it/s]98009it [00:38, 3325.30it/s]109661it [00:39, 3606.61it/s]101736it [00:38, 3454.29it/s]112670it [00:39, 3534.46it/s]98379it [00:38, 3431.89it/s]110023it [00:39, 3457.48it/s]102109it [00:39, 3532.93it/s]113041it [00:39, 3582.86it/s]98752it [00:38, 3517.04it/s]110401it [00:39, 3542.76it/s]102464it [00:39, 3401.88it/s]113401it [00:39, 3466.35it/s]99106it [00:39, 3366.63it/s]110758it [00:39, 3438.81it/s]102823it [00:39, 3453.43it/s]113777it [00:40, 3550.48it/s]99479it [00:39, 3461.37it/s]111138it [00:40, 3541.27it/s]103170it [00:39, 3350.28it/s]114134it [00:40, 3451.54it/s]99828it [00:39, 3363.43it/s]111494it [00:40, 3440.94it/s]103539it [00:39, 3442.13it/s]114509it [00:40, 3536.25it/s]100197it [00:39, 3454.66it/s]111865it [00:40, 3517.51it/s]103889it [00:39, 3352.43it/s]114864it [00:40, 3431.61it/s]100545it [00:39, 3350.23it/s]112221it [00:40, 3419.69it/s]104259it [00:39, 3449.76it/s]100905it [00:39, 3421.49it/s]112603it [00:40, 3532.92it/s]104626it [00:39, 3513.51it/s]101277it [00:39, 3506.78it/s]112982it [00:40, 3607.31it/s]104979it [00:39, 3385.52it/s]101629it [00:39, 3377.63it/s]113345it [00:40, 3473.92it/s]105348it [00:40, 3472.46it/s]102000it [00:39, 3471.63it/s]113717it [00:40, 3543.71it/s]105697it [00:40, 3342.49it/s]102349it [00:40, 3362.41it/s]114073it [00:40, 3443.65it/s]106062it [00:40, 3429.34it/s]102702it [00:40, 3409.86it/s]114458it [00:40, 3559.93it/s]106409it [00:40, 3335.19it/s]103050it [00:40, 3311.80it/s]106779it [00:40, 3438.02it/s]103421it [00:40, 3423.87it/s]107147it [00:40, 3508.02it/s]103792it [00:40, 3504.53it/s]107500it [00:40, 3390.78it/s]104144it [00:40, 3384.43it/s]107870it [00:40, 3478.78it/s]104500it [00:40, 3434.83it/s]108220it [00:40, 3373.49it/s]104845it [00:40, 3329.29it/s]108583it [00:40, 3445.85it/s]105216it [00:40, 3437.54it/s]108929it [00:41, 3355.68it/s]105570it [00:41, 3323.42it/s]109299it [00:41, 3454.56it/s]105942it [00:41, 3435.68it/s]109671it [00:41, 3531.32it/s]106303it [00:41, 3485.70it/s]110026it [00:41, 3411.08it/s]106654it [00:41, 3368.48it/s]110398it [00:41, 3499.58it/s]107025it [00:41, 3465.35it/s]110750it [00:41, 3382.38it/s]107374it [00:41, 3350.01it/s]111123it [00:41, 3480.05it/s]107744it [00:41, 3450.09it/s]111473it [00:41, 3355.99it/s]108091it [00:41, 3323.29it/s]111844it [00:41, 3455.76it/s]108465it [00:41, 3441.97it/s]112218it [00:42, 3537.32it/s]108842it [00:41, 3534.78it/s]112574it [00:42, 3411.16it/s]109198it [00:42, 3403.10it/s]112945it [00:42, 3496.43it/s]109560it [00:42, 3460.61it/s]113297it [00:42, 3390.41it/s]109908it [00:42, 3357.19it/s]113670it [00:42, 3486.36it/s]110278it [00:42, 3453.60it/s]114021it [00:42, 3362.62it/s]110625it [00:42, 3347.78it/s]114393it [00:42, 3463.82it/s]110997it [00:42, 3451.87it/s]114764it [00:42, 3532.78it/s]111359it [00:42, 3499.11it/s]111711it [00:42, 3376.15it/s]112080it [00:42, 3463.66it/s]112428it [00:43, 3344.25it/s]112795it [00:43, 3436.52it/s]113141it [00:43, 3288.53it/s]115209it [00:44, 297.39it/s] 113510it [00:43, 3401.26it/s]115586it [00:44, 416.49it/s]113863it [00:43, 3436.81it/s]115885it [00:44, 536.68it/s]114209it [00:43, 3337.25it/s]116266it [00:44, 741.42it/s]114570it [00:43, 3415.04it/s]116582it [00:44, 934.49it/s]116962it [00:44, 1233.46it/s]117339it [00:44, 1562.29it/s]114816it [00:44, 308.66it/s] 117684it [00:44, 1826.53it/s]115190it [00:44, 427.82it/s]118063it [00:44, 2179.30it/s]115566it [00:44, 585.30it/s]115882it [00:44, 748.09it/s]118411it [00:45, 2403.25it/s]118790it [00:45, 2712.19it/s]116263it [00:45, 1001.64it/s]116598it [00:45, 1240.41it/s]119142it [00:45, 2825.12it/s]116971it [00:45, 1564.94it/s]119508it [00:45, 3033.27it/s]117322it [00:45, 1869.93it/s]119861it [00:45, 3162.51it/s]117666it [00:45, 2124.19it/s]120211it [00:45, 3168.31it/s]118040it [00:45, 2454.71it/s]120588it [00:45, 3332.91it/s]118388it [00:45, 2632.82it/s]120939it [00:45, 3297.63it/s]118759it [00:45, 2890.83it/s]121316it [00:45, 3429.73it/s]119108it [00:45, 2968.19it/s]121669it [00:45, 3334.97it/s]119484it [00:46, 3174.41it/s]122042it [00:46, 3444.84it/s]122416it [00:46, 3529.80it/s]119850it [00:46, 3159.16it/s]120205it [00:46, 3264.20it/s]122774it [00:46, 3375.88it/s]120572it [00:46, 3377.18it/s]123145it [00:46, 3469.72it/s]120923it [00:46, 3323.59it/s]123496it [00:46, 3393.23it/s]121301it [00:46, 3451.70it/s]123866it [00:46, 3481.11it/s]121654it [00:46, 3363.50it/s]124217it [00:46, 3393.35it/s]122026it [00:46, 3464.42it/s]124597it [00:46, 3509.92it/s]122377it [00:46, 3359.25it/s]124961it [00:46, 3408.78it/s]122757it [00:46, 3484.05it/s]125338it [00:47, 3510.79it/s]123127it [00:47, 3545.63it/s]125691it [00:47, 3450.39it/s]123484it [00:47, 3386.60it/s]126038it [00:47, 3330.70it/s]115119it [00:46, 296.87it/s] 123846it [00:47, 3449.96it/s]126413it [00:47, 3447.84it/s]115480it [00:46, 409.07it/s]124194it [00:47, 3369.38it/s]126760it [00:47, 3374.92it/s]115783it [00:46, 530.04it/s]124573it [00:47, 3488.17it/s]127138it [00:47, 3489.83it/s]116152it [00:46, 725.47it/s]124924it [00:47, 3394.20it/s]127489it [00:47, 3406.67it/s]116525it [00:47, 969.24it/s]125302it [00:47, 3502.63it/s]127869it [00:47, 3519.06it/s]116856it [00:47, 1203.20it/s]125668it [00:47, 3545.78it/s]128246it [00:47, 3589.87it/s]117225it [00:47, 1522.22it/s]126024it [00:47, 3440.19it/s]128607it [00:47, 3475.47it/s]117563it [00:47, 1752.02it/s]126396it [00:48, 3518.90it/s]128975it [00:48, 3533.81it/s]117928it [00:47, 2086.83it/s]126750it [00:48, 3349.07it/s]129330it [00:48, 3376.72it/s]118262it [00:47, 2259.83it/s]127125it [00:48, 3461.35it/s]129703it [00:48, 3475.03it/s]118627it [00:47, 2561.09it/s]127474it [00:48, 3349.69it/s]130053it [00:48, 3378.56it/s]118999it [00:47, 2834.61it/s]127850it [00:48, 3465.20it/s]130429it [00:48, 3486.42it/s]119344it [00:47, 2901.63it/s]128227it [00:48, 3551.23it/s]130796it [00:48, 3539.00it/s]119710it [00:47, 3096.89it/s]128584it [00:48, 3450.97it/s]131152it [00:48, 3437.68it/s]120053it [00:48, 3110.34it/s]128958it [00:48, 3534.01it/s]131524it [00:48, 3518.14it/s]120413it [00:48, 3243.93it/s]129313it [00:48, 3410.16it/s]131878it [00:48, 3418.69it/s]114913it [00:48, 248.85it/s] 120758it [00:48, 3164.44it/s]129684it [00:48, 3495.06it/s]132249it [00:49, 3500.41it/s]115265it [00:48, 344.95it/s]121125it [00:48, 3302.50it/s]115621it [00:48, 474.44it/s]130036it [00:49, 3332.91it/s]132601it [00:49, 3348.10it/s]121486it [00:48, 3388.00it/s]130413it [00:49, 3456.15it/s]132961it [00:49, 3418.17it/s]115921it [00:48, 609.85it/s]121833it [00:48, 3296.75it/s]133335it [00:49, 3511.07it/s]116288it [00:48, 829.82it/s]130769it [00:49, 3364.28it/s]122202it [00:48, 3407.88it/s]116605it [00:48, 1043.47it/s]133688it [00:49, 3417.98it/s]131136it [00:49, 3449.42it/s]122548it [00:48, 3326.09it/s]116976it [00:48, 1355.08it/s]134060it [00:49, 3504.22it/s]131508it [00:49, 3525.86it/s]122921it [00:48, 3439.52it/s]117343it [00:48, 1686.09it/s]134412it [00:49, 3405.03it/s]131863it [00:49, 3415.07it/s]123278it [00:49, 3235.61it/s]134787it [00:49, 3502.12it/s]132237it [00:49, 3506.60it/s]117684it [00:48, 1918.54it/s]123626it [00:49, 3303.11it/s]118051it [00:49, 2251.28it/s]135139it [00:49, 3377.94it/s]132590it [00:49, 3394.72it/s]123995it [00:49, 3413.08it/s]135493it [00:49, 3423.98it/s]132935it [00:49, 3409.61it/s]118389it [00:49, 2426.21it/s]124340it [00:49, 3253.76it/s]135865it [00:50, 3509.65it/s]118750it [00:49, 2697.43it/s]133290it [00:50, 3314.50it/s]124706it [00:49, 3366.26it/s]136218it [00:50, 3411.49it/s]119087it [00:49, 2787.50it/s]133667it [00:50, 3442.22it/s]125046it [00:49, 3281.29it/s]136591it [00:50, 3501.51it/s]119451it [00:49, 3003.71it/s]134042it [00:50, 3531.13it/s]125412it [00:49, 3388.85it/s]136943it [00:50, 3410.87it/s]119789it [00:49, 3008.63it/s]134397it [00:50, 3395.92it/s]125782it [00:49, 3477.00it/s]137312it [00:50, 3489.09it/s]134770it [00:50, 3489.33it/s]120116it [00:49, 3028.96it/s]126132it [00:49, 3347.58it/s]137663it [00:50, 3396.94it/s]120486it [00:49, 3211.81it/s]135121it [00:50, 3389.19it/s]126492it [00:49, 3417.89it/s]138040it [00:50, 3502.91it/s]135497it [00:50, 3495.33it/s]120822it [00:49, 3126.62it/s]126836it [00:50, 3317.75it/s]138400it [00:50, 3385.94it/s]121191it [00:50, 3282.97it/s]135849it [00:50, 3383.86it/s]127199it [00:50, 3400.41it/s]138750it [00:50, 3417.05it/s]121546it [00:50, 3358.28it/s]136191it [00:50, 3393.16it/s]127541it [00:50, 3263.33it/s]139126it [00:51, 3513.73it/s]136564it [00:50, 3490.31it/s]121889it [00:50, 3215.38it/s]127911it [00:50, 3386.66it/s]139479it [00:51, 3395.93it/s]136915it [00:51, 3391.22it/s]122259it [00:50, 3351.92it/s]128280it [00:50, 3473.88it/s]139857it [00:51, 3504.83it/s]137288it [00:51, 3486.84it/s]122599it [00:50, 3279.83it/s]128630it [00:50, 3344.70it/s]140209it [00:51, 3405.85it/s]137638it [00:51, 3388.62it/s]122971it [00:50, 3405.29it/s]129003it [00:50, 3452.87it/s]140584it [00:51, 3503.52it/s]138001it [00:51, 3455.65it/s]123315it [00:50, 3309.93it/s]129351it [00:50, 3357.57it/s]140936it [00:51, 3400.23it/s]138348it [00:51, 3360.90it/s]123661it [00:50, 3351.73it/s]129708it [00:50, 3417.67it/s]141304it [00:51, 3479.76it/s]138720it [00:51, 3464.00it/s]124024it [00:50, 3430.75it/s]130052it [00:51, 3326.07it/s]141681it [00:51, 3563.36it/s]139094it [00:51, 3542.88it/s]124369it [00:50, 3310.97it/s]130417it [00:51, 3419.00it/s]124731it [00:51, 3398.95it/s]142039it [00:51, 3377.90it/s]139450it [00:51, 3363.66it/s]130775it [00:51, 3465.49it/s]142412it [00:51, 3475.84it/s]139817it [00:51, 3450.65it/s]125073it [00:51, 3288.21it/s]131123it [00:51, 3357.50it/s]142762it [00:52, 3387.55it/s]125445it [00:51, 3410.34it/s]140165it [00:52, 3369.55it/s]131478it [00:51, 3412.27it/s]143139it [00:52, 3495.63it/s]140542it [00:52, 3482.90it/s]125799it [00:51, 3310.51it/s]131821it [00:51, 3312.91it/s]140892it [00:52, 3389.76it/s]143491it [00:52, 3378.02it/s]126156it [00:51, 3383.98it/s]132191it [00:51, 3422.17it/s]143869it [00:52, 3491.77it/s]141270it [00:52, 3496.37it/s]126527it [00:51, 3478.18it/s]132535it [00:51, 3241.15it/s]144246it [00:52, 3570.91it/s]141646it [00:52, 3570.43it/s]126877it [00:51, 3243.73it/s]132901it [00:51, 3357.92it/s]144605it [00:52, 3452.54it/s]142005it [00:52, 3421.54it/s]127250it [00:51, 3378.29it/s]133263it [00:51, 3432.72it/s]144955it [00:52, 3463.85it/s]142351it [00:52, 3430.11it/s]127592it [00:51, 3217.88it/s]133609it [00:52, 3267.83it/s]145303it [00:52, 3377.32it/s]142696it [00:52, 3351.29it/s]127947it [00:52, 3310.52it/s]133976it [00:52, 3381.11it/s]145679it [00:52, 3487.11it/s]143073it [00:52, 3471.19it/s]128319it [00:52, 3246.24it/s]134317it [00:52, 3264.00it/s]146029it [00:53, 3403.65it/s]143422it [00:52, 3382.47it/s]128694it [00:52, 3384.06it/s]134688it [00:52, 3388.96it/s]146401it [00:53, 3494.80it/s]143787it [00:53, 3458.62it/s]129068it [00:52, 3483.63it/s]135039it [00:52, 3302.42it/s]146779it [00:53, 3575.83it/s]144159it [00:53, 3532.89it/s]129419it [00:52, 3370.57it/s]135411it [00:52, 3420.52it/s]147138it [00:53, 3463.48it/s]144514it [00:53, 3439.92it/s]129781it [00:52, 3440.52it/s]135770it [00:52, 3466.93it/s]147515it [00:53, 3549.70it/s]144890it [00:53, 3530.24it/s]130128it [00:52, 3285.52it/s]136119it [00:52, 3362.11it/s]145245it [00:53, 3392.94it/s]147872it [00:53, 3378.79it/s]130489it [00:52, 3373.96it/s]136476it [00:52, 3421.42it/s]145610it [00:53, 3463.67it/s]148236it [00:53, 3452.49it/s]130839it [00:52, 3239.30it/s]136820it [00:53, 3269.26it/s]148584it [00:53, 3366.13it/s]145958it [00:53, 3342.84it/s]131209it [00:52, 3367.31it/s]137189it [00:53, 3386.32it/s]148964it [00:53, 3489.45it/s]146333it [00:53, 3457.88it/s]131575it [00:53, 3448.93it/s]137556it [00:53, 3468.03it/s]146710it [00:53, 3547.34it/s]149321it [00:53, 3388.95it/s]131923it [00:53, 3324.61it/s]137905it [00:53, 3354.70it/s]147067it [00:54, 3450.27it/s]149694it [00:54, 3485.10it/s]132282it [00:53, 3398.98it/s]138273it [00:53, 3446.56it/s]147445it [00:54, 3544.61it/s]150071it [00:54, 3567.46it/s]132624it [00:53, 3288.70it/s]138620it [00:53, 3308.44it/s]147801it [00:54, 3442.53it/s]150430it [00:54, 3455.12it/s]132982it [00:53, 3371.31it/s]138986it [00:53, 3408.09it/s]148171it [00:54, 3516.29it/s]150800it [00:54, 3525.44it/s]133353it [00:53, 3467.88it/s]139329it [00:53, 3267.94it/s]148524it [00:54, 3304.45it/s]133702it [00:53, 3272.38it/s]139676it [00:53, 3324.88it/s]148891it [00:54, 3405.88it/s]134056it [00:53, 3346.56it/s]140046it [00:54, 3430.96it/s]149250it [00:54, 3323.97it/s]134394it [00:53, 3246.26it/s]140391it [00:54, 3318.85it/s]149632it [00:54, 3463.55it/s]134760it [00:54, 3362.65it/s]140759it [00:54, 3419.89it/s]150013it [00:54, 3561.43it/s]135099it [00:54, 3273.77it/s]141103it [00:54, 3321.82it/s]150372it [00:55, 3454.67it/s]135470it [00:54, 3397.16it/s]141471it [00:54, 3424.10it/s]150756it [00:55, 3563.56it/s]135827it [00:54, 3446.50it/s]141815it [00:54, 3283.57it/s]136174it [00:54, 3301.17it/s]142181it [00:54, 3390.25it/s]136536it [00:54, 3390.71it/s]142529it [00:54, 3413.33it/s]136877it [00:54, 3220.81it/s]142872it [00:54, 3260.69it/s]137248it [00:54, 3358.46it/s]143238it [00:54, 3372.30it/s]137587it [00:54, 3267.57it/s]143578it [00:55, 3288.88it/s]137960it [00:55, 3397.55it/s]143946it [00:55, 3399.99it/s]138319it [00:55, 3451.83it/s]144288it [00:55, 3308.64it/s]138666it [00:55, 3312.92it/s]144654it [00:55, 3406.79it/s]139029it [00:55, 3397.14it/s]145007it [00:55, 3440.59it/s]139371it [00:55, 3299.46it/s]145353it [00:55, 3324.22it/s]139729it [00:55, 3375.57it/s]145720it [00:55, 3421.78it/s]140079it [00:55, 3285.02it/s]146064it [00:55, 3334.35it/s]140439it [00:55, 3373.83it/s]146435it [00:55, 3442.46it/s]140810it [00:55, 3470.19it/s]146799it [00:56, 3347.42it/s]141159it [00:55, 3359.23it/s]147172it [00:56, 3454.31it/s]141530it [00:56, 3458.42it/s]147548it [00:56, 3540.02it/s]141878it [00:56, 3236.51it/s]147904it [00:56, 3315.10it/s]142234it [00:56, 3325.39it/s]148266it [00:56, 3399.50it/s]142592it [00:56, 3396.17it/s]148610it [00:56, 3243.06it/s]142935it [00:56, 3249.91it/s]148978it [00:56, 3364.80it/s]143303it [00:56, 3369.17it/s]149319it [00:56, 3293.63it/s]143643it [00:56, 3296.70it/s]149693it [00:56, 3418.62it/s]144013it [00:56, 3410.77it/s]150064it [00:56, 3501.11it/s]144356it [00:56, 3278.16it/s]150416it [00:57, 3394.65it/s]144721it [00:57, 3382.49it/s]150776it [00:57, 3452.27it/s]145091it [00:57, 3472.08it/s]145440it [00:57, 3350.16it/s]145805it [00:57, 3435.09it/s]146151it [00:57, 3338.81it/s]146521it [00:57, 3439.87it/s]146867it [00:57, 3304.86it/s]147239it [00:57, 3422.08it/s]147609it [00:57, 3502.18it/s]147961it [00:57, 3278.50it/s]148333it [00:58, 3401.94it/s]151154it [00:58, 250.83it/s] 148677it [00:58, 3216.34it/s]151526it [00:59, 350.50it/s]149042it [00:58, 3336.72it/s]151901it [00:59, 484.52it/s]149380it [00:58, 3218.56it/s]152220it [00:59, 628.74it/s]149750it [00:58, 3353.09it/s]152600it [00:59, 853.17it/s]150119it [00:58, 3449.27it/s]152935it [00:59, 1078.20it/s]150467it [00:58, 3336.89it/s]153305it [00:59, 1380.96it/s]151115it [00:59, 261.75it/s] 150825it [00:58, 3405.36it/s]151488it [00:59, 364.11it/s]153648it [00:59, 1650.90it/s]154020it [00:59, 1995.16it/s]151838it [00:59, 488.99it/s]154367it [00:59, 2278.15it/s]152190it [00:59, 653.94it/s]154713it [00:59, 2484.10it/s]152564it [00:59, 876.57it/s]155090it [01:00, 2781.08it/s]152896it [01:00, 1098.26it/s]153273it [01:00, 1411.17it/s]155439it [01:00, 2871.21it/s]155813it [01:00, 3093.82it/s]153615it [01:00, 1681.10it/s]153995it [01:00, 2037.30it/s]156162it [01:00, 3112.35it/s]156535it [01:00, 3277.43it/s]154358it [01:00, 2282.27it/s]156907it [01:00, 3399.02it/s]154732it [01:00, 2590.96it/s]155082it [01:00, 2794.27it/s]157263it [01:00, 3285.20it/s]157621it [01:00, 3367.62it/s]155431it [01:00, 2877.04it/s]155806it [01:00, 3100.32it/s]157967it [01:00, 3305.98it/s]158342it [01:01, 3431.45it/s]156155it [01:01, 3117.56it/s]156528it [01:01, 3277.41it/s]158691it [01:01, 3349.64it/s]159071it [01:01, 3476.12it/s]156878it [01:01, 3247.02it/s]159453it [01:01, 3575.37it/s]157243it [01:01, 3357.51it/s]157623it [01:01, 3482.26it/s]159814it [01:01, 3441.98it/s]160182it [01:01, 3509.29it/s]157980it [01:01, 3379.42it/s]158330it [01:01, 3411.16it/s]160536it [01:01, 3342.32it/s]158676it [01:01, 3319.26it/s]160903it [01:01, 3433.79it/s]159055it [01:01, 3453.28it/s]161249it [01:01, 3362.66it/s]161615it [01:01, 3447.78it/s]159404it [01:01, 3348.96it/s]159784it [01:02, 3477.69it/s]161989it [01:02, 3387.69it/s]160159it [01:02, 3555.14it/s]162367it [01:02, 3499.07it/s]160517it [01:02, 3445.32it/s]162748it [01:02, 3586.33it/s]160891it [01:02, 3529.84it/s]163109it [01:02, 3461.09it/s]151123it [01:01, 243.94it/s] 161246it [01:02, 3386.67it/s]163484it [01:02, 3543.21it/s]151488it [01:01, 340.96it/s]161594it [01:02, 3411.45it/s]163840it [01:02, 3356.38it/s]151849it [01:02, 468.84it/s]161937it [01:02, 3330.07it/s]164215it [01:02, 3466.87it/s]152154it [01:02, 603.78it/s]162318it [01:02, 3466.02it/s]164565it [01:02, 3389.08it/s]152525it [01:02, 821.53it/s]162696it [01:02, 3555.90it/s]164954it [01:02, 3531.01it/s]152846it [01:02, 1034.57it/s]163053it [01:03, 3452.77it/s]165342it [01:03, 3631.03it/s]153204it [01:02, 1326.25it/s]163422it [01:03, 3518.69it/s]165707it [01:03, 3504.91it/s]153575it [01:02, 1661.28it/s]163776it [01:03, 3426.06it/s]166085it [01:03, 3582.00it/s]153917it [01:02, 1912.68it/s]164158it [01:03, 3538.16it/s]166445it [01:03, 3475.11it/s]154282it [01:02, 2240.95it/s]164514it [01:03, 3425.11it/s]166835it [01:03, 3596.96it/s]154622it [01:02, 2423.58it/s]164875it [01:03, 3475.60it/s]167197it [01:03, 3415.47it/s]154981it [01:02, 2689.45it/s]165250it [01:03, 3555.30it/s]167581it [01:03, 3533.79it/s]155319it [01:03, 2791.92it/s]165607it [01:03, 3425.91it/s]167938it [01:03, 3430.73it/s]155686it [01:03, 3015.58it/s]165994it [01:03, 3553.21it/s]168322it [01:03, 3544.87it/s]156038it [01:03, 3146.96it/s]166351it [01:03, 3468.12it/s]168710it [01:04, 3469.38it/s]156382it [01:03, 3133.64it/s]166743it [01:04, 3597.28it/s]169093it [01:04, 3569.46it/s]156744it [01:03, 3268.10it/s]167105it [01:04, 3486.10it/s]169477it [01:04, 3644.72it/s]157086it [01:03, 3138.85it/s]167486it [01:04, 3577.95it/s]169844it [01:04, 3505.27it/s]157455it [01:03, 3290.21it/s]167846it [01:04, 3349.40it/s]170197it [01:04, 3498.81it/s]157794it [01:03, 3160.05it/s]168225it [01:04, 3470.66it/s]170549it [01:04, 3405.38it/s]158158it [01:03, 3292.35it/s]168612it [01:04, 3582.06it/s]170934it [01:04, 3532.43it/s]158515it [01:04, 3369.11it/s]168974it [01:04, 3483.11it/s]171289it [01:04, 3446.05it/s]158857it [01:04, 3294.99it/s]169351it [01:04, 3564.31it/s]171668it [01:04, 3544.54it/s]159232it [01:04, 3425.05it/s]151168it [01:04, 213.75it/s] 169710it [01:04, 3441.04it/s]172056it [01:04, 3641.27it/s]159578it [01:04, 3332.25it/s]151540it [01:04, 302.37it/s]170097it [01:05, 3561.52it/s]172422it [01:05, 3506.80it/s]159933it [01:04, 3394.14it/s]151895it [01:04, 415.55it/s]170456it [01:05, 3450.61it/s]172805it [01:05, 3597.74it/s]160301it [01:04, 3476.42it/s]152199it [01:04, 539.97it/s]170820it [01:05, 3503.23it/s]173167it [01:05, 3446.76it/s]152560it [01:04, 734.68it/s]160651it [01:04, 3340.01it/s]171172it [01:05, 3385.40it/s]173538it [01:05, 3521.35it/s]161009it [01:04, 3408.15it/s]152881it [01:04, 932.55it/s]171558it [01:05, 3518.35it/s]173893it [01:05, 3436.65it/s]153238it [01:04, 1209.50it/s]161352it [01:04, 3298.29it/s]171935it [01:05, 3589.55it/s]174280it [01:05, 3559.92it/s]161717it [01:04, 3396.62it/s]153588it [01:04, 1485.30it/s]172296it [01:05, 3480.61it/s]174638it [01:05, 3480.73it/s]162059it [01:05, 3312.02it/s]153957it [01:04, 1827.24it/s]172686it [01:05, 3601.24it/s]175023it [01:05, 3584.95it/s]162426it [01:05, 3414.08it/s]154323it [01:05, 2159.33it/s]173048it [01:05, 3511.15it/s]175407it [01:05, 3658.84it/s]162792it [01:05, 3484.50it/s]154668it [01:05, 2359.81it/s]173425it [01:05, 3584.88it/s]175775it [01:06, 3541.92it/s]155025it [01:05, 2628.43it/s]163142it [01:05, 3271.19it/s]173785it [01:06, 3470.51it/s]176143it [01:06, 3580.04it/s]163514it [01:05, 3396.45it/s]155365it [01:05, 2720.00it/s]174147it [01:06, 3512.24it/s]176503it [01:06, 3385.39it/s]155721it [01:05, 2928.72it/s]163857it [01:05, 3213.67it/s]174519it [01:06, 3440.37it/s]176889it [01:06, 3517.51it/s]156081it [01:05, 3104.69it/s]164228it [01:05, 3344.18it/s]174897it [01:06, 3537.19it/s]177244it [01:06, 3427.47it/s]156424it [01:05, 3093.29it/s]164566it [01:05, 3286.67it/s]175285it [01:06, 3635.53it/s]177617it [01:06, 3513.68it/s]156790it [01:05, 3247.12it/s]164945it [01:05, 3430.00it/s]175650it [01:06, 3513.52it/s]177971it [01:06, 3446.40it/s]165322it [01:06, 3527.89it/s]157132it [01:05, 3192.51it/s]176032it [01:06, 3598.93it/s]178357it [01:06, 3565.34it/s]157492it [01:05, 3306.43it/s]165677it [01:06, 3418.49it/s]176394it [01:06, 3459.65it/s]178734it [01:06, 3623.69it/s]166042it [01:06, 3484.70it/s]157832it [01:06, 3147.94it/s]176777it [01:06, 3563.58it/s]179098it [01:06, 3492.60it/s]166393it [01:06, 3402.61it/s]158201it [01:06, 3297.63it/s]177136it [01:07, 3395.92it/s]179451it [01:07, 3501.46it/s]166751it [01:06, 3453.16it/s]158558it [01:06, 3368.14it/s]177507it [01:07, 3484.22it/s]179803it [01:07, 3418.05it/s]167098it [01:06, 3349.32it/s]158900it [01:06, 3238.45it/s]177879it [01:07, 3420.34it/s]180190it [01:07, 3544.90it/s]167479it [01:06, 3479.34it/s]159273it [01:06, 3375.83it/s]178256it [01:07, 3518.01it/s]180546it [01:07, 3454.42it/s]167857it [01:06, 3564.55it/s]159615it [01:06, 3260.64it/s]178644it [01:07, 3620.08it/s]180933it [01:07, 3573.25it/s]168215it [01:06, 3458.56it/s]159984it [01:06, 3381.99it/s]179008it [01:07, 3497.72it/s]181310it [01:07, 3466.42it/s]168590it [01:06, 3542.20it/s]160326it [01:06, 3288.58it/s]179387it [01:07, 3581.25it/s]181694it [01:07, 3571.27it/s]168946it [01:07, 3384.20it/s]160685it [01:06, 3373.14it/s]179747it [01:07, 3485.32it/s]182080it [01:07, 3648.39it/s]169305it [01:07, 3442.61it/s]161053it [01:07, 3459.49it/s]180128it [01:07, 3576.63it/s]182447it [01:07, 3486.56it/s]169652it [01:07, 3314.84it/s]161401it [01:07, 3285.94it/s]180488it [01:07, 3405.98it/s]182830it [01:08, 3583.17it/s]170017it [01:07, 3407.42it/s]161765it [01:07, 3385.68it/s]180878it [01:08, 3544.08it/s]183191it [01:08, 3456.81it/s]170388it [01:07, 3352.11it/s]162107it [01:07, 3302.55it/s]181239it [01:08, 3443.53it/s]183572it [01:08, 3556.82it/s]170764it [01:07, 3465.81it/s]162478it [01:07, 3411.44it/s]181622it [01:08, 3552.77it/s]183930it [01:08, 3473.32it/s]171136it [01:07, 3538.67it/s]162828it [01:07, 3326.06it/s]182010it [01:08, 3646.20it/s]184320it [01:08, 3594.15it/s]171492it [01:07, 3428.60it/s]163203it [01:07, 3444.74it/s]182377it [01:08, 3529.76it/s]184682it [01:08, 3506.02it/s]171859it [01:07, 3496.36it/s]163559it [01:07, 3476.02it/s]182763it [01:08, 3622.46it/s]185071it [01:08, 3615.43it/s]172211it [01:08, 3357.29it/s]163908it [01:07, 3310.84it/s]185439it [01:08, 3634.15it/s]183127it [01:08, 3491.86it/s]172587it [01:08, 3471.78it/s]164279it [01:08, 3424.02it/s]183483it [01:08, 3509.95it/s]185804it [01:08, 3451.88it/s]172937it [01:08, 3327.14it/s]164624it [01:08, 3255.92it/s]183836it [01:08, 3418.23it/s]186194it [01:08, 3577.50it/s]173313it [01:08, 3448.74it/s]165008it [01:08, 3419.80it/s]184223it [01:09, 3547.72it/s]186555it [01:09, 3499.47it/s]173686it [01:08, 3527.13it/s]165354it [01:08, 3330.45it/s]184599it [01:09, 3467.34it/s]186966it [01:09, 3672.30it/s]174041it [01:08, 3417.98it/s]165719it [01:08, 3419.25it/s]184983it [01:09, 3571.42it/s]187336it [01:09, 3524.37it/s]174423it [01:08, 3533.11it/s]166101it [01:08, 3533.40it/s]185362it [01:09, 3633.95it/s]187728it [01:09, 3635.70it/s]174779it [01:08, 3384.29it/s]166457it [01:08, 3405.55it/s]185727it [01:09, 3524.39it/s]188094it [01:09, 3559.17it/s]175148it [01:08, 3464.61it/s]166829it [01:08, 3494.63it/s]186111it [01:09, 3613.92it/s]188470it [01:09, 3616.79it/s]175497it [01:08, 3376.48it/s]167181it [01:08, 3382.83it/s]188836it [01:09, 3628.65it/s]186474it [01:09, 3470.94it/s]175864it [01:09, 3458.79it/s]167534it [01:08, 3422.82it/s]186858it [01:09, 3575.58it/s]189200it [01:09, 3492.05it/s]176228it [01:09, 3508.81it/s]167878it [01:09, 3315.33it/s]187218it [01:09, 3481.26it/s]189570it [01:09, 3550.97it/s]176581it [01:09, 3397.20it/s]168260it [01:09, 3459.29it/s]187606it [01:09, 3593.18it/s]189927it [01:10, 3478.48it/s]176963it [01:09, 3517.37it/s]168638it [01:09, 3545.36it/s]187967it [01:10, 3506.02it/s]190321it [01:10, 3611.34it/s]177317it [01:09, 3413.74it/s]168995it [01:09, 3422.13it/s]188355it [01:10, 3611.16it/s]190684it [01:10, 3507.15it/s]177681it [01:09, 3477.47it/s]169368it [01:09, 3510.10it/s]188744it [01:10, 3690.12it/s]191063it [01:10, 3586.84it/s]178031it [01:09, 3243.00it/s]169721it [01:09, 3239.11it/s]189115it [01:10, 3521.66it/s]191423it [01:10, 3472.48it/s]178403it [01:09, 3375.76it/s]170099it [01:09, 3386.91it/s]189495it [01:10, 3600.70it/s]191791it [01:10, 3530.88it/s]178774it [01:09, 3468.33it/s]170443it [01:09, 3237.91it/s]192152it [01:10, 3553.61it/s]189858it [01:10, 3431.16it/s]179124it [01:10, 3302.83it/s]170812it [01:09, 3362.40it/s]192509it [01:10, 3457.39it/s]190251it [01:10, 3570.03it/s]179491it [01:10, 3405.82it/s]171186it [01:10, 3469.54it/s]192893it [01:10, 3567.84it/s]190611it [01:10, 3468.66it/s]179835it [01:10, 3352.67it/s]171537it [01:10, 3367.65it/s]193251it [01:10, 3477.50it/s]190988it [01:10, 3554.00it/s]180215it [01:10, 3478.71it/s]171919it [01:10, 3496.12it/s]193621it [01:11, 3540.92it/s]191346it [01:11, 3414.66it/s]180565it [01:10, 3378.16it/s]172272it [01:10, 3352.79it/s]193977it [01:11, 3460.27it/s]191726it [01:11, 3521.87it/s]180951it [01:10, 3513.83it/s]172649it [01:10, 3470.31it/s]194364it [01:11, 3578.63it/s]192118it [01:11, 3634.71it/s]181305it [01:10, 3477.25it/s]194745it [01:11, 3644.86it/s]172999it [01:10, 3359.91it/s]192484it [01:11, 3519.06it/s]181655it [01:10, 3381.09it/s]173376it [01:10, 3476.48it/s]195111it [01:11, 3512.63it/s]192848it [01:11, 3551.91it/s]182022it [01:10, 3462.99it/s]173745it [01:10, 3537.34it/s]195480it [01:11, 3562.69it/s]193205it [01:11, 3442.29it/s]182370it [01:10, 3319.67it/s]174101it [01:10, 3364.68it/s]195838it [01:11, 3484.15it/s]193566it [01:11, 3490.12it/s]182758it [01:11, 3476.50it/s]174476it [01:10, 3473.35it/s]196210it [01:11, 3549.74it/s]193917it [01:11, 3422.49it/s]183108it [01:11, 3370.84it/s]174826it [01:11, 3373.08it/s]194310it [01:11, 3567.32it/s]183483it [01:11, 3478.97it/s]175210it [01:11, 3503.93it/s]194679it [01:11, 3477.49it/s]183833it [01:11, 3389.78it/s]175563it [01:11, 3384.67it/s]195073it [01:12, 3610.15it/s]184205it [01:11, 3481.95it/s]175931it [01:11, 3467.25it/s]195464it [01:12, 3696.26it/s]184557it [01:11, 3491.96it/s]176280it [01:11, 3332.09it/s]195835it [01:12, 3547.67it/s]184908it [01:11, 3388.97it/s]176644it [01:11, 3419.01it/s]196192it [01:12, 3546.38it/s]185276it [01:11, 3470.27it/s]177011it [01:11, 3491.03it/s]185625it [01:11, 3367.96it/s]177362it [01:11, 3383.23it/s]186005it [01:12, 3489.89it/s]177732it [01:11, 3473.94it/s]186356it [01:12, 3407.30it/s]178081it [01:12, 3381.41it/s]186761it [01:12, 3590.92it/s]178444it [01:12, 3450.55it/s]187137it [01:12, 3637.44it/s]178791it [01:12, 3266.96it/s]187502it [01:12, 3367.82it/s]179159it [01:12, 3382.36it/s]187888it [01:12, 3503.70it/s]179534it [01:12, 3486.55it/s]188243it [01:12, 3349.78it/s]179885it [01:12, 3311.81it/s]188622it [01:12, 3472.12it/s]180268it [01:12, 3457.47it/s]188973it [01:12, 3378.38it/s]180617it [01:12, 3325.25it/s]189354it [01:13, 3500.33it/s]181003it [01:12, 3475.90it/s]189708it [01:13, 3388.01it/s]181354it [01:13, 3371.31it/s]190092it [01:13, 3514.26it/s]181728it [01:13, 3471.72it/s]190449it [01:13, 3528.15it/s]182099it [01:13, 3539.06it/s]190804it [01:13, 3416.51it/s]182455it [01:13, 3428.80it/s]191167it [01:13, 3476.24it/s]182822it [01:13, 3497.17it/s]191517it [01:13, 3289.86it/s]183174it [01:13, 3312.52it/s]191896it [01:13, 3429.75it/s]183551it [01:13, 3439.83it/s]192242it [01:13, 3356.05it/s]183898it [01:13, 3354.90it/s]192623it [01:13, 3485.36it/s]184280it [01:13, 3485.13it/s]192986it [01:14, 3525.26it/s]184654it [01:13, 3556.94it/s]193341it [01:14, 3386.80it/s]185012it [01:14, 3409.17it/s]193698it [01:14, 3434.36it/s]185379it [01:14, 3481.97it/s]194044it [01:14, 3362.11it/s]185730it [01:14, 3369.75it/s]194410it [01:14, 3447.33it/s]186107it [01:14, 3482.89it/s]194756it [01:14, 3351.12it/s]186458it [01:14, 3393.57it/s]195156it [01:14, 3536.37it/s]186852it [01:14, 3547.88it/s]195526it [01:14, 3582.05it/s]187209it [01:14, 3432.99it/s]195886it [01:14, 3427.95it/s]187590it [01:14, 3539.01it/s]196255it [01:15, 3501.47it/s]187975it [01:14, 3626.97it/s]188340it [01:15, 3375.99it/s]188730it [01:15, 3520.43it/s]189086it [01:15, 3273.27it/s]189466it [01:15, 3417.32it/s]189813it [01:15, 3340.58it/s]190203it [01:15, 3495.40it/s]190557it [01:15, 3383.06it/s]190928it [01:15, 3474.06it/s]191281it [01:15, 3487.33it/s]191632it [01:16, 3367.65it/s]192006it [01:16, 3472.86it/s]192356it [01:16, 3287.70it/s]192742it [01:16, 3448.54it/s]193090it [01:16, 3318.50it/s]193465it [01:16, 3438.65it/s]193843it [01:16, 3535.92it/s]194199it [01:16, 3407.37it/s]196566it [01:17, 200.34it/s] 194577it [01:16, 3512.75it/s]196954it [01:17, 284.96it/s]194931it [01:16, 3383.43it/s]197308it [01:17, 388.50it/s]195293it [01:17, 3450.10it/s]197622it [01:17, 508.05it/s]195640it [01:17, 3349.19it/s]198020it [01:18, 711.00it/s]196011it [01:17, 3451.12it/s]198358it [01:18, 914.69it/s]196388it [01:17, 3543.33it/s]196549it [01:18, 203.76it/s] 198747it [01:18, 1209.01it/s]196947it [01:18, 291.41it/s]199098it [01:18, 1483.00it/s]197272it [01:18, 386.35it/s]199489it [01:18, 1845.04it/s]197659it [01:18, 539.05it/s]199859it [01:18, 2121.32it/s]198062it [01:18, 744.74it/s]200220it [01:18, 2414.56it/s]198413it [01:18, 951.14it/s]200593it [01:18, 2702.61it/s]198804it [01:18, 1244.04it/s]200952it [01:18, 2832.99it/s]199160it [01:18, 1513.49it/s]201330it [01:18, 3067.00it/s]199559it [01:19, 1882.15it/s]201687it [01:19, 3116.08it/s]199921it [01:19, 2150.80it/s]202083it [01:19, 3341.65it/s]200303it [01:19, 2480.40it/s]202445it [01:19, 3367.84it/s]200665it [01:19, 2671.32it/s]202846it [01:19, 3546.77it/s]201047it [01:19, 2940.56it/s]203218it [01:19, 3483.70it/s]201419it [01:19, 3135.04it/s]203580it [01:19, 3521.92it/s]201783it [01:19, 3153.47it/s]203947it [01:19, 3562.64it/s]202174it [01:19, 3353.79it/s]204309it [01:19, 3477.05it/s]202537it [01:19, 3388.96it/s]204691it [01:19, 3574.83it/s]202942it [01:19, 3572.55it/s]205052it [01:20, 3486.28it/s]203314it [01:20, 3513.31it/s]205442it [01:20, 3604.34it/s]203691it [01:20, 3584.39it/s]205805it [01:20, 3509.00it/s]204058it [01:20, 3488.81it/s]206189it [01:20, 3603.53it/s]204440it [01:20, 3581.47it/s]206578it [01:20, 3516.38it/s]204811it [01:20, 3616.80it/s]206943it [01:20, 3553.96it/s]205176it [01:20, 3505.32it/s]207332it [01:20, 3650.29it/s]205560it [01:20, 3600.49it/s]207699it [01:20, 3543.13it/s]205923it [01:20, 3504.52it/s]208082it [01:20, 3624.33it/s]206312it [01:20, 3613.99it/s]208446it [01:20, 3549.85it/s]206676it [01:20, 3536.62it/s]208837it [01:21, 3652.03it/s]207069it [01:21, 3648.28it/s]209204it [01:21, 3566.88it/s]207436it [01:21, 3556.22it/s]209580it [01:21, 3621.64it/s]207822it [01:21, 3641.35it/s]209944it [01:21, 3476.60it/s]208189it [01:21, 3484.24it/s]210294it [01:21, 3463.94it/s]208574it [01:21, 3588.31it/s]210665it [01:21, 3533.94it/s]208963it [01:21, 3674.64it/s]211020it [01:21, 3435.77it/s]196607it [01:21, 191.63it/s] 209333it [01:21, 3577.16it/s]211404it [01:21, 3551.10it/s]196990it [01:21, 272.21it/s]209710it [01:21, 3630.48it/s]211761it [01:21, 3436.89it/s]197337it [01:21, 369.28it/s]210075it [01:21, 3493.88it/s]212145it [01:22, 3549.92it/s]197717it [01:21, 512.89it/s]210452it [01:22, 3570.20it/s]212502it [01:22, 3420.76it/s]198096it [01:21, 697.74it/s]210811it [01:22, 3459.73it/s]212879it [01:22, 3518.13it/s]198435it [01:21, 894.38it/s]211188it [01:22, 3547.14it/s]213235it [01:22, 3526.79it/s]198807it [01:21, 1164.39it/s]211545it [01:22, 3549.56it/s]213589it [01:22, 3395.11it/s]199151it [01:21, 1414.95it/s]211902it [01:22, 3439.72it/s]213975it [01:22, 3527.34it/s]199541it [01:21, 1774.04it/s]212272it [01:22, 3513.74it/s]214330it [01:22, 3427.89it/s]199890it [01:22, 2034.33it/s]212625it [01:22, 3433.59it/s]214698it [01:22, 3491.70it/s]200259it [01:22, 2355.01it/s]213002it [01:22, 3529.88it/s]215049it [01:22, 3404.39it/s]200632it [01:22, 2652.06it/s]213357it [01:22, 3435.69it/s]215441it [01:22, 3552.44it/s]200988it [01:22, 2768.35it/s]213729it [01:22, 3517.28it/s]215818it [01:23, 3459.43it/s]201351it [01:22, 2978.69it/s]214082it [01:23, 3435.37it/s]216203it [01:23, 3567.71it/s]201699it [01:22, 3026.45it/s]214434it [01:23, 3459.08it/s]216562it [01:23, 3533.47it/s]202072it [01:22, 3211.83it/s]214804it [01:23, 3527.54it/s]216917it [01:23, 3450.16it/s]202421it [01:22, 3232.92it/s]215158it [01:23, 3440.57it/s]217301it [01:23, 3560.35it/s]202805it [01:22, 3401.24it/s]215539it [01:23, 3545.44it/s]217659it [01:23, 3497.63it/s]203198it [01:22, 3551.58it/s]215895it [01:23, 3463.58it/s]218058it [01:23, 3634.85it/s]203565it [01:23, 3448.42it/s]216281it [01:23, 3576.95it/s]218423it [01:23, 3531.42it/s]203942it [01:23, 3538.59it/s]216640it [01:23, 3475.75it/s]218823it [01:23, 3664.51it/s]217033it [01:23, 3604.24it/s]204303it [01:23, 3365.04it/s]219191it [01:24, 3568.16it/s]217417it [01:24, 3672.44it/s]204679it [01:23, 3474.49it/s]219586it [01:24, 3676.91it/s]217786it [01:24, 3566.13it/s]205032it [01:23, 3371.52it/s]219956it [01:24, 3672.31it/s]218170it [01:24, 3644.77it/s]205399it [01:23, 3453.36it/s]220325it [01:24, 3597.84it/s]218536it [01:24, 3545.18it/s]205748it [01:23, 3349.86it/s]220729it [01:24, 3725.17it/s]218922it [01:24, 3634.80it/s]206135it [01:23, 3495.87it/s]221103it [01:24, 3597.41it/s]219287it [01:24, 3584.24it/s]206518it [01:23, 3591.54it/s]221507it [01:24, 3723.14it/s]219678it [01:24, 3678.47it/s]206880it [01:24, 3487.21it/s]221881it [01:24, 3637.89it/s]220047it [01:24, 3617.76it/s]207268it [01:24, 3599.07it/s]222261it [01:24, 3682.61it/s]220455it [01:24, 3750.36it/s]207630it [01:24, 3375.21it/s]196744it [01:24, 172.88it/s] 222631it [01:24, 3554.88it/s]220831it [01:24, 3648.40it/s]208017it [01:24, 3511.98it/s]197119it [01:24, 244.32it/s]222994it [01:25, 3573.79it/s]221207it [01:25, 3680.72it/s]208372it [01:24, 3351.10it/s]197413it [01:24, 319.08it/s]223363it [01:25, 3606.37it/s]221609it [01:25, 3778.51it/s]208752it [01:24, 3475.66it/s]197782it [01:24, 447.54it/s]223725it [01:25, 3507.46it/s]221988it [01:25, 3648.54it/s]209103it [01:24, 3416.40it/s]198170it [01:24, 624.77it/s]224111it [01:25, 3607.34it/s]222369it [01:25, 3693.61it/s]209480it [01:24, 3515.08it/s]198506it [01:24, 809.81it/s]224473it [01:25, 3501.42it/s]222740it [01:25, 3585.70it/s]209850it [01:24, 3567.28it/s]198884it [01:24, 1074.38it/s]224851it [01:25, 3579.43it/s]223120it [01:25, 3629.97it/s]210209it [01:24, 3437.88it/s]199229it [01:24, 1331.14it/s]225211it [01:25, 3485.92it/s]223485it [01:25, 3520.26it/s]210567it [01:25, 3476.94it/s]199611it [01:24, 1673.77it/s]225589it [01:25, 3569.76it/s]223874it [01:25, 3623.94it/s]210917it [01:25, 3359.67it/s]199962it [01:25, 1926.50it/s]225948it [01:25, 3446.55it/s]224238it [01:25, 3499.68it/s]211274it [01:25, 3419.35it/s]200326it [01:25, 2244.77it/s]226302it [01:25, 3470.78it/s]224611it [01:26, 3564.10it/s]200694it [01:25, 2545.51it/s]211618it [01:25, 3292.38it/s]226673it [01:26, 3538.89it/s]224989it [01:26, 3473.77it/s]211994it [01:25, 3424.88it/s]201046it [01:25, 2695.39it/s]227028it [01:26, 3429.56it/s]225364it [01:26, 3549.71it/s]212367it [01:25, 3510.74it/s]201415it [01:25, 2935.20it/s]227396it [01:26, 3409.25it/s]225749it [01:26, 3634.69it/s]212720it [01:25, 3398.70it/s]201764it [01:25, 3007.33it/s]227738it [01:26, 3343.91it/s]226114it [01:26, 3497.36it/s]213089it [01:25, 3481.12it/s]202135it [01:25, 3191.02it/s]228115it [01:26, 3464.02it/s]226491it [01:26, 3574.82it/s]202484it [01:25, 3217.21it/s]213439it [01:25, 3355.73it/s]228463it [01:26, 3379.76it/s]226851it [01:26, 3458.11it/s]202868it [01:25, 3385.23it/s]213790it [01:26, 3399.33it/s]228839it [01:26, 3479.76it/s]227225it [01:26, 3536.90it/s]203223it [01:26, 3346.19it/s]214138it [01:26, 3322.05it/s]229194it [01:26, 3499.17it/s]227581it [01:26, 3370.57it/s]203587it [01:26, 3427.33it/s]214494it [01:26, 3388.58it/s]229545it [01:26, 3420.90it/s]227954it [01:26, 3470.10it/s]203960it [01:26, 3512.21it/s]214861it [01:26, 3468.07it/s]229930it [01:27, 3543.13it/s]228332it [01:27, 3558.29it/s]204318it [01:26, 3410.59it/s]215209it [01:26, 3371.20it/s]230286it [01:27, 3440.75it/s]228690it [01:27, 3452.58it/s]204682it [01:26, 3474.61it/s]215589it [01:26, 3494.06it/s]230654it [01:27, 3509.38it/s]229064it [01:27, 3533.35it/s]205034it [01:26, 3371.83it/s]215940it [01:26, 3383.14it/s]231006it [01:27, 3439.45it/s]229419it [01:27, 3472.48it/s]205413it [01:26, 3490.23it/s]216318it [01:26, 3495.96it/s]231393it [01:27, 3563.73it/s]229798it [01:27, 3563.72it/s]231774it [01:27, 3634.76it/s]205765it [01:26, 3285.30it/s]216670it [01:26, 3266.61it/s]230156it [01:27, 3459.75it/s]206150it [01:26, 3442.09it/s]217056it [01:26, 3431.90it/s]232139it [01:27, 3496.70it/s]230530it [01:27, 3538.06it/s]206525it [01:26, 3528.34it/s]217422it [01:27, 3495.91it/s]232491it [01:27, 3456.11it/s]230886it [01:27, 3407.78it/s]217775it [01:27, 3369.66it/s]206881it [01:27, 3322.96it/s]232838it [01:27, 3341.22it/s]231277it [01:27, 3549.30it/s]218159it [01:27, 3502.68it/s]207272it [01:27, 3484.87it/s]233224it [01:27, 3488.39it/s]231653it [01:28, 3610.08it/s]218512it [01:27, 3410.34it/s]207625it [01:27, 3388.53it/s]233575it [01:28, 3419.91it/s]232016it [01:28, 3478.22it/s]218898it [01:27, 3537.98it/s]208010it [01:27, 3518.89it/s]233956it [01:28, 3532.14it/s]232394it [01:28, 3563.27it/s]219254it [01:27, 3443.40it/s]234311it [01:28, 3452.92it/s]208365it [01:27, 3438.27it/s]232752it [01:28, 3454.02it/s]219638it [01:27, 3556.35it/s]234678it [01:28, 3515.54it/s]208748it [01:27, 3545.71it/s]233131it [01:28, 3547.97it/s]235061it [01:28, 3605.85it/s]220018it [01:27, 3457.61it/s]209105it [01:27, 3404.59it/s]233488it [01:28, 3451.87it/s]220420it [01:27, 3614.78it/s]235423it [01:28, 3487.98it/s]209485it [01:27, 3514.53it/s]233849it [01:28, 3495.08it/s]220792it [01:28, 3644.85it/s]235774it [01:28, 3461.36it/s]209841it [01:27, 3526.35it/s]234229it [01:28, 3415.88it/s]236122it [01:28, 3397.45it/s]221159it [01:28, 3481.78it/s]210196it [01:28, 3337.40it/s]234601it [01:28, 3501.27it/s]236498it [01:28, 3501.99it/s]221553it [01:28, 3610.12it/s]210565it [01:28, 3434.66it/s]234980it [01:28, 3583.46it/s]236850it [01:29, 3407.12it/s]221917it [01:28, 3521.93it/s]210912it [01:28, 3332.84it/s]235340it [01:29, 3472.75it/s]237229it [01:29, 3514.94it/s]222282it [01:28, 3556.60it/s]211268it [01:28, 3390.96it/s]235704it [01:29, 3520.51it/s]237605it [01:29, 3584.70it/s]222640it [01:28, 3456.53it/s]211618it [01:28, 3311.89it/s]236058it [01:29, 3417.44it/s]237965it [01:29, 3484.25it/s]223005it [01:28, 3510.13it/s]211977it [01:28, 3389.78it/s]236430it [01:29, 3503.87it/s]238339it [01:29, 3555.78it/s]212352it [01:28, 3492.49it/s]223378it [01:28, 3404.42it/s]236782it [01:29, 3381.69it/s]238696it [01:29, 3386.44it/s]223745it [01:28, 3479.20it/s]212703it [01:28, 3324.25it/s]237149it [01:29, 3462.56it/s]239059it [01:29, 3453.96it/s]224118it [01:28, 3550.65it/s]213064it [01:28, 3403.42it/s]237522it [01:29, 3537.78it/s]239407it [01:29, 3395.11it/s]224475it [01:29, 3431.08it/s]213407it [01:28, 3306.47it/s]237878it [01:29, 3429.24it/s]239789it [01:29, 3516.71it/s]224846it [01:29, 3489.38it/s]213740it [01:29, 3221.40it/s]238246it [01:29, 3499.86it/s]240168it [01:29, 3594.88it/s]225197it [01:29, 3392.83it/s]214117it [01:29, 3374.66it/s]238598it [01:30, 3408.90it/s]240529it [01:30, 3480.75it/s]225569it [01:29, 3485.88it/s]214457it [01:29, 3293.23it/s]238975it [01:30, 3511.08it/s]240911it [01:30, 3576.87it/s]214820it [01:29, 3387.69it/s]225919it [01:29, 3274.84it/s]239328it [01:30, 3414.87it/s]241271it [01:30, 3471.25it/s]226284it [01:29, 3378.55it/s]215161it [01:29, 3293.68it/s]239706it [01:30, 3519.22it/s]241633it [01:30, 3512.27it/s]226642it [01:29, 3435.67it/s]215531it [01:29, 3407.89it/s]240067it [01:30, 3545.07it/s]241986it [01:30, 3413.89it/s]226988it [01:29, 3256.81it/s]215874it [01:29, 3256.00it/s]240423it [01:30, 3406.18it/s]242365it [01:30, 3521.15it/s]227353it [01:29, 3367.25it/s]216248it [01:29, 3392.66it/s]240800it [01:30, 3510.51it/s]242719it [01:30, 3432.54it/s]216628it [01:29, 3508.97it/s]227693it [01:30, 3258.02it/s]241153it [01:30, 3421.82it/s]243102it [01:30, 3546.66it/s]216981it [01:30, 3424.03it/s]228061it [01:30, 3375.09it/s]241529it [01:30, 3517.66it/s]243475it [01:30, 3599.50it/s]217358it [01:30, 3513.89it/s]228418it [01:30, 3289.04it/s]241883it [01:30, 3440.95it/s]243837it [01:31, 3507.89it/s]217711it [01:30, 3417.42it/s]228788it [01:30, 3403.33it/s]242249it [01:31, 3502.15it/s]244217it [01:31, 3591.71it/s]218081it [01:30, 3498.60it/s]229141it [01:30, 3437.12it/s]242625it [01:31, 3575.33it/s]244578it [01:31, 3470.39it/s]218433it [01:30, 3404.62it/s]229487it [01:30, 3370.71it/s]242984it [01:31, 3443.31it/s]244933it [01:31, 3491.60it/s]218817it [01:30, 3529.23it/s]229854it [01:30, 3456.90it/s]243350it [01:31, 3503.93it/s]245284it [01:31, 3377.67it/s]219178it [01:30, 3419.27it/s]230201it [01:30, 3272.88it/s]243702it [01:31, 3417.78it/s]245665it [01:31, 3500.86it/s]219572it [01:30, 3566.60it/s]230569it [01:30, 3386.18it/s]244079it [01:31, 3517.42it/s]246045it [01:31, 3587.50it/s]219944it [01:30, 3609.59it/s]230938it [01:31, 3318.78it/s]244432it [01:31, 3386.92it/s]246406it [01:31, 3485.41it/s]220307it [01:30, 3520.82it/s]231322it [01:31, 3463.35it/s]244807it [01:31, 3490.24it/s]246788it [01:31, 3579.45it/s]220709it [01:31, 3662.02it/s]231694it [01:31, 3536.77it/s]245158it [01:31, 3401.51it/s]247148it [01:31, 3493.46it/s]221077it [01:31, 3510.09it/s]232050it [01:31, 3389.16it/s]245536it [01:32, 3509.41it/s]247523it [01:32, 3565.09it/s]221469it [01:31, 3626.02it/s]232413it [01:31, 3457.02it/s]245910it [01:32, 3575.61it/s]247881it [01:32, 3459.40it/s]221834it [01:31, 3525.51it/s]232761it [01:31, 3337.94it/s]246269it [01:32, 3437.99it/s]248229it [01:32, 3462.28it/s]222194it [01:31, 3544.43it/s]233110it [01:31, 3380.43it/s]246626it [01:32, 3474.31it/s]248579it [01:32, 3387.64it/s]222550it [01:31, 3393.31it/s]233458it [01:31, 3308.82it/s]246975it [01:32, 3402.25it/s]248963it [01:32, 3517.39it/s]222928it [01:31, 3502.09it/s]233832it [01:31, 3430.87it/s]247354it [01:32, 3511.80it/s]249343it [01:32, 3597.44it/s]223307it [01:31, 3583.84it/s]234210it [01:31, 3531.15it/s]247707it [01:32, 3419.08it/s]249704it [01:32, 3471.09it/s]223668it [01:31, 3451.63it/s]234565it [01:32, 3394.29it/s]248082it [01:32, 3512.47it/s]250091it [01:32, 3583.70it/s]224055it [01:32, 3571.15it/s]234932it [01:32, 3470.79it/s]248455it [01:32, 3573.67it/s]250451it [01:32, 3505.21it/s]224415it [01:32, 3336.98it/s]235281it [01:32, 3263.82it/s]248814it [01:32, 3438.29it/s]250831it [01:33, 3590.18it/s]224778it [01:32, 3416.39it/s]235639it [01:32, 3351.67it/s]249191it [01:33, 3531.50it/s]251192it [01:33, 3436.75it/s]225124it [01:32, 3266.94it/s]235978it [01:32, 3206.16it/s]249546it [01:33, 3397.74it/s]251558it [01:33, 3498.55it/s]225495it [01:32, 3388.67it/s]236343it [01:32, 3329.03it/s]249918it [01:33, 3487.85it/s]251925it [01:33, 3546.69it/s]225869it [01:32, 3487.17it/s]236711it [01:32, 3428.87it/s]250269it [01:33, 3416.44it/s]252282it [01:33, 3483.65it/s]226221it [01:32, 3377.94it/s]237057it [01:32, 3342.12it/s]250655it [01:33, 3542.26it/s]252654it [01:33, 3552.21it/s]226594it [01:32, 3477.39it/s]237424it [01:32, 3435.55it/s]251027it [01:33, 3592.86it/s]253011it [01:33, 3469.58it/s]226944it [01:32, 3323.43it/s]237770it [01:33, 3333.44it/s]251388it [01:33, 3468.18it/s]253392it [01:33, 3567.51it/s]227304it [01:33, 3400.54it/s]238134it [01:33, 3419.03it/s]251764it [01:33, 3551.09it/s]238478it [01:33, 3414.40it/s]227647it [01:33, 3278.37it/s]252121it [01:33, 3466.29it/s]228008it [01:33, 3371.86it/s]238821it [01:33, 3309.70it/s]252492it [01:34, 3534.46it/s]228368it [01:33, 3436.84it/s]239177it [01:33, 3381.24it/s]252847it [01:34, 3407.54it/s]239517it [01:33, 3290.58it/s]253220it [01:34, 3497.32it/s]228714it [01:33, 3284.10it/s]239889it [01:33, 3413.34it/s]229073it [01:33, 3368.93it/s]253572it [01:34, 3415.75it/s]240232it [01:33, 3312.71it/s]229412it [01:33, 3314.99it/s]240598it [01:33, 3411.57it/s]229796it [01:33, 3464.25it/s]240969it [01:33, 3498.22it/s]230145it [01:33, 3349.69it/s]230502it [01:33, 3411.51it/s]241321it [01:34, 3280.99it/s]230876it [01:34, 3505.84it/s]241683it [01:34, 3374.84it/s]231228it [01:34, 3357.63it/s]242024it [01:34, 3250.42it/s]231597it [01:34, 3450.78it/s]242388it [01:34, 3359.43it/s]231944it [01:34, 3358.00it/s]242727it [01:34, 3281.88it/s]232318it [01:34, 3465.92it/s]243099it [01:34, 3405.27it/s]243468it [01:34, 3486.79it/s]232667it [01:34, 3361.62it/s]233041it [01:34, 3468.24it/s]243819it [01:34, 3374.56it/s]233398it [01:34, 3496.85it/s]244168it [01:34, 3397.25it/s]244510it [01:35, 3298.56it/s]233749it [01:34, 3297.26it/s]244873it [01:35, 3391.68it/s]234121it [01:35, 3415.94it/s]245218it [01:35, 3239.19it/s]234466it [01:35, 3250.29it/s]245590it [01:35, 3374.08it/s]234838it [01:35, 3380.27it/s]245958it [01:35, 3459.58it/s]235180it [01:35, 3301.85it/s]246306it [01:35, 3361.89it/s]235538it [01:35, 3378.64it/s]246676it [01:35, 3457.56it/s]235908it [01:35, 3465.56it/s]247024it [01:35, 3359.36it/s]236257it [01:35, 3356.25it/s]247370it [01:35, 3388.19it/s]236609it [01:35, 3401.13it/s]236951it [01:35, 3328.13it/s]247738it [01:36, 3285.13it/s]237305it [01:35, 3388.66it/s]248094it [01:36, 3361.95it/s]248459it [01:36, 3443.82it/s]237658it [01:36, 3253.13it/s]248805it [01:36, 3351.67it/s]238031it [01:36, 3386.08it/s]238401it [01:36, 3476.17it/s]249176it [01:36, 3452.66it/s]249523it [01:36, 3354.82it/s]238751it [01:36, 3368.99it/s]249893it [01:36, 3453.46it/s]239123it [01:36, 3469.47it/s]250255it [01:36, 3501.31it/s]239472it [01:36, 3252.84it/s]250607it [01:36, 3385.44it/s]239831it [01:36, 3346.32it/s]250968it [01:36, 3449.04it/s]240178it [01:36, 3200.60it/s]251315it [01:37, 3294.90it/s]240544it [01:36, 3327.63it/s]251684it [01:37, 3405.97it/s]240919it [01:37, 3447.13it/s]252027it [01:37, 3322.09it/s]241267it [01:37, 3349.57it/s]252402it [01:37, 3442.48it/s]241642it [01:37, 3462.41it/s]252773it [01:37, 3518.33it/s]241991it [01:37, 3334.57it/s]253127it [01:37, 3334.47it/s]242348it [01:37, 3394.08it/s]253494it [01:37, 3428.71it/s]242698it [01:37, 3306.78it/s]243060it [01:37, 3395.66it/s]243423it [01:37, 3461.95it/s]243771it [01:37, 3364.32it/s]244148it [01:38, 3478.94it/s]244498it [01:38, 3309.70it/s]244868it [01:38, 3415.76it/s]245218it [01:38, 3214.81it/s]245588it [01:38, 3348.89it/s]245952it [01:38, 3429.68it/s]246298it [01:38, 3247.43it/s]246668it [01:38, 3372.44it/s]247009it [01:38, 3247.66it/s]247382it [01:38, 3383.17it/s]247738it [01:39, 3289.55it/s]248104it [01:39, 3393.49it/s]248466it [01:39, 3457.95it/s]248814it [01:39, 3358.60it/s]249172it [01:39, 3421.92it/s]249516it [01:39, 3256.33it/s]249887it [01:39, 3384.52it/s]253750it [01:40, 172.51it/s] 250258it [01:39, 3308.26it/s]254117it [01:40, 241.82it/s]250638it [01:39, 3443.98it/s]254500it [01:40, 340.59it/s]251014it [01:40, 3532.84it/s]254823it [01:40, 449.90it/s]255204it [01:40, 622.14it/s]251370it [01:40, 3309.10it/s]251740it [01:40, 3415.92it/s]255541it [01:41, 807.08it/s]255887it [01:41, 1042.00it/s]252086it [01:40, 3217.27it/s]256224it [01:41, 1295.23it/s]252448it [01:40, 3326.43it/s]253915it [01:41, 164.30it/s] 256610it [01:41, 1647.48it/s]252785it [01:40, 3252.60it/s]254289it [01:41, 233.56it/s]256991it [01:41, 2001.73it/s]253160it [01:40, 3391.87it/s]254596it [01:41, 309.94it/s]257349it [01:41, 2260.07it/s]253533it [01:40, 3489.01it/s]254972it [01:41, 437.75it/s]257731it [01:41, 2587.76it/s]255298it [01:41, 578.16it/s]258090it [01:41, 2749.42it/s]255683it [01:41, 796.76it/s]258470it [01:41, 3005.02it/s]256062it [01:41, 1057.62it/s]258828it [01:42, 3073.59it/s]256411it [01:42, 1306.49it/s]259177it [01:42, 3181.64it/s]256780it [01:42, 1627.65it/s]259559it [01:42, 3355.96it/s]257126it [01:42, 1901.45it/s]259918it [01:42, 3329.82it/s]257506it [01:42, 2252.91it/s]260297it [01:42, 3457.83it/s]257857it [01:42, 2466.78it/s]260655it [01:42, 3401.35it/s]258231it [01:42, 2754.25it/s]261041it [01:42, 3530.45it/s]258610it [01:42, 3007.03it/s]261401it [01:42, 3477.56it/s]258970it [01:42, 3093.24it/s]261789it [01:42, 3591.42it/s]259352it [01:42, 3284.64it/s]262152it [01:42, 3488.92it/s]259713it [01:42, 3247.38it/s]262504it [01:43, 3479.96it/s]260077it [01:43, 3354.56it/s]262887it [01:43, 3580.25it/s]260430it [01:43, 3319.22it/s]263247it [01:43, 3498.71it/s]260819it [01:43, 3478.19it/s]263630it [01:43, 3593.69it/s]261178it [01:43, 3415.18it/s]263991it [01:43, 3511.68it/s]261565it [01:43, 3543.18it/s]264383it [01:43, 3627.49it/s]261948it [01:43, 3624.21it/s]264748it [01:43, 3492.22it/s]262315it [01:43, 3486.66it/s]265130it [01:43, 3585.14it/s]262698it [01:43, 3584.28it/s]265491it [01:43, 3425.76it/s]263060it [01:43, 3441.27it/s]265866it [01:44, 3517.30it/s]263435it [01:44, 3527.40it/s]266249it [01:44, 3606.58it/s]263791it [01:44, 3449.60it/s]266612it [01:44, 3532.19it/s]264167it [01:44, 3538.12it/s]266990it [01:44, 3602.26it/s]264538it [01:44, 3444.67it/s]267352it [01:44, 3530.22it/s]264919it [01:44, 3547.46it/s]267741it [01:44, 3632.93it/s]265302it [01:44, 3626.79it/s]268106it [01:44, 3549.24it/s]265667it [01:44, 3514.59it/s]268486it [01:44, 3621.83it/s]266038it [01:44, 3568.94it/s]268850it [01:44, 3431.42it/s]266397it [01:44, 3445.39it/s]269218it [01:44, 3501.76it/s]266778it [01:44, 3546.25it/s]269596it [01:45, 3580.33it/s]267135it [01:45, 3468.66it/s]269956it [01:45, 3493.40it/s]267523it [01:45, 3585.95it/s]270336it [01:45, 3580.19it/s]267891it [01:45, 3612.79it/s]270696it [01:45, 3485.98it/s]268254it [01:45, 3495.59it/s]271070it [01:45, 3557.00it/s]268632it [01:45, 3577.17it/s]271427it [01:45, 3462.09it/s]268992it [01:45, 3461.96it/s]271805it [01:45, 3551.64it/s]269352it [01:45, 3500.25it/s]272162it [01:45, 3553.61it/s]269704it [01:45, 3348.93it/s]272519it [01:45, 3431.16it/s]270081it [01:45, 3467.23it/s]272898it [01:45, 3532.37it/s]253840it [01:45, 151.42it/s] 270430it [01:46, 3391.22it/s]273253it [01:46, 3426.90it/s]254208it [01:45, 214.28it/s]270810it [01:46, 3506.16it/s]273635it [01:46, 3538.86it/s]254526it [01:45, 287.68it/s]271187it [01:46, 3580.30it/s]254898it [01:45, 405.12it/s]273991it [01:46, 3455.39it/s]271547it [01:46, 3465.99it/s]255265it [01:45, 557.27it/s]274375it [01:46, 3564.65it/s]271915it [01:46, 3527.49it/s]274733it [01:46, 3468.62it/s]255595it [01:45, 719.00it/s]272270it [01:46, 3419.64it/s]275115it [01:46, 3569.79it/s]255969it [01:45, 963.27it/s]272621it [01:46, 3444.70it/s]275474it [01:46, 3553.21it/s]256300it [01:46, 1188.14it/s]272967it [01:46, 3354.09it/s]275831it [01:46, 3461.28it/s]256668it [01:46, 1504.71it/s]273341it [01:46, 3464.17it/s]276213it [01:46, 3563.65it/s]257046it [01:46, 1805.51it/s]273708it [01:46, 3515.93it/s]276571it [01:47, 3473.61it/s]257417it [01:46, 2140.82it/s]274061it [01:47, 3440.15it/s]276950it [01:47, 3564.33it/s]257795it [01:46, 2469.07it/s]274440it [01:47, 3539.31it/s]277308it [01:47, 3466.59it/s]258148it [01:46, 2643.05it/s]274795it [01:47, 3445.19it/s]277679it [01:47, 3536.19it/s]258498it [01:46, 2846.28it/s]275173it [01:47, 3541.11it/s]278048it [01:47, 3442.67it/s]258843it [01:46, 2934.55it/s]275529it [01:47, 3392.36it/s]278410it [01:47, 3491.15it/s]259214it [01:46, 3135.52it/s]275897it [01:47, 3473.35it/s]278783it [01:47, 3559.94it/s]259567it [01:47, 3099.95it/s]276271it [01:47, 3548.40it/s]279141it [01:47, 3468.47it/s]259948it [01:47, 3290.72it/s]276628it [01:47, 3436.65it/s]279513it [01:47, 3538.84it/s]260326it [01:47, 3425.99it/s]277001it [01:47, 3518.90it/s]279868it [01:47, 3435.62it/s]260683it [01:47, 3352.63it/s]277355it [01:48, 3393.76it/s]280244it [01:48, 3528.26it/s]261052it [01:47, 3446.72it/s]277729it [01:48, 3492.73it/s]280599it [01:48, 3441.75it/s]261405it [01:47, 3370.77it/s]278080it [01:48, 3409.79it/s]280982it [01:48, 3552.96it/s]261772it [01:47, 3454.84it/s]278453it [01:48, 3500.51it/s]281357it [01:48, 3608.89it/s]262122it [01:47, 3365.10it/s]278808it [01:48, 3514.64it/s]281719it [01:48, 3419.00it/s]262483it [01:47, 3432.55it/s]279161it [01:48, 3399.11it/s]282093it [01:48, 3508.85it/s]262856it [01:47, 3517.75it/s]279522it [01:48, 3458.06it/s]282447it [01:48, 3427.79it/s]263210it [01:48, 3406.31it/s]279869it [01:48, 3373.40it/s]282823it [01:48, 3520.89it/s]263584it [01:48, 3501.34it/s]280242it [01:48, 3474.26it/s]283177it [01:48, 3432.65it/s]263936it [01:48, 3368.62it/s]280591it [01:48, 3391.98it/s]283555it [01:49, 3530.85it/s]264317it [01:48, 3493.32it/s]280971it [01:49, 3507.54it/s]283928it [01:49, 3427.18it/s]264669it [01:48, 3277.85it/s]281334it [01:49, 3542.23it/s]284315it [01:49, 3550.71it/s]265039it [01:48, 3395.46it/s]281690it [01:49, 3407.63it/s]284681it [01:49, 3579.94it/s]265408it [01:48, 3477.91it/s]282041it [01:49, 3434.87it/s]285041it [01:49, 3451.47it/s]265759it [01:48, 3324.66it/s]282386it [01:49, 3352.45it/s]285423it [01:49, 3556.58it/s]266136it [01:48, 3448.79it/s]282758it [01:49, 3457.61it/s]285781it [01:49, 3473.31it/s]266484it [01:49, 3345.22it/s]283105it [01:49, 3347.41it/s]286158it [01:49, 3558.34it/s]266859it [01:49, 3458.06it/s]283478it [01:49, 3456.20it/s]286516it [01:49, 3470.07it/s]267208it [01:49, 3380.78it/s]283857it [01:49, 3552.21it/s]253885it [01:49, 139.49it/s] 286899it [01:49, 3572.05it/s]267588it [01:49, 3500.34it/s]287112it [01:50, 2609.24it/s]
284214it [01:50, 3446.55it/s]254243it [01:49, 195.62it/s]267954it [01:49, 3545.23it/s]284592it [01:50, 3542.84it/s]254545it [01:49, 259.94it/s]268310it [01:49, 3437.81it/s]284948it [01:50, 3381.89it/s]254910it [01:49, 366.73it/s]268669it [01:49, 3479.05it/s]285318it [01:50, 3470.71it/s]255270it [01:49, 505.98it/s]269019it [01:49, 3312.33it/s]285668it [01:50, 3368.23it/s]255595it [01:49, 661.50it/s]269378it [01:49, 3390.78it/s]286041it [01:50, 3470.79it/s]255966it [01:49, 893.40it/s]269720it [01:49, 3309.05it/s]286390it [01:50, 3393.86it/s]256299it [01:49, 1124.94it/s]270098it [01:50, 3441.77it/s]286769it [01:50, 3488.48it/s]256674it [01:49, 1445.28it/s]270468it [01:50, 3515.77it/s]287112it [01:50, 2590.37it/s]
2022-07-18 16:37:50 | INFO | root | success load 287112 data
2022-07-18 16:37:50 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-18 16:37:50 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-18 16:37:50 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-18 16:37:50 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-18 16:37:50 | INFO | transformer.tokenization_utils | loading file None
2022-07-18 16:37:50 | INFO | transformer.tokenization_utils | loading file None
2022-07-18 16:37:50 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
257046it [01:50, 1740.67it/s]270822it [01:50, 3384.89it/s]257403it [01:50, 2053.82it/s]271187it [01:50, 3453.50it/s]257773it [01:50, 2377.27it/s]271534it [01:50, 3348.33it/s]258123it [01:50, 2533.21it/s]271887it [01:50, 3394.77it/s]258492it [01:50, 2801.66it/s]272228it [01:50, 3316.10it/s]258838it [01:50, 2899.53it/s]272593it [01:50, 3412.13it/s]259216it [01:50, 3125.18it/s]272965it [01:50, 3499.95it/s]259566it [01:50, 3135.68it/s]273317it [01:51, 3386.51it/s]259937it [01:50, 3290.49it/s]273693it [01:51, 3492.68it/s]260306it [01:51, 3399.94it/s]274044it [01:51, 3290.12it/s]260661it [01:51, 3298.14it/s]274415it [01:51, 3407.71it/s]261034it [01:51, 3417.33it/s]274759it [01:51, 3225.23it/s]261384it [01:51, 3286.74it/s]275134it [01:51, 3369.37it/s]261763it [01:51, 3425.73it/s]275514it [01:51, 3489.94it/s]262112it [01:51, 3319.77it/s]275867it [01:51, 3394.41it/s]262492it [01:51, 3454.48it/s]276238it [01:51, 3483.25it/s]262871it [01:51, 3550.39it/s]276589it [01:51, 3370.25it/s]263230it [01:51, 3432.79it/s]276949it [01:52, 3433.58it/s]263590it [01:51, 3479.36it/s]277295it [01:52, 3310.62it/s]263941it [01:52, 3387.42it/s]277656it [01:52, 3394.39it/s]264300it [01:52, 3443.70it/s]278018it [01:52, 3457.09it/s]264646it [01:52, 3316.51it/s]278366it [01:52, 3340.94it/s]265022it [01:52, 3440.40it/s]278741it [01:52, 3457.18it/s]265398it [01:52, 3532.41it/s]279089it [01:52, 3354.22it/s]265753it [01:52, 3419.52it/s]279457it [01:52, 3446.66it/s]266131it [01:52, 3522.18it/s]279804it [01:52, 3338.57it/s]266485it [01:52, 3377.74it/s]280142it [01:53, 3350.09it/s]266854it [01:52, 3466.09it/s]280510it [01:53, 3445.30it/s]267203it [01:53, 3366.84it/s]280856it [01:53, 3320.72it/s]267572it [01:53, 3457.32it/s]281215it [01:53, 3395.85it/s]267943it [01:53, 3529.31it/s]281556it [01:53, 3310.88it/s]268298it [01:53, 3419.70it/s]281927it [01:53, 3423.69it/s]268663it [01:53, 3485.60it/s]282271it [01:53, 3327.98it/s]269013it [01:53, 3353.74it/s]282642it [01:53, 3436.14it/s]269381it [01:53, 3446.31it/s]282998it [01:53, 3471.46it/s]269728it [01:53, 3223.55it/s]283347it [01:53, 3248.70it/s]270103it [01:53, 3369.90it/s]283717it [01:54, 3367.68it/s]270464it [01:54, 3411.41it/s]284057it [01:54, 3231.00it/s]270808it [01:54, 3244.56it/s]284427it [01:54, 3361.77it/s]271180it [01:54, 3376.90it/s]284767it [01:54, 3285.71it/s]271521it [01:54, 3287.72it/s]285140it [01:54, 3410.89it/s]271895it [01:54, 3413.62it/s]285501it [01:54, 3467.13it/s]272239it [01:54, 3311.19it/s]285850it [01:54, 3364.88it/s]272600it [01:54, 3394.74it/s]286214it [01:54, 3444.03it/s]272956it [01:54, 3442.34it/s]286560it [01:54, 3331.17it/s]273302it [01:54, 3326.46it/s]286929it [01:55, 3431.46it/s]273668it [01:54, 3421.07it/s]287112it [01:55, 2494.50it/s]
274012it [01:55, 3274.11it/s]274392it [01:55, 3421.63it/s]274737it [01:55, 3306.61it/s]275116it [01:55, 3442.66it/s]275500it [01:55, 3557.21it/s]275858it [01:55, 3417.73it/s]276227it [01:55, 3489.03it/s]276578it [01:55, 3359.75it/s]276927it [01:55, 3396.53it/s]277269it [01:56, 3316.29it/s]277640it [01:56, 3427.09it/s]278016it [01:56, 3523.08it/s]278370it [01:56, 3396.53it/s]278747it [01:56, 3503.70it/s]279100it [01:56, 3248.15it/s]279477it [01:56, 3391.87it/s]279821it [01:56, 3236.30it/s]280185it [01:56, 3348.00it/s]280561it [01:56, 3462.96it/s]280911it [01:57, 3367.19it/s]281271it [01:57, 3433.62it/s]281617it [01:57, 3337.75it/s]281990it [01:57, 3449.02it/s]282337it [01:57, 3314.33it/s]282711it [01:57, 3434.28it/s]283071it [01:57, 3480.51it/s]283421it [01:57, 3280.95it/s]283795it [01:57, 3409.23it/s]284139it [01:58, 3328.33it/s]284517it [01:58, 3456.18it/s]284865it [01:58, 3368.85it/s]285227it [01:58, 3439.71it/s]285592it [01:58, 3500.08it/s]285944it [01:58, 3364.14it/s]286307it [01:58, 3438.80it/s]286653it [01:58, 3362.29it/s]287028it [01:58, 3473.68it/s]287112it [01:58, 2414.36it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-18 16:38:41 | INFO | train_inner | epoch 005:     14 / 1122 loss=7.978, nll_loss=3.363, mask_ins=1.108, word_ins_ml=4.946, word_reposition=0.909, kpe=1.015, ppl=252.07, wps=4066.2, ups=0.2, wpb=19859.5, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=2.143, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-18 16:43:31 | INFO | train_inner | epoch 005:    114 / 1122 loss=7.887, nll_loss=3.3, mask_ins=1.102, word_ins_ml=4.89, word_reposition=0.894, kpe=1.001, ppl=236.68, wps=6930, ups=0.34, wpb=20097.9, bsz=256, num_updates=4600, lr=0.000460008, gnorm=2.057, clip=0, loss_scale=4096, train_wall=252, wall=0
2022-07-18 16:48:21 | INFO | train_inner | epoch 005:    214 / 1122 loss=7.936, nll_loss=3.331, mask_ins=1.105, word_ins_ml=4.917, word_reposition=0.903, kpe=1.012, ppl=244.85, wps=6900.7, ups=0.34, wpb=20008, bsz=256, num_updates=4700, lr=0.000470006, gnorm=2.143, clip=0, loss_scale=4096, train_wall=251, wall=0
2022-07-18 16:53:12 | INFO | train_inner | epoch 005:    314 / 1122 loss=7.848, nll_loss=3.253, mask_ins=1.11, word_ins_ml=4.847, word_reposition=0.888, kpe=1.004, ppl=230.44, wps=6925.2, ups=0.34, wpb=20126.3, bsz=256, num_updates=4800, lr=0.000480004, gnorm=2.08, clip=0, loss_scale=4096, train_wall=252, wall=0
2022-07-18 16:53:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-18 16:58:07 | INFO | train_inner | epoch 005:    415 / 1122 loss=7.912, nll_loss=3.318, mask_ins=1.101, word_ins_ml=4.903, word_reposition=0.897, kpe=1.011, ppl=240.9, wps=6778.7, ups=0.34, wpb=19988.2, bsz=256, num_updates=4900, lr=0.000490002, gnorm=2.124, clip=0, loss_scale=2312, train_wall=254, wall=0
2022-07-18 17:02:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-18 17:03:07 | INFO | train_inner | epoch 005:    516 / 1122 loss=7.817, nll_loss=3.243, mask_ins=1.085, word_ins_ml=4.836, word_reposition=0.889, kpe=1.007, ppl=225.49, wps=6626.4, ups=0.33, wpb=19884.7, bsz=256, num_updates=5000, lr=0.0005, gnorm=2.03, clip=0, loss_scale=1805, train_wall=255, wall=0
2022-07-18 17:08:46 | INFO | train_inner | epoch 005:    616 / 1122 loss=7.771, nll_loss=3.232, mask_ins=1.067, word_ins_ml=4.825, word_reposition=0.874, kpe=1.005, ppl=218.49, wps=5891.6, ups=0.3, wpb=19966.7, bsz=256, num_updates=5100, lr=0.000495074, gnorm=2.023, clip=0, loss_scale=1024, train_wall=300, wall=0
2022-07-18 17:13:36 | INFO | train_inner | epoch 005:    716 / 1122 loss=7.73, nll_loss=3.181, mask_ins=1.068, word_ins_ml=4.779, word_reposition=0.879, kpe=1.005, ppl=212.32, wps=6912.2, ups=0.35, wpb=20025.4, bsz=256, num_updates=5200, lr=0.00049029, gnorm=1.958, clip=0, loss_scale=1024, train_wall=252, wall=0
2022-07-18 17:18:25 | INFO | train_inner | epoch 005:    816 / 1122 loss=7.748, nll_loss=3.228, mask_ins=1.054, word_ins_ml=4.819, word_reposition=0.874, kpe=1.001, ppl=215.04, wps=6943.4, ups=0.35, wpb=20089.2, bsz=256, num_updates=5300, lr=0.000485643, gnorm=1.95, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-18 17:23:14 | INFO | train_inner | epoch 005:    916 / 1122 loss=7.723, nll_loss=3.189, mask_ins=1.062, word_ins_ml=4.783, word_reposition=0.877, kpe=1.001, ppl=211.32, wps=6934.7, ups=0.35, wpb=20060.4, bsz=256, num_updates=5400, lr=0.000481125, gnorm=1.994, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-18 17:28:04 | INFO | train_inner | epoch 005:   1016 / 1122 loss=7.658, nll_loss=3.147, mask_ins=1.049, word_ins_ml=4.745, word_reposition=0.862, kpe=1.001, ppl=201.92, wps=6867.6, ups=0.35, wpb=19904.6, bsz=256, num_updates=5500, lr=0.000476731, gnorm=1.914, clip=0, loss_scale=1147, train_wall=251, wall=0
2022-07-18 17:32:54 | INFO | train_inner | epoch 005:   1116 / 1122 loss=7.657, nll_loss=3.128, mask_ins=1.055, word_ins_ml=4.727, word_reposition=0.87, kpe=1.004, ppl=201.82, wps=6913.4, ups=0.34, wpb=20038.8, bsz=256, num_updates=5600, lr=0.000472456, gnorm=2.023, clip=0, loss_scale=2048, train_wall=252, wall=0
2022-07-18 17:33:10 | INFO | train | epoch 005 | loss 7.793 | nll_loss 3.234 | mask_ins 1.079 | word_ins_ml 4.826 | word_reposition 0.883 | kpe 1.005 | ppl 221.75 | wps 6380.9 | ups 0.32 | wpb 20003.5 | bsz 255.8 | num_updates 5606 | lr 0.000472203 | gnorm 2.04 | clip 0 | loss_scale 2178 | train_wall 2880 | wall 0
2022-07-18 17:34:30 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 12.032 | nll_loss 6.334 | mask_ins 1.67 | word_ins_ml 7.702 | word_reposition 1.323 | kpe 1.338 | ppl 4187.23 | wps 12160.5 | wpb 2325.1 | bsz 32 | num_updates 5606 | best_loss 12.032
2022-07-18 17:34:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_best.pt (epoch 5 @ 5606 updates, score 12.032) (writing took 5.881839339621365 seconds)
2022-07-18 17:39:08 | INFO | train_inner | epoch 006:     94 / 1122 loss=7.637, nll_loss=3.119, mask_ins=1.054, word_ins_ml=4.719, word_reposition=0.865, kpe=1, ppl=199.12, wps=5288.8, ups=0.27, wpb=19812.8, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=1.995, clip=0, loss_scale=2048, train_wall=251, wall=0
2022-07-18 17:43:58 | INFO | train_inner | epoch 006:    194 / 1122 loss=7.555, nll_loss=3.084, mask_ins=1.025, word_ins_ml=4.687, word_reposition=0.853, kpe=0.99, ppl=188.04, wps=6938.8, ups=0.35, wpb=20101, bsz=256, num_updates=5800, lr=0.000464238, gnorm=1.851, clip=0, loss_scale=2048, train_wall=252, wall=0
2022-07-18 17:48:48 | INFO | train_inner | epoch 006:    294 / 1122 loss=7.549, nll_loss=3.087, mask_ins=1.018, word_ins_ml=4.689, word_reposition=0.853, kpe=0.99, ppl=187.32, wps=6935.4, ups=0.35, wpb=20068.3, bsz=256, num_updates=5900, lr=0.000460287, gnorm=1.805, clip=0, loss_scale=2048, train_wall=251, wall=0
2022-07-18 17:53:37 | INFO | train_inner | epoch 006:    394 / 1122 loss=7.478, nll_loss=3.027, mask_ins=1.006, word_ins_ml=4.635, word_reposition=0.847, kpe=0.99, ppl=178.28, wps=6925.5, ups=0.35, wpb=20031.2, bsz=256, num_updates=6000, lr=0.000456435, gnorm=1.78, clip=0, loss_scale=2048, train_wall=251, wall=0
2022-07-18 17:58:26 | INFO | train_inner | epoch 006:    494 / 1122 loss=7.442, nll_loss=3.005, mask_ins=1.004, word_ins_ml=4.615, word_reposition=0.841, kpe=0.983, ppl=173.85, wps=6901.9, ups=0.35, wpb=19956.7, bsz=256, num_updates=6100, lr=0.000452679, gnorm=1.796, clip=0, loss_scale=4096, train_wall=251, wall=0
2022-07-18 18:03:16 | INFO | train_inner | epoch 006:    594 / 1122 loss=7.478, nll_loss=3.046, mask_ins=1, word_ins_ml=4.65, word_reposition=0.843, kpe=0.985, ppl=178.23, wps=6844.8, ups=0.35, wpb=19837.9, bsz=256, num_updates=6200, lr=0.000449013, gnorm=1.745, clip=0, loss_scale=4096, train_wall=252, wall=0
2022-07-18 18:04:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-18 18:08:08 | INFO | train_inner | epoch 006:    695 / 1122 loss=7.428, nll_loss=2.99, mask_ins=0.998, word_ins_ml=4.6, word_reposition=0.849, kpe=0.982, ppl=172.22, wps=6880.6, ups=0.34, wpb=20128.8, bsz=256, num_updates=6300, lr=0.000445435, gnorm=1.751, clip=0, loss_scale=2595, train_wall=253, wall=0
2022-07-18 18:13:46 | INFO | train_inner | epoch 006:    795 / 1122 loss=7.331, nll_loss=2.929, mask_ins=0.977, word_ins_ml=4.545, word_reposition=0.831, kpe=0.979, ppl=161.06, wps=5916.5, ups=0.3, wpb=19994.5, bsz=256, num_updates=6400, lr=0.000441942, gnorm=1.84, clip=0, loss_scale=2048, train_wall=299, wall=0
2022-07-18 18:18:35 | INFO | train_inner | epoch 006:    895 / 1122 loss=7.37, nll_loss=2.95, mask_ins=0.988, word_ins_ml=4.563, word_reposition=0.84, kpe=0.98, ppl=165.42, wps=6997.9, ups=0.35, wpb=20188.9, bsz=256, num_updates=6500, lr=0.000438529, gnorm=1.74, clip=0, loss_scale=2048, train_wall=250, wall=0
2022-07-18 18:23:27 | INFO | train_inner | epoch 006:    995 / 1122 loss=7.347, nll_loss=2.95, mask_ins=0.975, word_ins_ml=4.562, word_reposition=0.83, kpe=0.98, ppl=162.76, wps=6834.9, ups=0.34, wpb=19987.5, bsz=256, num_updates=6600, lr=0.000435194, gnorm=1.719, clip=0, loss_scale=2048, train_wall=251, wall=0
2022-07-18 18:28:21 | INFO | train_inner | epoch 006:   1095 / 1122 loss=7.342, nll_loss=2.965, mask_ins=0.958, word_ins_ml=4.574, word_reposition=0.828, kpe=0.982, ppl=162.29, wps=6798.4, ups=0.34, wpb=19996.3, bsz=256, num_updates=6700, lr=0.000431934, gnorm=1.724, clip=0, loss_scale=2048, train_wall=252, wall=0
2022-07-18 18:29:38 | INFO | train | epoch 006 | loss 7.447 | nll_loss 3.012 | mask_ins 0.999 | word_ins_ml 4.62 | word_reposition 0.843 | kpe 0.985 | ppl 174.52 | wps 6620.4 | ups 0.33 | wpb 20005.8 | bsz 255.8 | num_updates 6727 | lr 0.000431067 | gnorm 1.789 | clip 0 | loss_scale 2462 | train_wall 2866 | wall 0
2022-07-18 18:30:58 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.835 | nll_loss 6.251 | mask_ins 1.536 | word_ins_ml 7.636 | word_reposition 1.289 | kpe 1.373 | ppl 3652.59 | wps 12135.1 | wpb 2325.1 | bsz 32 | num_updates 6727 | best_loss 11.835
2022-07-18 18:31:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_best.pt (epoch 6 @ 6727 updates, score 11.835) (writing took 5.7618826143443584 seconds)
2022-07-18 18:34:35 | INFO | train_inner | epoch 007:     73 / 1122 loss=7.327, nll_loss=2.956, mask_ins=0.968, word_ins_ml=4.567, word_reposition=0.819, kpe=0.973, ppl=160.56, wps=5288.2, ups=0.27, wpb=19763.6, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=1.793, clip=0, loss_scale=3318, train_wall=250, wall=0
2022-07-18 18:39:24 | INFO | train_inner | epoch 007:    173 / 1122 loss=7.274, nll_loss=2.906, mask_ins=0.96, word_ins_ml=4.522, word_reposition=0.829, kpe=0.964, ppl=154.8, wps=6961.7, ups=0.35, wpb=20104.3, bsz=256, num_updates=6900, lr=0.000425628, gnorm=1.724, clip=0, loss_scale=4096, train_wall=251, wall=0
2022-07-18 18:44:14 | INFO | train_inner | epoch 007:    273 / 1122 loss=7.247, nll_loss=2.879, mask_ins=0.961, word_ins_ml=4.497, word_reposition=0.827, kpe=0.962, ppl=151.86, wps=6946.6, ups=0.34, wpb=20155.5, bsz=256, num_updates=7000, lr=0.000422577, gnorm=1.649, clip=0, loss_scale=4096, train_wall=252, wall=0
2022-07-18 18:49:03 | INFO | train_inner | epoch 007:    373 / 1122 loss=7.219, nll_loss=2.866, mask_ins=0.951, word_ins_ml=4.484, word_reposition=0.813, kpe=0.971, ppl=149, wps=6921.2, ups=0.35, wpb=19990.7, bsz=256, num_updates=7100, lr=0.000419591, gnorm=1.671, clip=0, loss_scale=4096, train_wall=251, wall=0
2022-07-18 18:53:52 | INFO | train_inner | epoch 007:    473 / 1122 loss=7.213, nll_loss=2.864, mask_ins=0.944, word_ins_ml=4.482, word_reposition=0.821, kpe=0.965, ppl=148.32, wps=6918.6, ups=0.35, wpb=20016, bsz=256, num_updates=7200, lr=0.000416667, gnorm=1.643, clip=0, loss_scale=4096, train_wall=251, wall=0
2022-07-18 18:56:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-18 18:58:44 | INFO | train_inner | epoch 007:    574 / 1122 loss=7.24, nll_loss=2.89, mask_ins=0.95, word_ins_ml=4.504, word_reposition=0.817, kpe=0.969, ppl=151.17, wps=6829.4, ups=0.34, wpb=19914, bsz=256, num_updates=7300, lr=0.000413803, gnorm=1.68, clip=0, loss_scale=4502, train_wall=253, wall=0
2022-07-18 19:03:33 | INFO | train_inner | epoch 007:    674 / 1122 loss=7.19, nll_loss=2.851, mask_ins=0.945, word_ins_ml=4.47, word_reposition=0.811, kpe=0.964, ppl=146.04, wps=6923.8, ups=0.35, wpb=20014.3, bsz=256, num_updates=7400, lr=0.000410997, gnorm=1.622, clip=0, loss_scale=4096, train_wall=250, wall=0
2022-07-18 19:06:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-18 19:08:24 | INFO | train_inner | epoch 007:    775 / 1122 loss=7.199, nll_loss=2.851, mask_ins=0.95, word_ins_ml=4.469, word_reposition=0.814, kpe=0.966, ppl=146.93, wps=6877.3, ups=0.34, wpb=20042.5, bsz=256, num_updates=7500, lr=0.000408248, gnorm=1.656, clip=0, loss_scale=3325, train_wall=253, wall=0
2022-07-18 19:09:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-18 19:11:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-18 19:11:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-18 19:13:45 | INFO | train_inner | epoch 007:    878 / 1122 loss=7.263, nll_loss=2.891, mask_ins=0.953, word_ins_ml=4.504, word_reposition=0.823, kpe=0.982, ppl=153.57, wps=6246.1, ups=0.31, wpb=20047.4, bsz=256, num_updates=7600, lr=0.000405554, gnorm=2.78, clip=0, loss_scale=935, train_wall=281, wall=0
2022-07-18 19:18:57 | INFO | train_inner | epoch 007:    978 / 1122 loss=7.156, nll_loss=2.815, mask_ins=0.936, word_ins_ml=4.437, word_reposition=0.811, kpe=0.973, ppl=142.58, wps=6442.8, ups=0.32, wpb=20084.2, bsz=256, num_updates=7700, lr=0.000402911, gnorm=1.697, clip=0, loss_scale=256, train_wall=273, wall=0
2022-07-18 19:23:46 | INFO | train_inner | epoch 007:   1078 / 1122 loss=7.146, nll_loss=2.826, mask_ins=0.93, word_ins_ml=4.446, word_reposition=0.805, kpe=0.966, ppl=141.66, wps=6931.5, ups=0.35, wpb=20000.3, bsz=256, num_updates=7800, lr=0.00040032, gnorm=1.652, clip=0, loss_scale=256, train_wall=250, wall=0
2022-07-18 19:25:52 | INFO | train | epoch 007 | loss 7.22 | nll_loss 2.869 | mask_ins 0.949 | word_ins_ml 4.486 | word_reposition 0.817 | kpe 0.968 | ppl 149.12 | wps 6624.2 | ups 0.33 | wpb 20007.7 | bsz 255.8 | num_updates 7844 | lr 0.000399196 | gnorm 1.777 | clip 0 | loss_scale 2918 | train_wall 2859 | wall 0
2022-07-18 19:27:11 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.49 | nll_loss 5.986 | mask_ins 1.505 | word_ins_ml 7.382 | word_reposition 1.237 | kpe 1.366 | ppl 2877.29 | wps 12186.7 | wpb 2325.1 | bsz 32 | num_updates 7844 | best_loss 11.49
2022-07-18 19:27:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_best.pt (epoch 7 @ 7844 updates, score 11.49) (writing took 5.834930000826716 seconds)
2022-07-18 19:29:59 | INFO | train_inner | epoch 008:     56 / 1122 loss=7.14, nll_loss=2.814, mask_ins=0.943, word_ins_ml=4.436, word_reposition=0.806, kpe=0.955, ppl=141.01, wps=5317.5, ups=0.27, wpb=19838.4, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=1.688, clip=0, loss_scale=256, train_wall=249, wall=0
2022-07-18 19:34:48 | INFO | train_inner | epoch 008:    156 / 1122 loss=7.086, nll_loss=2.783, mask_ins=0.922, word_ins_ml=4.407, word_reposition=0.805, kpe=0.952, ppl=135.88, wps=6929.8, ups=0.35, wpb=20033.8, bsz=256, num_updates=8000, lr=0.000395285, gnorm=1.6, clip=0, loss_scale=256, train_wall=251, wall=0
2022-07-18 19:39:36 | INFO | train_inner | epoch 008:    256 / 1122 loss=7.109, nll_loss=2.809, mask_ins=0.922, word_ins_ml=4.43, word_reposition=0.804, kpe=0.953, ppl=138.08, wps=6938.4, ups=0.35, wpb=20020.7, bsz=256, num_updates=8100, lr=0.000392837, gnorm=1.628, clip=0, loss_scale=317, train_wall=250, wall=0
2022-07-18 19:44:25 | INFO | train_inner | epoch 008:    356 / 1122 loss=7.105, nll_loss=2.804, mask_ins=0.921, word_ins_ml=4.425, word_reposition=0.804, kpe=0.956, ppl=137.65, wps=6922.6, ups=0.35, wpb=20007.5, bsz=256, num_updates=8200, lr=0.000390434, gnorm=1.593, clip=0, loss_scale=512, train_wall=251, wall=0
2022-07-18 19:49:20 | INFO | train_inner | epoch 008:    456 / 1122 loss=7.042, nll_loss=2.759, mask_ins=0.914, word_ins_ml=4.385, word_reposition=0.792, kpe=0.951, ppl=131.75, wps=6778.6, ups=0.34, wpb=19994.6, bsz=256, num_updates=8300, lr=0.000388075, gnorm=1.595, clip=0, loss_scale=512, train_wall=253, wall=0
2022-07-18 19:54:10 | INFO | train_inner | epoch 008:    556 / 1122 loss=7.075, nll_loss=2.771, mask_ins=0.922, word_ins_ml=4.394, word_reposition=0.802, kpe=0.956, ppl=134.79, wps=6935.7, ups=0.35, wpb=20080.1, bsz=256, num_updates=8400, lr=0.000385758, gnorm=1.576, clip=0, loss_scale=512, train_wall=250, wall=0
2022-07-18 19:58:59 | INFO | train_inner | epoch 008:    656 / 1122 loss=7.071, nll_loss=2.777, mask_ins=0.916, word_ins_ml=4.4, word_reposition=0.798, kpe=0.957, ppl=134.46, wps=6952.5, ups=0.35, wpb=20109, bsz=256, num_updates=8500, lr=0.000383482, gnorm=1.737, clip=0, loss_scale=512, train_wall=251, wall=0
2022-07-18 20:03:48 | INFO | train_inner | epoch 008:    756 / 1122 loss=7.037, nll_loss=2.746, mask_ins=0.915, word_ins_ml=4.372, word_reposition=0.796, kpe=0.954, ppl=131.33, wps=6914.6, ups=0.35, wpb=19975, bsz=256, num_updates=8600, lr=0.000381246, gnorm=1.601, clip=0, loss_scale=573, train_wall=251, wall=0
2022-07-18 20:08:37 | INFO | train_inner | epoch 008:    856 / 1122 loss=7.04, nll_loss=2.749, mask_ins=0.92, word_ins_ml=4.374, word_reposition=0.793, kpe=0.953, ppl=131.6, wps=6953.2, ups=0.35, wpb=20075.8, bsz=256, num_updates=8700, lr=0.000379049, gnorm=1.616, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-18 20:13:25 | INFO | train_inner | epoch 008:    956 / 1122 loss=7.009, nll_loss=2.733, mask_ins=0.912, word_ins_ml=4.359, word_reposition=0.787, kpe=0.95, ppl=128.76, wps=6930.9, ups=0.35, wpb=19989.5, bsz=256, num_updates=8800, lr=0.000376889, gnorm=1.514, clip=0, loss_scale=1024, train_wall=250, wall=0
2022-07-18 20:18:46 | INFO | train_inner | epoch 008:   1056 / 1122 loss=7.016, nll_loss=2.726, mask_ins=0.918, word_ins_ml=4.353, word_reposition=0.794, kpe=0.95, ppl=129.44, wps=6180.3, ups=0.31, wpb=19852.7, bsz=256, num_updates=8900, lr=0.000374766, gnorm=1.649, clip=0, loss_scale=1024, train_wall=283, wall=0
2022-07-18 20:22:08 | INFO | train | epoch 008 | loss 7.058 | nll_loss 2.764 | mask_ins 0.919 | word_ins_ml 4.388 | word_reposition 0.798 | kpe 0.953 | ppl 133.25 | wps 6647.8 | ups 0.33 | wpb 20006.4 | bsz 255.8 | num_updates 8966 | lr 0.000373384 | gnorm 1.622 | clip 0 | loss_scale 632 | train_wall 2859 | wall 0
2022-07-18 20:23:28 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 11.49 | nll_loss 5.934 | mask_ins 1.473 | word_ins_ml 7.339 | word_reposition 1.305 | kpe 1.374 | ppl 2877.02 | wps 12170.4 | wpb 2325.1 | bsz 32 | num_updates 8966 | best_loss 11.49
2022-07-18 20:23:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_best.pt (epoch 8 @ 8966 updates, score 11.49) (writing took 6.990386170335114 seconds)
2022-07-18 20:25:13 | INFO | train_inner | epoch 009:     34 / 1122 loss=7.021, nll_loss=2.727, mask_ins=0.915, word_ins_ml=4.354, word_reposition=0.799, kpe=0.954, ppl=129.91, wps=5118.3, ups=0.26, wpb=19809.2, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=1.814, clip=0, loss_scale=1024, train_wall=263, wall=0
2022-07-18 20:30:02 | INFO | train_inner | epoch 009:    134 / 1122 loss=6.981, nll_loss=2.722, mask_ins=0.898, word_ins_ml=4.349, word_reposition=0.789, kpe=0.945, ppl=126.31, wps=6981.8, ups=0.35, wpb=20177.5, bsz=256, num_updates=9100, lr=0.000370625, gnorm=1.696, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-18 20:34:51 | INFO | train_inner | epoch 009:    234 / 1122 loss=6.982, nll_loss=2.707, mask_ins=0.909, word_ins_ml=4.336, word_reposition=0.789, kpe=0.948, ppl=126.41, wps=6922.1, ups=0.35, wpb=19977, bsz=256, num_updates=9200, lr=0.000368605, gnorm=1.738, clip=0, loss_scale=2048, train_wall=251, wall=0
2022-07-18 20:38:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-18 20:39:44 | INFO | train_inner | epoch 009:    335 / 1122 loss=7.011, nll_loss=2.745, mask_ins=0.909, word_ins_ml=4.369, word_reposition=0.789, kpe=0.945, ppl=128.97, wps=6815.5, ups=0.34, wpb=19934.5, bsz=256, num_updates=9300, lr=0.000366618, gnorm=1.635, clip=0, loss_scale=1764, train_wall=254, wall=0
2022-07-18 20:44:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-18 20:44:35 | INFO | train_inner | epoch 009:    436 / 1122 loss=6.916, nll_loss=2.677, mask_ins=0.883, word_ins_ml=4.308, word_reposition=0.778, kpe=0.946, ppl=120.73, wps=6848.3, ups=0.34, wpb=19968.5, bsz=256, num_updates=9400, lr=0.000364662, gnorm=1.707, clip=0, loss_scale=973, train_wall=253, wall=0
2022-07-18 20:49:25 | INFO | train_inner | epoch 009:    536 / 1122 loss=6.963, nll_loss=2.698, mask_ins=0.902, word_ins_ml=4.327, word_reposition=0.79, kpe=0.945, ppl=124.79, wps=6906.2, ups=0.35, wpb=19994.7, bsz=256, num_updates=9500, lr=0.000362738, gnorm=1.615, clip=0, loss_scale=512, train_wall=251, wall=0
2022-07-18 20:50:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-18 20:54:16 | INFO | train_inner | epoch 009:    637 / 1122 loss=6.955, nll_loss=2.698, mask_ins=0.895, word_ins_ml=4.327, word_reposition=0.789, kpe=0.944, ppl=124.06, wps=6878.5, ups=0.34, wpb=20052.5, bsz=256, num_updates=9600, lr=0.000360844, gnorm=1.697, clip=0, loss_scale=297, train_wall=253, wall=0
2022-07-18 20:59:05 | INFO | train_inner | epoch 009:    737 / 1122 loss=6.867, nll_loss=2.619, mask_ins=0.89, word_ins_ml=4.256, word_reposition=0.776, kpe=0.945, ppl=116.72, wps=6926.9, ups=0.35, wpb=20012.8, bsz=256, num_updates=9700, lr=0.000358979, gnorm=1.814, clip=0, loss_scale=256, train_wall=251, wall=0
2022-07-18 21:03:54 | INFO | train_inner | epoch 009:    837 / 1122 loss=6.943, nll_loss=2.688, mask_ins=0.894, word_ins_ml=4.317, word_reposition=0.784, kpe=0.948, ppl=123.03, wps=6918, ups=0.35, wpb=19952.8, bsz=256, num_updates=9800, lr=0.000357143, gnorm=1.747, clip=0, loss_scale=256, train_wall=250, wall=0
2022-07-18 21:08:42 | INFO | train_inner | epoch 009:    937 / 1122 loss=6.93, nll_loss=2.683, mask_ins=0.891, word_ins_ml=4.313, word_reposition=0.779, kpe=0.948, ppl=121.95, wps=6970.6, ups=0.35, wpb=20084.4, bsz=256, num_updates=9900, lr=0.000355335, gnorm=1.863, clip=0, loss_scale=256, train_wall=250, wall=0
2022-07-18 21:13:37 | INFO | train_inner | epoch 009:   1037 / 1122 loss=6.948, nll_loss=2.691, mask_ins=0.899, word_ins_ml=4.319, word_reposition=0.784, kpe=0.946, ppl=123.44, wps=6765, ups=0.34, wpb=20009.8, bsz=256, num_updates=10000, lr=0.000353553, gnorm=1.771, clip=0, loss_scale=256, train_wall=252, wall=0
2022-07-18 21:17:42 | INFO | train | epoch 009 | loss 6.947 | nll_loss 2.689 | mask_ins 0.897 | word_ins_ml 4.319 | word_reposition 0.785 | kpe 0.946 | ppl 123.37 | wps 6715.7 | ups 0.34 | wpb 20007.7 | bsz 255.8 | num_updates 10085 | lr 0.00035206 | gnorm 1.731 | clip 0 | loss_scale 747 | train_wall 2814 | wall 0
2022-07-18 21:19:02 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 11.298 | nll_loss 5.779 | mask_ins 1.472 | word_ins_ml 7.188 | word_reposition 1.252 | kpe 1.386 | ppl 2517.56 | wps 12134.6 | wpb 2325.1 | bsz 32 | num_updates 10085 | best_loss 11.298
2022-07-18 21:19:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_best.pt (epoch 9 @ 10085 updates, score 11.298) (writing took 6.389747301116586 seconds)
2022-07-18 21:19:52 | INFO | train_inner | epoch 010:     15 / 1122 loss=6.88, nll_loss=2.633, mask_ins=0.888, word_ins_ml=4.267, word_reposition=0.778, kpe=0.946, ppl=117.78, wps=5342.8, ups=0.27, wpb=19999.8, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=1.712, clip=0, loss_scale=443, train_wall=250, wall=0
2022-07-18 21:25:25 | INFO | train_inner | epoch 010:    115 / 1122 loss=6.844, nll_loss=2.611, mask_ins=0.885, word_ins_ml=4.248, word_reposition=0.776, kpe=0.935, ppl=114.87, wps=6015, ups=0.3, wpb=20060.7, bsz=256, num_updates=10200, lr=0.00035007, gnorm=1.559, clip=0, loss_scale=512, train_wall=295, wall=0
2022-07-18 21:30:15 | INFO | train_inner | epoch 010:    215 / 1122 loss=6.869, nll_loss=2.639, mask_ins=0.882, word_ins_ml=4.272, word_reposition=0.778, kpe=0.937, ppl=116.93, wps=6894, ups=0.35, wpb=19975.7, bsz=256, num_updates=10300, lr=0.000348367, gnorm=1.655, clip=0, loss_scale=512, train_wall=252, wall=0
2022-07-18 21:35:04 | INFO | train_inner | epoch 010:    315 / 1122 loss=6.89, nll_loss=2.654, mask_ins=0.884, word_ins_ml=4.286, word_reposition=0.778, kpe=0.941, ppl=118.58, wps=6934.7, ups=0.35, wpb=20035.5, bsz=256, num_updates=10400, lr=0.000346688, gnorm=1.591, clip=0, loss_scale=512, train_wall=251, wall=0
2022-07-18 21:39:53 | INFO | train_inner | epoch 010:    415 / 1122 loss=6.847, nll_loss=2.619, mask_ins=0.88, word_ins_ml=4.254, word_reposition=0.773, kpe=0.94, ppl=115.09, wps=6945.5, ups=0.35, wpb=20052.3, bsz=256, num_updates=10500, lr=0.000345033, gnorm=1.703, clip=0, loss_scale=512, train_wall=251, wall=0
2022-07-18 21:44:41 | INFO | train_inner | epoch 010:    515 / 1122 loss=6.865, nll_loss=2.63, mask_ins=0.889, word_ins_ml=4.264, word_reposition=0.776, kpe=0.937, ppl=116.6, wps=6925.9, ups=0.35, wpb=19994.1, bsz=256, num_updates=10600, lr=0.000343401, gnorm=1.516, clip=0, loss_scale=824, train_wall=250, wall=0
2022-07-18 21:49:31 | INFO | train_inner | epoch 010:    615 / 1122 loss=6.889, nll_loss=2.642, mask_ins=0.892, word_ins_ml=4.274, word_reposition=0.79, kpe=0.932, ppl=118.49, wps=6961.2, ups=0.35, wpb=20151.8, bsz=256, num_updates=10700, lr=0.000341793, gnorm=1.549, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-18 21:54:20 | INFO | train_inner | epoch 010:    715 / 1122 loss=6.838, nll_loss=2.612, mask_ins=0.885, word_ins_ml=4.247, word_reposition=0.77, kpe=0.935, ppl=114.37, wps=6888.6, ups=0.35, wpb=19928.9, bsz=256, num_updates=10800, lr=0.000340207, gnorm=1.565, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-18 21:59:09 | INFO | train_inner | epoch 010:    815 / 1122 loss=6.839, nll_loss=2.624, mask_ins=0.875, word_ins_ml=4.257, word_reposition=0.776, kpe=0.931, ppl=114.5, wps=6927.3, ups=0.35, wpb=20038.5, bsz=256, num_updates=10900, lr=0.000338643, gnorm=1.568, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-18 22:03:58 | INFO | train_inner | epoch 010:    915 / 1122 loss=6.809, nll_loss=2.604, mask_ins=0.873, word_ins_ml=4.239, word_reposition=0.767, kpe=0.929, ppl=112.13, wps=6931.1, ups=0.35, wpb=20026.5, bsz=256, num_updates=11000, lr=0.0003371, gnorm=1.557, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-18 22:08:48 | INFO | train_inner | epoch 010:   1015 / 1122 loss=6.835, nll_loss=2.622, mask_ins=0.875, word_ins_ml=4.255, word_reposition=0.772, kpe=0.932, ppl=114.16, wps=6897.1, ups=0.35, wpb=19955.5, bsz=256, num_updates=11100, lr=0.000335578, gnorm=1.558, clip=0, loss_scale=1526, train_wall=252, wall=0
2022-07-18 22:13:36 | INFO | train_inner | epoch 010:   1115 / 1122 loss=6.752, nll_loss=2.558, mask_ins=0.865, word_ins_ml=4.199, word_reposition=0.76, kpe=0.929, ppl=107.8, wps=6927.2, ups=0.35, wpb=19976.5, bsz=256, num_updates=11200, lr=0.000334077, gnorm=1.537, clip=0, loss_scale=2048, train_wall=250, wall=0
2022-07-18 22:13:55 | INFO | train | epoch 010 | loss 6.844 | nll_loss 2.62 | mask_ins 0.881 | word_ins_ml 4.255 | word_reposition 0.774 | kpe 0.935 | ppl 114.89 | wps 6654.3 | ups 0.33 | wpb 20006.5 | bsz 255.8 | num_updates 11207 | lr 0.000333972 | gnorm 1.582 | clip 0 | loss_scale 959 | train_wall 2859 | wall 0
2022-07-18 22:15:15 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 11.157 | nll_loss 5.718 | mask_ins 1.446 | word_ins_ml 7.138 | word_reposition 1.209 | kpe 1.364 | ppl 2283.08 | wps 12219.9 | wpb 2325.1 | bsz 32 | num_updates 11207 | best_loss 11.157
2022-07-18 22:15:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_best.pt (epoch 10 @ 11207 updates, score 11.157) (writing took 6.1937484461814165 seconds)
2022-07-18 22:19:49 | INFO | train_inner | epoch 011:     93 / 1122 loss=6.79, nll_loss=2.592, mask_ins=0.873, word_ins_ml=4.229, word_reposition=0.767, kpe=0.921, ppl=110.65, wps=5313.8, ups=0.27, wpb=19829.8, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=1.574, clip=0, loss_scale=2048, train_wall=249, wall=0
2022-07-18 22:24:38 | INFO | train_inner | epoch 011:    193 / 1122 loss=6.779, nll_loss=2.584, mask_ins=0.868, word_ins_ml=4.221, word_reposition=0.768, kpe=0.921, ppl=109.79, wps=6907.1, ups=0.35, wpb=19968.2, bsz=256, num_updates=11400, lr=0.000331133, gnorm=1.597, clip=0, loss_scale=2048, train_wall=251, wall=0
2022-07-18 22:30:16 | INFO | train_inner | epoch 011:    293 / 1122 loss=6.797, nll_loss=2.595, mask_ins=0.871, word_ins_ml=4.231, word_reposition=0.771, kpe=0.924, ppl=111.22, wps=5929.3, ups=0.3, wpb=20026.4, bsz=256, num_updates=11500, lr=0.00032969, gnorm=1.676, clip=0, loss_scale=2048, train_wall=299, wall=0
2022-07-18 22:35:10 | INFO | train_inner | epoch 011:    393 / 1122 loss=6.73, nll_loss=2.542, mask_ins=0.863, word_ins_ml=4.184, word_reposition=0.763, kpe=0.92, ppl=106.15, wps=6818.5, ups=0.34, wpb=20045.6, bsz=256, num_updates=11600, lr=0.000328266, gnorm=1.562, clip=0, loss_scale=2806, train_wall=252, wall=0
2022-07-18 22:39:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-18 22:40:06 | INFO | train_inner | epoch 011:    494 / 1122 loss=6.746, nll_loss=2.559, mask_ins=0.855, word_ins_ml=4.199, word_reposition=0.763, kpe=0.929, ppl=107.3, wps=6794.6, ups=0.34, wpb=20123.6, bsz=256, num_updates=11700, lr=0.00032686, gnorm=1.632, clip=0, loss_scale=3690, train_wall=254, wall=0
2022-07-18 22:44:56 | INFO | train_inner | epoch 011:    594 / 1122 loss=6.728, nll_loss=2.55, mask_ins=0.857, word_ins_ml=4.19, word_reposition=0.756, kpe=0.925, ppl=106.04, wps=6917.7, ups=0.35, wpb=20004.2, bsz=256, num_updates=11800, lr=0.000325472, gnorm=1.61, clip=0, loss_scale=2048, train_wall=251, wall=0
2022-07-18 22:47:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-18 22:49:48 | INFO | train_inner | epoch 011:    695 / 1122 loss=6.761, nll_loss=2.564, mask_ins=0.866, word_ins_ml=4.203, word_reposition=0.765, kpe=0.927, ppl=108.47, wps=6839, ups=0.34, wpb=20014.7, bsz=256, num_updates=11900, lr=0.000324102, gnorm=1.531, clip=0, loss_scale=1551, train_wall=254, wall=0
2022-07-18 22:54:36 | INFO | train_inner | epoch 011:    795 / 1122 loss=6.748, nll_loss=2.558, mask_ins=0.861, word_ins_ml=4.197, word_reposition=0.766, kpe=0.923, ppl=107.47, wps=6915.2, ups=0.35, wpb=19890.4, bsz=256, num_updates=12000, lr=0.000322749, gnorm=1.545, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-07-18 22:59:24 | INFO | train_inner | epoch 011:    895 / 1122 loss=6.752, nll_loss=2.572, mask_ins=0.856, word_ins_ml=4.21, word_reposition=0.759, kpe=0.927, ppl=107.77, wps=6974.8, ups=0.35, wpb=20105.6, bsz=256, num_updates=12100, lr=0.000321412, gnorm=1.515, clip=0, loss_scale=1024, train_wall=250, wall=0
2022-07-18 23:04:13 | INFO | train_inner | epoch 011:    995 / 1122 loss=6.748, nll_loss=2.562, mask_ins=0.863, word_ins_ml=4.2, word_reposition=0.76, kpe=0.926, ppl=107.5, wps=6915.9, ups=0.35, wpb=20004.5, bsz=256, num_updates=12200, lr=0.000320092, gnorm=1.522, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-18 23:09:02 | INFO | train_inner | epoch 011:   1095 / 1122 loss=6.745, nll_loss=2.561, mask_ins=0.86, word_ins_ml=4.199, word_reposition=0.76, kpe=0.926, ppl=107.29, wps=6947.2, ups=0.35, wpb=20057.5, bsz=256, num_updates=12300, lr=0.000318788, gnorm=1.521, clip=0, loss_scale=1024, train_wall=250, wall=0
2022-07-18 23:10:19 | INFO | train | epoch 011 | loss 6.757 | nll_loss 2.567 | mask_ins 0.863 | word_ins_ml 4.205 | word_reposition 0.764 | kpe 0.925 | ppl 108.13 | wps 6623.4 | ups 0.33 | wpb 20008.4 | bsz 255.8 | num_updates 12327 | lr 0.000318439 | gnorm 1.57 | clip 0 | loss_scale 1829 | train_wall 2861 | wall 0
2022-07-18 23:11:39 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 11.217 | nll_loss 5.779 | mask_ins 1.437 | word_ins_ml 7.191 | word_reposition 1.227 | kpe 1.362 | ppl 2380.27 | wps 12147.9 | wpb 2325.1 | bsz 32 | num_updates 12327 | best_loss 11.157
2022-07-18 23:11:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_last.pt (epoch 11 @ 12327 updates, score 11.217) (writing took 3.0003648614510894 seconds)
2022-07-18 23:13:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-18 23:15:16 | INFO | train_inner | epoch 012:     74 / 1122 loss=6.727, nll_loss=2.548, mask_ins=0.861, word_ins_ml=4.187, word_reposition=0.76, kpe=0.918, ppl=105.93, wps=5327.2, ups=0.27, wpb=19909, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=1.63, clip=0, loss_scale=1115, train_wall=253, wall=0
2022-07-18 23:20:05 | INFO | train_inner | epoch 012:    174 / 1122 loss=6.66, nll_loss=2.495, mask_ins=0.849, word_ins_ml=4.14, word_reposition=0.756, kpe=0.916, ppl=101.16, wps=6925.1, ups=0.35, wpb=19992.8, bsz=256, num_updates=12500, lr=0.000316228, gnorm=1.581, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-18 23:24:54 | INFO | train_inner | epoch 012:    274 / 1122 loss=6.721, nll_loss=2.561, mask_ins=0.849, word_ins_ml=4.199, word_reposition=0.761, kpe=0.912, ppl=105.49, wps=6906.6, ups=0.35, wpb=19982.3, bsz=256, num_updates=12600, lr=0.00031497, gnorm=1.516, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-18 23:29:43 | INFO | train_inner | epoch 012:    374 / 1122 loss=6.7, nll_loss=2.54, mask_ins=0.853, word_ins_ml=4.18, word_reposition=0.754, kpe=0.913, ppl=103.94, wps=6940.1, ups=0.35, wpb=20036.5, bsz=256, num_updates=12700, lr=0.000313728, gnorm=1.537, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-18 23:34:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-18 23:35:20 | INFO | train_inner | epoch 012:    475 / 1122 loss=6.62, nll_loss=2.468, mask_ins=0.843, word_ins_ml=4.116, word_reposition=0.747, kpe=0.914, ppl=98.36, wps=5919.5, ups=0.3, wpb=19976.1, bsz=256, num_updates=12800, lr=0.0003125, gnorm=1.535, clip=0, loss_scale=912, train_wall=299, wall=0
2022-07-18 23:40:10 | INFO | train_inner | epoch 012:    575 / 1122 loss=6.631, nll_loss=2.475, mask_ins=0.843, word_ins_ml=4.122, word_reposition=0.751, kpe=0.915, ppl=99.11, wps=6952.5, ups=0.35, wpb=20129.7, bsz=256, num_updates=12900, lr=0.000311286, gnorm=1.534, clip=0, loss_scale=512, train_wall=251, wall=0
2022-07-18 23:44:58 | INFO | train_inner | epoch 012:    675 / 1122 loss=6.669, nll_loss=2.511, mask_ins=0.846, word_ins_ml=4.154, word_reposition=0.752, kpe=0.917, ppl=101.76, wps=6893.3, ups=0.35, wpb=19913.9, bsz=256, num_updates=13000, lr=0.000310087, gnorm=1.508, clip=0, loss_scale=512, train_wall=250, wall=0
2022-07-18 23:49:47 | INFO | train_inner | epoch 012:    775 / 1122 loss=6.703, nll_loss=2.528, mask_ins=0.857, word_ins_ml=4.169, word_reposition=0.759, kpe=0.917, ppl=104.18, wps=6955.9, ups=0.35, wpb=20054.3, bsz=256, num_updates=13100, lr=0.000308901, gnorm=1.49, clip=0, loss_scale=512, train_wall=250, wall=0
2022-07-18 23:54:35 | INFO | train_inner | epoch 012:    875 / 1122 loss=6.692, nll_loss=2.537, mask_ins=0.849, word_ins_ml=4.177, word_reposition=0.751, kpe=0.916, ppl=103.41, wps=6951.4, ups=0.35, wpb=20048.9, bsz=256, num_updates=13200, lr=0.000307729, gnorm=1.473, clip=0, loss_scale=512, train_wall=250, wall=0
2022-07-18 23:59:29 | INFO | train_inner | epoch 012:    975 / 1122 loss=6.674, nll_loss=2.521, mask_ins=0.845, word_ins_ml=4.163, word_reposition=0.751, kpe=0.915, ppl=102.12, wps=6799.8, ups=0.34, wpb=19983.4, bsz=256, num_updates=13300, lr=0.00030657, gnorm=1.512, clip=0, loss_scale=563, train_wall=251, wall=0
2022-07-19 00:04:21 | INFO | train_inner | epoch 012:   1075 / 1122 loss=6.661, nll_loss=2.508, mask_ins=0.844, word_ins_ml=4.151, word_reposition=0.749, kpe=0.918, ppl=101.23, wps=6871.9, ups=0.34, wpb=20025.2, bsz=256, num_updates=13400, lr=0.000305424, gnorm=1.572, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-19 00:06:36 | INFO | train | epoch 012 | loss 6.678 | nll_loss 2.518 | mask_ins 0.849 | word_ins_ml 4.16 | word_reposition 0.753 | kpe 0.915 | ppl 102.37 | wps 6635.7 | ups 0.33 | wpb 20006.4 | bsz 255.8 | num_updates 13447 | lr 0.000304889 | gnorm 1.535 | clip 0 | loss_scale 799 | train_wall 2859 | wall 0
2022-07-19 00:07:55 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.336 | nll_loss 5.741 | mask_ins 1.496 | word_ins_ml 7.166 | word_reposition 1.269 | kpe 1.405 | ppl 2584.5 | wps 12218.2 | wpb 2325.1 | bsz 32 | num_updates 13447 | best_loss 11.157
2022-07-19 00:07:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_last.pt (epoch 12 @ 13447 updates, score 11.336) (writing took 3.329858719371259 seconds)
2022-07-19 00:10:32 | INFO | train_inner | epoch 013:     53 / 1122 loss=6.693, nll_loss=2.529, mask_ins=0.856, word_ins_ml=4.168, word_reposition=0.756, kpe=0.913, ppl=103.46, wps=5344.6, ups=0.27, wpb=19844.8, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=1.555, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-19 00:15:20 | INFO | train_inner | epoch 013:    153 / 1122 loss=6.652, nll_loss=2.506, mask_ins=0.844, word_ins_ml=4.149, word_reposition=0.754, kpe=0.906, ppl=100.59, wps=6959, ups=0.35, wpb=20069.2, bsz=256, num_updates=13600, lr=0.00030317, gnorm=1.55, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-19 00:20:09 | INFO | train_inner | epoch 013:    253 / 1122 loss=6.646, nll_loss=2.498, mask_ins=0.849, word_ins_ml=4.141, word_reposition=0.753, kpe=0.903, ppl=100.14, wps=6919.7, ups=0.35, wpb=19951.7, bsz=256, num_updates=13700, lr=0.000302061, gnorm=1.451, clip=0, loss_scale=1024, train_wall=250, wall=0
2022-07-19 00:24:58 | INFO | train_inner | epoch 013:    353 / 1122 loss=6.619, nll_loss=2.482, mask_ins=0.84, word_ins_ml=4.128, word_reposition=0.742, kpe=0.909, ppl=98.27, wps=6932.8, ups=0.35, wpb=20050.3, bsz=256, num_updates=13800, lr=0.000300965, gnorm=1.756, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-19 00:28:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-19 00:29:50 | INFO | train_inner | epoch 013:    454 / 1122 loss=6.606, nll_loss=2.471, mask_ins=0.84, word_ins_ml=4.117, word_reposition=0.746, kpe=0.903, ppl=97.38, wps=6871.1, ups=0.34, wpb=20061, bsz=256, num_updates=13900, lr=0.00029988, gnorm=1.415, clip=0, loss_scale=1673, train_wall=253, wall=0
2022-07-19 00:34:39 | INFO | train_inner | epoch 013:    554 / 1122 loss=6.663, nll_loss=2.524, mask_ins=0.844, word_ins_ml=4.164, word_reposition=0.75, kpe=0.906, ppl=101.35, wps=6930.2, ups=0.35, wpb=20056.1, bsz=256, num_updates=14000, lr=0.000298807, gnorm=1.685, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-19 00:36:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-19 00:40:18 | INFO | train_inner | epoch 013:    655 / 1122 loss=6.633, nll_loss=2.49, mask_ins=0.845, word_ins_ml=4.134, word_reposition=0.745, kpe=0.91, ppl=99.28, wps=5888.4, ups=0.3, wpb=19956.7, bsz=256, num_updates=14100, lr=0.000297746, gnorm=1.704, clip=0, loss_scale=730, train_wall=300, wall=0
2022-07-19 00:45:10 | INFO | train_inner | epoch 013:    755 / 1122 loss=6.632, nll_loss=2.518, mask_ins=0.826, word_ins_ml=4.159, word_reposition=0.742, kpe=0.905, ppl=99.16, wps=6848.1, ups=0.34, wpb=19995.3, bsz=256, num_updates=14200, lr=0.000296695, gnorm=1.446, clip=0, loss_scale=512, train_wall=254, wall=0
2022-07-19 00:50:04 | INFO | train_inner | epoch 013:    855 / 1122 loss=6.625, nll_loss=2.496, mask_ins=0.831, word_ins_ml=4.139, word_reposition=0.75, kpe=0.905, ppl=98.7, wps=6795.2, ups=0.34, wpb=19987, bsz=256, num_updates=14300, lr=0.000295656, gnorm=1.515, clip=0, loss_scale=512, train_wall=257, wall=0
2022-07-19 00:54:56 | INFO | train_inner | epoch 013:    955 / 1122 loss=6.577, nll_loss=2.437, mask_ins=0.837, word_ins_ml=4.086, word_reposition=0.744, kpe=0.91, ppl=95.48, wps=6831.8, ups=0.34, wpb=19964.2, bsz=256, num_updates=14400, lr=0.000294628, gnorm=1.602, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-19 00:59:50 | INFO | train_inner | epoch 013:   1055 / 1122 loss=6.619, nll_loss=2.475, mask_ins=0.839, word_ins_ml=4.12, word_reposition=0.752, kpe=0.908, ppl=98.31, wps=6848.6, ups=0.34, wpb=20123.9, bsz=256, num_updates=14500, lr=0.00029361, gnorm=1.454, clip=0, loss_scale=512, train_wall=256, wall=0
2022-07-19 01:03:06 | INFO | train | epoch 013 | loss 6.633 | nll_loss 2.494 | mask_ins 0.841 | word_ins_ml 4.137 | word_reposition 0.748 | kpe 0.906 | ppl 99.22 | wps 6608.2 | ups 0.33 | wpb 20004.7 | bsz 255.8 | num_updates 14567 | lr 0.000292934 | gnorm 1.551 | clip 0 | loss_scale 849 | train_wall 2882 | wall 0
2022-07-19 01:04:25 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 11.175 | nll_loss 5.716 | mask_ins 1.442 | word_ins_ml 7.139 | word_reposition 1.227 | kpe 1.367 | ppl 2311.77 | wps 12314.4 | wpb 2325.1 | bsz 32 | num_updates 14567 | best_loss 11.157
2022-07-19 01:04:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_last.pt (epoch 13 @ 14567 updates, score 11.175) (writing took 3.5584749914705753 seconds)
2022-07-19 01:06:05 | INFO | train_inner | epoch 014:     33 / 1122 loss=6.627, nll_loss=2.498, mask_ins=0.836, word_ins_ml=4.14, word_reposition=0.749, kpe=0.903, ppl=98.87, wps=5305.4, ups=0.27, wpb=19873.4, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=1.512, clip=0, loss_scale=748, train_wall=255, wall=0
2022-07-19 01:10:59 | INFO | train_inner | epoch 014:    133 / 1122 loss=6.632, nll_loss=2.497, mask_ins=0.849, word_ins_ml=4.14, word_reposition=0.746, kpe=0.897, ppl=99.18, wps=6801.7, ups=0.34, wpb=19990.5, bsz=256, num_updates=14700, lr=0.000291606, gnorm=1.522, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-19 01:15:51 | INFO | train_inner | epoch 014:    233 / 1122 loss=6.574, nll_loss=2.443, mask_ins=0.833, word_ins_ml=4.091, word_reposition=0.748, kpe=0.901, ppl=95.25, wps=6834.7, ups=0.34, wpb=20003.2, bsz=256, num_updates=14800, lr=0.000290619, gnorm=1.464, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-19 01:20:44 | INFO | train_inner | epoch 014:    333 / 1122 loss=6.589, nll_loss=2.47, mask_ins=0.828, word_ins_ml=4.115, word_reposition=0.747, kpe=0.9, ppl=96.3, wps=6855.5, ups=0.34, wpb=20040.8, bsz=256, num_updates=14900, lr=0.000289642, gnorm=1.664, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-19 01:25:40 | INFO | train_inner | epoch 014:    433 / 1122 loss=6.582, nll_loss=2.465, mask_ins=0.834, word_ins_ml=4.111, word_reposition=0.741, kpe=0.896, ppl=95.78, wps=6753.1, ups=0.34, wpb=19996.6, bsz=256, num_updates=15000, lr=0.000288675, gnorm=1.483, clip=0, loss_scale=1024, train_wall=259, wall=0
2022-07-19 01:30:34 | INFO | train_inner | epoch 014:    533 / 1122 loss=6.574, nll_loss=2.453, mask_ins=0.834, word_ins_ml=4.099, word_reposition=0.744, kpe=0.897, ppl=95.27, wps=6796, ups=0.34, wpb=20000.6, bsz=256, num_updates=15100, lr=0.000287718, gnorm=1.503, clip=0, loss_scale=1372, train_wall=257, wall=0
2022-07-19 01:31:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-19 01:35:30 | INFO | train_inner | epoch 014:    634 / 1122 loss=6.538, nll_loss=2.424, mask_ins=0.827, word_ins_ml=4.074, word_reposition=0.742, kpe=0.895, ppl=92.92, wps=6767.9, ups=0.34, wpb=20010.8, bsz=256, num_updates=15200, lr=0.00028677, gnorm=1.547, clip=0, loss_scale=1156, train_wall=258, wall=0
2022-07-19 01:40:21 | INFO | train_inner | epoch 014:    734 / 1122 loss=6.554, nll_loss=2.436, mask_ins=0.827, word_ins_ml=4.085, word_reposition=0.744, kpe=0.899, ppl=93.99, wps=6900.8, ups=0.34, wpb=20092.3, bsz=256, num_updates=15300, lr=0.000285831, gnorm=1.444, clip=0, loss_scale=1024, train_wall=254, wall=0
2022-07-19 01:46:03 | INFO | train_inner | epoch 014:    834 / 1122 loss=6.585, nll_loss=2.468, mask_ins=0.836, word_ins_ml=4.112, word_reposition=0.736, kpe=0.901, ppl=96, wps=5836.9, ups=0.29, wpb=19931.1, bsz=256, num_updates=15400, lr=0.000284901, gnorm=1.457, clip=0, loss_scale=1024, train_wall=304, wall=0
2022-07-19 01:50:57 | INFO | train_inner | epoch 014:    934 / 1122 loss=6.574, nll_loss=2.452, mask_ins=0.834, word_ins_ml=4.098, word_reposition=0.745, kpe=0.897, ppl=95.3, wps=6801.1, ups=0.34, wpb=19990, bsz=256, num_updates=15500, lr=0.000283981, gnorm=1.446, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-19 01:55:50 | INFO | train_inner | epoch 014:   1034 / 1122 loss=6.526, nll_loss=2.417, mask_ins=0.825, word_ins_ml=4.067, word_reposition=0.74, kpe=0.894, ppl=92.18, wps=6819.5, ups=0.34, wpb=20011.6, bsz=256, num_updates=15600, lr=0.000283069, gnorm=1.445, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-19 02:00:08 | INFO | train | epoch 014 | loss 6.571 | nll_loss 2.45 | mask_ins 0.832 | word_ins_ml 4.097 | word_reposition 0.743 | kpe 0.898 | ppl 95.05 | wps 6554.4 | ups 0.33 | wpb 20007 | bsz 255.8 | num_updates 15688 | lr 0.000282274 | gnorm 1.501 | clip 0 | loss_scale 1125 | train_wall 2922 | wall 0
2022-07-19 02:01:27 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.055 | nll_loss 5.534 | mask_ins 1.447 | word_ins_ml 6.969 | word_reposition 1.234 | kpe 1.404 | ppl 2127 | wps 12280.3 | wpb 2325.1 | bsz 32 | num_updates 15688 | best_loss 11.055
2022-07-19 02:01:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_best.pt (epoch 14 @ 15688 updates, score 11.055) (writing took 6.275077025406063 seconds)
2022-07-19 02:02:08 | INFO | train_inner | epoch 015:     12 / 1122 loss=6.57, nll_loss=2.448, mask_ins=0.834, word_ins_ml=4.094, word_reposition=0.747, kpe=0.895, ppl=95.01, wps=5284.1, ups=0.26, wpb=19967.9, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=1.522, clip=0, loss_scale=1802, train_wall=256, wall=0
2022-07-19 02:07:04 | INFO | train_inner | epoch 015:    112 / 1122 loss=6.537, nll_loss=2.433, mask_ins=0.83, word_ins_ml=4.081, word_reposition=0.741, kpe=0.886, ppl=92.89, wps=6804.7, ups=0.34, wpb=20126.2, bsz=256, num_updates=15800, lr=0.000281272, gnorm=1.462, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-19 02:11:56 | INFO | train_inner | epoch 015:    212 / 1122 loss=6.504, nll_loss=2.403, mask_ins=0.826, word_ins_ml=4.055, word_reposition=0.736, kpe=0.887, ppl=90.76, wps=6841.9, ups=0.34, wpb=20036, bsz=256, num_updates=15900, lr=0.000280386, gnorm=1.461, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-19 02:16:48 | INFO | train_inner | epoch 015:    312 / 1122 loss=6.519, nll_loss=2.406, mask_ins=0.829, word_ins_ml=4.058, word_reposition=0.741, kpe=0.891, ppl=91.7, wps=6877.4, ups=0.34, wpb=20050.3, bsz=256, num_updates=16000, lr=0.000279508, gnorm=1.496, clip=0, loss_scale=2048, train_wall=254, wall=0
2022-07-19 02:21:43 | INFO | train_inner | epoch 015:    412 / 1122 loss=6.566, nll_loss=2.453, mask_ins=0.828, word_ins_ml=4.098, word_reposition=0.751, kpe=0.889, ppl=94.76, wps=6810.8, ups=0.34, wpb=20084.3, bsz=256, num_updates=16100, lr=0.000278639, gnorm=1.445, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-19 02:26:37 | INFO | train_inner | epoch 015:    512 / 1122 loss=6.536, nll_loss=2.435, mask_ins=0.825, word_ins_ml=4.083, word_reposition=0.739, kpe=0.889, ppl=92.8, wps=6790.3, ups=0.34, wpb=19975.6, bsz=256, num_updates=16200, lr=0.000277778, gnorm=1.422, clip=0, loss_scale=3359, train_wall=257, wall=0
2022-07-19 02:31:32 | INFO | train_inner | epoch 015:    612 / 1122 loss=6.547, nll_loss=2.447, mask_ins=0.83, word_ins_ml=4.094, word_reposition=0.732, kpe=0.892, ppl=93.51, wps=6777.3, ups=0.34, wpb=20001.8, bsz=256, num_updates=16300, lr=0.000276924, gnorm=1.472, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-19 02:36:33 | INFO | train_inner | epoch 015:    712 / 1122 loss=6.575, nll_loss=2.464, mask_ins=0.826, word_ins_ml=4.109, word_reposition=0.747, kpe=0.893, ppl=95.34, wps=6625.9, ups=0.33, wpb=19903.5, bsz=256, num_updates=16400, lr=0.000276079, gnorm=1.478, clip=0, loss_scale=4096, train_wall=262, wall=0
2022-07-19 02:41:33 | INFO | train_inner | epoch 015:    812 / 1122 loss=6.487, nll_loss=2.393, mask_ins=0.815, word_ins_ml=4.046, word_reposition=0.736, kpe=0.891, ppl=89.7, wps=6676.3, ups=0.33, wpb=20073.3, bsz=256, num_updates=16500, lr=0.000275241, gnorm=1.456, clip=0, loss_scale=4096, train_wall=263, wall=0
2022-07-19 02:44:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-19 02:46:33 | INFO | train_inner | epoch 015:    913 / 1122 loss=6.491, nll_loss=2.396, mask_ins=0.821, word_ins_ml=4.048, word_reposition=0.732, kpe=0.89, ppl=89.93, wps=6651, ups=0.33, wpb=19945.1, bsz=256, num_updates=16600, lr=0.000274411, gnorm=1.513, clip=0, loss_scale=3143, train_wall=262, wall=0
2022-07-19 02:54:18 | INFO | train_inner | epoch 015:   1013 / 1122 loss=6.5, nll_loss=2.398, mask_ins=0.826, word_ins_ml=4.05, word_reposition=0.736, kpe=0.888, ppl=90.49, wps=4297.2, ups=0.21, wpb=19989.4, bsz=256, num_updates=16700, lr=0.000273588, gnorm=1.412, clip=0, loss_scale=2048, train_wall=424, wall=0
2022-07-19 03:01:05 | INFO | train_inner | epoch 015:   1113 / 1122 loss=6.492, nll_loss=2.4, mask_ins=0.806, word_ins_ml=4.051, word_reposition=0.742, kpe=0.892, ppl=90, wps=4941.5, ups=0.25, wpb=20106.3, bsz=256, num_updates=16800, lr=0.000272772, gnorm=1.489, clip=0, loss_scale=2048, train_wall=366, wall=0
2022-07-19 03:01:31 | INFO | train | epoch 015 | loss 6.524 | nll_loss 2.422 | mask_ins 0.824 | word_ins_ml 4.071 | word_reposition 0.739 | kpe 0.89 | ppl 92.03 | wps 6089.5 | ups 0.3 | wpb 20005.3 | bsz 255.8 | num_updates 16809 | lr 0.000272699 | gnorm 1.469 | clip 0 | loss_scale 2811 | train_wall 3169 | wall 0
2022-07-19 03:02:51 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 11.108 | nll_loss 5.586 | mask_ins 1.456 | word_ins_ml 7.011 | word_reposition 1.229 | kpe 1.412 | ppl 2207.57 | wps 12032 | wpb 2325.1 | bsz 32 | num_updates 16809 | best_loss 11.055
2022-07-19 03:02:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_last.pt (epoch 15 @ 16809 updates, score 11.108) (writing took 3.421486753039062 seconds)
2022-07-19 03:03:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-19 03:07:25 | INFO | train_inner | epoch 016:     92 / 1122 loss=6.505, nll_loss=2.43, mask_ins=0.817, word_ins_ml=4.079, word_reposition=0.728, kpe=0.881, ppl=90.83, wps=5206.2, ups=0.26, wpb=19755.2, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=1.747, clip=0, loss_scale=1257, train_wall=254, wall=0
2022-07-19 03:12:19 | INFO | train_inner | epoch 016:    192 / 1122 loss=6.505, nll_loss=2.417, mask_ins=0.824, word_ins_ml=4.066, word_reposition=0.735, kpe=0.88, ppl=90.82, wps=6828.8, ups=0.34, wpb=20060.4, bsz=256, num_updates=17000, lr=0.000271163, gnorm=1.718, clip=0, loss_scale=1024, train_wall=252, wall=0
2022-07-19 03:14:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-19 03:17:14 | INFO | train_inner | epoch 016:    293 / 1122 loss=6.504, nll_loss=2.406, mask_ins=0.822, word_ins_ml=4.057, word_reposition=0.737, kpe=0.888, ppl=90.76, wps=6800.7, ups=0.34, wpb=20086.5, bsz=256, num_updates=17100, lr=0.000270369, gnorm=2.868, clip=0, loss_scale=730, train_wall=254, wall=0
2022-07-19 03:22:07 | INFO | train_inner | epoch 016:    393 / 1122 loss=6.529, nll_loss=2.438, mask_ins=0.819, word_ins_ml=4.085, word_reposition=0.734, kpe=0.89, ppl=92.37, wps=6784, ups=0.34, wpb=19914.2, bsz=256, num_updates=17200, lr=0.000269582, gnorm=1.697, clip=0, loss_scale=512, train_wall=252, wall=0
2022-07-19 03:27:02 | INFO | train_inner | epoch 016:    493 / 1122 loss=6.475, nll_loss=2.382, mask_ins=0.817, word_ins_ml=4.035, word_reposition=0.739, kpe=0.884, ppl=88.98, wps=6832.7, ups=0.34, wpb=20096.1, bsz=256, num_updates=17300, lr=0.000268802, gnorm=1.476, clip=0, loss_scale=512, train_wall=252, wall=0
2022-07-19 03:31:54 | INFO | train_inner | epoch 016:    593 / 1122 loss=6.467, nll_loss=2.367, mask_ins=0.824, word_ins_ml=4.021, word_reposition=0.735, kpe=0.887, ppl=88.48, wps=6852.4, ups=0.34, wpb=20072.9, bsz=256, num_updates=17400, lr=0.000268028, gnorm=1.473, clip=0, loss_scale=512, train_wall=253, wall=0
2022-07-19 03:36:47 | INFO | train_inner | epoch 016:    693 / 1122 loss=6.441, nll_loss=2.363, mask_ins=0.808, word_ins_ml=4.018, word_reposition=0.732, kpe=0.883, ppl=86.86, wps=6848.5, ups=0.34, wpb=20047.4, bsz=256, num_updates=17500, lr=0.000267261, gnorm=1.41, clip=0, loss_scale=512, train_wall=253, wall=0
2022-07-19 03:41:41 | INFO | train_inner | epoch 016:    793 / 1122 loss=6.46, nll_loss=2.37, mask_ins=0.815, word_ins_ml=4.024, word_reposition=0.74, kpe=0.88, ppl=88.01, wps=6820, ups=0.34, wpb=20009.9, bsz=256, num_updates=17600, lr=0.000266501, gnorm=1.521, clip=0, loss_scale=748, train_wall=254, wall=0
2022-07-19 03:46:34 | INFO | train_inner | epoch 016:    893 / 1122 loss=6.465, nll_loss=2.365, mask_ins=0.822, word_ins_ml=4.019, word_reposition=0.735, kpe=0.889, ppl=88.31, wps=6837.7, ups=0.34, wpb=20078.6, bsz=256, num_updates=17700, lr=0.000265747, gnorm=1.486, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-19 03:51:28 | INFO | train_inner | epoch 016:    993 / 1122 loss=6.507, nll_loss=2.421, mask_ins=0.818, word_ins_ml=4.069, word_reposition=0.736, kpe=0.884, ppl=90.97, wps=6800.3, ups=0.34, wpb=19945.1, bsz=256, num_updates=17800, lr=0.000264999, gnorm=1.555, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-19 03:56:21 | INFO | train_inner | epoch 016:   1093 / 1122 loss=6.495, nll_loss=2.409, mask_ins=0.82, word_ins_ml=4.059, word_reposition=0.734, kpe=0.883, ppl=90.19, wps=6803.4, ups=0.34, wpb=19940.7, bsz=256, num_updates=17900, lr=0.000264258, gnorm=1.441, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-19 03:57:56 | INFO | train | epoch 016 | loss 6.487 | nll_loss 2.397 | mask_ins 0.819 | word_ins_ml 4.048 | word_reposition 0.735 | kpe 0.885 | ppl 89.72 | wps 6618 | ups 0.33 | wpb 20004.3 | bsz 255.8 | num_updates 17929 | lr 0.000264044 | gnorm 1.669 | clip 0 | loss_scale 803 | train_wall 2854 | wall 0
2022-07-19 03:59:27 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 11.16 | nll_loss 5.63 | mask_ins 1.44 | word_ins_ml 7.057 | word_reposition 1.237 | kpe 1.426 | ppl 2288.2 | wps 10735.1 | wpb 2325.1 | bsz 32 | num_updates 17929 | best_loss 11.055
2022-07-19 03:59:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_last.pt (epoch 16 @ 17929 updates, score 11.16) (writing took 3.336250307969749 seconds)
2022-07-19 04:03:21 | INFO | train_inner | epoch 017:     71 / 1122 loss=6.478, nll_loss=2.386, mask_ins=0.823, word_ins_ml=4.038, word_reposition=0.738, kpe=0.879, ppl=89.12, wps=4745.9, ups=0.24, wpb=19944.9, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=1.533, clip=0, loss_scale=1024, train_wall=289, wall=0
2022-07-19 04:08:15 | INFO | train_inner | epoch 017:    171 / 1122 loss=6.458, nll_loss=2.378, mask_ins=0.821, word_ins_ml=4.031, word_reposition=0.735, kpe=0.871, ppl=87.93, wps=6796.3, ups=0.34, wpb=20016.3, bsz=256, num_updates=18100, lr=0.000262794, gnorm=1.489, clip=0, loss_scale=1372, train_wall=257, wall=0
2022-07-19 04:13:09 | INFO | train_inner | epoch 017:    271 / 1122 loss=6.431, nll_loss=2.363, mask_ins=0.814, word_ins_ml=4.018, word_reposition=0.727, kpe=0.871, ppl=86.26, wps=6806.8, ups=0.34, wpb=19953, bsz=256, num_updates=18200, lr=0.000262071, gnorm=1.455, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-19 04:18:01 | INFO | train_inner | epoch 017:    371 / 1122 loss=6.437, nll_loss=2.365, mask_ins=0.814, word_ins_ml=4.019, word_reposition=0.729, kpe=0.875, ppl=86.64, wps=6814.2, ups=0.34, wpb=19947.2, bsz=256, num_updates=18300, lr=0.000261354, gnorm=1.47, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-19 04:22:54 | INFO | train_inner | epoch 017:    471 / 1122 loss=6.459, nll_loss=2.388, mask_ins=0.816, word_ins_ml=4.04, word_reposition=0.733, kpe=0.871, ppl=88, wps=6820.3, ups=0.34, wpb=19976.7, bsz=256, num_updates=18400, lr=0.000260643, gnorm=1.469, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-19 04:27:46 | INFO | train_inner | epoch 017:    571 / 1122 loss=6.433, nll_loss=2.362, mask_ins=0.806, word_ins_ml=4.016, word_reposition=0.733, kpe=0.877, ppl=86.39, wps=6863.7, ups=0.34, wpb=19996.7, bsz=256, num_updates=18500, lr=0.000259938, gnorm=1.454, clip=0, loss_scale=2048, train_wall=254, wall=0
2022-07-19 04:32:39 | INFO | train_inner | epoch 017:    671 / 1122 loss=6.434, nll_loss=2.364, mask_ins=0.809, word_ins_ml=4.018, word_reposition=0.733, kpe=0.874, ppl=86.48, wps=6859.5, ups=0.34, wpb=20089.2, bsz=256, num_updates=18600, lr=0.000259238, gnorm=1.455, clip=0, loss_scale=2499, train_wall=255, wall=0
2022-07-19 04:37:32 | INFO | train_inner | epoch 017:    771 / 1122 loss=6.425, nll_loss=2.34, mask_ins=0.815, word_ins_ml=3.996, word_reposition=0.736, kpe=0.878, ppl=85.95, wps=6882.4, ups=0.34, wpb=20195.9, bsz=256, num_updates=18700, lr=0.000258544, gnorm=1.468, clip=0, loss_scale=4096, train_wall=255, wall=0
2022-07-19 04:42:27 | INFO | train_inner | epoch 017:    871 / 1122 loss=6.431, nll_loss=2.359, mask_ins=0.813, word_ins_ml=4.013, word_reposition=0.729, kpe=0.876, ppl=86.29, wps=6787.7, ups=0.34, wpb=20001.5, bsz=256, num_updates=18800, lr=0.000257855, gnorm=1.493, clip=0, loss_scale=4096, train_wall=253, wall=0
2022-07-19 04:47:20 | INFO | train_inner | epoch 017:    971 / 1122 loss=6.445, nll_loss=2.384, mask_ins=0.809, word_ins_ml=4.035, word_reposition=0.727, kpe=0.874, ppl=87.14, wps=6807.9, ups=0.34, wpb=19987.2, bsz=256, num_updates=18900, lr=0.000257172, gnorm=1.453, clip=0, loss_scale=4096, train_wall=254, wall=0
2022-07-19 04:52:14 | INFO | train_inner | epoch 017:   1071 / 1122 loss=6.388, nll_loss=2.338, mask_ins=0.793, word_ins_ml=3.995, word_reposition=0.724, kpe=0.876, ppl=83.75, wps=6802.3, ups=0.34, wpb=19984.8, bsz=256, num_updates=19000, lr=0.000256495, gnorm=1.428, clip=0, loss_scale=4096, train_wall=253, wall=0
2022-07-19 04:54:43 | INFO | train | epoch 017 | loss 6.437 | nll_loss 2.366 | mask_ins 0.812 | word_ins_ml 4.02 | word_reposition 0.73 | kpe 0.875 | ppl 86.61 | wps 6587.2 | ups 0.33 | wpb 20004.5 | bsz 255.8 | num_updates 19051 | lr 0.000256151 | gnorm 1.47 | clip 0 | loss_scale 2786 | train_wall 2882 | wall 0
2022-07-19 04:56:04 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 11.359 | nll_loss 5.692 | mask_ins 1.475 | word_ins_ml 7.121 | word_reposition 1.313 | kpe 1.451 | ppl 2626.24 | wps 12031.3 | wpb 2325.1 | bsz 32 | num_updates 19051 | best_loss 11.055
2022-07-19 04:56:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_last.pt (epoch 17 @ 19051 updates, score 11.359) (writing took 3.576612471602857 seconds)
2022-07-19 04:58:31 | INFO | train_inner | epoch 018:     49 / 1122 loss=6.444, nll_loss=2.389, mask_ins=0.813, word_ins_ml=4.04, word_reposition=0.722, kpe=0.87, ppl=87.09, wps=5236.6, ups=0.27, wpb=19746.3, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=1.537, clip=0, loss_scale=4506, train_wall=252, wall=0
2022-07-19 04:59:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-19 05:03:40 | INFO | train_inner | epoch 018:    150 / 1122 loss=6.365, nll_loss=2.309, mask_ins=0.811, word_ins_ml=3.969, word_reposition=0.724, kpe=0.861, ppl=82.44, wps=6498.2, ups=0.32, wpb=20074.7, bsz=256, num_updates=19200, lr=0.000255155, gnorm=1.397, clip=0, loss_scale=5272, train_wall=267, wall=0
2022-07-19 05:09:10 | INFO | train_inner | epoch 018:    250 / 1122 loss=6.371, nll_loss=2.325, mask_ins=0.796, word_ins_ml=3.983, word_reposition=0.724, kpe=0.868, ppl=82.77, wps=6080.9, ups=0.3, wpb=20078, bsz=256, num_updates=19300, lr=0.000254493, gnorm=1.471, clip=0, loss_scale=4096, train_wall=287, wall=0
2022-07-19 05:14:04 | INFO | train_inner | epoch 018:    350 / 1122 loss=6.409, nll_loss=2.356, mask_ins=0.804, word_ins_ml=4.01, word_reposition=0.731, kpe=0.864, ppl=85, wps=6776.7, ups=0.34, wpb=19913.1, bsz=256, num_updates=19400, lr=0.000253837, gnorm=1.468, clip=0, loss_scale=4096, train_wall=253, wall=0
2022-07-19 05:18:59 | INFO | train_inner | epoch 018:    450 / 1122 loss=6.405, nll_loss=2.36, mask_ins=0.802, word_ins_ml=4.014, word_reposition=0.724, kpe=0.866, ppl=84.76, wps=6806.6, ups=0.34, wpb=20054.5, bsz=256, num_updates=19500, lr=0.000253185, gnorm=1.502, clip=0, loss_scale=4096, train_wall=254, wall=0
2022-07-19 05:23:51 | INFO | train_inner | epoch 018:    550 / 1122 loss=6.418, nll_loss=2.359, mask_ins=0.803, word_ins_ml=4.013, word_reposition=0.728, kpe=0.875, ppl=85.53, wps=6807.1, ups=0.34, wpb=19914.5, bsz=256, num_updates=19600, lr=0.000252538, gnorm=1.478, clip=0, loss_scale=4096, train_wall=254, wall=0
2022-07-19 05:28:45 | INFO | train_inner | epoch 018:    650 / 1122 loss=6.398, nll_loss=2.35, mask_ins=0.802, word_ins_ml=4.005, word_reposition=0.727, kpe=0.864, ppl=84.32, wps=6807.6, ups=0.34, wpb=19984.6, bsz=256, num_updates=19700, lr=0.000251896, gnorm=1.461, clip=0, loss_scale=6554, train_wall=256, wall=0
2022-07-19 05:31:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-19 05:33:40 | INFO | train_inner | epoch 018:    751 / 1122 loss=6.367, nll_loss=2.316, mask_ins=0.799, word_ins_ml=3.974, word_reposition=0.73, kpe=0.864, ppl=82.53, wps=6811.8, ups=0.34, wpb=20125.9, bsz=256, num_updates=19800, lr=0.000251259, gnorm=1.431, clip=0, loss_scale=5921, train_wall=257, wall=0
2022-07-19 05:38:31 | INFO | train_inner | epoch 018:    851 / 1122 loss=6.389, nll_loss=2.334, mask_ins=0.806, word_ins_ml=3.99, word_reposition=0.726, kpe=0.867, ppl=83.78, wps=6923.2, ups=0.34, wpb=20142, bsz=256, num_updates=19900, lr=0.000250627, gnorm=1.505, clip=0, loss_scale=4096, train_wall=253, wall=0
2022-07-19 05:43:23 | INFO | train_inner | epoch 018:    951 / 1122 loss=6.398, nll_loss=2.34, mask_ins=0.803, word_ins_ml=3.996, word_reposition=0.732, kpe=0.867, ppl=84.35, wps=6836.5, ups=0.34, wpb=19952, bsz=256, num_updates=20000, lr=0.00025, gnorm=1.496, clip=0, loss_scale=4096, train_wall=254, wall=0
2022-07-19 05:48:17 | INFO | train_inner | epoch 018:   1051 / 1122 loss=6.388, nll_loss=2.328, mask_ins=0.807, word_ins_ml=3.985, word_reposition=0.725, kpe=0.871, ppl=83.78, wps=6810.4, ups=0.34, wpb=20009.1, bsz=256, num_updates=20100, lr=0.000249377, gnorm=1.481, clip=0, loss_scale=4096, train_wall=256, wall=0
2022-07-19 05:51:45 | INFO | train | epoch 018 | loss 6.396 | nll_loss 2.343 | mask_ins 0.804 | word_ins_ml 3.998 | word_reposition 0.727 | kpe 0.867 | ppl 84.21 | wps 6549 | ups 0.33 | wpb 20005.5 | bsz 255.8 | num_updates 20171 | lr 0.000248938 | gnorm 1.477 | clip 0 | loss_scale 4622 | train_wall 2897 | wall 0
2022-07-19 05:53:03 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.989 | nll_loss 5.49 | mask_ins 1.438 | word_ins_ml 6.929 | word_reposition 1.209 | kpe 1.412 | ppl 2032.01 | wps 12348.3 | wpb 2325.1 | bsz 32 | num_updates 20171 | best_loss 10.989
2022-07-19 05:53:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_best.pt (epoch 18 @ 20171 updates, score 10.989) (writing took 6.084024621173739 seconds)
2022-07-19 05:54:33 | INFO | train_inner | epoch 019:     29 / 1122 loss=6.407, nll_loss=2.352, mask_ins=0.808, word_ins_ml=4.006, word_reposition=0.726, kpe=0.868, ppl=84.86, wps=5266.1, ups=0.27, wpb=19790.5, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=1.547, clip=0, loss_scale=4096, train_wall=254, wall=0
2022-07-19 05:59:24 | INFO | train_inner | epoch 019:    129 / 1122 loss=6.361, nll_loss=2.321, mask_ins=0.803, word_ins_ml=3.978, word_reposition=0.73, kpe=0.849, ppl=82.2, wps=6806, ups=0.34, wpb=19840.3, bsz=256, num_updates=20300, lr=0.000248146, gnorm=1.458, clip=0, loss_scale=5898, train_wall=254, wall=0
2022-07-19 06:03:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-19 06:04:19 | INFO | train_inner | epoch 019:    230 / 1122 loss=6.365, nll_loss=2.318, mask_ins=0.804, word_ins_ml=3.977, word_reposition=0.727, kpe=0.858, ppl=82.45, wps=6778.2, ups=0.34, wpb=19978.3, bsz=256, num_updates=20400, lr=0.000247537, gnorm=1.498, clip=0, loss_scale=7421, train_wall=257, wall=0
2022-07-19 06:09:23 | INFO | train_inner | epoch 019:    330 / 1122 loss=6.406, nll_loss=2.359, mask_ins=0.807, word_ins_ml=4.012, word_reposition=0.73, kpe=0.856, ppl=84.78, wps=6581, ups=0.33, wpb=19998.5, bsz=256, num_updates=20500, lr=0.000246932, gnorm=1.553, clip=0, loss_scale=4096, train_wall=267, wall=0
2022-07-19 06:14:54 | INFO | train_inner | epoch 019:    430 / 1122 loss=6.406, nll_loss=2.362, mask_ins=0.804, word_ins_ml=4.015, word_reposition=0.724, kpe=0.863, ppl=84.78, wps=6027.3, ups=0.3, wpb=19955.2, bsz=256, num_updates=20600, lr=0.000246332, gnorm=1.513, clip=0, loss_scale=4096, train_wall=294, wall=0
2022-07-19 06:19:47 | INFO | train_inner | epoch 019:    530 / 1122 loss=6.372, nll_loss=2.34, mask_ins=0.794, word_ins_ml=3.995, word_reposition=0.72, kpe=0.862, ppl=82.85, wps=6868.4, ups=0.34, wpb=20098.2, bsz=256, num_updates=20700, lr=0.000245737, gnorm=1.501, clip=0, loss_scale=4096, train_wall=255, wall=0
2022-07-19 06:24:39 | INFO | train_inner | epoch 019:    630 / 1122 loss=6.372, nll_loss=2.327, mask_ins=0.799, word_ins_ml=3.984, word_reposition=0.727, kpe=0.862, ppl=82.83, wps=6883.8, ups=0.34, wpb=20097.5, bsz=256, num_updates=20800, lr=0.000245145, gnorm=1.525, clip=0, loss_scale=4096, train_wall=255, wall=0
2022-07-19 06:29:32 | INFO | train_inner | epoch 019:    730 / 1122 loss=6.426, nll_loss=2.38, mask_ins=0.803, word_ins_ml=4.031, word_reposition=0.73, kpe=0.861, ppl=85.97, wps=6861.6, ups=0.34, wpb=20161.6, bsz=256, num_updates=20900, lr=0.000244558, gnorm=1.487, clip=0, loss_scale=4383, train_wall=255, wall=0
2022-07-19 06:34:28 | INFO | train_inner | epoch 019:    830 / 1122 loss=6.279, nll_loss=2.25, mask_ins=0.789, word_ins_ml=3.916, word_reposition=0.713, kpe=0.861, ppl=77.63, wps=6759.2, ups=0.34, wpb=19994.2, bsz=256, num_updates=21000, lr=0.000243975, gnorm=1.423, clip=0, loss_scale=8192, train_wall=253, wall=0
2022-07-19 06:37:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-19 06:38:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-19 06:39:27 | INFO | train_inner | epoch 019:    932 / 1122 loss=6.366, nll_loss=2.318, mask_ins=0.805, word_ins_ml=3.976, word_reposition=0.724, kpe=0.861, ppl=82.46, wps=6675.4, ups=0.33, wpb=19957.8, bsz=256, num_updates=21100, lr=0.000243396, gnorm=1.48, clip=0, loss_scale=6345, train_wall=257, wall=0
2022-07-19 06:44:20 | INFO | train_inner | epoch 019:   1032 / 1122 loss=6.36, nll_loss=2.325, mask_ins=0.799, word_ins_ml=3.982, word_reposition=0.721, kpe=0.858, ppl=82.14, wps=6868.4, ups=0.34, wpb=20135.9, bsz=256, num_updates=21200, lr=0.000242821, gnorm=1.49, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-07-19 06:48:44 | INFO | train | epoch 019 | loss 6.371 | nll_loss 2.329 | mask_ins 0.801 | word_ins_ml 3.986 | word_reposition 0.724 | kpe 0.86 | ppl 82.79 | wps 6546.4 | ups 0.33 | wpb 20005.4 | bsz 255.8 | num_updates 21290 | lr 0.000242308 | gnorm 1.493 | clip 0 | loss_scale 4804 | train_wall 2901 | wall 0
2022-07-19 06:50:04 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.079 | nll_loss 5.531 | mask_ins 1.441 | word_ins_ml 6.964 | word_reposition 1.235 | kpe 1.439 | ppl 2162.82 | wps 12170.1 | wpb 2325.1 | bsz 32 | num_updates 21290 | best_loss 10.989
2022-07-19 06:50:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_last.pt (epoch 19 @ 21290 updates, score 11.079) (writing took 3.482604755088687 seconds)
2022-07-19 06:50:36 | INFO | train_inner | epoch 020:     10 / 1122 loss=6.382, nll_loss=2.335, mask_ins=0.805, word_ins_ml=3.991, word_reposition=0.723, kpe=0.863, ppl=83.38, wps=5306.2, ups=0.27, wpb=19943.7, bsz=253.8, num_updates=21300, lr=0.000242251, gnorm=1.513, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-07-19 06:55:25 | INFO | train_inner | epoch 020:    110 / 1122 loss=6.323, nll_loss=2.289, mask_ins=0.797, word_ins_ml=3.95, word_reposition=0.728, kpe=0.848, ppl=80.06, wps=6911.8, ups=0.35, wpb=19989.6, bsz=256, num_updates=21400, lr=0.000241684, gnorm=1.461, clip=0, loss_scale=2048, train_wall=250, wall=0
2022-07-19 07:00:15 | INFO | train_inner | epoch 020:    210 / 1122 loss=6.309, nll_loss=2.297, mask_ins=0.789, word_ins_ml=3.957, word_reposition=0.713, kpe=0.849, ppl=79.26, wps=6932.9, ups=0.35, wpb=20044.4, bsz=256, num_updates=21500, lr=0.000241121, gnorm=1.472, clip=0, loss_scale=2048, train_wall=251, wall=0
2022-07-19 07:05:05 | INFO | train_inner | epoch 020:    310 / 1122 loss=6.28, nll_loss=2.266, mask_ins=0.791, word_ins_ml=3.929, word_reposition=0.715, kpe=0.845, ppl=77.72, wps=6906.7, ups=0.34, wpb=20060.1, bsz=256, num_updates=21600, lr=0.000240563, gnorm=1.431, clip=0, loss_scale=2089, train_wall=251, wall=0
2022-07-19 07:07:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-19 07:09:57 | INFO | train_inner | epoch 020:    411 / 1122 loss=6.32, nll_loss=2.296, mask_ins=0.789, word_ins_ml=3.956, word_reposition=0.722, kpe=0.853, ppl=79.89, wps=6871.6, ups=0.34, wpb=20074.2, bsz=256, num_updates=21700, lr=0.000240008, gnorm=1.493, clip=0, loss_scale=3001, train_wall=253, wall=0
2022-07-19 07:14:57 | INFO | train_inner | epoch 020:    511 / 1122 loss=6.297, nll_loss=2.273, mask_ins=0.789, word_ins_ml=3.936, word_reposition=0.716, kpe=0.856, ppl=78.63, wps=6674.6, ups=0.33, wpb=19985.7, bsz=256, num_updates=21800, lr=0.000239457, gnorm=1.518, clip=0, loss_scale=2048, train_wall=261, wall=0
2022-07-19 07:20:26 | INFO | train_inner | epoch 020:    611 / 1122 loss=6.311, nll_loss=2.284, mask_ins=0.797, word_ins_ml=3.945, word_reposition=0.718, kpe=0.851, ppl=79.42, wps=6103.5, ups=0.3, wpb=20075, bsz=256, num_updates=21900, lr=0.000238909, gnorm=1.461, clip=0, loss_scale=2048, train_wall=289, wall=0
2022-07-19 07:25:15 | INFO | train_inner | epoch 020:    711 / 1122 loss=6.325, nll_loss=2.302, mask_ins=0.798, word_ins_ml=3.961, word_reposition=0.711, kpe=0.856, ppl=80.19, wps=6888.3, ups=0.35, wpb=19911.9, bsz=256, num_updates=22000, lr=0.000238366, gnorm=1.486, clip=0, loss_scale=2048, train_wall=250, wall=0
2022-07-19 07:30:06 | INFO | train_inner | epoch 020:    811 / 1122 loss=6.288, nll_loss=2.262, mask_ins=0.793, word_ins_ml=3.926, word_reposition=0.717, kpe=0.852, ppl=78.15, wps=6859.4, ups=0.34, wpb=19985.8, bsz=256, num_updates=22100, lr=0.000237826, gnorm=1.45, clip=0, loss_scale=2048, train_wall=252, wall=0
2022-07-19 07:34:55 | INFO | train_inner | epoch 020:    911 / 1122 loss=6.292, nll_loss=2.268, mask_ins=0.795, word_ins_ml=3.931, word_reposition=0.715, kpe=0.851, ppl=78.37, wps=6936.5, ups=0.35, wpb=20053.8, bsz=256, num_updates=22200, lr=0.000237289, gnorm=1.432, clip=0, loss_scale=2908, train_wall=250, wall=0
2022-07-19 07:39:49 | INFO | train_inner | epoch 020:   1011 / 1122 loss=6.344, nll_loss=2.315, mask_ins=0.792, word_ins_ml=3.973, word_reposition=0.723, kpe=0.856, ppl=81.22, wps=6815, ups=0.34, wpb=19996.4, bsz=256, num_updates=22300, lr=0.000236757, gnorm=1.5, clip=0, loss_scale=4096, train_wall=255, wall=0
2022-07-19 07:44:42 | INFO | train_inner | epoch 020:   1111 / 1122 loss=6.334, nll_loss=2.31, mask_ins=0.793, word_ins_ml=3.968, word_reposition=0.719, kpe=0.854, ppl=80.7, wps=6838.6, ups=0.34, wpb=20047.3, bsz=256, num_updates=22400, lr=0.000236228, gnorm=1.442, clip=0, loss_scale=4096, train_wall=256, wall=0
2022-07-19 07:45:13 | INFO | train | epoch 020 | loss 6.313 | nll_loss 2.289 | mask_ins 0.794 | word_ins_ml 3.95 | word_reposition 0.718 | kpe 0.852 | ppl 79.53 | wps 6618.4 | ups 0.33 | wpb 20006.2 | bsz 255.8 | num_updates 22411 | lr 0.00023617 | gnorm 1.472 | clip 0 | loss_scale 2599 | train_wall 2871 | wall 0
2022-07-19 07:46:32 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.041 | nll_loss 5.551 | mask_ins 1.411 | word_ins_ml 6.995 | word_reposition 1.224 | kpe 1.411 | ppl 2107.26 | wps 12273.9 | wpb 2325.1 | bsz 32 | num_updates 22411 | best_loss 10.989
2022-07-19 07:46:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_last.pt (epoch 20 @ 22411 updates, score 11.041) (writing took 3.6789309112355113 seconds)
2022-07-19 07:50:56 | INFO | train_inner | epoch 021:     89 / 1122 loss=6.287, nll_loss=2.273, mask_ins=0.788, word_ins_ml=3.936, word_reposition=0.717, kpe=0.847, ppl=78.1, wps=5307.7, ups=0.27, wpb=19877.6, bsz=253.8, num_updates=22500, lr=0.000235702, gnorm=1.516, clip=0, loss_scale=4096, train_wall=255, wall=0
2022-07-19 07:55:47 | INFO | train_inner | epoch 021:    189 / 1122 loss=6.318, nll_loss=2.297, mask_ins=0.801, word_ins_ml=3.956, word_reposition=0.719, kpe=0.842, ppl=79.79, wps=6850.3, ups=0.34, wpb=19893.1, bsz=256, num_updates=22600, lr=0.00023518, gnorm=1.477, clip=0, loss_scale=4096, train_wall=252, wall=0
2022-07-19 07:56:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-19 08:00:43 | INFO | train_inner | epoch 021:    290 / 1122 loss=6.289, nll_loss=2.279, mask_ins=0.79, word_ins_ml=3.94, word_reposition=0.717, kpe=0.842, ppl=78.19, wps=6784.8, ups=0.34, wpb=20113.5, bsz=256, num_updates=22700, lr=0.000234662, gnorm=1.521, clip=0, loss_scale=2474, train_wall=257, wall=0
2022-07-19 08:05:36 | INFO | train_inner | epoch 021:    390 / 1122 loss=6.28, nll_loss=2.269, mask_ins=0.793, word_ins_ml=3.931, word_reposition=0.711, kpe=0.845, ppl=77.73, wps=6851.2, ups=0.34, wpb=20051, bsz=256, num_updates=22800, lr=0.000234146, gnorm=1.485, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-19 08:10:26 | INFO | train_inner | epoch 021:    490 / 1122 loss=6.325, nll_loss=2.315, mask_ins=0.787, word_ins_ml=3.972, word_reposition=0.719, kpe=0.846, ppl=80.15, wps=6939.4, ups=0.34, wpb=20158.2, bsz=256, num_updates=22900, lr=0.000233635, gnorm=1.478, clip=0, loss_scale=2048, train_wall=252, wall=0
2022-07-19 08:15:15 | INFO | train_inner | epoch 021:    590 / 1122 loss=6.22, nll_loss=2.219, mask_ins=0.778, word_ins_ml=3.887, word_reposition=0.712, kpe=0.843, ppl=74.55, wps=6933.9, ups=0.35, wpb=19998.6, bsz=256, num_updates=23000, lr=0.000233126, gnorm=1.475, clip=0, loss_scale=2048, train_wall=250, wall=0
2022-07-19 08:20:25 | INFO | train_inner | epoch 021:    690 / 1122 loss=6.267, nll_loss=2.262, mask_ins=0.783, word_ins_ml=3.925, word_reposition=0.712, kpe=0.846, ppl=77.04, wps=6434.7, ups=0.32, wpb=19939.8, bsz=256, num_updates=23100, lr=0.000232621, gnorm=1.523, clip=0, loss_scale=2048, train_wall=272, wall=0
2022-07-19 08:25:40 | INFO | train_inner | epoch 021:    790 / 1122 loss=6.265, nll_loss=2.258, mask_ins=0.781, word_ins_ml=3.921, word_reposition=0.716, kpe=0.847, ppl=76.91, wps=6347.6, ups=0.32, wpb=20044.2, bsz=256, num_updates=23200, lr=0.000232119, gnorm=1.461, clip=0, loss_scale=3441, train_wall=276, wall=0
2022-07-19 08:30:39 | INFO | train_inner | epoch 021:    890 / 1122 loss=6.258, nll_loss=2.235, mask_ins=0.794, word_ins_ml=3.901, word_reposition=0.718, kpe=0.844, ppl=76.54, wps=6656.4, ups=0.33, wpb=19895.8, bsz=256, num_updates=23300, lr=0.000231621, gnorm=1.497, clip=0, loss_scale=4096, train_wall=257, wall=0
2022-07-19 08:35:39 | INFO | train_inner | epoch 021:    990 / 1122 loss=6.288, nll_loss=2.277, mask_ins=0.783, word_ins_ml=3.938, word_reposition=0.721, kpe=0.846, ppl=78.15, wps=6709.6, ups=0.33, wpb=20122.4, bsz=256, num_updates=23400, lr=0.000231125, gnorm=1.492, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-19 08:40:41 | INFO | train_inner | epoch 021:   1090 / 1122 loss=6.303, nll_loss=2.286, mask_ins=0.788, word_ins_ml=3.946, word_reposition=0.72, kpe=0.849, ppl=78.97, wps=6607.1, ups=0.33, wpb=19917.7, bsz=256, num_updates=23500, lr=0.000230633, gnorm=1.489, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-19 08:42:16 | INFO | train | epoch 021 | loss 6.279 | nll_loss 2.268 | mask_ins 0.787 | word_ins_ml 3.93 | word_reposition 0.716 | kpe 0.845 | ppl 77.63 | wps 6551.5 | ups 0.33 | wpb 20006.2 | bsz 255.8 | num_updates 23532 | lr 0.000230476 | gnorm 1.491 | clip 0 | loss_scale 3161 | train_wall 2899 | wall 0
2022-07-19 08:43:39 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.145 | nll_loss 5.57 | mask_ins 1.443 | word_ins_ml 7.01 | word_reposition 1.233 | kpe 1.459 | ppl 2264.36 | wps 11745.3 | wpb 2325.1 | bsz 32 | num_updates 23532 | best_loss 10.989
2022-07-19 08:43:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_target_ACL_cased/checkpoint_last.pt (epoch 21 @ 23532 updates, score 11.145) (writing took 3.552510142326355 seconds)
2022-07-19 08:47:02 | INFO | train_inner | epoch 022:     68 / 1122 loss=6.273, nll_loss=2.28, mask_ins=0.78, word_ins_ml=3.94, word_reposition=0.714, kpe=0.839, ppl=77.34, wps=5208.2, ups=0.26, wpb=19856.5, bsz=253.8, num_updates=23600, lr=0.000230144, gnorm=1.581, clip=0, loss_scale=4096, train_wall=255, wall=0
2022-07-19 08:51:58 | INFO | train_inner | epoch 022:    168 / 1122 loss=6.25, nll_loss=2.256, mask_ins=0.788, word_ins_ml=3.92, word_reposition=0.709, kpe=0.834, ppl=76.08, wps=6765.1, ups=0.34, wpb=20029.2, bsz=256, num_updates=23700, lr=0.000229658, gnorm=1.472, clip=0, loss_scale=6390, train_wall=255, wall=0
2022-07-19 08:53:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-19 08:54:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-19 08:56:59 | INFO | train_inner | epoch 022:    270 / 1122 loss=6.226, nll_loss=2.24, mask_ins=0.777, word_ins_ml=3.905, word_reposition=0.709, kpe=0.834, ppl=74.84, wps=6676.4, ups=0.33, wpb=20109, bsz=256, num_updates=23800, lr=0.000229175, gnorm=1.514, clip=0, loss_scale=4397, train_wall=260, wall=0
2022-07-19 09:01:58 | INFO | train_inner | epoch 022:    370 / 1122 loss=6.26, nll_loss=2.254, mask_ins=0.785, word_ins_ml=3.918, word_reposition=0.721, kpe=0.836, ppl=76.64, wps=6736.4, ups=0.33, wpb=20124.1, bsz=256, num_updates=23900, lr=0.000228695, gnorm=1.486, clip=0, loss_scale=2048, train_wall=258, wall=0
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
train.sh: line 42: --keyword_gran: command not found
