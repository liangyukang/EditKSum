nohup: ignoring input
2022-07-26 15:04:03 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:16743
2022-07-26 15:04:03 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:16743
2022-07-26 15:04:03 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-26 15:04:03 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:16743
2022-07-26 15:04:03 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-26 15:04:03 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:16743
2022-07-26 15:04:03 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-26 15:04:03 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-26 15:04:03 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-26 15:04:03 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-26 15:04:03 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-26 15:04:03 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-26 15:04:03 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-26 15:04:03 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-26 15:04:03 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-26 15:04:03 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-26 15:04:07 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:16743', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, constraint=True, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='keyword', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-26 15:04:07 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-26 15:04:07 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
start load cached examples valid ...
2022-07-26 15:04:07 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.source
0it [00:00, ?it/s]2022-07-26 15:04:07 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.target
2022-07-26 15:04:07 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]397it [00:00, 3959.38it/s]384it [00:00, 3834.91it/s]396it [00:00, 3956.39it/s]377it [00:00, 3765.69it/s]754it [00:00, 3462.96it/s]768it [00:00, 3488.60it/s]793it [00:00, 3547.58it/s]792it [00:00, 3550.08it/s]1117it [00:00, 3535.37it/s]1123it [00:00, 3514.92it/s]1152it [00:00, 3562.76it/s]1162it [00:00, 3611.75it/s]1476it [00:00, 3365.39it/s]1510it [00:00, 3420.00it/s]1472it [00:00, 3330.71it/s]1526it [00:00, 3437.72it/s]1854it [00:00, 3505.98it/s]1899it [00:00, 3578.90it/s]1862it [00:00, 3522.71it/s]1919it [00:00, 3601.93it/s]2259it [00:00, 3461.14it/s]2282it [00:00, 3513.66it/s]2236it [00:00, 3432.32it/s]2236it [00:00, 3401.74it/s]2657it [00:00, 3621.64it/s]2681it [00:00, 3660.16it/s]2621it [00:00, 3560.86it/s]2634it [00:00, 3577.93it/s]3044it [00:00, 3697.49it/s]3016it [00:00, 3677.71it/s]3026it [00:00, 3680.98it/s]3061it [00:00, 3577.83it/s]3416it [00:00, 3581.42it/s]3450it [00:00, 3668.80it/s]3386it [00:00, 3519.32it/s]3397it [00:00, 3526.93it/s]3806it [00:01, 3674.10it/s]3841it [00:01, 3739.54it/s]3776it [00:01, 3624.00it/s]3786it [00:01, 3630.94it/s]4175it [00:01, 3537.24it/s]4217it [00:01, 3607.02it/s]4141it [00:01, 3488.25it/s]4152it [00:01, 3489.06it/s]4555it [00:01, 3611.61it/s]4600it [00:01, 3670.32it/s]4520it [00:01, 3574.21it/s]4533it [00:01, 3580.89it/s]4918it [00:01, 3475.29it/s]4969it [00:01, 3543.10it/s]4880it [00:01, 3434.63it/s]4894it [00:01, 3470.66it/s]5296it [00:01, 3560.69it/s]5348it [00:01, 3612.58it/s]5254it [00:01, 3521.31it/s]5256it [00:01, 3512.07it/s]5654it [00:01, 3440.37it/s]5711it [00:01, 3473.24it/s]5609it [00:01, 3387.17it/s]5609it [00:01, 3372.24it/s]6000it [00:01, 3444.77it/s]6061it [00:01, 3426.92it/s]5952it [00:01, 3398.88it/s]5964it [00:01, 3421.66it/s]6346it [00:01, 3417.00it/s]6417it [00:01, 3463.10it/s]6309it [00:01, 3445.06it/s]6309it [00:01, 3429.35it/s]6654it [00:02, 1955.45it/s]6689it [00:02, 1795.37it/s]6655it [00:02, 1798.78it/s]6765it [00:02, 1742.69it/s]7009it [00:02, 2259.99it/s]7029it [00:02, 2082.64it/s]6997it [00:02, 2087.92it/s]7098it [00:02, 2015.64it/s]7346it [00:02, 2417.66it/s]7346it [00:02, 2265.97it/s]7346it [00:02, 2298.43it/s]7394it [00:02, 2201.42it/s]7701it [00:02, 2676.90it/s]7704it [00:02, 2557.54it/s]7688it [00:02, 2544.07it/s]7730it [00:02, 2456.26it/s]8046it [00:02, 2863.28it/s]8050it [00:02, 2774.62it/s]8043it [00:02, 2784.15it/s]8083it [00:02, 2711.62it/s]8368it [00:02, 2894.65it/s]8371it [00:02, 2837.95it/s]8365it [00:02, 2833.40it/s]8401it [00:02, 2776.70it/s]8709it [00:02, 3031.58it/s]8722it [00:02, 3013.57it/s]8723it [00:02, 3027.80it/s]8735it [00:02, 2923.81it/s]9032it [00:02, 2967.97it/s]9048it [00:02, 3019.74it/s]9051it [00:02, 3019.88it/s]9053it [00:02, 2946.57it/s]9376it [00:03, 3092.66it/s]9397it [00:03, 3149.53it/s]9409it [00:03, 3172.74it/s]9387it [00:03, 3054.66it/s]9735it [00:03, 3230.42it/s]9758it [00:03, 3279.04it/s]9770it [00:03, 3294.03it/s]9742it [00:03, 3193.17it/s]10067it [00:03, 3136.96it/s]10096it [00:03, 3184.72it/s]10110it [00:03, 3207.36it/s]10072it [00:03, 3068.61it/s]10423it [00:03, 3255.77it/s]10454it [00:03, 3296.01it/s]10468it [00:03, 3310.88it/s]10425it [00:03, 3198.10it/s]10754it [00:03, 3121.23it/s]10790it [00:03, 3187.69it/s]10805it [00:03, 3212.06it/s]10752it [00:03, 3092.18it/s]11108it [00:03, 3237.33it/s]11147it [00:03, 3294.02it/s]11161it [00:03, 3308.72it/s]11101it [00:03, 3203.41it/s]11464it [00:03, 3328.74it/s]11497it [00:03, 3352.07it/s]11519it [00:03, 3384.33it/s]11456it [00:03, 3300.72it/s]11800it [00:03, 3183.21it/s]11836it [00:03, 3249.32it/s]11861it [00:03, 3264.13it/s]11790it [00:03, 3155.11it/s]12159it [00:03, 3298.15it/s]12178it [00:03, 3297.18it/s]12221it [00:03, 3359.72it/s]12143it [00:03, 3261.24it/s]12492it [00:03, 3199.56it/s]12510it [00:03, 3217.18it/s]12560it [00:04, 3251.25it/s]12473it [00:04, 3137.67it/s]12849it [00:04, 3303.01it/s]12834it [00:04, 3212.10it/s]12917it [00:04, 3341.43it/s]12825it [00:04, 3245.44it/s]13192it [00:04, 3339.16it/s]13191it [00:04, 3315.19it/s]13254it [00:04, 3227.48it/s]13175it [00:04, 3318.26it/s]13368it [00:04, 3158.74it/s]
13368it [00:04, 3146.00it/s]
13368it [00:04, 3145.19it/s]
13368it [00:04, 3114.70it/s]
2022-07-26 15:04:11 | INFO | root | success load 13368 data
2022-07-26 15:04:11 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-26 15:04:11 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-26 15:04:11 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-26 15:04:11 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-26 15:04:11 | INFO | transformer.tokenization_utils | loading file None
2022-07-26 15:04:11 | INFO | transformer.tokenization_utils | loading file None
2022-07-26 15:04:11 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
2022-07-26 15:04:12 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-26 15:04:12 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-26 15:04:12 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-26 15:04:16 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-26 15:04:16 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-26 15:04:16 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-26 15:04:16 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-26 15:04:16 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-26 15:04:18 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-26 15:04:18 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-07-26 15:04:18 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-07-26 15:04:18 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-26 15:04:18 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 142456835)
2022-07-26 15:04:18 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-07-26 15:04:18 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)
2022-07-26 15:04:18 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-26 15:04:18 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8

Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]343it [00:00, 3421.34it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]696it [00:00, 3483.39it/s]344it [00:00, 3435.15it/s]1045it [00:00, 3234.19it/s]702it [00:00, 3514.70it/s]1381it [00:00, 3278.35it/s]1054it [00:00, 3160.27it/s]2022-07-26 15:04:20 | INFO | fairseq.trainer | loaded checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 15 @ 16810 updates)
1753it [00:00, 3430.06it/s]2022-07-26 15:04:20 | INFO | fairseq.trainer | loading train data for epoch 15
1404it [00:00, 3283.22it/s]2098it [00:00, 3290.34it/s]1768it [00:00, 3406.04it/s]2022-07-26 15:04:20 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.source
2022-07-26 15:04:20 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.target
2022-07-26 15:04:20 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]2460it [00:00, 3385.42it/s]2112it [00:00, 3178.27it/s]354it [00:00, 3536.80it/s]2801it [00:00, 3245.57it/s]2467it [00:00, 3288.85it/s]716it [00:00, 3584.59it/s]3173it [00:00, 3384.09it/s]2800it [00:00, 3156.94it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]1075it [00:00, 3362.41it/s]3514it [00:01, 3235.21it/s]3169it [00:00, 3311.76it/s]360it [00:00, 3599.30it/s]1438it [00:00, 3462.45it/s]3880it [00:01, 3355.54it/s]3510it [00:01, 3239.42it/s]738it [00:00, 3704.09it/s]1786it [00:00, 3357.16it/s]4240it [00:01, 3426.61it/s]3866it [00:01, 3331.63it/s]1109it [00:00, 3462.23it/s]2158it [00:00, 3475.52it/s]4585it [00:01, 3281.96it/s]4230it [00:01, 3420.06it/s]1482it [00:00, 3561.03it/s]2530it [00:00, 3544.14it/s]4956it [00:01, 3403.52it/s]4574it [00:01, 3301.85it/s]1840it [00:00, 3466.14it/s]2886it [00:00, 3390.60it/s]5299it [00:01, 3287.09it/s]4926it [00:01, 3363.23it/s]2224it [00:00, 3588.52it/s]3260it [00:00, 3493.05it/s]5667it [00:01, 3397.75it/s]5264it [00:01, 3276.55it/s]2600it [00:00, 3640.45it/s]3611it [00:01, 3358.47it/s]6030it [00:01, 3318.22it/s]5633it [00:01, 3394.45it/s]2966it [00:00, 3513.41it/s]3977it [00:01, 3445.28it/s]6392it [00:01, 3402.30it/s]5989it [00:01, 3442.50it/s]3346it [00:00, 3599.24it/s]4324it [00:01, 3346.54it/s]6765it [00:02, 3496.46it/s]6335it [00:01, 3330.95it/s]3708it [00:01, 3477.49it/s]4698it [00:01, 3457.56it/s]6707it [00:02, 3442.49it/s]4085it [00:01, 3562.03it/s]5050it [00:01, 3472.46it/s]4443it [00:01, 3425.18it/s]5399it [00:01, 3326.55it/s]4816it [00:01, 3512.73it/s]5762it [00:01, 3412.55it/s]5190it [00:01, 3397.41it/s]6105it [00:01, 3258.63it/s]5567it [00:01, 3500.70it/s]6462it [00:01, 3346.35it/s]5944it [00:01, 3577.58it/s]6800it [00:02, 3204.53it/s]6304it [00:01, 3447.54it/s]7117it [00:02, 1053.92it/s]6683it [00:01, 3544.19it/s]7490it [00:03, 1351.93it/s]7053it [00:02, 1043.99it/s]7783it [00:03, 1566.46it/s]7425it [00:02, 1343.86it/s]8158it [00:03, 1923.57it/s]7779it [00:03, 1615.26it/s]8523it [00:03, 2250.40it/s]8152it [00:03, 1957.45it/s]8854it [00:03, 2437.17it/s]8524it [00:03, 2287.68it/s]9217it [00:03, 2711.66it/s]8859it [00:03, 2465.65it/s]9553it [00:03, 2807.23it/s]9233it [00:03, 2756.40it/s]9920it [00:03, 3028.31it/s]7040it [00:02, 1158.97it/s]9574it [00:03, 2831.20it/s]7123it [00:03, 864.79it/s] 10294it [00:03, 3219.12it/s]7411it [00:02, 1462.38it/s]9949it [00:03, 3062.92it/s]7491it [00:03, 1140.41it/s]10644it [00:03, 3190.96it/s]7779it [00:02, 1746.30it/s]10299it [00:03, 3086.96it/s]7779it [00:03, 1352.78it/s]11015it [00:04, 3333.65it/s]8160it [00:03, 2094.01it/s]10676it [00:03, 3270.43it/s]8157it [00:03, 1714.05it/s]8541it [00:03, 2426.83it/s]11364it [00:04, 3265.68it/s]11050it [00:04, 3400.33it/s]8534it [00:03, 2074.18it/s]11736it [00:04, 3391.65it/s]8884it [00:03, 2604.22it/s]11406it [00:04, 3339.15it/s]8863it [00:03, 2278.05it/s]9260it [00:03, 2873.18it/s]12084it [00:04, 3297.86it/s]11773it [00:04, 3431.93it/s]9226it [00:03, 2573.65it/s]12468it [00:04, 3450.09it/s]9608it [00:03, 2963.64it/s]12125it [00:04, 3355.50it/s]9560it [00:03, 2671.24it/s]9992it [00:03, 3190.43it/s]12820it [00:04, 3356.88it/s]12511it [00:04, 3497.28it/s]9936it [00:03, 2939.62it/s]13214it [00:04, 3521.96it/s]10345it [00:03, 3178.40it/s]12866it [00:04, 3405.02it/s]10274it [00:04, 2935.02it/s]10724it [00:03, 3344.56it/s]13590it [00:04, 3582.78it/s]13253it [00:04, 3537.43it/s]10644it [00:04, 3135.41it/s]11103it [00:03, 3468.53it/s]13952it [00:04, 3492.85it/s]13640it [00:04, 3631.47it/s]11013it [00:04, 3286.14it/s]14330it [00:04, 3573.88it/s]11464it [00:03, 3405.80it/s]14006it [00:04, 3527.05it/s]11360it [00:04, 3193.69it/s]11839it [00:04, 3501.25it/s]14690it [00:05, 3475.91it/s]14394it [00:04, 3627.10it/s]11736it [00:04, 3350.13it/s]12197it [00:04, 3431.97it/s]15069it [00:05, 3566.15it/s]14759it [00:05, 3509.58it/s]12082it [00:04, 3280.14it/s]12586it [00:04, 3560.36it/s]15428it [00:05, 3449.89it/s]15145it [00:05, 3608.83it/s]12450it [00:04, 3392.53it/s]12947it [00:04, 3475.27it/s]15795it [00:05, 3510.91it/s]15508it [00:05, 3473.24it/s]12796it [00:04, 3327.71it/s]13332it [00:04, 3580.78it/s]16167it [00:05, 3568.93it/s]15884it [00:05, 3553.99it/s]13169it [00:04, 3437.37it/s]13693it [00:04, 3498.23it/s]16242it [00:05, 3382.07it/s]13532it [00:04, 3491.93it/s]14084it [00:04, 3614.28it/s]13884it [00:05, 3312.36it/s]14475it [00:04, 3700.00it/s]14254it [00:05, 3421.48it/s]14847it [00:04, 3576.71it/s]14600it [00:05, 3288.38it/s]15236it [00:05, 3665.25it/s]14979it [00:05, 3428.84it/s]15605it [00:05, 3521.45it/s]15325it [00:05, 3334.13it/s]15972it [00:05, 3561.61it/s]15675it [00:05, 3380.46it/s]16041it [00:05, 3460.71it/s]16526it [00:06, 930.20it/s] 16583it [00:06, 1022.17it/s]16896it [00:06, 1202.15it/s]16962it [00:06, 1321.68it/s]17213it [00:06, 1443.96it/s]17274it [00:06, 1562.16it/s]17579it [00:06, 1774.89it/s]17649it [00:06, 1911.89it/s]17929it [00:06, 2046.38it/s]17968it [00:06, 2145.96it/s]18304it [00:07, 2383.64it/s]18348it [00:06, 2491.95it/s]16330it [00:06, 1112.39it/s]18690it [00:07, 2708.44it/s]18728it [00:07, 2792.24it/s]16723it [00:06, 1434.19it/s]19042it [00:07, 2802.30it/s]19077it [00:07, 2856.25it/s]17089it [00:06, 1721.56it/s]19420it [00:07, 3045.05it/s]19460it [00:07, 3101.78it/s]17479it [00:06, 2080.74it/s]17870it [00:06, 2428.71it/s]19770it [00:07, 3047.08it/s]19810it [00:07, 3110.98it/s]20149it [00:07, 3242.24it/s]20175it [00:07, 3255.12it/s]18222it [00:06, 2623.00it/s]18616it [00:06, 2927.90it/s]20522it [00:07, 3219.02it/s]20498it [00:07, 3178.78it/s]16389it [00:07, 745.05it/s] 20883it [00:07, 3325.83it/s]18976it [00:06, 3016.20it/s]20875it [00:07, 3339.80it/s]16777it [00:07, 1003.25it/s]21260it [00:07, 3451.22it/s]19354it [00:06, 3212.51it/s]21254it [00:07, 3465.33it/s]17097it [00:07, 1234.37it/s]21614it [00:07, 3369.55it/s]19712it [00:07, 3224.37it/s]21611it [00:08, 3340.77it/s]17449it [00:07, 1532.97it/s]21982it [00:08, 3457.50it/s]20095it [00:07, 3388.22it/s]21989it [00:08, 3461.87it/s]17821it [00:07, 1876.92it/s]22333it [00:08, 3375.70it/s]20454it [00:07, 3333.21it/s]22342it [00:08, 3387.32it/s]18154it [00:07, 2093.17it/s]22706it [00:08, 3475.62it/s]20837it [00:07, 3470.34it/s]22725it [00:08, 3512.95it/s]18529it [00:07, 2430.65it/s]21219it [00:07, 3566.40it/s]23057it [00:08, 3376.10it/s]23081it [00:08, 3412.65it/s]18867it [00:07, 2587.24it/s]23419it [00:08, 3443.83it/s]21584it [00:07, 3477.22it/s]23453it [00:08, 3498.34it/s]19234it [00:07, 2846.13it/s]23792it [00:08, 3526.53it/s]21960it [00:07, 3555.37it/s]23809it [00:08, 3392.25it/s]19574it [00:07, 2923.46it/s]22321it [00:07, 3474.86it/s]24147it [00:08, 3387.96it/s]24175it [00:08, 3467.72it/s]19942it [00:08, 3121.35it/s]22711it [00:07, 3594.20it/s]24520it [00:08, 3483.58it/s]24552it [00:08, 3553.67it/s]20307it [00:08, 3264.55it/s]23074it [00:07, 3492.76it/s]24871it [00:08, 3351.95it/s]24910it [00:08, 3431.06it/s]20657it [00:08, 3158.28it/s]23451it [00:08, 3570.87it/s]25245it [00:08, 3459.98it/s]25283it [00:09, 3515.42it/s]21023it [00:08, 3293.78it/s]23811it [00:08, 3459.11it/s]25594it [00:09, 3319.59it/s]25637it [00:09, 3389.45it/s]21366it [00:08, 3157.24it/s]24193it [00:08, 3561.07it/s]25967it [00:09, 3435.73it/s]26011it [00:09, 3488.04it/s]21708it [00:08, 3228.21it/s]24570it [00:08, 3620.22it/s]26328it [00:09, 3485.37it/s]26362it [00:09, 3383.44it/s]22059it [00:08, 3181.41it/s]24934it [00:08, 3462.70it/s]26679it [00:09, 3377.23it/s]26735it [00:09, 3481.36it/s]22383it [00:08, 3142.64it/s]25312it [00:08, 3551.65it/s]27043it [00:09, 3452.56it/s]27094it [00:09, 3512.27it/s]22723it [00:08, 3213.97it/s]25670it [00:08, 3444.47it/s]27390it [00:09, 3367.66it/s]27447it [00:09, 3412.73it/s]23048it [00:09, 3189.59it/s]26044it [00:08, 3528.61it/s]27752it [00:09, 3439.56it/s]27821it [00:09, 3505.72it/s]23418it [00:09, 3335.30it/s]26399it [00:08, 3408.34it/s]23754it [00:09, 3267.53it/s]26773it [00:09, 3502.38it/s]24134it [00:09, 3420.21it/s]27159it [00:09, 3603.60it/s]24492it [00:09, 3466.77it/s]27521it [00:09, 3479.45it/s]24840it [00:09, 3337.54it/s]27894it [00:09, 3550.07it/s]25204it [00:09, 3424.42it/s]25549it [00:09, 3311.79it/s]25907it [00:09, 3387.59it/s]26259it [00:09, 3290.91it/s]26639it [00:10, 3434.66it/s]27004it [00:10, 3495.53it/s]27356it [00:10, 3372.16it/s]27718it [00:10, 3440.48it/s]28173it [00:11, 703.16it/s] 28098it [00:11, 661.78it/s] 28544it [00:11, 934.82it/s]28469it [00:11, 887.37it/s]28902it [00:11, 1197.05it/s]28826it [00:11, 1143.92it/s]29212it [00:11, 1430.80it/s]29133it [00:11, 1373.40it/s]29582it [00:11, 1771.44it/s]29501it [00:11, 1702.53it/s]29909it [00:11, 2019.83it/s]29822it [00:11, 1949.83it/s]30283it [00:11, 2363.22it/s]30189it [00:11, 2287.64it/s]30621it [00:12, 2513.50it/s]30539it [00:11, 2553.11it/s]30992it [00:12, 2793.96it/s]28251it [00:11, 597.54it/s] 30877it [00:12, 2691.30it/s]31364it [00:12, 3026.46it/s]28624it [00:11, 801.89it/s]31245it [00:12, 2937.54it/s]31713it [00:12, 3066.90it/s]28919it [00:11, 983.00it/s]31586it [00:12, 2964.03it/s]32077it [00:12, 3219.59it/s]29286it [00:11, 1274.21it/s]31960it [00:12, 3170.40it/s]32424it [00:12, 3201.82it/s]29653it [00:11, 1595.41it/s]32303it [00:12, 3159.70it/s]32786it [00:12, 3316.08it/s]29983it [00:11, 1844.69it/s]28064it [00:11, 657.90it/s] 32666it [00:12, 3288.87it/s]33131it [00:12, 3258.86it/s]30353it [00:11, 2187.27it/s]28422it [00:12, 872.26it/s]33033it [00:12, 3397.01it/s]33491it [00:12, 3353.01it/s]30690it [00:11, 2361.28it/s]28780it [00:12, 1129.03it/s]33383it [00:12, 3309.04it/s]33869it [00:12, 3474.44it/s]31057it [00:11, 2653.57it/s]29085it [00:12, 1354.53it/s]33745it [00:12, 3396.15it/s]34222it [00:13, 3390.21it/s]31425it [00:12, 2901.00it/s]29444it [00:12, 1679.22it/s]34091it [00:13, 3322.91it/s]34593it [00:13, 3479.65it/s]29764it [00:12, 1921.29it/s]31771it [00:12, 2959.97it/s]34463it [00:13, 3435.96it/s]34945it [00:13, 3341.55it/s]30120it [00:12, 2240.71it/s]32149it [00:12, 3174.32it/s]34811it [00:13, 3329.21it/s]35316it [00:13, 3445.50it/s]30488it [00:12, 2554.46it/s]32497it [00:12, 3154.87it/s]35182it [00:13, 3437.52it/s]35664it [00:13, 3352.72it/s]30827it [00:12, 2667.28it/s]32864it [00:12, 3294.19it/s]35552it [00:13, 3511.03it/s]36038it [00:13, 3463.10it/s]31193it [00:12, 2911.18it/s]33210it [00:12, 3235.12it/s]35906it [00:13, 3405.73it/s]36396it [00:13, 3494.81it/s]33580it [00:12, 3365.12it/s]31532it [00:13, 2927.89it/s]36270it [00:13, 3464.11it/s]36748it [00:13, 3360.74it/s]33947it [00:12, 3450.66it/s]31894it [00:13, 3109.83it/s]36618it [00:13, 3351.74it/s]37116it [00:13, 3450.00it/s]34299it [00:12, 3353.34it/s]32231it [00:13, 3097.73it/s]36984it [00:13, 3437.85it/s]37463it [00:14, 3310.96it/s]34669it [00:13, 3451.60it/s]32593it [00:13, 3241.88it/s]37330it [00:13, 3336.26it/s]37832it [00:14, 3417.94it/s]32956it [00:13, 3349.33it/s]35019it [00:13, 3340.94it/s]37694it [00:14, 3422.49it/s]38176it [00:14, 3343.23it/s]35393it [00:13, 3454.40it/s]33302it [00:13, 3249.58it/s]38065it [00:14, 3505.22it/s]38532it [00:14, 3403.87it/s]33673it [00:13, 3379.87it/s]35742it [00:13, 3339.84it/s]38417it [00:14, 3372.94it/s]38899it [00:14, 3479.27it/s]34018it [00:13, 3318.52it/s]36120it [00:13, 3463.07it/s]38783it [00:14, 3453.05it/s]39249it [00:14, 3383.63it/s]34391it [00:13, 3434.91it/s]36479it [00:13, 3342.41it/s]39130it [00:14, 3337.80it/s]39620it [00:14, 3476.88it/s]34739it [00:13, 3322.59it/s]36853it [00:13, 3452.31it/s]39501it [00:14, 3444.05it/s]39969it [00:14, 3372.05it/s]35103it [00:14, 3411.69it/s]37207it [00:13, 3475.24it/s]39848it [00:14, 3342.97it/s]40331it [00:14, 3443.06it/s]35470it [00:14, 3485.77it/s]37557it [00:13, 3345.54it/s]40222it [00:14, 3455.34it/s]40680it [00:14, 3300.42it/s]35821it [00:14, 3343.72it/s]37924it [00:13, 3437.63it/s]40576it [00:14, 3477.22it/s]41052it [00:15, 3418.48it/s]36191it [00:14, 3444.18it/s]38270it [00:14, 3348.18it/s]40925it [00:14, 3375.61it/s]41430it [00:15, 3521.60it/s]36538it [00:14, 3334.34it/s]38624it [00:14, 3401.13it/s]41297it [00:15, 3472.83it/s]41784it [00:15, 3377.49it/s]36905it [00:14, 3429.61it/s]39000it [00:14, 3323.07it/s]41646it [00:15, 3365.58it/s]42156it [00:15, 3473.74it/s]37250it [00:14, 3292.26it/s]39379it [00:14, 3452.28it/s]41984it [00:15, 3365.59it/s]42506it [00:15, 3383.18it/s]37614it [00:14, 3389.90it/s]39750it [00:14, 3524.99it/s]42358it [00:15, 3472.18it/s]42877it [00:15, 3475.34it/s]37981it [00:14, 3469.19it/s]40105it [00:14, 3330.23it/s]42707it [00:15, 3373.38it/s]43227it [00:15, 3331.57it/s]40470it [00:14, 3417.96it/s]38330it [00:15, 3194.97it/s]43067it [00:15, 3438.68it/s]38690it [00:15, 3305.84it/s]40815it [00:14, 3314.80it/s]39026it [00:15, 3233.82it/s]41181it [00:14, 3412.10it/s]39404it [00:15, 3387.11it/s]41525it [00:15, 3305.93it/s]41892it [00:15, 3408.59it/s]39768it [00:15, 3304.60it/s]42266it [00:15, 3503.17it/s]40139it [00:15, 3416.57it/s]40508it [00:15, 3494.08it/s]42618it [00:15, 3392.14it/s]42990it [00:15, 3478.85it/s]40860it [00:15, 3322.58it/s]41219it [00:15, 3397.42it/s]41562it [00:15, 3277.19it/s]41920it [00:16, 3360.75it/s]42288it [00:16, 3271.43it/s]42649it [00:16, 3365.33it/s]43015it [00:16, 3447.24it/s]43412it [00:17, 581.82it/s] 43563it [00:17, 554.72it/s] 43770it [00:17, 779.34it/s]43937it [00:17, 755.91it/s]44108it [00:17, 994.88it/s]44232it [00:17, 937.38it/s]44484it [00:17, 1295.12it/s]44604it [00:17, 1231.28it/s]44856it [00:17, 1620.64it/s]44948it [00:17, 1498.87it/s]45188it [00:17, 1858.06it/s]45309it [00:18, 1826.89it/s]45558it [00:18, 2196.47it/s]45683it [00:18, 2174.34it/s]45894it [00:18, 2388.50it/s]46023it [00:18, 2369.70it/s]46261it [00:18, 2676.08it/s]46391it [00:18, 2662.47it/s]46617it [00:18, 2890.76it/s]46732it [00:18, 2750.74it/s]46961it [00:18, 2943.68it/s]47107it [00:18, 2999.55it/s]47335it [00:18, 3152.43it/s]47468it [00:18, 3039.07it/s]43340it [00:17, 473.86it/s] 47681it [00:18, 3133.35it/s]47837it [00:18, 3211.44it/s]43697it [00:17, 639.32it/s]48038it [00:18, 3251.35it/s]48192it [00:18, 3304.54it/s]44071it [00:17, 859.20it/s]48379it [00:18, 3201.02it/s]43362it [00:18, 556.66it/s] 48540it [00:19, 3262.96it/s]44380it [00:18, 1063.62it/s]48756it [00:18, 3359.81it/s]43722it [00:18, 746.98it/s]48918it [00:19, 3408.50it/s]44745it [00:18, 1363.55it/s]49130it [00:19, 3466.74it/s]44037it [00:18, 943.01it/s]49269it [00:19, 3319.52it/s]45070it [00:18, 1613.71it/s]44400it [00:18, 1224.44it/s]49484it [00:19, 3337.53it/s]49642it [00:19, 3434.56it/s]45432it [00:18, 1949.22it/s]44762it [00:18, 1536.11it/s]49852it [00:19, 3434.03it/s]49991it [00:19, 3301.55it/s]45788it [00:18, 2192.47it/s]45089it [00:18, 1761.90it/s]50200it [00:19, 3322.68it/s]50361it [00:19, 3412.20it/s]46152it [00:18, 2496.95it/s]45451it [00:18, 2094.74it/s]50569it [00:19, 3424.85it/s]50732it [00:19, 3496.17it/s]46516it [00:18, 2759.83it/s]45779it [00:19, 2279.03it/s]50915it [00:19, 3288.14it/s]51085it [00:19, 3402.57it/s]46860it [00:18, 2837.07it/s]46138it [00:19, 2567.64it/s]51289it [00:19, 3415.45it/s]51444it [00:19, 3456.11it/s]47234it [00:18, 3066.61it/s]46490it [00:19, 2795.71it/s]51657it [00:19, 3490.09it/s]51792it [00:19, 3349.23it/s]47579it [00:18, 3065.48it/s]46826it [00:19, 2868.56it/s]52009it [00:19, 3370.75it/s]52159it [00:20, 3439.29it/s]47930it [00:19, 3184.64it/s]47189it [00:19, 3066.43it/s]52368it [00:20, 3427.31it/s]52508it [00:20, 3342.07it/s]48299it [00:19, 3324.40it/s]47527it [00:19, 3092.12it/s]52713it [00:20, 3318.62it/s]52869it [00:20, 3418.62it/s]48647it [00:19, 3237.44it/s]47887it [00:19, 3231.99it/s]53082it [00:20, 3422.86it/s]53238it [00:20, 3496.14it/s]49012it [00:19, 3351.55it/s]48237it [00:19, 3126.50it/s]53427it [00:20, 3301.71it/s]53589it [00:20, 3380.15it/s]49356it [00:19, 3250.24it/s]48562it [00:19, 3157.03it/s]53795it [00:20, 3409.11it/s]53965it [00:20, 3488.04it/s]49733it [00:19, 3396.87it/s]48929it [00:19, 3301.04it/s]54170it [00:20, 3507.06it/s]54316it [00:20, 3355.48it/s]50078it [00:19, 3319.15it/s]49267it [00:20, 3206.82it/s]54523it [00:20, 3390.64it/s]54687it [00:20, 3455.39it/s]50451it [00:19, 3436.14it/s]49638it [00:20, 3349.13it/s]54882it [00:20, 3447.61it/s]55035it [00:20, 3354.10it/s]50798it [00:19, 3435.35it/s]49978it [00:20, 3276.50it/s]55229it [00:20, 3283.84it/s]55399it [00:21, 3435.35it/s]51144it [00:20, 3358.72it/s]50335it [00:20, 3359.79it/s]55592it [00:20, 3379.75it/s]55765it [00:21, 3497.88it/s]51486it [00:20, 3376.12it/s]50692it [00:20, 3418.85it/s]55933it [00:21, 3283.89it/s]56117it [00:21, 3344.20it/s]51825it [00:20, 3304.23it/s]51036it [00:20, 3313.31it/s]56296it [00:21, 3381.43it/s]56488it [00:21, 3446.21it/s]52168it [00:20, 3339.05it/s]51403it [00:20, 3415.83it/s]56666it [00:21, 3473.07it/s]56835it [00:21, 3359.72it/s]52509it [00:20, 3274.23it/s]51747it [00:20, 3298.55it/s]57015it [00:21, 3375.50it/s]57209it [00:21, 3467.51it/s]52880it [00:20, 3338.45it/s]52119it [00:20, 3417.66it/s]57389it [00:21, 3480.20it/s]57558it [00:21, 3370.62it/s]53250it [00:20, 3442.72it/s]52463it [00:21, 3286.81it/s]57739it [00:21, 3351.89it/s]57924it [00:21, 3453.48it/s]53596it [00:20, 3344.93it/s]52821it [00:21, 3368.08it/s]58108it [00:21, 3442.81it/s]58290it [00:21, 3512.39it/s]53956it [00:20, 3418.24it/s]53181it [00:21, 3434.09it/s]58454it [00:21, 3331.22it/s]58643it [00:21, 3390.94it/s]54299it [00:20, 3325.91it/s]53526it [00:21, 3277.45it/s]58820it [00:21, 3422.91it/s]59009it [00:22, 3467.80it/s]54664it [00:21, 3417.29it/s]53897it [00:21, 3398.91it/s]59177it [00:22, 3464.20it/s]59358it [00:22, 3355.08it/s]55025it [00:21, 3472.03it/s]54240it [00:21, 3341.91it/s]59525it [00:22, 3343.48it/s]59722it [00:22, 3436.67it/s]55374it [00:21, 3337.13it/s]54581it [00:21, 3359.49it/s]59892it [00:22, 3435.47it/s]60068it [00:22, 3329.71it/s]55728it [00:21, 3393.57it/s]54934it [00:21, 3408.34it/s]60238it [00:22, 3322.78it/s]60433it [00:22, 3420.16it/s]56069it [00:21, 3301.77it/s]55276it [00:21, 3271.10it/s]60594it [00:22, 3387.97it/s]60792it [00:22, 3468.01it/s]56438it [00:21, 3412.74it/s]55619it [00:21, 3315.23it/s]60935it [00:22, 3287.95it/s]61140it [00:22, 3361.69it/s]56781it [00:21, 3338.93it/s]55952it [00:22, 3248.09it/s]61310it [00:22, 3419.42it/s]61509it [00:22, 3454.25it/s]57147it [00:21, 3430.56it/s]56314it [00:22, 3353.40it/s]61672it [00:22, 3475.54it/s]57509it [00:21, 3485.07it/s]56651it [00:22, 3241.07it/s]57859it [00:22, 3351.40it/s]57001it [00:22, 3313.18it/s]58215it [00:22, 3409.70it/s]57375it [00:22, 3435.64it/s]58558it [00:22, 3318.73it/s]57720it [00:22, 3343.73it/s]58908it [00:22, 3369.98it/s]58056it [00:22, 3346.18it/s]59247it [00:22, 3278.25it/s]58392it [00:22, 3228.01it/s]59611it [00:22, 3380.13it/s]58738it [00:22, 3282.43it/s]59967it [00:22, 3430.95it/s]59100it [00:23, 3379.32it/s]60312it [00:22, 3325.49it/s]59440it [00:23, 3266.67it/s]60687it [00:22, 3445.98it/s]59805it [00:23, 3374.76it/s]61033it [00:22, 3349.98it/s]60144it [00:23, 3241.73it/s]61404it [00:23, 3452.44it/s]60512it [00:23, 3365.48it/s]61751it [00:23, 3348.51it/s]60851it [00:23, 3285.72it/s]61189it [00:23, 3312.23it/s]61557it [00:23, 3418.12it/s]61856it [00:25, 461.76it/s] 62232it [00:25, 635.55it/s]62602it [00:25, 849.95it/s]62021it [00:25, 428.06it/s] 62913it [00:25, 1054.33it/s]62391it [00:25, 588.27it/s]63288it [00:25, 1363.12it/s]62693it [00:25, 747.16it/s]63617it [00:25, 1616.36it/s]63062it [00:25, 998.51it/s]63988it [00:25, 1962.47it/s]63422it [00:25, 1281.22it/s]64338it [00:25, 2202.66it/s]63750it [00:25, 1533.40it/s]64667it [00:25, 2408.85it/s]64120it [00:25, 1879.27it/s]65037it [00:26, 2703.94it/s]64456it [00:26, 2109.12it/s]65375it [00:26, 2793.50it/s]64810it [00:26, 2403.57it/s]65739it [00:26, 3005.92it/s]65176it [00:26, 2688.50it/s]66077it [00:26, 3029.56it/s]65520it [00:26, 2773.30it/s]66441it [00:26, 3194.48it/s]65884it [00:26, 2992.16it/s]66813it [00:26, 3340.96it/s]66225it [00:26, 3005.96it/s]67163it [00:26, 3264.63it/s]66579it [00:26, 3147.23it/s]67533it [00:26, 3385.35it/s]66916it [00:26, 3131.05it/s]67880it [00:26, 3309.73it/s]67286it [00:26, 3288.44it/s]68251it [00:27, 3422.45it/s]67656it [00:26, 3404.68it/s]68599it [00:27, 3285.51it/s]61900it [00:26, 395.22it/s] 62088it [00:26, 360.18it/s] 68006it [00:27, 3288.41it/s]68970it [00:27, 3403.87it/s]62274it [00:26, 549.66it/s]62462it [00:26, 502.89it/s]68375it [00:27, 3400.54it/s]69342it [00:27, 3493.53it/s]62587it [00:26, 708.67it/s]62767it [00:26, 647.77it/s]68721it [00:27, 3297.09it/s]69695it [00:27, 3357.93it/s]62945it [00:26, 941.59it/s]63142it [00:26, 880.91it/s]69089it [00:27, 3404.04it/s]70070it [00:27, 3467.76it/s]63300it [00:26, 1212.37it/s]63498it [00:26, 1125.76it/s]69433it [00:27, 3270.78it/s]70420it [00:27, 3353.72it/s]63624it [00:26, 1450.68it/s]63870it [00:26, 1437.34it/s]69801it [00:27, 3384.16it/s]70786it [00:27, 3440.51it/s]63964it [00:27, 1750.04it/s]64242it [00:26, 1771.24it/s]70174it [00:27, 3482.95it/s]71133it [00:27, 3316.98it/s]64285it [00:27, 1955.26it/s]64585it [00:26, 2014.93it/s]70525it [00:27, 3367.29it/s]71494it [00:27, 3399.51it/s]64629it [00:27, 2251.90it/s]64959it [00:26, 2350.94it/s]70871it [00:27, 3393.10it/s]71867it [00:28, 3493.03it/s]65001it [00:27, 2577.67it/s]65304it [00:27, 2533.46it/s]71213it [00:28, 3292.52it/s]72219it [00:28, 3362.77it/s]65336it [00:27, 2671.56it/s]65669it [00:27, 2792.52it/s]71582it [00:28, 3404.43it/s]72591it [00:28, 3462.61it/s]65703it [00:27, 2920.16it/s]66018it [00:27, 2893.10it/s]71925it [00:28, 3314.57it/s]72940it [00:28, 3379.93it/s]66039it [00:27, 2929.01it/s]66389it [00:27, 3102.67it/s]72291it [00:28, 3412.59it/s]73305it [00:28, 3456.37it/s]66407it [00:27, 3128.09it/s]66764it [00:27, 3276.25it/s]72645it [00:28, 3448.99it/s]73653it [00:28, 3369.73it/s]66767it [00:27, 3257.88it/s]67119it [00:27, 3243.07it/s]72992it [00:28, 3360.34it/s]74023it [00:28, 3460.66it/s]67111it [00:28, 3156.05it/s]67488it [00:27, 3357.84it/s]73358it [00:28, 3445.98it/s]74371it [00:28, 3311.94it/s]67838it [00:27, 3304.06it/s]67440it [00:28, 3047.85it/s]73704it [00:28, 3342.34it/s]74705it [00:28, 3208.25it/s]68201it [00:27, 3394.98it/s]67755it [00:28, 3002.76it/s]74071it [00:28, 3434.20it/s]75069it [00:29, 3329.42it/s]68107it [00:28, 3145.18it/s]68548it [00:28, 3291.83it/s]74418it [00:28, 3315.26it/s]75404it [00:29, 3202.85it/s]68921it [00:28, 3413.63it/s]68467it [00:28, 3121.92it/s]74783it [00:29, 3409.22it/s]75748it [00:29, 3267.36it/s]69268it [00:28, 3381.95it/s]68817it [00:28, 3222.28it/s]75139it [00:29, 3450.46it/s]76098it [00:29, 3176.35it/s]69170it [00:28, 3309.11it/s]69610it [00:28, 3302.24it/s]75486it [00:29, 3323.35it/s]76442it [00:29, 3249.76it/s]69983it [00:28, 3423.09it/s]75846it [00:29, 3400.71it/s]69504it [00:28, 3061.97it/s]76769it [00:29, 3178.82it/s]70328it [00:28, 3337.56it/s]69821it [00:28, 3091.35it/s]76188it [00:29, 3272.86it/s]77089it [00:29, 3116.70it/s]70700it [00:28, 3446.91it/s]70147it [00:29, 3065.77it/s]76528it [00:29, 3307.09it/s]77424it [00:29, 3181.66it/s]71058it [00:28, 3315.37it/s]70504it [00:29, 3206.23it/s]76884it [00:29, 3380.12it/s]77778it [00:29, 3151.69it/s]71428it [00:28, 3423.48it/s]70858it [00:29, 3300.60it/s]77224it [00:29, 3276.88it/s]78146it [00:30, 3301.61it/s]71800it [00:28, 3507.98it/s]71191it [00:29, 3211.95it/s]77589it [00:29, 3383.37it/s]78512it [00:30, 3403.80it/s]72153it [00:29, 3388.52it/s]71550it [00:29, 3294.15it/s]77929it [00:30, 3275.59it/s]78854it [00:30, 3295.55it/s]72522it [00:29, 3473.11it/s]71881it [00:29, 3183.04it/s]78296it [00:30, 3388.02it/s]79220it [00:30, 3398.95it/s]72872it [00:29, 3381.76it/s]72226it [00:29, 3213.92it/s]78637it [00:30, 3277.32it/s]79562it [00:30, 3260.23it/s]73243it [00:29, 3474.02it/s]72572it [00:29, 3283.79it/s]78998it [00:30, 3371.81it/s]79933it [00:30, 3387.57it/s]73592it [00:29, 3386.44it/s]72902it [00:29, 3222.03it/s]79349it [00:30, 3411.59it/s]80299it [00:30, 3290.11it/s]73972it [00:29, 3504.68it/s]73240it [00:29, 3266.46it/s]79692it [00:30, 3297.19it/s]80672it [00:30, 3412.74it/s]74345it [00:29, 3569.09it/s]73568it [00:30, 3206.73it/s]80058it [00:30, 3401.09it/s]81038it [00:30, 3482.28it/s]74704it [00:29, 3425.94it/s]73944it [00:30, 3366.63it/s]80400it [00:30, 3302.48it/s]81389it [00:30, 3353.68it/s]75049it [00:29, 3392.46it/s]74313it [00:30, 3461.25it/s]80768it [00:30, 3409.62it/s]81752it [00:31, 3432.22it/s]75390it [00:30, 3299.88it/s]74661it [00:30, 3346.90it/s]81127it [00:30, 3461.10it/s]82098it [00:31, 3296.70it/s]75752it [00:30, 3390.41it/s]75023it [00:30, 3425.05it/s]81475it [00:31, 3324.42it/s]82464it [00:31, 3397.42it/s]76098it [00:30, 3288.62it/s]75367it [00:30, 3310.10it/s]81837it [00:31, 3406.87it/s]82819it [00:31, 3293.07it/s]76459it [00:30, 3379.23it/s]75727it [00:30, 3393.32it/s]82180it [00:31, 3263.70it/s]83171it [00:31, 3355.10it/s]76813it [00:30, 3423.78it/s]76068it [00:30, 3224.04it/s]82537it [00:31, 3349.06it/s]83538it [00:31, 3443.35it/s]77157it [00:30, 3314.37it/s]76414it [00:30, 3288.42it/s]82874it [00:31, 3261.72it/s]83884it [00:31, 3329.39it/s]77529it [00:30, 3429.92it/s]76761it [00:31, 3339.36it/s]83237it [00:31, 3365.31it/s]84251it [00:31, 3425.71it/s]77874it [00:30, 3332.57it/s]83596it [00:31, 3427.64it/s]77097it [00:31, 3176.44it/s]84596it [00:31, 3317.69it/s]78240it [00:30, 3426.51it/s]77449it [00:31, 3271.15it/s]83941it [00:31, 3314.47it/s]84950it [00:32, 3379.67it/s]78609it [00:30, 3500.78it/s]84304it [00:31, 3403.88it/s]77779it [00:31, 3150.00it/s]85310it [00:32, 3443.20it/s]78961it [00:31, 3372.46it/s]78139it [00:31, 3276.66it/s]84646it [00:32, 3201.47it/s]79323it [00:31, 3442.38it/s]78488it [00:31, 3335.77it/s]85009it [00:32, 3319.70it/s]79669it [00:31, 3317.75it/s]85344it [00:32, 3218.39it/s]78824it [00:31, 3050.23it/s]80039it [00:31, 3426.88it/s]79173it [00:31, 3168.71it/s]80384it [00:31, 3297.24it/s]79495it [00:31, 3088.38it/s]80755it [00:31, 3413.65it/s]79848it [00:31, 3211.92it/s]81121it [00:31, 3482.14it/s]80198it [00:32, 3292.06it/s]81471it [00:31, 3350.85it/s]80530it [00:32, 3188.96it/s]81838it [00:31, 3441.43it/s]80882it [00:32, 3281.81it/s]82184it [00:32, 3335.67it/s]81213it [00:32, 3134.12it/s]82548it [00:32, 3420.79it/s]81562it [00:32, 3232.65it/s]82892it [00:32, 3314.19it/s]81905it [00:32, 3287.19it/s]83260it [00:32, 3418.58it/s]82236it [00:32, 3174.85it/s]83627it [00:32, 3490.38it/s]82573it [00:32, 3228.44it/s]83978it [00:32, 3364.98it/s]82898it [00:32, 3132.97it/s]84345it [00:32, 3451.74it/s]83238it [00:33, 3201.50it/s]84692it [00:32, 3327.02it/s]83587it [00:33, 3284.07it/s]85044it [00:32, 3381.88it/s]83917it [00:33, 3147.16it/s]85384it [00:33, 3272.29it/s]84266it [00:33, 3242.90it/s]84593it [00:33, 3108.01it/s]84942it [00:33, 3214.69it/s]85268it [00:33, 3059.38it/s]85656it [00:35, 371.69it/s] 86023it [00:35, 514.01it/s]86316it [00:35, 654.24it/s]85669it [00:35, 360.69it/s] 86680it [00:35, 882.81it/s]86032it [00:35, 502.13it/s]87044it [00:35, 1154.67it/s]86328it [00:35, 645.18it/s]87370it [00:35, 1402.14it/s]86695it [00:35, 877.39it/s]87740it [00:35, 1743.96it/s]87056it [00:35, 1147.32it/s]88075it [00:35, 1993.65it/s]87378it [00:35, 1390.79it/s]88448it [00:35, 2336.17it/s]87749it [00:35, 1735.86it/s]88788it [00:35, 2507.03it/s]88081it [00:35, 1979.87it/s]89147it [00:36, 2760.48it/s]88451it [00:35, 2319.80it/s]89510it [00:36, 2977.94it/s]88788it [00:36, 2468.31it/s]89856it [00:36, 2995.30it/s]89152it [00:36, 2741.52it/s]90217it [00:36, 3157.87it/s]89513it [00:36, 2957.78it/s]90559it [00:36, 3048.64it/s]89857it [00:36, 2928.29it/s]90884it [00:36, 3100.99it/s]90191it [00:36, 3036.47it/s]85713it [00:35, 401.24it/s] 91238it [00:36, 3220.71it/s]86070it [00:35, 552.00it/s]90520it [00:36, 2998.29it/s]91571it [00:36, 3068.64it/s]86378it [00:35, 712.88it/s]90873it [00:36, 3143.00it/s]91924it [00:36, 3194.36it/s]86745it [00:35, 959.33it/s]91218it [00:36, 3221.87it/s]92251it [00:37, 3127.52it/s]87088it [00:36, 1207.13it/s]91551it [00:36, 3134.56it/s]92605it [00:37, 3242.08it/s]87461it [00:36, 1536.40it/s]91902it [00:37, 3239.66it/s]92958it [00:37, 3324.55it/s]87833it [00:36, 1879.32it/s]92232it [00:37, 3131.04it/s]93294it [00:37, 3204.45it/s]88174it [00:36, 2122.17it/s]92584it [00:37, 3239.88it/s]93648it [00:37, 3299.98it/s]88547it [00:36, 2451.18it/s]92935it [00:37, 3317.51it/s]93981it [00:37, 3182.52it/s]88891it [00:36, 2611.53it/s]93270it [00:37, 3190.36it/s]94325it [00:37, 3255.64it/s]89258it [00:36, 2864.47it/s]93623it [00:37, 3284.92it/s]94653it [00:37, 3156.29it/s]89608it [00:36, 2937.31it/s]93955it [00:37, 3144.48it/s]95005it [00:37, 3259.49it/s]89976it [00:36, 3129.97it/s]94309it [00:37, 3254.37it/s]95360it [00:37, 3341.62it/s]90337it [00:36, 3259.95it/s]94648it [00:37, 3147.41it/s]95696it [00:38, 3212.58it/s]90687it [00:37, 3144.20it/s]95000it [00:37, 3252.32it/s]96049it [00:38, 3300.20it/s]91044it [00:37, 3260.58it/s]95353it [00:38, 3331.82it/s]96381it [00:38, 3187.23it/s]91384it [00:37, 3175.04it/s]95689it [00:38, 3200.63it/s]96734it [00:38, 3283.61it/s]91740it [00:37, 3281.88it/s]96043it [00:38, 3296.71it/s]97079it [00:38, 3330.73it/s]92096it [00:37, 3358.65it/s]96375it [00:38, 3140.81it/s]85577it [00:37, 245.38it/s] 97414it [00:38, 3203.08it/s]92438it [00:37, 3237.92it/s]96729it [00:38, 3251.65it/s]85911it [00:37, 341.53it/s]97770it [00:38, 3302.71it/s]92795it [00:37, 3331.57it/s]97082it [00:38, 3329.14it/s]86210it [00:38, 454.09it/s]98102it [00:38, 3180.78it/s]93132it [00:37, 3208.08it/s]97418it [00:38, 3195.47it/s]86551it [00:38, 623.20it/s]98457it [00:38, 3283.70it/s]93489it [00:37, 3310.60it/s]97767it [00:38, 3278.14it/s]86889it [00:38, 831.50it/s]98812it [00:39, 3356.70it/s]93823it [00:38, 3186.60it/s]98097it [00:38, 3144.12it/s]87193it [00:38, 1038.38it/s]99150it [00:39, 3210.91it/s]94180it [00:38, 3294.56it/s]98450it [00:39, 3252.47it/s]87547it [00:38, 1341.60it/s]99496it [00:39, 3281.57it/s]94539it [00:38, 3377.76it/s]98803it [00:39, 3331.70it/s]87861it [00:38, 1581.53it/s]99827it [00:39, 3171.02it/s]94879it [00:38, 3240.21it/s]99139it [00:39, 3163.59it/s]88219it [00:38, 1924.90it/s]100179it [00:39, 3261.43it/s]95233it [00:38, 3324.98it/s]99493it [00:39, 3267.91it/s]88584it [00:38, 2263.88it/s]100529it [00:39, 3164.93it/s]95568it [00:38, 3172.45it/s]99823it [00:39, 3163.08it/s]88918it [00:38, 2458.14it/s]100884it [00:39, 3271.50it/s]95924it [00:38, 3281.59it/s]100175it [00:39, 3263.13it/s]89269it [00:39, 2705.28it/s]101240it [00:39, 3353.16it/s]96281it [00:38, 3363.77it/s]89603it [00:39, 2814.19it/s]100528it [00:39, 3160.80it/s]101578it [00:39, 3221.28it/s]96620it [00:38, 3239.41it/s]89957it [00:39, 3001.61it/s]100878it [00:39, 3254.81it/s]101929it [00:40, 3298.67it/s]96975it [00:38, 3326.09it/s]90317it [00:39, 3163.63it/s]101229it [00:39, 3325.65it/s]102261it [00:40, 3166.54it/s]97310it [00:39, 3223.40it/s]90660it [00:39, 3091.18it/s]101564it [00:40, 3199.58it/s]102616it [00:40, 3273.67it/s]97666it [00:39, 3318.28it/s]91016it [00:39, 3220.45it/s]101905it [00:40, 3257.03it/s]102971it [00:40, 3351.37it/s]98008it [00:39, 3217.98it/s]91353it [00:39, 3123.00it/s]102233it [00:40, 3148.44it/s]103308it [00:40, 3222.65it/s]98363it [00:39, 3311.95it/s]91707it [00:39, 3231.50it/s]102586it [00:40, 3254.86it/s]103660it [00:40, 3305.15it/s]98718it [00:39, 3380.58it/s]92052it [00:39, 3291.66it/s]102939it [00:40, 3333.17it/s]103993it [00:40, 3190.70it/s]99058it [00:39, 3252.82it/s]92388it [00:39, 3189.03it/s]103274it [00:40, 3205.64it/s]104346it [00:40, 3285.83it/s]99416it [00:39, 3344.92it/s]92733it [00:40, 3261.39it/s]103624it [00:40, 3289.20it/s]104689it [00:40, 3325.85it/s]99753it [00:39, 3233.97it/s]93063it [00:40, 3166.92it/s]103955it [00:40, 3180.52it/s]105023it [00:40, 3198.69it/s]100108it [00:39, 3316.41it/s]93404it [00:40, 3236.15it/s]104296it [00:40, 3243.64it/s]105374it [00:41, 3286.04it/s]100456it [00:40, 3345.00it/s]104647it [00:40, 3320.35it/s]93737it [00:40, 3155.05it/s]105705it [00:41, 3177.34it/s]100792it [00:40, 3224.34it/s]94076it [00:40, 3221.71it/s]104981it [00:41, 3191.18it/s]106057it [00:41, 3273.33it/s]101148it [00:40, 3318.08it/s]94433it [00:40, 3320.53it/s]105332it [00:41, 3281.22it/s]106409it [00:41, 3174.19it/s]101482it [00:40, 3199.79it/s]94767it [00:40, 3207.94it/s]105662it [00:41, 3164.95it/s]106764it [00:41, 3277.90it/s]101835it [00:40, 3291.57it/s]95117it [00:40, 3289.58it/s]106014it [00:41, 3264.46it/s]107117it [00:41, 3349.63it/s]102190it [00:40, 3364.31it/s]106364it [00:41, 3330.14it/s]95448it [00:40, 3180.00it/s]107454it [00:41, 3208.97it/s]102528it [00:40, 3236.37it/s]95801it [00:41, 3280.05it/s]106699it [00:41, 3209.07it/s]107809it [00:41, 3301.29it/s]102883it [00:40, 3324.01it/s]96155it [00:41, 3353.30it/s]107042it [00:41, 3271.37it/s]108142it [00:41, 3195.58it/s]103218it [00:40, 3212.53it/s]96492it [00:41, 3206.11it/s]107371it [00:41, 3170.20it/s]108496it [00:42, 3291.58it/s]103570it [00:41, 3299.83it/s]96847it [00:41, 3302.93it/s]107723it [00:41, 3268.08it/s]108854it [00:42, 3373.89it/s]103902it [00:41, 3199.04it/s]108078it [00:42, 3347.82it/s]97180it [00:41, 3172.57it/s]109193it [00:42, 3247.14it/s]104259it [00:41, 3302.87it/s]97537it [00:41, 3284.24it/s]108415it [00:42, 3202.72it/s]109548it [00:42, 3332.28it/s]104613it [00:41, 3371.51it/s]97893it [00:41, 3363.45it/s]108773it [00:42, 3309.64it/s]109883it [00:42, 3216.92it/s]104952it [00:41, 3216.95it/s]98232it [00:41, 3242.17it/s]109106it [00:42, 3197.19it/s]110229it [00:42, 3285.56it/s]105307it [00:41, 3310.19it/s]98589it [00:41, 3335.50it/s]109459it [00:42, 3291.47it/s]110583it [00:42, 3356.93it/s]105641it [00:41, 3190.51it/s]98925it [00:41, 3213.42it/s]109790it [00:42, 3142.75it/s]110921it [00:42, 3226.18it/s]105996it [00:41, 3291.21it/s]99282it [00:42, 3313.69it/s]110144it [00:42, 3254.72it/s]111274it [00:42, 3310.75it/s]106351it [00:41, 3364.86it/s]110494it [00:42, 3324.58it/s]99617it [00:42, 3206.28it/s]111607it [00:42, 3190.62it/s]106690it [00:41, 3239.35it/s]99973it [00:42, 3304.67it/s]110829it [00:42, 3200.21it/s]111959it [00:43, 3283.96it/s]107046it [00:42, 3329.66it/s]100327it [00:42, 3371.41it/s]111180it [00:42, 3286.11it/s]112290it [00:43, 3174.48it/s]107381it [00:42, 3224.99it/s]100666it [00:42, 3216.33it/s]111511it [00:43, 3173.49it/s]112633it [00:43, 3246.66it/s]107738it [00:42, 3315.66it/s]101018it [00:42, 3293.83it/s]111862it [00:43, 3267.29it/s]112986it [00:43, 3326.99it/s]108088it [00:42, 3203.51it/s]112195it [00:43, 3283.22it/s]101350it [00:42, 3188.60it/s]113321it [00:43, 3208.88it/s]108444it [00:42, 3302.33it/s]101703it [00:42, 3285.73it/s]112525it [00:43, 3174.18it/s]113676it [00:43, 3304.69it/s]108804it [00:42, 3386.76it/s]102058it [00:42, 3360.46it/s]112875it [00:43, 3266.26it/s]114009it [00:43, 3191.04it/s]109145it [00:42, 3266.09it/s]113204it [00:43, 3152.26it/s]102396it [00:43, 3082.28it/s]114363it [00:43, 3290.34it/s]109501it [00:42, 3349.92it/s]113557it [00:43, 3257.60it/s]102751it [00:43, 3209.48it/s]114717it [00:43, 3361.86it/s]109838it [00:42, 3219.90it/s]113909it [00:43, 3332.79it/s]103077it [00:43, 3131.92it/s]110196it [00:43, 3321.65it/s]114244it [00:43, 3220.40it/s]103430it [00:43, 3244.14it/s]110554it [00:43, 3395.54it/s]114598it [00:44, 3304.38it/s]103786it [00:43, 3332.90it/s]110896it [00:43, 3272.96it/s]104122it [00:43, 3226.42it/s]111250it [00:43, 3347.77it/s]104460it [00:43, 3269.55it/s]111587it [00:43, 3233.58it/s]104789it [00:43, 3099.29it/s]111944it [00:43, 3327.39it/s]105122it [00:43, 3162.83it/s]112288it [00:43, 3223.56it/s]105454it [00:43, 3207.67it/s]112643it [00:43, 3315.82it/s]105777it [00:44, 3086.45it/s]112998it [00:43, 3381.94it/s]106094it [00:44, 3108.56it/s]113338it [00:43, 3263.42it/s]106407it [00:44, 3003.88it/s]113696it [00:44, 3353.97it/s]106731it [00:44, 3069.61it/s]114034it [00:44, 3251.59it/s]107071it [00:44, 3163.59it/s]114381it [00:44, 3312.43it/s]107389it [00:44, 3003.87it/s]114737it [00:44, 3383.75it/s]107738it [00:44, 3132.65it/s]108054it [00:44, 3077.49it/s]108409it [00:44, 3212.02it/s]108769it [00:45, 3324.48it/s]109104it [00:45, 3183.38it/s]109458it [00:45, 3284.56it/s]109789it [00:45, 3169.87it/s]110146it [00:45, 3283.62it/s]110482it [00:45, 3304.99it/s]110814it [00:45, 3200.43it/s]111168it [00:45, 3294.21it/s]111499it [00:45, 3184.91it/s]111854it [00:46, 3287.02it/s]112197it [00:46, 3326.57it/s]112531it [00:46, 3202.58it/s]112886it [00:46, 3301.30it/s]113218it [00:46, 3190.55it/s]113562it [00:46, 3260.92it/s]113898it [00:46, 3161.41it/s]114256it [00:46, 3279.56it/s]114613it [00:46, 3360.93it/s]115055it [00:47, 276.01it/s] 115409it [00:47, 384.17it/s]115717it [00:48, 504.49it/s]116065it [00:48, 684.42it/s]114930it [00:48, 269.49it/s] 116419it [00:48, 911.36it/s]115282it [00:48, 375.80it/s]115632it [00:48, 515.23it/s]116735it [00:48, 1130.96it/s]115926it [00:48, 658.27it/s]117092it [00:48, 1438.67it/s]116278it [00:48, 882.68it/s]117415it [00:48, 1683.81it/s]116585it [00:48, 1097.80it/s]117773it [00:48, 2018.23it/s]116940it [00:48, 1405.15it/s]118127it [00:48, 2323.75it/s]117295it [00:48, 1730.46it/s]118462it [00:48, 2478.64it/s]117624it [00:48, 1965.54it/s]118807it [00:48, 2707.44it/s]115077it [00:47, 302.08it/s] 117976it [00:48, 2275.26it/s]119137it [00:49, 2773.75it/s]115430it [00:48, 418.20it/s]118305it [00:49, 2434.19it/s]119488it [00:49, 2964.56it/s]115721it [00:48, 539.93it/s]118644it [00:49, 2658.54it/s]119835it [00:49, 3100.06it/s]116076it [00:48, 736.32it/s]118994it [00:49, 2868.57it/s]116425it [00:48, 971.04it/s]120169it [00:49, 3030.20it/s]119326it [00:49, 2877.45it/s]116741it [00:48, 1200.36it/s]120520it [00:49, 3160.89it/s]119675it [00:49, 3039.23it/s]117100it [00:48, 1518.92it/s]120850it [00:49, 3085.10it/s]117425it [00:48, 1770.04it/s]120003it [00:49, 3003.28it/s]121204it [00:49, 3212.28it/s]117782it [00:48, 2100.47it/s]120353it [00:49, 3138.72it/s]121544it [00:49, 3265.16it/s]118137it [00:48, 2401.03it/s]120707it [00:49, 3250.39it/s]121876it [00:49, 3164.16it/s]118474it [00:49, 2543.34it/s]121042it [00:49, 3096.74it/s]122228it [00:50, 3258.43it/s]118831it [00:49, 2789.10it/s]121394it [00:50, 3214.99it/s]122558it [00:50, 3164.03it/s]119165it [00:49, 2839.45it/s]121722it [00:50, 3122.61it/s]122914it [00:50, 3275.67it/s]119519it [00:49, 3022.25it/s]122073it [00:50, 3231.24it/s]123267it [00:50, 3348.73it/s]119870it [00:49, 3153.51it/s]122424it [00:50, 3310.87it/s]123604it [00:50, 3208.35it/s]120208it [00:49, 3078.85it/s]122759it [00:50, 3160.75it/s]123935it [00:50, 3236.52it/s]120564it [00:49, 3210.45it/s]123110it [00:50, 3258.43it/s]124261it [00:50, 3133.10it/s]120898it [00:49, 3107.72it/s]124613it [00:50, 3242.66it/s]123439it [00:50, 3133.47it/s]121252it [00:49, 3227.07it/s]123776it [00:50, 3198.63it/s]124958it [00:50, 3143.04it/s]121596it [00:49, 3146.36it/s]125310it [00:50, 3247.76it/s]124118it [00:50, 3102.48it/s]121947it [00:50, 3247.41it/s]125661it [00:51, 3322.88it/s]124469it [00:50, 3215.81it/s]122303it [00:50, 3336.67it/s]124821it [00:51, 3302.43it/s]125996it [00:51, 3219.16it/s]122641it [00:50, 3231.15it/s]126349it [00:51, 3307.72it/s]125154it [00:51, 3180.41it/s]122996it [00:50, 3318.17it/s]125505it [00:51, 3272.42it/s]126682it [00:51, 3156.22it/s]123331it [00:50, 3221.03it/s]127033it [00:51, 3254.07it/s]125835it [00:51, 3167.38it/s]123683it [00:50, 3306.43it/s]127387it [00:51, 3334.39it/s]126178it [00:51, 3239.99it/s]124036it [00:50, 3370.34it/s]126527it [00:51, 3310.91it/s]127723it [00:51, 3212.31it/s]114951it [00:51, 257.42it/s] 128076it [00:51, 3302.84it/s]124375it [00:50, 3131.73it/s]126860it [00:51, 3184.03it/s]115301it [00:51, 357.70it/s]124720it [00:50, 3219.35it/s]127209it [00:51, 3270.89it/s]128409it [00:51, 3188.15it/s]115638it [00:51, 485.14it/s]127538it [00:51, 3173.20it/s]125046it [00:51, 3120.76it/s]128764it [00:52, 3290.39it/s]115930it [00:51, 623.83it/s]127890it [00:52, 3270.29it/s]125376it [00:51, 3169.25it/s]129119it [00:52, 3364.23it/s]116284it [00:51, 843.24it/s]128242it [00:52, 3340.71it/s]125725it [00:51, 3259.21it/s]129457it [00:52, 3209.98it/s]116594it [00:51, 1058.55it/s]126053it [00:51, 3158.89it/s]128578it [00:52, 3220.14it/s]129810it [00:52, 3300.80it/s]116950it [00:51, 1362.22it/s]126402it [00:51, 3253.65it/s]128920it [00:52, 3276.09it/s]130143it [00:52, 3187.84it/s]117286it [00:51, 1657.12it/s]129250it [00:52, 3164.47it/s]130498it [00:52, 3290.34it/s]126730it [00:51, 2962.60it/s]117612it [00:51, 1893.75it/s]129601it [00:52, 3263.21it/s]130838it [00:52, 3182.06it/s]127062it [00:51, 3059.29it/s]117962it [00:51, 2207.29it/s]129954it [00:52, 3339.46it/s]131194it [00:52, 3287.98it/s]127389it [00:51, 3117.69it/s]118287it [00:52, 2382.60it/s]130290it [00:52, 3218.05it/s]131548it [00:52, 3360.17it/s]127705it [00:51, 3071.40it/s]118643it [00:52, 2656.73it/s]130641it [00:52, 3300.39it/s]128055it [00:51, 3192.66it/s]131886it [00:53, 3201.11it/s]118986it [00:52, 2849.42it/s]130973it [00:52, 3183.86it/s]128377it [00:52, 3118.33it/s]132238it [00:53, 3283.64it/s]119319it [00:52, 2883.23it/s]131325it [00:53, 3279.00it/s]128732it [00:52, 3241.98it/s]132569it [00:53, 3186.38it/s]119669it [00:52, 3044.96it/s]131662it [00:53, 3305.14it/s]129086it [00:52, 3326.48it/s]132923it [00:53, 3286.32it/s]119999it [00:52, 3022.17it/s]131994it [00:53, 3173.06it/s]133271it [00:53, 3340.52it/s]129421it [00:52, 3199.37it/s]120341it [00:52, 3130.74it/s]132344it [00:53, 3266.39it/s]129759it [00:52, 3250.28it/s]133607it [00:53, 3217.72it/s]120686it [00:52, 3091.34it/s]132673it [00:53, 3158.64it/s]130086it [00:52, 3151.72it/s]133963it [00:53, 3315.25it/s]121039it [00:52, 3212.99it/s]133025it [00:53, 3260.84it/s]130436it [00:52, 3251.55it/s]134297it [00:53, 3201.81it/s]121395it [00:53, 3310.26it/s]130786it [00:52, 3321.35it/s]133358it [00:53, 3142.68it/s]134638it [00:53, 3260.57it/s]121732it [00:53, 3193.69it/s]133712it [00:53, 3253.08it/s]134995it [00:53, 3348.23it/s]131120it [00:52, 3192.54it/s]122085it [00:53, 3289.11it/s]134058it [00:53, 3308.29it/s]131467it [00:53, 3270.90it/s]135332it [00:54, 3223.53it/s]122418it [00:53, 3203.52it/s]134391it [00:54, 3201.84it/s]131796it [00:53, 3170.20it/s]135689it [00:54, 3322.35it/s]122777it [00:53, 3312.02it/s]134746it [00:54, 3300.23it/s]132144it [00:53, 3258.42it/s]136023it [00:54, 3220.34it/s]123123it [00:53, 3354.12it/s]132490it [00:53, 3315.15it/s]135078it [00:54, 3188.57it/s]136379it [00:54, 3315.23it/s]123461it [00:53, 3234.42it/s]135434it [00:54, 3292.92it/s]132823it [00:53, 3183.09it/s]136718it [00:54, 3218.13it/s]123814it [00:53, 3317.72it/s]135790it [00:54, 3368.57it/s]133171it [00:53, 3266.16it/s]137064it [00:54, 3284.63it/s]124148it [00:53, 3213.93it/s]136129it [00:54, 3243.53it/s]137419it [00:54, 3359.07it/s]133500it [00:53, 3157.54it/s]124493it [00:53, 3281.19it/s]136482it [00:54, 3323.01it/s]133848it [00:53, 3248.44it/s]137757it [00:54, 3259.46it/s]124848it [00:54, 3358.43it/s]136816it [00:54, 3188.12it/s]138118it [00:54, 3350.09it/s]134197it [00:53, 3139.81it/s]125186it [00:54, 3238.70it/s]137170it [00:54, 3286.28it/s]134547it [00:53, 3235.44it/s]138455it [00:55, 3249.97it/s]125541it [00:54, 3326.66it/s]137525it [00:54, 3360.65it/s]134897it [00:54, 3306.40it/s]138813it [00:55, 3342.80it/s]125876it [00:54, 3204.47it/s]137863it [00:55, 3234.84it/s]139170it [00:55, 3406.42it/s]135230it [00:54, 3200.96it/s]126230it [00:54, 3299.63it/s]138219it [00:55, 3326.18it/s]135581it [00:54, 3287.36it/s]139512it [00:55, 3301.21it/s]126567it [00:54, 3202.09it/s]139868it [00:55, 3372.92it/s]138554it [00:55, 3205.11it/s]135912it [00:54, 3168.66it/s]126911it [00:54, 3268.97it/s]138910it [00:55, 3303.84it/s]136261it [00:54, 3259.97it/s]140207it [00:55, 3231.38it/s]127266it [00:54, 3346.28it/s]136607it [00:54, 3316.35it/s]139243it [00:55, 3192.77it/s]140564it [00:55, 3326.15it/s]127602it [00:54, 3221.20it/s]139585it [00:55, 3255.06it/s]136941it [00:54, 3189.81it/s]140917it [00:55, 3384.44it/s]127955it [00:55, 3307.59it/s]139938it [00:55, 3332.86it/s]137289it [00:54, 3271.85it/s]141257it [00:55, 3274.24it/s]128288it [00:55, 3183.75it/s]140273it [00:55, 3217.42it/s]141614it [00:55, 3356.72it/s]137618it [00:54, 3165.11it/s]128645it [00:55, 3293.68it/s]140627it [00:55, 3308.69it/s]137969it [00:55, 3261.20it/s]141952it [00:56, 3253.01it/s]128999it [00:55, 3363.40it/s]138319it [00:55, 3329.12it/s]142312it [00:56, 3350.50it/s]140960it [00:56, 3194.61it/s]129337it [00:55, 3246.23it/s]141316it [00:56, 3296.81it/s]142649it [00:56, 3248.41it/s]138654it [00:55, 3201.02it/s]129679it [00:55, 3294.92it/s]141669it [00:56, 3364.25it/s]143003it [00:56, 3331.79it/s]138995it [00:55, 3259.97it/s]130010it [00:55, 3199.20it/s]143361it [00:56, 3403.32it/s]142007it [00:56, 3209.14it/s]139323it [00:55, 3159.51it/s]130365it [00:55, 3297.52it/s]142361it [00:56, 3302.17it/s]143703it [00:56, 3288.42it/s]139672it [00:55, 3253.19it/s]130720it [00:55, 3368.93it/s]144060it [00:56, 3368.96it/s]140022it [00:55, 3324.26it/s]142694it [00:56, 3194.24it/s]131059it [00:56, 3229.42it/s]143049it [00:56, 3293.32it/s]144399it [00:56, 3264.17it/s]140356it [00:55, 3208.85it/s]131413it [00:56, 3316.23it/s]143403it [00:56, 3362.69it/s]144760it [00:56, 3361.72it/s]140704it [00:55, 3284.27it/s]131747it [00:56, 3207.42it/s]143741it [00:56, 3238.39it/s]141034it [00:55, 3173.83it/s]145118it [00:57, 3258.26it/s]132099it [00:56, 3295.73it/s]144095it [00:56, 3323.03it/s]141383it [00:56, 3261.91it/s]145478it [00:57, 3353.03it/s]132447it [00:56, 3180.00it/s]141727it [00:56, 3310.23it/s]145820it [00:57, 3370.59it/s]144429it [00:57, 3204.81it/s]132800it [00:56, 3277.27it/s]144776it [00:57, 3279.85it/s]146159it [00:57, 3270.36it/s]142060it [00:56, 3125.00it/s]133152it [00:56, 3346.38it/s]146515it [00:57, 3352.70it/s]142399it [00:56, 3197.55it/s]145118it [00:57, 3182.80it/s]133489it [00:56, 3233.41it/s]145475it [00:57, 3292.27it/s]146852it [00:57, 3254.11it/s]142722it [00:56, 3115.75it/s]133836it [00:56, 3299.66it/s]145831it [00:57, 3367.68it/s]147211it [00:57, 3349.35it/s]143072it [00:56, 3223.24it/s]134168it [00:56, 3200.86it/s]147568it [00:57, 3409.32it/s]143410it [00:56, 3266.47it/s]146170it [00:57, 3233.51it/s]134520it [00:57, 3291.58it/s]146520it [00:57, 3308.04it/s]147911it [00:57, 3290.18it/s]143739it [00:56, 3160.25it/s]134876it [00:57, 3369.21it/s]148267it [00:57, 3367.66it/s]144087it [00:56, 3249.62it/s]146853it [00:57, 3188.48it/s]135215it [00:57, 3230.05it/s]147206it [00:57, 3285.06it/s]148606it [00:58, 3226.02it/s]144414it [00:57, 3112.31it/s]135571it [00:57, 3322.86it/s]147552it [00:58, 3333.49it/s]148966it [00:58, 3330.02it/s]144733it [00:57, 3132.04it/s]135906it [00:57, 3220.82it/s]145072it [00:57, 3204.01it/s]147887it [00:58, 3211.73it/s]149318it [00:58, 3229.71it/s]136252it [00:57, 3289.16it/s]148240it [00:58, 3302.15it/s]149678it [00:58, 3332.86it/s]145394it [00:57, 3076.80it/s]136608it [00:57, 3365.83it/s]150035it [00:58, 3400.46it/s]145723it [00:57, 3137.11it/s]148572it [00:58, 3185.74it/s]136946it [00:57, 3251.33it/s]148925it [00:58, 3281.90it/s]150377it [00:58, 3280.84it/s]146039it [00:57, 3028.67it/s]137300it [00:57, 3333.02it/s]149280it [00:58, 3357.44it/s]150738it [00:58, 3373.52it/s]146373it [00:57, 3116.47it/s]137635it [00:58, 3198.51it/s]146727it [00:57, 3236.60it/s]149618it [00:58, 3236.05it/s]137992it [00:58, 3303.49it/s]149963it [00:58, 3296.58it/s]147053it [00:57, 3075.09it/s]138327it [00:58, 3207.21it/s]150295it [00:58, 3182.34it/s]147381it [00:58, 3131.24it/s]138681it [00:58, 3300.34it/s]150648it [00:58, 3279.94it/s]147697it [00:58, 3072.63it/s]139029it [00:58, 3347.94it/s]148055it [00:58, 3217.57it/s]150998it [00:59, 3181.39it/s]139366it [00:58, 3245.55it/s]148407it [00:58, 3302.34it/s]139723it [00:58, 3336.73it/s]148739it [00:58, 3214.24it/s]140059it [00:58, 3225.50it/s]149097it [00:58, 3318.33it/s]140408it [00:58, 3299.00it/s]149431it [00:58, 3189.42it/s]140765it [00:58, 3376.28it/s]149785it [00:58, 3288.44it/s]141104it [00:59, 3246.64it/s]150123it [00:58, 3314.19it/s]141461it [00:59, 3338.81it/s]150456it [00:58, 3139.95it/s]141797it [00:59, 3185.69it/s]150791it [00:59, 3197.69it/s]142155it [00:59, 3296.03it/s]142511it [00:59, 3371.62it/s]142851it [00:59, 3240.81it/s]143199it [00:59, 3308.85it/s]143532it [00:59, 3193.04it/s]143889it [00:59, 3298.52it/s]144221it [01:00, 3183.49it/s]144573it [01:00, 3276.73it/s]144930it [01:00, 3360.85it/s]145268it [01:00, 3245.50it/s]145623it [01:00, 3321.47it/s]145957it [01:00, 3217.71it/s]146313it [01:00, 3313.51it/s]146665it [01:00, 3371.54it/s]147004it [01:00, 3210.39it/s]147361it [01:00, 3311.51it/s]147695it [01:01, 3200.37it/s]148055it [01:01, 3313.03it/s]148401it [01:01, 3354.18it/s]148738it [01:01, 3223.46it/s]149097it [01:01, 3327.68it/s]149432it [01:01, 3204.93it/s]149782it [01:01, 3288.02it/s]150113it [01:01, 3180.37it/s]150473it [01:01, 3298.78it/s]150830it [01:02, 3376.00it/s]151078it [01:03, 245.05it/s] 151438it [01:03, 342.82it/s]151798it [01:03, 472.97it/s]152099it [01:03, 608.14it/s]152457it [01:03, 820.38it/s]152772it [01:03, 1028.39it/s]153134it [01:03, 1328.94it/s]153494it [01:03, 1651.43it/s]153829it [01:04, 1903.95it/s]154194it [01:04, 2237.44it/s]154531it [01:04, 2420.17it/s]154879it [01:04, 2664.16it/s]151319it [01:04, 209.95it/s] 155242it [01:04, 2901.52it/s]151675it [01:04, 296.63it/s]151957it [01:04, 386.58it/s]155584it [01:04, 2943.05it/s]152316it [01:04, 541.79it/s]155947it [01:04, 3124.42it/s]152675it [01:04, 739.08it/s]156288it [01:04, 3106.44it/s]151113it [01:03, 223.67it/s] 152991it [01:04, 938.37it/s]156651it [01:04, 3250.18it/s]151474it [01:03, 318.26it/s]153350it [01:04, 1222.62it/s]156991it [01:04, 3193.25it/s]151836it [01:03, 444.86it/s]153674it [01:04, 1472.78it/s]157346it [01:05, 3291.71it/s]152135it [01:04, 577.11it/s]154036it [01:05, 1811.52it/s]157714it [01:05, 3400.59it/s]152496it [01:04, 786.63it/s]154396it [01:05, 2140.25it/s]158061it [01:05, 3292.22it/s]152812it [01:04, 994.94it/s]154735it [01:05, 2318.73it/s]158423it [01:05, 3384.86it/s]153149it [01:04, 1263.63it/s]155094it [01:05, 2601.47it/s]153486it [01:04, 1556.48it/s]158766it [01:05, 3280.98it/s]155429it [01:05, 2699.61it/s]159133it [01:05, 3391.40it/s]153810it [01:04, 1796.70it/s]155789it [01:05, 2924.62it/s]154167it [01:04, 2129.58it/s]159475it [01:05, 3292.06it/s]156124it [01:05, 2930.31it/s]159844it [01:05, 3404.21it/s]154493it [01:04, 2298.82it/s]156485it [01:05, 3110.77it/s]160197it [01:05, 3438.10it/s]154831it [01:04, 2544.04it/s]156842it [01:05, 3237.31it/s]155183it [01:05, 2782.54it/s]160543it [01:06, 3322.72it/s]160907it [01:06, 3413.42it/s]157183it [01:06, 3144.83it/s]155514it [01:05, 2831.69it/s]157531it [01:06, 3236.45it/s]155871it [01:05, 3024.86it/s]161250it [01:06, 3302.06it/s]161613it [01:06, 3395.99it/s]157864it [01:06, 3140.02it/s]156202it [01:05, 3011.45it/s]161985it [01:06, 3489.26it/s]158223it [01:06, 3266.44it/s]156562it [01:05, 3172.29it/s]158584it [01:06, 3362.86it/s]156914it [01:05, 3268.73it/s]162336it [01:06, 3363.57it/s]162703it [01:06, 3451.28it/s]158925it [01:06, 3247.35it/s]157253it [01:05, 3182.97it/s]159288it [01:06, 3355.60it/s]157594it [01:05, 3245.84it/s]163050it [01:06, 3302.83it/s]163418it [01:06, 3408.54it/s]159627it [01:06, 3239.15it/s]157925it [01:05, 3169.60it/s]159978it [01:06, 3314.00it/s]158270it [01:05, 3247.91it/s]163761it [01:06, 3313.31it/s]164131it [01:07, 3421.94it/s]160312it [01:06, 3201.90it/s]158626it [01:06, 3337.72it/s]164496it [01:07, 3485.65it/s]160674it [01:07, 3318.89it/s]158963it [01:06, 3201.54it/s]161033it [01:07, 3394.63it/s]159305it [01:06, 3262.63it/s]164847it [01:07, 3368.17it/s]165211it [01:07, 3445.40it/s]161375it [01:07, 3258.47it/s]159634it [01:06, 3083.94it/s]161736it [01:07, 3358.09it/s]165558it [01:07, 3326.79it/s]159984it [01:06, 3199.29it/s]165930it [01:07, 3438.28it/s]162074it [01:07, 3258.24it/s]160308it [01:06, 3104.71it/s]162437it [01:07, 3364.37it/s]166276it [01:07, 3352.46it/s]160672it [01:06, 3254.15it/s]162788it [01:07, 3405.80it/s]166651it [01:07, 3465.11it/s]161020it [01:06, 3317.77it/s]151170it [01:07, 214.79it/s] 167022it [01:07, 3535.00it/s]163130it [01:07, 3291.65it/s]161354it [01:06, 3182.89it/s]151528it [01:07, 301.72it/s]163495it [01:07, 3394.69it/s]167377it [01:08, 3397.18it/s]161714it [01:07, 3299.28it/s]151836it [01:07, 399.20it/s]167748it [01:08, 3485.18it/s]163837it [01:08, 3273.84it/s]162047it [01:07, 3187.57it/s]152196it [01:07, 553.51it/s]164202it [01:08, 3380.31it/s]168099it [01:08, 3381.68it/s]162385it [01:07, 3241.14it/s]152555it [01:07, 749.08it/s]168462it [01:08, 3451.32it/s]164542it [01:08, 3274.48it/s]162751it [01:07, 3361.19it/s]152874it [01:07, 949.67it/s]164916it [01:08, 3405.96it/s]168809it [01:08, 3354.99it/s]163089it [01:07, 3225.28it/s]153233it [01:07, 1231.90it/s]165279it [01:08, 3468.35it/s]169176it [01:08, 3442.84it/s]163431it [01:07, 3279.08it/s]153560it [01:07, 1483.97it/s]169542it [01:08, 3503.33it/s]165628it [01:08, 3314.06it/s]163761it [01:07, 3158.46it/s]153923it [01:07, 1821.81it/s]169894it [01:08, 3380.04it/s]166000it [01:08, 3428.16it/s]164095it [01:07, 3208.14it/s]154285it [01:08, 2151.98it/s]170264it [01:08, 3470.00it/s]166345it [01:08, 3322.05it/s]164432it [01:07, 3253.31it/s]154626it [01:08, 2351.93it/s]166721it [01:08, 3446.47it/s]170613it [01:08, 3352.55it/s]164759it [01:07, 3110.23it/s]154956it [01:08, 2515.05it/s]170971it [01:09, 3416.23it/s]167068it [01:08, 3324.00it/s]165108it [01:08, 3218.20it/s]155278it [01:08, 2643.42it/s]171315it [01:09, 3305.77it/s]167434it [01:09, 3417.60it/s]165432it [01:08, 3101.65it/s]155641it [01:08, 2890.31it/s]171682it [01:09, 3409.11it/s]167802it [01:09, 3491.74it/s]165788it [01:08, 3231.16it/s]156001it [01:08, 3077.73it/s]172053it [01:09, 3495.49it/s]168153it [01:09, 3336.43it/s]166138it [01:08, 3307.45it/s]156340it [01:08, 3070.14it/s]172404it [01:09, 3388.23it/s]168526it [01:09, 3446.66it/s]166471it [01:08, 3180.57it/s]156700it [01:08, 3214.12it/s]172772it [01:09, 3471.45it/s]168873it [01:09, 3334.83it/s]166827it [01:08, 3285.86it/s]157038it [01:08, 3168.22it/s]173121it [01:09, 3379.25it/s]169238it [01:09, 3423.64it/s]167158it [01:08, 3210.32it/s]157400it [01:09, 3292.90it/s]173488it [01:09, 3462.61it/s]169583it [01:09, 3311.66it/s]167503it [01:08, 3277.51it/s]157739it [01:09, 3223.64it/s]173836it [01:09, 3323.45it/s]169945it [01:09, 3398.75it/s]167842it [01:08, 3309.30it/s]158101it [01:09, 3336.31it/s]174211it [01:10, 3443.26it/s]170316it [01:09, 3486.74it/s]168174it [01:09, 3244.33it/s]158465it [01:09, 3423.94it/s]168507it [01:09, 3266.98it/s]174587it [01:10, 3368.74it/s]170667it [01:10, 3348.35it/s]158812it [01:09, 3309.95it/s]174951it [01:10, 3444.30it/s]171018it [01:10, 3391.31it/s]168835it [01:09, 3192.85it/s]159165it [01:09, 3370.55it/s]175326it [01:10, 3530.73it/s]169188it [01:09, 3290.41it/s]171359it [01:10, 3298.04it/s]159505it [01:09, 3284.88it/s]175681it [01:10, 3408.94it/s]171718it [01:10, 3379.06it/s]169546it [01:09, 3215.85it/s]159866it [01:09, 3377.91it/s]176047it [01:10, 3479.42it/s]169911it [01:09, 3337.91it/s]172068it [01:10, 3286.69it/s]160225it [01:09, 3437.42it/s]170281it [01:09, 3439.97it/s]172437it [01:10, 3399.61it/s]176397it [01:10, 3331.80it/s]160571it [01:09, 3322.59it/s]172804it [01:10, 3477.80it/s]176767it [01:10, 3434.94it/s]170627it [01:09, 3324.63it/s]160933it [01:10, 3406.77it/s]170994it [01:09, 3421.74it/s]173154it [01:10, 3360.58it/s]177113it [01:10, 3325.24it/s]161276it [01:10, 3306.07it/s]173515it [01:10, 3430.55it/s]177481it [01:10, 3424.00it/s]171338it [01:09, 3310.63it/s]161627it [01:10, 3363.35it/s]177853it [01:11, 3508.17it/s]171701it [01:10, 3400.00it/s]173860it [01:10, 3281.91it/s]161965it [01:10, 3277.74it/s]174227it [01:11, 3390.82it/s]178206it [01:11, 3313.24it/s]172066it [01:10, 3301.51it/s]162330it [01:10, 3384.41it/s]178563it [01:11, 3383.59it/s]172435it [01:10, 3409.44it/s]162694it [01:10, 3457.35it/s]174588it [01:11, 3310.23it/s]172805it [01:10, 3492.17it/s]174947it [01:11, 3388.70it/s]178904it [01:11, 3268.93it/s]163041it [01:10, 3346.05it/s]175317it [01:11, 3476.49it/s]179235it [01:11, 3280.07it/s]173156it [01:10, 3352.68it/s]163406it [01:10, 3433.58it/s]179596it [01:11, 3374.94it/s]173521it [01:10, 3436.78it/s]175667it [01:11, 3352.14it/s]163751it [01:10, 3336.40it/s]176031it [01:11, 3432.15it/s]179936it [01:11, 3260.40it/s]164119it [01:11, 3433.99it/s]173867it [01:10, 3325.53it/s]180310it [01:11, 3396.49it/s]174236it [01:10, 3419.35it/s]176376it [01:11, 3255.51it/s]164464it [01:11, 3304.53it/s]180652it [01:11, 3290.07it/s]174586it [01:10, 3341.81it/s]176745it [01:11, 3376.87it/s]164835it [01:11, 3420.13it/s]181025it [01:12, 3414.80it/s]174947it [01:11, 3418.12it/s]165206it [01:11, 3495.82it/s]177108it [01:11, 3277.15it/s]175319it [01:11, 3503.97it/s]181369it [01:12, 3310.96it/s]165558it [01:11, 3384.25it/s]177474it [01:12, 3382.92it/s]181732it [01:12, 3399.70it/s]175671it [01:11, 3376.01it/s]165929it [01:11, 3477.16it/s]177842it [01:12, 3467.71it/s]182103it [01:12, 3488.57it/s]176037it [01:11, 3455.31it/s]166279it [01:11, 3382.71it/s]178191it [01:12, 3366.53it/s]182454it [01:12, 3334.21it/s]166653it [01:11, 3485.31it/s]176385it [01:11, 3332.77it/s]178554it [01:12, 3441.23it/s]182830it [01:12, 3452.99it/s]176755it [01:11, 3435.99it/s]167003it [01:11, 3356.68it/s]178900it [01:12, 3347.09it/s]183178it [01:12, 3343.36it/s]167371it [01:11, 3447.22it/s]179262it [01:12, 3423.48it/s]177107it [01:11, 3306.38it/s]183524it [01:12, 3376.08it/s]167741it [01:12, 3518.19it/s]177457it [01:11, 3352.03it/s]179627it [01:12, 3487.10it/s]177794it [01:11, 3337.09it/s]183864it [01:12, 3260.51it/s]168095it [01:12, 3405.83it/s]179977it [01:12, 3344.98it/s]184227it [01:13, 3364.96it/s]168469it [01:12, 3499.62it/s]180354it [01:12, 3465.48it/s]178129it [01:12, 3206.03it/s]184566it [01:13, 3367.22it/s]178477it [01:12, 3282.16it/s]168821it [01:12, 3387.78it/s]180703it [01:13, 3330.05it/s]169189it [01:12, 3469.12it/s]184904it [01:13, 3196.42it/s]181079it [01:13, 3451.04it/s]178807it [01:12, 3105.29it/s]185264it [01:13, 3310.25it/s]169538it [01:12, 3318.68it/s]179177it [01:12, 3270.88it/s]181427it [01:13, 3318.38it/s]169903it [01:12, 3412.33it/s]185598it [01:13, 3210.55it/s]179545it [01:12, 3386.29it/s]181796it [01:13, 3397.60it/s]170276it [01:12, 3502.23it/s]185965it [01:13, 3323.84it/s]179887it [01:12, 3288.26it/s]182148it [01:13, 3304.96it/s]186342it [01:13, 3451.86it/s]170628it [01:12, 3378.88it/s]180261it [01:12, 3416.10it/s]182519it [01:13, 3417.71it/s]170998it [01:13, 3468.86it/s]186689it [01:13, 3344.87it/s]182894it [01:13, 3512.34it/s]180605it [01:12, 3304.01it/s]187048it [01:13, 3408.22it/s]171347it [01:13, 3359.15it/s]180975it [01:12, 3416.96it/s]183247it [01:13, 3383.18it/s]187391it [01:13, 3336.89it/s]171711it [01:13, 3438.44it/s]183614it [01:13, 3464.64it/s]181319it [01:12, 3294.84it/s]187740it [01:14, 3379.08it/s]172057it [01:13, 3312.67it/s]181687it [01:13, 3404.11it/s]183963it [01:13, 3345.38it/s]188079it [01:14, 3316.07it/s]172425it [01:13, 3416.25it/s]182059it [01:13, 3493.34it/s]184335it [01:14, 3451.13it/s]188440it [01:14, 3401.16it/s]172793it [01:13, 3491.57it/s]182411it [01:13, 3366.87it/s]184682it [01:14, 3333.28it/s]188803it [01:14, 3467.82it/s]173144it [01:13, 3387.57it/s]182774it [01:13, 3441.11it/s]185057it [01:14, 3451.11it/s]189151it [01:14, 3330.82it/s]173510it [01:13, 3464.63it/s]185418it [01:14, 3495.26it/s]183120it [01:13, 3328.99it/s]189517it [01:14, 3423.24it/s]173858it [01:13, 3353.36it/s]183488it [01:13, 3426.86it/s]185770it [01:14, 3362.43it/s]189861it [01:14, 3315.60it/s]174229it [01:13, 3454.44it/s]186146it [01:14, 3475.35it/s]183833it [01:13, 3312.76it/s]190235it [01:14, 3436.61it/s]174576it [01:14, 3366.02it/s]184204it [01:13, 3424.54it/s]186496it [01:14, 3330.05it/s]190581it [01:14, 3322.34it/s]174928it [01:14, 3407.90it/s]184576it [01:13, 3509.30it/s]186885it [01:14, 3486.40it/s]190944it [01:15, 3409.64it/s]175300it [01:14, 3497.01it/s]184929it [01:14, 3331.10it/s]187237it [01:14, 3363.67it/s]191318it [01:15, 3474.69it/s]175651it [01:14, 3376.41it/s]185289it [01:14, 3407.20it/s]187607it [01:15, 3458.04it/s]176017it [01:14, 3457.82it/s]191667it [01:15, 3354.31it/s]187980it [01:15, 3535.41it/s]185633it [01:14, 3266.85it/s]192047it [01:15, 3479.70it/s]176365it [01:14, 3340.73it/s]186007it [01:14, 3398.19it/s]188336it [01:15, 3400.73it/s]176734it [01:14, 3438.48it/s]192397it [01:15, 3332.02it/s]188712it [01:15, 3502.74it/s]186350it [01:14, 3320.19it/s]192781it [01:15, 3475.26it/s]177080it [01:14, 3323.13it/s]186740it [01:14, 3485.15it/s]189065it [01:15, 3277.21it/s]177437it [01:14, 3393.47it/s]193131it [01:15, 3336.28it/s]187110it [01:14, 3545.63it/s]189434it [01:15, 3390.02it/s]177803it [01:15, 3469.81it/s]193497it [01:15, 3426.85it/s]187467it [01:14, 3416.06it/s]189777it [01:15, 3299.58it/s]193868it [01:15, 3506.87it/s]178152it [01:15, 3385.41it/s]187844it [01:14, 3515.04it/s]190153it [01:15, 3428.77it/s]178515it [01:15, 3454.37it/s]194221it [01:15, 3402.49it/s]188198it [01:14, 3363.24it/s]190521it [01:15, 3498.80it/s]194591it [01:16, 3486.02it/s]178862it [01:15, 3360.12it/s]188574it [01:15, 3473.55it/s]190874it [01:15, 3364.50it/s]179228it [01:15, 3444.93it/s]194942it [01:16, 3357.74it/s]188924it [01:15, 3359.75it/s]191243it [01:16, 3456.71it/s]179574it [01:15, 3346.29it/s]195321it [01:16, 3478.44it/s]189293it [01:15, 3451.80it/s]191591it [01:16, 3347.37it/s]179932it [01:15, 3413.45it/s]195671it [01:16, 3370.08it/s]189662it [01:15, 3519.16it/s]191971it [01:16, 3474.62it/s]180315it [01:15, 3533.43it/s]196030it [01:16, 3430.95it/s]190016it [01:15, 3410.14it/s]192321it [01:16, 3331.47it/s]196395it [01:16, 3492.40it/s]180670it [01:15, 3397.11it/s]190395it [01:15, 3517.15it/s]192703it [01:16, 3468.27it/s]181044it [01:15, 3495.48it/s]190749it [01:15, 3397.09it/s]193068it [01:16, 3364.75it/s]181396it [01:16, 3373.89it/s]191118it [01:15, 3478.43it/s]193438it [01:16, 3457.00it/s]181770it [01:16, 3476.97it/s]191468it [01:15, 3372.30it/s]193814it [01:16, 3541.47it/s]182120it [01:16, 3378.69it/s]191847it [01:16, 3489.37it/s]194171it [01:16, 3424.41it/s]182495it [01:16, 3484.41it/s]192221it [01:16, 3561.78it/s]194516it [01:17, 3390.28it/s]182861it [01:16, 3534.95it/s]192579it [01:16, 3432.52it/s]194857it [01:17, 3292.48it/s]183216it [01:16, 3403.99it/s]192945it [01:16, 3495.35it/s]195243it [01:17, 3452.63it/s]183589it [01:16, 3496.83it/s]193297it [01:16, 3389.80it/s]195590it [01:17, 3340.79it/s]183941it [01:16, 3374.92it/s]193662it [01:16, 3462.41it/s]195955it [01:17, 3428.94it/s]184317it [01:16, 3484.93it/s]194010it [01:16, 3387.46it/s]196325it [01:17, 3507.08it/s]184668it [01:17, 3377.27it/s]194387it [01:16, 3495.15it/s]185046it [01:17, 3492.18it/s]194747it [01:16, 3371.94it/s]185400it [01:17, 3504.97it/s]195140it [01:16, 3529.88it/s]185752it [01:17, 3394.20it/s]195510it [01:17, 3577.52it/s]186136it [01:17, 3522.22it/s]195870it [01:17, 3435.78it/s]186490it [01:17, 3398.67it/s]196237it [01:17, 3502.50it/s]186880it [01:17, 3541.98it/s]187237it [01:17, 3417.34it/s]187609it [01:17, 3502.98it/s]187962it [01:17, 3384.36it/s]188334it [01:18, 3479.02it/s]188715it [01:18, 3572.98it/s]189074it [01:18, 3427.41it/s]189448it [01:18, 3516.17it/s]189802it [01:18, 3394.24it/s]190185it [01:18, 3517.44it/s]190539it [01:18, 3358.77it/s]190909it [01:18, 3454.24it/s]191283it [01:18, 3533.96it/s]191639it [01:19, 3413.45it/s]192022it [01:19, 3531.64it/s]192378it [01:19, 3408.24it/s]192721it [01:19, 3408.22it/s]193064it [01:19, 3258.27it/s]193417it [01:19, 3332.03it/s]193795it [01:19, 3459.82it/s]194143it [01:19, 3322.78it/s]194504it [01:19, 3402.74it/s]194847it [01:20, 3262.66it/s]195223it [01:20, 3402.29it/s]195566it [01:20, 3309.03it/s]195932it [01:20, 3409.09it/s]196284it [01:20, 3439.58it/s]196678it [01:23, 189.30it/s] 197056it [01:23, 268.36it/s]197369it [01:23, 355.61it/s]197741it [01:23, 496.06it/s]198127it [01:24, 685.35it/s]196746it [01:24, 150.87it/s] 198466it [01:24, 880.47it/s]197126it [01:24, 215.48it/s]198826it [01:24, 1140.12it/s]197437it [01:24, 286.89it/s]199165it [01:24, 1394.45it/s]197820it [01:24, 407.51it/s]199546it [01:24, 1744.31it/s]198176it [01:24, 550.20it/s]199892it [01:24, 1992.49it/s]198560it [01:24, 753.30it/s]200258it [01:24, 2314.04it/s]198925it [01:24, 988.09it/s]200626it [01:24, 2605.35it/s]199271it [01:25, 1233.99it/s]200976it [01:24, 2725.82it/s]199644it [01:25, 1553.53it/s]201326it [01:25, 2915.42it/s]199993it [01:25, 1814.23it/s]201667it [01:25, 2975.30it/s]200349it [01:25, 2124.74it/s]202041it [01:25, 3177.60it/s]200697it [01:25, 2347.32it/s]202386it [01:25, 3178.39it/s]201058it [01:25, 2625.14it/s]202768it [01:25, 3355.87it/s]201429it [01:25, 2884.24it/s]203147it [01:25, 3477.09it/s]201780it [01:25, 2936.52it/s]203506it [01:25, 3362.89it/s]202160it [01:25, 3159.94it/s]203874it [01:25, 3452.19it/s]202510it [01:25, 3171.78it/s]204226it [01:25, 3310.10it/s]202894it [01:26, 3354.07it/s]204595it [01:25, 3416.94it/s]196590it [01:25, 149.43it/s] 203248it [01:26, 3226.73it/s]204942it [01:26, 3321.63it/s]196966it [01:25, 211.98it/s]203620it [01:26, 3362.10it/s]205316it [01:26, 3438.55it/s]203982it [01:26, 3434.76it/s]197335it [01:25, 294.81it/s]205688it [01:26, 3517.36it/s]204334it [01:26, 3353.76it/s]197698it [01:25, 405.60it/s]206043it [01:26, 3407.87it/s]204697it [01:26, 3431.45it/s]198074it [01:25, 557.58it/s]206423it [01:26, 3519.63it/s]205045it [01:26, 3354.62it/s]198408it [01:25, 723.24it/s]206778it [01:26, 3355.88it/s]205413it [01:26, 3445.88it/s]198772it [01:25, 955.25it/s]207148it [01:26, 3451.90it/s]199110it [01:25, 1192.50it/s]205761it [01:26, 3356.39it/s]207496it [01:26, 3342.87it/s]199488it [01:25, 1519.00it/s]206131it [01:27, 3454.94it/s]207871it [01:26, 3456.68it/s]199853it [01:26, 1844.46it/s]206507it [01:27, 3537.75it/s]208252it [01:27, 3557.10it/s]200204it [01:26, 2095.19it/s]206863it [01:27, 3387.81it/s]208610it [01:27, 3423.07it/s]200569it [01:26, 2405.28it/s]207242it [01:27, 3500.53it/s]208969it [01:27, 3469.39it/s]200916it [01:26, 2574.52it/s]207595it [01:27, 3355.88it/s]209318it [01:27, 3360.87it/s]201269it [01:26, 2798.82it/s]207974it [01:27, 3478.20it/s]209681it [01:27, 3435.53it/s]201610it [01:26, 2894.31it/s]208325it [01:27, 3387.31it/s]210027it [01:27, 3314.73it/s]201984it [01:26, 3113.62it/s]208698it [01:27, 3482.77it/s]210394it [01:27, 3414.22it/s]202362it [01:26, 3293.73it/s]209069it [01:27, 3546.16it/s]210760it [01:27, 3483.52it/s]202717it [01:26, 3287.34it/s]209426it [01:27, 3420.53it/s]211110it [01:27, 3357.52it/s]203094it [01:27, 3421.15it/s]209784it [01:28, 3465.82it/s]211476it [01:27, 3438.83it/s]203450it [01:27, 3334.39it/s]210133it [01:28, 3356.90it/s]211822it [01:28, 3308.01it/s]203817it [01:27, 3428.85it/s]210496it [01:28, 3434.05it/s]212186it [01:28, 3400.51it/s]204168it [01:27, 3341.82it/s]210841it [01:28, 3334.16it/s]212528it [01:28, 3301.00it/s]204538it [01:27, 3442.85it/s]211199it [01:28, 3402.30it/s]212894it [01:28, 3403.54it/s]211570it [01:28, 3489.81it/s]204896it [01:27, 3347.35it/s]196630it [01:27, 152.39it/s] 213261it [01:28, 3478.30it/s]205269it [01:27, 3455.12it/s]211921it [01:28, 3368.59it/s]197011it [01:28, 218.76it/s]213611it [01:28, 3348.24it/s]205638it [01:27, 3521.36it/s]212290it [01:28, 3460.56it/s]197325it [01:28, 292.56it/s]213967it [01:28, 3408.05it/s]205993it [01:27, 3377.52it/s]212638it [01:28, 3342.86it/s]197691it [01:28, 409.87it/s]214310it [01:28, 3295.06it/s]206366it [01:27, 3476.15it/s]213008it [01:29, 3444.48it/s]198078it [01:28, 573.84it/s]214674it [01:28, 3393.42it/s]206716it [01:28, 3377.15it/s]213355it [01:29, 3330.17it/s]198415it [01:28, 746.26it/s]215015it [01:29, 3287.20it/s]207089it [01:28, 3476.23it/s]213717it [01:29, 3411.62it/s]198792it [01:28, 995.79it/s]215387it [01:29, 3409.64it/s]214084it [01:29, 3485.68it/s]207439it [01:28, 3374.67it/s]199134it [01:28, 1233.14it/s]215750it [01:29, 3472.17it/s]207810it [01:28, 3470.43it/s]214434it [01:29, 3349.57it/s]199510it [01:28, 1559.38it/s]216099it [01:29, 3333.94it/s]208188it [01:28, 3559.05it/s]214802it [01:29, 3441.65it/s]199853it [01:28, 1820.47it/s]216470it [01:29, 3440.02it/s]208546it [01:28, 3452.64it/s]215148it [01:29, 3326.53it/s]200186it [01:28, 2052.56it/s]216816it [01:29, 3350.94it/s]208921it [01:28, 3538.22it/s]215518it [01:29, 3432.06it/s]200552it [01:29, 2374.91it/s]217192it [01:29, 3467.17it/s]209277it [01:28, 3428.46it/s]215863it [01:29, 3326.55it/s]200888it [01:29, 2539.00it/s]217541it [01:29, 3364.60it/s]209642it [01:28, 3491.18it/s]216235it [01:29, 3437.46it/s]201256it [01:29, 2804.04it/s]217926it [01:29, 3502.64it/s]209993it [01:29, 3372.91it/s]216597it [01:30, 3488.20it/s]201595it [01:29, 2892.25it/s]218302it [01:29, 3574.89it/s]210358it [01:29, 3452.09it/s]216948it [01:30, 3409.23it/s]201967it [01:29, 3106.85it/s]218661it [01:30, 3433.28it/s]210724it [01:29, 3510.75it/s]217312it [01:30, 3475.35it/s]202310it [01:29, 3127.60it/s]219036it [01:30, 3519.12it/s]211077it [01:29, 3337.18it/s]217661it [01:30, 3407.93it/s]202676it [01:29, 3272.50it/s]219390it [01:30, 3429.37it/s]211435it [01:29, 3405.85it/s]218042it [01:30, 3523.38it/s]203062it [01:29, 3436.23it/s]219774it [01:30, 3545.71it/s]218396it [01:30, 3423.33it/s]211778it [01:29, 3288.50it/s]203419it [01:29, 3341.60it/s]220131it [01:30, 3441.08it/s]218779it [01:30, 3539.83it/s]212145it [01:29, 3396.72it/s]203776it [01:30, 3400.11it/s]220523it [01:30, 3577.82it/s]219158it [01:30, 3612.80it/s]212487it [01:29, 3312.10it/s]204123it [01:30, 3317.08it/s]220883it [01:30, 3431.88it/s]219521it [01:30, 3496.23it/s]212858it [01:29, 3424.39it/s]204484it [01:30, 3398.65it/s]221260it [01:30, 3526.22it/s]219905it [01:30, 3593.40it/s]213226it [01:29, 3491.64it/s]204828it [01:30, 3311.23it/s]221655it [01:30, 3647.14it/s]220266it [01:31, 3479.27it/s]213577it [01:30, 3371.02it/s]205193it [01:30, 3405.60it/s]222022it [01:31, 3497.26it/s]220655it [01:31, 3596.88it/s]213949it [01:30, 3470.13it/s]205566it [01:30, 3497.42it/s]222389it [01:31, 3544.05it/s]214298it [01:30, 3361.03it/s]221017it [01:31, 3462.52it/s]205918it [01:30, 3357.28it/s]222746it [01:31, 3428.82it/s]214663it [01:30, 3441.47it/s]221403it [01:31, 3574.06it/s]206295it [01:30, 3474.04it/s]223114it [01:31, 3499.74it/s]221763it [01:31, 3490.99it/s]215009it [01:30, 3337.78it/s]206645it [01:30, 3347.57it/s]223466it [01:31, 3335.04it/s]222135it [01:31, 3554.82it/s]215380it [01:30, 3443.82it/s]207016it [01:30, 3450.08it/s]223838it [01:31, 3443.19it/s]222498it [01:31, 3574.83it/s]215746it [01:30, 3500.28it/s]207364it [01:31, 3333.63it/s]224214it [01:31, 3531.57it/s]222857it [01:31, 3456.36it/s]216098it [01:30, 3399.20it/s]207736it [01:31, 3443.52it/s]224570it [01:31, 3392.98it/s]223219it [01:31, 3501.83it/s]216452it [01:30, 3438.08it/s]208105it [01:31, 3513.35it/s]224939it [01:31, 3477.21it/s]216797it [01:31, 3355.38it/s]223571it [01:32, 3392.51it/s]208459it [01:31, 3428.04it/s]225289it [01:32, 3360.43it/s]217179it [01:31, 3487.94it/s]223935it [01:32, 3463.42it/s]208822it [01:31, 3485.77it/s]225646it [01:32, 3418.34it/s]217529it [01:31, 3394.93it/s]224283it [01:32, 3379.90it/s]209172it [01:31, 3395.49it/s]225990it [01:32, 3301.53it/s]217916it [01:31, 3528.50it/s]224640it [01:32, 3434.55it/s]209528it [01:31, 3440.98it/s]226353it [01:32, 3394.54it/s]218293it [01:31, 3597.73it/s]225012it [01:32, 3517.41it/s]209874it [01:31, 3332.63it/s]226718it [01:32, 3467.31it/s]218654it [01:31, 3493.96it/s]225365it [01:32, 3375.61it/s]210229it [01:31, 3394.15it/s]227067it [01:32, 3320.81it/s]219032it [01:31, 3576.71it/s]225737it [01:32, 3472.53it/s]210601it [01:31, 3488.39it/s]227428it [01:32, 3402.56it/s]219391it [01:31, 3479.45it/s]226086it [01:32, 3332.23it/s]210951it [01:32, 3343.21it/s]227771it [01:32, 3295.35it/s]219774it [01:31, 3580.64it/s]226453it [01:32, 3426.23it/s]211322it [01:32, 3447.16it/s]228119it [01:32, 3347.32it/s]220134it [01:31, 3481.14it/s]226798it [01:33, 3286.74it/s]211669it [01:32, 3316.54it/s]228456it [01:32, 3234.80it/s]220525it [01:32, 3602.75it/s]227162it [01:33, 3386.66it/s]212039it [01:32, 3423.64it/s]228822it [01:33, 3353.98it/s]220887it [01:32, 3490.26it/s]227517it [01:33, 3432.64it/s]212386it [01:32, 3292.94it/s]229186it [01:33, 3434.64it/s]221263it [01:32, 3565.63it/s]227862it [01:33, 3330.41it/s]212759it [01:32, 3414.33it/s]229531it [01:33, 3343.55it/s]221637it [01:32, 3616.25it/s]228220it [01:33, 3401.16it/s]213117it [01:32, 3460.74it/s]229896it [01:33, 3431.17it/s]222000it [01:32, 3495.32it/s]228562it [01:33, 3304.47it/s]213465it [01:32, 3341.81it/s]230241it [01:33, 3285.15it/s]222366it [01:32, 3541.65it/s]228920it [01:33, 3382.04it/s]213824it [01:32, 3411.62it/s]230604it [01:33, 3381.93it/s]222722it [01:32, 3429.31it/s]229260it [01:33, 3293.69it/s]214167it [01:33, 3310.56it/s]230945it [01:33, 3287.23it/s]223090it [01:32, 3500.82it/s]229629it [01:33, 3405.44it/s]214519it [01:33, 3368.37it/s]231320it [01:33, 3418.20it/s]229997it [01:33, 3483.40it/s]223442it [01:32, 3383.41it/s]214886it [01:33, 3454.66it/s]231686it [01:33, 3487.91it/s]223808it [01:33, 3461.18it/s]230347it [01:34, 3340.43it/s]215233it [01:33, 3322.18it/s]232037it [01:34, 3357.20it/s]224185it [01:33, 3549.11it/s]230713it [01:34, 3430.77it/s]215606it [01:33, 3429.71it/s]232404it [01:34, 3446.16it/s]224542it [01:33, 3416.07it/s]231058it [01:34, 3330.75it/s]215951it [01:33, 3301.64it/s]232751it [01:34, 3302.75it/s]224908it [01:33, 3485.25it/s]231427it [01:34, 3430.84it/s]216324it [01:33, 3422.44it/s]233118it [01:34, 3405.95it/s]225259it [01:33, 3370.64it/s]231777it [01:34, 3315.68it/s]216669it [01:33, 3306.33it/s]233461it [01:34, 3310.59it/s]225598it [01:33, 3366.52it/s]232141it [01:34, 3405.45it/s]217053it [01:33, 3456.13it/s]233829it [01:34, 3415.64it/s]232496it [01:34, 3445.66it/s]225936it [01:33, 3223.22it/s]217411it [01:34, 3489.85it/s]234198it [01:34, 3493.96it/s]226266it [01:33, 3242.88it/s]232842it [01:34, 3335.30it/s]217762it [01:34, 3422.41it/s]234549it [01:34, 3358.63it/s]226633it [01:33, 3365.22it/s]233207it [01:34, 3423.50it/s]218128it [01:34, 3490.14it/s]234904it [01:34, 3411.21it/s]226971it [01:33, 3218.14it/s]233551it [01:35, 3329.16it/s]218479it [01:34, 3387.33it/s]235247it [01:34, 3307.05it/s]227335it [01:34, 3337.67it/s]233914it [01:35, 3413.25it/s]218854it [01:34, 3490.76it/s]235610it [01:35, 3398.41it/s]234287it [01:35, 3503.36it/s]227671it [01:34, 3249.09it/s]219205it [01:34, 3401.25it/s]235976it [01:35, 3471.95it/s]228036it [01:34, 3362.86it/s]234639it [01:35, 3344.31it/s]219576it [01:34, 3489.34it/s]236325it [01:35, 3346.02it/s]228403it [01:34, 3451.84it/s]235006it [01:35, 3436.21it/s]219946it [01:34, 3373.20it/s]236686it [01:35, 3420.33it/s]228750it [01:34, 3336.74it/s]235352it [01:35, 3314.83it/s]220339it [01:34, 3531.02it/s]237030it [01:35, 3314.94it/s]229116it [01:34, 3421.43it/s]235716it [01:35, 3406.80it/s]220723it [01:34, 3619.49it/s]237384it [01:35, 3377.91it/s]229460it [01:34, 3336.73it/s]236059it [01:35, 3290.62it/s]221087it [01:35, 3390.22it/s]237724it [01:35, 3276.23it/s]229829it [01:34, 3435.93it/s]236425it [01:35, 3394.40it/s]221468it [01:35, 3507.24it/s]238087it [01:35, 3377.04it/s]230174it [01:34, 3318.36it/s]236783it [01:35, 3446.10it/s]221823it [01:35, 3440.48it/s]238447it [01:35, 3440.34it/s]230537it [01:35, 3405.99it/s]237130it [01:36, 3344.44it/s]222181it [01:35, 3479.10it/s]238793it [01:36, 3314.73it/s]230905it [01:35, 3485.10it/s]237488it [01:36, 3410.38it/s]222531it [01:35, 3370.40it/s]239154it [01:36, 3399.26it/s]231255it [01:35, 3377.87it/s]237831it [01:36, 3308.70it/s]222886it [01:35, 3420.95it/s]239496it [01:36, 3253.74it/s]231599it [01:35, 3394.43it/s]238187it [01:36, 3379.13it/s]223256it [01:35, 3500.66it/s]239862it [01:36, 3368.10it/s]231940it [01:35, 3295.87it/s]238527it [01:36, 3285.93it/s]223608it [01:35, 3352.54it/s]240201it [01:36, 3264.91it/s]232305it [01:35, 3397.67it/s]238883it [01:36, 3363.09it/s]223985it [01:35, 3469.35it/s]240564it [01:36, 3368.62it/s]232647it [01:35, 3297.94it/s]239248it [01:36, 3445.29it/s]224334it [01:36, 3341.73it/s]240929it [01:36, 3448.24it/s]233018it [01:35, 3414.71it/s]239594it [01:36, 3313.78it/s]224702it [01:36, 3437.75it/s]241276it [01:36, 3336.42it/s]233389it [01:35, 3499.48it/s]239963it [01:36, 3419.64it/s]225048it [01:36, 3309.52it/s]241644it [01:36, 3434.35it/s]233741it [01:35, 3378.54it/s]240307it [01:37, 3294.29it/s]225414it [01:36, 3407.00it/s]234112it [01:36, 3471.77it/s]241990it [01:36, 3309.11it/s]240670it [01:37, 3388.25it/s]225773it [01:36, 3459.13it/s]242353it [01:37, 3400.32it/s]234461it [01:36, 3350.67it/s]241017it [01:37, 3273.31it/s]226121it [01:36, 3334.88it/s]242697it [01:37, 3292.58it/s]234827it [01:36, 3438.67it/s]241389it [01:37, 3399.38it/s]226478it [01:36, 3400.83it/s]243064it [01:37, 3398.27it/s]235173it [01:36, 3328.54it/s]241748it [01:37, 3454.02it/s]226820it [01:36, 3270.65it/s]243431it [01:37, 3475.96it/s]235536it [01:36, 3414.33it/s]242096it [01:37, 3352.79it/s]227176it [01:36, 3351.87it/s]235902it [01:36, 3484.84it/s]243781it [01:37, 3351.01it/s]242453it [01:37, 3414.93it/s]227513it [01:36, 3249.44it/s]244137it [01:37, 3408.49it/s]236252it [01:36, 3354.19it/s]242796it [01:37, 3317.19it/s]227869it [01:37, 3337.41it/s]236603it [01:36, 3396.25it/s]244480it [01:37, 3294.77it/s]243153it [01:37, 3389.40it/s]228233it [01:37, 3423.47it/s]244844it [01:37, 3391.69it/s]236945it [01:36, 3299.60it/s]243522it [01:37, 3475.35it/s]228577it [01:37, 3285.29it/s]245207it [01:37, 3459.60it/s]237306it [01:37, 3384.96it/s]243871it [01:38, 3336.41it/s]228944it [01:37, 3393.17it/s]245555it [01:38, 3335.19it/s]237656it [01:37, 3279.62it/s]244237it [01:38, 3419.01it/s]229286it [01:37, 3264.27it/s]245917it [01:38, 3410.94it/s]238023it [01:37, 3389.15it/s]244581it [01:38, 3276.73it/s]229663it [01:37, 3406.45it/s]238383it [01:37, 3449.74it/s]246260it [01:38, 3286.81it/s]244947it [01:38, 3383.62it/s]230019it [01:37, 3448.84it/s]246623it [01:38, 3382.35it/s]238730it [01:37, 3325.00it/s]245288it [01:38, 3292.34it/s]230366it [01:37, 3326.65it/s]246963it [01:38, 3285.29it/s]239091it [01:37, 3405.86it/s]245648it [01:38, 3379.79it/s]230718it [01:37, 3380.30it/s]247325it [01:38, 3380.44it/s]239434it [01:37, 3288.23it/s]245988it [01:38, 3349.86it/s]231058it [01:38, 3298.43it/s]247691it [01:38, 3460.66it/s]239802it [01:37, 3397.51it/s]246325it [01:38, 3193.75it/s]231416it [01:38, 3379.41it/s]248039it [01:38, 3338.29it/s]240160it [01:37, 3449.85it/s]246687it [01:38, 3313.67it/s]231756it [01:38, 3286.16it/s]248404it [01:38, 3425.21it/s]240507it [01:37, 3272.18it/s]247021it [01:39, 3173.13it/s]232114it [01:38, 3369.79it/s]248749it [01:38, 3292.45it/s]240872it [01:38, 3377.74it/s]247388it [01:39, 3311.53it/s]232480it [01:38, 3452.86it/s]249120it [01:39, 3410.22it/s]241213it [01:38, 3291.29it/s]247737it [01:39, 3206.90it/s]232827it [01:38, 3304.95it/s]249463it [01:39, 3309.04it/s]241577it [01:38, 3389.63it/s]248105it [01:39, 3339.88it/s]233200it [01:38, 3424.50it/s]249827it [01:39, 3401.97it/s]241918it [01:38, 3316.79it/s]248462it [01:39, 3405.47it/s]233545it [01:38, 3303.12it/s]250193it [01:39, 3475.97it/s]242280it [01:38, 3402.59it/s]248805it [01:39, 3170.89it/s]233915it [01:38, 3415.79it/s]250543it [01:39, 3368.66it/s]242644it [01:38, 3470.79it/s]249178it [01:39, 3325.22it/s]234259it [01:38, 3300.60it/s]250904it [01:39, 3436.88it/s]242993it [01:38, 3351.39it/s]249515it [01:39, 3252.38it/s]234621it [01:39, 3391.03it/s]251250it [01:39, 3328.36it/s]243361it [01:38, 3443.64it/s]249881it [01:39, 3367.50it/s]234981it [01:39, 3449.98it/s]251614it [01:39, 3417.39it/s]243707it [01:38, 3340.26it/s]250242it [01:39, 3436.65it/s]235328it [01:39, 3331.89it/s]251958it [01:39, 3300.74it/s]244072it [01:39, 3427.95it/s]250588it [01:40, 3318.82it/s]235682it [01:39, 3390.12it/s]252330it [01:40, 3419.15it/s]244417it [01:39, 3317.39it/s]250951it [01:40, 3406.94it/s]236023it [01:39, 3289.24it/s]252694it [01:40, 3482.32it/s]244782it [01:39, 3411.43it/s]251294it [01:40, 3285.82it/s]236377it [01:39, 3359.92it/s]253044it [01:40, 3363.05it/s]245146it [01:39, 3467.22it/s]251650it [01:40, 3363.74it/s]236742it [01:39, 3442.08it/s]253401it [01:40, 3420.21it/s]245494it [01:39, 3355.28it/s]251989it [01:40, 3248.66it/s]237088it [01:39, 3300.20it/s]245842it [01:39, 3389.22it/s]252351it [01:40, 3353.35it/s]237449it [01:39, 3388.31it/s]246183it [01:39, 3294.16it/s]252714it [01:40, 3401.33it/s]237790it [01:40, 3263.60it/s]246546it [01:39, 3384.66it/s]253056it [01:40, 3294.30it/s]238153it [01:40, 3366.65it/s]246896it [01:39, 3298.41it/s]253422it [01:40, 3397.89it/s]238492it [01:40, 3240.79it/s]247261it [01:39, 3397.78it/s]238853it [01:40, 3344.06it/s]247623it [01:40, 3461.60it/s]239206it [01:40, 3397.23it/s]247971it [01:40, 3345.21it/s]239548it [01:40, 3285.73it/s]248334it [01:40, 3426.31it/s]239906it [01:40, 3368.74it/s]248678it [01:40, 3317.37it/s]240245it [01:40, 3270.19it/s]249044it [01:40, 3413.19it/s]240599it [01:40, 3346.46it/s]249407it [01:40, 3475.55it/s]240946it [01:40, 3254.44it/s]249756it [01:40, 3352.52it/s]241308it [01:41, 3357.99it/s]250120it [01:40, 3433.58it/s]241677it [01:41, 3452.34it/s]250465it [01:40, 3348.53it/s]242024it [01:41, 3310.09it/s]250802it [01:41, 3337.78it/s]242394it [01:41, 3419.47it/s]251137it [01:41, 3244.61it/s]242738it [01:41, 3277.75it/s]251501it [01:41, 3356.12it/s]243094it [01:41, 3356.92it/s]251863it [01:41, 3432.79it/s]243465it [01:41, 3456.39it/s]252208it [01:41, 3326.24it/s]243813it [01:41, 3304.72it/s]252566it [01:41, 3394.74it/s]244177it [01:41, 3399.27it/s]252907it [01:41, 3293.48it/s]244520it [01:42, 3255.32it/s]253267it [01:41, 3380.81it/s]244887it [01:42, 3372.15it/s]253616it [01:41, 3280.30it/s]245227it [01:42, 3240.85it/s]245596it [01:42, 3367.62it/s]245950it [01:42, 3414.82it/s]246294it [01:42, 3301.47it/s]246650it [01:42, 3375.20it/s]246990it [01:42, 3276.60it/s]247346it [01:42, 3355.64it/s]247684it [01:43, 3248.76it/s]248042it [01:43, 3341.22it/s]248410it [01:43, 3437.02it/s]248756it [01:43, 3288.70it/s]249129it [01:43, 3413.58it/s]249473it [01:43, 3278.16it/s]249836it [01:43, 3371.14it/s]250186it [01:43, 3241.16it/s]250559it [01:43, 3377.43it/s]250920it [01:43, 3442.09it/s]251267it [01:44, 3314.03it/s]251621it [01:44, 3378.12it/s]251961it [01:44, 3256.87it/s]252324it [01:44, 3360.61it/s]252687it [01:44, 3437.56it/s]253033it [01:44, 3289.14it/s]253400it [01:44, 3396.14it/s]253745it [01:48, 141.45it/s] 254104it [01:48, 199.63it/s]254467it [01:48, 280.34it/s]254770it [01:48, 369.49it/s]255139it [01:48, 516.79it/s]255460it [01:48, 674.25it/s]255832it [01:49, 912.20it/s]256200it [01:49, 1189.82it/s]256543it [01:49, 1443.54it/s]256910it [01:49, 1776.17it/s]257251it [01:49, 2020.62it/s]257613it [01:49, 2336.51it/s]257953it [01:49, 2508.35it/s]258319it [01:49, 2778.69it/s]258678it [01:49, 2981.23it/s]259024it [01:49, 3011.01it/s]259392it [01:50, 3189.30it/s]259737it [01:50, 3150.39it/s]260106it [01:50, 3299.27it/s]260450it [01:50, 3213.75it/s]260821it [01:50, 3351.71it/s]261180it [01:50, 3417.67it/s]261528it [01:50, 3320.80it/s]261901it [01:50, 3437.45it/s]262249it [01:50, 3320.31it/s]262617it [01:51, 3421.55it/s]262962it [01:51, 3308.33it/s]263322it [01:51, 3390.71it/s]263695it [01:51, 3488.01it/s]264046it [01:51, 3354.70it/s]264423it [01:51, 3473.20it/s]264773it [01:51, 3352.62it/s]265146it [01:51, 3459.58it/s]265494it [01:51, 3332.28it/s]265866it [01:51, 3438.64it/s]253422it [01:52, 3397.89it/s]253687it [01:52, 95.46it/s]  266236it [01:52, 3513.71it/s]254057it [01:52, 139.76it/s]266590it [01:52, 3386.17it/s]254415it [01:52, 199.11it/s]266960it [01:52, 3475.46it/s]254767it [01:52, 278.02it/s]253946it [01:51, 116.61it/s] 267310it [01:52, 3363.31it/s]255106it [01:52, 379.87it/s]254299it [01:51, 165.33it/s]267682it [01:52, 3464.08it/s]255436it [01:52, 509.27it/s]254593it [01:51, 220.77it/s]268031it [01:52, 3337.19it/s]255808it [01:52, 702.56it/s]254949it [01:51, 313.43it/s]268402it [01:52, 3440.95it/s]256174it [01:52, 936.76it/s]255309it [01:51, 438.75it/s]268773it [01:52, 3518.10it/s]255627it [01:51, 579.06it/s]256520it [01:53, 1178.32it/s]269127it [01:52, 3379.90it/s]255989it [01:52, 787.51it/s]256889it [01:53, 1492.98it/s]269495it [01:53, 3464.82it/s]256317it [01:52, 1003.48it/s]257234it [01:53, 1754.47it/s]269844it [01:53, 3357.75it/s]256684it [01:52, 1303.00it/s]257582it [01:53, 2056.10it/s]270205it [01:53, 3428.91it/s]257045it [01:52, 1584.85it/s]257919it [01:53, 2269.21it/s]270550it [01:53, 3315.95it/s]257404it [01:52, 1908.77it/s]258288it [01:53, 2578.94it/s]270915it [01:53, 3410.18it/s]257769it [01:52, 2234.65it/s]258645it [01:53, 2813.36it/s]271281it [01:53, 3481.89it/s]258114it [01:52, 2432.41it/s]258990it [01:53, 2892.20it/s]271631it [01:53, 3344.86it/s]258482it [01:52, 2715.96it/s]259342it [01:53, 3053.99it/s]272000it [01:53, 3442.65it/s]258827it [01:52, 2826.35it/s]259682it [01:53, 3057.84it/s]272347it [01:53, 3315.86it/s]259197it [01:53, 3047.84it/s]260040it [01:54, 3200.36it/s]272701it [01:53, 3378.03it/s]259565it [01:53, 3069.67it/s]260406it [01:54, 3159.78it/s]273041it [01:54, 3276.85it/s]259937it [01:53, 3242.64it/s]260765it [01:54, 3276.63it/s]273407it [01:54, 3383.75it/s]260307it [01:53, 3368.73it/s]261136it [01:54, 3397.33it/s]273776it [01:54, 3468.48it/s]260661it [01:53, 3287.63it/s]261484it [01:54, 3277.66it/s]274125it [01:54, 3354.38it/s]261035it [01:53, 3405.12it/s]261857it [01:54, 3404.26it/s]274488it [01:54, 3430.99it/s]261385it [01:53, 3323.22it/s]262203it [01:54, 3282.58it/s]274833it [01:54, 3289.16it/s]261758it [01:53, 3437.90it/s]262574it [01:54, 3402.28it/s]275202it [01:54, 3401.81it/s]262108it [01:53, 3336.55it/s]262926it [01:54, 3289.75it/s]275545it [01:54, 3307.21it/s]262479it [01:53, 3440.87it/s]263296it [01:55, 3403.49it/s]275913it [01:54, 3413.54it/s]262851it [01:54, 3519.33it/s]263659it [01:55, 3466.65it/s]253742it [01:54, 117.72it/s] 276279it [01:55, 3484.09it/s]263206it [01:54, 3388.79it/s]264008it [01:55, 3358.83it/s]254107it [01:54, 167.49it/s]276629it [01:55, 3355.48it/s]263575it [01:54, 3473.38it/s]264372it [01:55, 3437.28it/s]276996it [01:55, 3435.73it/s]254454it [01:54, 231.98it/s]263925it [01:54, 3305.58it/s]264718it [01:55, 3332.86it/s]254819it [01:54, 325.41it/s]277342it [01:55, 3309.89it/s]264300it [01:54, 3430.16it/s]265079it [01:55, 3410.12it/s]255187it [01:54, 451.46it/s]277706it [01:55, 3401.81it/s]264646it [01:54, 3334.84it/s]265446it [01:55, 3322.82it/s]255514it [01:54, 592.87it/s]278048it [01:55, 3300.64it/s]265018it [01:54, 3443.92it/s]265810it [01:55, 3410.90it/s]255874it [01:55, 796.09it/s]278413it [01:55, 3399.25it/s]265388it [01:54, 3516.80it/s]266181it [01:55, 3495.59it/s]256206it [01:55, 1010.91it/s]278781it [01:55, 3480.51it/s]265742it [01:54, 3403.83it/s]266533it [01:55, 3356.73it/s]256576it [01:55, 1309.28it/s]279131it [01:55, 3359.34it/s]266114it [01:55, 3492.14it/s]266907it [01:56, 3463.39it/s]256943it [01:55, 1632.37it/s]279487it [01:55, 3415.87it/s]266465it [01:55, 3380.60it/s]267256it [01:56, 3327.96it/s]257290it [01:55, 1884.69it/s]279831it [01:56, 3305.90it/s]266840it [01:55, 3484.25it/s]267628it [01:56, 3437.36it/s]257656it [01:55, 2213.16it/s]280194it [01:56, 3397.39it/s]267191it [01:55, 3375.74it/s]267974it [01:56, 3323.78it/s]257999it [01:55, 2412.41it/s]280560it [01:56, 3472.40it/s]267562it [01:55, 3470.83it/s]268348it [01:56, 3441.99it/s]258364it [01:55, 2692.43it/s]280909it [01:56, 3336.56it/s]267935it [01:55, 3545.65it/s]268707it [01:56, 3482.39it/s]258707it [01:55, 2773.30it/s]281268it [01:56, 3407.76it/s]268291it [01:55, 3427.17it/s]269057it [01:56, 3360.90it/s]259076it [01:56, 3003.64it/s]281611it [01:56, 3308.12it/s]268665it [01:55, 3508.25it/s]269417it [01:56, 3426.97it/s]259442it [01:56, 3177.30it/s]281966it [01:56, 3368.12it/s]269018it [01:55, 3386.24it/s]269762it [01:56, 3330.60it/s]259791it [01:56, 3130.15it/s]282305it [01:56, 3279.56it/s]269387it [01:56, 3472.58it/s]270128it [01:57, 3422.90it/s]260161it [01:56, 3283.49it/s]282668it [01:56, 3378.20it/s]269736it [01:56, 3368.37it/s]270486it [01:57, 3322.01it/s]260506it [01:56, 3219.08it/s]283031it [01:57, 3450.94it/s]270114it [01:56, 3484.14it/s]270844it [01:57, 3394.86it/s]260877it [01:56, 3355.69it/s]283378it [01:57, 3326.82it/s]270483it [01:56, 3542.55it/s]271212it [01:57, 3476.60it/s]261222it [01:56, 3255.16it/s]283743it [01:57, 3418.81it/s]270839it [01:56, 3403.31it/s]271562it [01:57, 3332.64it/s]261592it [01:56, 3379.82it/s]284087it [01:57, 3282.36it/s]271206it [01:56, 3478.77it/s]271935it [01:57, 3444.29it/s]261961it [01:56, 3468.02it/s]284452it [01:57, 3384.60it/s]271556it [01:56, 3362.25it/s]272282it [01:57, 3302.89it/s]262313it [01:56, 3349.86it/s]284793it [01:57, 3284.04it/s]271928it [01:56, 3464.01it/s]272649it [01:57, 3405.53it/s]262669it [01:57, 3407.81it/s]285159it [01:57, 3390.32it/s]272277it [01:56, 3345.88it/s]273006it [01:57, 3279.10it/s]263013it [01:57, 3310.13it/s]285526it [01:57, 3469.21it/s]272643it [01:56, 3435.34it/s]273361it [01:58, 3353.55it/s]263380it [01:57, 3411.78it/s]285875it [01:57, 3345.56it/s]273005it [01:57, 3326.73it/s]273733it [01:58, 3457.87it/s]263724it [01:57, 3316.15it/s]286238it [01:57, 3424.79it/s]273371it [01:57, 3418.95it/s]274081it [01:58, 3324.29it/s]264082it [01:57, 3389.68it/s]286583it [01:58, 3301.12it/s]273722it [01:57, 3443.89it/s]274450it [01:58, 3426.27it/s]264456it [01:57, 3489.93it/s]286950it [01:58, 3405.92it/s]287112it [01:58, 2427.92it/s]
274068it [01:57, 3345.88it/s]274795it [01:58, 3288.29it/s]264807it [01:57, 3364.42it/s]274435it [01:57, 3436.64it/s]275167it [01:58, 3409.23it/s]265177it [01:57, 3459.68it/s]274781it [01:57, 3327.33it/s]265525it [01:57, 3332.43it/s]275526it [01:58, 3294.36it/s]275151it [01:57, 3433.10it/s]265894it [01:58, 3433.58it/s]275896it [01:58, 3408.04it/s]275522it [01:57, 3511.92it/s]276252it [01:58, 3451.16it/s]266240it [01:58, 3321.14it/s]275875it [01:57, 3377.90it/s]266615it [01:58, 3441.54it/s]276600it [01:58, 3276.27it/s]276243it [01:58, 3464.05it/s]266974it [01:58, 3481.76it/s]276962it [01:59, 3372.44it/s]276592it [01:58, 3346.38it/s]267324it [01:58, 3376.57it/s]277302it [01:59, 3291.35it/s]276961it [01:58, 3442.53it/s]267695it [01:58, 3470.40it/s]277662it [01:59, 3378.23it/s]277307it [01:58, 3333.96it/s]278032it [01:59, 3469.89it/s]268044it [01:58, 3377.30it/s]277674it [01:58, 3429.06it/s]268406it [01:58, 3447.08it/s]278381it [01:59, 3317.43it/s]278042it [01:58, 3501.47it/s]278746it [01:59, 3407.43it/s]268752it [01:58, 3321.19it/s]278394it [01:58, 3366.65it/s]269122it [01:58, 3428.41it/s]279089it [01:59, 3295.08it/s]278763it [01:58, 3458.33it/s]269478it [01:59, 3465.81it/s]279445it [01:59, 3369.16it/s]279111it [01:58, 3345.64it/s]269826it [01:59, 3325.33it/s]279784it [01:59, 3260.06it/s]279480it [01:58, 3441.36it/s]270199it [01:59, 3438.60it/s]280146it [02:00, 3360.90it/s]279826it [01:59, 3328.28it/s]280506it [02:00, 3429.97it/s]270545it [01:59, 3329.11it/s]280191it [01:59, 3417.74it/s]270901it [01:59, 3394.79it/s]280851it [02:00, 3332.53it/s]280559it [01:59, 3492.57it/s]281206it [02:00, 3392.83it/s]271255it [01:59, 3292.03it/s]280910it [01:59, 3376.90it/s]271624it [01:59, 3403.48it/s]281547it [02:00, 3298.79it/s]281265it [01:59, 3419.17it/s]271969it [01:59, 3416.31it/s]281904it [02:00, 3375.51it/s]281609it [01:59, 3315.23it/s]282246it [02:00, 3275.75it/s]272312it [01:59, 3127.43it/s]281968it [01:59, 3393.19it/s]282601it [02:00, 3352.43it/s]272676it [02:00, 3268.84it/s]282309it [01:59, 3283.47it/s]282963it [02:00, 3429.36it/s]273008it [02:00, 3216.37it/s]282671it [01:59, 3379.27it/s]283308it [02:00, 3289.41it/s]273375it [02:00, 3342.49it/s]283036it [02:00, 3455.87it/s]283673it [02:01, 3390.43it/s]273736it [02:00, 3417.86it/s]283383it [02:00, 3287.18it/s]284014it [02:01, 3261.55it/s]274081it [02:00, 3335.19it/s]283750it [02:00, 3394.07it/s]284382it [02:01, 3380.46it/s]274449it [02:00, 3432.15it/s]284092it [02:00, 3294.24it/s]284739it [02:01, 3432.92it/s]274794it [02:00, 3333.67it/s]284459it [02:00, 3400.52it/s]285084it [02:01, 3324.19it/s]275154it [02:00, 3409.36it/s]284801it [02:00, 3299.12it/s]285444it [02:01, 3403.34it/s]275497it [02:00, 3327.42it/s]285168it [02:00, 3404.61it/s]285786it [02:01, 3296.34it/s]275866it [02:00, 3431.49it/s]285535it [02:00, 3474.98it/s]286144it [02:01, 3376.98it/s]276232it [02:01, 3497.74it/s]285884it [02:00, 3335.29it/s]286484it [02:01, 3287.43it/s]276583it [02:01, 3351.64it/s]286249it [02:00, 3424.09it/s]286845it [02:02, 3379.47it/s]276952it [02:01, 3447.57it/s]287112it [02:02, 2351.63it/s]
286594it [02:01, 3326.73it/s]277299it [02:01, 3353.49it/s]286965it [02:01, 3434.66it/s]277669it [02:01, 3451.84it/s]287112it [02:01, 2368.54it/s]
278016it [02:01, 3329.36it/s]278384it [02:01, 3429.09it/s]278756it [02:01, 3512.20it/s]279109it [02:01, 3405.74it/s]279470it [02:02, 3463.91it/s]279818it [02:02, 3357.34it/s]280185it [02:02, 3445.29it/s]280531it [02:02, 3324.44it/s]280907it [02:02, 3446.77it/s]281273it [02:02, 3507.41it/s]281626it [02:02, 3395.77it/s]281985it [02:02, 3450.64it/s]282332it [02:02, 3355.04it/s]282700it [02:02, 3447.97it/s]283047it [02:03, 3344.47it/s]283401it [02:03, 3399.12it/s]283769it [02:03, 3480.54it/s]284119it [02:03, 3369.88it/s]284485it [02:03, 3447.41it/s]284831it [02:03, 3268.10it/s]285184it [02:03, 3341.49it/s]285535it [02:03, 3259.86it/s]285903it [02:03, 3376.47it/s]286261it [02:04, 3432.92it/s]286606it [02:04, 3269.94it/s]286971it [02:04, 3376.14it/s]287112it [02:04, 2309.71it/s]
2022-07-26 15:06:24 | INFO | root | success load 287112 data
2022-07-26 15:06:24 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-26 15:06:24 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-26 15:06:24 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-26 15:06:24 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-26 15:06:24 | INFO | transformer.tokenization_utils | loading file None
2022-07-26 15:06:24 | INFO | transformer.tokenization_utils | loading file None
2022-07-26 15:06:24 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-26 15:10:53 | INFO | train_inner | epoch 016:     90 / 1122 loss=nan, nll_loss=3.556, mask_ins=1.663, word_ins_ml=5.079, word_reposition=1.904, kpe=nan, ppl=nan, wps=4025.2, ups=0.2, wpb=20272.3, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=2.03, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-26 15:15:50 | INFO | train_inner | epoch 016:    190 / 1122 loss=9.445, nll_loss=3.543, mask_ins=1.645, word_ins_ml=5.067, word_reposition=1.898, kpe=0.835, ppl=696.78, wps=6905.8, ups=0.34, wpb=20493.9, bsz=256, num_updates=17000, lr=0.000271163, gnorm=1.912, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-26 15:20:44 | INFO | train_inner | epoch 016:    290 / 1122 loss=9.382, nll_loss=3.479, mask_ins=1.651, word_ins_ml=5.012, word_reposition=1.882, kpe=0.838, ppl=667.21, wps=7030.1, ups=0.34, wpb=20656.5, bsz=256, num_updates=17100, lr=0.000270369, gnorm=1.954, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-26 15:25:38 | INFO | train_inner | epoch 016:    390 / 1122 loss=9.338, nll_loss=3.456, mask_ins=1.646, word_ins_ml=4.991, word_reposition=1.862, kpe=0.84, ppl=647.3, wps=6946.5, ups=0.34, wpb=20436.9, bsz=256, num_updates=17200, lr=0.000269582, gnorm=1.905, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-26 15:30:32 | INFO | train_inner | epoch 016:    490 / 1122 loss=9.417, nll_loss=3.511, mask_ins=1.643, word_ins_ml=5.04, word_reposition=1.891, kpe=0.843, ppl=683.52, wps=7007.8, ups=0.34, wpb=20636.2, bsz=256, num_updates=17300, lr=0.000268802, gnorm=1.949, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-26 15:32:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-26 15:35:43 | INFO | train_inner | epoch 016:    591 / 1122 loss=9.322, nll_loss=3.446, mask_ins=1.64, word_ins_ml=4.982, word_reposition=1.857, kpe=0.842, ppl=639.99, wps=6609.6, ups=0.32, wpb=20550.7, bsz=256, num_updates=17400, lr=0.000268028, gnorm=1.849, clip=0, loss_scale=2413, train_wall=273, wall=0
2022-07-26 15:38:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-26 15:40:41 | INFO | train_inner | epoch 016:    692 / 1122 loss=9.375, nll_loss=3.46, mask_ins=1.647, word_ins_ml=4.995, word_reposition=1.888, kpe=0.845, ppl=663.91, wps=6903, ups=0.34, wpb=20573, bsz=256, num_updates=17500, lr=0.000267261, gnorm=1.925, clip=0, loss_scale=1551, train_wall=260, wall=0
2022-07-26 15:45:35 | INFO | train_inner | epoch 016:    792 / 1122 loss=9.382, nll_loss=3.481, mask_ins=1.639, word_ins_ml=5.012, word_reposition=1.887, kpe=0.844, ppl=667.4, wps=6988.9, ups=0.34, wpb=20523.2, bsz=256, num_updates=17600, lr=0.000266501, gnorm=1.918, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-26 15:50:28 | INFO | train_inner | epoch 016:    892 / 1122 loss=9.382, nll_loss=3.482, mask_ins=1.642, word_ins_ml=5.014, word_reposition=1.881, kpe=0.846, ppl=667.32, wps=7003.8, ups=0.34, wpb=20546.7, bsz=256, num_updates=17700, lr=0.000265747, gnorm=1.89, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-26 15:55:20 | INFO | train_inner | epoch 016:    992 / 1122 loss=9.346, nll_loss=3.439, mask_ins=1.639, word_ins_ml=4.976, word_reposition=1.885, kpe=0.846, ppl=650.79, wps=7034.1, ups=0.34, wpb=20535.5, bsz=256, num_updates=17800, lr=0.000264999, gnorm=1.882, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 16:00:12 | INFO | train_inner | epoch 016:   1092 / 1122 loss=nan, nll_loss=3.441, mask_ins=1.64, word_ins_ml=4.978, word_reposition=1.863, kpe=nan, ppl=nan, wps=7019.7, ups=0.34, wpb=20492.2, bsz=256, num_updates=17900, lr=0.000264258, gnorm=1.985, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 16:01:39 | INFO | train | epoch 016 | loss nan | nll_loss 3.484 | mask_ins 1.645 | word_ins_ml 5.016 | word_reposition 1.885 | kpe nan | ppl nan | wps 6527.2 | ups 0.32 | wpb 20520.7 | bsz 255.8 | num_updates 17930 | lr 0.000264037 | gnorm 1.935 | clip 0 | loss_scale 1644 | train_wall 2891 | wall 0
2022-07-26 16:03:00 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 15.556 | nll_loss 7.532 | mask_ins 2.28 | word_ins_ml 8.734 | word_reposition 3.09 | kpe 1.453 | ppl 48163.7 | wps 12198.6 | wpb 2367.6 | bsz 32 | num_updates 17930 | best_loss 15.153
2022-07-26 16:03:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 16 @ 17930 updates, score 15.556) (writing took 11.58000679127872 seconds)
2022-07-26 16:06:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-26 16:06:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-26 16:06:41 | INFO | train_inner | epoch 017:     72 / 1122 loss=9.261, nll_loss=3.396, mask_ins=1.637, word_ins_ml=4.938, word_reposition=1.853, kpe=0.833, ppl=613.69, wps=5261.2, ups=0.26, wpb=20445.5, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=1.988, clip=0, loss_scale=1275, train_wall=259, wall=0
2022-07-26 16:11:33 | INFO | train_inner | epoch 017:    172 / 1122 loss=9.213, nll_loss=3.374, mask_ins=1.623, word_ins_ml=4.919, word_reposition=1.842, kpe=0.829, ppl=593.28, wps=7021.2, ups=0.34, wpb=20498.3, bsz=256, num_updates=18100, lr=0.000262794, gnorm=1.955, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-26 16:16:25 | INFO | train_inner | epoch 017:    272 / 1122 loss=9.2, nll_loss=3.373, mask_ins=1.621, word_ins_ml=4.918, word_reposition=1.834, kpe=0.827, ppl=588.29, wps=7012.9, ups=0.34, wpb=20482.1, bsz=256, num_updates=18200, lr=0.000262071, gnorm=1.922, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-26 16:21:17 | INFO | train_inner | epoch 017:    372 / 1122 loss=9.22, nll_loss=3.36, mask_ins=1.636, word_ins_ml=4.907, word_reposition=1.842, kpe=0.836, ppl=596.29, wps=7009.8, ups=0.34, wpb=20436.9, bsz=256, num_updates=18300, lr=0.000261354, gnorm=1.918, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-26 16:26:08 | INFO | train_inner | epoch 017:    472 / 1122 loss=9.19, nll_loss=3.367, mask_ins=1.62, word_ins_ml=4.912, word_reposition=1.827, kpe=0.831, ppl=584.16, wps=7018.4, ups=0.34, wpb=20478.7, bsz=256, num_updates=18400, lr=0.000260643, gnorm=1.971, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-26 16:31:26 | INFO | train_inner | epoch 017:    572 / 1122 loss=9.182, nll_loss=3.364, mask_ins=1.618, word_ins_ml=4.909, word_reposition=1.823, kpe=0.831, ppl=580.84, wps=6449.6, ups=0.31, wpb=20497.2, bsz=256, num_updates=18500, lr=0.000259938, gnorm=1.96, clip=0, loss_scale=512, train_wall=281, wall=0
2022-07-26 16:36:19 | INFO | train_inner | epoch 017:    672 / 1122 loss=9.209, nll_loss=3.337, mask_ins=1.633, word_ins_ml=4.886, word_reposition=1.855, kpe=0.835, ppl=591.88, wps=7040.7, ups=0.34, wpb=20626.8, bsz=256, num_updates=18600, lr=0.000259238, gnorm=1.914, clip=0, loss_scale=1004, train_wall=255, wall=0
2022-07-26 16:41:10 | INFO | train_inner | epoch 017:    772 / 1122 loss=9.23, nll_loss=3.355, mask_ins=1.631, word_ins_ml=4.901, word_reposition=1.865, kpe=0.832, ppl=600.38, wps=7120.2, ups=0.34, wpb=20738.2, bsz=256, num_updates=18700, lr=0.000258544, gnorm=1.935, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 16:46:02 | INFO | train_inner | epoch 017:    872 / 1122 loss=9.199, nll_loss=3.334, mask_ins=1.627, word_ins_ml=4.884, word_reposition=1.855, kpe=0.834, ppl=587.82, wps=7021.7, ups=0.34, wpb=20467.8, bsz=256, num_updates=18800, lr=0.000257855, gnorm=1.932, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 16:50:54 | INFO | train_inner | epoch 017:    972 / 1122 loss=9.193, nll_loss=3.354, mask_ins=1.624, word_ins_ml=4.901, word_reposition=1.833, kpe=0.835, ppl=585.4, wps=7025.2, ups=0.34, wpb=20495, bsz=256, num_updates=18900, lr=0.000257172, gnorm=1.87, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 16:55:45 | INFO | train_inner | epoch 017:   1072 / 1122 loss=nan, nll_loss=3.314, mask_ins=1.623, word_ins_ml=4.866, word_reposition=1.821, kpe=nan, ppl=nan, wps=7050.6, ups=0.34, wpb=20547.4, bsz=256, num_updates=19000, lr=0.000256495, gnorm=1.848, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 16:58:10 | INFO | train | epoch 017 | loss nan | nll_loss 3.353 | mask_ins 1.626 | word_ins_ml 4.9 | word_reposition 1.841 | kpe nan | ppl nan | wps 6777.3 | ups 0.33 | wpb 20518.9 | bsz 255.8 | num_updates 19050 | lr 0.000256158 | gnorm 1.924 | clip 0 | loss_scale 848 | train_wall 2885 | wall 0
2022-07-26 16:59:31 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 15.889 | nll_loss 7.663 | mask_ins 2.363 | word_ins_ml 8.862 | word_reposition 3.169 | kpe 1.494 | ppl 60666.7 | wps 12202.8 | wpb 2367.6 | bsz 32 | num_updates 19050 | best_loss 15.153
2022-07-26 16:59:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 17 @ 19050 updates, score 15.889) (writing took 10.877693340182304 seconds)
2022-07-26 17:02:07 | INFO | train_inner | epoch 018:     50 / 1122 loss=nan, nll_loss=3.275, mask_ins=1.617, word_ins_ml=4.831, word_reposition=1.83, kpe=nan, ppl=nan, wps=5309.6, ups=0.26, wpb=20291.8, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=1.944, clip=0, loss_scale=1884, train_wall=254, wall=0
2022-07-26 17:06:58 | INFO | train_inner | epoch 018:    150 / 1122 loss=9.062, nll_loss=3.27, mask_ins=1.604, word_ins_ml=4.826, word_reposition=1.816, kpe=0.815, ppl=534.5, wps=7070.9, ups=0.34, wpb=20564.2, bsz=256, num_updates=19200, lr=0.000255155, gnorm=1.836, clip=0, loss_scale=2048, train_wall=254, wall=0
2022-07-26 17:11:50 | INFO | train_inner | epoch 018:    250 / 1122 loss=9.061, nll_loss=3.266, mask_ins=1.612, word_ins_ml=4.823, word_reposition=1.805, kpe=0.82, ppl=534.17, wps=7057.2, ups=0.34, wpb=20591.3, bsz=256, num_updates=19300, lr=0.000254493, gnorm=1.907, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 17:16:42 | INFO | train_inner | epoch 018:    350 / 1122 loss=9.054, nll_loss=3.264, mask_ins=1.614, word_ins_ml=4.821, word_reposition=1.797, kpe=0.822, ppl=531.69, wps=7005.8, ups=0.34, wpb=20470.2, bsz=256, num_updates=19400, lr=0.000253837, gnorm=1.879, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 17:21:34 | INFO | train_inner | epoch 018:    450 / 1122 loss=9.054, nll_loss=3.26, mask_ins=1.609, word_ins_ml=4.817, word_reposition=1.808, kpe=0.82, ppl=531.42, wps=7043.1, ups=0.34, wpb=20563.1, bsz=256, num_updates=19500, lr=0.000253185, gnorm=1.875, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 17:26:25 | INFO | train_inner | epoch 018:    550 / 1122 loss=9.135, nll_loss=3.296, mask_ins=1.617, word_ins_ml=4.85, word_reposition=1.844, kpe=0.825, ppl=562.29, wps=7030.4, ups=0.34, wpb=20465.2, bsz=256, num_updates=19600, lr=0.000252538, gnorm=1.967, clip=0, loss_scale=3523, train_wall=254, wall=0
2022-07-26 17:26:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-26 17:31:42 | INFO | train_inner | epoch 018:    651 / 1122 loss=9.091, nll_loss=3.294, mask_ins=1.606, word_ins_ml=4.847, word_reposition=1.813, kpe=0.825, ppl=545.25, wps=6441.9, ups=0.32, wpb=20441.7, bsz=256, num_updates=19700, lr=0.000251896, gnorm=2.007, clip=0, loss_scale=2190, train_wall=280, wall=0
2022-07-26 17:36:36 | INFO | train_inner | epoch 018:    751 / 1122 loss=nan, nll_loss=3.285, mask_ins=1.618, word_ins_ml=4.84, word_reposition=1.815, kpe=nan, ppl=nan, wps=7033.2, ups=0.34, wpb=20629.3, bsz=256, num_updates=19800, lr=0.000251259, gnorm=1.969, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-26 17:41:28 | INFO | train_inner | epoch 018:    851 / 1122 loss=9.095, nll_loss=3.268, mask_ins=1.62, word_ins_ml=4.824, word_reposition=1.826, kpe=0.826, ppl=546.98, wps=7087, ups=0.34, wpb=20693.4, bsz=256, num_updates=19900, lr=0.000250627, gnorm=1.926, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 17:46:19 | INFO | train_inner | epoch 018:    951 / 1122 loss=9.053, nll_loss=3.274, mask_ins=1.611, word_ins_ml=4.831, word_reposition=1.785, kpe=0.825, ppl=531.01, wps=7015.1, ups=0.34, wpb=20455.5, bsz=256, num_updates=20000, lr=0.00025, gnorm=1.89, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 17:51:11 | INFO | train_inner | epoch 018:   1051 / 1122 loss=9.114, nll_loss=3.282, mask_ins=1.615, word_ins_ml=4.837, word_reposition=1.834, kpe=0.828, ppl=554.2, wps=7042.5, ups=0.34, wpb=20525.4, bsz=256, num_updates=20100, lr=0.000249377, gnorm=1.927, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 17:54:37 | INFO | train | epoch 018 | loss nan | nll_loss 3.275 | mask_ins 1.614 | word_ins_ml 4.831 | word_reposition 1.816 | kpe nan | ppl nan | wps 6792.1 | ups 0.33 | wpb 20521.5 | bsz 255.8 | num_updates 20171 | lr 0.000248938 | gnorm 1.922 | clip 0 | loss_scale 2289 | train_wall 2883 | wall 0
2022-07-26 17:55:57 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 15.462 | nll_loss 7.527 | mask_ins 2.243 | word_ins_ml 8.744 | word_reposition 3.05 | kpe 1.425 | ppl 45142.2 | wps 12240.6 | wpb 2367.6 | bsz 32 | num_updates 20171 | best_loss 15.153
2022-07-26 17:56:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 18 @ 20171 updates, score 15.462) (writing took 12.308972503058612 seconds)
2022-07-26 17:57:34 | INFO | train_inner | epoch 019:     29 / 1122 loss=9.09, nll_loss=3.27, mask_ins=1.617, word_ins_ml=4.827, word_reposition=1.825, kpe=0.821, ppl=544.87, wps=5303.2, ups=0.26, wpb=20315.7, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=1.976, clip=0, loss_scale=3727, train_wall=254, wall=0
2022-07-26 18:02:26 | INFO | train_inner | epoch 019:    129 / 1122 loss=8.965, nll_loss=3.205, mask_ins=1.589, word_ins_ml=4.768, word_reposition=1.803, kpe=0.805, ppl=499.67, wps=6975.4, ups=0.34, wpb=20343.6, bsz=256, num_updates=20300, lr=0.000248146, gnorm=1.931, clip=0, loss_scale=4096, train_wall=255, wall=0
2022-07-26 18:05:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-26 18:07:20 | INFO | train_inner | epoch 019:    230 / 1122 loss=nan, nll_loss=3.244, mask_ins=1.596, word_ins_ml=4.803, word_reposition=1.809, kpe=nan, ppl=nan, wps=6967.2, ups=0.34, wpb=20536.2, bsz=256, num_updates=20400, lr=0.000247537, gnorm=1.944, clip=0, loss_scale=3508, train_wall=258, wall=0
2022-07-26 18:12:12 | INFO | train_inner | epoch 019:    330 / 1122 loss=8.973, nll_loss=3.21, mask_ins=1.601, word_ins_ml=4.774, word_reposition=1.789, kpe=0.81, ppl=502.49, wps=7031, ups=0.34, wpb=20516.1, bsz=256, num_updates=20500, lr=0.000246932, gnorm=1.971, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 18:17:04 | INFO | train_inner | epoch 019:    430 / 1122 loss=9.006, nll_loss=3.223, mask_ins=1.604, word_ins_ml=4.785, word_reposition=1.805, kpe=0.812, ppl=514.17, wps=6989.5, ups=0.34, wpb=20392.4, bsz=256, num_updates=20600, lr=0.000246332, gnorm=1.96, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 18:21:56 | INFO | train_inner | epoch 019:    530 / 1122 loss=8.994, nll_loss=3.223, mask_ins=1.607, word_ins_ml=4.785, word_reposition=1.787, kpe=0.815, ppl=510.01, wps=7086.2, ups=0.34, wpb=20678.2, bsz=256, num_updates=20700, lr=0.000245737, gnorm=1.903, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 18:26:48 | INFO | train_inner | epoch 019:    630 / 1122 loss=8.962, nll_loss=3.213, mask_ins=1.597, word_ins_ml=4.775, word_reposition=1.776, kpe=0.813, ppl=498.71, wps=7057.6, ups=0.34, wpb=20625.7, bsz=256, num_updates=20800, lr=0.000245145, gnorm=1.977, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-26 18:32:02 | INFO | train_inner | epoch 019:    730 / 1122 loss=8.996, nll_loss=3.217, mask_ins=1.606, word_ins_ml=4.779, word_reposition=1.796, kpe=0.815, ppl=510.62, wps=6584.1, ups=0.32, wpb=20687.8, bsz=256, num_updates=20900, lr=0.000244558, gnorm=1.981, clip=0, loss_scale=2396, train_wall=278, wall=0
2022-07-26 18:36:55 | INFO | train_inner | epoch 019:    830 / 1122 loss=9.026, nll_loss=3.209, mask_ins=1.614, word_ins_ml=4.772, word_reposition=1.816, kpe=0.824, ppl=521.38, wps=7019.2, ups=0.34, wpb=20536.2, bsz=256, num_updates=21000, lr=0.000243975, gnorm=1.95, clip=0, loss_scale=4096, train_wall=256, wall=0
2022-07-26 18:38:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-26 18:41:48 | INFO | train_inner | epoch 019:    931 / 1122 loss=8.994, nll_loss=3.198, mask_ins=1.605, word_ins_ml=4.763, word_reposition=1.807, kpe=0.819, ppl=509.73, wps=6972.9, ups=0.34, wpb=20468, bsz=256, num_updates=21100, lr=0.000243396, gnorm=1.972, clip=0, loss_scale=2595, train_wall=257, wall=0
2022-07-26 18:46:40 | INFO | train_inner | epoch 019:   1031 / 1122 loss=9.008, nll_loss=3.205, mask_ins=1.61, word_ins_ml=4.769, word_reposition=1.811, kpe=0.818, ppl=514.85, wps=7072.9, ups=0.34, wpb=20609.7, bsz=256, num_updates=21200, lr=0.000242821, gnorm=1.967, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 18:51:04 | INFO | train | epoch 019 | loss nan | nll_loss 3.213 | mask_ins 1.603 | word_ins_ml 4.776 | word_reposition 1.798 | kpe nan | ppl nan | wps 6785 | ups 0.33 | wpb 20521 | bsz 255.8 | num_updates 21291 | lr 0.000242302 | gnorm 1.955 | clip 0 | loss_scale 2678 | train_wall 2883 | wall 0
2022-07-26 18:52:25 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 15.772 | nll_loss 7.593 | mask_ins 2.331 | word_ins_ml 8.801 | word_reposition 3.157 | kpe 1.484 | ppl 55969.6 | wps 12206.9 | wpb 2367.6 | bsz 32 | num_updates 21291 | best_loss 15.153
2022-07-26 18:52:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 19 @ 21291 updates, score 15.772) (writing took 13.410712270997465 seconds)
2022-07-26 18:53:04 | INFO | train_inner | epoch 020:      9 / 1122 loss=8.967, nll_loss=3.192, mask_ins=1.608, word_ins_ml=4.757, word_reposition=1.786, kpe=0.817, ppl=500.46, wps=5312.9, ups=0.26, wpb=20443.5, bsz=253.8, num_updates=21300, lr=0.000242251, gnorm=1.959, clip=0, loss_scale=2048, train_wall=254, wall=0
2022-07-26 18:57:57 | INFO | train_inner | epoch 020:    109 / 1122 loss=nan, nll_loss=3.162, mask_ins=1.579, word_ins_ml=4.731, word_reposition=1.752, kpe=nan, ppl=nan, wps=7007.4, ups=0.34, wpb=20494.8, bsz=256, num_updates=21400, lr=0.000241684, gnorm=1.999, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-26 19:01:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-26 19:02:50 | INFO | train_inner | epoch 020:    210 / 1122 loss=8.871, nll_loss=3.151, mask_ins=1.58, word_ins_ml=4.721, word_reposition=1.77, kpe=0.8, ppl=468.09, wps=7003.1, ups=0.34, wpb=20552.4, bsz=256, num_updates=21500, lr=0.000241121, gnorm=2.002, clip=0, loss_scale=1693, train_wall=256, wall=0
2022-07-26 19:07:43 | INFO | train_inner | epoch 020:    310 / 1122 loss=8.918, nll_loss=3.164, mask_ins=1.592, word_ins_ml=4.733, word_reposition=1.792, kpe=0.8, ppl=483.69, wps=7040.4, ups=0.34, wpb=20573.7, bsz=256, num_updates=21600, lr=0.000240563, gnorm=1.967, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 19:12:34 | INFO | train_inner | epoch 020:    410 / 1122 loss=nan, nll_loss=3.174, mask_ins=1.588, word_ins_ml=4.741, word_reposition=1.768, kpe=nan, ppl=nan, wps=7062.9, ups=0.34, wpb=20578.7, bsz=256, num_updates=21700, lr=0.000240008, gnorm=2.039, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 19:17:25 | INFO | train_inner | epoch 020:    510 / 1122 loss=8.901, nll_loss=3.158, mask_ins=1.585, word_ins_ml=4.727, word_reposition=1.787, kpe=0.803, ppl=478.11, wps=7050.4, ups=0.34, wpb=20520.6, bsz=256, num_updates=21800, lr=0.000239457, gnorm=1.982, clip=0, loss_scale=1024, train_wall=254, wall=0
2022-07-26 19:22:17 | INFO | train_inner | epoch 020:    610 / 1122 loss=8.917, nll_loss=3.17, mask_ins=1.598, word_ins_ml=4.738, word_reposition=1.776, kpe=0.806, ppl=483.42, wps=7053, ups=0.34, wpb=20615.2, bsz=256, num_updates=21900, lr=0.000238909, gnorm=2.104, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-26 19:27:09 | INFO | train_inner | epoch 020:    710 / 1122 loss=8.864, nll_loss=3.121, mask_ins=1.585, word_ins_ml=4.694, word_reposition=1.776, kpe=0.809, ppl=465.78, wps=7007.6, ups=0.34, wpb=20409.1, bsz=256, num_updates=22000, lr=0.000238366, gnorm=2.051, clip=0, loss_scale=1260, train_wall=254, wall=0
2022-07-26 19:32:37 | INFO | train_inner | epoch 020:    810 / 1122 loss=8.928, nll_loss=3.149, mask_ins=1.594, word_ins_ml=4.718, word_reposition=1.809, kpe=0.806, ppl=487.12, wps=6248.6, ups=0.3, wpb=20488.6, bsz=256, num_updates=22100, lr=0.000237826, gnorm=2.023, clip=0, loss_scale=2048, train_wall=291, wall=0
2022-07-26 19:37:29 | INFO | train_inner | epoch 020:    910 / 1122 loss=8.954, nll_loss=3.155, mask_ins=1.605, word_ins_ml=4.724, word_reposition=1.816, kpe=0.809, ppl=496.07, wps=7047, ups=0.34, wpb=20586.6, bsz=256, num_updates=22200, lr=0.000237289, gnorm=1.996, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 19:42:20 | INFO | train_inner | epoch 020:   1010 / 1122 loss=8.907, nll_loss=3.155, mask_ins=1.591, word_ins_ml=4.724, word_reposition=1.788, kpe=0.805, ppl=480.14, wps=7049.1, ups=0.34, wpb=20544.3, bsz=256, num_updates=22300, lr=0.000236757, gnorm=1.965, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 19:47:12 | INFO | train_inner | epoch 020:   1110 / 1122 loss=8.96, nll_loss=3.194, mask_ins=1.599, word_ins_ml=4.759, word_reposition=1.791, kpe=0.812, ppl=498.06, wps=7034.8, ups=0.34, wpb=20503.2, bsz=256, num_updates=22400, lr=0.000236228, gnorm=1.978, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 19:47:46 | INFO | train | epoch 020 | loss nan | nll_loss 3.159 | mask_ins 1.59 | word_ins_ml 4.728 | word_reposition 1.784 | kpe nan | ppl nan | wps 6762.8 | ups 0.33 | wpb 20520.8 | bsz 255.8 | num_updates 22412 | lr 0.000236165 | gnorm 2.017 | clip 0 | loss_scale 1581 | train_wall 2894 | wall 0
2022-07-26 19:49:07 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 15.422 | nll_loss 7.451 | mask_ins 2.26 | word_ins_ml 8.666 | word_reposition 3.071 | kpe 1.425 | ppl 43909.9 | wps 12209.5 | wpb 2367.6 | bsz 32 | num_updates 22412 | best_loss 15.153
2022-07-26 19:49:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 20 @ 22412 updates, score 15.422) (writing took 14.076015681028366 seconds)
2022-07-26 19:53:37 | INFO | train_inner | epoch 021:     88 / 1122 loss=8.842, nll_loss=3.124, mask_ins=1.574, word_ins_ml=4.697, word_reposition=1.781, kpe=0.79, ppl=458.94, wps=5309.5, ups=0.26, wpb=20437.2, bsz=253.8, num_updates=22500, lr=0.000235702, gnorm=2.038, clip=0, loss_scale=2273, train_wall=253, wall=0
2022-07-26 19:54:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-26 19:58:31 | INFO | train_inner | epoch 021:    189 / 1122 loss=8.808, nll_loss=3.095, mask_ins=1.579, word_ins_ml=4.672, word_reposition=1.767, kpe=0.791, ppl=448.17, wps=6942.2, ups=0.34, wpb=20468.2, bsz=256, num_updates=22600, lr=0.00023518, gnorm=2.032, clip=0, loss_scale=2595, train_wall=258, wall=0
2022-07-26 20:03:23 | INFO | train_inner | epoch 021:    289 / 1122 loss=8.817, nll_loss=3.107, mask_ins=1.582, word_ins_ml=4.681, word_reposition=1.762, kpe=0.792, ppl=451.05, wps=7082, ups=0.34, wpb=20651.1, bsz=256, num_updates=22700, lr=0.000234662, gnorm=1.987, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 20:08:14 | INFO | train_inner | epoch 021:    389 / 1122 loss=8.812, nll_loss=3.101, mask_ins=1.575, word_ins_ml=4.676, word_reposition=1.769, kpe=0.792, ppl=449.48, wps=7050.5, ups=0.34, wpb=20519.5, bsz=256, num_updates=22800, lr=0.000234146, gnorm=1.99, clip=0, loss_scale=2048, train_wall=254, wall=0
2022-07-26 20:13:06 | INFO | train_inner | epoch 021:    489 / 1122 loss=nan, nll_loss=3.104, mask_ins=1.576, word_ins_ml=4.679, word_reposition=1.754, kpe=nan, ppl=nan, wps=7069.9, ups=0.34, wpb=20652.7, bsz=256, num_updates=22900, lr=0.000233635, gnorm=2.041, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 20:17:58 | INFO | train_inner | epoch 021:    589 / 1122 loss=8.838, nll_loss=3.138, mask_ins=1.572, word_ins_ml=4.709, word_reposition=1.765, kpe=0.792, ppl=457.69, wps=7043.5, ups=0.34, wpb=20523.2, bsz=256, num_updates=23000, lr=0.000233126, gnorm=2.011, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 20:22:50 | INFO | train_inner | epoch 021:    689 / 1122 loss=8.858, nll_loss=3.132, mask_ins=1.578, word_ins_ml=4.704, word_reposition=1.777, kpe=0.799, ppl=464.03, wps=6982.2, ups=0.34, wpb=20399.4, bsz=256, num_updates=23100, lr=0.000232621, gnorm=2.023, clip=0, loss_scale=3318, train_wall=255, wall=0
2022-07-26 20:26:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-26 20:27:44 | INFO | train_inner | epoch 021:    790 / 1122 loss=8.822, nll_loss=3.1, mask_ins=1.581, word_ins_ml=4.676, word_reposition=1.765, kpe=0.8, ppl=452.63, wps=6996.7, ups=0.34, wpb=20563.2, bsz=256, num_updates=23200, lr=0.000232119, gnorm=2.027, clip=0, loss_scale=3386, train_wall=257, wall=0
2022-07-26 20:33:10 | INFO | train_inner | epoch 021:    890 / 1122 loss=8.844, nll_loss=3.124, mask_ins=1.577, word_ins_ml=4.697, word_reposition=1.777, kpe=0.795, ppl=459.65, wps=6253.1, ups=0.31, wpb=20385.6, bsz=256, num_updates=23300, lr=0.000231621, gnorm=1.952, clip=0, loss_scale=2048, train_wall=289, wall=0
2022-07-26 20:36:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-26 20:38:04 | INFO | train_inner | epoch 021:    991 / 1122 loss=8.845, nll_loss=3.129, mask_ins=1.581, word_ins_ml=4.701, word_reposition=1.762, kpe=0.802, ppl=459.97, wps=6988.2, ups=0.34, wpb=20595.2, bsz=256, num_updates=23400, lr=0.000231125, gnorm=2.027, clip=0, loss_scale=1673, train_wall=258, wall=0
2022-07-26 20:42:56 | INFO | train_inner | epoch 021:   1091 / 1122 loss=8.895, nll_loss=3.144, mask_ins=1.587, word_ins_ml=4.713, word_reposition=1.795, kpe=0.799, ppl=475.98, wps=7029.6, ups=0.34, wpb=20490.4, bsz=256, num_updates=23500, lr=0.000230633, gnorm=1.999, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 20:44:25 | INFO | train | epoch 021 | loss nan | nll_loss 3.118 | mask_ins 1.579 | word_ins_ml 4.691 | word_reposition 1.77 | kpe nan | ppl nan | wps 6754.3 | ups 0.33 | wpb 20521.5 | bsz 255.8 | num_updates 23531 | lr 0.000230481 | gnorm 2.01 | clip 0 | loss_scale 2198 | train_wall 2894 | wall 0
2022-07-26 20:45:47 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 15.5 | nll_loss 7.447 | mask_ins 2.3 | word_ins_ml 8.669 | word_reposition 3.057 | kpe 1.474 | ppl 46342.1 | wps 12204.7 | wpb 2367.6 | bsz 32 | num_updates 23531 | best_loss 15.153
2022-07-26 20:46:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 21 @ 23531 updates, score 15.5) (writing took 14.746535772457719 seconds)
2022-07-26 20:49:22 | INFO | train_inner | epoch 022:     69 / 1122 loss=8.828, nll_loss=3.121, mask_ins=1.581, word_ins_ml=4.695, word_reposition=1.76, kpe=0.792, ppl=454.53, wps=5268.8, ups=0.26, wpb=20344.2, bsz=253.8, num_updates=23600, lr=0.000230144, gnorm=2.057, clip=0, loss_scale=1024, train_wall=254, wall=0
2022-07-26 20:54:13 | INFO | train_inner | epoch 022:    169 / 1122 loss=8.706, nll_loss=3.037, mask_ins=1.557, word_ins_ml=4.62, word_reposition=1.752, kpe=0.776, ppl=417.54, wps=7056.7, ups=0.34, wpb=20545.5, bsz=256, num_updates=23700, lr=0.000229658, gnorm=2.034, clip=0, loss_scale=1024, train_wall=254, wall=0
2022-07-26 20:59:05 | INFO | train_inner | epoch 022:    269 / 1122 loss=8.752, nll_loss=3.083, mask_ins=1.561, word_ins_ml=4.66, word_reposition=1.751, kpe=0.78, ppl=431.07, wps=7069, ups=0.34, wpb=20618.3, bsz=256, num_updates=23800, lr=0.000229175, gnorm=1.988, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 21:03:56 | INFO | train_inner | epoch 022:    369 / 1122 loss=8.693, nll_loss=3.04, mask_ins=1.558, word_ins_ml=4.622, word_reposition=1.736, kpe=0.777, ppl=413.73, wps=7081.8, ups=0.34, wpb=20636, bsz=256, num_updates=23900, lr=0.000228695, gnorm=2, clip=0, loss_scale=1280, train_wall=254, wall=0
2022-07-26 21:08:48 | INFO | train_inner | epoch 022:    469 / 1122 loss=8.805, nll_loss=3.088, mask_ins=1.576, word_ins_ml=4.664, word_reposition=1.777, kpe=0.788, ppl=447.38, wps=7089.7, ups=0.34, wpb=20654, bsz=256, num_updates=24000, lr=0.000228218, gnorm=2.017, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 21:13:39 | INFO | train_inner | epoch 022:    569 / 1122 loss=8.748, nll_loss=3.083, mask_ins=1.569, word_ins_ml=4.659, word_reposition=1.732, kpe=0.788, ppl=429.98, wps=7037.8, ups=0.34, wpb=20486.2, bsz=256, num_updates=24100, lr=0.000227744, gnorm=2.059, clip=0, loss_scale=2048, train_wall=254, wall=0
2022-07-26 21:18:30 | INFO | train_inner | epoch 022:    669 / 1122 loss=8.773, nll_loss=3.056, mask_ins=1.577, word_ins_ml=4.637, word_reposition=1.769, kpe=0.789, ppl=437.36, wps=7071.6, ups=0.34, wpb=20586.7, bsz=256, num_updates=24200, lr=0.000227273, gnorm=2.031, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 21:23:22 | INFO | train_inner | epoch 022:    769 / 1122 loss=8.775, nll_loss=3.073, mask_ins=1.574, word_ins_ml=4.651, word_reposition=1.762, kpe=0.788, ppl=438.2, wps=7032.6, ups=0.34, wpb=20524.1, bsz=256, num_updates=24300, lr=0.000226805, gnorm=2.03, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 21:28:13 | INFO | train_inner | epoch 022:    869 / 1122 loss=8.779, nll_loss=3.084, mask_ins=1.564, word_ins_ml=4.661, word_reposition=1.765, kpe=0.789, ppl=439.39, wps=7011.1, ups=0.34, wpb=20433.2, bsz=256, num_updates=24400, lr=0.000226339, gnorm=2.03, clip=0, loss_scale=2314, train_wall=255, wall=0
2022-07-26 21:33:51 | INFO | train_inner | epoch 022:    969 / 1122 loss=nan, nll_loss=3.078, mask_ins=1.569, word_ins_ml=4.655, word_reposition=1.742, kpe=nan, ppl=nan, wps=6095.5, ups=0.3, wpb=20610.6, bsz=256, num_updates=24500, lr=0.000225877, gnorm=2.006, clip=0, loss_scale=4096, train_wall=301, wall=0
2022-07-26 21:34:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-26 21:38:47 | INFO | train_inner | epoch 022:   1070 / 1122 loss=8.799, nll_loss=3.1, mask_ins=1.569, word_ins_ml=4.674, word_reposition=1.768, kpe=0.787, ppl=445.27, wps=6912, ups=0.34, wpb=20420.2, bsz=256, num_updates=24600, lr=0.000225417, gnorm=1.967, clip=0, loss_scale=2230, train_wall=259, wall=0
2022-07-26 21:41:17 | INFO | train | epoch 022 | loss nan | nll_loss 3.076 | mask_ins 1.568 | word_ins_ml 4.654 | word_reposition 1.756 | kpe nan | ppl nan | wps 6742.6 | ups 0.33 | wpb 20521.8 | bsz 255.8 | num_updates 24652 | lr 0.00022518 | gnorm 2.02 | clip 0 | loss_scale 1957 | train_wall 2904 | wall 0
2022-07-26 21:42:38 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 15.616 | nll_loss 7.581 | mask_ins 2.292 | word_ins_ml 8.801 | word_reposition 3.045 | kpe 1.477 | ppl 50207.5 | wps 12246.2 | wpb 2367.6 | bsz 32 | num_updates 24652 | best_loss 15.153
2022-07-26 21:42:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 22 @ 24652 updates, score 15.616) (writing took 3.9599226620048285 seconds)
2022-07-26 21:45:02 | INFO | train_inner | epoch 023:     48 / 1122 loss=8.753, nll_loss=3.074, mask_ins=1.566, word_ins_ml=4.652, word_reposition=1.753, kpe=0.782, ppl=431.58, wps=5394.9, ups=0.27, wpb=20256.3, bsz=253.8, num_updates=24700, lr=0.000224961, gnorm=2.089, clip=0, loss_scale=2048, train_wall=254, wall=0
2022-07-26 21:49:54 | INFO | train_inner | epoch 023:    148 / 1122 loss=8.66, nll_loss=3.016, mask_ins=1.55, word_ins_ml=4.601, word_reposition=1.742, kpe=0.767, ppl=404.46, wps=7083.7, ups=0.34, wpb=20667.9, bsz=256, num_updates=24800, lr=0.000224507, gnorm=2.022, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 21:54:46 | INFO | train_inner | epoch 023:    248 / 1122 loss=8.645, nll_loss=3.02, mask_ins=1.551, word_ins_ml=4.604, word_reposition=1.72, kpe=0.769, ppl=400.25, wps=7045.7, ups=0.34, wpb=20550.2, bsz=256, num_updates=24900, lr=0.000224055, gnorm=2.031, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 21:55:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-26 21:59:41 | INFO | train_inner | epoch 023:    349 / 1122 loss=8.703, nll_loss=3.051, mask_ins=1.563, word_ins_ml=4.631, word_reposition=1.732, kpe=0.777, ppl=416.73, wps=6956.1, ups=0.34, wpb=20541.7, bsz=256, num_updates=25000, lr=0.000223607, gnorm=2.111, clip=0, loss_scale=1115, train_wall=259, wall=0
2022-07-26 22:04:32 | INFO | train_inner | epoch 023:    449 / 1122 loss=8.681, nll_loss=3.024, mask_ins=1.551, word_ins_ml=4.608, word_reposition=1.748, kpe=0.774, ppl=410.31, wps=7032.5, ups=0.34, wpb=20476, bsz=256, num_updates=25100, lr=0.000223161, gnorm=2.063, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 22:09:24 | INFO | train_inner | epoch 023:    549 / 1122 loss=nan, nll_loss=3.037, mask_ins=1.56, word_ins_ml=4.619, word_reposition=1.748, kpe=nan, ppl=nan, wps=7041.3, ups=0.34, wpb=20541.3, bsz=256, num_updates=25200, lr=0.000222718, gnorm=2.062, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 22:14:15 | INFO | train_inner | epoch 023:    649 / 1122 loss=nan, nll_loss=3.033, mask_ins=1.569, word_ins_ml=4.615, word_reposition=1.74, kpe=nan, ppl=nan, wps=7071.5, ups=0.34, wpb=20581.1, bsz=256, num_updates=25300, lr=0.000222277, gnorm=2.077, clip=0, loss_scale=1024, train_wall=254, wall=0
2022-07-26 22:19:07 | INFO | train_inner | epoch 023:    749 / 1122 loss=8.728, nll_loss=3.059, mask_ins=1.56, word_ins_ml=4.638, word_reposition=1.75, kpe=0.78, ppl=423.94, wps=7023.5, ups=0.34, wpb=20526.4, bsz=256, num_updates=25400, lr=0.000221839, gnorm=2.141, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 22:20:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-26 22:24:01 | INFO | train_inner | epoch 023:    850 / 1122 loss=8.715, nll_loss=3.059, mask_ins=1.557, word_ins_ml=4.638, word_reposition=1.737, kpe=0.782, ppl=420.11, wps=6986.2, ups=0.34, wpb=20537.8, bsz=256, num_updates=25500, lr=0.000221404, gnorm=2.286, clip=0, loss_scale=1105, train_wall=257, wall=0
2022-07-26 22:28:53 | INFO | train_inner | epoch 023:    950 / 1122 loss=8.688, nll_loss=3.031, mask_ins=1.559, word_ins_ml=4.614, word_reposition=1.735, kpe=0.78, ppl=412.48, wps=7070.5, ups=0.34, wpb=20620.7, bsz=256, num_updates=25600, lr=0.000220971, gnorm=2.078, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 22:34:19 | INFO | train_inner | epoch 023:   1050 / 1122 loss=8.686, nll_loss=3.043, mask_ins=1.551, word_ins_ml=4.624, word_reposition=1.729, kpe=0.782, ppl=411.88, wps=6265.9, ups=0.31, wpb=20416.5, bsz=256, num_updates=25700, lr=0.000220541, gnorm=2.236, clip=0, loss_scale=1024, train_wall=289, wall=0
2022-07-26 22:37:48 | INFO | train | epoch 023 | loss nan | nll_loss 3.038 | mask_ins 1.557 | word_ins_ml 4.62 | word_reposition 1.736 | kpe nan | ppl nan | wps 6779 | ups 0.33 | wpb 20520.9 | bsz 255.8 | num_updates 25772 | lr 0.000220232 | gnorm 2.108 | clip 0 | loss_scale 1266 | train_wall 2893 | wall 0
2022-07-26 22:39:08 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 15.642 | nll_loss 7.443 | mask_ins 2.344 | word_ins_ml 8.668 | word_reposition 3.193 | kpe 1.437 | ppl 51149.6 | wps 12256.4 | wpb 2367.6 | bsz 32 | num_updates 25772 | best_loss 15.153
2022-07-26 22:39:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 23 @ 25772 updates, score 15.642) (writing took 10.582419303245842 seconds)
2022-07-26 22:40:40 | INFO | train_inner | epoch 024:     28 / 1122 loss=8.634, nll_loss=3.022, mask_ins=1.544, word_ins_ml=4.606, word_reposition=1.71, kpe=0.774, ppl=397.24, wps=5302.4, ups=0.26, wpb=20238.6, bsz=253.8, num_updates=25800, lr=0.000220113, gnorm=2.1, clip=0, loss_scale=1024, train_wall=254, wall=0
2022-07-26 22:45:32 | INFO | train_inner | epoch 024:    128 / 1122 loss=nan, nll_loss=2.964, mask_ins=1.538, word_ins_ml=4.555, word_reposition=1.691, kpe=nan, ppl=nan, wps=7039.8, ups=0.34, wpb=20506, bsz=256, num_updates=25900, lr=0.000219687, gnorm=2.192, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 22:50:23 | INFO | train_inner | epoch 024:    228 / 1122 loss=8.623, nll_loss=3.002, mask_ins=1.541, word_ins_ml=4.588, word_reposition=1.729, kpe=0.764, ppl=394.15, wps=7031.6, ups=0.34, wpb=20494.2, bsz=256, num_updates=26000, lr=0.000219265, gnorm=2.145, clip=0, loss_scale=1649, train_wall=255, wall=0
2022-07-26 22:55:14 | INFO | train_inner | epoch 024:    328 / 1122 loss=8.637, nll_loss=2.998, mask_ins=1.549, word_ins_ml=4.584, word_reposition=1.737, kpe=0.766, ppl=398, wps=7031.7, ups=0.34, wpb=20485, bsz=256, num_updates=26100, lr=0.000218844, gnorm=2.209, clip=0, loss_scale=2048, train_wall=254, wall=0
2022-07-26 22:55:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-26 23:00:09 | INFO | train_inner | epoch 024:    429 / 1122 loss=nan, nll_loss=3.02, mask_ins=1.549, word_ins_ml=4.604, word_reposition=1.719, kpe=nan, ppl=nan, wps=6990.4, ups=0.34, wpb=20593.1, bsz=256, num_updates=26200, lr=0.000218426, gnorm=2.105, clip=0, loss_scale=1024, train_wall=257, wall=0
2022-07-26 23:05:01 | INFO | train_inner | epoch 024:    529 / 1122 loss=8.587, nll_loss=2.993, mask_ins=1.541, word_ins_ml=4.581, word_reposition=1.702, kpe=0.763, ppl=384.61, wps=7031.8, ups=0.34, wpb=20554.2, bsz=256, num_updates=26300, lr=0.00021801, gnorm=2.206, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-26 23:09:53 | INFO | train_inner | epoch 024:    629 / 1122 loss=8.653, nll_loss=3.017, mask_ins=1.548, word_ins_ml=4.601, word_reposition=1.734, kpe=0.769, ppl=402.46, wps=7018.9, ups=0.34, wpb=20472, bsz=256, num_updates=26400, lr=0.000217597, gnorm=2.219, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 23:14:45 | INFO | train_inner | epoch 024:    729 / 1122 loss=8.647, nll_loss=3.019, mask_ins=1.551, word_ins_ml=4.603, word_reposition=1.72, kpe=0.774, ppl=400.85, wps=7049.3, ups=0.34, wpb=20588.3, bsz=256, num_updates=26500, lr=0.000217186, gnorm=2.256, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-26 23:18:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-26 23:19:39 | INFO | train_inner | epoch 024:    830 / 1122 loss=8.641, nll_loss=2.989, mask_ins=1.553, word_ins_ml=4.576, word_reposition=1.743, kpe=0.77, ppl=399.33, wps=6996.3, ups=0.34, wpb=20568, bsz=256, num_updates=26600, lr=0.000216777, gnorm=2.122, clip=0, loss_scale=897, train_wall=257, wall=0
2022-07-26 23:24:31 | INFO | train_inner | epoch 024:    930 / 1122 loss=8.633, nll_loss=3.003, mask_ins=1.557, word_ins_ml=4.589, word_reposition=1.711, kpe=0.776, ppl=396.96, wps=7051.6, ups=0.34, wpb=20566.2, bsz=256, num_updates=26700, lr=0.000216371, gnorm=2.209, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-26 23:29:22 | INFO | train_inner | epoch 024:   1030 / 1122 loss=8.709, nll_loss=3.033, mask_ins=1.559, word_ins_ml=4.615, word_reposition=1.759, kpe=0.776, ppl=418.49, wps=7044.1, ups=0.34, wpb=20526, bsz=256, num_updates=26800, lr=0.000215967, gnorm=2.125, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-26 23:34:37 | INFO | train | epoch 024 | loss nan | nll_loss 3.006 | mask_ins 1.547 | word_ins_ml 4.592 | word_reposition 1.721 | kpe nan | ppl nan | wps 6741.9 | ups 0.33 | wpb 20519.8 | bsz 255.8 | num_updates 26892 | lr 0.000215597 | gnorm 2.18 | clip 0 | loss_scale 1026 | train_wall 2905 | wall 0
2022-07-26 23:35:57 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 16.042 | nll_loss 7.56 | mask_ins 2.49 | word_ins_ml 8.776 | word_reposition 3.268 | kpe 1.508 | ppl 67476.1 | wps 12233.8 | wpb 2367.6 | bsz 32 | num_updates 26892 | best_loss 15.153
2022-07-26 23:36:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 24 @ 26892 updates, score 16.042) (writing took 10.083439708687365 seconds)
2022-07-26 23:36:31 | INFO | train_inner | epoch 025:      8 / 1122 loss=8.638, nll_loss=3.049, mask_ins=1.543, word_ins_ml=4.629, word_reposition=1.692, kpe=0.773, ppl=398.51, wps=4771.4, ups=0.23, wpb=20453.9, bsz=253.8, num_updates=26900, lr=0.000215565, gnorm=2.236, clip=0, loss_scale=512, train_wall=301, wall=0
2022-07-26 23:41:21 | INFO | train_inner | epoch 025:    108 / 1122 loss=8.538, nll_loss=2.965, mask_ins=1.532, word_ins_ml=4.555, word_reposition=1.698, kpe=0.753, ppl=371.78, wps=7090.9, ups=0.34, wpb=20614.8, bsz=256, num_updates=27000, lr=0.000215166, gnorm=2.373, clip=0, loss_scale=512, train_wall=254, wall=0
2022-07-26 23:46:13 | INFO | train_inner | epoch 025:    208 / 1122 loss=8.586, nll_loss=2.977, mask_ins=1.541, word_ins_ml=4.566, word_reposition=1.723, kpe=0.757, ppl=384.15, wps=7028.4, ups=0.34, wpb=20505.9, bsz=256, num_updates=27100, lr=0.000214768, gnorm=2.127, clip=0, loss_scale=579, train_wall=255, wall=0
2022-07-26 23:51:04 | INFO | train_inner | epoch 025:    308 / 1122 loss=8.55, nll_loss=2.97, mask_ins=1.524, word_ins_ml=4.559, word_reposition=1.712, kpe=0.755, ppl=374.85, wps=7081.4, ups=0.34, wpb=20623.9, bsz=256, num_updates=27200, lr=0.000214373, gnorm=2.222, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-26 23:55:56 | INFO | train_inner | epoch 025:    408 / 1122 loss=8.555, nll_loss=2.979, mask_ins=1.533, word_ins_ml=4.568, word_reposition=1.697, kpe=0.757, ppl=375.98, wps=7072.7, ups=0.34, wpb=20607.9, bsz=256, num_updates=27300, lr=0.00021398, gnorm=2.151, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-27 00:00:47 | INFO | train_inner | epoch 025:    508 / 1122 loss=8.552, nll_loss=2.976, mask_ins=1.534, word_ins_ml=4.564, word_reposition=1.691, kpe=0.763, ppl=375.37, wps=7027.7, ups=0.34, wpb=20460.8, bsz=256, num_updates=27400, lr=0.000213589, gnorm=2.375, clip=0, loss_scale=1024, train_wall=254, wall=0
2022-07-27 00:05:38 | INFO | train_inner | epoch 025:    608 / 1122 loss=nan, nll_loss=2.945, mask_ins=1.536, word_ins_ml=4.537, word_reposition=1.707, kpe=nan, ppl=nan, wps=7018.4, ups=0.34, wpb=20434.2, bsz=256, num_updates=27500, lr=0.000213201, gnorm=2.133, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-27 00:07:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-27 00:10:33 | INFO | train_inner | epoch 025:    709 / 1122 loss=nan, nll_loss=2.963, mask_ins=1.543, word_ins_ml=4.553, word_reposition=1.695, kpe=nan, ppl=nan, wps=6959, ups=0.34, wpb=20519.7, bsz=256, num_updates=27600, lr=0.000212814, gnorm=2.115, clip=0, loss_scale=730, train_wall=258, wall=0
2022-07-27 00:15:25 | INFO | train_inner | epoch 025:    809 / 1122 loss=8.554, nll_loss=2.973, mask_ins=1.544, word_ins_ml=4.561, word_reposition=1.688, kpe=0.761, ppl=375.97, wps=7092, ups=0.34, wpb=20736.3, bsz=256, num_updates=27700, lr=0.00021243, gnorm=2.141, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-27 00:20:17 | INFO | train_inner | epoch 025:    909 / 1122 loss=8.569, nll_loss=2.973, mask_ins=1.534, word_ins_ml=4.562, word_reposition=1.708, kpe=0.764, ppl=379.72, wps=7005.3, ups=0.34, wpb=20400.9, bsz=256, num_updates=27800, lr=0.000212047, gnorm=2.136, clip=0, loss_scale=512, train_wall=254, wall=0
2022-07-27 00:25:08 | INFO | train_inner | epoch 025:   1009 / 1122 loss=8.584, nll_loss=2.984, mask_ins=1.544, word_ins_ml=4.571, word_reposition=1.7, kpe=0.768, ppl=383.71, wps=7016, ups=0.34, wpb=20447.2, bsz=256, num_updates=27900, lr=0.000211667, gnorm=2.173, clip=0, loss_scale=512, train_wall=254, wall=0
2022-07-27 00:30:11 | INFO | train_inner | epoch 025:   1109 / 1122 loss=8.583, nll_loss=2.997, mask_ins=1.535, word_ins_ml=4.583, word_reposition=1.695, kpe=0.768, ppl=383.41, wps=6753.2, ups=0.33, wpb=20439.2, bsz=256, num_updates=28000, lr=0.000211289, gnorm=2.132, clip=0, loss_scale=512, train_wall=266, wall=0
2022-07-27 00:31:23 | INFO | train | epoch 025 | loss nan | nll_loss 2.974 | mask_ins 1.537 | word_ins_ml 4.563 | word_reposition 1.702 | kpe nan | ppl nan | wps 6753.1 | ups 0.33 | wpb 20521.2 | bsz 255.8 | num_updates 28013 | lr 0.00021124 | gnorm 2.196 | clip 0 | loss_scale 720 | train_wall 2904 | wall 0
2022-07-27 00:32:45 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 15.599 | nll_loss 7.4 | mask_ins 2.296 | word_ins_ml 8.627 | word_reposition 3.162 | kpe 1.513 | ppl 49633.1 | wps 12141.1 | wpb 2367.6 | bsz 32 | num_updates 28013 | best_loss 15.153
2022-07-27 00:32:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 25 @ 28013 updates, score 15.599) (writing took 5.713144312612712 seconds)
2022-07-27 00:37:04 | INFO | train_inner | epoch 026:     87 / 1122 loss=8.492, nll_loss=2.951, mask_ins=1.523, word_ins_ml=4.542, word_reposition=1.678, kpe=0.749, ppl=360.13, wps=4953.2, ups=0.24, wpb=20455.7, bsz=253.8, num_updates=28100, lr=0.000210912, gnorm=2.34, clip=0, loss_scale=748, train_wall=289, wall=0
2022-07-27 00:41:56 | INFO | train_inner | epoch 026:    187 / 1122 loss=8.491, nll_loss=2.949, mask_ins=1.515, word_ins_ml=4.541, word_reposition=1.689, kpe=0.746, ppl=359.7, wps=7020.6, ups=0.34, wpb=20494.1, bsz=256, num_updates=28200, lr=0.000210538, gnorm=2.187, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-27 00:46:46 | INFO | train_inner | epoch 026:    287 / 1122 loss=8.421, nll_loss=2.889, mask_ins=1.51, word_ins_ml=4.488, word_reposition=1.682, kpe=0.741, ppl=342.67, wps=7027, ups=0.34, wpb=20430.1, bsz=256, num_updates=28300, lr=0.000210166, gnorm=2.155, clip=0, loss_scale=1024, train_wall=254, wall=0
2022-07-27 00:51:39 | INFO | train_inner | epoch 026:    387 / 1122 loss=8.511, nll_loss=2.958, mask_ins=1.53, word_ins_ml=4.548, word_reposition=1.681, kpe=0.752, ppl=364.9, wps=7056.3, ups=0.34, wpb=20621.4, bsz=256, num_updates=28400, lr=0.000209795, gnorm=2.184, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-27 00:56:31 | INFO | train_inner | epoch 026:    487 / 1122 loss=8.531, nll_loss=2.951, mask_ins=1.535, word_ins_ml=4.543, word_reposition=1.702, kpe=0.752, ppl=369.84, wps=7030.5, ups=0.34, wpb=20544.8, bsz=256, num_updates=28500, lr=0.000209427, gnorm=2.253, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-27 01:01:23 | INFO | train_inner | epoch 026:    587 / 1122 loss=8.5, nll_loss=2.942, mask_ins=1.519, word_ins_ml=4.534, word_reposition=1.695, kpe=0.752, ppl=362.09, wps=6998.7, ups=0.34, wpb=20419.9, bsz=256, num_updates=28600, lr=0.000209061, gnorm=2.16, clip=0, loss_scale=1372, train_wall=255, wall=0
2022-07-27 01:06:14 | INFO | train_inner | epoch 026:    687 / 1122 loss=nan, nll_loss=2.94, mask_ins=1.527, word_ins_ml=4.532, word_reposition=1.693, kpe=nan, ppl=nan, wps=7054.4, ups=0.34, wpb=20558.9, bsz=256, num_updates=28700, lr=0.000208696, gnorm=2.226, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-27 01:06:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-27 01:08:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-27 01:11:11 | INFO | train_inner | epoch 026:    789 / 1122 loss=nan, nll_loss=2.921, mask_ins=1.524, word_ins_ml=4.515, word_reposition=1.667, kpe=nan, ppl=nan, wps=6931.4, ups=0.34, wpb=20586.5, bsz=256, num_updates=28800, lr=0.000208333, gnorm=2.175, clip=0, loss_scale=833, train_wall=260, wall=0
2022-07-27 01:16:03 | INFO | train_inner | epoch 026:    889 / 1122 loss=8.556, nll_loss=2.969, mask_ins=1.539, word_ins_ml=4.558, word_reposition=1.703, kpe=0.756, ppl=376.31, wps=7067.7, ups=0.34, wpb=20620.4, bsz=256, num_updates=28900, lr=0.000207973, gnorm=2.233, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-27 01:20:54 | INFO | train_inner | epoch 026:    989 / 1122 loss=8.563, nll_loss=2.956, mask_ins=1.545, word_ins_ml=4.546, word_reposition=1.71, kpe=0.761, ppl=378.09, wps=7063.2, ups=0.34, wpb=20575.5, bsz=256, num_updates=29000, lr=0.000207614, gnorm=2.165, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-27 01:25:45 | INFO | train_inner | epoch 026:   1089 / 1122 loss=8.53, nll_loss=2.957, mask_ins=1.53, word_ins_ml=4.547, word_reposition=1.691, kpe=0.762, ppl=369.72, wps=7063.8, ups=0.34, wpb=20532.2, bsz=256, num_updates=29100, lr=0.000207257, gnorm=2.242, clip=0, loss_scale=512, train_wall=254, wall=0
2022-07-27 01:27:20 | INFO | train | epoch 026 | loss nan | nll_loss 2.945 | mask_ins 1.527 | word_ins_ml 4.536 | word_reposition 1.691 | kpe nan | ppl nan | wps 6846.4 | ups 0.33 | wpb 20520.8 | bsz 255.8 | num_updates 29133 | lr 0.000207139 | gnorm 2.208 | clip 0 | loss_scale 958 | train_wall 2858 | wall 0
2022-07-27 01:28:41 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 15.838 | nll_loss 7.525 | mask_ins 2.36 | word_ins_ml 8.753 | word_reposition 3.209 | kpe 1.516 | ppl 58577.5 | wps 12192.7 | wpb 2367.6 | bsz 32 | num_updates 29133 | best_loss 15.153
2022-07-27 01:28:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 26 @ 29133 updates, score 15.838) (writing took 16.10294223949313 seconds)
2022-07-27 01:32:57 | INFO | train_inner | epoch 027:     67 / 1122 loss=8.496, nll_loss=2.943, mask_ins=1.521, word_ins_ml=4.535, word_reposition=1.696, kpe=0.744, ppl=361.13, wps=4715.1, ups=0.23, wpb=20365, bsz=253.8, num_updates=29200, lr=0.000206901, gnorm=2.212, clip=0, loss_scale=512, train_wall=298, wall=0
2022-07-27 01:37:50 | INFO | train_inner | epoch 027:    167 / 1122 loss=8.431, nll_loss=2.9, mask_ins=1.512, word_ins_ml=4.496, word_reposition=1.683, kpe=0.739, ppl=345.14, wps=7060.6, ups=0.34, wpb=20671.4, bsz=256, num_updates=29300, lr=0.000206548, gnorm=2.188, clip=0, loss_scale=737, train_wall=256, wall=0
2022-07-27 01:42:41 | INFO | train_inner | epoch 027:    267 / 1122 loss=8.404, nll_loss=2.907, mask_ins=1.512, word_ins_ml=4.504, word_reposition=1.649, kpe=0.739, ppl=338.81, wps=7068.9, ups=0.34, wpb=20620.9, bsz=256, num_updates=29400, lr=0.000206197, gnorm=2.235, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-27 01:47:32 | INFO | train_inner | epoch 027:    367 / 1122 loss=nan, nll_loss=2.896, mask_ins=1.512, word_ins_ml=4.493, word_reposition=1.639, kpe=nan, ppl=nan, wps=6999.7, ups=0.34, wpb=20350.4, bsz=256, num_updates=29500, lr=0.000205847, gnorm=2.292, clip=0, loss_scale=1024, train_wall=254, wall=0
2022-07-27 01:52:24 | INFO | train_inner | epoch 027:    467 / 1122 loss=8.49, nll_loss=2.937, mask_ins=1.523, word_ins_ml=4.529, word_reposition=1.693, kpe=0.745, ppl=359.44, wps=7006.2, ups=0.34, wpb=20438.5, bsz=256, num_updates=29600, lr=0.000205499, gnorm=2.24, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-27 01:53:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-27 01:57:18 | INFO | train_inner | epoch 027:    568 / 1122 loss=8.496, nll_loss=2.923, mask_ins=1.521, word_ins_ml=4.517, word_reposition=1.712, kpe=0.746, ppl=361.09, wps=6983.5, ups=0.34, wpb=20535.2, bsz=256, num_updates=29700, lr=0.000205152, gnorm=2.284, clip=0, loss_scale=634, train_wall=257, wall=0
2022-07-27 02:02:09 | INFO | train_inner | epoch 027:    668 / 1122 loss=8.504, nll_loss=2.938, mask_ins=1.522, word_ins_ml=4.53, word_reposition=1.706, kpe=0.746, ppl=363.06, wps=7071.1, ups=0.34, wpb=20575.1, bsz=256, num_updates=29800, lr=0.000204808, gnorm=2.181, clip=0, loss_scale=512, train_wall=254, wall=0
2022-07-27 02:07:01 | INFO | train_inner | epoch 027:    768 / 1122 loss=8.449, nll_loss=2.906, mask_ins=1.525, word_ins_ml=4.501, word_reposition=1.677, kpe=0.746, ppl=349.52, wps=7038.5, ups=0.34, wpb=20554.9, bsz=256, num_updates=29900, lr=0.000204465, gnorm=2.242, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-27 02:11:52 | INFO | train_inner | epoch 027:    868 / 1122 loss=nan, nll_loss=2.909, mask_ins=1.521, word_ins_ml=4.504, word_reposition=1.676, kpe=nan, ppl=nan, wps=7063.8, ups=0.34, wpb=20553.9, bsz=256, num_updates=30000, lr=0.000204124, gnorm=2.303, clip=0, loss_scale=512, train_wall=254, wall=0
2022-07-27 02:16:44 | INFO | train_inner | epoch 027:    968 / 1122 loss=8.458, nll_loss=2.919, mask_ins=1.523, word_ins_ml=4.514, word_reposition=1.672, kpe=0.75, ppl=351.62, wps=7055, ups=0.34, wpb=20586.8, bsz=256, num_updates=30100, lr=0.000203785, gnorm=2.172, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-27 02:21:35 | INFO | train_inner | epoch 027:   1068 / 1122 loss=8.441, nll_loss=2.917, mask_ins=1.52, word_ins_ml=4.511, word_reposition=1.658, kpe=0.751, ppl=347.48, wps=7032, ups=0.34, wpb=20521.2, bsz=256, num_updates=30200, lr=0.000203447, gnorm=2.207, clip=0, loss_scale=845, train_wall=254, wall=0
2022-07-27 02:24:11 | INFO | train | epoch 027 | loss nan | nll_loss 2.914 | mask_ins 1.518 | word_ins_ml 4.509 | word_reposition 1.676 | kpe nan | ppl nan | wps 6744.2 | ups 0.33 | wpb 20522 | bsz 255.8 | num_updates 30254 | lr 0.000203265 | gnorm 2.238 | clip 0 | loss_scale 734 | train_wall 2900 | wall 0
2022-07-27 02:25:32 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 15.778 | nll_loss 7.461 | mask_ins 2.401 | word_ins_ml 8.68 | word_reposition 3.214 | kpe 1.483 | ppl 56191.9 | wps 12189.7 | wpb 2367.6 | bsz 32 | num_updates 30254 | best_loss 15.153
2022-07-27 02:25:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 27 @ 30254 updates, score 15.778) (writing took 3.49041851144284 seconds)
2022-07-27 02:27:50 | INFO | train_inner | epoch 028:     46 / 1122 loss=8.38, nll_loss=2.876, mask_ins=1.51, word_ins_ml=4.476, word_reposition=1.653, kpe=0.74, ppl=333.07, wps=5423.8, ups=0.27, wpb=20313.4, bsz=253.8, num_updates=30300, lr=0.000203111, gnorm=2.343, clip=0, loss_scale=1024, train_wall=253, wall=0
2022-07-27 02:33:28 | INFO | train_inner | epoch 028:    146 / 1122 loss=8.395, nll_loss=2.896, mask_ins=1.505, word_ins_ml=4.493, word_reposition=1.666, kpe=0.731, ppl=336.61, wps=6083.8, ups=0.3, wpb=20543.3, bsz=256, num_updates=30400, lr=0.000202777, gnorm=2.187, clip=0, loss_scale=1024, train_wall=300, wall=0
2022-07-27 02:38:21 | INFO | train_inner | epoch 028:    246 / 1122 loss=8.366, nll_loss=2.869, mask_ins=1.498, word_ins_ml=4.469, word_reposition=1.67, kpe=0.729, ppl=329.93, wps=7003.3, ups=0.34, wpb=20511.7, bsz=256, num_updates=30500, lr=0.000202444, gnorm=2.309, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-27 02:43:13 | INFO | train_inner | epoch 028:    346 / 1122 loss=8.433, nll_loss=2.901, mask_ins=1.513, word_ins_ml=4.498, word_reposition=1.685, kpe=0.736, ppl=345.5, wps=7006.2, ups=0.34, wpb=20505.4, bsz=256, num_updates=30600, lr=0.000202113, gnorm=2.246, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-07-27 02:46:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-27 02:47:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-27 02:48:13 | INFO | train_inner | epoch 028:    448 / 1122 loss=8.399, nll_loss=2.9, mask_ins=1.507, word_ins_ml=4.497, word_reposition=1.661, kpe=0.734, ppl=337.47, wps=6897.2, ups=0.33, wpb=20671.9, bsz=256, num_updates=30700, lr=0.000201784, gnorm=2.322, clip=0, loss_scale=1084, train_wall=261, wall=0
2022-07-27 02:53:06 | INFO | train_inner | epoch 028:    548 / 1122 loss=nan, nll_loss=2.846, mask_ins=1.512, word_ins_ml=4.449, word_reposition=1.666, kpe=nan, ppl=nan, wps=7003, ups=0.34, wpb=20507.7, bsz=256, num_updates=30800, lr=0.000201456, gnorm=2.393, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-27 02:57:59 | INFO | train_inner | epoch 028:    648 / 1122 loss=nan, nll_loss=2.911, mask_ins=1.509, word_ins_ml=4.506, word_reposition=1.677, kpe=nan, ppl=nan, wps=6983.4, ups=0.34, wpb=20454, bsz=256, num_updates=30900, lr=0.000201129, gnorm=2.302, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-27 03:02:51 | INFO | train_inner | epoch 028:    748 / 1122 loss=8.39, nll_loss=2.897, mask_ins=1.505, word_ins_ml=4.494, word_reposition=1.652, kpe=0.74, ppl=335.55, wps=7016.1, ups=0.34, wpb=20473.9, bsz=256, num_updates=31000, lr=0.000200805, gnorm=2.313, clip=0, loss_scale=512, train_wall=254, wall=0
2022-07-27 03:07:42 | INFO | train_inner | epoch 028:    848 / 1122 loss=8.41, nll_loss=2.89, mask_ins=1.512, word_ins_ml=4.488, word_reposition=1.673, kpe=0.738, ppl=340.21, wps=7022.3, ups=0.34, wpb=20485.2, bsz=256, num_updates=31100, lr=0.000200482, gnorm=2.639, clip=0, loss_scale=512, train_wall=255, wall=0
2022-07-27 03:12:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-27 03:12:38 | INFO | train_inner | epoch 028:    949 / 1122 loss=8.46, nll_loss=2.915, mask_ins=1.516, word_ins_ml=4.51, word_reposition=1.691, kpe=0.744, ppl=352.22, wps=6941.7, ups=0.34, wpb=20500.9, bsz=256, num_updates=31200, lr=0.00020016, gnorm=2.357, clip=0, loss_scale=517, train_wall=258, wall=0
2022-07-27 03:17:33 | INFO | train_inner | epoch 028:   1049 / 1122 loss=8.448, nll_loss=2.905, mask_ins=1.522, word_ins_ml=4.501, word_reposition=1.681, kpe=0.744, ppl=349.22, wps=6967, ups=0.34, wpb=20571.5, bsz=256, num_updates=31300, lr=0.00019984, gnorm=2.426, clip=0, loss_scale=512, train_wall=258, wall=0
2022-07-27 03:19:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-27 03:21:05 | INFO | train | epoch 028 | loss nan | nll_loss 2.892 | mask_ins 1.51 | word_ins_ml 4.49 | word_reposition 1.671 | kpe nan | ppl nan | wps 6720.3 | ups 0.33 | wpb 20520.4 | bsz 255.8 | num_updates 31372 | lr 0.000199611 | gnorm 2.367 | clip 0 | loss_scale 713 | train_wall 2909 | wall 0
2022-07-27 03:22:26 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 16.039 | nll_loss 7.479 | mask_ins 2.445 | word_ins_ml 8.697 | word_reposition 3.367 | kpe 1.531 | ppl 67352.9 | wps 12221.2 | wpb 2367.6 | bsz 32 | num_updates 31372 | best_loss 15.153
2022-07-27 03:22:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_finetune_cased/checkpoint_last.pt (epoch 28 @ 31372 updates, score 16.039) (writing took 15.726704956032336 seconds)
2022-07-27 03:22:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-27 03:24:06 | INFO | train_inner | epoch 029:     29 / 1122 loss=8.385, nll_loss=2.872, mask_ins=1.513, word_ins_ml=4.472, word_reposition=1.66, kpe=0.74, ppl=334.34, wps=5214.1, ups=0.25, wpb=20491.9, bsz=253.8, num_updates=31400, lr=0.000199522, gnorm=2.578, clip=0, loss_scale=299, train_wall=259, wall=0
2022-07-27 03:28:58 | INFO | train_inner | epoch 029:    129 / 1122 loss=8.328, nll_loss=2.853, mask_ins=1.497, word_ins_ml=4.455, word_reposition=1.648, kpe=0.728, ppl=321.38, wps=7049.1, ups=0.34, wpb=20577.9, bsz=256, num_updates=31500, lr=0.000199205, gnorm=2.481, clip=0, loss_scale=128, train_wall=255, wall=0
2022-07-27 03:34:24 | INFO | train_inner | epoch 029:    229 / 1122 loss=8.332, nll_loss=2.844, mask_ins=1.497, word_ins_ml=4.447, word_reposition=1.66, kpe=0.728, ppl=322.23, wps=6287.5, ups=0.31, wpb=20501.8, bsz=256, num_updates=31600, lr=0.000198889, gnorm=2.759, clip=1, loss_scale=128, train_wall=289, wall=0
2022-07-27 03:39:19 | INFO | train_inner | epoch 029:    329 / 1122 loss=8.353, nll_loss=2.898, mask_ins=1.495, word_ins_ml=4.495, word_reposition=1.631, kpe=0.733, ppl=326.98, wps=6963.5, ups=0.34, wpb=20537.6, bsz=256, num_updates=31700, lr=0.000198575, gnorm=2.868, clip=0, loss_scale=128, train_wall=258, wall=0
2022-07-27 03:44:11 | INFO | train_inner | epoch 029:    429 / 1122 loss=nan, nll_loss=2.878, mask_ins=1.497, word_ins_ml=4.477, word_reposition=1.676, kpe=nan, ppl=nan, wps=7010.2, ups=0.34, wpb=20501.5, bsz=256, num_updates=31800, lr=0.000198263, gnorm=2.716, clip=1, loss_scale=128, train_wall=255, wall=0
2022-07-27 03:49:04 | INFO | train_inner | epoch 029:    529 / 1122 loss=8.291, nll_loss=2.82, mask_ins=1.502, word_ins_ml=4.426, word_reposition=1.632, kpe=0.73, ppl=313.15, wps=7015.4, ups=0.34, wpb=20500.6, bsz=256, num_updates=31900, lr=0.000197952, gnorm=2.628, clip=0, loss_scale=146, train_wall=255, wall=0
2022-07-27 03:53:56 | INFO | train_inner | epoch 029:    629 / 1122 loss=nan, nll_loss=2.896, mask_ins=1.499, word_ins_ml=4.493, word_reposition=1.663, kpe=nan, ppl=nan, wps=7036.3, ups=0.34, wpb=20576.6, bsz=256, num_updates=32000, lr=0.000197642, gnorm=2.35, clip=0, loss_scale=256, train_wall=255, wall=0
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
