nohup: ignoring input
2022-07-08 15:58:15 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:12291
2022-07-08 15:58:15 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:12291
2022-07-08 15:58:15 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:12291
2022-07-08 15:58:15 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:12291
2022-07-08 15:58:15 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-08 15:58:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-08 15:58:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-08 15:58:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-08 15:58:16 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-08 15:58:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-08 15:58:16 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-08 15:58:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-08 15:58:16 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-08 15:58:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-08 15:58:16 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-08 15:58:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-08 15:58:20 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:12291', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='/data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, no_share_maskpredictor=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-08 15:58:20 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-08 15:58:20 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
start load cached examples valid ...
0it [00:00, ?it/s]2022-07-08 15:58:20 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.source
2022-07-08 15:58:20 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.target
2022-07-08 15:58:20 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]388it [00:00, 3875.79it/s]381it [00:00, 3805.11it/s]372it [00:00, 3717.96it/s]393it [00:00, 3922.62it/s]744it [00:00, 3472.05it/s]776it [00:00, 3432.83it/s]762it [00:00, 3420.65it/s]786it [00:00, 3531.21it/s]1111it [00:00, 3555.21it/s]1140it [00:00, 3518.24it/s]1116it [00:00, 3468.86it/s]1154it [00:00, 3592.21it/s]1468it [00:00, 3404.88it/s]1495it [00:00, 3390.66it/s]1465it [00:00, 3315.86it/s]1515it [00:00, 3442.30it/s]1859it [00:00, 3578.86it/s]1883it [00:00, 3556.54it/s]1826it [00:00, 3414.99it/s]1904it [00:00, 3594.06it/s]2232it [00:00, 3625.40it/s]2208it [00:00, 3546.78it/s]2241it [00:00, 3426.96it/s]2266it [00:00, 3503.98it/s]2596it [00:00, 3534.05it/s]2641it [00:00, 3604.00it/s]2665it [00:00, 3654.64it/s]2565it [00:00, 3444.03it/s]2994it [00:00, 3671.35it/s]3034it [00:00, 3703.59it/s]2956it [00:00, 3585.49it/s]3059it [00:00, 3564.22it/s]3363it [00:00, 3547.25it/s]3407it [00:00, 3600.00it/s]3317it [00:00, 3470.32it/s]3449it [00:00, 3663.38it/s]3731it [00:01, 3585.80it/s]3779it [00:01, 3632.73it/s]3678it [00:01, 3509.06it/s]3837it [00:01, 3726.74it/s]4031it [00:01, 3421.98it/s]4144it [00:01, 3504.92it/s]4091it [00:01, 3396.06it/s]4212it [00:01, 3579.48it/s]4405it [00:01, 3514.21it/s]4518it [00:01, 3573.08it/s]4465it [00:01, 3493.12it/s]4588it [00:01, 3629.07it/s]4758it [00:01, 3179.56it/s]4877it [00:01, 3173.84it/s]4953it [00:01, 3220.28it/s]4817it [00:01, 3016.21it/s]5085it [00:01, 3201.69it/s]5209it [00:01, 3213.04it/s]5284it [00:01, 3241.62it/s]5131it [00:01, 2700.01it/s]5410it [00:01, 3092.74it/s]5537it [00:01, 3077.24it/s]5615it [00:01, 3046.30it/s]5452it [00:01, 2826.48it/s]5723it [00:01, 2961.72it/s]5850it [00:01, 3028.14it/s]5957it [00:01, 3146.02it/s]5747it [00:01, 2842.90it/s]6047it [00:01, 3036.38it/s]6210it [00:01, 3185.44it/s]6294it [00:01, 3207.80it/s]6105it [00:01, 3042.24it/s]6381it [00:01, 3120.09it/s]6435it [00:01, 3027.57it/s]6533it [00:02, 2009.28it/s]6619it [00:02, 1855.59it/s]6886it [00:02, 2317.42it/s]6744it [00:02, 1992.45it/s]6696it [00:02, 1779.07it/s]6964it [00:02, 2152.38it/s]7240it [00:02, 2591.75it/s]7095it [00:02, 2306.56it/s]7041it [00:02, 2094.83it/s]7316it [00:02, 2443.62it/s]7546it [00:02, 2672.11it/s]7392it [00:02, 2457.63it/s]7344it [00:02, 2259.12it/s]7618it [00:02, 2537.76it/s]7902it [00:02, 2897.56it/s]7731it [00:02, 2685.38it/s]7691it [00:02, 2537.38it/s]7958it [00:02, 2747.42it/s]8220it [00:02, 2919.69it/s]8082it [00:02, 2898.11it/s]8037it [00:02, 2764.21it/s]8267it [00:02, 2747.55it/s]8576it [00:02, 3092.07it/s]8398it [00:02, 2916.83it/s]8350it [00:02, 2746.26it/s]8609it [00:02, 2924.52it/s]8920it [00:02, 3188.95it/s]8748it [00:02, 3076.77it/s]8695it [00:02, 2931.68it/s]8948it [00:02, 3050.03it/s]9251it [00:02, 3126.74it/s]9070it [00:02, 3027.53it/s]9024it [00:03, 2916.78it/s]9268it [00:03, 2993.88it/s]9606it [00:03, 3244.48it/s]9423it [00:03, 3167.13it/s]9370it [00:03, 3062.99it/s]9620it [00:03, 3140.68it/s]9937it [00:03, 3139.78it/s]9780it [00:03, 3280.57it/s]9715it [00:03, 3170.18it/s]9943it [00:03, 3080.60it/s]10291it [00:03, 3251.94it/s]10115it [00:03, 3202.93it/s]10295it [00:03, 3204.63it/s]10041it [00:03, 3037.27it/s]10644it [00:03, 3330.04it/s]10453it [00:03, 3252.48it/s]10647it [00:03, 3293.78it/s]10386it [00:03, 3150.77it/s]10981it [00:03, 3235.71it/s]10782it [00:03, 3179.12it/s]10981it [00:03, 3187.28it/s]10707it [00:03, 3057.62it/s]11324it [00:03, 3289.91it/s]11134it [00:03, 3276.15it/s]11332it [00:03, 3279.26it/s]11049it [00:03, 3158.99it/s]11656it [00:03, 3201.64it/s]11488it [00:03, 3352.62it/s]11392it [00:03, 3234.41it/s]11663it [00:03, 3184.30it/s]12011it [00:03, 3300.53it/s]11826it [00:03, 3250.85it/s]12016it [00:03, 3283.12it/s]11719it [00:03, 3072.06it/s]12366it [00:03, 3372.57it/s]12169it [00:03, 3300.90it/s]12365it [00:03, 3343.01it/s]12062it [00:03, 3172.39it/s]12705it [00:04, 3213.52it/s]12501it [00:04, 3150.84it/s]12701it [00:04, 3159.75it/s]12384it [00:04, 3080.41it/s]13050it [00:04, 3279.34it/s]12827it [00:04, 3180.24it/s]12699it [00:04, 3099.74it/s]13020it [00:04, 3062.20it/s]13368it [00:04, 3149.72it/s]
13147it [00:04, 3159.62it/s]13011it [00:04, 3073.93it/s]13329it [00:04, 2869.34it/s]13368it [00:04, 3085.04it/s]
13368it [00:04, 3082.35it/s]
2022-07-08 15:58:25 | INFO | root | success load 13368 data
2022-07-08 15:58:25 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-08 15:58:25 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-08 15:58:25 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-08 15:58:25 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-08 15:58:25 | INFO | transformer.tokenization_utils | loading file None
2022-07-08 15:58:25 | INFO | transformer.tokenization_utils | loading file None
2022-07-08 15:58:25 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
13320it [00:04, 2833.13it/s]13368it [00:04, 3001.51it/s]
2022-07-08 15:58:26 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-08 15:58:26 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-08 15:58:26 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-08 15:58:29 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-08 15:58:29 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-08 15:58:29 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-08 15:58:29 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-08 15:58:29 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
Trained parameters: len 319
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 319
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-08 15:58:32 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_msk.encoder_attn_fc1.weight', 'layers.0.adapter_msk.encoder_attn_fc2.weight', 'layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_msk.encoder_attn_fc1.weight', 'layers.1.adapter_msk.encoder_attn_fc2.weight', 'layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_msk.encoder_attn_fc1.weight', 'layers.2.adapter_msk.encoder_attn_fc2.weight', 'layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_msk.encoder_attn_fc1.weight', 'layers.3.adapter_msk.encoder_attn_fc2.weight', 'layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_msk.encoder_attn_fc1.weight', 'layers.4.adapter_msk.encoder_attn_fc2.weight', 'layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_msk.encoder_attn_fc1.weight', 'layers.5.adapter_msk.encoder_attn_fc2.weight', 'layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_msk.encoder_attn_fc1.weight', 'layers.6.adapter_msk.encoder_attn_fc2.weight', 'layers.6.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_msk.encoder_attn_fc1.weight', 'layers.7.adapter_msk.encoder_attn_fc2.weight', 'layers.7.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_msk.encoder_attn_fc1.weight', 'layers.8.adapter_msk.encoder_attn_fc2.weight', 'layers.8.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_msk.encoder_attn_fc1.weight', 'layers.9.adapter_msk.encoder_attn_fc2.weight', 'layers.9.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_msk.encoder_attn_fc1.weight', 'layers.10.adapter_msk.encoder_attn_fc2.weight', 'layers.10.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_msk.encoder_attn_fc1.weight', 'layers.11.adapter_msk.encoder_attn_fc2.weight', 'layers.11.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-08 15:58:32 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 319
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-07-08 15:58:32 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-07-08 15:58:32 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-08 15:58:32 | INFO | fairseq_cli.train | num. model params: 418522883 (num. trained: 180224003)
2022-07-08 15:58:32 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-07-08 15:58:32 | INFO | fairseq_cli.train | num. Decoder model params: 272050176 (Decoder num. trained: 142061568)
Trained parameters: len 319
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]325it [00:00, 3247.89it/s]650it [00:00, 3213.84it/s]972it [00:00, 2965.80it/s]1283it [00:00, 3019.12it/s]1617it [00:00, 3127.96it/s]1932it [00:00, 2910.72it/s]2252it [00:00, 2997.21it/s]2555it [00:00, 2860.02it/s]2871it [00:00, 2946.28it/s]3195it [00:01, 3032.26it/s]3501it [00:01, 2843.73it/s]3817it [00:01, 2931.16it/s]4185it [00:01, 3144.71it/s]2022-07-08 15:58:36 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-08 15:58:36 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
4503it [00:01, 3065.95it/s]4865it [00:01, 3225.48it/s]5190it [00:01, 3140.01it/s]5545it [00:01, 3256.08it/s]5892it [00:01, 3191.38it/s]6250it [00:02, 3300.68it/s]6606it [00:02, 3373.84it/s]6945it [00:03, 917.99it/s] 
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]7316it [00:03, 1202.74it/s]371it [00:00, 3706.24it/s]7641it [00:03, 1446.03it/s]2022-07-08 15:58:38 | INFO | fairseq.trainer | loaded checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 11 @ 12324 updates)
742it [00:00, 3631.57it/s]2022-07-08 15:58:38 | INFO | fairseq.trainer | loading train data for epoch 11
8003it [00:03, 1776.98it/s]1106it [00:00, 3342.83it/s]8364it [00:03, 2104.29it/s]2022-07-08 15:58:38 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.source
1469it [00:00, 3446.82it/s]2022-07-08 15:58:38 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.target
2022-07-08 15:58:38 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]8690it [00:03, 2315.57it/s]347it [00:00, 3464.63it/s]1816it [00:00, 3341.36it/s]9053it [00:03, 2607.95it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]724it [00:00, 3642.35it/s]2191it [00:00, 3471.37it/s]9387it [00:03, 2744.94it/s]363it [00:00, 3624.59it/s]1089it [00:00, 3378.82it/s]2540it [00:00, 3283.27it/s]9752it [00:03, 2973.38it/s]728it [00:00, 3637.44it/s]1465it [00:00, 3520.53it/s]2917it [00:00, 3427.10it/s]10131it [00:04, 3188.99it/s]1092it [00:00, 3335.62it/s]3294it [00:00, 3527.34it/s]1819it [00:00, 3394.34it/s]10482it [00:04, 3172.47it/s]1452it [00:00, 3433.11it/s]2198it [00:00, 3521.18it/s]3649it [00:01, 3414.72it/s]10856it [00:04, 3328.28it/s]1798it [00:00, 3314.87it/s]4012it [00:01, 3476.99it/s]2552it [00:00, 3381.23it/s]11206it [00:04, 3268.61it/s]2169it [00:00, 3441.48it/s]2931it [00:00, 3503.49it/s]4362it [00:01, 3382.23it/s]11580it [00:04, 3400.14it/s]2532it [00:00, 3314.22it/s]3299it [00:00, 3555.94it/s]4718it [00:01, 3431.88it/s]11930it [00:04, 3311.79it/s]2882it [00:00, 3369.73it/s]3657it [00:01, 3434.43it/s]5063it [00:01, 3329.58it/s]12317it [00:04, 3468.57it/s]3254it [00:00, 3474.08it/s]4020it [00:01, 3491.30it/s]5436it [00:01, 3444.51it/s]12682it [00:04, 3375.52it/s]3604it [00:01, 3332.46it/s]5809it [00:01, 3525.53it/s]4371it [00:01, 3394.18it/s]13073it [00:04, 3525.98it/s]3968it [00:01, 3421.49it/s]4738it [00:01, 3473.41it/s]6163it [00:01, 3388.74it/s]13450it [00:05, 3595.02it/s]4313it [00:01, 3305.06it/s]6537it [00:01, 3489.28it/s]5087it [00:01, 3361.16it/s]13813it [00:05, 3456.35it/s]4679it [00:01, 3406.90it/s]5451it [00:01, 3439.11it/s]14199it [00:05, 3567.64it/s]5043it [00:01, 3472.30it/s]5815it [00:01, 3495.14it/s]14559it [00:05, 3410.53it/s]5392it [00:01, 3340.62it/s]6166it [00:01, 3306.86it/s]14928it [00:05, 3489.34it/s]5728it [00:01, 3339.81it/s]6500it [00:01, 3299.23it/s]15280it [00:05, 3250.06it/s]6064it [00:01, 3030.10it/s]15610it [00:05, 3175.18it/s]6398it [00:01, 3112.96it/s]15944it [00:05, 3219.19it/s]6732it [00:02, 3007.56it/s]6888it [00:02, 1048.57it/s]7269it [00:02, 1354.68it/s]7641it [00:03, 1646.82it/s]8019it [00:03, 1989.78it/s]6832it [00:02, 1031.63it/s]8398it [00:03, 2325.44it/s]7209it [00:02, 1342.52it/s]8740it [00:03, 2526.05it/s]7576it [00:02, 1667.38it/s]9118it [00:03, 2813.36it/s]7893it [00:03, 1916.94it/s]9467it [00:03, 2929.36it/s]16269it [00:06, 936.18it/s] 7037it [00:02, 924.61it/s] 8258it [00:03, 2249.07it/s]9849it [00:03, 3158.31it/s]16654it [00:06, 1242.58it/s]7410it [00:03, 1225.85it/s]8586it [00:03, 2449.64it/s]10203it [00:03, 3189.07it/s]16967it [00:06, 1488.99it/s]7704it [00:03, 1453.45it/s]8956it [00:03, 2740.50it/s]10585it [00:03, 3359.65it/s]17354it [00:07, 1862.86it/s]8073it [00:03, 1810.01it/s]9321it [00:03, 2864.77it/s]10964it [00:03, 3479.83it/s]17727it [00:07, 2204.42it/s]8445it [00:03, 2163.59it/s]9693it [00:03, 3082.18it/s]11328it [00:04, 3423.96it/s]18063it [00:07, 2430.17it/s]8769it [00:03, 2353.68it/s]10076it [00:03, 3280.88it/s]11705it [00:04, 3519.95it/s]18436it [00:07, 2724.86it/s]9144it [00:03, 2671.32it/s]10432it [00:03, 3237.33it/s]12066it [00:04, 3446.07it/s]18781it [00:07, 2847.22it/s]9479it [00:03, 2741.85it/s]10805it [00:03, 3371.33it/s]12445it [00:04, 3542.17it/s]19147it [00:07, 3055.03it/s]9851it [00:03, 2987.61it/s]11157it [00:04, 3299.80it/s]12804it [00:04, 3469.05it/s]19493it [00:07, 3103.54it/s]10188it [00:03, 3004.70it/s]11538it [00:04, 3441.24it/s]13197it [00:04, 3600.40it/s]19857it [00:07, 3249.55it/s]10567it [00:04, 3213.91it/s]11891it [00:04, 3336.24it/s]20238it [00:07, 3404.11it/s]13560it [00:04, 3503.83it/s]10932it [00:04, 3333.80it/s]12276it [00:04, 3479.92it/s]13941it [00:04, 3588.69it/s]20595it [00:07, 3304.36it/s]11282it [00:04, 3257.72it/s]12654it [00:04, 3563.92it/s]14329it [00:04, 3671.30it/s]20969it [00:08, 3424.84it/s]11649it [00:04, 3372.80it/s]13015it [00:04, 3412.50it/s]14698it [00:04, 3511.90it/s]21321it [00:08, 3261.71it/s]11995it [00:04, 3295.23it/s]13381it [00:04, 3481.74it/s]15052it [00:05, 3483.07it/s]21675it [00:08, 3337.45it/s]12346it [00:04, 3354.05it/s]13733it [00:04, 3295.83it/s]15402it [00:05, 3277.00it/s]22015it [00:08, 3176.50it/s]12687it [00:04, 3064.17it/s]15736it [00:05, 3294.10it/s]22338it [00:08, 3054.78it/s]14067it [00:04, 2855.33it/s]13002it [00:04, 3015.04it/s]22688it [00:08, 3175.81it/s]14365it [00:05, 2883.98it/s]16068it [00:05, 2934.28it/s]13352it [00:04, 3147.20it/s]23010it [00:08, 3143.01it/s]14739it [00:05, 3111.26it/s]13672it [00:04, 3091.42it/s]23381it [00:08, 3303.37it/s]15131it [00:05, 3334.20it/s]14052it [00:05, 3290.47it/s]23715it [00:08, 3257.03it/s]15473it [00:05, 3267.55it/s]14385it [00:05, 3268.04it/s]24082it [00:09, 3373.81it/s]15853it [00:05, 3416.50it/s]14769it [00:05, 3430.97it/s]24462it [00:09, 3496.35it/s]15154it [00:05, 3551.21it/s]24814it [00:09, 3409.89it/s]15512it [00:05, 3412.73it/s]25191it [00:09, 3512.04it/s]15871it [00:05, 3462.11it/s]16370it [00:06, 1042.56it/s]25544it [00:09, 3417.77it/s]16755it [00:06, 1373.32it/s]25923it [00:09, 3523.62it/s]17086it [00:06, 1651.98it/s]26277it [00:09, 3423.20it/s]17470it [00:06, 2025.52it/s]26645it [00:09, 3495.15it/s]17799it [00:06, 2272.85it/s]27026it [00:09, 3585.74it/s]18180it [00:06, 2607.66it/s]27386it [00:09, 3490.62it/s]18566it [00:06, 2904.54it/s]27767it [00:10, 3582.33it/s]16200it [00:06, 930.79it/s] 18919it [00:06, 2988.48it/s]16590it [00:06, 1226.24it/s]19309it [00:07, 3226.21it/s]16950it [00:06, 1510.57it/s]16220it [00:06, 992.75it/s] 19667it [00:07, 3234.56it/s]17329it [00:06, 1846.59it/s]16606it [00:06, 1297.29it/s]20054it [00:07, 3406.56it/s]17720it [00:06, 2208.80it/s]16950it [00:06, 1566.48it/s]20414it [00:07, 3372.10it/s]18065it [00:06, 2447.09it/s]17328it [00:06, 1914.45it/s]20796it [00:07, 3495.99it/s]18455it [00:07, 2769.42it/s]17713it [00:06, 2269.40it/s]21156it [00:07, 3410.70it/s]18812it [00:07, 2889.03it/s]18057it [00:07, 2474.02it/s]21538it [00:07, 3524.54it/s]19193it [00:07, 3119.84it/s]18439it [00:07, 2778.25it/s]21921it [00:07, 3604.71it/s]19550it [00:07, 3132.26it/s]18789it [00:07, 2879.73it/s]22286it [00:07, 3434.99it/s]19923it [00:07, 3290.93it/s]19151it [00:07, 3067.41it/s]20276it [00:07, 3312.96it/s]22634it [00:08, 3154.17it/s]19498it [00:07, 2973.63it/s]20625it [00:07, 3054.06it/s]22957it [00:08, 2901.31it/s]19832it [00:07, 3067.51it/s]20952it [00:07, 3109.80it/s]23288it [00:08, 3007.37it/s]20169it [00:07, 3147.66it/s]23640it [00:08, 3144.38it/s]21275it [00:07, 3023.60it/s]28127it [00:11, 701.38it/s] 20500it [00:07, 3094.42it/s]21646it [00:08, 3211.21it/s]23961it [00:08, 3137.46it/s]28500it [00:11, 929.93it/s]20874it [00:07, 3272.98it/s]24335it [00:08, 3304.60it/s]21990it [00:08, 3193.73it/s]28813it [00:11, 1144.02it/s]21211it [00:08, 3225.74it/s]24670it [00:08, 3253.96it/s]22342it [00:08, 3284.60it/s]29193it [00:11, 1468.02it/s]21585it [00:08, 3371.20it/s]25035it [00:08, 3367.71it/s]22729it [00:08, 3450.90it/s]29563it [00:11, 1798.97it/s]21961it [00:08, 3482.13it/s]23078it [00:08, 3398.89it/s]25375it [00:08, 3284.93it/s]29900it [00:12, 2043.79it/s]22314it [00:08, 3383.80it/s]23437it [00:08, 3452.72it/s]25746it [00:09, 3406.82it/s]30277it [00:12, 2384.47it/s]22680it [00:08, 3463.04it/s]26110it [00:09, 3472.74it/s]23785it [00:08, 3381.59it/s]30621it [00:12, 2569.66it/s]23029it [00:08, 3364.24it/s]24167it [00:08, 3507.91it/s]26459it [00:09, 3362.14it/s]30993it [00:12, 2817.88it/s]23393it [00:08, 3442.20it/s]26816it [00:09, 3421.25it/s]24520it [00:08, 3415.85it/s]31336it [00:12, 2907.42it/s]23740it [00:08, 3332.13it/s]27160it [00:09, 3386.94it/s]24897it [00:08, 3517.33it/s]31718it [00:12, 3143.11it/s]24115it [00:08, 3450.59it/s]27542it [00:09, 3513.05it/s]25251it [00:09, 3496.66it/s]32097it [00:12, 3316.87it/s]24488it [00:08, 3529.14it/s]25602it [00:09, 3415.95it/s]27895it [00:09, 3384.30it/s]32455it [00:12, 3290.08it/s]24843it [00:09, 3397.88it/s]25980it [00:09, 3511.08it/s]32824it [00:12, 3400.72it/s]25206it [00:09, 3462.64it/s]26333it [00:09, 3412.62it/s]33178it [00:13, 3295.75it/s]25554it [00:09, 3291.44it/s]26710it [00:09, 3514.25it/s]33551it [00:13, 3416.04it/s]25926it [00:09, 3412.51it/s]27063it [00:09, 3434.22it/s]33901it [00:13, 3363.89it/s]26270it [00:09, 3295.50it/s]27443it [00:09, 3538.14it/s]34275it [00:13, 3469.43it/s]26634it [00:09, 3391.56it/s]27821it [00:09, 3608.49it/s]34651it [00:13, 3551.03it/s]27006it [00:09, 3484.53it/s]35010it [00:13, 3432.54it/s]27357it [00:09, 3389.29it/s]35387it [00:13, 3527.62it/s]27731it [00:09, 3480.95it/s]35743it [00:13, 3396.13it/s]36118it [00:13, 3494.79it/s]36470it [00:14, 3299.67it/s]36811it [00:14, 3328.89it/s]37147it [00:14, 3177.47it/s]37468it [00:14, 3056.94it/s]28235it [00:11, 638.67it/s] 37800it [00:14, 3129.57it/s]28607it [00:11, 860.20it/s]38116it [00:14, 3137.67it/s]28921it [00:11, 1072.18it/s]38471it [00:14, 3254.42it/s]28183it [00:11, 831.07it/s] 29301it [00:11, 1392.29it/s]38848it [00:14, 3404.19it/s]28559it [00:11, 1089.29it/s]29621it [00:11, 1648.73it/s]39190it [00:14, 3318.14it/s]28867it [00:11, 1311.90it/s]30000it [00:11, 2012.23it/s]39571it [00:14, 3460.54it/s]29246it [00:11, 1653.18it/s]30379it [00:11, 2359.23it/s]39919it [00:15, 3241.55it/s]28081it [00:11, 743.98it/s] 29620it [00:11, 1947.09it/s]30728it [00:11, 2562.88it/s]40303it [00:15, 3408.01it/s]28452it [00:11, 985.35it/s]29996it [00:11, 2284.14it/s]31104it [00:12, 2842.13it/s]40648it [00:15, 3320.88it/s]28780it [00:11, 1216.04it/s]30366it [00:11, 2580.82it/s]31456it [00:12, 2943.67it/s]41027it [00:15, 3451.80it/s]29149it [00:11, 1534.64it/s]30715it [00:11, 2742.55it/s]31838it [00:12, 3168.97it/s]29516it [00:11, 1865.92it/s]41381it [00:15, 3366.05it/s]31090it [00:11, 2982.98it/s]32193it [00:12, 3195.55it/s]41745it [00:15, 3443.48it/s]29848it [00:11, 2101.68it/s]31441it [00:11, 3050.61it/s]32564it [00:12, 3336.30it/s]42125it [00:15, 3544.35it/s]30210it [00:11, 2410.82it/s]31813it [00:12, 3227.83it/s]32936it [00:12, 3442.95it/s]42482it [00:15, 3445.85it/s]30546it [00:12, 2574.80it/s]32164it [00:12, 3246.64it/s]33295it [00:12, 3373.95it/s]42855it [00:15, 3526.30it/s]30916it [00:12, 2844.51it/s]32539it [00:12, 3386.51it/s]33661it [00:12, 3453.99it/s]31288it [00:12, 3066.61it/s]32909it [00:12, 3473.83it/s]34015it [00:12, 3401.85it/s]31638it [00:12, 3083.93it/s]33268it [00:12, 3370.75it/s]34388it [00:12, 3493.87it/s]32008it [00:12, 3250.06it/s]33643it [00:12, 3476.78it/s]34742it [00:13, 3402.96it/s]32356it [00:12, 3218.16it/s]33997it [00:12, 3429.67it/s]35086it [00:13, 3329.11it/s]32720it [00:12, 3334.48it/s]34367it [00:12, 3505.60it/s]35466it [00:13, 3461.41it/s]33066it [00:12, 3213.86it/s]34721it [00:12, 3410.88it/s]35815it [00:13, 3389.84it/s]33434it [00:12, 3344.05it/s]35095it [00:13, 3503.91it/s]36192it [00:13, 3497.12it/s]33806it [00:12, 3450.92it/s]35470it [00:13, 3573.22it/s]36544it [00:13, 3337.47it/s]34157it [00:13, 3373.20it/s]35830it [00:13, 3384.90it/s]36901it [00:13, 3399.31it/s]34499it [00:13, 3372.72it/s]36172it [00:13, 3393.13it/s]37243it [00:13, 3182.54it/s]34840it [00:13, 3154.08it/s]36514it [00:13, 3224.31it/s]37572it [00:13, 3210.13it/s]35171it [00:13, 3194.72it/s]36849it [00:13, 3259.44it/s]37904it [00:14, 3240.34it/s]37178it [00:13, 3252.55it/s]35500it [00:13, 3068.43it/s]38231it [00:14, 3188.19it/s]37505it [00:13, 3225.59it/s]35854it [00:13, 3197.93it/s]38578it [00:14, 3268.67it/s]37879it [00:13, 3374.33it/s]36223it [00:13, 3337.78it/s]38907it [00:14, 3255.59it/s]38218it [00:13, 3351.29it/s]36560it [00:13, 3251.74it/s]39281it [00:14, 3389.02it/s]38571it [00:14, 3401.67it/s]36932it [00:13, 3383.30it/s]39655it [00:14, 3491.79it/s]38912it [00:14, 3356.48it/s]43210it [00:17, 561.13it/s] 37273it [00:14, 3283.20it/s]40006it [00:14, 3384.41it/s]39296it [00:14, 3496.62it/s]43581it [00:17, 756.57it/s]37638it [00:14, 3385.82it/s]40385it [00:14, 3499.74it/s]39672it [00:14, 3572.49it/s]43951it [00:18, 996.26it/s]38008it [00:14, 3474.26it/s]40737it [00:14, 3381.13it/s]40030it [00:14, 3449.89it/s]44271it [00:18, 1225.86it/s]38357it [00:14, 3352.95it/s]41112it [00:14, 3485.08it/s]40407it [00:14, 3541.39it/s]44651it [00:18, 1559.44it/s]38718it [00:14, 3425.57it/s]41463it [00:15, 3386.29it/s]44986it [00:18, 1828.45it/s]40763it [00:14, 3440.08it/s]39063it [00:14, 3275.75it/s]41840it [00:15, 3494.51it/s]45349it [00:18, 2155.09it/s]41141it [00:14, 3537.11it/s]39437it [00:14, 3405.77it/s]42211it [00:15, 3556.62it/s]45690it [00:18, 2377.25it/s]41496it [00:14, 3431.20it/s]39780it [00:14, 3313.78it/s]42568it [00:15, 3470.78it/s]46064it [00:18, 2682.11it/s]41873it [00:15, 3528.56it/s]40150it [00:14, 3423.73it/s]42918it [00:15, 3478.29it/s]46432it [00:18, 2924.07it/s]42228it [00:15, 3448.58it/s]40524it [00:14, 3512.87it/s]46785it [00:18, 2968.22it/s]42611it [00:15, 3557.94it/s]40877it [00:15, 3383.54it/s]47163it [00:18, 3180.17it/s]42979it [00:15, 3590.91it/s]41248it [00:15, 3476.85it/s]47514it [00:19, 3190.44it/s]41598it [00:15, 3359.98it/s]47890it [00:19, 3339.22it/s]41970it [00:15, 3461.47it/s]48242it [00:19, 3272.75it/s]42318it [00:15, 3306.51it/s]48624it [00:19, 3424.23it/s]42692it [00:15, 3427.03it/s]49006it [00:19, 3535.45it/s]43052it [00:15, 3475.24it/s]49367it [00:19, 3424.45it/s]49722it [00:19, 3457.88it/s]50072it [00:19, 3279.87it/s]50406it [00:19, 3294.55it/s]50739it [00:20, 3130.50it/s]51069it [00:20, 3176.02it/s]51428it [00:20, 3291.85it/s]51760it [00:20, 3246.75it/s]52138it [00:20, 3398.35it/s]43267it [00:17, 581.64it/s] 52480it [00:20, 3306.11it/s]43640it [00:17, 787.36it/s]52856it [00:20, 3436.18it/s]43340it [00:17, 625.87it/s] 43971it [00:17, 999.49it/s]53210it [00:20, 3338.74it/s]43716it [00:17, 839.35it/s]44355it [00:17, 1307.23it/s]53561it [00:20, 3387.67it/s]44040it [00:17, 1051.95it/s]44728it [00:17, 1631.83it/s]53939it [00:20, 3499.06it/s]44415it [00:17, 1355.14it/s]45066it [00:17, 1902.58it/s]44794it [00:17, 1692.06it/s]54291it [00:21, 3408.44it/s]45431it [00:17, 2226.59it/s]54666it [00:21, 3504.85it/s]45135it [00:17, 1959.66it/s]45775it [00:18, 2445.36it/s]43402it [00:17, 601.13it/s] 45510it [00:17, 2296.07it/s]55018it [00:21, 3388.37it/s]46138it [00:18, 2715.20it/s]43769it [00:17, 806.81it/s]55381it [00:21, 3457.02it/s]45857it [00:17, 2492.09it/s]46491it [00:18, 2842.23it/s]44078it [00:17, 1006.24it/s]46228it [00:17, 2769.93it/s]55730it [00:21, 3302.80it/s]46853it [00:18, 3039.37it/s]44442it [00:17, 1298.20it/s]46576it [00:17, 2881.24it/s]56110it [00:21, 3442.60it/s]47238it [00:18, 3254.69it/s]44810it [00:17, 1585.33it/s]46951it [00:18, 3103.42it/s]56485it [00:21, 3528.75it/s]47595it [00:18, 3223.90it/s]45184it [00:17, 1929.90it/s]47327it [00:18, 3278.41it/s]56840it [00:21, 3425.99it/s]47971it [00:18, 3370.71it/s]45556it [00:18, 2263.08it/s]47685it [00:18, 3253.70it/s]57216it [00:21, 3519.70it/s]48325it [00:18, 3306.15it/s]45900it [00:18, 2451.50it/s]48062it [00:18, 3396.00it/s]57570it [00:22, 3412.23it/s]48708it [00:18, 3450.63it/s]46268it [00:18, 2729.05it/s]48418it [00:18, 3346.39it/s]57945it [00:22, 3507.03it/s]49063it [00:18, 3364.45it/s]46612it [00:18, 2818.46it/s]48789it [00:18, 3448.32it/s]58298it [00:22, 3380.36it/s]49439it [00:19, 3475.38it/s]46980it [00:18, 3033.97it/s]49143it [00:18, 3387.54it/s]58659it [00:22, 3445.11it/s]49821it [00:19, 3573.66it/s]47330it [00:18, 3068.85it/s]49518it [00:18, 3490.99it/s]59022it [00:22, 3497.52it/s]50183it [00:19, 3466.78it/s]47696it [00:18, 3226.34it/s]49872it [00:18, 3369.83it/s]59374it [00:22, 3352.19it/s]50551it [00:19, 3519.05it/s]48069it [00:18, 3365.46it/s]50225it [00:19, 3414.76it/s]59736it [00:22, 3427.04it/s]50906it [00:19, 3247.28it/s]48422it [00:18, 3206.15it/s]50570it [00:19, 3396.23it/s]60081it [00:22, 3210.42it/s]51237it [00:19, 3171.91it/s]48756it [00:19, 3230.21it/s]60409it [00:22, 3227.95it/s]50912it [00:19, 3050.48it/s]51558it [00:19, 3053.92it/s]49088it [00:19, 3044.45it/s]60736it [00:22, 3238.42it/s]51250it [00:19, 3130.52it/s]51885it [00:19, 3112.60it/s]49423it [00:19, 3127.71it/s]61062it [00:23, 3122.10it/s]51570it [00:19, 3048.14it/s]52255it [00:19, 3277.27it/s]49792it [00:19, 3285.10it/s]61424it [00:23, 3261.95it/s]51945it [00:19, 3242.95it/s]52586it [00:20, 3246.05it/s]50126it [00:19, 3228.40it/s]52321it [00:19, 3388.28it/s]52963it [00:20, 3396.98it/s]50491it [00:19, 3348.68it/s]52664it [00:19, 3293.95it/s]53305it [00:20, 3303.27it/s]50830it [00:19, 3255.00it/s]53044it [00:19, 3437.04it/s]53671it [00:20, 3405.48it/s]51205it [00:19, 3394.37it/s]53391it [00:19, 3365.50it/s]54051it [00:20, 3354.41it/s]51547it [00:19, 3287.62it/s]53763it [00:20, 3466.93it/s]54429it [00:20, 3474.66it/s]51917it [00:19, 3405.36it/s]54112it [00:20, 3364.53it/s]54793it [00:20, 3519.91it/s]52287it [00:20, 3490.30it/s]54488it [00:20, 3475.48it/s]55147it [00:20, 3415.95it/s]52638it [00:20, 3358.92it/s]54861it [00:20, 3548.57it/s]55516it [00:20, 3492.94it/s]53007it [00:20, 3452.24it/s]55218it [00:20, 3440.48it/s]55867it [00:20, 3403.96it/s]53355it [00:20, 3325.80it/s]55573it [00:20, 3469.73it/s]56209it [00:21, 3336.66it/s]53719it [00:20, 3413.87it/s]55922it [00:20, 3382.11it/s]56571it [00:21, 3303.24it/s]54063it [00:20, 3295.64it/s]56302it [00:20, 3500.87it/s]56953it [00:21, 3448.91it/s]54437it [00:20, 3419.94it/s]56654it [00:20, 3413.06it/s]57337it [00:21, 3560.97it/s]54803it [00:20, 3487.37it/s]57037it [00:21, 3531.98it/s]57695it [00:21, 3459.37it/s]55154it [00:20, 3353.50it/s]57410it [00:21, 3438.90it/s]58073it [00:21, 3550.57it/s]55515it [00:21, 3426.60it/s]57786it [00:21, 3530.49it/s]58430it [00:21, 3442.54it/s]55860it [00:21, 3304.89it/s]58161it [00:21, 3593.54it/s]58805it [00:21, 3530.47it/s]56231it [00:21, 3418.15it/s]58522it [00:21, 3474.03it/s]59160it [00:21, 3431.89it/s]56575it [00:21, 3309.30it/s]58891it [00:21, 3535.49it/s]59513it [00:22, 3458.76it/s]56947it [00:21, 3425.18it/s]59246it [00:21, 3430.18it/s]59884it [00:22, 3531.20it/s]61753it [00:25, 471.65it/s] 57321it [00:21, 3515.67it/s]59608it [00:21, 3482.92it/s]62120it [00:25, 651.25it/s]60239it [00:22, 3368.44it/s]57675it [00:21, 3379.19it/s]59958it [00:21, 3362.56it/s]62475it [00:25, 866.49it/s]60584it [00:22, 3389.93it/s]58015it [00:21, 3345.81it/s]60296it [00:21, 3293.02it/s]62774it [00:25, 1058.14it/s]60925it [00:22, 2919.28it/s]60627it [00:22, 3295.28it/s]58351it [00:21, 3098.38it/s]63096it [00:25, 1315.90it/s]61262it [00:22, 3029.43it/s]58681it [00:22, 3151.89it/s]60958it [00:22, 3140.05it/s]63395it [00:25, 1538.91it/s]61605it [00:22, 3137.44it/s]59024it [00:22, 3229.84it/s]61319it [00:22, 3271.67it/s]63760it [00:25, 1898.02it/s]59350it [00:22, 3173.43it/s]61649it [00:22, 3227.05it/s]64130it [00:26, 2249.82it/s]59715it [00:22, 3307.73it/s]64461it [00:26, 2435.22it/s]60048it [00:22, 3219.25it/s]64833it [00:26, 2735.61it/s]60412it [00:22, 3339.45it/s]65171it [00:26, 2840.53it/s]60770it [00:22, 3248.00it/s]65537it [00:26, 3052.29it/s]61137it [00:22, 3366.90it/s]65879it [00:26, 3076.54it/s]61500it [00:22, 3442.43it/s]66250it [00:26, 3250.09it/s]66624it [00:26, 3387.67it/s]66977it [00:26, 3286.45it/s]67351it [00:26, 3411.95it/s]67701it [00:27, 3337.15it/s]68075it [00:27, 3451.08it/s]68425it [00:27, 3359.45it/s]68801it [00:27, 3473.47it/s]69171it [00:27, 3537.78it/s]69528it [00:27, 3417.31it/s]69892it [00:27, 3480.28it/s]70242it [00:27, 3389.20it/s]70615it [00:27, 3486.38it/s]61974it [00:24, 509.74it/s] 70966it [00:28, 3356.43it/s]62354it [00:24, 709.90it/s]71337it [00:28, 3455.90it/s]62672it [00:24, 907.27it/s]61927it [00:25, 439.86it/s] 71698it [00:28, 3498.02it/s]62998it [00:24, 1148.13it/s]62293it [00:25, 609.54it/s]72050it [00:28, 3324.29it/s]63359it [00:24, 1432.34it/s]62589it [00:25, 774.08it/s]72385it [00:28, 3311.99it/s]63685it [00:24, 1709.28it/s]62916it [00:25, 1000.87it/s]64017it [00:24, 1995.11it/s]63238it [00:25, 1255.60it/s]72718it [00:28, 2928.98it/s]61846it [00:24, 517.73it/s] 64334it [00:25, 2175.68it/s]73058it [00:28, 3053.65it/s]63540it [00:25, 1467.24it/s]62160it [00:24, 672.13it/s]64689it [00:25, 2475.09it/s]73408it [00:28, 3176.40it/s]63910it [00:25, 1833.47it/s]62519it [00:25, 887.44it/s]73733it [00:28, 3174.23it/s]65039it [00:25, 2660.51it/s]64223it [00:25, 2075.52it/s]62887it [00:25, 1162.85it/s]74100it [00:29, 3313.98it/s]65408it [00:25, 2917.43it/s]64592it [00:25, 2418.54it/s]63257it [00:25, 1475.92it/s]65777it [00:25, 3119.80it/s]74436it [00:29, 3269.19it/s]64958it [00:25, 2707.31it/s]63583it [00:25, 1721.35it/s]74810it [00:29, 3405.21it/s]66122it [00:25, 3147.27it/s]65298it [00:26, 2820.40it/s]63952it [00:25, 2064.59it/s]66482it [00:25, 3271.63it/s]65666it [00:26, 3041.57it/s]75153it [00:29, 3299.82it/s]64285it [00:25, 2270.13it/s]66827it [00:25, 3246.75it/s]75525it [00:29, 3419.40it/s]66009it [00:26, 3072.08it/s]64652it [00:25, 2575.43it/s]67203it [00:25, 3391.43it/s]75896it [00:29, 3501.54it/s]66370it [00:26, 3219.04it/s]65018it [00:25, 2831.80it/s]67559it [00:26, 3314.21it/s]76248it [00:29, 3382.26it/s]66721it [00:26, 3195.38it/s]65364it [00:25, 2869.96it/s]67939it [00:26, 3451.17it/s]76619it [00:29, 3474.08it/s]67097it [00:26, 3351.38it/s]65725it [00:26, 3058.94it/s]68308it [00:26, 3518.75it/s]67468it [00:26, 3452.52it/s]76969it [00:29, 3374.49it/s]66065it [00:26, 3054.82it/s]77342it [00:29, 3476.33it/s]68665it [00:26, 3412.13it/s]67822it [00:26, 3355.23it/s]66426it [00:26, 3204.01it/s]69033it [00:26, 3487.08it/s]68191it [00:26, 3447.11it/s]77692it [00:30, 3382.14it/s]66765it [00:26, 3159.15it/s]78054it [00:30, 3449.48it/s]69385it [00:26, 3353.04it/s]68541it [00:27, 3355.44it/s]67133it [00:26, 3303.34it/s]78426it [00:30, 3527.15it/s]69755it [00:26, 3449.83it/s]68917it [00:27, 3469.83it/s]67494it [00:26, 3390.69it/s]78780it [00:30, 3415.53it/s]70103it [00:26, 3377.52it/s]69268it [00:27, 3339.60it/s]67841it [00:26, 3294.67it/s]79150it [00:30, 3495.66it/s]70480it [00:26, 3488.29it/s]69640it [00:27, 3445.96it/s]68204it [00:26, 3388.80it/s]70849it [00:26, 3538.36it/s]79501it [00:30, 3392.71it/s]70015it [00:27, 3532.87it/s]68548it [00:26, 3280.62it/s]79876it [00:30, 3495.30it/s]71205it [00:27, 3420.38it/s]70371it [00:27, 3421.17it/s]68915it [00:26, 3391.36it/s]71577it [00:27, 3504.57it/s]80227it [00:30, 3392.91it/s]70738it [00:27, 3490.14it/s]69258it [00:27, 3273.05it/s]80603it [00:30, 3496.69it/s]71929it [00:27, 3405.28it/s]71089it [00:27, 3368.33it/s]69623it [00:27, 3377.73it/s]80964it [00:31, 3529.21it/s]72297it [00:27, 3484.01it/s]71463it [00:27, 3472.32it/s]69979it [00:27, 3429.98it/s]81319it [00:31, 3399.56it/s]72647it [00:27, 3302.24it/s]71813it [00:27, 3315.13it/s]70324it [00:27, 3320.78it/s]81678it [00:31, 3452.78it/s]73022it [00:27, 3427.01it/s]72167it [00:28, 3376.91it/s]70658it [00:27, 3141.09it/s]73368it [00:27, 3412.50it/s]82025it [00:31, 3263.40it/s]72507it [00:28, 3354.03it/s]82355it [00:31, 3253.97it/s]73711it [00:27, 3223.60it/s]70975it [00:27, 2944.10it/s]72844it [00:28, 3140.25it/s]74045it [00:27, 3255.24it/s]73162it [00:28, 3133.77it/s]71273it [00:27, 2847.61it/s]82683it [00:31, 3000.79it/s]74373it [00:28, 3141.82it/s]71604it [00:27, 2972.15it/s]83033it [00:31, 3135.60it/s]73478it [00:28, 3039.41it/s]74738it [00:28, 3283.73it/s]83396it [00:31, 3274.15it/s]71905it [00:27, 2934.58it/s]73860it [00:28, 3257.62it/s]75092it [00:28, 3355.91it/s]72262it [00:28, 3113.81it/s]83728it [00:31, 3245.14it/s]74233it [00:28, 3391.88it/s]75430it [00:28, 3273.66it/s]84095it [00:31, 3366.90it/s]72599it [00:28, 3097.12it/s]74575it [00:28, 3304.83it/s]75798it [00:28, 3389.02it/s]84435it [00:32, 3304.04it/s]72977it [00:28, 3290.51it/s]74946it [00:28, 3421.21it/s]84805it [00:32, 3417.07it/s]73341it [00:28, 3389.80it/s]76139it [00:28, 3298.42it/s]75291it [00:29, 3327.10it/s]85176it [00:32, 3501.83it/s]76501it [00:28, 3390.84it/s]73682it [00:28, 3303.07it/s]75661it [00:29, 3432.89it/s]76842it [00:28, 3305.57it/s]74058it [00:28, 3433.97it/s]76006it [00:29, 3310.44it/s]77206it [00:28, 3400.16it/s]74404it [00:28, 3280.86it/s]76373it [00:29, 3410.94it/s]77571it [00:28, 3470.36it/s]74769it [00:28, 3378.65it/s]76740it [00:29, 3485.56it/s]77920it [00:29, 3353.49it/s]77091it [00:29, 3367.62it/s]75119it [00:28, 3272.17it/s]78292it [00:29, 3457.95it/s]77455it [00:29, 3444.95it/s]75483it [00:29, 3375.09it/s]78640it [00:29, 3370.25it/s]75849it [00:29, 3454.63it/s]77802it [00:29, 3353.35it/s]79009it [00:29, 3462.06it/s]78173it [00:29, 3455.23it/s]76197it [00:29, 3306.79it/s]79357it [00:29, 3371.36it/s]76560it [00:29, 3397.99it/s]78520it [00:29, 3371.84it/s]79728it [00:29, 3468.24it/s]78877it [00:30, 3427.29it/s]76902it [00:29, 3282.18it/s]80096it [00:29, 3529.27it/s]79248it [00:30, 3507.54it/s]77266it [00:29, 3383.35it/s]80450it [00:29, 3423.20it/s]77630it [00:29, 3456.82it/s]79600it [00:30, 3385.79it/s]80804it [00:29, 3455.82it/s]79973it [00:30, 3482.43it/s]77978it [00:29, 3302.95it/s]81151it [00:30, 3311.42it/s]80323it [00:30, 3354.05it/s]78336it [00:29, 3379.65it/s]81500it [00:30, 3360.49it/s]80698it [00:30, 3466.25it/s]78676it [00:30, 3221.81it/s]81840it [00:30, 3260.34it/s]81047it [00:30, 3359.51it/s]79036it [00:30, 3327.73it/s]82206it [00:30, 3372.60it/s]81406it [00:30, 3422.81it/s]79372it [00:30, 3231.45it/s]82564it [00:30, 3431.49it/s]81750it [00:30, 3408.41it/s]79702it [00:30, 3248.41it/s]82909it [00:30, 3220.35it/s]80029it [00:30, 3248.97it/s]82092it [00:31, 3190.28it/s]83235it [00:30, 3130.68it/s]82415it [00:31, 3200.15it/s]80355it [00:30, 3076.06it/s]83551it [00:30, 2951.63it/s]80670it [00:30, 3094.90it/s]82738it [00:31, 3024.28it/s]83901it [00:30, 3099.87it/s]83101it [00:31, 3185.45it/s]80999it [00:30, 3003.93it/s]84270it [00:31, 3264.95it/s]83470it [00:31, 3326.10it/s]81349it [00:30, 3140.05it/s]84601it [00:31, 3207.36it/s]81709it [00:30, 3269.86it/s]83806it [00:31, 3257.47it/s]84971it [00:31, 3347.76it/s]85528it [00:34, 426.31it/s] 84163it [00:31, 3346.79it/s]82039it [00:31, 3200.99it/s]85894it [00:34, 583.64it/s]82399it [00:31, 3309.45it/s]84500it [00:31, 3279.63it/s]86179it [00:35, 729.52it/s]84859it [00:31, 3368.99it/s]82732it [00:31, 3220.15it/s]86549it [00:35, 981.30it/s]85201it [00:31, 3289.11it/s]83082it [00:31, 3297.96it/s]86917it [00:35, 1272.62it/s]83444it [00:31, 3390.73it/s]87244it [00:35, 1529.32it/s]83785it [00:31, 3269.54it/s]87621it [00:35, 1886.47it/s]84146it [00:31, 3367.16it/s]87960it [00:35, 2136.06it/s]84485it [00:31, 3262.07it/s]88333it [00:35, 2465.58it/s]84845it [00:31, 3358.68it/s]88677it [00:35, 2645.60it/s]85199it [00:32, 3249.62it/s]89043it [00:35, 2890.36it/s]89414it [00:35, 3099.63it/s]89767it [00:36, 3122.35it/s]90139it [00:36, 3282.48it/s]90490it [00:36, 3223.46it/s]90852it [00:36, 3331.47it/s]91198it [00:36, 3243.23it/s]91553it [00:36, 3328.85it/s]91909it [00:36, 3394.46it/s]92254it [00:36, 3193.79it/s]92579it [00:36, 3088.81it/s]92893it [00:37, 2961.20it/s]93193it [00:37, 2664.62it/s]93488it [00:37, 2737.09it/s]93782it [00:37, 2791.30it/s]85309it [00:33, 400.20it/s] 94133it [00:37, 2990.26it/s]85672it [00:33, 552.15it/s]94488it [00:37, 3149.54it/s]86043it [00:34, 750.96it/s]94808it [00:37, 3101.51it/s]86355it [00:34, 946.30it/s]95166it [00:37, 3237.09it/s]86725it [00:34, 1236.45it/s]95493it [00:37, 3158.46it/s]87051it [00:34, 1488.96it/s]95852it [00:38, 3281.41it/s]85532it [00:34, 359.98it/s] 87420it [00:34, 1833.11it/s]96189it [00:38, 3194.14it/s]85900it [00:35, 502.84it/s]87788it [00:34, 2106.99it/s]96547it [00:38, 3302.24it/s]86222it [00:35, 660.28it/s]88158it [00:34, 2428.00it/s]96897it [00:38, 3358.94it/s]86595it [00:35, 894.89it/s]88521it [00:34, 2697.18it/s]97235it [00:38, 3253.16it/s]85526it [00:34, 395.15it/s] 86950it [00:35, 1141.76it/s]88869it [00:34, 2789.33it/s]97592it [00:38, 3341.85it/s]85888it [00:34, 546.16it/s]87324it [00:35, 1458.17it/s]89241it [00:35, 3021.02it/s]97928it [00:38, 3241.47it/s]86189it [00:34, 700.67it/s]87700it [00:35, 1798.92it/s]89587it [00:35, 3048.57it/s]98286it [00:38, 3338.49it/s]86555it [00:34, 943.85it/s]88045it [00:35, 2058.94it/s]89958it [00:35, 3217.63it/s]98641it [00:38, 3399.23it/s]86914it [00:35, 1221.91it/s]88420it [00:35, 2390.54it/s]90308it [00:35, 3160.25it/s]98983it [00:38, 3282.68it/s]87236it [00:35, 1469.22it/s]88768it [00:35, 2583.69it/s]90665it [00:35, 3270.38it/s]99339it [00:39, 3361.04it/s]87605it [00:35, 1815.35it/s]89141it [00:35, 2853.06it/s]91014it [00:35, 3331.86it/s]87937it [00:35, 2056.41it/s]99677it [00:39, 3233.82it/s]89491it [00:36, 2945.24it/s]91357it [00:35, 3247.03it/s]88301it [00:35, 2378.51it/s]100035it [00:39, 3332.05it/s]89865it [00:36, 3150.65it/s]91712it [00:35, 3332.37it/s]88637it [00:35, 2527.12it/s]100389it [00:39, 3220.63it/s]90234it [00:36, 3295.14it/s]92051it [00:35, 3241.73it/s]89004it [00:35, 2799.49it/s]100748it [00:39, 3322.18it/s]90590it [00:36, 3211.80it/s]92402it [00:35, 3317.68it/s]89367it [00:35, 3010.30it/s]101096it [00:39, 3365.09it/s]90941it [00:36, 3287.77it/s]92752it [00:36, 3369.50it/s]89712it [00:35, 2958.98it/s]101435it [00:39, 3054.39it/s]91284it [00:36, 3149.21it/s]93092it [00:36, 3127.90it/s]90039it [00:36, 2915.69it/s]101747it [00:39, 3068.59it/s]91610it [00:36, 3142.91it/s]102059it [00:39, 3077.76it/s]93410it [00:36, 2853.89it/s]90352it [00:36, 2803.43it/s]91932it [00:36, 3032.08it/s]90660it [00:36, 2874.67it/s]93703it [00:36, 2738.00it/s]102370it [00:40, 2904.50it/s]92241it [00:36, 2925.25it/s]91006it [00:36, 3033.63it/s]94052it [00:36, 2936.96it/s]102718it [00:40, 3062.18it/s]92597it [00:37, 3097.99it/s]94405it [00:36, 3098.50it/s]91320it [00:36, 2984.70it/s]103029it [00:40, 3025.85it/s]92912it [00:37, 3063.06it/s]91671it [00:36, 3130.19it/s]94721it [00:36, 3065.18it/s]103386it [00:40, 3180.00it/s]93271it [00:37, 3210.99it/s]95073it [00:36, 3193.28it/s]103741it [00:40, 3284.46it/s]91990it [00:36, 3056.03it/s]93630it [00:37, 3319.32it/s]95396it [00:36, 3134.48it/s]92342it [00:36, 3187.74it/s]104072it [00:40, 3193.67it/s]93965it [00:37, 3240.94it/s]95746it [00:37, 3238.64it/s]92679it [00:36, 3239.74it/s]104427it [00:40, 3296.08it/s]94326it [00:37, 3347.42it/s]96097it [00:37, 3316.49it/s]93006it [00:36, 3137.22it/s]104759it [00:40, 3177.22it/s]94663it [00:37, 3257.01it/s]96431it [00:37, 3214.85it/s]93359it [00:37, 3248.39it/s]105114it [00:40, 3281.51it/s]95023it [00:37, 3355.86it/s]96782it [00:37, 3298.79it/s]93687it [00:37, 3141.26it/s]105444it [00:41, 3177.77it/s]95361it [00:37, 3258.34it/s]97114it [00:37, 3194.58it/s]94040it [00:37, 3251.29it/s]105800it [00:41, 3285.35it/s]95689it [00:37, 3196.50it/s]97467it [00:37, 3289.43it/s]94395it [00:37, 3335.98it/s]106157it [00:41, 3366.58it/s]96048it [00:38, 3308.48it/s]97818it [00:37, 3352.74it/s]94731it [00:37, 3203.83it/s]106496it [00:41, 3245.49it/s]96381it [00:38, 3227.19it/s]98155it [00:37, 3239.54it/s]95080it [00:37, 3284.31it/s]106853it [00:41, 3337.94it/s]96740it [00:38, 3330.49it/s]98503it [00:37, 3308.71it/s]95411it [00:37, 3159.12it/s]107189it [00:41, 3221.56it/s]97075it [00:38, 3238.91it/s]98836it [00:38, 3213.84it/s]95759it [00:37, 3248.78it/s]107538it [00:41, 3296.12it/s]97435it [00:38, 3342.31it/s]99187it [00:38, 3297.35it/s]96109it [00:37, 3320.30it/s]107896it [00:41, 3376.95it/s]97793it [00:38, 3410.99it/s]99538it [00:38, 3358.66it/s]96443it [00:38, 3192.19it/s]108236it [00:41, 3254.09it/s]98136it [00:38, 3297.11it/s]99875it [00:38, 3238.82it/s]96792it [00:38, 3277.07it/s]108593it [00:41, 3343.56it/s]98494it [00:38, 3377.47it/s]100224it [00:38, 3308.64it/s]97122it [00:38, 3154.22it/s]108929it [00:42, 3231.43it/s]98834it [00:38, 3276.03it/s]100557it [00:38, 3203.25it/s]97456it [00:38, 3206.80it/s]109286it [00:42, 3327.31it/s]99194it [00:39, 3368.76it/s]100902it [00:38, 3272.93it/s]97806it [00:38, 3289.94it/s]109629it [00:42, 3222.67it/s]99538it [00:39, 3387.38it/s]101231it [00:38, 3122.84it/s]98137it [00:38, 3172.22it/s]109973it [00:42, 3282.96it/s]99878it [00:39, 3264.98it/s]101568it [00:38, 3190.41it/s]98487it [00:38, 3265.13it/s]110320it [00:42, 3335.97it/s]100227it [00:39, 3329.22it/s]101903it [00:38, 3235.56it/s]98816it [00:38, 3087.47it/s]110655it [00:42, 3138.01it/s]100562it [00:39, 3107.40it/s]102229it [00:39, 3004.06it/s]99130it [00:38, 3101.48it/s]110972it [00:42, 3121.93it/s]100877it [00:39, 3118.70it/s]102542it [00:39, 3038.88it/s]99443it [00:39, 3092.29it/s]111287it [00:42, 3128.74it/s]101192it [00:39, 3121.22it/s]102849it [00:39, 3018.78it/s]99754it [00:39, 2934.09it/s]111602it [00:42, 2959.26it/s]101506it [00:39, 3020.31it/s]103153it [00:39, 3000.54it/s]100103it [00:39, 3088.80it/s]111954it [00:43, 3115.95it/s]101861it [00:39, 3168.03it/s]103470it [00:39, 3047.98it/s]100415it [00:39, 3013.69it/s]112269it [00:43, 3065.43it/s]102180it [00:39, 3129.82it/s]103776it [00:39, 3036.89it/s]100766it [00:39, 3153.98it/s]112613it [00:43, 3170.88it/s]102526it [00:40, 3223.78it/s]104137it [00:39, 3203.53it/s]101116it [00:39, 3252.57it/s]112969it [00:43, 3283.25it/s]102884it [00:40, 3326.02it/s]104494it [00:39, 3311.52it/s]113300it [00:43, 3191.64it/s]101444it [00:39, 3131.13it/s]103218it [00:40, 3236.45it/s]104826it [00:39, 3228.51it/s]113656it [00:43, 3296.64it/s]101790it [00:39, 3222.83it/s]103575it [00:40, 3331.29it/s]105184it [00:39, 3328.79it/s]113988it [00:43, 3200.29it/s]102115it [00:39, 3081.58it/s]103910it [00:40, 3240.68it/s]105518it [00:40, 3237.81it/s]114346it [00:43, 3309.19it/s]102465it [00:39, 3199.26it/s]104268it [00:40, 3336.83it/s]105877it [00:40, 3337.43it/s]102815it [00:40, 3283.95it/s]114679it [00:43, 3207.58it/s]104603it [00:40, 3242.15it/s]106225it [00:40, 3376.77it/s]103146it [00:40, 3161.90it/s]104962it [00:40, 3341.52it/s]106564it [00:40, 3282.09it/s]103496it [00:40, 3257.60it/s]105309it [00:40, 3376.49it/s]106906it [00:40, 3319.74it/s]103824it [00:40, 3138.95it/s]105648it [00:41, 3278.68it/s]107239it [00:40, 3245.24it/s]104174it [00:40, 3241.09it/s]106006it [00:41, 3365.16it/s]107599it [00:40, 3341.37it/s]104522it [00:40, 3308.26it/s]106344it [00:41, 3272.97it/s]107949it [00:40, 3228.41it/s]104855it [00:40, 3180.79it/s]106704it [00:41, 3365.42it/s]108310it [00:40, 3336.22it/s]105204it [00:40, 3267.55it/s]107064it [00:41, 3433.29it/s]108671it [00:41, 3412.79it/s]105533it [00:40, 3147.73it/s]107409it [00:41, 3318.70it/s]109014it [00:41, 3236.63it/s]105882it [00:41, 3243.76it/s]107758it [00:41, 3367.12it/s]109374it [00:41, 3339.49it/s]106231it [00:41, 3313.94it/s]108096it [00:41, 3270.38it/s]109711it [00:41, 3259.00it/s]106564it [00:41, 3182.55it/s]108456it [00:41, 3364.06it/s]110071it [00:41, 3355.05it/s]106901it [00:41, 3235.69it/s]108794it [00:41, 3279.80it/s]110431it [00:41, 3423.98it/s]109150it [00:42, 3360.17it/s]107227it [00:41, 3145.50it/s]110775it [00:41, 3223.34it/s]109502it [00:42, 3405.62it/s]107582it [00:41, 3260.88it/s]111101it [00:41, 3036.19it/s]107910it [00:41, 3246.56it/s]109844it [00:42, 3101.97it/s]108236it [00:41, 3011.47it/s]110160it [00:42, 2823.01it/s]111409it [00:42, 2302.11it/s]108545it [00:41, 3031.30it/s]110450it [00:42, 2778.05it/s]111701it [00:42, 2442.31it/s]108851it [00:41, 2919.88it/s]110733it [00:42, 2751.46it/s]112031it [00:42, 2645.38it/s]109200it [00:42, 3078.48it/s]111056it [00:42, 2882.67it/s]112320it [00:42, 2708.45it/s]109552it [00:42, 3204.21it/s]111348it [00:42, 2883.73it/s]112679it [00:42, 2945.55it/s]109875it [00:42, 3108.12it/s]111702it [00:42, 3071.47it/s]112989it [00:42, 2922.49it/s]110229it [00:42, 3230.93it/s]112059it [00:43, 3214.28it/s]113346it [00:42, 3103.41it/s]110555it [00:42, 3135.23it/s]112383it [00:43, 3137.35it/s]113695it [00:42, 3211.93it/s]110911it [00:42, 3255.07it/s]112740it [00:43, 3261.63it/s]114023it [00:42, 3159.06it/s]111262it [00:42, 3327.85it/s]113068it [00:43, 3147.41it/s]114370it [00:42, 3247.77it/s]111597it [00:42, 3154.38it/s]113416it [00:43, 3241.99it/s]114699it [00:43, 3174.75it/s]111949it [00:42, 3256.45it/s]113763it [00:43, 3307.41it/s]112278it [00:43, 3157.11it/s]114096it [00:43, 3208.52it/s]112631it [00:43, 3261.28it/s]114447it [00:43, 3295.03it/s]112982it [00:43, 3331.83it/s]113317it [00:43, 3207.40it/s]113671it [00:43, 3302.37it/s]114004it [00:43, 3190.03it/s]115002it [00:47, 292.41it/s] 114358it [00:43, 3289.91it/s]115361it [00:47, 411.00it/s]114689it [00:43, 3177.28it/s]115656it [00:47, 535.02it/s]116020it [00:47, 737.56it/s]116382it [00:47, 983.20it/s]116702it [00:47, 1215.86it/s]117052it [00:48, 1520.64it/s]117377it [00:48, 1753.56it/s]117721it [00:48, 2060.78it/s]118043it [00:48, 2291.61it/s]118363it [00:48, 2386.76it/s]118681it [00:48, 2574.46it/s]118989it [00:48, 2664.31it/s]119343it [00:48, 2892.21it/s]119692it [00:48, 3053.79it/s]120020it [00:49, 3044.81it/s]120382it [00:49, 3205.45it/s]120715it [00:49, 3147.88it/s]121075it [00:49, 3274.41it/s]121434it [00:49, 3364.77it/s]121776it [00:49, 3254.15it/s]122126it [00:49, 3322.46it/s]122462it [00:49, 3236.82it/s]122822it [00:49, 3339.30it/s]123159it [00:49, 3247.91it/s]123521it [00:50, 3354.23it/s]123882it [00:50, 3426.67it/s]124227it [00:50, 3307.66it/s]124587it [00:50, 3390.41it/s]124928it [00:50, 3244.22it/s]125284it [00:50, 3331.48it/s]125644it [00:50, 3408.61it/s]125987it [00:50, 3292.76it/s]126347it [00:50, 3378.26it/s]126687it [00:51, 3197.35it/s]127036it [00:51, 3279.36it/s]127367it [00:51, 3064.04it/s]115020it [00:47, 228.39it/s] 127678it [00:51, 3058.23it/s]115247it [00:47, 285.83it/s]127993it [00:51, 3083.74it/s]114778it [00:48, 238.70it/s] 115465it [00:47, 359.74it/s]115009it [00:47, 265.21it/s] 128304it [00:51, 2996.31it/s]115118it [00:48, 331.27it/s]115735it [00:47, 483.77it/s]115358it [00:47, 371.03it/s]128659it [00:51, 3153.66it/s]115472it [00:48, 460.13it/s]116082it [00:48, 693.96it/s]115635it [00:47, 478.47it/s]129015it [00:51, 3270.51it/s]115767it [00:48, 595.20it/s]116417it [00:48, 926.55it/s]115988it [00:48, 661.88it/s]116126it [00:48, 810.58it/s]129344it [00:51, 3190.49it/s]116768it [00:48, 1221.91it/s]116339it [00:48, 887.14it/s]129699it [00:51, 3293.32it/s]116439it [00:48, 1021.81it/s]117117it [00:48, 1539.59it/s]116650it [00:48, 1102.74it/s]116800it [00:48, 1324.56it/s]130030it [00:52, 3210.24it/s]117431it [00:48, 1787.69it/s]117002it [00:48, 1406.20it/s]117161it [00:48, 1651.17it/s]130386it [00:52, 3309.57it/s]117782it [00:48, 2116.16it/s]117320it [00:48, 1649.29it/s]117496it [00:49, 1902.57it/s]130719it [00:52, 3165.96it/s]118103it [00:48, 2302.35it/s]117671it [00:48, 1976.85it/s]117846it [00:49, 2208.59it/s]131076it [00:52, 3278.94it/s]118452it [00:48, 2575.05it/s]118022it [00:48, 2284.69it/s]118178it [00:49, 2384.93it/s]131431it [00:52, 3355.78it/s]118798it [00:48, 2794.01it/s]118352it [00:48, 2403.32it/s]118526it [00:49, 2636.86it/s]131769it [00:52, 3240.44it/s]119128it [00:49, 2835.25it/s]118702it [00:48, 2659.30it/s]118883it [00:49, 2866.35it/s]132126it [00:52, 3334.62it/s]119473it [00:49, 2997.22it/s]119027it [00:48, 2711.40it/s]132462it [00:52, 3228.33it/s]119221it [00:49, 2766.63it/s]119800it [00:49, 2964.38it/s]119375it [00:49, 2906.69it/s]132820it [00:52, 3327.56it/s]119575it [00:49, 2963.70it/s]120149it [00:49, 3107.87it/s]119720it [00:49, 3051.63it/s]133165it [00:53, 3361.00it/s]119900it [00:49, 2971.76it/s]120495it [00:49, 3206.71it/s]120050it [00:49, 2999.14it/s]133503it [00:53, 3247.72it/s]120257it [00:49, 3132.35it/s]120827it [00:49, 3103.64it/s]120401it [00:49, 3138.66it/s]133861it [00:53, 3342.65it/s]120618it [00:50, 3263.99it/s]121173it [00:49, 3203.72it/s]120729it [00:49, 3057.15it/s]134197it [00:53, 3243.08it/s]120957it [00:50, 3181.98it/s]121500it [00:49, 3103.80it/s]121076it [00:49, 3170.76it/s]134550it [00:53, 3323.89it/s]121314it [00:50, 3291.15it/s]121843it [00:49, 3194.57it/s]121428it [00:49, 3268.11it/s]134900it [00:53, 3225.73it/s]121650it [00:50, 3156.16it/s]122180it [00:49, 3243.13it/s]121761it [00:49, 3145.77it/s]135259it [00:53, 3327.96it/s]122005it [00:50, 3264.38it/s]122508it [00:50, 3164.51it/s]122110it [00:49, 3242.34it/s]135620it [00:53, 3407.70it/s]122337it [00:50, 3176.64it/s]122866it [00:50, 3284.08it/s]122439it [00:50, 3124.28it/s]122671it [00:50, 3222.38it/s]135963it [00:53, 3209.21it/s]123197it [00:50, 3116.82it/s]122791it [00:50, 3236.22it/s]123019it [00:50, 3294.53it/s]136314it [00:53, 3291.79it/s]123544it [00:50, 3214.72it/s]123118it [00:50, 3186.99it/s]123351it [00:50, 3087.14it/s]136646it [00:54, 3100.52it/s]123869it [00:50, 3182.20it/s]123439it [00:50, 2968.43it/s]136960it [00:54, 3105.95it/s]123664it [00:51, 3058.40it/s]124190it [00:50, 2889.40it/s]123740it [00:50, 2907.36it/s]123976it [00:51, 3075.01it/s]137280it [00:54, 3123.10it/s]124485it [00:50, 2727.34it/s]124034it [00:50, 2800.76it/s]137595it [00:54, 3004.14it/s]124286it [00:51, 2872.50it/s]124817it [00:50, 2786.04it/s]124382it [00:50, 2986.58it/s]137951it [00:54, 3160.35it/s]124644it [00:51, 3068.23it/s]125168it [00:50, 2980.68it/s]124732it [00:50, 3130.83it/s]138270it [00:54, 3101.51it/s]124955it [00:51, 3025.38it/s]125523it [00:51, 3138.73it/s]138625it [00:54, 3229.35it/s]125049it [00:50, 3032.93it/s]125300it [00:51, 3144.88it/s]125842it [00:51, 3098.40it/s]138969it [00:54, 3288.09it/s]125398it [00:51, 3160.68it/s]125648it [00:51, 3241.05it/s]126188it [00:51, 3199.95it/s]139300it [00:54, 3204.70it/s]125717it [00:51, 3068.56it/s]125975it [00:51, 3156.42it/s]126511it [00:51, 3125.40it/s]139658it [00:55, 3312.37it/s]126067it [00:51, 3189.28it/s]126326it [00:51, 3257.27it/s]126851it [00:51, 3201.90it/s]126414it [00:51, 3269.03it/s]139991it [00:55, 3217.46it/s]126654it [00:51, 3162.13it/s]127207it [00:51, 3303.57it/s]140350it [00:55, 3323.93it/s]127000it [00:52, 3247.44it/s]126743it [00:51, 3130.16it/s]127539it [00:51, 3211.36it/s]140709it [00:55, 3399.74it/s]127091it [00:51, 3228.57it/s]127340it [00:52, 3165.42it/s]127878it [00:51, 3261.54it/s]141051it [00:55, 3289.15it/s]127690it [00:52, 3255.40it/s]127416it [00:51, 3047.58it/s]128206it [00:51, 3168.01it/s]141391it [00:55, 3320.50it/s]128040it [00:52, 3317.70it/s]127764it [00:51, 3167.06it/s]128564it [00:51, 3285.48it/s]141725it [00:55, 3223.18it/s]128113it [00:51, 3257.16it/s]128373it [00:52, 3149.37it/s]128924it [00:52, 3375.92it/s]142085it [00:55, 3330.63it/s]128723it [00:52, 3248.62it/s]128442it [00:51, 3137.43it/s]129263it [00:52, 3174.87it/s]142443it [00:55, 3401.63it/s]128794it [00:52, 3244.39it/s]129051it [00:52, 3163.73it/s]129622it [00:52, 3290.54it/s]142785it [00:55, 3268.67it/s]129400it [00:52, 3255.84it/s]129121it [00:52, 3124.10it/s]129954it [00:52, 3196.44it/s]143143it [00:56, 3355.87it/s]129750it [00:52, 3324.40it/s]129469it [00:52, 3223.66it/s]130313it [00:52, 3306.60it/s]143481it [00:56, 3249.81it/s]129817it [00:52, 3296.07it/s]130084it [00:53, 3220.50it/s]130663it [00:52, 3360.01it/s]143841it [00:56, 3347.81it/s]130435it [00:53, 3302.27it/s]130149it [00:52, 3161.32it/s]131001it [00:52, 3240.98it/s]144178it [00:56, 3234.45it/s]130499it [00:52, 3256.19it/s]130767it [00:53, 3200.85it/s]131341it [00:52, 3284.68it/s]144528it [00:56, 3309.50it/s]131122it [00:53, 3298.84it/s]130827it [00:52, 3138.52it/s]131671it [00:52, 3195.56it/s]144889it [00:56, 3395.18it/s]131470it [00:53, 3349.18it/s]131176it [00:52, 3237.55it/s]132025it [00:53, 3292.08it/s]145230it [00:56, 3217.12it/s]131525it [00:52, 3308.88it/s]131807it [00:53, 3151.40it/s]132356it [00:53, 3290.47it/s]145582it [00:56, 3302.16it/s]132148it [00:53, 3223.27it/s]131858it [00:53, 3127.85it/s]132686it [00:53, 3121.66it/s]145915it [00:56, 3088.07it/s]132174it [00:53, 3091.88it/s]132473it [00:53, 2990.45it/s]133001it [00:53, 2969.46it/s]132783it [00:53, 3019.15it/s]146228it [00:57, 2932.61it/s]132486it [00:53, 2930.40it/s]133301it [00:53, 2781.38it/s]146525it [00:57, 2942.15it/s]133091it [00:53, 3024.71it/s]132799it [00:53, 2984.89it/s]133625it [00:53, 2903.62it/s]133396it [00:54, 2994.11it/s]133131it [00:53, 3078.10it/s]146822it [00:57, 2792.89it/s]133974it [00:53, 3065.25it/s]133748it [00:54, 3143.58it/s]133441it [00:53, 2994.91it/s]147175it [00:57, 2994.17it/s]134285it [00:53, 3021.95it/s]134065it [00:54, 3087.55it/s]133791it [00:53, 3137.74it/s]147500it [00:57, 2995.31it/s]134634it [00:53, 3154.26it/s]134417it [00:54, 3211.13it/s]147843it [00:57, 3116.34it/s]134107it [00:53, 3052.52it/s]134952it [00:54, 3106.09it/s]134767it [00:54, 3293.06it/s]148200it [00:57, 3245.97it/s]134459it [00:53, 3185.14it/s]135306it [00:54, 3231.21it/s]135098it [00:54, 3205.84it/s]134811it [00:54, 3280.13it/s]148527it [00:57, 3201.64it/s]135664it [00:54, 3325.26it/s]135452it [00:54, 3302.50it/s]148887it [00:57, 3314.91it/s]135141it [00:54, 3149.46it/s]135998it [00:54, 3218.96it/s]135784it [00:54, 3205.55it/s]149221it [00:58, 3248.11it/s]135495it [00:54, 3260.94it/s]136360it [00:54, 3333.58it/s]136140it [00:54, 3305.60it/s]149579it [00:58, 3343.30it/s]135823it [00:54, 3135.96it/s]136695it [00:54, 3197.25it/s]136491it [00:55, 3364.84it/s]149934it [00:58, 3401.47it/s]136177it [00:54, 3248.84it/s]137055it [00:54, 3311.49it/s]136829it [00:55, 3254.15it/s]136515it [00:54, 3285.96it/s]150276it [00:58, 3162.29it/s]137405it [00:54, 3365.19it/s]137181it [00:55, 3330.69it/s]150640it [00:58, 3294.87it/s]136846it [00:54, 3165.82it/s]137744it [00:54, 3148.54it/s]137516it [00:55, 3229.07it/s]137199it [00:54, 3268.27it/s]138109it [00:54, 3287.48it/s]137869it [00:55, 3314.89it/s]137528it [00:54, 3142.93it/s]138442it [00:55, 3182.01it/s]138225it [00:55, 3385.99it/s]137879it [00:54, 3245.56it/s]138804it [00:55, 3304.29it/s]138565it [00:55, 3271.05it/s]138232it [00:55, 3324.92it/s]139138it [00:55, 3210.27it/s]138915it [00:55, 3335.74it/s]138567it [00:55, 3173.92it/s]139494it [00:55, 3308.21it/s]139250it [00:55, 3236.72it/s]138917it [00:55, 3265.84it/s]139849it [00:55, 3376.27it/s]139603it [00:55, 3318.61it/s]139246it [00:55, 3146.67it/s]140189it [00:55, 3257.15it/s]139940it [00:56, 3225.74it/s]139596it [00:55, 3245.96it/s]140541it [00:55, 3330.27it/s]140293it [00:56, 3311.32it/s]139937it [00:55, 3139.66it/s]140876it [00:55, 3226.49it/s]140645it [00:56, 3371.56it/s]140287it [00:55, 3238.43it/s]141221it [00:55, 3289.94it/s]140984it [00:56, 3190.47it/s]140628it [00:55, 3286.76it/s]141561it [00:56, 3321.23it/s]141314it [00:56, 3220.07it/s]140959it [00:55, 3102.19it/s]141895it [00:56, 3071.29it/s]141638it [00:56, 3040.93it/s]141273it [00:56, 2846.53it/s]142207it [00:56, 3074.07it/s]141946it [00:56, 3026.21it/s]141563it [00:56, 2678.05it/s]142518it [00:56, 2918.86it/s]142256it [00:56, 3046.57it/s]141836it [00:56, 2649.77it/s]142847it [00:56, 3021.01it/s]142563it [00:56, 2996.89it/s]142187it [00:56, 2881.08it/s]143204it [00:56, 3175.46it/s]142919it [00:57, 3158.09it/s]142480it [00:56, 2865.24it/s]143525it [00:56, 3126.95it/s]143269it [00:57, 3257.51it/s]142831it [00:56, 3045.59it/s]143882it [00:56, 3254.42it/s]143597it [00:57, 3192.99it/s]143180it [00:56, 3172.47it/s]144210it [00:56, 3179.03it/s]143960it [00:57, 3319.17it/s]143501it [00:56, 3080.39it/s]144570it [00:56, 3300.46it/s]144294it [00:57, 3232.92it/s]143852it [00:56, 3202.73it/s]144928it [00:57, 3379.52it/s]144659it [00:57, 3351.51it/s]144175it [00:57, 3093.29it/s]145268it [00:57, 3236.24it/s]144996it [00:57, 3271.91it/s]144530it [00:57, 3222.03it/s]145633it [00:57, 3352.75it/s]145359it [00:57, 3374.61it/s]144870it [00:57, 3273.09it/s]145724it [00:57, 3453.31it/s]145971it [00:57, 3197.85it/s]145199it [00:57, 3154.56it/s]146333it [00:57, 3315.79it/s]146071it [00:57, 3348.94it/s]145541it [00:57, 3229.71it/s]146668it [00:57, 3232.67it/s]146408it [00:58, 3248.69it/s]145866it [00:57, 3129.88it/s]147033it [00:57, 3350.63it/s]146735it [00:58, 3189.57it/s]146218it [00:57, 3241.54it/s]147371it [00:57, 3327.91it/s]147093it [00:58, 3298.75it/s]146567it [00:57, 3312.11it/s]147706it [00:57, 3255.40it/s]147454it [00:58, 3386.94it/s]146900it [00:57, 3182.45it/s]148063it [00:58, 3345.94it/s]147794it [00:58, 3287.75it/s]147251it [00:57, 3274.74it/s]148151it [00:58, 3366.83it/s]148399it [00:58, 3236.27it/s]147581it [00:58, 3157.95it/s]148728it [00:58, 3249.97it/s]148489it [00:58, 3269.66it/s]147933it [00:58, 3260.09it/s]149089it [00:58, 3352.33it/s]148824it [00:58, 3290.66it/s]148286it [00:58, 3335.61it/s]149426it [00:58, 3221.71it/s]149180it [00:58, 3220.18it/s]148622it [00:58, 3192.12it/s]149788it [00:58, 3333.81it/s]149540it [00:59, 3326.46it/s]148964it [00:58, 3256.48it/s]150123it [00:58, 3254.05it/s]149890it [00:59, 3374.48it/s]149292it [00:58, 3137.74it/s]150481it [00:58, 3345.84it/s]150229it [00:59, 3226.03it/s]149640it [00:58, 3234.99it/s]150831it [00:58, 3388.18it/s]150563it [00:59, 3257.47it/s]149966it [00:58, 3213.12it/s]150891it [00:59, 3051.80it/s]150289it [00:58, 2837.27it/s]150604it [00:59, 2918.93it/s]150903it [00:59, 2869.04it/s]150974it [01:04, 198.04it/s] 151326it [01:04, 277.47it/s]151682it [01:04, 385.91it/s]151978it [01:04, 501.89it/s]152332it [01:04, 685.62it/s]152646it [01:04, 877.32it/s]153009it [01:04, 1154.73it/s]153375it [01:04, 1471.60it/s]153711it [01:04, 1717.21it/s]154079it [01:04, 2061.75it/s]154415it [01:05, 2251.30it/s]154780it [01:05, 2554.85it/s]155128it [01:05, 2772.45it/s]155467it [01:05, 2740.49it/s]155786it [01:05, 2852.80it/s]156104it [01:05, 2758.64it/s]156403it [01:05, 2778.74it/s]156756it [01:05, 2980.59it/s]157068it [01:05, 2977.14it/s]157438it [01:06, 3179.33it/s]157764it [01:06, 3140.53it/s]158134it [01:06, 3299.09it/s]158490it [01:06, 3232.80it/s]158862it [01:06, 3368.86it/s]159203it [01:06, 3308.47it/s]159537it [01:06, 3263.66it/s]159905it [01:06, 3382.63it/s]160245it [01:06, 3297.87it/s]160618it [01:06, 3421.42it/s]160985it [01:07, 3476.79it/s]161334it [01:07, 3383.23it/s]161693it [01:07, 3440.54it/s]162039it [01:07, 3363.70it/s]162401it [01:07, 3435.51it/s]162746it [01:07, 3360.22it/s]163110it [01:07, 3432.16it/s]163486it [01:07, 3525.56it/s]163840it [01:07, 3390.81it/s]164215it [01:08, 3494.02it/s]151195it [01:04, 192.28it/s] 164566it [01:08, 3297.49it/s]151550it [01:04, 279.01it/s]164900it [01:08, 3308.23it/s]151801it [01:04, 357.25it/s]151200it [01:05, 185.58it/s] 165233it [01:08, 3111.92it/s]152117it [01:04, 492.41it/s]151507it [01:05, 253.50it/s]165568it [01:08, 3177.03it/s]151171it [01:04, 185.65it/s] 152440it [01:04, 668.76it/s]151769it [01:05, 329.72it/s]165896it [01:08, 3204.88it/s]151453it [01:04, 244.80it/s]152723it [01:04, 844.04it/s]152086it [01:05, 455.61it/s]166219it [01:08, 3154.22it/s]151767it [01:05, 334.04it/s]153079it [01:04, 1133.75it/s]152449it [01:05, 644.60it/s]166588it [01:08, 3307.43it/s]152133it [01:05, 475.09it/s]153436it [01:05, 1455.90it/s]152749it [01:05, 829.02it/s]166921it [01:08, 3271.45it/s]152486it [01:05, 649.58it/s]153753it [01:05, 1703.27it/s]153113it [01:05, 1111.92it/s]167287it [01:08, 3383.44it/s]152799it [01:05, 835.46it/s]154111it [01:05, 2045.52it/s]153449it [01:05, 1376.10it/s]167661it [01:09, 3487.16it/s]153157it [01:05, 1101.87it/s]153814it [01:05, 1718.39it/s]154435it [01:05, 2240.43it/s]168011it [01:09, 3368.73it/s]153482it [01:05, 1353.83it/s]154173it [01:06, 2049.22it/s]154792it [01:05, 2537.98it/s]168390it [01:09, 3489.47it/s]153841it [01:05, 1683.03it/s]154511it [01:06, 2276.09it/s]155127it [01:05, 2648.39it/s]168741it [01:09, 3372.51it/s]154209it [01:05, 2029.43it/s]154869it [01:06, 2562.35it/s]155485it [01:05, 2881.46it/s]169115it [01:09, 3477.39it/s]154551it [01:05, 2250.90it/s]155827it [01:05, 3018.57it/s]155207it [01:06, 2701.11it/s]169465it [01:09, 3349.43it/s]154915it [01:05, 2552.47it/s]155562it [01:06, 2913.64it/s]156160it [01:05, 2998.26it/s]169831it [01:09, 3437.10it/s]155256it [01:06, 2683.23it/s]155924it [01:06, 3099.55it/s]156518it [01:05, 3155.37it/s]170191it [01:09, 3482.93it/s]155618it [01:06, 2912.44it/s]156269it [01:06, 3078.70it/s]156850it [01:06, 3084.42it/s]170541it [01:09, 3380.12it/s]155968it [01:06, 2945.94it/s]156630it [01:06, 3222.74it/s]157210it [01:06, 3227.01it/s]170904it [01:10, 3449.86it/s]156334it [01:06, 3133.76it/s]157571it [01:06, 3334.11it/s]156971it [01:06, 3175.07it/s]171251it [01:10, 3354.09it/s]156685it [01:06, 3236.45it/s]157334it [01:06, 3301.16it/s]157912it [01:06, 3210.88it/s]171610it [01:10, 3421.88it/s]157028it [01:06, 3196.35it/s]157674it [01:07, 3216.19it/s]158272it [01:06, 3320.59it/s]171954it [01:10, 3343.15it/s]157384it [01:06, 3297.55it/s]158038it [01:07, 3334.60it/s]158609it [01:06, 3203.35it/s]172312it [01:10, 3409.75it/s]157724it [01:06, 3237.69it/s]158393it [01:07, 3395.30it/s]158973it [01:06, 3325.33it/s]172687it [01:10, 3507.09it/s]158077it [01:06, 3319.39it/s]158737it [01:07, 3307.21it/s]159327it [01:06, 3215.64it/s]173039it [01:10, 3383.98it/s]158444it [01:07, 3418.31it/s]159100it [01:07, 3398.69it/s]159690it [01:06, 3329.61it/s]173408it [01:10, 3470.62it/s]158791it [01:07, 3302.24it/s]159443it [01:07, 3311.06it/s]160049it [01:07, 3403.98it/s]173757it [01:10, 3371.08it/s]159161it [01:07, 3414.90it/s]159800it [01:07, 3385.17it/s]160392it [01:07, 3255.10it/s]174123it [01:10, 3452.48it/s]159506it [01:07, 3246.70it/s]160158it [01:07, 3439.56it/s]160733it [01:07, 3297.32it/s]174470it [01:11, 3329.56it/s]159856it [01:07, 3315.79it/s]161065it [01:07, 3115.33it/s]174805it [01:11, 3304.47it/s]160504it [01:08, 2767.36it/s]160191it [01:07, 3087.01it/s]161380it [01:07, 3091.52it/s]175137it [01:11, 3281.84it/s]160803it [01:08, 2790.81it/s]160517it [01:07, 3129.07it/s]161703it [01:07, 3128.25it/s]175466it [01:11, 3100.16it/s]161098it [01:08, 2737.09it/s]160834it [01:07, 3030.99it/s]162018it [01:07, 2991.97it/s]175820it [01:11, 3223.17it/s]161447it [01:08, 2937.28it/s]161140it [01:07, 2993.04it/s]162381it [01:07, 3171.06it/s]176145it [01:11, 3186.78it/s]161806it [01:08, 3115.21it/s]161497it [01:08, 3150.08it/s]162701it [01:07, 3102.81it/s]176510it [01:11, 3318.62it/s]162127it [01:08, 3085.30it/s]161847it [01:08, 3162.03it/s]163064it [01:07, 3251.62it/s]176886it [01:11, 3445.39it/s]162482it [01:08, 3216.60it/s]162208it [01:08, 3289.25it/s]163427it [01:08, 3359.04it/s]177233it [01:11, 3342.91it/s]162809it [01:08, 3160.18it/s]162581it [01:08, 3414.67it/s]163765it [01:08, 3240.62it/s]177599it [01:12, 3429.33it/s]163168it [01:08, 3281.29it/s]162925it [01:08, 3305.09it/s]164129it [01:08, 3354.86it/s]163528it [01:08, 3372.11it/s]177944it [01:12, 3348.12it/s]163295it [01:08, 3416.96it/s]164467it [01:08, 3236.00it/s]178318it [01:12, 3459.30it/s]163868it [01:09, 3259.69it/s]163639it [01:08, 3325.31it/s]164837it [01:08, 3367.60it/s]164225it [01:09, 3348.86it/s]178666it [01:12, 3347.75it/s]164012it [01:08, 3440.11it/s]165204it [01:08, 3454.63it/s]179038it [01:12, 3454.55it/s]164562it [01:09, 3254.78it/s]164367it [01:08, 3339.09it/s]165552it [01:08, 3271.12it/s]179399it [01:12, 3497.36it/s]164923it [01:09, 3355.94it/s]164740it [01:08, 3450.39it/s]165918it [01:08, 3378.99it/s]179750it [01:12, 3406.06it/s]165261it [01:09, 3260.45it/s]165113it [01:09, 3529.59it/s]180113it [01:12, 3469.56it/s]166259it [01:08, 3271.39it/s]165620it [01:09, 3354.22it/s]165468it [01:09, 3426.76it/s]166629it [01:09, 3391.31it/s]180462it [01:12, 3395.36it/s]165980it [01:09, 3425.28it/s]165837it [01:09, 3501.15it/s]180825it [01:12, 3461.98it/s]166971it [01:09, 3271.67it/s]166324it [01:09, 3322.75it/s]166189it [01:09, 3414.57it/s]167339it [01:09, 3386.63it/s]181173it [01:13, 3372.19it/s]166680it [01:09, 3383.73it/s]166560it [01:09, 3498.03it/s]167707it [01:09, 3468.89it/s]181532it [01:13, 3434.19it/s]167020it [01:09, 3319.64it/s]166911it [01:09, 3417.22it/s]181904it [01:13, 3517.69it/s]168056it [01:09, 3328.42it/s]167388it [01:10, 3423.83it/s]167278it [01:09, 3490.04it/s]182257it [01:13, 3412.14it/s]168425it [01:09, 3431.20it/s]167732it [01:10, 3323.26it/s]167656it [01:09, 3573.75it/s]182635it [01:13, 3517.46it/s]168107it [01:10, 3446.12it/s]168771it [01:09, 3300.34it/s]168015it [01:09, 3434.04it/s]182988it [01:13, 3402.46it/s]168482it [01:10, 3534.32it/s]169133it [01:09, 3389.83it/s]168394it [01:10, 3535.60it/s]183363it [01:13, 3502.21it/s]168837it [01:10, 3403.11it/s]169475it [01:09, 3272.61it/s]168750it [01:10, 3408.30it/s]169198it [01:10, 3460.78it/s]169835it [01:10, 3363.36it/s]183715it [01:13, 3317.85it/s]169118it [01:10, 3484.91it/s]170200it [01:10, 3443.80it/s]184080it [01:13, 3411.04it/s]169546it [01:10, 3266.54it/s]169469it [01:10, 3275.96it/s]184424it [01:14, 3357.30it/s]169876it [01:10, 3252.52it/s]169800it [01:10, 3270.79it/s]170546it [01:10, 2908.23it/s]170204it [01:10, 3205.21it/s]184762it [01:14, 3126.00it/s]170130it [01:10, 3259.54it/s]170879it [01:10, 3015.85it/s]185089it [01:14, 3164.15it/s]170526it [01:11, 3035.36it/s]170458it [01:10, 3099.76it/s]171193it [01:10, 2859.14it/s]185409it [01:14, 3087.04it/s]170880it [01:11, 3168.77it/s]170815it [01:10, 3229.62it/s]171560it [01:10, 3073.88it/s]185775it [01:14, 3247.35it/s]171200it [01:11, 3110.19it/s]171141it [01:10, 3194.80it/s]171928it [01:10, 3064.00it/s]186157it [01:14, 3409.94it/s]171570it [01:11, 3276.78it/s]171503it [01:10, 3315.97it/s]172292it [01:10, 3218.25it/s]186501it [01:14, 3333.16it/s]171900it [01:11, 3208.06it/s]171872it [01:11, 3423.25it/s]172662it [01:10, 3352.52it/s]186897it [01:14, 3513.11it/s]172223it [01:11, 3159.53it/s]172216it [01:11, 3329.36it/s]173003it [01:11, 3253.57it/s]187251it [01:14, 3405.00it/s]172594it [01:11, 3315.79it/s]172594it [01:11, 3459.21it/s]173369it [01:11, 3365.77it/s]187626it [01:14, 3503.19it/s]172928it [01:11, 3245.07it/s]172942it [01:11, 3370.40it/s]173710it [01:11, 3249.15it/s]187979it [01:15, 3409.21it/s]173301it [01:11, 3382.75it/s]173316it [01:11, 3476.32it/s]174076it [01:11, 3363.69it/s]188358it [01:15, 3517.58it/s]173641it [01:12, 3297.28it/s]173665it [01:11, 3363.92it/s]174448it [01:11, 3273.71it/s]174006it [01:12, 3398.28it/s]188730it [01:15, 3419.93it/s]174034it [01:11, 3457.18it/s]174812it [01:11, 3375.54it/s]174358it [01:12, 3432.39it/s]189102it [01:15, 3504.71it/s]174401it [01:11, 3517.15it/s]175159it [01:11, 3400.56it/s]189474it [01:15, 3564.46it/s]174703it [01:12, 3357.78it/s]174754it [01:11, 3432.67it/s]175502it [01:11, 3271.12it/s]175072it [01:12, 3454.29it/s]189832it [01:15, 3456.10it/s]175116it [01:12, 3486.12it/s]175869it [01:11, 3381.93it/s]190209it [01:15, 3545.11it/s]175419it [01:12, 3310.15it/s]175466it [01:12, 3392.55it/s]176210it [01:11, 3250.49it/s]175788it [01:12, 3418.46it/s]190565it [01:15, 3440.76it/s]175830it [01:12, 3461.71it/s]176573it [01:12, 3357.38it/s]190934it [01:15, 3511.97it/s]176132it [01:12, 3270.97it/s]176178it [01:12, 3361.99it/s]176942it [01:12, 3451.45it/s]191287it [01:16, 3423.49it/s]176505it [01:12, 3399.88it/s]176540it [01:12, 3434.51it/s]177290it [01:12, 3307.39it/s]191659it [01:16, 3506.18it/s]176868it [01:12, 3465.11it/s]176914it [01:12, 3522.46it/s]177650it [01:12, 3389.97it/s]192043it [01:16, 3601.43it/s]177217it [01:13, 3356.53it/s]177268it [01:12, 3390.02it/s]177992it [01:12, 3280.30it/s]192405it [01:16, 3476.44it/s]177573it [01:13, 3413.35it/s]177637it [01:12, 3474.63it/s]178364it [01:12, 3404.79it/s]192790it [01:16, 3582.52it/s]177916it [01:13, 3335.45it/s]177986it [01:12, 3355.24it/s]178707it [01:12, 3281.78it/s]178281it [01:13, 3423.95it/s]193150it [01:16, 3465.68it/s]178363it [01:12, 3473.32it/s]179071it [01:12, 3381.34it/s]178646it [01:13, 3488.76it/s]193515it [01:16, 3499.72it/s]178713it [01:13, 3317.37it/s]179435it [01:12, 3453.71it/s]193867it [01:16, 3390.49it/s]178996it [01:13, 3193.31it/s]179052it [01:13, 3336.80it/s]179783it [01:13, 3160.55it/s]194208it [01:16, 3371.01it/s]179388it [01:13, 3303.43it/s]179321it [01:13, 2906.89it/s]180106it [01:13, 3178.25it/s]194546it [01:16, 3238.29it/s]179720it [01:13, 3139.87it/s]179620it [01:13, 2831.82it/s]180428it [01:13, 3039.38it/s]180047it [01:13, 3168.49it/s]179939it [01:13, 2925.80it/s]194872it [01:17, 2543.52it/s]180746it [01:13, 3077.91it/s]180366it [01:13, 3126.29it/s]180289it [01:14, 3082.43it/s]195257it [01:17, 2858.13it/s]181111it [01:13, 3240.08it/s]180733it [01:13, 3282.14it/s]180602it [01:14, 3082.18it/s]195567it [01:17, 2919.76it/s]181438it [01:13, 3159.08it/s]181112it [01:13, 3428.15it/s]180965it [01:14, 3238.59it/s]195935it [01:17, 3122.82it/s]181807it [01:13, 3308.77it/s]181457it [01:13, 3303.06it/s]181292it [01:14, 3202.62it/s]196290it [01:17, 3123.26it/s]182141it [01:13, 3245.68it/s]181834it [01:14, 3434.88it/s]181649it [01:14, 3308.85it/s]182510it [01:13, 3372.24it/s]182180it [01:14, 3373.37it/s]182010it [01:14, 3268.29it/s]182849it [01:14, 3297.12it/s]182557it [01:14, 3485.79it/s]182367it [01:14, 3354.63it/s]183221it [01:14, 3417.89it/s]182749it [01:14, 3488.54it/s]182907it [01:14, 3364.68it/s]183593it [01:14, 3505.07it/s]183284it [01:14, 3479.30it/s]183100it [01:14, 3337.71it/s]183945it [01:14, 3369.02it/s]183657it [01:14, 3550.91it/s]183475it [01:14, 3453.59it/s]184321it [01:14, 3479.90it/s]184014it [01:14, 3452.29it/s]183823it [01:15, 3337.17it/s]184671it [01:14, 3319.50it/s]184392it [01:14, 3546.48it/s]184193it [01:15, 3440.10it/s]185046it [01:14, 3440.89it/s]184748it [01:14, 3411.92it/s]184539it [01:15, 3335.18it/s]185393it [01:14, 3306.82it/s]185128it [01:14, 3520.66it/s]184915it [01:15, 3455.19it/s]185765it [01:14, 3421.73it/s]185272it [01:15, 3485.78it/s]185482it [01:15, 3400.84it/s]186145it [01:14, 3528.64it/s]185860it [01:15, 3507.28it/s]185622it [01:15, 3367.63it/s]186500it [01:15, 3393.99it/s]186213it [01:15, 3432.03it/s]185984it [01:15, 3440.02it/s]186889it [01:15, 3534.23it/s]186559it [01:15, 3439.00it/s]186330it [01:15, 3368.08it/s]187245it [01:15, 3401.76it/s]186956it [01:15, 3591.13it/s]186700it [01:15, 3457.02it/s]187615it [01:15, 3484.38it/s]187317it [01:15, 3481.40it/s]187050it [01:16, 3391.14it/s]187966it [01:15, 3379.23it/s]187695it [01:15, 3565.55it/s]187411it [01:16, 3453.97it/s]188339it [01:15, 3477.73it/s]187788it [01:16, 3543.94it/s]188053it [01:15, 3473.65it/s]188719it [01:15, 3568.88it/s]188430it [01:15, 3556.90it/s]188144it [01:16, 3394.57it/s]189078it [01:15, 3403.49it/s]188511it [01:16, 3473.36it/s]188787it [01:16, 3389.94it/s]189421it [01:15, 3377.82it/s]189129it [01:16, 3370.62it/s]188860it [01:16, 3275.84it/s]189761it [01:16, 3115.64it/s]189468it [01:16, 3358.01it/s]189191it [01:16, 3236.89it/s]190094it [01:16, 3174.13it/s]189805it [01:16, 3082.72it/s]189517it [01:16, 2855.25it/s]190416it [01:16, 3035.08it/s]190134it [01:16, 3137.19it/s]189812it [01:16, 2872.87it/s]190787it [01:16, 3221.16it/s]190452it [01:16, 3137.39it/s]190193it [01:17, 3127.30it/s]191156it [01:16, 3352.84it/s]190827it [01:16, 3304.07it/s]190513it [01:17, 3090.17it/s]191495it [01:16, 3265.19it/s]191202it [01:16, 3430.62it/s]190885it [01:17, 3265.56it/s]191870it [01:16, 3402.10it/s]191548it [01:16, 3356.89it/s]191250it [01:17, 3212.06it/s]192213it [01:16, 3311.22it/s]191934it [01:16, 3501.39it/s]191625it [01:17, 3361.67it/s]192587it [01:16, 3431.74it/s]192286it [01:17, 3373.36it/s]192001it [01:17, 3474.55it/s]192933it [01:17, 3339.25it/s]192667it [01:17, 3497.45it/s]192352it [01:17, 3375.47it/s]193307it [01:17, 3453.03it/s]193019it [01:17, 3419.15it/s]192726it [01:17, 3477.27it/s]193672it [01:17, 3509.46it/s]193394it [01:17, 3514.14it/s]193076it [01:17, 3382.68it/s]194025it [01:17, 3403.24it/s]193761it [01:17, 3559.42it/s]193439it [01:17, 3453.01it/s]194398it [01:17, 3489.27it/s]194119it [01:17, 3456.72it/s]193786it [01:18, 3363.66it/s]194499it [01:17, 3555.64it/s]194749it [01:17, 3321.64it/s]194160it [01:18, 3470.06it/s]195142it [01:17, 3491.37it/s]194856it [01:17, 3426.62it/s]194537it [01:18, 3556.27it/s]195246it [01:17, 3562.26it/s]195494it [01:17, 3319.71it/s]194894it [01:18, 3429.58it/s]195866it [01:17, 3425.84it/s]195605it [01:18, 3462.85it/s]195286it [01:18, 3568.46it/s]196232it [01:17, 3490.27it/s]195974it [01:18, 3525.39it/s]195645it [01:18, 3448.34it/s]196328it [01:18, 3400.03it/s]196011it [01:18, 3506.44it/s]196613it [01:24, 155.03it/s] 197001it [01:24, 225.83it/s]197318it [01:24, 303.65it/s]197694it [01:25, 429.42it/s]198038it [01:25, 576.62it/s]198410it [01:25, 783.27it/s]198775it [01:25, 1029.14it/s]199118it [01:25, 1266.32it/s]199456it [01:25, 1546.62it/s]199787it [01:25, 1782.41it/s]200121it [01:25, 2065.16it/s]200491it [01:25, 2401.16it/s]200828it [01:25, 2592.07it/s]201203it [01:26, 2871.78it/s]201548it [01:26, 2949.60it/s]201932it [01:26, 3183.86it/s]202283it [01:26, 3221.67it/s]202673it [01:26, 3409.26it/s]203057it [01:26, 3530.16it/s]203423it [01:26, 3448.11it/s]203797it [01:26, 3530.28it/s]204157it [01:26, 3436.23it/s]204528it [01:26, 3514.21it/s]204884it [01:27, 3436.81it/s]205265it [01:27, 3543.46it/s]205622it [01:27, 3451.60it/s]205999it [01:27, 3540.61it/s]206383it [01:27, 3624.98it/s]206748it [01:27, 3517.10it/s]207130it [01:27, 3604.40it/s]196584it [01:23, 193.11it/s] 207492it [01:27, 3465.16it/s]196969it [01:24, 274.81it/s]207876it [01:27, 3569.68it/s]197276it [01:24, 361.25it/s]208235it [01:28, 3434.34it/s]197649it [01:24, 503.75it/s]208609it [01:28, 3515.58it/s]198035it [01:24, 687.22it/s]208963it [01:28, 3294.03it/s]198369it [01:24, 884.78it/s]198698it [01:24, 1116.11it/s]209296it [01:28, 3299.15it/s]209629it [01:28, 3273.77it/s]199023it [01:24, 1343.03it/s]196670it [01:24, 171.90it/s] 199381it [01:24, 1665.41it/s]209959it [01:28, 3141.05it/s]197044it [01:25, 243.93it/s]210334it [01:28, 3312.07it/s]199715it [01:24, 1912.18it/s]197355it [01:25, 324.25it/s]210668it [01:28, 3263.65it/s]200075it [01:25, 2237.11it/s]197730it [01:25, 456.12it/s]211041it [01:28, 3397.53it/s]200442it [01:25, 2545.91it/s]198061it [01:25, 604.85it/s]211406it [01:29, 3470.20it/s]200783it [01:25, 2670.95it/s]198433it [01:25, 820.65it/s]211755it [01:29, 3383.37it/s]201151it [01:25, 2919.03it/s]198814it [01:25, 1089.72it/s]212128it [01:29, 3483.44it/s]201492it [01:25, 2955.00it/s]199163it [01:25, 1351.04it/s]212478it [01:29, 3397.16it/s]201871it [01:25, 3176.19it/s]199550it [01:25, 1701.01it/s]212844it [01:29, 3471.41it/s]202235it [01:25, 3168.68it/s]199905it [01:25, 1963.56it/s]213193it [01:29, 3391.47it/s]202621it [01:25, 3356.13it/s]200276it [01:25, 2290.52it/s]213564it [01:29, 3482.66it/s]203004it [01:25, 3487.41it/s]200627it [01:26, 2497.81it/s]213939it [01:29, 3558.65it/s]196364it [01:26, 149.44it/s] 203365it [01:25, 3357.67it/s]201001it [01:26, 2780.76it/s]214296it [01:29, 3413.22it/s]196750it [01:26, 213.73it/s]203734it [01:26, 3448.32it/s]201367it [01:26, 2997.02it/s]214665it [01:29, 3492.15it/s]197119it [01:26, 297.69it/s]204086it [01:26, 3325.57it/s]201723it [01:26, 3063.63it/s]215016it [01:30, 3395.04it/s]197438it [01:26, 394.16it/s]204455it [01:26, 3427.98it/s]202104it [01:26, 3261.18it/s]215393it [01:30, 3502.64it/s]197808it [01:26, 544.48it/s]202461it [01:26, 3284.86it/s]204803it [01:26, 3274.57it/s]198139it [01:27, 711.51it/s]215745it [01:30, 3374.56it/s]202842it [01:26, 3430.57it/s]205176it [01:26, 3397.18it/s]198509it [01:27, 950.53it/s]216124it [01:30, 3490.85it/s]205545it [01:26, 3480.65it/s]203202it [01:26, 3384.51it/s]216501it [01:30, 3569.67it/s]198879it [01:27, 1213.40it/s]203574it [01:26, 3479.32it/s]205897it [01:26, 3361.97it/s]199245it [01:27, 1521.45it/s]216860it [01:30, 3482.39it/s]206271it [01:26, 3468.88it/s]203931it [01:27, 3372.88it/s]199631it [01:27, 1878.03it/s]217225it [01:30, 3529.60it/s]204308it [01:27, 3482.91it/s]206621it [01:26, 3359.94it/s]217580it [01:30, 3456.17it/s]199988it [01:27, 2108.46it/s]204686it [01:27, 3562.61it/s]206990it [01:27, 3452.89it/s]217963it [01:30, 3563.99it/s]200349it [01:27, 2404.21it/s]207338it [01:27, 3346.31it/s]205047it [01:27, 3396.94it/s]218321it [01:31, 3404.29it/s]200695it [01:27, 2519.42it/s]207699it [01:27, 3420.58it/s]205410it [01:27, 3461.41it/s]218666it [01:31, 3415.36it/s]201028it [01:27, 2706.25it/s]208043it [01:27, 3404.62it/s]205760it [01:27, 3251.27it/s]219009it [01:31, 3400.53it/s]201357it [01:28, 2801.55it/s]206098it [01:27, 3286.01it/s]208385it [01:27, 3066.53it/s]219351it [01:31, 3238.84it/s]206434it [01:27, 3305.42it/s]201680it [01:28, 2613.99it/s]208719it [01:27, 3140.32it/s]219704it [01:31, 3319.33it/s]202027it [01:28, 2826.45it/s]206767it [01:27, 3206.98it/s]209039it [01:27, 3100.13it/s]220038it [01:31, 3294.13it/s]202338it [01:28, 2900.38it/s]207142it [01:27, 3361.71it/s]209412it [01:27, 3276.92it/s]220434it [01:31, 3486.84it/s]202712it [01:28, 3128.57it/s]207481it [01:28, 3325.52it/s]209763it [01:27, 3342.75it/s]220785it [01:31, 3420.57it/s]207860it [01:28, 3458.66it/s]203079it [01:28, 3154.03it/s]210101it [01:28, 3250.29it/s]221167it [01:31, 3536.52it/s]203448it [01:28, 3302.70it/s]208208it [01:28, 3392.84it/s]210473it [01:28, 3382.56it/s]221548it [01:31, 3616.01it/s]203822it [01:28, 3424.50it/s]208592it [01:28, 3520.66it/s]210814it [01:28, 3278.75it/s]221911it [01:32, 3520.12it/s]204172it [01:28, 3357.03it/s]208957it [01:28, 3439.99it/s]211181it [01:28, 3390.08it/s]222285it [01:32, 3582.66it/s]204546it [01:29, 3465.88it/s]209338it [01:28, 3546.01it/s]211522it [01:28, 3289.73it/s]222645it [01:32, 3480.00it/s]209701it [01:28, 3569.29it/s]204897it [01:29, 3377.49it/s]211895it [01:28, 3413.58it/s]223009it [01:32, 3526.10it/s]205273it [01:29, 3486.95it/s]210059it [01:28, 3460.82it/s]212265it [01:28, 3495.00it/s]223363it [01:32, 3420.88it/s]210430it [01:28, 3531.57it/s]205625it [01:29, 3395.91it/s]212617it [01:28, 3376.35it/s]223734it [01:32, 3503.85it/s]205996it [01:29, 3485.19it/s]210785it [01:29, 3433.00it/s]212988it [01:28, 3471.25it/s]224086it [01:32, 3391.68it/s]206376it [01:29, 3575.32it/s]211150it [01:29, 3494.49it/s]213337it [01:28, 3353.66it/s]224460it [01:32, 3489.97it/s]206736it [01:29, 3444.10it/s]211501it [01:29, 3401.28it/s]213703it [01:29, 3439.08it/s]224832it [01:32, 3556.75it/s]207116it [01:29, 3544.45it/s]211881it [01:29, 3514.74it/s]214049it [01:29, 3337.88it/s]225189it [01:32, 3448.33it/s]212252it [01:29, 3570.45it/s]207473it [01:29, 3285.74it/s]214415it [01:29, 3426.85it/s]225549it [01:33, 3486.12it/s]212611it [01:29, 3447.18it/s]207852it [01:29, 3423.94it/s]214765it [01:29, 3444.79it/s]225899it [01:33, 3373.68it/s]212987it [01:29, 3535.49it/s]208199it [01:30, 3384.83it/s]215111it [01:29, 3338.11it/s]226266it [01:33, 3457.87it/s]213342it [01:29, 3431.39it/s]208583it [01:30, 3512.42it/s]215481it [01:29, 3441.33it/s]226614it [01:33, 3360.10it/s]213711it [01:29, 3504.69it/s]208959it [01:30, 3437.55it/s]215827it [01:29, 3330.27it/s]226961it [01:33, 3390.30it/s]214063it [01:29, 3398.61it/s]209339it [01:30, 3540.14it/s]216203it [01:29, 3452.26it/s]227327it [01:33, 3467.76it/s]214431it [01:30, 3479.13it/s]209704it [01:30, 3542.76it/s]216550it [01:29, 3344.67it/s]227675it [01:33, 3331.45it/s]214796it [01:30, 3528.55it/s]216931it [01:29, 3476.36it/s]210060it [01:30, 3371.92it/s]228035it [01:33, 3406.58it/s]215150it [01:30, 3346.32it/s]217297it [01:30, 3527.29it/s]210424it [01:30, 3445.70it/s]228378it [01:33, 3231.10it/s]215488it [01:30, 3305.00it/s]217652it [01:30, 3310.19it/s]210771it [01:30, 3207.69it/s]228704it [01:34, 3233.95it/s]215821it [01:30, 3134.73it/s]217987it [01:30, 3293.33it/s]211096it [01:30, 3148.73it/s]229030it [01:34, 3184.67it/s]216137it [01:30, 3093.56it/s]218319it [01:30, 3123.79it/s]211414it [01:31, 2794.32it/s]229350it [01:34, 2904.35it/s]216448it [01:30, 3041.24it/s]218689it [01:30, 3284.03it/s]211708it [01:31, 2831.31it/s]229716it [01:34, 3107.73it/s]216768it [01:30, 3085.02it/s]219036it [01:30, 3223.72it/s]212073it [01:31, 3051.12it/s]230033it [01:34, 3099.18it/s]217149it [01:30, 3292.08it/s]219419it [01:30, 3392.61it/s]212385it [01:31, 3051.84it/s]230397it [01:34, 3251.19it/s]217480it [01:31, 3262.89it/s]219781it [01:30, 3455.81it/s]212750it [01:31, 3219.27it/s]230764it [01:34, 3371.06it/s]217860it [01:31, 3419.41it/s]220129it [01:30, 3351.05it/s]213126it [01:31, 3373.65it/s]231105it [01:34, 3302.53it/s]218204it [01:31, 3369.89it/s]220517it [01:31, 3500.20it/s]213467it [01:31, 3287.59it/s]231476it [01:34, 3420.17it/s]218583it [01:31, 3492.39it/s]220870it [01:31, 3385.59it/s]213833it [01:31, 3394.41it/s]231821it [01:35, 3334.69it/s]218967it [01:31, 3586.49it/s]221243it [01:31, 3483.71it/s]214175it [01:31, 3299.95it/s]232190it [01:35, 3436.39it/s]219327it [01:31, 3490.81it/s]221594it [01:31, 3390.16it/s]214535it [01:32, 3384.91it/s]232536it [01:35, 3333.80it/s]219712it [01:31, 3593.24it/s]221972it [01:31, 3500.54it/s]214876it [01:32, 3281.31it/s]232902it [01:35, 3425.33it/s]220073it [01:31, 3494.40it/s]222333it [01:31, 3530.19it/s]215243it [01:32, 3391.66it/s]233279it [01:35, 3525.38it/s]220468it [01:31, 3623.88it/s]222688it [01:31, 3380.20it/s]215607it [01:32, 3462.30it/s]233633it [01:35, 3393.18it/s]220832it [01:31, 3483.63it/s]223054it [01:31, 3458.06it/s]215955it [01:32, 3352.81it/s]234006it [01:35, 3488.46it/s]221213it [01:32, 3576.25it/s]223402it [01:31, 3319.47it/s]216322it [01:32, 3442.52it/s]234357it [01:35, 3392.83it/s]221573it [01:32, 3490.48it/s]223768it [01:32, 3415.35it/s]216668it [01:32, 3343.92it/s]234720it [01:35, 3459.01it/s]221960it [01:32, 3597.66it/s]224112it [01:32, 3305.84it/s]217044it [01:32, 3463.05it/s]235068it [01:35, 3354.20it/s]222322it [01:32, 3603.25it/s]224477it [01:32, 3403.39it/s]217392it [01:32, 3358.66it/s]235433it [01:36, 3437.02it/s]222684it [01:32, 3475.68it/s]224820it [01:32, 3408.59it/s]217776it [01:32, 3495.79it/s]235798it [01:36, 3497.33it/s]223057it [01:32, 3548.36it/s]225163it [01:32, 3282.63it/s]218150it [01:33, 3566.02it/s]236149it [01:36, 3386.63it/s]223414it [01:32, 3421.77it/s]225523it [01:32, 3371.41it/s]218508it [01:33, 3446.41it/s]236510it [01:36, 3449.55it/s]223780it [01:32, 3489.05it/s]225862it [01:32, 3236.72it/s]218891it [01:33, 3554.39it/s]236857it [01:36, 3356.00it/s]224131it [01:32, 3394.42it/s]226221it [01:32, 3334.90it/s]219249it [01:33, 3409.50it/s]237224it [01:36, 3444.89it/s]224500it [01:32, 3477.81it/s]226581it [01:32, 3409.28it/s]219616it [01:33, 3481.95it/s]237570it [01:36, 3296.84it/s]224863it [01:33, 3520.20it/s]226924it [01:32, 3249.05it/s]219967it [01:33, 3352.15it/s]237922it [01:36, 3358.61it/s]225217it [01:33, 3274.64it/s]227253it [01:33, 3258.73it/s]238260it [01:36, 3332.91it/s]220305it [01:33, 3223.94it/s]225549it [01:33, 3259.82it/s]220630it [01:33, 3226.81it/s]238595it [01:37, 3122.16it/s]227581it [01:33, 2826.27it/s]225878it [01:33, 3072.21it/s]238915it [01:37, 3143.12it/s]220955it [01:33, 3076.71it/s]227886it [01:33, 2872.68it/s]226189it [01:33, 3068.92it/s]221296it [01:34, 3168.02it/s]228201it [01:33, 2946.27it/s]239232it [01:37, 3043.49it/s]226548it [01:33, 3213.90it/s]221627it [01:34, 3207.40it/s]239596it [01:37, 3211.58it/s]228503it [01:33, 2943.31it/s]226872it [01:33, 3150.40it/s]221996it [01:34, 3346.56it/s]239963it [01:37, 3341.02it/s]228869it [01:33, 3145.00it/s]227234it [01:33, 3284.20it/s]222358it [01:34, 3425.94it/s]229188it [01:33, 3096.48it/s]240300it [01:37, 3258.28it/s]227565it [01:33, 3198.25it/s]222702it [01:34, 3346.14it/s]229550it [01:33, 3244.93it/s]240651it [01:37, 3328.40it/s]227929it [01:34, 3324.13it/s]223067it [01:34, 3433.84it/s]229918it [01:33, 3370.35it/s]240986it [01:37, 3265.26it/s]228277it [01:34, 3248.93it/s]223412it [01:34, 3335.66it/s]241363it [01:37, 3408.89it/s]230258it [01:34, 3255.84it/s]228648it [01:34, 3378.93it/s]223782it [01:34, 3426.52it/s]230622it [01:34, 3365.11it/s]241719it [01:37, 3328.42it/s]229006it [01:34, 3435.85it/s]224126it [01:34, 3345.08it/s]242081it [01:38, 3411.25it/s]230961it [01:34, 3269.77it/s]229351it [01:34, 3339.60it/s]224499it [01:34, 3446.56it/s]242453it [01:38, 3498.88it/s]231335it [01:34, 3404.15it/s]229726it [01:34, 3457.87it/s]224862it [01:35, 3498.48it/s]242805it [01:38, 3388.86it/s]231678it [01:34, 3286.99it/s]230074it [01:34, 3342.33it/s]225213it [01:35, 3378.66it/s]243178it [01:38, 3484.66it/s]232044it [01:34, 3393.25it/s]230432it [01:34, 3409.68it/s]225575it [01:35, 3446.88it/s]232408it [01:34, 3463.79it/s]243528it [01:38, 3363.11it/s]230797it [01:34, 3311.89it/s]225921it [01:35, 3321.17it/s]243898it [01:38, 3457.13it/s]232756it [01:34, 3330.34it/s]231176it [01:35, 3447.30it/s]226278it [01:35, 3391.94it/s]233127it [01:34, 3437.46it/s]244246it [01:38, 3358.10it/s]231545it [01:35, 3516.06it/s]226619it [01:35, 3291.12it/s]244605it [01:38, 3422.54it/s]233473it [01:35, 3321.63it/s]231899it [01:35, 3374.91it/s]226972it [01:35, 3357.58it/s]244976it [01:38, 3504.92it/s]233842it [01:35, 3425.36it/s]232267it [01:35, 3451.48it/s]227330it [01:35, 3419.91it/s]245328it [01:39, 3383.96it/s]234187it [01:35, 3272.67it/s]232614it [01:35, 3349.60it/s]227674it [01:35, 3312.29it/s]245699it [01:39, 3477.18it/s]234552it [01:35, 3378.15it/s]232982it [01:35, 3441.52it/s]228032it [01:36, 3387.71it/s]234920it [01:35, 3463.26it/s]246049it [01:39, 3352.85it/s]233328it [01:35, 3328.03it/s]228372it [01:36, 3293.14it/s]246415it [01:39, 3438.93it/s]235269it [01:35, 3331.77it/s]233701it [01:35, 3441.26it/s]228731it [01:36, 3377.32it/s]246761it [01:39, 3349.16it/s]235626it [01:35, 3399.43it/s]234073it [01:35, 3519.79it/s]229088it [01:36, 3432.71it/s]247128it [01:39, 3438.94it/s]235968it [01:35, 3288.01it/s]234427it [01:35, 3297.81it/s]229433it [01:36, 3271.10it/s]247479it [01:39, 3459.08it/s]236331it [01:35, 3383.62it/s]234781it [01:36, 3365.32it/s]229771it [01:36, 3301.82it/s]236672it [01:35, 3376.16it/s]247826it [01:39, 3176.31it/s]235121it [01:36, 3142.84it/s]230103it [01:36, 3085.88it/s]237011it [01:36, 3165.62it/s]248149it [01:39, 3004.19it/s]235440it [01:36, 3149.68it/s]230416it [01:36, 3096.65it/s]237334it [01:36, 3183.39it/s]248454it [01:40, 2901.62it/s]235759it [01:36, 3154.59it/s]230737it [01:36, 3127.42it/s]237655it [01:36, 3044.84it/s]248795it [01:40, 3039.80it/s]236077it [01:36, 3089.77it/s]231052it [01:36, 3119.82it/s]238015it [01:36, 3198.62it/s]249160it [01:40, 3209.29it/s]236435it [01:36, 3229.62it/s]231413it [01:37, 3261.17it/s]238356it [01:36, 3134.29it/s]249485it [01:40, 3170.67it/s]236760it [01:36, 3172.29it/s]231741it [01:37, 3198.22it/s]238720it [01:36, 3275.70it/s]249842it [01:40, 3284.70it/s]237115it [01:36, 3280.43it/s]232094it [01:37, 3294.29it/s]239064it [01:36, 3321.47it/s]250173it [01:40, 3210.77it/s]237482it [01:36, 3392.96it/s]232456it [01:37, 3388.01it/s]239399it [01:36, 3222.62it/s]250539it [01:40, 3338.81it/s]237823it [01:37, 3294.45it/s]232796it [01:37, 3283.55it/s]239764it [01:36, 3344.75it/s]250905it [01:40, 3429.83it/s]238190it [01:37, 3402.01it/s]233161it [01:37, 3389.44it/s]240101it [01:37, 3245.27it/s]251250it [01:40, 3326.23it/s]238532it [01:37, 3273.38it/s]233502it [01:37, 3302.83it/s]240461it [01:37, 3344.89it/s]251607it [01:40, 3394.49it/s]238897it [01:37, 3379.90it/s]233865it [01:37, 3397.15it/s]240826it [01:37, 3431.41it/s]251948it [01:41, 3298.40it/s]239237it [01:37, 3274.59it/s]234206it [01:37, 3313.08it/s]252317it [01:41, 3411.08it/s]241171it [01:37, 3310.93it/s]239604it [01:37, 3385.87it/s]234565it [01:38, 3390.67it/s]241538it [01:37, 3413.23it/s]252660it [01:41, 3307.23it/s]239963it [01:37, 3444.63it/s]234927it [01:38, 3457.41it/s]253021it [01:41, 3392.37it/s]241881it [01:37, 3305.62it/s]240309it [01:37, 3324.49it/s]235274it [01:38, 3345.90it/s]253387it [01:41, 3469.36it/s]242247it [01:37, 3405.03it/s]240673it [01:37, 3412.96it/s]235633it [01:38, 3414.10it/s]242590it [01:37, 3285.57it/s]241016it [01:37, 3323.80it/s]235976it [01:38, 3336.78it/s]242956it [01:37, 3390.50it/s]241379it [01:38, 3409.32it/s]236340it [01:38, 3424.44it/s]243325it [01:37, 3476.65it/s]241722it [01:38, 3315.15it/s]236684it [01:38, 3328.90it/s]243675it [01:38, 3346.06it/s]242092it [01:38, 3425.26it/s]237054it [01:38, 3435.65it/s]244026it [01:38, 3384.77it/s]242461it [01:38, 3500.87it/s]237420it [01:38, 3499.42it/s]244366it [01:38, 3274.40it/s]242813it [01:38, 3357.25it/s]237771it [01:38, 3369.20it/s]244726it [01:38, 3366.26it/s]243183it [01:38, 3453.14it/s]238133it [01:39, 3439.20it/s]245076it [01:38, 3258.26it/s]243531it [01:38, 3355.49it/s]238479it [01:39, 3303.91it/s]245439it [01:38, 3362.10it/s]243890it [01:38, 3420.10it/s]238826it [01:39, 3350.40it/s]245802it [01:38, 3438.94it/s]244234it [01:38, 3416.23it/s]239163it [01:39, 3343.88it/s]246148it [01:38, 3206.15it/s]244577it [01:39, 3176.07it/s]239499it [01:39, 3081.62it/s]246475it [01:38, 3222.44it/s]244899it [01:39, 3140.63it/s]239812it [01:39, 3030.25it/s]246801it [01:39, 3041.41it/s]245216it [01:39, 2992.85it/s]240118it [01:39, 2930.09it/s]247144it [01:39, 3148.20it/s]245580it [01:39, 3171.15it/s]240479it [01:39, 3118.52it/s]247497it [01:39, 3254.89it/s]245917it [01:39, 3139.76it/s]240844it [01:39, 3268.04it/s]247826it [01:39, 3179.94it/s]246285it [01:39, 3291.04it/s]241174it [01:40, 3181.41it/s]248194it [01:39, 3320.74it/s]246643it [01:39, 3373.20it/s]241546it [01:40, 3333.56it/s]248529it [01:39, 3228.64it/s]246983it [01:39, 3287.13it/s]241882it [01:40, 3231.56it/s]248879it [01:39, 3304.51it/s]247350it [01:39, 3396.10it/s]242251it [01:40, 3361.92it/s]249247it [01:39, 3412.76it/s]247692it [01:40, 3303.59it/s]242590it [01:40, 3282.28it/s]249590it [01:39, 3253.24it/s]248051it [01:40, 3384.42it/s]242957it [01:40, 3393.10it/s]249943it [01:40, 3329.64it/s]248413it [01:40, 3452.76it/s]243316it [01:40, 3449.87it/s]250279it [01:40, 3160.00it/s]248760it [01:40, 3342.86it/s]243663it [01:40, 3363.93it/s]250637it [01:40, 3275.79it/s]249128it [01:40, 3438.08it/s]244029it [01:40, 3443.95it/s]250968it [01:40, 3115.83it/s]249474it [01:40, 3316.56it/s]244375it [01:40, 3350.77it/s]251324it [01:40, 3238.85it/s]249837it [01:40, 3400.09it/s]244739it [01:41, 3432.54it/s]251674it [01:40, 3313.10it/s]250179it [01:40, 3317.94it/s]245084it [01:41, 3340.12it/s]252008it [01:40, 3149.54it/s]250547it [01:40, 3421.27it/s]245420it [01:41, 3257.43it/s]252364it [01:40, 3264.95it/s]250908it [01:40, 3474.12it/s]245783it [01:41, 3363.83it/s]251257it [01:41, 3352.98it/s]252694it [01:40, 3101.89it/s]246121it [01:41, 3301.22it/s]251622it [01:41, 3436.34it/s]253053it [01:40, 3236.96it/s]246483it [01:41, 3393.27it/s]253403it [01:41, 3309.44it/s]251968it [01:41, 3314.34it/s]246824it [01:41, 3317.11it/s]252331it [01:41, 3403.41it/s]247192it [01:41, 3421.27it/s]247557it [01:41, 3486.88it/s]252673it [01:41, 3292.59it/s]253043it [01:41, 3407.32it/s]247907it [01:42, 3318.58it/s]253401it [01:41, 3455.88it/s]248266it [01:42, 3394.11it/s]248608it [01:42, 3204.21it/s]248932it [01:42, 3193.64it/s]249264it [01:42, 3228.25it/s]249589it [01:42, 3066.18it/s]249963it [01:42, 3254.41it/s]250292it [01:42, 3216.64it/s]250661it [01:42, 3352.07it/s]250999it [01:42, 3300.53it/s]251352it [01:43, 3365.14it/s]251720it [01:43, 3455.43it/s]252067it [01:43, 3368.40it/s]252428it [01:43, 3436.99it/s]252773it [01:43, 3353.06it/s]253127it [01:43, 3405.30it/s]253479it [01:43, 3332.66it/s]253736it [01:49, 140.54it/s] 254106it [01:49, 199.89it/s]254410it [01:49, 265.95it/s]254774it [01:49, 374.47it/s]255147it [01:50, 521.84it/s]255477it [01:50, 683.29it/s]255852it [01:50, 920.88it/s]256191it [01:50, 1155.10it/s]256562it [01:50, 1470.73it/s]256908it [01:50, 1745.54it/s]257277it [01:50, 2084.45it/s]257642it [01:50, 2395.89it/s]257993it [01:50, 2579.44it/s]258367it [01:50, 2851.74it/s]258718it [01:51, 2916.03it/s]259084it [01:51, 3105.93it/s]259430it [01:51, 3039.24it/s]259768it [01:51, 3129.31it/s]260100it [01:51, 3169.73it/s]260431it [01:51, 3037.21it/s]260808it [01:51, 3237.38it/s]261141it [01:51, 3210.33it/s]261511it [01:51, 3347.27it/s]261885it [01:52, 3458.72it/s]262235it [01:52, 3366.42it/s]262610it [01:52, 3474.25it/s]262961it [01:52, 3363.91it/s]263332it [01:52, 3462.75it/s]253737it [01:48, 145.98it/s] 263681it [01:52, 3360.75it/s]254087it [01:48, 205.90it/s]264048it [01:52, 3448.88it/s]254384it [01:48, 273.84it/s]264417it [01:52, 3516.30it/s]254727it [01:49, 381.87it/s]264771it [01:52, 3390.56it/s]255076it [01:49, 526.77it/s]265144it [01:52, 3487.69it/s]255384it [01:49, 680.34it/s]265495it [01:53, 3380.68it/s]255739it [01:49, 913.33it/s]265858it [01:53, 3451.58it/s]256064it [01:49, 1135.14it/s]266205it [01:53, 3354.29it/s]256414it [01:49, 1434.46it/s]266574it [01:53, 3449.59it/s]256766it [01:49, 1754.37it/s]253748it [01:49, 140.01it/s] 266943it [01:53, 3518.83it/s]257093it [01:49, 1968.65it/s]254111it [01:49, 198.09it/s]267297it [01:53, 3390.34it/s]257439it [01:49, 2266.37it/s]254415it [01:50, 263.79it/s]267669it [01:53, 3484.77it/s]257762it [01:49, 2304.19it/s]254780it [01:50, 372.38it/s]268020it [01:53, 3394.44it/s]258120it [01:50, 2593.07it/s]255139it [01:50, 513.29it/s]268382it [01:53, 3457.30it/s]258470it [01:50, 2813.81it/s]255462it [01:50, 667.05it/s]268729it [01:54, 3290.53it/s]255806it [01:50, 880.26it/s]258796it [01:50, 2641.01it/s]269061it [01:54, 3263.23it/s]259102it [01:50, 2745.10it/s]256126it [01:50, 1089.52it/s]269389it [01:54, 3245.57it/s]259402it [01:50, 2709.94it/s]256453it [01:50, 1355.02it/s]269715it [01:54, 3035.85it/s]256765it [01:50, 1590.70it/s]259691it [01:50, 2606.01it/s]270023it [01:54, 3047.81it/s]257068it [01:50, 1837.16it/s]260017it [01:50, 2778.69it/s]270348it [01:54, 3059.99it/s]257427it [01:51, 2182.62it/s]260306it [01:50, 2760.88it/s]270721it [01:54, 3250.59it/s]257746it [01:51, 2373.76it/s]260628it [01:50, 2886.72it/s]271084it [01:54, 3360.35it/s]258112it [01:51, 2677.99it/s]260987it [01:51, 3085.71it/s]271422it [01:54, 3301.08it/s]258478it [01:51, 2924.80it/s]261302it [01:51, 2997.37it/s]271793it [01:54, 3418.28it/s]258819it [01:51, 2970.50it/s]261654it [01:51, 3145.60it/s]272137it [01:55, 3336.96it/s]259186it [01:51, 3157.58it/s]261973it [01:51, 3024.52it/s]272498it [01:55, 3415.36it/s]259528it [01:51, 3132.27it/s]262330it [01:51, 3177.34it/s]272867it [01:55, 3493.13it/s]259894it [01:51, 3278.52it/s]262683it [01:51, 3276.61it/s]273218it [01:55, 3377.48it/s]260254it [01:51, 3368.61it/s]273589it [01:55, 3473.13it/s]263014it [01:51, 3101.92it/s]260601it [01:51, 3277.11it/s]263366it [01:51, 3218.21it/s]273938it [01:55, 3356.87it/s]260965it [01:52, 3374.08it/s]274307it [01:55, 3449.88it/s]263691it [01:51, 3089.38it/s]261309it [01:52, 3314.98it/s]264045it [01:52, 3214.70it/s]274654it [01:55, 3350.99it/s]261673it [01:52, 3407.77it/s]264405it [01:52, 3325.10it/s]275026it [01:55, 3456.63it/s]262018it [01:52, 3280.42it/s]275388it [01:56, 3346.91it/s]264740it [01:52, 3124.18it/s]262385it [01:52, 3389.43it/s]275756it [01:56, 3441.07it/s]265094it [01:52, 3237.04it/s]262753it [01:52, 3471.83it/s]253814it [01:53, 120.71it/s] 276126it [01:56, 3513.74it/s]265422it [01:52, 3110.23it/s]263103it [01:52, 3368.64it/s]254177it [01:53, 172.23it/s]276479it [01:56, 3408.07it/s]265781it [01:52, 3243.62it/s]263468it [01:52, 3448.32it/s]254485it [01:53, 231.96it/s]276838it [01:56, 3459.66it/s]266135it [01:52, 3326.68it/s]254841it [01:53, 326.85it/s]263815it [01:52, 3354.28it/s]277186it [01:56, 3382.96it/s]266471it [01:52, 3165.88it/s]255207it [01:53, 457.26it/s]264169it [01:53, 3405.54it/s]277553it [01:56, 3464.26it/s]266809it [01:52, 3224.79it/s]255531it [01:53, 602.01it/s]264511it [01:53, 3294.56it/s]277908it [01:56, 3285.02it/s]267134it [01:53, 3055.98it/s]255891it [01:53, 811.88it/s]264868it [01:53, 3372.72it/s]278268it [01:56, 3371.28it/s]267491it [01:53, 3197.35it/s]265224it [01:53, 3424.51it/s]256221it [01:53, 1025.11it/s]278608it [01:57, 3288.61it/s]267814it [01:53, 3164.27it/s]256539it [01:53, 1249.45it/s]265568it [01:53, 3187.57it/s]268133it [01:53, 2934.67it/s]278939it [01:57, 2915.95it/s]265891it [01:53, 3144.53it/s]256845it [01:54, 1452.14it/s]268447it [01:53, 2989.93it/s]279246it [01:57, 2955.11it/s]266208it [01:53, 2991.57it/s]257134it [01:54, 1653.63it/s]279585it [01:57, 3072.42it/s]268750it [01:53, 2776.57it/s]266568it [01:53, 3159.97it/s]257477it [01:54, 1979.52it/s]279898it [01:57, 3058.04it/s]269094it [01:53, 2956.53it/s]266933it [01:53, 3296.91it/s]257776it [01:54, 2184.96it/s]280260it [01:57, 3217.49it/s]269441it [01:53, 3099.41it/s]267266it [01:53, 3250.34it/s]258141it [01:54, 2518.87it/s]280586it [01:57, 3148.21it/s]269756it [01:53, 3008.77it/s]267628it [01:54, 3355.02it/s]258513it [01:54, 2813.00it/s]280956it [01:57, 3304.62it/s]270115it [01:53, 3171.51it/s]267966it [01:54, 3282.99it/s]258847it [01:54, 2892.77it/s]281289it [01:57, 3218.34it/s]270436it [01:54, 3024.64it/s]268338it [01:54, 3408.93it/s]259217it [01:54, 3105.70it/s]281656it [01:57, 3346.75it/s]270775it [01:54, 3125.81it/s]268681it [01:54, 3315.48it/s]259557it [01:54, 3101.44it/s]282007it [01:58, 3393.53it/s]271125it [01:54, 3232.66it/s]269051it [01:54, 3425.70it/s]259927it [01:54, 3266.98it/s]282348it [01:58, 3304.96it/s]271451it [01:54, 3094.32it/s]269408it [01:54, 3467.21it/s]260270it [01:55, 3217.60it/s]282710it [01:58, 3393.73it/s]271799it [01:54, 3202.39it/s]260639it [01:55, 3349.98it/s]269756it [01:54, 3355.87it/s]283051it [01:58, 3280.53it/s]272122it [01:54, 3055.45it/s]261004it [01:55, 3435.42it/s]270131it [01:54, 3468.63it/s]283400it [01:58, 3340.35it/s]272474it [01:54, 3186.09it/s]261354it [01:55, 3367.19it/s]270480it [01:54, 3351.74it/s]283763it [01:58, 3424.15it/s]272823it [01:54, 3270.96it/s]261717it [01:55, 3442.54it/s]270845it [01:55, 3434.72it/s]284107it [01:58, 3306.78it/s]262065it [01:55, 3370.06it/s]273153it [01:54, 3068.26it/s]271190it [01:55, 3302.68it/s]284471it [01:58, 3402.49it/s]262428it [01:55, 3443.10it/s]273503it [01:55, 3186.73it/s]271558it [01:55, 3407.77it/s]284813it [01:58, 3265.96it/s]271928it [01:55, 3490.77it/s]262787it [01:55, 3361.17it/s]273826it [01:55, 3055.91it/s]285180it [01:59, 3379.71it/s]263152it [01:55, 3442.81it/s]274179it [01:55, 3187.97it/s]272279it [01:55, 3370.92it/s]285520it [01:59, 3282.27it/s]263522it [01:55, 3516.55it/s]274527it [01:55, 3270.60it/s]272643it [01:55, 3448.09it/s]285887it [01:59, 3391.31it/s]263876it [01:56, 3404.07it/s]272990it [01:55, 3319.78it/s]274857it [01:55, 3065.97it/s]286238it [01:59, 3424.89it/s]264251it [01:56, 3501.04it/s]273353it [01:55, 3406.08it/s]275214it [01:55, 3204.82it/s]286582it [01:59, 3331.10it/s]264603it [01:56, 3376.32it/s]273706it [01:55, 3312.31it/s]275539it [01:55, 3063.61it/s]286952it [01:59, 3435.44it/s]287112it [01:59, 2400.85it/s]
264972it [01:56, 3463.78it/s]274075it [01:55, 3418.09it/s]275889it [01:55, 3184.95it/s]274431it [01:56, 3458.62it/s]265321it [01:56, 3294.07it/s]276224it [01:55, 2993.26it/s]265653it [01:56, 3276.17it/s]274779it [01:56, 3248.86it/s]276528it [01:56, 2975.60it/s]265983it [01:56, 3030.77it/s]275108it [01:56, 3108.43it/s]276838it [01:56, 3008.19it/s]275422it [01:56, 2989.02it/s]277142it [01:56, 2768.56it/s]266291it [01:56, 2613.95it/s]275746it [01:56, 3057.80it/s]277461it [01:56, 2881.47it/s]266627it [01:57, 2801.14it/s]276096it [01:56, 3181.68it/s]277828it [01:56, 3101.98it/s]266987it [01:57, 2902.82it/s]276417it [01:56, 3147.53it/s]278143it [01:56, 3014.04it/s]267351it [01:57, 3098.02it/s]276781it [01:56, 3288.51it/s]278497it [01:56, 3160.96it/s]267721it [01:57, 3262.91it/s]277112it [01:56, 3226.94it/s]278817it [01:56, 2989.58it/s]268055it [01:57, 3225.37it/s]277479it [01:57, 3355.23it/s]279171it [01:56, 3141.99it/s]268429it [01:57, 3371.03it/s]277844it [01:57, 3439.65it/s]279521it [01:57, 3243.39it/s]268771it [01:57, 3285.78it/s]278190it [01:57, 3301.07it/s]279849it [01:57, 3097.46it/s]269142it [01:57, 3405.43it/s]278561it [01:57, 3416.27it/s]280195it [01:57, 3197.41it/s]269503it [01:57, 3464.57it/s]278905it [01:57, 3313.90it/s]269852it [01:57, 3377.25it/s]280518it [01:57, 3049.42it/s]279273it [01:57, 3417.28it/s]270213it [01:58, 3444.35it/s]280876it [01:57, 3196.14it/s]279617it [01:57, 3291.25it/s]281222it [01:57, 3270.23it/s]270560it [01:58, 3347.73it/s]279984it [01:57, 3397.31it/s]270920it [01:58, 3419.24it/s]281552it [01:57, 3117.75it/s]280348it [01:57, 3466.94it/s]271264it [01:58, 3322.57it/s]281899it [01:57, 3215.92it/s]280697it [01:58, 3359.53it/s]271621it [01:58, 3391.82it/s]282224it [01:57, 3046.06it/s]281063it [01:58, 3443.64it/s]271966it [01:58, 3406.20it/s]282532it [01:57, 3039.85it/s]281409it [01:58, 3291.55it/s]272308it [01:58, 3255.24it/s]282881it [01:58, 3167.09it/s]281771it [01:58, 3384.49it/s]272671it [01:58, 3360.78it/s]283200it [01:58, 3035.75it/s]282112it [01:58, 3284.00it/s]273009it [01:58, 3267.98it/s]283547it [01:58, 3157.28it/s]282479it [01:58, 3393.02it/s]273371it [01:59, 3367.12it/s]283866it [01:58, 3017.95it/s]282843it [01:58, 3462.91it/s]273710it [01:59, 3280.21it/s]284218it [01:58, 3159.13it/s]283191it [01:58, 3308.75it/s]274082it [01:59, 3404.36it/s]284565it [01:58, 3246.71it/s]283550it [01:58, 3386.47it/s]274435it [01:59, 3438.27it/s]284892it [01:58, 3012.92it/s]283891it [01:58, 3203.93it/s]274780it [01:59, 3260.04it/s]285226it [01:58, 3103.18it/s]284226it [01:59, 3244.80it/s]275109it [01:59, 3246.01it/s]285541it [01:58, 2870.75it/s]284553it [01:59, 3228.89it/s]275436it [01:59, 2978.81it/s]285858it [01:59, 2950.00it/s]284878it [01:59, 3011.23it/s]275761it [01:59, 3052.03it/s]286171it [01:59, 2999.49it/s]285207it [01:59, 3087.93it/s]276100it [01:59, 3126.58it/s]286475it [01:59, 2875.31it/s]285519it [01:59, 3086.57it/s]276416it [01:59, 3127.06it/s]286831it [01:59, 3065.58it/s]285885it [01:59, 3249.63it/s]276782it [02:00, 3280.75it/s]287112it [01:59, 2402.96it/s]
286247it [01:59, 3356.80it/s]277113it [02:00, 3220.30it/s]286585it [01:59, 3293.89it/s]277481it [02:00, 3351.28it/s]286954it [01:59, 3408.70it/s]277842it [02:00, 3424.73it/s]287112it [01:59, 2393.47it/s]
2022-07-08 16:00:38 | INFO | root | success load 287112 data
2022-07-08 16:00:38 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-08 16:00:38 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-08 16:00:38 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-08 16:00:38 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-08 16:00:38 | INFO | transformer.tokenization_utils | loading file None
2022-07-08 16:00:38 | INFO | transformer.tokenization_utils | loading file None
2022-07-08 16:00:38 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
278186it [02:00, 3344.03it/s]278551it [02:00, 3430.23it/s]278896it [02:00, 3347.12it/s]279257it [02:00, 3422.63it/s]279601it [02:00, 3345.92it/s]279960it [02:01, 3414.56it/s]280330it [02:01, 3497.46it/s]280681it [02:01, 3375.22it/s]281057it [02:01, 3485.54it/s]281407it [02:01, 3363.32it/s]281776it [02:01, 3455.84it/s]282124it [02:01, 3341.01it/s]282481it [02:01, 3396.59it/s]282851it [02:01, 3483.70it/s]283201it [02:01, 3381.71it/s]283567it [02:02, 3461.02it/s]283915it [02:02, 3294.41it/s]284247it [02:02, 3236.60it/s]284573it [02:02, 3230.49it/s]284898it [02:02, 3032.61it/s]285217it [02:02, 3074.44it/s]285527it [02:02, 2963.42it/s]285867it [02:02, 3083.58it/s]286217it [02:02, 3200.76it/s]286540it [02:03, 3161.22it/s]286912it [02:03, 3322.48it/s]287112it [02:03, 2330.83it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-08 16:04:36 | INFO | train_inner | epoch 012:     76 / 1122 loss=6.869, nll_loss=2.571, mask_ins=0.959, word_ins_ml=4.211, word_reposition=0.796, kpe=0.903, ppl=116.87, wps=3944.1, ups=0.19, wpb=20450, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=1.641, clip=0, loss_scale=512, train_wall=270, wall=0
2022-07-08 16:09:39 | INFO | train_inner | epoch 012:    176 / 1122 loss=6.778, nll_loss=2.496, mask_ins=0.94, word_ins_ml=4.144, word_reposition=0.792, kpe=0.902, ppl=109.73, wps=6754.2, ups=0.33, wpb=20499, bsz=256, num_updates=12500, lr=0.000316228, gnorm=1.619, clip=0, loss_scale=512, train_wall=261, wall=0
2022-07-08 16:14:45 | INFO | train_inner | epoch 012:    276 / 1122 loss=6.832, nll_loss=2.561, mask_ins=0.945, word_ins_ml=4.201, word_reposition=0.787, kpe=0.899, ppl=113.92, wps=6712.5, ups=0.33, wpb=20515.5, bsz=256, num_updates=12600, lr=0.00031497, gnorm=1.601, clip=0, loss_scale=512, train_wall=260, wall=0
2022-07-08 16:19:51 | INFO | train_inner | epoch 012:    376 / 1122 loss=6.789, nll_loss=2.518, mask_ins=0.941, word_ins_ml=4.163, word_reposition=0.785, kpe=0.899, ppl=110.56, wps=6742.7, ups=0.33, wpb=20609.7, bsz=256, num_updates=12700, lr=0.000313728, gnorm=1.504, clip=0, loss_scale=512, train_wall=258, wall=0
2022-07-08 16:22:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-08 16:22:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-08 16:25:03 | INFO | train_inner | epoch 012:    478 / 1122 loss=6.822, nll_loss=2.545, mask_ins=0.942, word_ins_ml=4.186, word_reposition=0.792, kpe=0.902, ppl=113.14, wps=6563, ups=0.32, wpb=20503, bsz=256, num_updates=12800, lr=0.0003125, gnorm=1.649, clip=0, loss_scale=344, train_wall=266, wall=0
2022-07-08 16:30:32 | INFO | train_inner | epoch 012:    578 / 1122 loss=6.807, nll_loss=2.543, mask_ins=0.934, word_ins_ml=4.184, word_reposition=0.788, kpe=0.901, ppl=112, wps=6262.3, ups=0.3, wpb=20574.2, bsz=256, num_updates=12900, lr=0.000311286, gnorm=1.63, clip=0, loss_scale=128, train_wall=287, wall=0
2022-07-08 16:35:36 | INFO | train_inner | epoch 012:    678 / 1122 loss=6.76, nll_loss=2.491, mask_ins=0.936, word_ins_ml=4.138, word_reposition=0.786, kpe=0.9, ppl=108.41, wps=6720.9, ups=0.33, wpb=20426.7, bsz=256, num_updates=13000, lr=0.000310087, gnorm=1.506, clip=0, loss_scale=128, train_wall=261, wall=0
2022-07-08 16:40:40 | INFO | train_inner | epoch 012:    778 / 1122 loss=6.782, nll_loss=2.521, mask_ins=0.934, word_ins_ml=4.165, word_reposition=0.785, kpe=0.899, ppl=110.08, wps=6769.7, ups=0.33, wpb=20591.3, bsz=256, num_updates=13100, lr=0.000308901, gnorm=1.502, clip=0, loss_scale=128, train_wall=262, wall=0
2022-07-08 16:45:48 | INFO | train_inner | epoch 012:    878 / 1122 loss=nan, nll_loss=2.541, mask_ins=0.946, word_ins_ml=4.182, word_reposition=0.792, kpe=nan, ppl=nan, wps=6671.5, ups=0.32, wpb=20541.3, bsz=256, num_updates=13200, lr=0.000307729, gnorm=1.598, clip=0, loss_scale=128, train_wall=261, wall=0
2022-07-08 16:50:51 | INFO | train_inner | epoch 012:    978 / 1122 loss=6.769, nll_loss=2.5, mask_ins=0.93, word_ins_ml=4.147, word_reposition=0.786, kpe=0.906, ppl=109.08, wps=6772.3, ups=0.33, wpb=20545.3, bsz=256, num_updates=13300, lr=0.00030657, gnorm=1.656, clip=0, loss_scale=169, train_wall=260, wall=0
2022-07-08 16:55:58 | INFO | train_inner | epoch 012:   1078 / 1122 loss=nan, nll_loss=2.505, mask_ins=0.928, word_ins_ml=4.151, word_reposition=0.781, kpe=nan, ppl=nan, wps=6670.3, ups=0.33, wpb=20467.4, bsz=256, num_updates=13400, lr=0.000305424, gnorm=1.912, clip=0, loss_scale=256, train_wall=260, wall=0
2022-07-08 16:58:11 | INFO | train | epoch 012 | loss nan | nll_loss 2.527 | mask_ins 0.939 | word_ins_ml 4.17 | word_reposition 0.788 | kpe nan | ppl nan | wps 6284.9 | ups 0.31 | wpb 20520.9 | bsz 255.8 | num_updates 13444 | lr 0.000304923 | gnorm 1.627 | clip 0 | loss_scale 296 | train_wall 2952 | wall 0
2022-07-08 16:59:35 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.224 | nll_loss 5.659 | mask_ins 1.469 | word_ins_ml 7.089 | word_reposition 1.264 | kpe 1.402 | ppl 2391.61 | wps 11762 | wpb 2367.6 | bsz 32 | num_updates 13444 | best_loss 11.224
2022-07-08 16:59:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 12 @ 13444 updates, score 11.224) (writing took 8.858850304968655 seconds)
2022-07-08 17:02:35 | INFO | train_inner | epoch 013:     56 / 1122 loss=6.795, nll_loss=2.508, mask_ins=0.945, word_ins_ml=4.152, word_reposition=0.795, kpe=0.903, ppl=111.01, wps=5142.4, ups=0.25, wpb=20394.3, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=1.911, clip=0, loss_scale=256, train_wall=260, wall=0
2022-07-08 17:07:40 | INFO | train_inner | epoch 013:    156 / 1122 loss=6.784, nll_loss=2.529, mask_ins=0.932, word_ins_ml=4.171, word_reposition=0.784, kpe=0.898, ppl=110.23, wps=6745.1, ups=0.33, wpb=20585.5, bsz=256, num_updates=13600, lr=0.00030317, gnorm=1.525, clip=0, loss_scale=256, train_wall=261, wall=0
2022-07-08 17:12:44 | INFO | train_inner | epoch 013:    256 / 1122 loss=6.771, nll_loss=2.517, mask_ins=0.935, word_ins_ml=4.16, word_reposition=0.783, kpe=0.894, ppl=109.24, wps=6714.1, ups=0.33, wpb=20453, bsz=256, num_updates=13700, lr=0.000302061, gnorm=1.552, clip=0, loss_scale=256, train_wall=258, wall=0
2022-07-08 17:17:49 | INFO | train_inner | epoch 013:    356 / 1122 loss=nan, nll_loss=2.516, mask_ins=0.921, word_ins_ml=4.159, word_reposition=0.781, kpe=nan, ppl=nan, wps=6750.5, ups=0.33, wpb=20534.9, bsz=256, num_updates=13800, lr=0.000300965, gnorm=1.524, clip=0, loss_scale=307, train_wall=260, wall=0
2022-07-08 17:22:52 | INFO | train_inner | epoch 013:    456 / 1122 loss=6.76, nll_loss=2.508, mask_ins=0.927, word_ins_ml=4.153, word_reposition=0.787, kpe=0.894, ppl=108.37, wps=6789.5, ups=0.33, wpb=20605, bsz=256, num_updates=13900, lr=0.00029988, gnorm=1.486, clip=0, loss_scale=512, train_wall=259, wall=0
2022-07-08 17:27:56 | INFO | train_inner | epoch 013:    556 / 1122 loss=6.736, nll_loss=2.505, mask_ins=0.919, word_ins_ml=4.149, word_reposition=0.779, kpe=0.889, ppl=106.58, wps=6739.6, ups=0.33, wpb=20508.6, bsz=256, num_updates=14000, lr=0.000298807, gnorm=1.489, clip=0, loss_scale=512, train_wall=260, wall=0
2022-07-08 17:33:02 | INFO | train_inner | epoch 013:    656 / 1122 loss=6.75, nll_loss=2.505, mask_ins=0.934, word_ins_ml=4.15, word_reposition=0.774, kpe=0.892, ppl=107.62, wps=6689.2, ups=0.33, wpb=20465.6, bsz=256, num_updates=14100, lr=0.000297746, gnorm=1.514, clip=0, loss_scale=512, train_wall=258, wall=0
2022-07-08 17:38:49 | INFO | train_inner | epoch 013:    756 / 1122 loss=nan, nll_loss=2.513, mask_ins=0.914, word_ins_ml=4.156, word_reposition=0.784, kpe=nan, ppl=nan, wps=5926.7, ups=0.29, wpb=20550, bsz=256, num_updates=14200, lr=0.000296695, gnorm=1.491, clip=0, loss_scale=512, train_wall=295, wall=0
2022-07-08 17:43:58 | INFO | train_inner | epoch 013:    856 / 1122 loss=6.77, nll_loss=2.528, mask_ins=0.93, word_ins_ml=4.169, word_reposition=0.782, kpe=0.889, ppl=109.14, wps=6629.9, ups=0.32, wpb=20513.8, bsz=256, num_updates=14300, lr=0.000295656, gnorm=1.492, clip=0, loss_scale=553, train_wall=263, wall=0
2022-07-08 17:49:04 | INFO | train_inner | epoch 013:    956 / 1122 loss=6.724, nll_loss=2.477, mask_ins=0.926, word_ins_ml=4.123, word_reposition=0.783, kpe=0.892, ppl=105.74, wps=6698.8, ups=0.33, wpb=20479.1, bsz=256, num_updates=14400, lr=0.000294628, gnorm=1.437, clip=0, loss_scale=1024, train_wall=258, wall=0
2022-07-08 17:54:09 | INFO | train_inner | epoch 013:   1056 / 1122 loss=6.714, nll_loss=2.478, mask_ins=0.921, word_ins_ml=4.125, word_reposition=0.78, kpe=0.887, ppl=104.97, wps=6767.7, ups=0.33, wpb=20644.2, bsz=256, num_updates=14500, lr=0.00029361, gnorm=1.416, clip=0, loss_scale=1024, train_wall=257, wall=0
2022-07-08 17:57:27 | INFO | train | epoch 013 | loss nan | nll_loss 2.505 | mask_ins 0.927 | word_ins_ml 4.149 | word_reposition 0.782 | kpe nan | ppl nan | wps 6475.2 | ups 0.32 | wpb 20520.1 | bsz 255.8 | num_updates 14566 | lr 0.000292944 | gnorm 1.517 | clip 0 | loss_scale 560 | train_wall 2945 | wall 0
2022-07-08 17:58:50 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 11.274 | nll_loss 5.702 | mask_ins 1.474 | word_ins_ml 7.136 | word_reposition 1.259 | kpe 1.406 | ppl 2476.82 | wps 11849.9 | wpb 2367.6 | bsz 32 | num_updates 14566 | best_loss 11.224
2022-07-08 17:58:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 13 @ 14566 updates, score 11.274) (writing took 4.8140237014740705 seconds)
2022-07-08 18:00:38 | INFO | train_inner | epoch 014:     34 / 1122 loss=6.727, nll_loss=2.492, mask_ins=0.916, word_ins_ml=4.138, word_reposition=0.781, kpe=0.892, ppl=105.93, wps=5244.2, ups=0.26, wpb=20397.3, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=1.51, clip=0, loss_scale=1024, train_wall=259, wall=0
2022-07-08 18:05:42 | INFO | train_inner | epoch 014:    134 / 1122 loss=6.704, nll_loss=2.482, mask_ins=0.923, word_ins_ml=4.128, word_reposition=0.773, kpe=0.879, ppl=104.25, wps=6736.4, ups=0.33, wpb=20495.2, bsz=256, num_updates=14700, lr=0.000291606, gnorm=1.434, clip=0, loss_scale=1024, train_wall=259, wall=0
2022-07-08 18:10:46 | INFO | train_inner | epoch 014:    234 / 1122 loss=6.698, nll_loss=2.475, mask_ins=0.92, word_ins_ml=4.122, word_reposition=0.781, kpe=0.876, ppl=103.82, wps=6780.8, ups=0.33, wpb=20552.3, bsz=256, num_updates=14800, lr=0.000290619, gnorm=1.431, clip=0, loss_scale=1024, train_wall=259, wall=0
2022-07-08 18:15:51 | INFO | train_inner | epoch 014:    334 / 1122 loss=nan, nll_loss=2.416, mask_ins=0.903, word_ins_ml=4.07, word_reposition=0.774, kpe=nan, ppl=nan, wps=6734.6, ups=0.33, wpb=20551.1, bsz=256, num_updates=14900, lr=0.000289642, gnorm=1.411, clip=0, loss_scale=2007, train_wall=260, wall=0
2022-07-08 18:20:55 | INFO | train_inner | epoch 014:    434 / 1122 loss=6.663, nll_loss=2.44, mask_ins=0.919, word_ins_ml=4.091, word_reposition=0.773, kpe=0.881, ppl=101.31, wps=6738.2, ups=0.33, wpb=20530.9, bsz=256, num_updates=15000, lr=0.000288675, gnorm=1.448, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-08 18:25:59 | INFO | train_inner | epoch 014:    534 / 1122 loss=6.637, nll_loss=2.423, mask_ins=0.915, word_ins_ml=4.075, word_reposition=0.771, kpe=0.876, ppl=99.51, wps=6746.4, ups=0.33, wpb=20482.1, bsz=256, num_updates=15100, lr=0.000287718, gnorm=1.452, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-08 18:31:02 | INFO | train_inner | epoch 014:    634 / 1122 loss=6.643, nll_loss=2.424, mask_ins=0.914, word_ins_ml=4.076, word_reposition=0.774, kpe=0.879, ppl=99.97, wps=6765.3, ups=0.33, wpb=20515.6, bsz=256, num_updates=15200, lr=0.00028677, gnorm=1.385, clip=0, loss_scale=2048, train_wall=261, wall=0
2022-07-08 18:36:08 | INFO | train_inner | epoch 014:    734 / 1122 loss=6.636, nll_loss=2.437, mask_ins=0.898, word_ins_ml=4.087, word_reposition=0.771, kpe=0.88, ppl=99.47, wps=6760.1, ups=0.33, wpb=20636.2, bsz=256, num_updates=15300, lr=0.000285831, gnorm=1.417, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-08 18:38:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-08 18:41:15 | INFO | train_inner | epoch 014:    835 / 1122 loss=6.71, nll_loss=2.486, mask_ins=0.918, word_ins_ml=4.131, word_reposition=0.777, kpe=0.885, ppl=104.7, wps=6643.3, ups=0.33, wpb=20425, bsz=256, num_updates=15400, lr=0.000284901, gnorm=1.428, clip=0, loss_scale=2494, train_wall=263, wall=0
2022-07-08 18:47:00 | INFO | train_inner | epoch 014:    935 / 1122 loss=6.681, nll_loss=2.467, mask_ins=0.907, word_ins_ml=4.114, word_reposition=0.775, kpe=0.886, ppl=102.64, wps=5938.5, ups=0.29, wpb=20504.7, bsz=256, num_updates=15500, lr=0.000283981, gnorm=1.402, clip=0, loss_scale=2048, train_wall=300, wall=0
2022-07-08 18:52:05 | INFO | train_inner | epoch 014:   1035 / 1122 loss=6.636, nll_loss=2.433, mask_ins=0.908, word_ins_ml=4.084, word_reposition=0.763, kpe=0.881, ppl=99.45, wps=6741.3, ups=0.33, wpb=20540.5, bsz=256, num_updates=15600, lr=0.000283069, gnorm=1.42, clip=0, loss_scale=2048, train_wall=261, wall=0
2022-07-08 18:56:30 | INFO | train | epoch 014 | loss nan | nll_loss 2.448 | mask_ins 0.911 | word_ins_ml 4.097 | word_reposition 0.773 | kpe nan | ppl nan | wps 6492.7 | ups 0.32 | wpb 20520.9 | bsz 255.8 | num_updates 15687 | lr 0.000282283 | gnorm 1.432 | clip 0 | loss_scale 1871 | train_wall 2954 | wall 0
2022-07-08 18:57:54 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.172 | nll_loss 5.626 | mask_ins 1.465 | word_ins_ml 7.064 | word_reposition 1.258 | kpe 1.384 | ppl 2307.6 | wps 11814.5 | wpb 2367.6 | bsz 32 | num_updates 15687 | best_loss 11.172
2022-07-08 18:58:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 14 @ 15687 updates, score 11.172) (writing took 8.308712932281196 seconds)
2022-07-08 18:58:41 | INFO | train_inner | epoch 015:     13 / 1122 loss=nan, nll_loss=2.431, mask_ins=0.9, word_ins_ml=4.082, word_reposition=0.775, kpe=nan, ppl=nan, wps=5168.4, ups=0.25, wpb=20470.5, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=1.516, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-08 19:03:48 | INFO | train_inner | epoch 015:    113 / 1122 loss=6.63, nll_loss=2.439, mask_ins=0.896, word_ins_ml=4.089, word_reposition=0.776, kpe=0.869, ppl=99.07, wps=6738.1, ups=0.33, wpb=20648.4, bsz=256, num_updates=15800, lr=0.000281272, gnorm=1.423, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-08 19:07:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-08 19:08:57 | INFO | train_inner | epoch 015:    214 / 1122 loss=6.594, nll_loss=2.411, mask_ins=0.894, word_ins_ml=4.064, word_reposition=0.772, kpe=0.865, ppl=96.63, wps=6647.3, ups=0.32, wpb=20590.3, bsz=256, num_updates=15900, lr=0.000280386, gnorm=1.404, clip=0, loss_scale=2372, train_wall=259, wall=0
2022-07-08 19:14:03 | INFO | train_inner | epoch 015:    314 / 1122 loss=nan, nll_loss=2.392, mask_ins=0.889, word_ins_ml=4.047, word_reposition=0.764, kpe=nan, ppl=nan, wps=6720, ups=0.33, wpb=20536.6, bsz=256, num_updates=16000, lr=0.000279508, gnorm=1.423, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-08 19:19:07 | INFO | train_inner | epoch 015:    414 / 1122 loss=6.605, nll_loss=2.417, mask_ins=0.896, word_ins_ml=4.069, word_reposition=0.772, kpe=0.869, ppl=97.37, wps=6747.4, ups=0.33, wpb=20532.1, bsz=256, num_updates=16100, lr=0.000278639, gnorm=1.439, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-08 19:24:11 | INFO | train_inner | epoch 015:    514 / 1122 loss=6.603, nll_loss=2.425, mask_ins=0.887, word_ins_ml=4.076, word_reposition=0.769, kpe=0.872, ppl=97.21, wps=6738.7, ups=0.33, wpb=20493.1, bsz=256, num_updates=16200, lr=0.000277778, gnorm=1.402, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-08 19:29:17 | INFO | train_inner | epoch 015:    614 / 1122 loss=nan, nll_loss=2.451, mask_ins=0.893, word_ins_ml=4.099, word_reposition=0.77, kpe=nan, ppl=nan, wps=6705, ups=0.33, wpb=20509.3, bsz=256, num_updates=16300, lr=0.000276924, gnorm=1.423, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-08 19:34:24 | INFO | train_inner | epoch 015:    714 / 1122 loss=6.636, nll_loss=2.448, mask_ins=0.899, word_ins_ml=4.097, word_reposition=0.77, kpe=0.871, ppl=99.49, wps=6686.5, ups=0.33, wpb=20500.2, bsz=256, num_updates=16400, lr=0.000276079, gnorm=1.481, clip=0, loss_scale=2540, train_wall=259, wall=0
2022-07-08 19:37:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-08 19:39:31 | INFO | train_inner | epoch 015:    815 / 1122 loss=6.592, nll_loss=2.435, mask_ins=0.872, word_ins_ml=4.084, word_reposition=0.759, kpe=0.876, ppl=96.48, wps=6681.3, ups=0.33, wpb=20529.5, bsz=256, num_updates=16500, lr=0.000275241, gnorm=1.413, clip=0, loss_scale=3204, train_wall=263, wall=0
2022-07-08 19:44:38 | INFO | train_inner | epoch 015:    915 / 1122 loss=6.594, nll_loss=2.416, mask_ins=0.895, word_ins_ml=4.068, word_reposition=0.763, kpe=0.868, ppl=96.63, wps=6659, ups=0.33, wpb=20450.5, bsz=256, num_updates=16600, lr=0.000274411, gnorm=1.416, clip=0, loss_scale=2048, train_wall=262, wall=0
2022-07-08 19:49:43 | INFO | train_inner | epoch 015:   1015 / 1122 loss=6.618, nll_loss=2.433, mask_ins=0.893, word_ins_ml=4.082, word_reposition=0.769, kpe=0.873, ppl=98.21, wps=6734.4, ups=0.33, wpb=20537.3, bsz=256, num_updates=16700, lr=0.000273588, gnorm=1.419, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-08 19:55:27 | INFO | train_inner | epoch 015:   1115 / 1122 loss=6.579, nll_loss=2.395, mask_ins=0.889, word_ins_ml=4.048, word_reposition=0.77, kpe=0.872, ppl=95.6, wps=5988.7, ups=0.29, wpb=20598.7, bsz=256, num_updates=16800, lr=0.000272772, gnorm=1.403, clip=0, loss_scale=2048, train_wall=297, wall=0
2022-07-08 19:55:48 | INFO | train | epoch 015 | loss nan | nll_loss 2.425 | mask_ins 0.892 | word_ins_ml 4.076 | word_reposition 0.769 | kpe nan | ppl nan | wps 6459.3 | ups 0.31 | wpb 20520.9 | bsz 255.8 | num_updates 16807 | lr 0.000272716 | gnorm 1.43 | clip 0 | loss_scale 2225 | train_wall 2947 | wall 0
2022-07-08 19:57:12 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 11.163 | nll_loss 5.567 | mask_ins 1.48 | word_ins_ml 7.006 | word_reposition 1.253 | kpe 1.424 | ppl 2292.63 | wps 11773.9 | wpb 2367.6 | bsz 32 | num_updates 16807 | best_loss 11.163
2022-07-08 19:57:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 15 @ 16807 updates, score 11.163) (writing took 8.641062100417912 seconds)
2022-07-08 20:02:04 | INFO | train_inner | epoch 016:     93 / 1122 loss=nan, nll_loss=2.396, mask_ins=0.876, word_ins_ml=4.05, word_reposition=0.766, kpe=nan, ppl=nan, wps=5113.9, ups=0.25, wpb=20279.3, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=1.498, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-08 20:07:08 | INFO | train_inner | epoch 016:    193 / 1122 loss=6.582, nll_loss=2.425, mask_ins=0.883, word_ins_ml=4.075, word_reposition=0.765, kpe=0.858, ppl=95.77, wps=6742.5, ups=0.33, wpb=20489.7, bsz=256, num_updates=17000, lr=0.000271163, gnorm=1.447, clip=0, loss_scale=2703, train_wall=260, wall=0
2022-07-08 20:09:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-08 20:12:16 | INFO | train_inner | epoch 016:    294 / 1122 loss=6.52, nll_loss=2.354, mask_ins=0.881, word_ins_ml=4.012, word_reposition=0.767, kpe=0.859, ppl=91.75, wps=6702.5, ups=0.32, wpb=20656.8, bsz=256, num_updates=17100, lr=0.000270369, gnorm=1.438, clip=0, loss_scale=2879, train_wall=261, wall=0
2022-07-08 20:17:21 | INFO | train_inner | epoch 016:    394 / 1122 loss=6.55, nll_loss=2.39, mask_ins=0.881, word_ins_ml=4.044, word_reposition=0.766, kpe=0.859, ppl=93.72, wps=6687.3, ups=0.33, wpb=20438.9, bsz=256, num_updates=17200, lr=0.000269582, gnorm=1.448, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-08 20:22:26 | INFO | train_inner | epoch 016:    494 / 1122 loss=6.547, nll_loss=2.396, mask_ins=0.872, word_ins_ml=4.05, word_reposition=0.764, kpe=0.862, ppl=93.52, wps=6776.3, ups=0.33, wpb=20624.1, bsz=256, num_updates=17300, lr=0.000268802, gnorm=1.435, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-08 20:27:30 | INFO | train_inner | epoch 016:    594 / 1122 loss=6.555, nll_loss=2.395, mask_ins=0.88, word_ins_ml=4.048, word_reposition=0.764, kpe=0.862, ppl=94.03, wps=6762.4, ups=0.33, wpb=20554.8, bsz=256, num_updates=17400, lr=0.000268028, gnorm=1.441, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-08 20:32:33 | INFO | train_inner | epoch 016:    694 / 1122 loss=6.526, nll_loss=2.385, mask_ins=0.862, word_ins_ml=4.039, word_reposition=0.762, kpe=0.863, ppl=92.14, wps=6785.7, ups=0.33, wpb=20579.4, bsz=256, num_updates=17500, lr=0.000267261, gnorm=1.412, clip=0, loss_scale=2048, train_wall=261, wall=0
2022-07-08 20:37:39 | INFO | train_inner | epoch 016:    794 / 1122 loss=6.537, nll_loss=2.381, mask_ins=0.872, word_ins_ml=4.036, word_reposition=0.766, kpe=0.864, ppl=92.86, wps=6705.2, ups=0.33, wpb=20507.6, bsz=256, num_updates=17600, lr=0.000266501, gnorm=1.39, clip=0, loss_scale=3031, train_wall=261, wall=0
2022-07-08 20:42:45 | INFO | train_inner | epoch 016:    894 / 1122 loss=6.547, nll_loss=2.399, mask_ins=0.873, word_ins_ml=4.051, word_reposition=0.759, kpe=0.863, ppl=93.5, wps=6728.2, ups=0.33, wpb=20575.1, bsz=256, num_updates=17700, lr=0.000265747, gnorm=1.402, clip=0, loss_scale=4096, train_wall=263, wall=0
2022-07-08 20:47:51 | INFO | train_inner | epoch 016:    994 / 1122 loss=6.546, nll_loss=2.392, mask_ins=0.873, word_ins_ml=4.045, word_reposition=0.762, kpe=0.866, ppl=93.47, wps=6697, ups=0.33, wpb=20502, bsz=256, num_updates=17800, lr=0.000264999, gnorm=1.426, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-08 20:52:56 | INFO | train_inner | epoch 016:   1094 / 1122 loss=nan, nll_loss=2.41, mask_ins=0.878, word_ins_ml=4.062, word_reposition=0.762, kpe=nan, ppl=nan, wps=6722.5, ups=0.33, wpb=20513.8, bsz=256, num_updates=17900, lr=0.000264258, gnorm=1.456, clip=0, loss_scale=4096, train_wall=261, wall=0
2022-07-08 20:54:20 | INFO | train | epoch 016 | loss nan | nll_loss 2.393 | mask_ins 0.875 | word_ins_ml 4.047 | word_reposition 0.764 | kpe nan | ppl nan | wps 6549.6 | ups 0.32 | wpb 20519.8 | bsz 255.8 | num_updates 17928 | lr 0.000264052 | gnorm 1.437 | clip 0 | loss_scale 2868 | train_wall 2914 | wall 0
2022-07-08 20:55:44 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 11.049 | nll_loss 5.529 | mask_ins 1.444 | word_ins_ml 6.966 | word_reposition 1.219 | kpe 1.42 | ppl 2118.74 | wps 11746 | wpb 2367.6 | bsz 32 | num_updates 17928 | best_loss 11.049
2022-07-08 20:55:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 16 @ 17928 updates, score 11.049) (writing took 8.666670783422887 seconds)
2022-07-08 20:59:33 | INFO | train_inner | epoch 017:     72 / 1122 loss=6.548, nll_loss=2.397, mask_ins=0.876, word_ins_ml=4.05, word_reposition=0.768, kpe=0.854, ppl=93.56, wps=5145, ups=0.25, wpb=20446.3, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=1.5, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-08 21:04:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-08 21:05:20 | INFO | train_inner | epoch 017:    173 / 1122 loss=6.534, nll_loss=2.394, mask_ins=0.877, word_ins_ml=4.047, word_reposition=0.761, kpe=0.849, ppl=92.67, wps=5915.3, ups=0.29, wpb=20508.6, bsz=256, num_updates=18100, lr=0.000262794, gnorm=1.433, clip=0, loss_scale=4623, train_wall=300, wall=0
2022-07-08 21:07:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-08 21:10:28 | INFO | train_inner | epoch 017:    274 / 1122 loss=6.482, nll_loss=2.345, mask_ins=0.874, word_ins_ml=4.003, word_reposition=0.755, kpe=0.85, ppl=89.41, wps=6661.5, ups=0.32, wpb=20503.8, bsz=256, num_updates=18200, lr=0.000262071, gnorm=1.424, clip=0, loss_scale=2778, train_wall=261, wall=0
2022-07-08 21:15:33 | INFO | train_inner | epoch 017:    374 / 1122 loss=6.54, nll_loss=2.402, mask_ins=0.874, word_ins_ml=4.054, word_reposition=0.757, kpe=0.855, ppl=93.04, wps=6687.6, ups=0.33, wpb=20385.9, bsz=256, num_updates=18300, lr=0.000261354, gnorm=1.442, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-08 21:20:38 | INFO | train_inner | epoch 017:    474 / 1122 loss=6.487, nll_loss=2.352, mask_ins=0.865, word_ins_ml=4.01, word_reposition=0.761, kpe=0.852, ppl=89.71, wps=6716.3, ups=0.33, wpb=20497.5, bsz=256, num_updates=18400, lr=0.000260643, gnorm=1.412, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-08 21:25:44 | INFO | train_inner | epoch 017:    574 / 1122 loss=6.445, nll_loss=2.325, mask_ins=0.855, word_ins_ml=3.985, word_reposition=0.757, kpe=0.849, ppl=87.14, wps=6711.8, ups=0.33, wpb=20511.5, bsz=256, num_updates=18500, lr=0.000259938, gnorm=1.439, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-08 21:30:38 | INFO | train_inner | epoch 017:    674 / 1122 loss=6.493, nll_loss=2.361, mask_ins=0.863, word_ins_ml=4.018, word_reposition=0.756, kpe=0.856, ppl=90.05, wps=6998.3, ups=0.34, wpb=20623.2, bsz=256, num_updates=18600, lr=0.000259238, gnorm=1.442, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-08 21:35:31 | INFO | train_inner | epoch 017:    774 / 1122 loss=6.493, nll_loss=2.353, mask_ins=0.871, word_ins_ml=4.01, word_reposition=0.759, kpe=0.853, ppl=90.05, wps=7080.7, ups=0.34, wpb=20750.8, bsz=256, num_updates=18700, lr=0.000258544, gnorm=1.433, clip=0, loss_scale=3133, train_wall=256, wall=0
2022-07-08 21:40:29 | INFO | train_inner | epoch 017:    874 / 1122 loss=6.49, nll_loss=2.348, mask_ins=0.872, word_ins_ml=4.006, word_reposition=0.758, kpe=0.854, ppl=89.88, wps=6882.1, ups=0.34, wpb=20449.2, bsz=256, num_updates=18800, lr=0.000257855, gnorm=1.433, clip=0, loss_scale=4096, train_wall=260, wall=0
2022-07-08 21:45:23 | INFO | train_inner | epoch 017:    974 / 1122 loss=6.494, nll_loss=2.365, mask_ins=0.86, word_ins_ml=4.021, word_reposition=0.759, kpe=0.854, ppl=90.12, wps=6950.5, ups=0.34, wpb=20495.8, bsz=256, num_updates=18900, lr=0.000257172, gnorm=1.409, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-08 21:50:19 | INFO | train_inner | epoch 017:   1074 / 1122 loss=nan, nll_loss=2.331, mask_ins=0.857, word_ins_ml=3.991, word_reposition=0.76, kpe=nan, ppl=nan, wps=6940.1, ups=0.34, wpb=20549.3, bsz=256, num_updates=19000, lr=0.000256495, gnorm=1.425, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-08 21:52:41 | INFO | train | epoch 017 | loss nan | nll_loss 2.359 | mask_ins 0.867 | word_ins_ml 4.016 | word_reposition 0.759 | kpe nan | ppl nan | wps 6564.1 | ups 0.32 | wpb 20520.7 | bsz 255.8 | num_updates 19048 | lr 0.000256171 | gnorm 1.432 | clip 0 | loss_scale 3209 | train_wall 2941 | wall 0
2022-07-08 21:54:01 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 11.166 | nll_loss 5.498 | mask_ins 1.476 | word_ins_ml 6.939 | word_reposition 1.257 | kpe 1.494 | ppl 2298.44 | wps 12476.5 | wpb 2367.6 | bsz 32 | num_updates 19048 | best_loss 11.049
2022-07-08 21:54:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 17 @ 19048 updates, score 11.166) (writing took 4.6692049568519 seconds)
2022-07-08 21:56:38 | INFO | train_inner | epoch 018:     52 / 1122 loss=nan, nll_loss=2.351, mask_ins=0.865, word_ins_ml=4.008, word_reposition=0.757, kpe=nan, ppl=nan, wps=5363.2, ups=0.26, wpb=20306.8, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=1.476, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-08 21:59:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-08 22:01:36 | INFO | train_inner | epoch 018:    153 / 1122 loss=6.421, nll_loss=2.303, mask_ins=0.861, word_ins_ml=3.965, word_reposition=0.756, kpe=0.839, ppl=85.67, wps=6900.1, ups=0.34, wpb=20568.2, bsz=256, num_updates=19200, lr=0.000255155, gnorm=1.47, clip=0, loss_scale=4380, train_wall=261, wall=0
2022-07-08 22:06:32 | INFO | train_inner | epoch 018:    253 / 1122 loss=6.439, nll_loss=2.324, mask_ins=0.854, word_ins_ml=3.984, word_reposition=0.758, kpe=0.842, ppl=86.76, wps=6958.1, ups=0.34, wpb=20600.5, bsz=256, num_updates=19300, lr=0.000254493, gnorm=1.438, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-08 22:07:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-08 22:12:08 | INFO | train_inner | epoch 018:    354 / 1122 loss=6.474, nll_loss=2.365, mask_ins=0.855, word_ins_ml=4.02, word_reposition=0.752, kpe=0.846, ppl=88.87, wps=6100.4, ups=0.3, wpb=20470.8, bsz=256, num_updates=19400, lr=0.000253837, gnorm=1.707, clip=0, loss_scale=2514, train_wall=298, wall=0
2022-07-08 22:17:02 | INFO | train_inner | epoch 018:    454 / 1122 loss=6.447, nll_loss=2.336, mask_ins=0.856, word_ins_ml=3.994, word_reposition=0.752, kpe=0.845, ppl=87.26, wps=6980.1, ups=0.34, wpb=20554.4, bsz=256, num_updates=19500, lr=0.000253185, gnorm=1.559, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-08 22:21:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-08 22:21:59 | INFO | train_inner | epoch 018:    555 / 1122 loss=6.481, nll_loss=2.381, mask_ins=0.848, word_ins_ml=4.034, word_reposition=0.752, kpe=0.847, ppl=89.32, wps=6895.8, ups=0.34, wpb=20428.7, bsz=256, num_updates=19600, lr=0.000252538, gnorm=1.466, clip=0, loss_scale=1855, train_wall=259, wall=0
2022-07-08 22:26:52 | INFO | train_inner | epoch 018:    655 / 1122 loss=6.486, nll_loss=2.364, mask_ins=0.863, word_ins_ml=4.019, word_reposition=0.755, kpe=0.849, ppl=89.66, wps=6961.5, ups=0.34, wpb=20449.6, bsz=256, num_updates=19700, lr=0.000251896, gnorm=1.643, clip=0, loss_scale=1024, train_wall=257, wall=0
2022-07-08 22:31:47 | INFO | train_inner | epoch 018:    755 / 1122 loss=nan, nll_loss=2.346, mask_ins=0.859, word_ins_ml=4.003, word_reposition=0.76, kpe=nan, ppl=nan, wps=7008.7, ups=0.34, wpb=20659.9, bsz=256, num_updates=19800, lr=0.000251259, gnorm=1.554, clip=0, loss_scale=1024, train_wall=258, wall=0
2022-07-08 22:33:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-08 22:36:44 | INFO | train_inner | epoch 018:    856 / 1122 loss=6.44, nll_loss=2.334, mask_ins=0.844, word_ins_ml=3.993, word_reposition=0.758, kpe=0.846, ppl=86.85, wps=6956.6, ups=0.34, wpb=20657.8, bsz=256, num_updates=19900, lr=0.000250627, gnorm=1.459, clip=0, loss_scale=705, train_wall=260, wall=0
2022-07-08 22:41:39 | INFO | train_inner | epoch 018:    956 / 1122 loss=6.472, nll_loss=2.354, mask_ins=0.859, word_ins_ml=4.011, word_reposition=0.755, kpe=0.847, ppl=88.78, wps=6935.6, ups=0.34, wpb=20467.2, bsz=256, num_updates=20000, lr=0.00025, gnorm=1.434, clip=0, loss_scale=512, train_wall=258, wall=0
2022-07-08 22:46:33 | INFO | train_inner | epoch 018:   1056 / 1122 loss=6.437, nll_loss=2.324, mask_ins=0.853, word_ins_ml=3.983, word_reposition=0.753, kpe=0.848, ppl=86.67, wps=6976.2, ups=0.34, wpb=20511.2, bsz=256, num_updates=20100, lr=0.000249377, gnorm=1.438, clip=0, loss_scale=512, train_wall=257, wall=0
2022-07-08 22:49:47 | INFO | train | epoch 018 | loss nan | nll_loss 2.346 | mask_ins 0.857 | word_ins_ml 4.003 | word_reposition 0.755 | kpe nan | ppl nan | wps 6698 | ups 0.33 | wpb 20520.8 | bsz 255.8 | num_updates 20166 | lr 0.000248969 | gnorm 1.517 | clip 0 | loss_scale 1892 | train_wall 2928 | wall 0
2022-07-08 22:51:06 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 11.126 | nll_loss 5.527 | mask_ins 1.478 | word_ins_ml 6.966 | word_reposition 1.259 | kpe 1.422 | ppl 2234.27 | wps 12442.1 | wpb 2367.6 | bsz 32 | num_updates 20166 | best_loss 11.049
2022-07-08 22:51:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 18 @ 20166 updates, score 11.126) (writing took 4.387347184121609 seconds)
2022-07-08 22:52:51 | INFO | train_inner | epoch 019:     34 / 1122 loss=6.45, nll_loss=2.339, mask_ins=0.86, word_ins_ml=3.997, word_reposition=0.751, kpe=0.842, ppl=87.42, wps=5379.7, ups=0.26, wpb=20320.3, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=1.522, clip=0, loss_scale=512, train_wall=257, wall=0
2022-07-08 22:57:47 | INFO | train_inner | epoch 019:    134 / 1122 loss=6.393, nll_loss=2.303, mask_ins=0.852, word_ins_ml=3.965, word_reposition=0.749, kpe=0.828, ppl=84.04, wps=6869.1, ups=0.34, wpb=20340, bsz=256, num_updates=20300, lr=0.000248146, gnorm=1.432, clip=0, loss_scale=512, train_wall=259, wall=0
2022-07-08 23:02:45 | INFO | train_inner | epoch 019:    234 / 1122 loss=nan, nll_loss=2.313, mask_ins=0.843, word_ins_ml=3.973, word_reposition=0.755, kpe=nan, ppl=nan, wps=6890.5, ups=0.34, wpb=20538.6, bsz=256, num_updates=20400, lr=0.000247537, gnorm=1.432, clip=0, loss_scale=773, train_wall=261, wall=0
2022-07-08 23:07:42 | INFO | train_inner | epoch 019:    334 / 1122 loss=6.424, nll_loss=2.316, mask_ins=0.857, word_ins_ml=3.977, word_reposition=0.76, kpe=0.831, ppl=85.85, wps=6920.9, ups=0.34, wpb=20526.8, bsz=256, num_updates=20500, lr=0.000246932, gnorm=1.45, clip=0, loss_scale=1024, train_wall=260, wall=0
2022-07-08 23:12:49 | INFO | train_inner | epoch 019:    434 / 1122 loss=6.453, nll_loss=2.356, mask_ins=0.852, word_ins_ml=4.012, word_reposition=0.754, kpe=0.835, ppl=87.59, wps=6649.7, ups=0.33, wpb=20402.4, bsz=256, num_updates=20600, lr=0.000246332, gnorm=1.434, clip=0, loss_scale=1024, train_wall=270, wall=0
2022-07-08 23:18:18 | INFO | train_inner | epoch 019:    534 / 1122 loss=6.453, nll_loss=2.349, mask_ins=0.853, word_ins_ml=4.006, word_reposition=0.756, kpe=0.838, ppl=87.6, wps=6266.5, ups=0.3, wpb=20659.2, bsz=256, num_updates=20700, lr=0.000245737, gnorm=1.431, clip=0, loss_scale=1024, train_wall=292, wall=0
2022-07-08 23:23:13 | INFO | train_inner | epoch 019:    634 / 1122 loss=6.402, nll_loss=2.324, mask_ins=0.837, word_ins_ml=3.983, word_reposition=0.747, kpe=0.835, ppl=84.59, wps=6992.3, ups=0.34, wpb=20619.6, bsz=256, num_updates=20800, lr=0.000245145, gnorm=1.439, clip=0, loss_scale=1024, train_wall=258, wall=0
2022-07-08 23:28:07 | INFO | train_inner | epoch 019:    734 / 1122 loss=6.412, nll_loss=2.324, mask_ins=0.846, word_ins_ml=3.983, word_reposition=0.747, kpe=0.836, ppl=85.14, wps=7042.7, ups=0.34, wpb=20709.2, bsz=256, num_updates=20900, lr=0.000244558, gnorm=1.424, clip=0, loss_scale=1423, train_wall=257, wall=0
2022-07-08 23:33:04 | INFO | train_inner | epoch 019:    834 / 1122 loss=6.404, nll_loss=2.311, mask_ins=0.835, word_ins_ml=3.971, word_reposition=0.753, kpe=0.845, ppl=84.69, wps=6923.9, ups=0.34, wpb=20558.9, bsz=256, num_updates=21000, lr=0.000243975, gnorm=1.44, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-08 23:37:57 | INFO | train_inner | epoch 019:    934 / 1122 loss=nan, nll_loss=2.278, mask_ins=0.839, word_ins_ml=3.942, word_reposition=0.745, kpe=nan, ppl=nan, wps=6966.9, ups=0.34, wpb=20432.8, bsz=256, num_updates=21100, lr=0.000243396, gnorm=1.432, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-08 23:42:54 | INFO | train_inner | epoch 019:   1034 / 1122 loss=6.401, nll_loss=2.322, mask_ins=0.841, word_ins_ml=3.981, word_reposition=0.74, kpe=0.84, ppl=84.54, wps=6963.7, ups=0.34, wpb=20620.4, bsz=256, num_updates=21200, lr=0.000242821, gnorm=1.468, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-08 23:47:13 | INFO | train | epoch 019 | loss nan | nll_loss 2.318 | mask_ins 0.845 | word_ins_ml 3.978 | word_reposition 0.75 | kpe nan | ppl nan | wps 6680.6 | ups 0.33 | wpb 20519.9 | bsz 255.8 | num_updates 21288 | lr 0.000242319 | gnorm 1.444 | clip 0 | loss_scale 1330 | train_wall 2950 | wall 0
2022-07-08 23:48:32 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.215 | nll_loss 5.563 | mask_ins 1.457 | word_ins_ml 7.007 | word_reposition 1.274 | kpe 1.477 | ppl 2377.51 | wps 12462.4 | wpb 2367.6 | bsz 32 | num_updates 21288 | best_loss 11.049
2022-07-08 23:48:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 19 @ 21288 updates, score 11.215) (writing took 4.492411396466196 seconds)
2022-07-08 23:49:12 | INFO | train_inner | epoch 020:     12 / 1122 loss=6.413, nll_loss=2.32, mask_ins=0.842, word_ins_ml=3.98, word_reposition=0.752, kpe=0.839, ppl=85.23, wps=5395.9, ups=0.26, wpb=20413.8, bsz=253.8, num_updates=21300, lr=0.000242251, gnorm=1.516, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-08 23:54:08 | INFO | train_inner | epoch 020:    112 / 1122 loss=nan, nll_loss=2.293, mask_ins=0.835, word_ins_ml=3.956, word_reposition=0.745, kpe=nan, ppl=nan, wps=6939.6, ups=0.34, wpb=20531.2, bsz=256, num_updates=21400, lr=0.000241684, gnorm=1.448, clip=0, loss_scale=2601, train_wall=259, wall=0
2022-07-08 23:59:00 | INFO | train_inner | epoch 020:    212 / 1122 loss=6.357, nll_loss=2.277, mask_ins=0.843, word_ins_ml=3.941, word_reposition=0.748, kpe=0.825, ppl=81.97, wps=7021.3, ups=0.34, wpb=20515.2, bsz=256, num_updates=21500, lr=0.000241121, gnorm=1.451, clip=0, loss_scale=4096, train_wall=255, wall=0
2022-07-09 00:03:53 | INFO | train_inner | epoch 020:    312 / 1122 loss=6.372, nll_loss=2.295, mask_ins=0.839, word_ins_ml=3.957, word_reposition=0.752, kpe=0.824, ppl=82.84, wps=7025.6, ups=0.34, wpb=20617.9, bsz=256, num_updates=21600, lr=0.000240563, gnorm=1.444, clip=0, loss_scale=4096, train_wall=257, wall=0
2022-07-09 00:04:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 00:08:50 | INFO | train_inner | epoch 020:    413 / 1122 loss=nan, nll_loss=2.305, mask_ins=0.842, word_ins_ml=3.965, word_reposition=0.751, kpe=nan, ppl=nan, wps=6930.7, ups=0.34, wpb=20571.5, bsz=256, num_updates=21700, lr=0.000240008, gnorm=1.473, clip=0, loss_scale=2190, train_wall=259, wall=0
2022-07-09 00:13:47 | INFO | train_inner | epoch 020:    513 / 1122 loss=6.311, nll_loss=2.248, mask_ins=0.827, word_ins_ml=3.915, word_reposition=0.743, kpe=0.826, ppl=79.42, wps=6922.6, ups=0.34, wpb=20513.4, bsz=256, num_updates=21800, lr=0.000239457, gnorm=1.474, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-09 00:18:54 | INFO | train_inner | epoch 020:    613 / 1122 loss=6.36, nll_loss=2.294, mask_ins=0.841, word_ins_ml=3.955, word_reposition=0.735, kpe=0.829, ppl=82.12, wps=6699.9, ups=0.32, wpb=20615.1, bsz=256, num_updates=21900, lr=0.000238909, gnorm=1.439, clip=0, loss_scale=2048, train_wall=271, wall=0
2022-07-09 00:24:26 | INFO | train_inner | epoch 020:    713 / 1122 loss=6.382, nll_loss=2.309, mask_ins=0.836, word_ins_ml=3.969, word_reposition=0.745, kpe=0.832, ppl=83.43, wps=6144.5, ups=0.3, wpb=20377.8, bsz=256, num_updates=22000, lr=0.000238366, gnorm=1.474, clip=0, loss_scale=2048, train_wall=294, wall=0
2022-07-09 00:29:19 | INFO | train_inner | epoch 020:    813 / 1122 loss=6.362, nll_loss=2.289, mask_ins=0.838, word_ins_ml=3.951, word_reposition=0.747, kpe=0.826, ppl=82.26, wps=6982.3, ups=0.34, wpb=20490.5, bsz=256, num_updates=22100, lr=0.000237826, gnorm=1.45, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-09 00:34:14 | INFO | train_inner | epoch 020:    913 / 1122 loss=6.381, nll_loss=2.293, mask_ins=0.85, word_ins_ml=3.955, word_reposition=0.744, kpe=0.832, ppl=83.36, wps=7005.8, ups=0.34, wpb=20604.9, bsz=256, num_updates=22200, lr=0.000237289, gnorm=1.439, clip=0, loss_scale=3727, train_wall=257, wall=0
2022-07-09 00:39:09 | INFO | train_inner | epoch 020:   1013 / 1122 loss=6.368, nll_loss=2.301, mask_ins=0.831, word_ins_ml=3.962, word_reposition=0.747, kpe=0.828, ppl=82.58, wps=6957.3, ups=0.34, wpb=20556.6, bsz=256, num_updates=22300, lr=0.000236757, gnorm=1.405, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-09 00:39:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 00:44:06 | INFO | train_inner | epoch 020:   1114 / 1122 loss=6.404, nll_loss=2.316, mask_ins=0.84, word_ins_ml=3.976, word_reposition=0.757, kpe=0.832, ppl=84.69, wps=6902.7, ups=0.34, wpb=20479.1, bsz=256, num_updates=22400, lr=0.000236228, gnorm=1.451, clip=0, loss_scale=2251, train_wall=260, wall=0
2022-07-09 00:44:28 | INFO | train | epoch 020 | loss nan | nll_loss 2.293 | mask_ins 0.838 | word_ins_ml 3.955 | word_reposition 0.747 | kpe nan | ppl nan | wps 6690.6 | ups 0.33 | wpb 20520.1 | bsz 255.8 | num_updates 22408 | lr 0.000236186 | gnorm 1.455 | clip 0 | loss_scale 2826 | train_wall 2938 | wall 0
2022-07-09 00:45:48 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.112 | nll_loss 5.521 | mask_ins 1.448 | word_ins_ml 6.972 | word_reposition 1.266 | kpe 1.426 | ppl 2213.49 | wps 12390.6 | wpb 2367.6 | bsz 32 | num_updates 22408 | best_loss 11.049
2022-07-09 00:45:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 20 @ 22408 updates, score 11.112) (writing took 4.693072647787631 seconds)
2022-07-09 00:50:23 | INFO | train_inner | epoch 021:     92 / 1122 loss=6.354, nll_loss=2.293, mask_ins=0.834, word_ins_ml=3.955, word_reposition=0.749, kpe=0.815, ppl=81.8, wps=5420.2, ups=0.26, wpb=20463.7, bsz=253.8, num_updates=22500, lr=0.000235702, gnorm=1.543, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-09 00:55:16 | INFO | train_inner | epoch 021:    192 / 1122 loss=6.367, nll_loss=2.3, mask_ins=0.836, word_ins_ml=3.961, word_reposition=0.753, kpe=0.817, ppl=82.53, wps=6996.6, ups=0.34, wpb=20455.7, bsz=256, num_updates=22600, lr=0.00023518, gnorm=1.481, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-09 01:00:10 | INFO | train_inner | epoch 021:    292 / 1122 loss=6.33, nll_loss=2.277, mask_ins=0.832, word_ins_ml=3.941, word_reposition=0.741, kpe=0.816, ppl=80.43, wps=7020, ups=0.34, wpb=20633.5, bsz=256, num_updates=22700, lr=0.000234662, gnorm=1.491, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 01:05:03 | INFO | train_inner | epoch 021:    392 / 1122 loss=6.316, nll_loss=2.255, mask_ins=0.835, word_ins_ml=3.922, word_reposition=0.744, kpe=0.816, ppl=79.66, wps=7003, ups=0.34, wpb=20517.7, bsz=256, num_updates=22800, lr=0.000234146, gnorm=1.486, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-09 01:09:58 | INFO | train_inner | epoch 021:    492 / 1122 loss=nan, nll_loss=2.275, mask_ins=0.835, word_ins_ml=3.939, word_reposition=0.752, kpe=nan, ppl=nan, wps=6983.1, ups=0.34, wpb=20638.4, bsz=256, num_updates=22900, lr=0.000233635, gnorm=1.474, clip=0, loss_scale=3666, train_wall=259, wall=0
2022-07-09 01:14:53 | INFO | train_inner | epoch 021:    592 / 1122 loss=6.309, nll_loss=2.254, mask_ins=0.832, word_ins_ml=3.92, word_reposition=0.741, kpe=0.816, ppl=79.29, wps=6961.1, ups=0.34, wpb=20503.2, bsz=256, num_updates=23000, lr=0.000233126, gnorm=1.446, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 01:19:47 | INFO | train_inner | epoch 021:    692 / 1122 loss=6.349, nll_loss=2.299, mask_ins=0.828, word_ins_ml=3.959, word_reposition=0.74, kpe=0.821, ppl=81.5, wps=6940.7, ups=0.34, wpb=20432.8, bsz=256, num_updates=23100, lr=0.000232621, gnorm=1.484, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 01:25:02 | INFO | train_inner | epoch 021:    792 / 1122 loss=6.332, nll_loss=2.271, mask_ins=0.832, word_ins_ml=3.935, word_reposition=0.742, kpe=0.823, ppl=80.59, wps=6517.4, ups=0.32, wpb=20552.8, bsz=256, num_updates=23200, lr=0.000232119, gnorm=1.499, clip=0, loss_scale=4096, train_wall=278, wall=0
2022-07-09 01:30:22 | INFO | train_inner | epoch 021:    892 / 1122 loss=6.348, nll_loss=2.287, mask_ins=0.838, word_ins_ml=3.949, word_reposition=0.742, kpe=0.82, ppl=81.46, wps=6380.4, ups=0.31, wpb=20409.5, bsz=256, num_updates=23300, lr=0.000231621, gnorm=1.495, clip=0, loss_scale=4096, train_wall=283, wall=0
2022-07-09 01:35:16 | INFO | train_inner | epoch 021:    992 / 1122 loss=6.331, nll_loss=2.282, mask_ins=0.821, word_ins_ml=3.945, word_reposition=0.74, kpe=0.824, ppl=80.51, wps=7004.6, ups=0.34, wpb=20591, bsz=256, num_updates=23400, lr=0.000231125, gnorm=1.475, clip=0, loss_scale=6840, train_wall=258, wall=0
2022-07-09 01:39:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-09 01:40:13 | INFO | train_inner | epoch 021:   1093 / 1122 loss=6.353, nll_loss=2.302, mask_ins=0.826, word_ins_ml=3.962, word_reposition=0.744, kpe=0.821, ppl=81.74, wps=6905.6, ups=0.34, wpb=20487, bsz=256, num_updates=23500, lr=0.000230633, gnorm=1.441, clip=0, loss_scale=7665, train_wall=260, wall=0
2022-07-09 01:41:39 | INFO | train | epoch 021 | loss nan | nll_loss 2.28 | mask_ins 0.831 | word_ins_ml 3.943 | word_reposition 0.744 | kpe nan | ppl nan | wps 6705.9 | ups 0.33 | wpb 20521 | bsz 255.8 | num_updates 23529 | lr 0.000230491 | gnorm 1.482 | clip 0 | loss_scale 3908 | train_wall 2934 | wall 0
2022-07-09 01:42:58 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.121 | nll_loss 5.513 | mask_ins 1.456 | word_ins_ml 6.958 | word_reposition 1.26 | kpe 1.447 | ppl 2226.91 | wps 12473.6 | wpb 2367.6 | bsz 32 | num_updates 23529 | best_loss 11.049
2022-07-09 01:43:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 21 @ 23529 updates, score 11.121) (writing took 4.430740305222571 seconds)
2022-07-09 01:46:29 | INFO | train_inner | epoch 022:     71 / 1122 loss=6.316, nll_loss=2.265, mask_ins=0.831, word_ins_ml=3.929, word_reposition=0.74, kpe=0.816, ppl=79.65, wps=5401.7, ups=0.27, wpb=20341.5, bsz=253.8, num_updates=23600, lr=0.000230144, gnorm=1.527, clip=0, loss_scale=4096, train_wall=257, wall=0
2022-07-09 01:51:26 | INFO | train_inner | epoch 022:    171 / 1122 loss=6.268, nll_loss=2.227, mask_ins=0.827, word_ins_ml=3.896, word_reposition=0.741, kpe=0.804, ppl=77.06, wps=6931.3, ups=0.34, wpb=20554.7, bsz=256, num_updates=23700, lr=0.000229658, gnorm=1.56, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-09 01:54:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 01:56:24 | INFO | train_inner | epoch 022:    272 / 1122 loss=6.277, nll_loss=2.242, mask_ins=0.824, word_ins_ml=3.909, word_reposition=0.737, kpe=0.807, ppl=77.53, wps=6927.3, ups=0.34, wpb=20626.8, bsz=256, num_updates=23800, lr=0.000229175, gnorm=1.423, clip=0, loss_scale=3204, train_wall=261, wall=0
2022-07-09 02:01:18 | INFO | train_inner | epoch 022:    372 / 1122 loss=6.28, nll_loss=2.243, mask_ins=0.82, word_ins_ml=3.91, word_reposition=0.746, kpe=0.804, ppl=77.7, wps=7008, ups=0.34, wpb=20632.8, bsz=256, num_updates=23900, lr=0.000228695, gnorm=1.477, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 02:06:12 | INFO | train_inner | epoch 022:    472 / 1122 loss=6.3, nll_loss=2.271, mask_ins=0.815, word_ins_ml=3.935, word_reposition=0.739, kpe=0.811, ppl=78.79, wps=7026.8, ups=0.34, wpb=20631.7, bsz=256, num_updates=24000, lr=0.000228218, gnorm=1.452, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-09 02:11:06 | INFO | train_inner | epoch 022:    572 / 1122 loss=6.281, nll_loss=2.235, mask_ins=0.827, word_ins_ml=3.902, word_reposition=0.741, kpe=0.81, ppl=77.74, wps=6965.1, ups=0.34, wpb=20507.6, bsz=256, num_updates=24100, lr=0.000227744, gnorm=1.459, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 02:16:01 | INFO | train_inner | epoch 022:    672 / 1122 loss=6.315, nll_loss=2.274, mask_ins=0.826, word_ins_ml=3.938, word_reposition=0.739, kpe=0.813, ppl=79.64, wps=6982.6, ups=0.34, wpb=20578, bsz=256, num_updates=24200, lr=0.000227273, gnorm=1.459, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 02:20:55 | INFO | train_inner | epoch 022:    772 / 1122 loss=6.303, nll_loss=2.27, mask_ins=0.815, word_ins_ml=3.934, word_reposition=0.741, kpe=0.813, ppl=78.98, wps=6986.6, ups=0.34, wpb=20533.8, bsz=256, num_updates=24300, lr=0.000226805, gnorm=1.502, clip=0, loss_scale=2703, train_wall=257, wall=0
2022-07-09 02:25:51 | INFO | train_inner | epoch 022:    872 / 1122 loss=6.255, nll_loss=2.215, mask_ins=0.819, word_ins_ml=3.884, word_reposition=0.738, kpe=0.814, ppl=76.36, wps=6898.4, ups=0.34, wpb=20399, bsz=256, num_updates=24400, lr=0.000226339, gnorm=1.457, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-09 02:31:26 | INFO | train_inner | epoch 022:    972 / 1122 loss=nan, nll_loss=2.216, mask_ins=0.821, word_ins_ml=3.886, word_reposition=0.743, kpe=nan, ppl=nan, wps=6155.2, ups=0.3, wpb=20630, bsz=256, num_updates=24500, lr=0.000225877, gnorm=1.454, clip=0, loss_scale=4096, train_wall=298, wall=0
2022-07-09 02:36:33 | INFO | train_inner | epoch 022:   1072 / 1122 loss=6.269, nll_loss=2.23, mask_ins=0.819, word_ins_ml=3.897, word_reposition=0.74, kpe=0.813, ppl=77.11, wps=6639.3, ups=0.33, wpb=20420.8, bsz=256, num_updates=24600, lr=0.000225417, gnorm=1.445, clip=0, loss_scale=4096, train_wall=270, wall=0
2022-07-09 02:38:59 | INFO | train | epoch 022 | loss nan | nll_loss 2.245 | mask_ins 0.822 | word_ins_ml 3.911 | word_reposition 0.74 | kpe nan | ppl nan | wps 6686.4 | ups 0.33 | wpb 20521.3 | bsz 255.8 | num_updates 24650 | lr 0.000225189 | gnorm 1.476 | clip 0 | loss_scale 3161 | train_wall 2942 | wall 0
2022-07-09 02:40:19 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 10.938 | nll_loss 5.404 | mask_ins 1.432 | word_ins_ml 6.853 | word_reposition 1.228 | kpe 1.425 | ppl 1962.21 | wps 12452.2 | wpb 2367.6 | bsz 32 | num_updates 24650 | best_loss 10.938
2022-07-09 02:40:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 22 @ 24650 updates, score 10.938) (writing took 7.831139070913196 seconds)
2022-07-09 02:42:52 | INFO | train_inner | epoch 023:     50 / 1122 loss=6.297, nll_loss=2.265, mask_ins=0.823, word_ins_ml=3.929, word_reposition=0.737, kpe=0.808, ppl=78.62, wps=5341.2, ups=0.26, wpb=20241.8, bsz=253.8, num_updates=24700, lr=0.000224961, gnorm=1.578, clip=0, loss_scale=4096, train_wall=254, wall=0
2022-07-09 02:47:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-09 02:47:51 | INFO | train_inner | epoch 023:    151 / 1122 loss=6.247, nll_loss=2.224, mask_ins=0.812, word_ins_ml=3.893, word_reposition=0.744, kpe=0.797, ppl=75.94, wps=6923.1, ups=0.33, wpb=20690.5, bsz=256, num_updates=24800, lr=0.000224507, gnorm=1.507, clip=0, loss_scale=4583, train_wall=261, wall=0
2022-07-09 02:52:47 | INFO | train_inner | epoch 023:    251 / 1122 loss=6.261, nll_loss=2.248, mask_ins=0.816, word_ins_ml=3.914, word_reposition=0.735, kpe=0.796, ppl=76.67, wps=6944.6, ups=0.34, wpb=20505.6, bsz=256, num_updates=24900, lr=0.000224055, gnorm=1.488, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-09 02:57:45 | INFO | train_inner | epoch 023:    351 / 1122 loss=6.275, nll_loss=2.257, mask_ins=0.816, word_ins_ml=3.922, word_reposition=0.735, kpe=0.802, ppl=77.43, wps=6882.7, ups=0.33, wpb=20560, bsz=256, num_updates=25000, lr=0.000223607, gnorm=1.529, clip=0, loss_scale=4096, train_wall=262, wall=0
2022-07-09 03:02:41 | INFO | train_inner | epoch 023:    451 / 1122 loss=6.256, nll_loss=2.231, mask_ins=0.824, word_ins_ml=3.898, word_reposition=0.737, kpe=0.797, ppl=76.43, wps=6939.1, ups=0.34, wpb=20498.9, bsz=256, num_updates=25100, lr=0.000223161, gnorm=1.531, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 03:07:35 | INFO | train_inner | epoch 023:    551 / 1122 loss=nan, nll_loss=2.197, mask_ins=0.818, word_ins_ml=3.868, word_reposition=0.73, kpe=nan, ppl=nan, wps=6987.6, ups=0.34, wpb=20535.5, bsz=256, num_updates=25200, lr=0.000222718, gnorm=1.503, clip=0, loss_scale=4096, train_wall=257, wall=0
2022-07-09 03:12:29 | INFO | train_inner | epoch 023:    651 / 1122 loss=nan, nll_loss=2.231, mask_ins=0.823, word_ins_ml=3.898, word_reposition=0.742, kpe=nan, ppl=nan, wps=6977.2, ups=0.34, wpb=20566.7, bsz=256, num_updates=25300, lr=0.000222277, gnorm=1.544, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 03:16:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-09 03:17:30 | INFO | train_inner | epoch 023:    752 / 1122 loss=6.241, nll_loss=2.22, mask_ins=0.814, word_ins_ml=3.888, word_reposition=0.731, kpe=0.808, ppl=75.65, wps=6839.1, ups=0.33, wpb=20544.7, bsz=256, num_updates=25400, lr=0.000221839, gnorm=1.539, clip=0, loss_scale=7381, train_wall=264, wall=0
2022-07-09 03:18:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 03:22:26 | INFO | train_inner | epoch 023:    853 / 1122 loss=6.271, nll_loss=2.24, mask_ins=0.819, word_ins_ml=3.907, word_reposition=0.74, kpe=0.805, ppl=77.23, wps=6928.7, ups=0.34, wpb=20527.2, bsz=256, num_updates=25500, lr=0.000221404, gnorm=1.609, clip=0, loss_scale=2393, train_wall=259, wall=0
2022-07-09 03:24:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-09 03:25:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-09 03:27:25 | INFO | train_inner | epoch 023:    955 / 1122 loss=6.284, nll_loss=2.253, mask_ins=0.82, word_ins_ml=3.918, word_reposition=0.742, kpe=0.805, ppl=77.91, wps=6890.4, ups=0.33, wpb=20601.5, bsz=256, num_updates=25600, lr=0.000220971, gnorm=1.59, clip=0, loss_scale=1300, train_wall=261, wall=0
2022-07-09 03:32:21 | INFO | train_inner | epoch 023:   1055 / 1122 loss=6.255, nll_loss=2.225, mask_ins=0.818, word_ins_ml=3.893, word_reposition=0.737, kpe=0.807, ppl=76.39, wps=6924.1, ups=0.34, wpb=20469.3, bsz=256, num_updates=25700, lr=0.000220541, gnorm=1.57, clip=0, loss_scale=512, train_wall=259, wall=0
2022-07-09 03:35:50 | INFO | train | epoch 023 | loss nan | nll_loss 2.234 | mask_ins 0.817 | word_ins_ml 3.901 | word_reposition 0.737 | kpe nan | ppl nan | wps 6720.4 | ups 0.33 | wpb 20521.4 | bsz 255.8 | num_updates 25767 | lr 0.000220254 | gnorm 1.553 | clip 0 | loss_scale 3495 | train_wall 2910 | wall 0
2022-07-09 03:37:11 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 11.065 | nll_loss 5.455 | mask_ins 1.452 | word_ins_ml 6.909 | word_reposition 1.259 | kpe 1.445 | ppl 2142.07 | wps 12249 | wpb 2367.6 | bsz 32 | num_updates 25767 | best_loss 10.938
2022-07-09 03:37:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 23 @ 25767 updates, score 11.065) (writing took 4.334851912222803 seconds)
2022-07-09 03:39:28 | INFO | train_inner | epoch 024:     33 / 1122 loss=6.218, nll_loss=2.206, mask_ins=0.813, word_ins_ml=3.876, word_reposition=0.728, kpe=0.801, ppl=74.43, wps=4725.1, ups=0.23, wpb=20200.5, bsz=253.8, num_updates=25800, lr=0.000220113, gnorm=1.66, clip=0, loss_scale=512, train_wall=305, wall=0
2022-07-09 03:44:23 | INFO | train_inner | epoch 024:    133 / 1122 loss=nan, nll_loss=2.253, mask_ins=0.817, word_ins_ml=3.918, word_reposition=0.734, kpe=nan, ppl=nan, wps=6960.1, ups=0.34, wpb=20511.2, bsz=256, num_updates=25900, lr=0.000219687, gnorm=1.528, clip=0, loss_scale=512, train_wall=257, wall=0
2022-07-09 03:49:18 | INFO | train_inner | epoch 024:    233 / 1122 loss=6.244, nll_loss=2.241, mask_ins=0.808, word_ins_ml=3.907, word_reposition=0.738, kpe=0.791, ppl=75.81, wps=6953.1, ups=0.34, wpb=20501.7, bsz=256, num_updates=26000, lr=0.000219265, gnorm=1.53, clip=0, loss_scale=512, train_wall=258, wall=0
2022-07-09 03:54:13 | INFO | train_inner | epoch 024:    333 / 1122 loss=6.235, nll_loss=2.222, mask_ins=0.814, word_ins_ml=3.891, word_reposition=0.739, kpe=0.792, ppl=75.33, wps=6956.4, ups=0.34, wpb=20513.1, bsz=256, num_updates=26100, lr=0.000218844, gnorm=1.523, clip=0, loss_scale=609, train_wall=258, wall=0
2022-07-09 03:59:08 | INFO | train_inner | epoch 024:    433 / 1122 loss=nan, nll_loss=2.232, mask_ins=0.823, word_ins_ml=3.9, word_reposition=0.742, kpe=nan, ppl=nan, wps=6967, ups=0.34, wpb=20573.1, bsz=256, num_updates=26200, lr=0.000218426, gnorm=1.51, clip=0, loss_scale=1024, train_wall=259, wall=0
2022-07-09 04:04:03 | INFO | train_inner | epoch 024:    533 / 1122 loss=6.197, nll_loss=2.196, mask_ins=0.811, word_ins_ml=3.867, word_reposition=0.73, kpe=0.789, ppl=73.38, wps=6957.4, ups=0.34, wpb=20529.2, bsz=256, num_updates=26300, lr=0.00021801, gnorm=1.457, clip=0, loss_scale=1024, train_wall=258, wall=0
2022-07-09 04:08:56 | INFO | train_inner | epoch 024:    633 / 1122 loss=6.232, nll_loss=2.23, mask_ins=0.81, word_ins_ml=3.897, word_reposition=0.73, kpe=0.795, ppl=75.18, wps=6993.1, ups=0.34, wpb=20497.6, bsz=256, num_updates=26400, lr=0.000217597, gnorm=1.517, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-09 04:13:53 | INFO | train_inner | epoch 024:    733 / 1122 loss=6.278, nll_loss=2.265, mask_ins=0.815, word_ins_ml=3.927, word_reposition=0.739, kpe=0.797, ppl=77.6, wps=6944.1, ups=0.34, wpb=20582.8, bsz=256, num_updates=26500, lr=0.000217186, gnorm=1.588, clip=0, loss_scale=1024, train_wall=259, wall=0
2022-07-09 04:18:46 | INFO | train_inner | epoch 024:    833 / 1122 loss=6.208, nll_loss=2.205, mask_ins=0.81, word_ins_ml=3.875, word_reposition=0.729, kpe=0.794, ppl=73.95, wps=7009.1, ups=0.34, wpb=20556.9, bsz=256, num_updates=26600, lr=0.000216777, gnorm=1.514, clip=0, loss_scale=1096, train_wall=256, wall=0
2022-07-09 04:23:41 | INFO | train_inner | epoch 024:    933 / 1122 loss=6.238, nll_loss=2.227, mask_ins=0.814, word_ins_ml=3.894, word_reposition=0.731, kpe=0.798, ppl=75.46, wps=6963.4, ups=0.34, wpb=20582.2, bsz=256, num_updates=26700, lr=0.000216371, gnorm=1.519, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-09 04:28:34 | INFO | train_inner | epoch 024:   1033 / 1122 loss=6.237, nll_loss=2.225, mask_ins=0.812, word_ins_ml=3.893, word_reposition=0.733, kpe=0.799, ppl=75.44, wps=7002.2, ups=0.34, wpb=20515.8, bsz=256, num_updates=26800, lr=0.000215967, gnorm=1.536, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-09 04:32:55 | INFO | train | epoch 024 | loss nan | nll_loss 2.228 | mask_ins 0.813 | word_ins_ml 3.895 | word_reposition 0.735 | kpe nan | ppl nan | wps 6722.1 | ups 0.33 | wpb 20520.7 | bsz 255.8 | num_updates 26889 | lr 0.000215609 | gnorm 1.528 | clip 0 | loss_scale 1151 | train_wall 2926 | wall 0
2022-07-09 04:34:14 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 11.131 | nll_loss 5.521 | mask_ins 1.436 | word_ins_ml 6.976 | word_reposition 1.276 | kpe 1.444 | ppl 2243.15 | wps 12484.5 | wpb 2367.6 | bsz 32 | num_updates 26889 | best_loss 10.938
2022-07-09 04:34:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 24 @ 26889 updates, score 11.131) (writing took 4.330183065496385 seconds)
2022-07-09 04:34:51 | INFO | train_inner | epoch 025:     11 / 1122 loss=6.247, nll_loss=2.233, mask_ins=0.815, word_ins_ml=3.899, word_reposition=0.736, kpe=0.797, ppl=75.96, wps=5430.9, ups=0.27, wpb=20459.6, bsz=253.8, num_updates=26900, lr=0.000215565, gnorm=1.611, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-09 04:39:45 | INFO | train_inner | epoch 025:    111 / 1122 loss=6.2, nll_loss=2.226, mask_ins=0.8, word_ins_ml=3.894, word_reposition=0.728, kpe=0.779, ppl=73.53, wps=7014.8, ups=0.34, wpb=20600.8, bsz=256, num_updates=27000, lr=0.000215166, gnorm=1.54, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-09 04:45:29 | INFO | train_inner | epoch 025:    211 / 1122 loss=6.202, nll_loss=2.208, mask_ins=0.808, word_ins_ml=3.878, word_reposition=0.734, kpe=0.781, ppl=73.59, wps=5963.3, ups=0.29, wpb=20535.5, bsz=256, num_updates=27100, lr=0.000214768, gnorm=1.506, clip=0, loss_scale=2048, train_wall=307, wall=0
2022-07-09 04:48:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 04:50:29 | INFO | train_inner | epoch 025:    312 / 1122 loss=6.186, nll_loss=2.195, mask_ins=0.805, word_ins_ml=3.866, word_reposition=0.731, kpe=0.784, ppl=72.82, wps=6882, ups=0.33, wpb=20620.7, bsz=256, num_updates=27200, lr=0.000214373, gnorm=1.592, clip=0, loss_scale=2981, train_wall=262, wall=0
2022-07-09 04:55:23 | INFO | train_inner | epoch 025:    412 / 1122 loss=6.225, nll_loss=2.229, mask_ins=0.809, word_ins_ml=3.896, word_reposition=0.737, kpe=0.784, ppl=74.79, wps=6994, ups=0.34, wpb=20605.5, bsz=256, num_updates=27300, lr=0.00021398, gnorm=1.549, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-09 05:00:17 | INFO | train_inner | epoch 025:    512 / 1122 loss=6.195, nll_loss=2.19, mask_ins=0.816, word_ins_ml=3.862, word_reposition=0.729, kpe=0.788, ppl=73.26, wps=6972.7, ups=0.34, wpb=20484.7, bsz=256, num_updates=27400, lr=0.000213589, gnorm=1.519, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-09 05:05:13 | INFO | train_inner | epoch 025:    612 / 1122 loss=nan, nll_loss=2.185, mask_ins=0.811, word_ins_ml=3.857, word_reposition=0.734, kpe=nan, ppl=nan, wps=6910.7, ups=0.34, wpb=20420, bsz=256, num_updates=27500, lr=0.000213201, gnorm=1.551, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-09 05:10:07 | INFO | train_inner | epoch 025:    712 / 1122 loss=nan, nll_loss=2.18, mask_ins=0.804, word_ins_ml=3.853, word_reposition=0.725, kpe=nan, ppl=nan, wps=6976.3, ups=0.34, wpb=20502.3, bsz=256, num_updates=27600, lr=0.000212814, gnorm=1.547, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-09 05:15:03 | INFO | train_inner | epoch 025:    812 / 1122 loss=6.211, nll_loss=2.216, mask_ins=0.806, word_ins_ml=3.885, word_reposition=0.734, kpe=0.786, ppl=74.07, wps=6990.7, ups=0.34, wpb=20742.3, bsz=256, num_updates=27700, lr=0.00021243, gnorm=1.531, clip=0, loss_scale=2826, train_wall=259, wall=0
2022-07-09 05:19:55 | INFO | train_inner | epoch 025:    912 / 1122 loss=6.206, nll_loss=2.209, mask_ins=0.806, word_ins_ml=3.878, word_reposition=0.731, kpe=0.791, ppl=73.84, wps=6994.2, ups=0.34, wpb=20413.1, bsz=256, num_updates=27800, lr=0.000212047, gnorm=1.518, clip=0, loss_scale=4096, train_wall=255, wall=0
2022-07-09 05:23:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 05:24:53 | INFO | train_inner | epoch 025:   1013 / 1122 loss=6.176, nll_loss=2.178, mask_ins=0.799, word_ins_ml=3.851, word_reposition=0.734, kpe=0.793, ppl=72.31, wps=6856.7, ups=0.34, wpb=20406.2, bsz=256, num_updates=27900, lr=0.000211667, gnorm=1.508, clip=0, loss_scale=3690, train_wall=260, wall=0
2022-07-09 05:29:48 | INFO | train_inner | epoch 025:   1113 / 1122 loss=6.227, nll_loss=2.219, mask_ins=0.81, word_ins_ml=3.887, word_reposition=0.735, kpe=0.794, ppl=74.88, wps=6930.1, ups=0.34, wpb=20461.9, bsz=256, num_updates=28000, lr=0.000211289, gnorm=1.576, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-09 05:30:14 | INFO | train | epoch 025 | loss nan | nll_loss 2.204 | mask_ins 0.807 | word_ins_ml 3.874 | word_reposition 0.732 | kpe nan | ppl nan | wps 6683.8 | ups 0.33 | wpb 20520.3 | bsz 255.8 | num_updates 28009 | lr 0.000211255 | gnorm 1.545 | clip 0 | loss_scale 2532 | train_wall 2940 | wall 0
2022-07-09 05:31:33 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 11.089 | nll_loss 5.472 | mask_ins 1.467 | word_ins_ml 6.92 | word_reposition 1.202 | kpe 1.5 | ppl 2178.23 | wps 12485.6 | wpb 2367.6 | bsz 32 | num_updates 28009 | best_loss 10.938
2022-07-09 05:31:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 25 @ 28009 updates, score 11.089) (writing took 4.3170332396402955 seconds)
2022-07-09 05:36:05 | INFO | train_inner | epoch 026:     91 / 1122 loss=6.19, nll_loss=2.212, mask_ins=0.808, word_ins_ml=3.881, word_reposition=0.729, kpe=0.772, ppl=73.03, wps=5430.1, ups=0.27, wpb=20440.2, bsz=253.8, num_updates=28100, lr=0.000210912, gnorm=1.597, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-09 05:38:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-09 05:41:04 | INFO | train_inner | epoch 026:    192 / 1122 loss=6.148, nll_loss=2.17, mask_ins=0.802, word_ins_ml=3.843, word_reposition=0.731, kpe=0.773, ppl=70.92, wps=6839.1, ups=0.33, wpb=20501, bsz=256, num_updates=28200, lr=0.000210538, gnorm=1.509, clip=0, loss_scale=1592, train_wall=263, wall=0
2022-07-09 05:45:59 | INFO | train_inner | epoch 026:    292 / 1122 loss=6.155, nll_loss=2.178, mask_ins=0.806, word_ins_ml=3.851, word_reposition=0.727, kpe=0.771, ppl=71.28, wps=6928.1, ups=0.34, wpb=20422.1, bsz=256, num_updates=28300, lr=0.000210166, gnorm=1.56, clip=0, loss_scale=1024, train_wall=258, wall=0
2022-07-09 05:51:39 | INFO | train_inner | epoch 026:    392 / 1122 loss=6.158, nll_loss=2.171, mask_ins=0.801, word_ins_ml=3.845, word_reposition=0.732, kpe=0.78, ppl=71.39, wps=6057.2, ups=0.29, wpb=20614, bsz=256, num_updates=28400, lr=0.000209795, gnorm=1.539, clip=0, loss_scale=1024, train_wall=303, wall=0
2022-07-09 05:56:37 | INFO | train_inner | epoch 026:    492 / 1122 loss=6.15, nll_loss=2.176, mask_ins=0.792, word_ins_ml=3.848, word_reposition=0.733, kpe=0.777, ppl=71.01, wps=6909.3, ups=0.34, wpb=20534.5, bsz=256, num_updates=28500, lr=0.000209427, gnorm=1.539, clip=0, loss_scale=1024, train_wall=261, wall=0
2022-07-09 06:01:32 | INFO | train_inner | epoch 026:    592 / 1122 loss=6.134, nll_loss=2.164, mask_ins=0.794, word_ins_ml=3.838, word_reposition=0.725, kpe=0.777, ppl=70.24, wps=6907.4, ups=0.34, wpb=20434.2, bsz=256, num_updates=28600, lr=0.000209061, gnorm=1.523, clip=0, loss_scale=1024, train_wall=259, wall=0
2022-07-09 06:06:26 | INFO | train_inner | epoch 026:    692 / 1122 loss=nan, nll_loss=2.165, mask_ins=0.809, word_ins_ml=3.839, word_reposition=0.729, kpe=nan, ppl=nan, wps=7015.9, ups=0.34, wpb=20589.7, bsz=256, num_updates=28700, lr=0.000208696, gnorm=1.566, clip=0, loss_scale=1362, train_wall=256, wall=0
2022-07-09 06:11:20 | INFO | train_inner | epoch 026:    792 / 1122 loss=nan, nll_loss=2.119, mask_ins=0.793, word_ins_ml=3.798, word_reposition=0.725, kpe=nan, ppl=nan, wps=6993.1, ups=0.34, wpb=20574.5, bsz=256, num_updates=28800, lr=0.000208333, gnorm=1.546, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 06:16:16 | INFO | train_inner | epoch 026:    892 / 1122 loss=6.173, nll_loss=2.188, mask_ins=0.804, word_ins_ml=3.858, word_reposition=0.731, kpe=0.78, ppl=72.15, wps=6962.5, ups=0.34, wpb=20627.2, bsz=256, num_updates=28900, lr=0.000207973, gnorm=1.539, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-09 06:21:10 | INFO | train_inner | epoch 026:    992 / 1122 loss=6.178, nll_loss=2.185, mask_ins=0.806, word_ins_ml=3.856, word_reposition=0.732, kpe=0.784, ppl=72.39, wps=7003.8, ups=0.34, wpb=20546.1, bsz=256, num_updates=29000, lr=0.000207614, gnorm=1.564, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-09 06:26:07 | INFO | train_inner | epoch 026:   1092 / 1122 loss=6.159, nll_loss=2.177, mask_ins=0.799, word_ins_ml=3.849, word_reposition=0.726, kpe=0.785, ppl=71.44, wps=6922.5, ups=0.34, wpb=20558.4, bsz=256, num_updates=29100, lr=0.000207257, gnorm=1.555, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-09 06:27:34 | INFO | train | epoch 026 | loss nan | nll_loss 2.174 | mask_ins 0.801 | word_ins_ml 3.847 | word_reposition 0.729 | kpe nan | ppl nan | wps 6686 | ups 0.33 | wpb 20520.6 | bsz 255.8 | num_updates 29130 | lr 0.00020715 | gnorm 1.547 | clip 0 | loss_scale 1581 | train_wall 2944 | wall 0
2022-07-09 06:28:53 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 10.905 | nll_loss 5.357 | mask_ins 1.447 | word_ins_ml 6.819 | word_reposition 1.181 | kpe 1.458 | ppl 1917.72 | wps 12504.7 | wpb 2367.6 | bsz 32 | num_updates 29130 | best_loss 10.905
2022-07-09 06:29:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 26 @ 29130 updates, score 10.905) (writing took 8.528724011033773 seconds)
2022-07-09 06:32:29 | INFO | train_inner | epoch 027:     70 / 1122 loss=6.182, nll_loss=2.205, mask_ins=0.802, word_ins_ml=3.874, word_reposition=0.733, kpe=0.773, ppl=72.63, wps=5327.5, ups=0.26, wpb=20348.1, bsz=253.8, num_updates=29200, lr=0.000206901, gnorm=1.577, clip=0, loss_scale=2478, train_wall=257, wall=0
2022-07-09 06:37:25 | INFO | train_inner | epoch 027:    170 / 1122 loss=6.09, nll_loss=2.119, mask_ins=0.802, word_ins_ml=3.797, word_reposition=0.726, kpe=0.766, ppl=68.13, wps=6989.7, ups=0.34, wpb=20676.8, bsz=256, num_updates=29300, lr=0.000206548, gnorm=1.521, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-09 06:42:20 | INFO | train_inner | epoch 027:    270 / 1122 loss=6.126, nll_loss=2.168, mask_ins=0.796, word_ins_ml=3.841, word_reposition=0.724, kpe=0.765, ppl=69.85, wps=6987.4, ups=0.34, wpb=20666.8, bsz=256, num_updates=29400, lr=0.000206197, gnorm=1.561, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-09 06:47:15 | INFO | train_inner | epoch 027:    370 / 1122 loss=nan, nll_loss=2.156, mask_ins=0.796, word_ins_ml=3.83, word_reposition=0.72, kpe=nan, ppl=nan, wps=6892.8, ups=0.34, wpb=20301.6, bsz=256, num_updates=29500, lr=0.000205847, gnorm=1.539, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 06:52:10 | INFO | train_inner | epoch 027:    470 / 1122 loss=6.14, nll_loss=2.177, mask_ins=0.795, word_ins_ml=3.849, word_reposition=0.728, kpe=0.768, ppl=70.5, wps=6927.8, ups=0.34, wpb=20444.7, bsz=256, num_updates=29600, lr=0.000205499, gnorm=1.56, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-09 06:53:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 06:57:52 | INFO | train_inner | epoch 027:    571 / 1122 loss=6.114, nll_loss=2.164, mask_ins=0.787, word_ins_ml=3.838, word_reposition=0.719, kpe=0.77, ppl=69.24, wps=6014, ups=0.29, wpb=20558.8, bsz=256, num_updates=29700, lr=0.000205152, gnorm=1.616, clip=0, loss_scale=2616, train_wall=304, wall=0
2022-07-09 07:02:48 | INFO | train_inner | epoch 027:    671 / 1122 loss=6.155, nll_loss=2.188, mask_ins=0.797, word_ins_ml=3.859, word_reposition=0.728, kpe=0.771, ppl=71.24, wps=6944, ups=0.34, wpb=20557.9, bsz=256, num_updates=29800, lr=0.000204808, gnorm=1.538, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-09 07:07:40 | INFO | train_inner | epoch 027:    771 / 1122 loss=6.146, nll_loss=2.182, mask_ins=0.798, word_ins_ml=3.853, word_reposition=0.725, kpe=0.771, ppl=70.83, wps=7038.1, ups=0.34, wpb=20540.3, bsz=256, num_updates=29900, lr=0.000204465, gnorm=1.568, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-09 07:12:34 | INFO | train_inner | epoch 027:    871 / 1122 loss=nan, nll_loss=2.164, mask_ins=0.805, word_ins_ml=3.838, word_reposition=0.725, kpe=nan, ppl=nan, wps=6986.6, ups=0.34, wpb=20552.4, bsz=256, num_updates=30000, lr=0.000204124, gnorm=1.591, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-09 07:17:27 | INFO | train_inner | epoch 027:    971 / 1122 loss=6.112, nll_loss=2.145, mask_ins=0.795, word_ins_ml=3.82, word_reposition=0.721, kpe=0.775, ppl=69.16, wps=7030.7, ups=0.34, wpb=20585, bsz=256, num_updates=30100, lr=0.000203785, gnorm=1.543, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-09 07:22:23 | INFO | train_inner | epoch 027:   1071 / 1122 loss=6.177, nll_loss=2.195, mask_ins=0.8, word_ins_ml=3.865, word_reposition=0.737, kpe=0.776, ppl=72.38, wps=6925.8, ups=0.34, wpb=20504.8, bsz=256, num_updates=30200, lr=0.000203447, gnorm=1.532, clip=0, loss_scale=3297, train_wall=259, wall=0
2022-07-09 07:24:54 | INFO | train | epoch 027 | loss nan | nll_loss 2.168 | mask_ins 0.797 | word_ins_ml 3.841 | word_reposition 0.725 | kpe nan | ppl nan | wps 6687.3 | ups 0.33 | wpb 20521.6 | bsz 255.8 | num_updates 30251 | lr 0.000203276 | gnorm 1.56 | clip 0 | loss_scale 3072 | train_wall 2938 | wall 0
2022-07-09 07:26:14 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 10.949 | nll_loss 5.358 | mask_ins 1.422 | word_ins_ml 6.816 | word_reposition 1.259 | kpe 1.453 | ppl 1976.71 | wps 12443.1 | wpb 2367.6 | bsz 32 | num_updates 30251 | best_loss 10.905
2022-07-09 07:26:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 27 @ 30251 updates, score 10.949) (writing took 4.358005594462156 seconds)
2022-07-09 07:28:43 | INFO | train_inner | epoch 028:     49 / 1122 loss=6.109, nll_loss=2.156, mask_ins=0.794, word_ins_ml=3.83, word_reposition=0.722, kpe=0.764, ppl=69.03, wps=5349.7, ups=0.26, wpb=20347.8, bsz=253.8, num_updates=30300, lr=0.000203111, gnorm=1.584, clip=0, loss_scale=4096, train_wall=260, wall=0
2022-07-09 07:30:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 07:33:42 | INFO | train_inner | epoch 028:    150 / 1122 loss=6.164, nll_loss=2.212, mask_ins=0.795, word_ins_ml=3.88, word_reposition=0.731, kpe=0.757, ppl=71.69, wps=6870.6, ups=0.33, wpb=20510.5, bsz=256, num_updates=30400, lr=0.000202777, gnorm=1.558, clip=0, loss_scale=2595, train_wall=262, wall=0
2022-07-09 07:38:38 | INFO | train_inner | epoch 028:    250 / 1122 loss=6.098, nll_loss=2.163, mask_ins=0.79, word_ins_ml=3.837, word_reposition=0.718, kpe=0.754, ppl=68.52, wps=6939.1, ups=0.34, wpb=20532.5, bsz=256, num_updates=30500, lr=0.000202444, gnorm=1.544, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-09 07:43:32 | INFO | train_inner | epoch 028:    350 / 1122 loss=6.143, nll_loss=2.188, mask_ins=0.8, word_ins_ml=3.859, word_reposition=0.722, kpe=0.762, ppl=70.68, wps=6955.1, ups=0.34, wpb=20494, bsz=256, num_updates=30600, lr=0.000202113, gnorm=1.576, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 07:48:29 | INFO | train_inner | epoch 028:    450 / 1122 loss=6.114, nll_loss=2.161, mask_ins=0.795, word_ins_ml=3.835, word_reposition=0.725, kpe=0.759, ppl=69.25, wps=6980, ups=0.34, wpb=20691.6, bsz=256, num_updates=30700, lr=0.000201784, gnorm=1.575, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-09 07:53:25 | INFO | train_inner | epoch 028:    550 / 1122 loss=nan, nll_loss=2.162, mask_ins=0.793, word_ins_ml=3.835, word_reposition=0.719, kpe=nan, ppl=nan, wps=6911.6, ups=0.34, wpb=20487.6, bsz=256, num_updates=30800, lr=0.000201456, gnorm=1.563, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-09 07:58:18 | INFO | train_inner | epoch 028:    650 / 1122 loss=nan, nll_loss=2.173, mask_ins=0.795, word_ins_ml=3.845, word_reposition=0.725, kpe=nan, ppl=nan, wps=6988.3, ups=0.34, wpb=20475, bsz=256, num_updates=30900, lr=0.000201129, gnorm=1.594, clip=0, loss_scale=3318, train_wall=256, wall=0
2022-07-09 08:04:01 | INFO | train_inner | epoch 028:    750 / 1122 loss=6.115, nll_loss=2.159, mask_ins=0.793, word_ins_ml=3.833, word_reposition=0.726, kpe=0.763, ppl=69.29, wps=5973, ups=0.29, wpb=20465.5, bsz=256, num_updates=31000, lr=0.000200805, gnorm=1.579, clip=0, loss_scale=4096, train_wall=305, wall=0
2022-07-09 08:08:55 | INFO | train_inner | epoch 028:    850 / 1122 loss=6.073, nll_loss=2.125, mask_ins=0.786, word_ins_ml=3.802, word_reposition=0.722, kpe=0.763, ppl=67.34, wps=6971.4, ups=0.34, wpb=20481.8, bsz=256, num_updates=31100, lr=0.000200482, gnorm=1.518, clip=0, loss_scale=4096, train_wall=257, wall=0
2022-07-09 08:13:47 | INFO | train_inner | epoch 028:    950 / 1122 loss=6.136, nll_loss=2.177, mask_ins=0.795, word_ins_ml=3.849, word_reposition=0.722, kpe=0.77, ppl=70.31, wps=7018.9, ups=0.34, wpb=20498.2, bsz=256, num_updates=31200, lr=0.00020016, gnorm=1.601, clip=0, loss_scale=4096, train_wall=256, wall=0
2022-07-09 08:18:42 | INFO | train_inner | epoch 028:   1050 / 1122 loss=6.163, nll_loss=2.188, mask_ins=0.803, word_ins_ml=3.858, word_reposition=0.731, kpe=0.769, ppl=71.63, wps=6971.2, ups=0.34, wpb=20590.3, bsz=256, num_updates=31300, lr=0.00019984, gnorm=1.591, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 08:22:15 | INFO | train | epoch 028 | loss nan | nll_loss 2.168 | mask_ins 0.794 | word_ins_ml 3.841 | word_reposition 0.725 | kpe nan | ppl nan | wps 6685.5 | ups 0.33 | wpb 20520.9 | bsz 255.8 | num_updates 31372 | lr 0.000199611 | gnorm 1.571 | clip 0 | loss_scale 3242 | train_wall 2945 | wall 0
2022-07-09 08:23:35 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 11.001 | nll_loss 5.415 | mask_ins 1.449 | word_ins_ml 6.865 | word_reposition 1.252 | kpe 1.435 | ppl 2049.31 | wps 12431.8 | wpb 2367.6 | bsz 32 | num_updates 31372 | best_loss 10.905
2022-07-09 08:23:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 28 @ 31372 updates, score 11.001) (writing took 4.386833377182484 seconds)
2022-07-09 08:24:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-09 08:25:03 | INFO | train_inner | epoch 029:     29 / 1122 loss=6.102, nll_loss=2.148, mask_ins=0.789, word_ins_ml=3.823, word_reposition=0.726, kpe=0.764, ppl=68.71, wps=5366.3, ups=0.26, wpb=20470.1, bsz=253.8, num_updates=31400, lr=0.000199522, gnorm=1.59, clip=0, loss_scale=5759, train_wall=260, wall=0
2022-07-09 08:29:59 | INFO | train_inner | epoch 029:    129 / 1122 loss=6.084, nll_loss=2.155, mask_ins=0.785, word_ins_ml=3.829, word_reposition=0.72, kpe=0.751, ppl=67.86, wps=6970.4, ups=0.34, wpb=20577.9, bsz=256, num_updates=31500, lr=0.000199205, gnorm=1.619, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 08:33:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 08:34:58 | INFO | train_inner | epoch 029:    230 / 1122 loss=6.049, nll_loss=2.119, mask_ins=0.783, word_ins_ml=3.797, word_reposition=0.723, kpe=0.747, ppl=66.23, wps=6846.3, ups=0.33, wpb=20516.9, bsz=256, num_updates=31600, lr=0.000198889, gnorm=1.581, clip=0, loss_scale=3488, train_wall=263, wall=0
2022-07-09 08:39:55 | INFO | train_inner | epoch 029:    330 / 1122 loss=6.095, nll_loss=2.15, mask_ins=0.793, word_ins_ml=3.824, word_reposition=0.724, kpe=0.753, ppl=68.36, wps=6928.5, ups=0.34, wpb=20523.2, bsz=256, num_updates=31700, lr=0.000198575, gnorm=1.684, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-09 08:44:50 | INFO | train_inner | epoch 029:    430 / 1122 loss=nan, nll_loss=2.15, mask_ins=0.787, word_ins_ml=3.824, word_reposition=0.724, kpe=nan, ppl=nan, wps=6931.7, ups=0.34, wpb=20498.9, bsz=256, num_updates=31800, lr=0.000198263, gnorm=1.583, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-09 08:49:46 | INFO | train_inner | epoch 029:    530 / 1122 loss=6.101, nll_loss=2.157, mask_ins=0.79, word_ins_ml=3.831, word_reposition=0.727, kpe=0.753, ppl=68.64, wps=6937.6, ups=0.34, wpb=20517.3, bsz=256, num_updates=31900, lr=0.000197952, gnorm=1.595, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-09 08:54:40 | INFO | train_inner | epoch 029:    630 / 1122 loss=nan, nll_loss=2.168, mask_ins=0.789, word_ins_ml=3.841, word_reposition=0.725, kpe=nan, ppl=nan, wps=6990, ups=0.34, wpb=20577.3, bsz=256, num_updates=32000, lr=0.000197642, gnorm=1.578, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 08:59:37 | INFO | train_inner | epoch 029:    730 / 1122 loss=6.083, nll_loss=2.146, mask_ins=0.784, word_ins_ml=3.821, word_reposition=0.721, kpe=0.758, ppl=67.81, wps=6908.2, ups=0.34, wpb=20499.4, bsz=256, num_updates=32100, lr=0.000197334, gnorm=1.577, clip=0, loss_scale=2417, train_wall=260, wall=0
2022-07-09 09:04:43 | INFO | train_inner | epoch 029:    830 / 1122 loss=6.071, nll_loss=2.137, mask_ins=0.785, word_ins_ml=3.812, word_reposition=0.718, kpe=0.756, ppl=67.24, wps=6714.2, ups=0.33, wpb=20548.1, bsz=256, num_updates=32200, lr=0.000197028, gnorm=1.551, clip=0, loss_scale=4096, train_wall=269, wall=0
2022-07-09 09:10:13 | INFO | train_inner | epoch 029:    930 / 1122 loss=6.077, nll_loss=2.143, mask_ins=0.78, word_ins_ml=3.818, word_reposition=0.723, kpe=0.757, ppl=67.52, wps=6253.5, ups=0.3, wpb=20613.2, bsz=256, num_updates=32300, lr=0.000196722, gnorm=1.6, clip=0, loss_scale=4096, train_wall=292, wall=0
2022-07-09 09:15:06 | INFO | train_inner | epoch 029:   1030 / 1122 loss=6.149, nll_loss=2.195, mask_ins=0.791, word_ins_ml=3.864, word_reposition=0.73, kpe=0.764, ppl=70.94, wps=6986.2, ups=0.34, wpb=20475.5, bsz=256, num_updates=32400, lr=0.000196419, gnorm=1.617, clip=0, loss_scale=4096, train_wall=256, wall=0
2022-07-09 09:18:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 09:19:36 | INFO | train | epoch 029 | loss nan | nll_loss 2.15 | mask_ins 0.787 | word_ins_ml 3.825 | word_reposition 0.723 | kpe nan | ppl nan | wps 6672.8 | ups 0.33 | wpb 20520 | bsz 255.8 | num_updates 32491 | lr 0.000196143 | gnorm 1.604 | clip 0 | loss_scale 3198 | train_wall 2943 | wall 0
2022-07-09 09:20:56 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 11.04 | nll_loss 5.399 | mask_ins 1.432 | word_ins_ml 6.852 | word_reposition 1.277 | kpe 1.478 | ppl 2105.03 | wps 12471 | wpb 2367.6 | bsz 32 | num_updates 32491 | best_loss 10.905
2022-07-09 09:21:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 29 @ 32491 updates, score 11.04) (writing took 4.597945673391223 seconds)
2022-07-09 09:21:26 | INFO | train_inner | epoch 030:      9 / 1122 loss=6.06, nll_loss=2.127, mask_ins=0.788, word_ins_ml=3.803, word_reposition=0.715, kpe=0.755, ppl=66.74, wps=5351.1, ups=0.26, wpb=20353.1, bsz=253.8, num_updates=32500, lr=0.000196116, gnorm=1.688, clip=0, loss_scale=3549, train_wall=259, wall=0
2022-07-09 09:26:21 | INFO | train_inner | epoch 030:    109 / 1122 loss=6.087, nll_loss=2.161, mask_ins=0.787, word_ins_ml=3.835, word_reposition=0.721, kpe=0.743, ppl=67.96, wps=6964.3, ups=0.34, wpb=20511.5, bsz=256, num_updates=32600, lr=0.000195815, gnorm=1.583, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 09:31:18 | INFO | train_inner | epoch 030:    209 / 1122 loss=6.084, nll_loss=2.152, mask_ins=0.792, word_ins_ml=3.826, word_reposition=0.725, kpe=0.741, ppl=67.83, wps=6909.8, ups=0.34, wpb=20516.2, bsz=256, num_updates=32700, lr=0.000195515, gnorm=1.611, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-09 09:36:14 | INFO | train_inner | epoch 030:    309 / 1122 loss=6.054, nll_loss=2.136, mask_ins=0.778, word_ins_ml=3.812, word_reposition=0.724, kpe=0.741, ppl=66.46, wps=6923.1, ups=0.34, wpb=20513, bsz=256, num_updates=32800, lr=0.000195217, gnorm=1.617, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-09 09:41:09 | INFO | train_inner | epoch 030:    409 / 1122 loss=6.046, nll_loss=2.125, mask_ins=0.782, word_ins_ml=3.802, word_reposition=0.718, kpe=0.744, ppl=66.08, wps=6923.6, ups=0.34, wpb=20416.1, bsz=256, num_updates=32900, lr=0.00019492, gnorm=1.625, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 09:46:03 | INFO | train_inner | epoch 030:    509 / 1122 loss=nan, nll_loss=2.126, mask_ins=0.784, word_ins_ml=3.803, word_reposition=0.729, kpe=nan, ppl=nan, wps=7029.7, ups=0.34, wpb=20691.1, bsz=256, num_updates=33000, lr=0.000194625, gnorm=1.596, clip=0, loss_scale=2355, train_wall=257, wall=0
2022-07-09 09:50:59 | INFO | train_inner | epoch 030:    609 / 1122 loss=6.072, nll_loss=2.138, mask_ins=0.784, word_ins_ml=3.813, word_reposition=0.725, kpe=0.75, ppl=67.27, wps=6930.1, ups=0.34, wpb=20469.6, bsz=256, num_updates=33100, lr=0.000194331, gnorm=1.572, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-09 09:55:55 | INFO | train_inner | epoch 030:    709 / 1122 loss=6.072, nll_loss=2.149, mask_ins=0.786, word_ins_ml=3.824, word_reposition=0.714, kpe=0.749, ppl=67.28, wps=6888.9, ups=0.34, wpb=20417.6, bsz=256, num_updates=33200, lr=0.000194038, gnorm=1.614, clip=0, loss_scale=4096, train_wall=260, wall=0
2022-07-09 09:56:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 10:00:51 | INFO | train_inner | epoch 030:    810 / 1122 loss=6.052, nll_loss=2.118, mask_ins=0.79, word_ins_ml=3.796, word_reposition=0.717, kpe=0.749, ppl=66.35, wps=6946.5, ups=0.34, wpb=20584.5, bsz=256, num_updates=33300, lr=0.000193746, gnorm=1.598, clip=0, loss_scale=2109, train_wall=259, wall=0
2022-07-09 10:05:47 | INFO | train_inner | epoch 030:    910 / 1122 loss=6.036, nll_loss=2.099, mask_ins=0.785, word_ins_ml=3.779, word_reposition=0.721, kpe=0.752, ppl=65.63, wps=6977.1, ups=0.34, wpb=20596.2, bsz=256, num_updates=33400, lr=0.000193456, gnorm=1.582, clip=0, loss_scale=2048, train_wall=259, wall=0
2022-07-09 10:10:56 | INFO | train_inner | epoch 030:   1010 / 1122 loss=6.058, nll_loss=2.125, mask_ins=0.781, word_ins_ml=3.801, word_reposition=0.725, kpe=0.75, ppl=66.61, wps=6655.4, ups=0.32, wpb=20619.7, bsz=256, num_updates=33500, lr=0.000193167, gnorm=1.654, clip=0, loss_scale=2048, train_wall=273, wall=0
2022-07-09 10:16:30 | INFO | train_inner | epoch 030:   1110 / 1122 loss=nan, nll_loss=2.11, mask_ins=0.782, word_ins_ml=3.788, word_reposition=0.713, kpe=nan, ppl=nan, wps=6156.8, ups=0.3, wpb=20567.5, bsz=256, num_updates=33600, lr=0.000192879, gnorm=1.59, clip=0, loss_scale=2048, train_wall=297, wall=0
2022-07-09 10:17:06 | INFO | train | epoch 030 | loss nan | nll_loss 2.13 | mask_ins 0.785 | word_ins_ml 3.807 | word_reposition 0.721 | kpe nan | ppl nan | wps 6669 | ups 0.32 | wpb 20520.4 | bsz 255.8 | num_updates 33612 | lr 0.000192845 | gnorm 1.61 | clip 0 | loss_scale 2446 | train_wall 2954 | wall 0
2022-07-09 10:18:25 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 11.037 | nll_loss 5.359 | mask_ins 1.435 | word_ins_ml 6.817 | word_reposition 1.289 | kpe 1.496 | ppl 2101.19 | wps 12465.9 | wpb 2367.6 | bsz 32 | num_updates 33612 | best_loss 10.905
2022-07-09 10:18:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 30 @ 33612 updates, score 11.037) (writing took 4.343686439096928 seconds)
2022-07-09 10:22:48 | INFO | train_inner | epoch 031:     88 / 1122 loss=nan, nll_loss=2.117, mask_ins=0.78, word_ins_ml=3.795, word_reposition=0.723, kpe=nan, ppl=nan, wps=5436.5, ups=0.27, wpb=20501.3, bsz=253.8, num_updates=33700, lr=0.000192593, gnorm=1.656, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-09 10:27:42 | INFO | train_inner | epoch 031:    188 / 1122 loss=6.027, nll_loss=2.11, mask_ins=0.785, word_ins_ml=3.789, word_reposition=0.723, kpe=0.73, ppl=65.2, wps=6997.7, ups=0.34, wpb=20616.6, bsz=256, num_updates=33800, lr=0.000192308, gnorm=1.612, clip=0, loss_scale=3809, train_wall=258, wall=0
2022-07-09 10:32:37 | INFO | train_inner | epoch 031:    288 / 1122 loss=6.023, nll_loss=2.11, mask_ins=0.784, word_ins_ml=3.788, word_reposition=0.717, kpe=0.734, ppl=65.04, wps=6915.4, ups=0.34, wpb=20394.1, bsz=256, num_updates=33900, lr=0.000192024, gnorm=1.609, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 10:37:32 | INFO | train_inner | epoch 031:    388 / 1122 loss=6.072, nll_loss=2.15, mask_ins=0.786, word_ins_ml=3.825, word_reposition=0.726, kpe=0.735, ppl=67.25, wps=6993.6, ups=0.34, wpb=20602, bsz=256, num_updates=34000, lr=0.000191741, gnorm=1.678, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 10:42:27 | INFO | train_inner | epoch 031:    488 / 1122 loss=6.022, nll_loss=2.114, mask_ins=0.784, word_ins_ml=3.792, word_reposition=0.708, kpe=0.738, ppl=65, wps=6929.3, ups=0.34, wpb=20446.6, bsz=256, num_updates=34100, lr=0.00019146, gnorm=1.655, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 10:47:20 | INFO | train_inner | epoch 031:    588 / 1122 loss=6.024, nll_loss=2.097, mask_ins=0.784, word_ins_ml=3.777, word_reposition=0.724, kpe=0.739, ppl=65.09, wps=7027.8, ups=0.34, wpb=20621.1, bsz=256, num_updates=34200, lr=0.00019118, gnorm=1.581, clip=0, loss_scale=4096, train_wall=257, wall=0
2022-07-09 10:49:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-09 10:52:18 | INFO | train_inner | epoch 031:    689 / 1122 loss=6.02, nll_loss=2.1, mask_ins=0.781, word_ins_ml=3.779, word_reposition=0.717, kpe=0.742, ppl=64.87, wps=6890.6, ups=0.34, wpb=20545.8, bsz=256, num_updates=34300, lr=0.000190901, gnorm=1.587, clip=0, loss_scale=4380, train_wall=261, wall=0
2022-07-09 10:57:13 | INFO | train_inner | epoch 031:    789 / 1122 loss=6.044, nll_loss=2.122, mask_ins=0.782, word_ins_ml=3.799, word_reposition=0.72, kpe=0.742, ppl=65.96, wps=6979.2, ups=0.34, wpb=20555.8, bsz=256, num_updates=34400, lr=0.000190623, gnorm=1.632, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 11:02:06 | INFO | train_inner | epoch 031:    889 / 1122 loss=6.028, nll_loss=2.116, mask_ins=0.776, word_ins_ml=3.794, word_reposition=0.715, kpe=0.744, ppl=65.28, wps=6989.3, ups=0.34, wpb=20495.4, bsz=256, num_updates=34500, lr=0.000190347, gnorm=1.621, clip=0, loss_scale=4096, train_wall=257, wall=0
2022-07-09 11:07:02 | INFO | train_inner | epoch 031:    989 / 1122 loss=6.016, nll_loss=2.095, mask_ins=0.781, word_ins_ml=3.775, word_reposition=0.718, kpe=0.741, ppl=64.69, wps=6892.2, ups=0.34, wpb=20402.1, bsz=256, num_updates=34600, lr=0.000190071, gnorm=1.618, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-09 11:07:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 11:12:01 | INFO | train_inner | epoch 031:   1090 / 1122 loss=6.016, nll_loss=2.099, mask_ins=0.776, word_ins_ml=3.778, word_reposition=0.718, kpe=0.743, ppl=64.71, wps=6877.3, ups=0.33, wpb=20556.5, bsz=256, num_updates=34700, lr=0.000189797, gnorm=1.66, clip=0, loss_scale=2170, train_wall=262, wall=0
2022-07-09 11:13:34 | INFO | train | epoch 031 | loss nan | nll_loss 2.111 | mask_ins 0.781 | word_ins_ml 3.79 | word_reposition 0.719 | kpe nan | ppl nan | wps 6783.3 | ups 0.33 | wpb 20520.5 | bsz 255.8 | num_updates 34732 | lr 0.00018971 | gnorm 1.625 | clip 0 | loss_scale 3704 | train_wall 2891 | wall 0
2022-07-09 11:14:54 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 11.054 | nll_loss 5.412 | mask_ins 1.443 | word_ins_ml 6.865 | word_reposition 1.237 | kpe 1.509 | ppl 2126.27 | wps 12389 | wpb 2367.6 | bsz 32 | num_updates 34732 | best_loss 10.905
2022-07-09 11:14:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 31 @ 34732 updates, score 11.054) (writing took 4.416868454776704 seconds)
2022-07-09 11:18:31 | INFO | train_inner | epoch 032:     68 / 1122 loss=nan, nll_loss=2.11, mask_ins=0.78, word_ins_ml=3.788, word_reposition=0.715, kpe=nan, ppl=nan, wps=5213.4, ups=0.26, wpb=20307.2, bsz=253.8, num_updates=34800, lr=0.000189525, gnorm=1.68, clip=0, loss_scale=2048, train_wall=269, wall=0
2022-07-09 11:24:02 | INFO | train_inner | epoch 032:    168 / 1122 loss=6.018, nll_loss=2.13, mask_ins=0.777, word_ins_ml=3.806, word_reposition=0.71, kpe=0.724, ppl=64.82, wps=6180.2, ups=0.3, wpb=20469.9, bsz=256, num_updates=34900, lr=0.000189253, gnorm=1.648, clip=0, loss_scale=2048, train_wall=294, wall=0
2022-07-09 11:28:57 | INFO | train_inner | epoch 032:    268 / 1122 loss=6.011, nll_loss=2.124, mask_ins=0.774, word_ins_ml=3.801, word_reposition=0.709, kpe=0.727, ppl=64.49, wps=6951.2, ups=0.34, wpb=20522.5, bsz=256, num_updates=35000, lr=0.000188982, gnorm=1.676, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 11:33:50 | INFO | train_inner | epoch 032:    368 / 1122 loss=6.011, nll_loss=2.1, mask_ins=0.781, word_ins_ml=3.78, word_reposition=0.722, kpe=0.727, ppl=64.48, wps=7053.5, ups=0.34, wpb=20675.9, bsz=256, num_updates=35100, lr=0.000188713, gnorm=1.618, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-09 11:38:44 | INFO | train_inner | epoch 032:    468 / 1122 loss=6.01, nll_loss=2.099, mask_ins=0.787, word_ins_ml=3.779, word_reposition=0.718, kpe=0.726, ppl=64.45, wps=7057.2, ups=0.34, wpb=20742.9, bsz=256, num_updates=35200, lr=0.000188445, gnorm=1.641, clip=0, loss_scale=3748, train_wall=257, wall=0
2022-07-09 11:43:39 | INFO | train_inner | epoch 032:    568 / 1122 loss=nan, nll_loss=2.081, mask_ins=0.768, word_ins_ml=3.762, word_reposition=0.711, kpe=nan, ppl=nan, wps=6981.3, ups=0.34, wpb=20548.1, bsz=256, num_updates=35300, lr=0.000188177, gnorm=1.664, clip=0, loss_scale=4096, train_wall=257, wall=0
2022-07-09 11:48:32 | INFO | train_inner | epoch 032:    668 / 1122 loss=6.062, nll_loss=2.137, mask_ins=0.787, word_ins_ml=3.813, word_reposition=0.726, kpe=0.737, ppl=66.83, wps=6956, ups=0.34, wpb=20442, bsz=256, num_updates=35400, lr=0.000187912, gnorm=1.662, clip=0, loss_scale=4096, train_wall=257, wall=0
2022-07-09 11:53:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 11:53:32 | INFO | train_inner | epoch 032:    769 / 1122 loss=5.993, nll_loss=2.09, mask_ins=0.777, word_ins_ml=3.771, word_reposition=0.712, kpe=0.732, ppl=63.67, wps=6780.1, ups=0.33, wpb=20344.6, bsz=256, num_updates=35500, lr=0.000187647, gnorm=1.638, clip=0, loss_scale=3954, train_wall=263, wall=0
2022-07-09 11:58:27 | INFO | train_inner | epoch 032:    869 / 1122 loss=5.992, nll_loss=2.094, mask_ins=0.771, word_ins_ml=3.774, word_reposition=0.712, kpe=0.736, ppl=63.65, wps=6938.4, ups=0.34, wpb=20421, bsz=256, num_updates=35600, lr=0.000187383, gnorm=1.682, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 12:03:20 | INFO | train_inner | epoch 032:    969 / 1122 loss=6.018, nll_loss=2.11, mask_ins=0.778, word_ins_ml=3.788, word_reposition=0.716, kpe=0.735, ppl=64.81, wps=7043.9, ups=0.34, wpb=20636.4, bsz=256, num_updates=35700, lr=0.00018712, gnorm=1.65, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-09 12:08:13 | INFO | train_inner | epoch 032:   1069 / 1122 loss=nan, nll_loss=2.074, mask_ins=0.78, word_ins_ml=3.756, word_reposition=0.71, kpe=nan, ppl=nan, wps=7019.1, ups=0.34, wpb=20567.6, bsz=256, num_updates=35800, lr=0.000186859, gnorm=1.636, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-09 12:10:47 | INFO | train | epoch 032 | loss nan | nll_loss 2.104 | mask_ins 0.778 | word_ins_ml 3.783 | word_reposition 0.715 | kpe nan | ppl nan | wps 6699.9 | ups 0.33 | wpb 20520 | bsz 255.8 | num_updates 35853 | lr 0.000186721 | gnorm 1.651 | clip 0 | loss_scale 2736 | train_wall 2937 | wall 0
2022-07-09 12:12:07 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 11.008 | nll_loss 5.367 | mask_ins 1.442 | word_ins_ml 6.817 | word_reposition 1.241 | kpe 1.508 | ppl 2059 | wps 12439.9 | wpb 2367.6 | bsz 32 | num_updates 35853 | best_loss 10.905
2022-07-09 12:12:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 32 @ 35853 updates, score 11.008) (writing took 4.531504054553807 seconds)
2022-07-09 12:14:29 | INFO | train_inner | epoch 033:     47 / 1122 loss=6.016, nll_loss=2.114, mask_ins=0.779, word_ins_ml=3.792, word_reposition=0.715, kpe=0.73, ppl=64.73, wps=5425.1, ups=0.27, wpb=20408.3, bsz=253.8, num_updates=35900, lr=0.000186598, gnorm=1.671, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-09 12:19:26 | INFO | train_inner | epoch 033:    147 / 1122 loss=5.973, nll_loss=2.088, mask_ins=0.772, word_ins_ml=3.768, word_reposition=0.716, kpe=0.717, ppl=62.8, wps=6963, ups=0.34, wpb=20682.7, bsz=256, num_updates=36000, lr=0.000186339, gnorm=1.666, clip=0, loss_scale=2048, train_wall=260, wall=0
2022-07-09 12:24:40 | INFO | train_inner | epoch 033:    247 / 1122 loss=5.986, nll_loss=2.088, mask_ins=0.78, word_ins_ml=3.768, word_reposition=0.719, kpe=0.719, ppl=63.37, wps=6541.8, ups=0.32, wpb=20534.4, bsz=256, num_updates=36100, lr=0.000186081, gnorm=1.628, clip=0, loss_scale=3994, train_wall=277, wall=0
2022-07-09 12:30:00 | INFO | train_inner | epoch 033:    347 / 1122 loss=5.945, nll_loss=2.063, mask_ins=0.768, word_ins_ml=3.747, word_reposition=0.713, kpe=0.717, ppl=61.59, wps=6436.1, ups=0.31, wpb=20586.8, bsz=256, num_updates=36200, lr=0.000185824, gnorm=1.62, clip=0, loss_scale=4096, train_wall=283, wall=0
2022-07-09 12:34:53 | INFO | train_inner | epoch 033:    447 / 1122 loss=5.964, nll_loss=2.075, mask_ins=0.775, word_ins_ml=3.757, word_reposition=0.708, kpe=0.724, ppl=62.43, wps=6978.9, ups=0.34, wpb=20441.4, bsz=256, num_updates=36300, lr=0.000185567, gnorm=1.641, clip=0, loss_scale=4096, train_wall=256, wall=0
2022-07-09 12:39:49 | INFO | train_inner | epoch 033:    547 / 1122 loss=5.988, nll_loss=2.118, mask_ins=0.762, word_ins_ml=3.795, word_reposition=0.711, kpe=0.721, ppl=63.48, wps=6920.4, ups=0.34, wpb=20521.4, bsz=256, num_updates=36400, lr=0.000185312, gnorm=1.678, clip=0, loss_scale=4096, train_wall=260, wall=0
2022-07-09 12:44:45 | INFO | train_inner | epoch 033:    647 / 1122 loss=nan, nll_loss=2.098, mask_ins=0.771, word_ins_ml=3.777, word_reposition=0.715, kpe=nan, ppl=nan, wps=6918.7, ups=0.34, wpb=20485.9, bsz=256, num_updates=36500, lr=0.000185058, gnorm=1.626, clip=0, loss_scale=4096, train_wall=259, wall=0
2022-07-09 12:49:40 | INFO | train_inner | epoch 033:    747 / 1122 loss=6, nll_loss=2.108, mask_ins=0.777, word_ins_ml=3.786, word_reposition=0.711, kpe=0.726, ppl=64, wps=6953.1, ups=0.34, wpb=20494.4, bsz=256, num_updates=36600, lr=0.000184805, gnorm=1.641, clip=0, loss_scale=7496, train_wall=258, wall=0
2022-07-09 12:50:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-09 12:54:38 | INFO | train_inner | epoch 033:    848 / 1122 loss=6.018, nll_loss=2.12, mask_ins=0.77, word_ins_ml=3.796, word_reposition=0.724, kpe=0.728, ppl=64.81, wps=6887.9, ups=0.34, wpb=20504.2, bsz=256, num_updates=36700, lr=0.000184553, gnorm=1.642, clip=0, loss_scale=4988, train_wall=260, wall=0
2022-07-09 12:59:33 | INFO | train_inner | epoch 033:    948 / 1122 loss=5.976, nll_loss=2.089, mask_ins=0.773, word_ins_ml=3.769, word_reposition=0.708, kpe=0.726, ppl=62.94, wps=6940.7, ups=0.34, wpb=20513, bsz=256, num_updates=36800, lr=0.000184302, gnorm=1.666, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 13:04:28 | INFO | train_inner | epoch 033:   1048 / 1122 loss=nan, nll_loss=2.096, mask_ins=0.773, word_ins_ml=3.775, word_reposition=0.712, kpe=nan, ppl=nan, wps=6997.7, ups=0.34, wpb=20620.1, bsz=256, num_updates=36900, lr=0.000184053, gnorm=1.616, clip=0, loss_scale=4096, train_wall=258, wall=0
2022-07-09 13:05:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 13:08:03 | INFO | train | epoch 033 | loss nan | nll_loss 2.098 | mask_ins 0.772 | word_ins_ml 3.777 | word_reposition 0.713 | kpe nan | ppl nan | wps 6689.3 | ups 0.33 | wpb 20520.8 | bsz 255.8 | num_updates 36973 | lr 0.000183871 | gnorm 1.656 | clip 0 | loss_scale 4094 | train_wall 2938 | wall 0
2022-07-09 13:09:23 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 11.042 | nll_loss 5.425 | mask_ins 1.413 | word_ins_ml 6.878 | word_reposition 1.271 | kpe 1.48 | ppl 2108.02 | wps 12380.6 | wpb 2367.6 | bsz 32 | num_updates 36973 | best_loss 10.905
2022-07-09 13:09:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 33 @ 36973 updates, score 11.042) (writing took 4.626632918603718 seconds)
2022-07-09 13:10:47 | INFO | train_inner | epoch 034:     27 / 1122 loss=6.007, nll_loss=2.119, mask_ins=0.772, word_ins_ml=3.795, word_reposition=0.712, kpe=0.728, ppl=64.31, wps=5359.8, ups=0.26, wpb=20315.6, bsz=253.8, num_updates=37000, lr=0.000183804, gnorm=1.795, clip=0, loss_scale=2352, train_wall=257, wall=0
2022-07-09 13:15:41 | INFO | train_inner | epoch 034:    127 / 1122 loss=5.96, nll_loss=2.087, mask_ins=0.773, word_ins_ml=3.767, word_reposition=0.712, kpe=0.708, ppl=62.25, wps=6966.1, ups=0.34, wpb=20470.5, bsz=256, num_updates=37100, lr=0.000183556, gnorm=1.719, clip=0, loss_scale=2048, train_wall=257, wall=0
2022-07-09 13:20:34 | INFO | train_inner | epoch 034:    227 / 1122 loss=5.968, nll_loss=2.094, mask_ins=0.773, word_ins_ml=3.774, word_reposition=0.713, kpe=0.707, ppl=62.58, wps=6983.7, ups=0.34, wpb=20452.5, bsz=256, num_updates=37200, lr=0.000183309, gnorm=1.706, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-09 13:25:29 | INFO | train_inner | epoch 034:    327 / 1122 loss=5.96, nll_loss=2.09, mask_ins=0.767, word_ins_ml=3.77, word_reposition=0.709, kpe=0.713, ppl=62.26, wps=6956, ups=0.34, wpb=20514.9, bsz=256, num_updates=37300, lr=0.000183063, gnorm=1.651, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 13:30:46 | INFO | train_inner | epoch 034:    427 / 1122 loss=nan, nll_loss=2.085, mask_ins=0.769, word_ins_ml=3.765, word_reposition=0.714, kpe=nan, ppl=nan, wps=6458.3, ups=0.31, wpb=20518.7, bsz=256, num_updates=37400, lr=0.000182818, gnorm=1.625, clip=0, loss_scale=2048, train_wall=281, wall=0
2022-07-09 13:35:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 13:36:13 | INFO | train_inner | epoch 034:    528 / 1122 loss=5.983, nll_loss=2.101, mask_ins=0.772, word_ins_ml=3.78, word_reposition=0.712, kpe=0.719, ppl=63.27, wps=6278.7, ups=0.31, wpb=20505.1, bsz=256, num_updates=37500, lr=0.000182574, gnorm=1.682, clip=0, loss_scale=3386, train_wall=289, wall=0
2022-07-09 13:41:08 | INFO | train_inner | epoch 034:    628 / 1122 loss=5.946, nll_loss=2.071, mask_ins=0.768, word_ins_ml=3.752, word_reposition=0.707, kpe=0.718, ppl=61.63, wps=6979.2, ups=0.34, wpb=20606.7, bsz=256, num_updates=37600, lr=0.000182331, gnorm=1.615, clip=0, loss_scale=2048, train_wall=258, wall=0
2022-07-09 13:45:59 | INFO | train_inner | epoch 034:    728 / 1122 loss=5.992, nll_loss=2.099, mask_ins=0.776, word_ins_ml=3.777, word_reposition=0.719, kpe=0.72, ppl=63.64, wps=7120.3, ups=0.34, wpb=20678.9, bsz=256, num_updates=37700, lr=0.000182089, gnorm=1.725, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-07-09 13:50:48 | INFO | train_inner | epoch 034:    828 / 1122 loss=5.921, nll_loss=2.061, mask_ins=0.758, word_ins_ml=3.743, word_reposition=0.703, kpe=0.717, ppl=60.58, wps=7124.7, ups=0.35, wpb=20575.8, bsz=256, num_updates=37800, lr=0.000181848, gnorm=1.658, clip=0, loss_scale=2048, train_wall=252, wall=0
2022-07-09 13:55:37 | INFO | train_inner | epoch 034:    928 / 1122 loss=5.98, nll_loss=2.103, mask_ins=0.765, word_ins_ml=3.782, word_reposition=0.713, kpe=0.722, ppl=63.14, wps=7097.4, ups=0.35, wpb=20562.6, bsz=256, num_updates=37900, lr=0.000181608, gnorm=1.705, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-07-09 14:00:26 | INFO | train_inner | epoch 034:   1028 / 1122 loss=5.942, nll_loss=2.056, mask_ins=0.768, word_ins_ml=3.739, word_reposition=0.709, kpe=0.725, ppl=61.47, wps=7089.1, ups=0.35, wpb=20483.9, bsz=256, num_updates=38000, lr=0.000181369, gnorm=1.673, clip=0, loss_scale=2048, train_wall=252, wall=0
2022-07-09 14:00:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 14:02:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-09 14:04:57 | INFO | train | epoch 034 | loss nan | nll_loss 2.081 | mask_ins 0.768 | word_ins_ml 3.762 | word_reposition 0.71 | kpe nan | ppl nan | wps 6727.2 | ups 0.33 | wpb 20522.5 | bsz 255.8 | num_updates 38092 | lr 0.00018115 | gnorm 1.684 | clip 0 | loss_scale 2118 | train_wall 2916 | wall 0
2022-07-09 14:06:16 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 11.158 | nll_loss 5.444 | mask_ins 1.417 | word_ins_ml 6.898 | word_reposition 1.262 | kpe 1.581 | ppl 2284.95 | wps 12476.3 | wpb 2367.6 | bsz 32 | num_updates 38092 | best_loss 10.905
2022-07-09 14:06:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 34 @ 38092 updates, score 11.158) (writing took 4.72983559127897 seconds)
2022-07-09 14:06:44 | INFO | train_inner | epoch 035:      8 / 1122 loss=5.901, nll_loss=2.038, mask_ins=0.76, word_ins_ml=3.724, word_reposition=0.701, kpe=0.717, ppl=59.77, wps=5403.6, ups=0.27, wpb=20388.1, bsz=253.8, num_updates=38100, lr=0.000181131, gnorm=1.742, clip=0, loss_scale=1416, train_wall=256, wall=0
2022-07-09 14:11:33 | INFO | train_inner | epoch 035:    108 / 1122 loss=nan, nll_loss=2.082, mask_ins=0.773, word_ins_ml=3.763, word_reposition=0.711, kpe=nan, ppl=nan, wps=7079.7, ups=0.35, wpb=20477.6, bsz=256, num_updates=38200, lr=0.000180894, gnorm=1.669, clip=0, loss_scale=1024, train_wall=252, wall=0
2022-07-09 14:16:23 | INFO | train_inner | epoch 035:    208 / 1122 loss=5.928, nll_loss=2.07, mask_ins=0.764, word_ins_ml=3.752, word_reposition=0.71, kpe=0.702, ppl=60.88, wps=7085, ups=0.35, wpb=20529.7, bsz=256, num_updates=38300, lr=0.000180657, gnorm=1.71, clip=0, loss_scale=1024, train_wall=253, wall=0
2022-07-09 14:21:12 | INFO | train_inner | epoch 035:    308 / 1122 loss=5.934, nll_loss=2.059, mask_ins=0.769, word_ins_ml=3.742, word_reposition=0.713, kpe=0.709, ppl=61.13, wps=7111.8, ups=0.35, wpb=20594.1, bsz=256, num_updates=38400, lr=0.000180422, gnorm=1.7, clip=0, loss_scale=1024, train_wall=252, wall=0
2022-07-09 14:26:02 | INFO | train_inner | epoch 035:    408 / 1122 loss=5.955, nll_loss=2.074, mask_ins=0.778, word_ins_ml=3.756, word_reposition=0.714, kpe=0.707, ppl=62.03, wps=7095.2, ups=0.34, wpb=20567.1, bsz=256, num_updates=38500, lr=0.000180187, gnorm=1.672, clip=0, loss_scale=1024, train_wall=253, wall=0
2022-07-09 14:30:51 | INFO | train_inner | epoch 035:    508 / 1122 loss=nan, nll_loss=2.065, mask_ins=0.763, word_ins_ml=3.747, word_reposition=0.716, kpe=nan, ppl=nan, wps=7122.1, ups=0.35, wpb=20596.6, bsz=256, num_updates=38600, lr=0.000179954, gnorm=1.664, clip=0, loss_scale=1608, train_wall=252, wall=0
2022-07-09 14:36:05 | INFO | train_inner | epoch 035:    608 / 1122 loss=5.92, nll_loss=2.059, mask_ins=0.763, word_ins_ml=3.742, word_reposition=0.706, kpe=0.709, ppl=60.54, wps=6512.7, ups=0.32, wpb=20406.1, bsz=256, num_updates=38700, lr=0.000179721, gnorm=1.713, clip=0, loss_scale=2048, train_wall=276, wall=0
2022-07-09 14:41:18 | INFO | train_inner | epoch 035:    708 / 1122 loss=5.946, nll_loss=2.072, mask_ins=0.768, word_ins_ml=3.754, word_reposition=0.714, kpe=0.71, ppl=61.63, wps=6573.8, ups=0.32, wpb=20601.1, bsz=256, num_updates=38800, lr=0.00017949, gnorm=1.634, clip=0, loss_scale=2048, train_wall=276, wall=0
2022-07-09 14:46:07 | INFO | train_inner | epoch 035:    808 / 1122 loss=5.935, nll_loss=2.071, mask_ins=0.762, word_ins_ml=3.752, word_reposition=0.711, kpe=0.711, ppl=61.2, wps=7077, ups=0.35, wpb=20462.5, bsz=256, num_updates=38900, lr=0.000179259, gnorm=1.639, clip=0, loss_scale=2048, train_wall=252, wall=0
2022-07-09 14:50:56 | INFO | train_inner | epoch 035:    908 / 1122 loss=5.926, nll_loss=2.07, mask_ins=0.756, word_ins_ml=3.752, word_reposition=0.708, kpe=0.71, ppl=60.8, wps=7094.1, ups=0.35, wpb=20500.3, bsz=256, num_updates=39000, lr=0.000179029, gnorm=1.68, clip=0, loss_scale=2048, train_wall=252, wall=0
2022-07-09 14:55:46 | INFO | train_inner | epoch 035:   1008 / 1122 loss=5.939, nll_loss=2.072, mask_ins=0.764, word_ins_ml=3.753, word_reposition=0.704, kpe=0.718, ppl=61.34, wps=7096.3, ups=0.35, wpb=20543.9, bsz=256, num_updates=39100, lr=0.0001788, gnorm=1.675, clip=0, loss_scale=2970, train_wall=253, wall=0
2022-07-09 14:57:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 15:00:38 | INFO | train_inner | epoch 035:   1109 / 1122 loss=5.894, nll_loss=2.032, mask_ins=0.756, word_ins_ml=3.718, word_reposition=0.706, kpe=0.714, ppl=59.46, wps=7048.9, ups=0.34, wpb=20581.1, bsz=256, num_updates=39200, lr=0.000178571, gnorm=1.676, clip=0, loss_scale=2595, train_wall=255, wall=0
2022-07-09 15:01:14 | INFO | train | epoch 035 | loss nan | nll_loss 2.065 | mask_ins 0.765 | word_ins_ml 3.748 | word_reposition 0.71 | kpe nan | ppl nan | wps 6811.3 | ups 0.33 | wpb 20519.6 | bsz 255.8 | num_updates 39213 | lr 0.000178542 | gnorm 1.68 | clip 0 | loss_scale 1768 | train_wall 2878 | wall 0
2022-07-09 15:02:33 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 11.003 | nll_loss 5.34 | mask_ins 1.436 | word_ins_ml 6.801 | word_reposition 1.248 | kpe 1.518 | ppl 2052.02 | wps 12445.8 | wpb 2367.6 | bsz 32 | num_updates 39213 | best_loss 10.905
2022-07-09 15:02:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 35 @ 39213 updates, score 11.003) (writing took 4.522537359036505 seconds)
2022-07-09 15:06:49 | INFO | train_inner | epoch 036:     87 / 1122 loss=nan, nll_loss=2.062, mask_ins=0.766, word_ins_ml=3.745, word_reposition=0.704, kpe=nan, ppl=nan, wps=5483.3, ups=0.27, wpb=20375.4, bsz=253.8, num_updates=39300, lr=0.000178344, gnorm=1.748, clip=0, loss_scale=2048, train_wall=251, wall=0
2022-07-09 15:11:39 | INFO | train_inner | epoch 036:    187 / 1122 loss=5.903, nll_loss=2.044, mask_ins=0.765, word_ins_ml=3.729, word_reposition=0.713, kpe=0.696, ppl=59.82, wps=7118.5, ups=0.35, wpb=20603.6, bsz=256, num_updates=39400, lr=0.000178118, gnorm=1.725, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-07-09 15:16:28 | INFO | train_inner | epoch 036:    287 / 1122 loss=5.888, nll_loss=2.038, mask_ins=0.763, word_ins_ml=3.723, word_reposition=0.702, kpe=0.7, ppl=59.21, wps=7097.6, ups=0.35, wpb=20543.1, bsz=256, num_updates=39500, lr=0.000177892, gnorm=1.641, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-07-09 15:21:18 | INFO | train_inner | epoch 036:    387 / 1122 loss=5.894, nll_loss=2.029, mask_ins=0.766, word_ins_ml=3.715, word_reposition=0.713, kpe=0.699, ppl=59.46, wps=7103.6, ups=0.35, wpb=20582.8, bsz=256, num_updates=39600, lr=0.000177667, gnorm=1.736, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-07-09 15:24:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 15:26:10 | INFO | train_inner | epoch 036:    488 / 1122 loss=5.929, nll_loss=2.076, mask_ins=0.761, word_ins_ml=3.758, word_reposition=0.71, kpe=0.7, ppl=60.93, wps=6985.5, ups=0.34, wpb=20428, bsz=256, num_updates=39700, lr=0.000177443, gnorm=1.717, clip=0, loss_scale=2494, train_wall=255, wall=0
2022-07-09 15:31:00 | INFO | train_inner | epoch 036:    588 / 1122 loss=5.887, nll_loss=2.044, mask_ins=0.753, word_ins_ml=3.728, word_reposition=0.706, kpe=0.7, ppl=59.18, wps=7064.1, ups=0.34, wpb=20490.8, bsz=256, num_updates=39800, lr=0.00017722, gnorm=1.77, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-07-09 15:35:50 | INFO | train_inner | epoch 036:    688 / 1122 loss=nan, nll_loss=2.043, mask_ins=0.765, word_ins_ml=3.728, word_reposition=0.714, kpe=nan, ppl=nan, wps=7093.8, ups=0.34, wpb=20572.5, bsz=256, num_updates=39900, lr=0.000176998, gnorm=1.723, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-07-09 15:41:03 | INFO | train_inner | epoch 036:    788 / 1122 loss=5.899, nll_loss=2.039, mask_ins=0.763, word_ins_ml=3.724, word_reposition=0.707, kpe=0.706, ppl=59.65, wps=6541.7, ups=0.32, wpb=20458.3, bsz=256, num_updates=40000, lr=0.000176777, gnorm=1.742, clip=0, loss_scale=2048, train_wall=276, wall=0
2022-07-09 15:46:15 | INFO | train_inner | epoch 036:    888 / 1122 loss=5.931, nll_loss=2.069, mask_ins=0.762, word_ins_ml=3.75, word_reposition=0.712, kpe=0.707, ppl=61.02, wps=6550.3, ups=0.32, wpb=20462.9, bsz=256, num_updates=40100, lr=0.000176556, gnorm=1.825, clip=0, loss_scale=2048, train_wall=275, wall=0
2022-07-09 15:51:05 | INFO | train_inner | epoch 036:    988 / 1122 loss=5.909, nll_loss=2.05, mask_ins=0.759, word_ins_ml=3.733, word_reposition=0.711, kpe=0.705, ppl=60.07, wps=7141.8, ups=0.35, wpb=20647.6, bsz=256, num_updates=40200, lr=0.000176336, gnorm=1.675, clip=0, loss_scale=2642, train_wall=253, wall=0
2022-07-09 15:55:54 | INFO | train_inner | epoch 036:   1088 / 1122 loss=5.899, nll_loss=2.036, mask_ins=0.756, word_ins_ml=3.722, word_reposition=0.714, kpe=0.707, ppl=59.67, wps=7106, ups=0.35, wpb=20576.8, bsz=256, num_updates=40300, lr=0.000176117, gnorm=1.685, clip=0, loss_scale=4096, train_wall=253, wall=0
2022-07-09 15:57:31 | INFO | train | epoch 036 | loss nan | nll_loss 2.049 | mask_ins 0.762 | word_ins_ml 3.733 | word_reposition 0.71 | kpe nan | ppl nan | wps 6810.5 | ups 0.33 | wpb 20520.2 | bsz 255.8 | num_updates 40334 | lr 0.000176043 | gnorm 1.726 | clip 0 | loss_scale 2386 | train_wall 2881 | wall 0
2022-07-09 15:58:51 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 11.165 | nll_loss 5.411 | mask_ins 1.447 | word_ins_ml 6.866 | word_reposition 1.247 | kpe 1.605 | ppl 2295.94 | wps 12487.9 | wpb 2367.6 | bsz 32 | num_updates 40334 | best_loss 10.905
2022-07-09 15:58:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 36 @ 40334 updates, score 11.165) (writing took 4.354829519055784 seconds)
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
train.sh: line 43: --finetune_position_embedding: command not found
