nohup: ignoring input
2022-07-29 14:55:35 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:17265
2022-07-29 14:55:36 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:17265
2022-07-29 14:55:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-29 14:55:36 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:17265
2022-07-29 14:55:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-29 14:55:36 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:17265
2022-07-29 14:55:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-29 14:55:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-29 14:55:36 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-29 14:55:36 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-29 14:55:36 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-29 14:55:36 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-29 14:55:36 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-29 14:55:36 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-29 14:55:36 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-29 14:55:36 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-29 14:55:48 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:17265', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_cased_XSum', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-XSum', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510_XSum', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
start load cached examples valid ...
0it [00:00, ?it/s]2022-07-29 14:55:48 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-29 14:55:48 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
start load cached examples valid ...
0it [00:00, ?it/s]2022-07-29 14:55:48 | INFO | fairseq.data.data_utils | loaded 11321 examples from: ../data-bin-bert-cased-XSum/valid.source-target.source
2022-07-29 14:55:48 | INFO | fairseq.data.data_utils | loaded 11321 examples from: ../data-bin-bert-cased-XSum/valid.source-target.target
2022-07-29 14:55:48 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-XSum valid source-target 11321 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]124it [00:00, 1238.04it/s]312it [00:00, 3115.99it/s]233it [00:00, 2307.52it/s]151it [00:00, 1428.19it/s]276it [00:00, 1372.61it/s]624it [00:00, 2780.36it/s]493it [00:00, 2474.76it/s]325it [00:00, 1557.34it/s]503it [00:00, 1775.97it/s]845it [00:00, 2949.38it/s]557it [00:00, 1870.49it/s]905it [00:00, 2504.11it/s]725it [00:00, 1928.52it/s]1141it [00:00, 2737.42it/s]897it [00:00, 2395.99it/s]1159it [00:00, 2458.46it/s]984it [00:00, 2163.50it/s]1417it [00:00, 2254.51it/s]1407it [00:00, 1977.88it/s]1137it [00:00, 1834.20it/s]1201it [00:00, 1793.94it/s]1816it [00:00, 2750.82it/s]1640it [00:00, 2064.25it/s]1390it [00:00, 1784.86it/s]1336it [00:00, 1700.69it/s]2221it [00:00, 2999.06it/s]1870it [00:00, 2129.73it/s]1575it [00:00, 1675.86it/s]2623it [00:00, 3288.09it/s]1517it [00:00, 1506.30it/s]2091it [00:00, 1939.48it/s]1873it [00:01, 2012.57it/s]3027it [00:01, 3505.29it/s]1771it [00:01, 1761.21it/s]2237it [00:01, 2405.02it/s]1999it [00:01, 1895.28it/s]2293it [00:01, 1504.03it/s]2655it [00:01, 2901.79it/s]3388it [00:01, 2927.43it/s]2201it [00:01, 1864.41it/s]3042it [00:01, 3176.94it/s]3703it [00:01, 2966.92it/s]2461it [00:01, 1471.67it/s]2404it [00:01, 1908.71it/s]2620it [00:01, 1464.43it/s]2601it [00:01, 1902.83it/s]3368it [00:01, 2626.22it/s]4016it [00:01, 2343.06it/s]2808it [00:01, 1533.22it/s]2975it [00:01, 2421.74it/s]3652it [00:01, 2454.37it/s]3020it [00:01, 1677.63it/s]4280it [00:01, 2112.79it/s]3969it [00:01, 2632.48it/s]3224it [00:01, 2098.02it/s]4383it [00:01, 3027.11it/s]3466it [00:01, 2181.62it/s]3195it [00:01, 1367.13it/s]4514it [00:01, 1870.43it/s]3869it [00:01, 2683.22it/s]3357it [00:01, 1426.60it/s]4702it [00:01, 2889.34it/s]4719it [00:01, 1857.02it/s]3511it [00:02, 1440.90it/s]4150it [00:02, 2493.43it/s]3753it [00:02, 1697.93it/s]4917it [00:02, 1512.32it/s]4410it [00:02, 2343.00it/s]5003it [00:02, 2073.51it/s]3955it [00:02, 1727.69it/s]5084it [00:02, 1544.35it/s]5250it [00:02, 2083.83it/s]4653it [00:02, 2201.68it/s]4135it [00:02, 1718.65it/s]5298it [00:02, 1662.19it/s]4880it [00:02, 2044.05it/s]5486it [00:02, 1965.31it/s]4383it [00:02, 1929.61it/s]5556it [00:02, 1886.12it/s]4643it [00:02, 2118.92it/s]5778it [00:02, 1933.86it/s]5090it [00:02, 1856.86it/s]5702it [00:02, 1734.87it/s]4860it [00:02, 2025.82it/s]6103it [00:02, 2283.53it/s]5960it [00:02, 1900.43it/s]5281it [00:02, 1641.26it/s]6364it [00:02, 2364.41it/s]5067it [00:02, 1967.69it/s]6167it [00:02, 1870.67it/s]5452it [00:02, 1570.13it/s]5320it [00:02, 2089.24it/s]6427it [00:02, 2050.39it/s]5648it [00:02, 1664.50it/s]5559it [00:02, 2172.53it/s]5836it [00:03, 1719.83it/s]5908it [00:03, 2550.34it/s]6013it [00:03, 1705.64it/s]6259it [00:03, 2828.73it/s]6297it [00:03, 2005.70it/s]6609it [00:03, 1037.26it/s]6502it [00:03, 1687.49it/s]6794it [00:03, 1139.70it/s]7017it [00:03, 1327.64it/s]6644it [00:03, 798.37it/s] 7210it [00:03, 1387.22it/s]6886it [00:03, 1000.76it/s]6545it [00:03, 1188.97it/s]7393it [00:03, 1301.22it/s]7123it [00:03, 1207.81it/s]6761it [00:03, 1272.38it/s]7643it [00:03, 1553.71it/s]7415it [00:03, 1511.96it/s]6961it [00:04, 1267.95it/s]7849it [00:04, 1597.14it/s]7687it [00:04, 1738.10it/s]7139it [00:04, 1342.02it/s]7964it [00:04, 1944.06it/s]8109it [00:04, 1813.27it/s]7475it [00:04, 1749.83it/s]8221it [00:04, 2092.71it/s]8419it [00:04, 2137.23it/s]7861it [00:04, 2222.54it/s]8617it [00:04, 2567.44it/s]8724it [00:04, 2377.84it/s]6682it [00:04, 549.48it/s] 8949it [00:04, 2766.64it/s]9072it [00:04, 2679.19it/s]6933it [00:04, 751.07it/s]8133it [00:04, 2177.20it/s]9488it [00:04, 3096.33it/s]7160it [00:04, 945.39it/s]8385it [00:04, 2248.29it/s]9254it [00:04, 2259.29it/s]9852it [00:04, 3171.52it/s]7346it [00:04, 1057.57it/s]9604it [00:04, 2546.77it/s]8636it [00:04, 1936.90it/s]10207it [00:04, 3260.42it/s]7622it [00:04, 1345.86it/s]8943it [00:04, 2203.10it/s]9890it [00:04, 2358.09it/s]10540it [00:04, 3004.09it/s]7837it [00:04, 1506.20it/s]9287it [00:04, 2513.11it/s]10232it [00:05, 2618.27it/s]8043it [00:04, 1591.13it/s]10849it [00:05, 2644.20it/s]9692it [00:05, 2917.76it/s]10594it [00:05, 2799.81it/s]10053it [00:05, 3105.38it/s]11126it [00:05, 2586.66it/s]8243it [00:05, 1440.61it/s]10381it [00:05, 2997.17it/s]8419it [00:05, 1511.69it/s]11321it [00:05, 2142.62it/s]
2022-07-29 14:55:54 | INFO | root | success load 11321 data
2022-07-29 14:55:54 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-29 14:55:54 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-29 14:55:54 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-29 14:55:54 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-29 14:55:54 | INFO | transformer.tokenization_utils | loading file None
2022-07-29 14:55:54 | INFO | transformer.tokenization_utils | loading file None
2022-07-29 14:55:54 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
10891it [00:05, 2058.69it/s]8595it [00:05, 1537.06it/s]10694it [00:05, 2823.80it/s]11136it [00:05, 2069.62it/s]8795it [00:05, 1617.36it/s]11321it [00:05, 2031.82it/s]
10987it [00:05, 2517.11it/s]8970it [00:05, 1503.54it/s]11287it [00:05, 2638.17it/s]11321it [00:05, 1999.19it/s]
2022-07-29 14:55:54 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-29 14:55:54 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-29 14:55:54 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
9191it [00:05, 1681.99it/s]9487it [00:05, 2021.41it/s]9701it [00:05, 1976.62it/s]10007it [00:06, 2257.37it/s]10264it [00:06, 2343.16it/s]10505it [00:06, 2193.34it/s]10731it [00:06, 1724.26it/s]10990it [00:06, 1904.90it/s]11321it [00:06, 1703.83it/s]
2022-07-29 14:56:02 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-29 14:56:02 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-29 14:56:02 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-29 14:56:02 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-29 14:56:02 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-29 14:56:10 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-29 14:56:10 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-07-29 14:56:10 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-07-29 14:56:10 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-29 14:56:10 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 142456835)
2022-07-29 14:56:10 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-07-29 14:56:10 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]2022-07-29 14:56:31 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-29 14:56:31 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-29 14:56:31 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt
2022-07-29 14:56:31 | INFO | fairseq.trainer | loading train data for epoch 1
150it [00:00, 168.28it/s]2022-07-29 14:56:31 | INFO | fairseq.data.data_utils | loaded 203998 examples from: ../data-bin-bert-cased-XSum/train.source-target.source
2022-07-29 14:56:31 | INFO | fairseq.data.data_utils | loaded 203998 examples from: ../data-bin-bert-cased-XSum/train.source-target.target
2022-07-29 14:56:31 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-XSum train source-target 203998 examples
start load cached examples train ...
0it [00:00, ?it/s]491it [00:00, 614.11it/s]695it [00:01, 804.39it/s]884it [00:01, 1002.02it/s]1116it [00:01, 1273.40it/s]1367it [00:01, 1554.75it/s]150it [00:00, 175.19it/s]1621it [00:01, 1775.30it/s]400it [00:00, 507.41it/s]150it [00:00, 187.91it/s]1847it [00:01, 1824.15it/s]692it [00:01, 916.41it/s]361it [00:00, 479.26it/s]2063it [00:01, 1906.78it/s]149it [00:00, 184.70it/s]909it [00:01, 1097.59it/s]594it [00:00, 808.63it/s]2363it [00:01, 2202.18it/s]425it [00:00, 563.51it/s]847it [00:01, 1149.11it/s]1107it [00:01, 1237.93it/s]580it [00:01, 720.56it/s]1295it [00:01, 1360.25it/s]1051it [00:01, 1275.23it/s]2605it [00:02, 1741.74it/s]799it [00:01, 1008.85it/s]1479it [00:01, 1447.70it/s]1243it [00:01, 1371.87it/s]2887it [00:02, 1994.50it/s]975it [00:01, 1154.33it/s]1450it [00:01, 1526.72it/s]1659it [00:01, 1409.45it/s]3218it [00:02, 2324.19it/s]1176it [00:01, 1355.35it/s]1883it [00:01, 1597.42it/s]1637it [00:01, 1527.83it/s]3510it [00:02, 2270.56it/s]1447it [00:01, 1675.61it/s]2063it [00:01, 1636.26it/s]1830it [00:01, 1520.72it/s]1694it [00:01, 1883.44it/s]3756it [00:02, 2124.69it/s]2274it [00:01, 1730.59it/s]1914it [00:01, 1882.38it/s]4020it [00:02, 2254.01it/s]1999it [00:01, 1389.37it/s]2582it [00:02, 2099.66it/s]2167it [00:01, 2054.30it/s]4258it [00:02, 2158.17it/s]2926it [00:02, 2463.18it/s]2151it [00:01, 1305.82it/s]2474it [00:01, 2267.12it/s]2372it [00:02, 1525.11it/s]4483it [00:02, 2044.19it/s]3183it [00:02, 1967.70it/s]2639it [00:02, 1819.73it/s]4694it [00:02, 1987.55it/s]2713it [00:02, 1712.96it/s]3403it [00:02, 1968.55it/s]4911it [00:03, 2034.63it/s]2835it [00:02, 1583.52it/s]2918it [00:02, 1768.63it/s]5119it [00:03, 1928.49it/s]3008it [00:02, 1562.92it/s]3616it [00:02, 1665.33it/s]3196it [00:02, 2014.59it/s]5315it [00:03, 1770.15it/s]3302it [00:02, 1912.12it/s]3885it [00:02, 1888.39it/s]3418it [00:02, 1903.92it/s]5522it [00:03, 1847.56it/s]4093it [00:02, 1889.61it/s]3507it [00:02, 1853.81it/s]5768it [00:03, 2013.06it/s]4350it [00:02, 1938.15it/s]3624it [00:02, 1528.50it/s]3702it [00:02, 1664.34it/s]6011it [00:03, 2128.54it/s]3804it [00:02, 1582.50it/s]3942it [00:02, 1848.08it/s]6287it [00:03, 2307.62it/s]4553it [00:03, 1601.61it/s]4017it [00:02, 1712.33it/s]4140it [00:03, 1874.85it/s]6690it [00:03, 2803.65it/s]4818it [00:03, 1841.94it/s]4350it [00:03, 1773.66it/s]4203it [00:03, 1458.48it/s]6975it [00:03, 2521.70it/s]5020it [00:03, 1836.77it/s]4549it [00:03, 1830.30it/s]7261it [00:04, 2611.63it/s]4367it [00:03, 1480.73it/s]5242it [00:03, 1933.50it/s]4779it [00:03, 1957.87it/s]7653it [00:04, 2977.37it/s]4635it [00:03, 1746.23it/s]5446it [00:03, 1935.03it/s]4899it [00:03, 1975.15it/s]5718it [00:03, 2149.58it/s]4980it [00:03, 1745.85it/s]7959it [00:04, 2844.82it/s]5944it [00:03, 2150.31it/s]8250it [00:04, 2854.85it/s]5190it [00:03, 1806.39it/s]5112it [00:03, 1953.85it/s]5411it [00:03, 1914.32it/s]8540it [00:04, 2770.43it/s]5316it [00:03, 1742.67it/s]6164it [00:03, 1841.62it/s]5644it [00:03, 2029.40it/s]5609it [00:03, 2040.55it/s]6359it [00:04, 1707.04it/s]5858it [00:03, 2059.56it/s]8821it [00:04, 2139.32it/s]5977it [00:03, 2476.37it/s]6558it [00:04, 1776.72it/s]9080it [00:04, 2245.79it/s]6068it [00:04, 1942.27it/s]6329it [00:03, 2762.98it/s]6800it [00:04, 1915.21it/s]9336it [00:04, 2323.57it/s]6327it [00:04, 2106.45it/s]6618it [00:04, 2340.96it/s]6541it [00:04, 1978.99it/s]6998it [00:04, 1681.93it/s]6816it [00:04, 2189.71it/s]7245it [00:04, 1880.12it/s]6872it [00:04, 2104.98it/s]7040it [00:04, 2202.19it/s]7512it [00:04, 2070.28it/s]7111it [00:04, 2162.51it/s]7385it [00:04, 2559.85it/s]7728it [00:04, 2028.55it/s]7340it [00:04, 2069.46it/s]7710it [00:04, 2755.61it/s]7938it [00:04, 2033.56it/s]7556it [00:04, 1882.82it/s]7989it [00:04, 2740.29it/s]8193it [00:04, 2173.75it/s]8415it [00:05, 2111.35it/s]7753it [00:04, 1675.89it/s]8266it [00:04, 2411.56it/s]8723it [00:05, 2382.98it/s]7951it [00:04, 1747.69it/s]8516it [00:05, 2320.23it/s]8983it [00:05, 2444.83it/s]8134it [00:04, 1704.06it/s]8793it [00:05, 2440.52it/s]9319it [00:05, 2708.84it/s]9052it [00:05, 2481.65it/s]8310it [00:05, 1640.61it/s]8549it [00:05, 1782.18it/s]9305it [00:05, 1941.31it/s]8805it [00:05, 1987.93it/s]9584it [00:06, 543.63it/s] 9050it [00:05, 2114.49it/s]9953it [00:06, 795.35it/s]9306it [00:05, 2238.40it/s]10300it [00:06, 1068.99it/s]10728it [00:06, 1478.96it/s]11044it [00:06, 1505.85it/s]11313it [00:06, 1546.77it/s]11552it [00:07, 1601.43it/s]11773it [00:07, 1697.38it/s]11991it [00:07, 1551.89it/s]9593it [00:06, 535.32it/s] 9520it [00:06, 527.60it/s] 9534it [00:06, 602.76it/s] 12181it [00:07, 1490.68it/s]9792it [00:06, 709.77it/s]9888it [00:06, 716.45it/s]9741it [00:06, 745.77it/s]12354it [00:07, 1435.28it/s]10091it [00:06, 951.73it/s]10181it [00:07, 932.95it/s]9918it [00:06, 868.13it/s]12514it [00:07, 1443.90it/s]10512it [00:07, 1224.39it/s]10176it [00:06, 1116.71it/s]10314it [00:07, 1114.40it/s]12780it [00:07, 1726.48it/s]10915it [00:07, 1641.98it/s]10535it [00:07, 1271.71it/s]10373it [00:06, 1201.81it/s]12968it [00:07, 1749.94it/s]11268it [00:07, 1972.45it/s]10759it [00:07, 1447.46it/s]10624it [00:07, 1451.96it/s]13242it [00:08, 1990.81it/s]11683it [00:07, 2407.84it/s]11011it [00:07, 1650.57it/s]13531it [00:08, 2233.52it/s]10828it [00:07, 1507.83it/s]12033it [00:07, 2609.53it/s]11236it [00:07, 1779.17it/s]13766it [00:08, 2264.68it/s]11091it [00:07, 1748.50it/s]12449it [00:07, 2977.08it/s]11506it [00:07, 1977.71it/s]11302it [00:07, 1754.59it/s]12820it [00:07, 3128.24it/s]14000it [00:08, 2005.94it/s]11750it [00:07, 2074.47it/s]13212it [00:07, 3336.26it/s]11503it [00:07, 1723.24it/s]11984it [00:07, 1781.10it/s]14211it [00:08, 1640.95it/s]13618it [00:08, 3531.54it/s]11782it [00:07, 1982.28it/s]14392it [00:08, 1664.59it/s]12188it [00:07, 1694.48it/s]13999it [00:08, 3050.79it/s]11997it [00:07, 1617.01it/s]12376it [00:08, 1728.97it/s]14571it [00:08, 1521.03it/s]14335it [00:08, 2857.70it/s]12219it [00:07, 1755.27it/s]12563it [00:08, 1762.06it/s]14751it [00:08, 1574.86it/s]12512it [00:08, 2045.37it/s]14644it [00:08, 2773.14it/s]14917it [00:09, 1560.21it/s]12750it [00:08, 1525.56it/s]12736it [00:08, 1938.53it/s]14937it [00:08, 2699.25it/s]12915it [00:08, 1522.26it/s]15079it [00:09, 1419.50it/s]15218it [00:08, 2584.41it/s]15270it [00:09, 1540.45it/s]13130it [00:08, 1663.43it/s]15484it [00:08, 2599.94it/s]12945it [00:08, 1398.27it/s]15430it [00:09, 1512.99it/s]13304it [00:08, 1416.14it/s]15586it [00:09, 1497.48it/s]15750it [00:08, 2172.55it/s]13115it [00:08, 1341.01it/s]13473it [00:08, 1481.50it/s]15772it [00:09, 1596.40it/s]13304it [00:08, 1457.49it/s]13669it [00:08, 1546.47it/s]15982it [00:09, 1918.10it/s]15940it [00:09, 1603.34it/s]13479it [00:08, 1525.09it/s]13992it [00:09, 1965.79it/s]16187it [00:09, 1856.97it/s]13658it [00:08, 1567.26it/s]16103it [00:09, 1507.79it/s]14328it [00:09, 2339.99it/s]16408it [00:09, 1941.02it/s]13927it [00:08, 1859.06it/s]14608it [00:09, 2466.97it/s]16620it [00:09, 1985.73it/s]16257it [00:10, 1156.80it/s]14126it [00:09, 1759.97it/s]14926it [00:09, 2667.80it/s]16902it [00:09, 2184.57it/s]16386it [00:10, 1143.66it/s]14375it [00:09, 1921.99it/s]15237it [00:09, 2767.18it/s]16723it [00:10, 1659.75it/s]15519it [00:09, 2727.68it/s]17127it [00:09, 1733.08it/s]14576it [00:09, 1736.47it/s]15867it [00:09, 2943.06it/s]16906it [00:10, 1599.28it/s]14779it [00:09, 1810.93it/s]17319it [00:09, 1759.60it/s]16207it [00:09, 3074.57it/s]15012it [00:09, 1949.68it/s]17530it [00:09, 1846.74it/s]17078it [00:10, 1460.01it/s]16618it [00:09, 3378.56it/s]17726it [00:10, 1782.57it/s]17263it [00:10, 1554.87it/s]15214it [00:09, 1825.01it/s]17020it [00:09, 3414.44it/s]17450it [00:10, 1636.03it/s]17912it [00:10, 1652.28it/s]15403it [00:09, 1543.62it/s]17363it [00:10, 3321.02it/s]17646it [00:10, 1701.28it/s]18140it [00:10, 1811.75it/s]15697it [00:09, 1882.15it/s]17768it [00:10, 3527.02it/s]17822it [00:10, 1623.63it/s]18329it [00:10, 1827.37it/s]15953it [00:10, 2055.75it/s]18123it [00:10, 3429.89it/s]18642it [00:10, 2186.05it/s]17989it [00:11, 1550.31it/s]18547it [00:10, 3661.79it/s]16172it [00:10, 1983.97it/s]18206it [00:11, 1716.53it/s]18382it [00:11, 1506.18it/s]16380it [00:10, 1504.32it/s]18573it [00:11, 1568.23it/s]16737it [00:10, 1962.45it/s]18736it [00:11, 1486.25it/s]16966it [00:10, 1927.22it/s]17181it [00:10, 1761.44it/s]17387it [00:10, 1827.34it/s]17584it [00:11, 1529.24it/s]17754it [00:11, 1464.13it/s]17912it [00:11, 1377.80it/s]18088it [00:11, 1452.62it/s]18868it [00:11, 489.31it/s] 18271it [00:11, 1546.02it/s]19129it [00:11, 663.58it/s]18463it [00:11, 1625.57it/s]19423it [00:12, 902.68it/s]18632it [00:11, 1636.51it/s]19646it [00:12, 1068.08it/s]19895it [00:12, 1289.99it/s]20203it [00:12, 1616.40it/s]20456it [00:12, 1724.80it/s]18916it [00:12, 580.86it/s] 20735it [00:12, 1958.20it/s]19180it [00:12, 676.33it/s]21003it [00:12, 2129.64it/s]21275it [00:12, 2261.28it/s]19410it [00:12, 793.93it/s]21554it [00:12, 2399.83it/s]18889it [00:13, 263.64it/s] 19632it [00:12, 888.83it/s]21832it [00:12, 2500.66it/s]19221it [00:13, 451.70it/s]19831it [00:12, 1008.06it/s]19537it [00:13, 670.47it/s]22100it [00:13, 2296.37it/s]20025it [00:13, 1052.85it/s]19766it [00:13, 837.93it/s]22346it [00:13, 2259.98it/s]20198it [00:13, 1157.09it/s]19988it [00:13, 994.10it/s]22583it [00:13, 2247.82it/s]20200it [00:14, 1164.22it/s]22816it [00:13, 2142.86it/s]20370it [00:13, 1142.04it/s]20412it [00:14, 1259.17it/s]23036it [00:13, 1909.36it/s]20524it [00:13, 1113.32it/s]18800it [00:13, 336.15it/s] 23382it [00:13, 2303.02it/s]20663it [00:13, 1134.08it/s]18978it [00:13, 439.85it/s]20803it [00:13, 1170.07it/s]20608it [00:14, 1032.80it/s]19149it [00:13, 559.20it/s]20940it [00:13, 1214.91it/s]20766it [00:14, 1093.15it/s]19451it [00:13, 853.25it/s]23625it [00:13, 1621.89it/s]21121it [00:13, 1363.20it/s]21056it [00:14, 1437.84it/s]19640it [00:13, 983.09it/s]23823it [00:14, 1660.55it/s]21290it [00:13, 1446.72it/s]21294it [00:14, 1639.14it/s]19945it [00:13, 1333.14it/s]24041it [00:14, 1775.45it/s]21459it [00:14, 1512.83it/s]21614it [00:14, 2004.68it/s]24251it [00:14, 1847.81it/s]20164it [00:13, 1451.11it/s]21620it [00:14, 1535.52it/s]21855it [00:14, 2000.95it/s]24650it [00:14, 2320.45it/s]20373it [00:14, 1472.07it/s]21779it [00:14, 1531.65it/s]22130it [00:15, 2155.42it/s]25067it [00:14, 2805.67it/s]20598it [00:14, 1637.26it/s]21939it [00:14, 1535.99it/s]22534it [00:15, 2650.10it/s]25457it [00:14, 3101.99it/s]20920it [00:14, 2013.37it/s]22113it [00:14, 1593.30it/s]22942it [00:15, 3039.88it/s]22275it [00:14, 1522.11it/s]21158it [00:14, 1873.83it/s]25783it [00:14, 2390.00it/s]22511it [00:14, 1759.20it/s]23266it [00:15, 2499.66it/s]26071it [00:14, 2503.67it/s]21372it [00:14, 1755.72it/s]22804it [00:14, 2095.07it/s]23545it [00:15, 2362.07it/s]21700it [00:14, 2120.63it/s]26349it [00:15, 2407.33it/s]23070it [00:14, 2249.25it/s]23802it [00:15, 2333.43it/s]21935it [00:14, 2038.42it/s]26609it [00:15, 2338.24it/s]23349it [00:14, 2382.08it/s]24058it [00:15, 2389.27it/s]22214it [00:14, 2227.97it/s]26953it [00:15, 2609.17it/s]23631it [00:15, 2508.52it/s]24482it [00:15, 2877.73it/s]22452it [00:14, 2145.74it/s]23884it [00:15, 2470.92it/s]24840it [00:16, 3068.54it/s]27228it [00:15, 2236.98it/s]24182it [00:15, 2619.07it/s]25159it [00:16, 2938.47it/s]22677it [00:15, 1767.40it/s]27470it [00:15, 2123.94it/s]24465it [00:15, 2639.28it/s]22907it [00:15, 1854.57it/s]27695it [00:15, 2121.87it/s]25462it [00:16, 2652.25it/s]24730it [00:15, 2640.90it/s]27916it [00:15, 2084.39it/s]24995it [00:15, 2587.87it/s]23106it [00:15, 1718.90it/s]25260it [00:15, 2604.29it/s]28130it [00:15, 2004.26it/s]23359it [00:15, 1915.49it/s]25738it [00:16, 1829.52it/s]25521it [00:15, 2545.61it/s]28368it [00:15, 2073.69it/s]23641it [00:15, 2148.58it/s]26014it [00:16, 2017.48it/s]25864it [00:15, 2802.83it/s]28757it [00:16, 2563.40it/s]23868it [00:15, 2093.28it/s]26351it [00:16, 2320.61it/s]26218it [00:15, 3016.66it/s]29021it [00:16, 2322.01it/s]26747it [00:16, 2716.84it/s]24086it [00:15, 1793.89it/s]26534it [00:16, 3057.89it/s]27124it [00:16, 2986.26it/s]29263it [00:16, 2073.58it/s]26918it [00:16, 3287.78it/s]24278it [00:16, 1740.96it/s]27458it [00:17, 3058.09it/s]27248it [00:16, 3284.11it/s]24461it [00:16, 1641.58it/s]27646it [00:16, 3488.18it/s]27785it [00:17, 2711.53it/s]29481it [00:16, 1675.61it/s]24658it [00:16, 1723.39it/s]28010it [00:16, 3446.59it/s]29666it [00:16, 1662.86it/s]28078it [00:17, 2682.94it/s]24861it [00:16, 1775.01it/s]28408it [00:16, 3600.31it/s]28361it [00:17, 2658.69it/s]25080it [00:16, 1866.34it/s]28792it [00:16, 3669.39it/s]29844it [00:16, 1344.70it/s]28655it [00:17, 2732.53it/s]25287it [00:16, 1922.18it/s]30160it [00:17, 1725.79it/s]29160it [00:16, 3551.64it/s]28937it [00:17, 2706.67it/s]25483it [00:16, 1914.67it/s]30373it [00:17, 1819.05it/s]29550it [00:16, 3642.91it/s]25677it [00:16, 1892.10it/s]29214it [00:17, 2558.93it/s]29916it [00:17, 3532.84it/s]26063it [00:16, 2455.45it/s]29493it [00:17, 2504.04it/s]30292it [00:17, 3595.60it/s]26380it [00:16, 2660.90it/s]26777it [00:17, 3044.39it/s]29747it [00:18, 1584.13it/s]27085it [00:17, 2300.32it/s]29986it [00:18, 1741.77it/s]27345it [00:17, 2305.53it/s]30256it [00:18, 1903.49it/s]27659it [00:17, 2508.94it/s]30529it [00:18, 2095.87it/s]27999it [00:17, 2713.51it/s]28286it [00:17, 2729.07it/s]28570it [00:17, 1894.93it/s]28891it [00:18, 2175.14it/s]29307it [00:18, 2637.63it/s]30653it [00:18, 822.76it/s] 29651it [00:18, 2836.18it/s]31034it [00:18, 1082.20it/s]31428it [00:18, 1397.70it/s]29968it [00:18, 2543.55it/s]31754it [00:18, 1643.71it/s]32136it [00:18, 1996.18it/s]30250it [00:18, 2097.59it/s]32478it [00:18, 2263.38it/s]30490it [00:18, 2040.80it/s]32882it [00:18, 2638.93it/s]33242it [00:19, 2824.51it/s]30600it [00:19, 293.14it/s] 33596it [00:19, 2429.45it/s]30808it [00:19, 384.40it/s]33898it [00:19, 2250.74it/s]31016it [00:19, 495.32it/s]31312it [00:19, 711.79it/s]34166it [00:19, 2110.54it/s]34517it [00:19, 2416.32it/s]31520it [00:19, 832.11it/s]34839it [00:19, 2606.94it/s]31844it [00:20, 1151.87it/s]32076it [00:20, 1327.84it/s]35127it [00:19, 2511.75it/s]35398it [00:20, 2429.11it/s]32305it [00:20, 1371.70it/s]32510it [00:20, 1478.03it/s]35655it [00:20, 2172.21it/s]32768it [00:20, 1713.20it/s]35885it [00:20, 2125.27it/s]32986it [00:20, 1770.43it/s]36200it [00:20, 2381.27it/s]30766it [00:21, 282.54it/s] 36479it [00:20, 2469.60it/s]31177it [00:21, 453.16it/s]36740it [00:20, 2468.27it/s]31435it [00:21, 580.00it/s]33197it [00:20, 1248.26it/s]37052it [00:20, 2647.65it/s]33439it [00:20, 1471.64it/s]37323it [00:20, 2616.72it/s]31681it [00:21, 679.18it/s]33732it [00:21, 1783.45it/s]37639it [00:20, 2771.16it/s]34062it [00:21, 2138.00it/s]37956it [00:21, 2885.18it/s]31892it [00:21, 754.57it/s]34433it [00:21, 2498.81it/s]32074it [00:21, 869.03it/s]38248it [00:21, 2652.18it/s]34717it [00:21, 2319.15it/s]38519it [00:21, 2448.91it/s]32254it [00:22, 898.52it/s]38824it [00:21, 2608.28it/s]39092it [00:21, 2600.30it/s]34975it [00:21, 1657.69it/s]32410it [00:22, 820.33it/s]39381it [00:21, 2679.72it/s]35187it [00:21, 1748.98it/s]32539it [00:22, 868.08it/s]39695it [00:21, 2810.28it/s]35397it [00:21, 1651.39it/s]39980it [00:21, 2687.47it/s]32662it [00:22, 885.16it/s]35709it [00:22, 1975.61it/s]40272it [00:21, 2751.15it/s]32835it [00:22, 1000.70it/s]36071it [00:22, 2371.59it/s]40655it [00:22, 3059.23it/s]30715it [00:21, 265.72it/s] 32967it [00:22, 1066.44it/s]36376it [00:22, 2544.95it/s]40994it [00:22, 3153.03it/s]30875it [00:21, 316.85it/s]33112it [00:22, 1134.44it/s]36678it [00:22, 2670.35it/s]31054it [00:22, 395.82it/s]37035it [00:22, 2918.62it/s]33240it [00:23, 1129.17it/s]31352it [00:22, 578.14it/s]41312it [00:22, 2276.99it/s]37342it [00:22, 2812.16it/s]33364it [00:23, 1040.73it/s]41576it [00:22, 2245.06it/s]37635it [00:22, 2614.77it/s]33507it [00:23, 1134.68it/s]31551it [00:22, 662.03it/s]41839it [00:22, 2336.62it/s]38002it [00:22, 2895.46it/s]33629it [00:23, 1136.39it/s]42166it [00:22, 2573.08it/s]31724it [00:22, 724.61it/s]38302it [00:22, 2886.71it/s]42442it [00:22, 2569.83it/s]33748it [00:23, 988.20it/s] 31876it [00:22, 814.24it/s]38639it [00:23, 3022.06it/s]33857it [00:23, 956.41it/s]32065it [00:22, 971.77it/s]42712it [00:22, 2337.94it/s]38948it [00:23, 2403.77it/s]34015it [00:23, 1095.48it/s]43020it [00:23, 2526.26it/s]32223it [00:22, 1006.21it/s]39279it [00:23, 2624.48it/s]34270it [00:23, 1472.09it/s]32367it [00:22, 1084.46it/s]43284it [00:23, 2188.06it/s]34571it [00:24, 1887.81it/s]39563it [00:23, 2475.50it/s]32594it [00:23, 1337.73it/s]43518it [00:23, 2203.37it/s]34907it [00:24, 2297.84it/s]32866it [00:23, 1660.69it/s]43821it [00:23, 2417.74it/s]35310it [00:24, 2789.40it/s]33078it [00:23, 1774.89it/s]39827it [00:23, 1921.85it/s]44074it [00:23, 2404.58it/s]35640it [00:24, 2933.52it/s]33358it [00:23, 2044.44it/s]44322it [00:23, 2346.19it/s]36056it [00:24, 3288.66it/s]33670it [00:23, 2337.96it/s]36451it [00:24, 3480.92it/s]40047it [00:23, 1473.43it/s]44562it [00:23, 2181.92it/s]33959it [00:23, 2414.70it/s]36804it [00:24, 3487.25it/s]44840it [00:23, 2340.54it/s]40227it [00:24, 1459.47it/s]37200it [00:24, 3624.90it/s]45086it [00:23, 2372.71it/s]34213it [00:23, 1980.88it/s]40460it [00:24, 1635.74it/s]37565it [00:24, 3504.06it/s]45461it [00:24, 2759.95it/s]34461it [00:23, 2094.06it/s]40700it [00:24, 1806.28it/s]37959it [00:24, 3630.50it/s]40903it [00:24, 1850.05it/s]38325it [00:25, 3578.31it/s]34688it [00:24, 1685.20it/s]41105it [00:24, 1862.95it/s]34939it [00:24, 1854.15it/s]41320it [00:24, 1938.28it/s]35160it [00:24, 1921.90it/s]41524it [00:24, 1897.37it/s]38685it [00:25, 2450.21it/s]35429it [00:24, 2117.18it/s]41779it [00:24, 2075.16it/s]38979it [00:25, 2339.55it/s]42013it [00:24, 2096.51it/s]35656it [00:24, 2014.88it/s]39247it [00:25, 2193.59it/s]42320it [00:24, 2369.88it/s]42562it [00:25, 2290.93it/s]39615it [00:25, 2355.49it/s]35869it [00:24, 1493.68it/s]42795it [00:25, 2225.84it/s]36079it [00:24, 1623.88it/s]39869it [00:25, 2264.25it/s]36456it [00:24, 2127.45it/s]40154it [00:25, 2402.75it/s]43021it [00:25, 1941.14it/s]40453it [00:26, 2550.36it/s]43272it [00:25, 2087.64it/s]40719it [00:26, 2570.93it/s]36699it [00:25, 1711.86it/s]43489it [00:25, 1817.90it/s]41018it [00:26, 2684.47it/s]36904it [00:25, 1688.09it/s]43682it [00:25, 1807.48it/s]41343it [00:26, 2843.31it/s]37096it [00:25, 1621.97it/s]41633it [00:26, 2736.85it/s]43870it [00:25, 1595.12it/s]37274it [00:25, 1511.94it/s]41930it [00:26, 2800.19it/s]44038it [00:26, 1615.68it/s]37459it [00:25, 1574.16it/s]42245it [00:26, 2899.51it/s]44206it [00:26, 1607.89it/s]37869it [00:25, 2198.64it/s]42570it [00:26, 2994.56it/s]44373it [00:26, 1624.01it/s]38199it [00:25, 2485.44it/s]42909it [00:26, 3062.40it/s]44539it [00:26, 1576.42it/s]38466it [00:25, 2499.93it/s]43217it [00:26, 2741.10it/s]44740it [00:26, 1694.96it/s]38823it [00:26, 2795.29it/s]44914it [00:26, 1694.40it/s]43499it [00:27, 2316.39it/s]45086it [00:26, 1661.31it/s]39114it [00:26, 2360.74it/s]43746it [00:27, 2270.38it/s]45272it [00:26, 1717.54it/s]39370it [00:26, 2364.30it/s]44019it [00:27, 2371.94it/s]45445it [00:26, 1675.52it/s]39631it [00:26, 2428.22it/s]44265it [00:27, 2246.48it/s]45638it [00:26, 1748.27it/s]39885it [00:26, 2035.13it/s]44496it [00:27, 2149.80it/s]40135it [00:26, 2147.57it/s]44803it [00:27, 2389.36it/s]40425it [00:26, 2340.25it/s]40701it [00:26, 2418.46it/s]45049it [00:27, 1816.83it/s]40958it [00:27, 2418.90it/s]45789it [00:27, 290.82it/s] 45255it [00:28, 1664.05it/s]41242it [00:27, 2536.04it/s]46042it [00:27, 378.28it/s]41519it [00:27, 2496.83it/s]45439it [00:28, 1545.49it/s]46267it [00:27, 477.88it/s]41783it [00:27, 2511.68it/s]45606it [00:28, 1464.99it/s]46483it [00:27, 590.71it/s]42038it [00:27, 2338.94it/s]46691it [00:27, 708.23it/s]45761it [00:28, 1251.60it/s]42296it [00:27, 2367.26it/s]46886it [00:27, 831.86it/s]47144it [00:27, 1067.00it/s]42536it [00:27, 1960.82it/s]47460it [00:28, 1394.86it/s]42868it [00:27, 2295.90it/s]43204it [00:27, 2572.87it/s]47695it [00:28, 1564.05it/s]43614it [00:28, 2986.25it/s]47953it [00:28, 1777.07it/s]44007it [00:28, 3248.46it/s]48281it [00:28, 2122.17it/s]48569it [00:28, 2309.11it/s]44345it [00:28, 3248.20it/s]48872it [00:28, 2497.17it/s]44744it [00:28, 3460.77it/s]45098it [00:28, 3423.82it/s]49154it [00:28, 2141.94it/s]45491it [00:28, 3568.40it/s]49400it [00:28, 2069.06it/s]49769it [00:28, 2464.43it/s]50123it [00:29, 2742.24it/s]50528it [00:29, 3093.58it/s]45814it [00:29, 223.37it/s] 50875it [00:29, 3197.85it/s]45967it [00:29, 288.50it/s]51209it [00:29, 3151.69it/s]46162it [00:29, 398.07it/s]46517it [00:29, 671.88it/s]46727it [00:29, 828.32it/s]51534it [00:29, 2252.18it/s]47029it [00:29, 1126.52it/s]51802it [00:29, 1992.18it/s]47266it [00:30, 1250.81it/s]52205it [00:29, 2424.50it/s]52488it [00:30, 2490.49it/s]47484it [00:30, 1202.37it/s]47742it [00:30, 1447.12it/s]52767it [00:30, 2006.32it/s]47978it [00:30, 1631.51it/s]53138it [00:30, 2374.10it/s]48238it [00:30, 1840.70it/s]53487it [00:30, 2637.76it/s]48465it [00:30, 1821.98it/s]53785it [00:30, 2598.95it/s]48762it [00:30, 2074.82it/s]54069it [00:30, 2604.86it/s]49014it [00:30, 2173.22it/s]45895it [00:31, 175.49it/s] 54346it [00:30, 2109.64it/s]46123it [00:31, 261.87it/s]49251it [00:31, 1760.78it/s]54649it [00:30, 2321.77it/s]46384it [00:31, 391.93it/s]49514it [00:31, 1961.01it/s]55028it [00:31, 2652.27it/s]46590it [00:31, 511.76it/s]49745it [00:31, 2033.05it/s]55450it [00:31, 3056.10it/s]49967it [00:31, 1980.39it/s]46769it [00:32, 581.92it/s]55778it [00:31, 2979.31it/s]46994it [00:32, 765.26it/s]50178it [00:31, 1721.10it/s]56092it [00:31, 2563.44it/s]47169it [00:32, 891.81it/s]50522it [00:31, 2135.58it/s]56368it [00:31, 2477.78it/s]50867it [00:31, 2472.92it/s]47341it [00:32, 948.16it/s]56641it [00:31, 2539.18it/s]51274it [00:31, 2900.45it/s]47515it [00:32, 1087.06it/s]51668it [00:31, 2942.20it/s]47821it [00:32, 1482.78it/s]56906it [00:31, 2148.29it/s]45852it [00:31, 365.68it/s] 48133it [00:32, 1846.54it/s]51975it [00:32, 2784.75it/s]57139it [00:31, 2142.40it/s]46109it [00:31, 449.32it/s]48452it [00:32, 2170.41it/s]52263it [00:32, 2463.24it/s]46345it [00:31, 551.26it/s]57365it [00:32, 1799.54it/s]48713it [00:32, 2025.60it/s]46615it [00:32, 705.13it/s]57560it [00:32, 1645.98it/s]52521it [00:32, 1917.65it/s]46857it [00:32, 863.44it/s]57775it [00:32, 1731.50it/s]52767it [00:32, 2033.59it/s]47251it [00:32, 1224.08it/s]48948it [00:33, 1493.45it/s]58170it [00:32, 2266.47it/s]52992it [00:32, 2055.11it/s]47593it [00:32, 1538.47it/s]49150it [00:33, 1592.44it/s]58489it [00:32, 2502.63it/s]48002it [00:32, 1972.28it/s]53348it [00:32, 2315.09it/s]49398it [00:33, 1787.56it/s]58897it [00:32, 2923.47it/s]53747it [00:32, 2737.42it/s]49699it [00:33, 2080.05it/s]48337it [00:32, 1940.25it/s]59228it [00:32, 2850.38it/s]50060it [00:33, 2464.75it/s]54038it [00:33, 2436.13it/s]48628it [00:32, 2111.51it/s]50334it [00:33, 2397.62it/s]59526it [00:32, 2422.74it/s]54298it [00:33, 2354.74it/s]48915it [00:32, 2074.55it/s]50611it [00:33, 2480.03it/s]54545it [00:33, 2330.18it/s]59787it [00:33, 2200.04it/s]50961it [00:33, 2756.91it/s]54786it [00:33, 2290.07it/s]49176it [00:33, 1783.10it/s]60022it [00:33, 2015.86it/s]51250it [00:34, 2591.07it/s]49431it [00:33, 1909.37it/s]60235it [00:33, 1926.49it/s]51520it [00:34, 2493.04it/s]55020it [00:33, 1778.22it/s]49719it [00:33, 2124.24it/s]60595it [00:33, 2257.55it/s]51777it [00:34, 2427.44it/s]49980it [00:33, 2223.61it/s]55218it [00:33, 1573.79it/s]52025it [00:34, 2330.18it/s]60831it [00:33, 2086.58it/s]50227it [00:33, 2139.52it/s]55400it [00:33, 1603.13it/s]52262it [00:34, 2085.16it/s]61048it [00:33, 1889.77it/s]50461it [00:33, 2189.57it/s]55609it [00:33, 1695.13it/s]52498it [00:34, 2145.45it/s]50693it [00:33, 2152.12it/s]61244it [00:33, 1797.59it/s]55789it [00:34, 1545.19it/s]50918it [00:33, 2146.39it/s]61428it [00:34, 1718.40it/s]52718it [00:34, 1864.41it/s]55952it [00:34, 1544.07it/s]52933it [00:34, 1934.61it/s]56338it [00:34, 2107.66it/s]61603it [00:34, 1601.64it/s]51139it [00:33, 1817.05it/s]53181it [00:35, 1941.80it/s]56579it [00:34, 2175.58it/s]51397it [00:34, 1987.37it/s]61765it [00:34, 1331.65it/s]53381it [00:35, 1954.77it/s]51668it [00:34, 2123.60it/s]56806it [00:34, 2034.08it/s]61905it [00:34, 1271.39it/s]53581it [00:35, 1894.38it/s]57136it [00:34, 2367.85it/s]51890it [00:34, 1938.71it/s]62106it [00:34, 1431.48it/s]57504it [00:34, 2726.87it/s]53774it [00:35, 1655.92it/s]52093it [00:34, 1680.65it/s]62256it [00:34, 1342.04it/s]57787it [00:34, 2625.36it/s]53946it [00:35, 1559.25it/s]52272it [00:34, 1684.06it/s]62398it [00:34, 1360.52it/s]54107it [00:35, 1507.14it/s]52448it [00:34, 1663.89it/s]58058it [00:35, 1926.17it/s]62588it [00:34, 1327.06it/s]54440it [00:35, 1962.58it/s]52784it [00:34, 2103.27it/s]62830it [00:35, 1596.27it/s]54805it [00:35, 2411.24it/s]58282it [00:35, 1774.34it/s]53040it [00:34, 2166.96it/s]62998it [00:35, 1603.14it/s]55086it [00:35, 2521.13it/s]53288it [00:35, 2252.27it/s]58482it [00:35, 1715.19it/s]63411it [00:35, 2285.64it/s]55348it [00:36, 2292.76it/s]58701it [00:35, 1804.85it/s]53520it [00:35, 2121.62it/s]63651it [00:35, 2233.36it/s]55588it [00:36, 2273.55it/s]58984it [00:35, 2054.72it/s]63896it [00:35, 2293.07it/s]53738it [00:35, 1796.56it/s]55848it [00:36, 2361.30it/s]59277it [00:35, 2281.18it/s]64268it [00:35, 2658.83it/s]53939it [00:35, 1769.74it/s]59547it [00:35, 2392.83it/s]56090it [00:36, 1979.35it/s]54292it [00:35, 2212.20it/s]56374it [00:36, 2190.55it/s]59798it [00:36, 1918.59it/s]54644it [00:35, 2516.11it/s]56620it [00:36, 2260.51it/s]60111it [00:36, 2192.91it/s]54908it [00:35, 2297.25it/s]60517it [00:36, 2664.08it/s]56857it [00:36, 2089.85it/s]55149it [00:35, 2241.47it/s]60908it [00:36, 2899.64it/s]57076it [00:36, 1940.84it/s]55397it [00:36, 2282.16it/s]61317it [00:36, 3217.98it/s]57338it [00:37, 2109.50it/s]55631it [00:36, 2139.28it/s]61655it [00:36, 3168.59it/s]57558it [00:37, 2062.88it/s]55850it [00:36, 1714.83it/s]57770it [00:37, 1788.24it/s]61983it [00:36, 2191.04it/s]57958it [00:37, 1688.97it/s]56037it [00:36, 1502.89it/s]62250it [00:36, 2159.45it/s]58207it [00:37, 1839.74it/s]56288it [00:36, 1675.48it/s]62523it [00:37, 2286.36it/s]56480it [00:36, 1732.07it/s]58398it [00:37, 1644.17it/s]62779it [00:37, 2283.17it/s]56790it [00:36, 2075.35it/s]58668it [00:37, 1877.32it/s]57196it [00:36, 2598.38it/s]63027it [00:37, 1988.46it/s]58874it [00:37, 1917.09it/s]57548it [00:37, 2794.46it/s]63296it [00:37, 2152.52it/s]59089it [00:38, 1977.34it/s]57889it [00:37, 2964.83it/s]59293it [00:38, 1956.21it/s]58260it [00:37, 3175.52it/s]63529it [00:37, 1765.42it/s]59524it [00:38, 1995.08it/s]63809it [00:37, 1996.08it/s]59727it [00:38, 1944.70it/s]58586it [00:37, 2448.25it/s]64032it [00:37, 2033.51it/s]59994it [00:38, 2145.30it/s]58862it [00:37, 2475.21it/s]64266it [00:37, 2111.28it/s]59132it [00:37, 2424.11it/s]60212it [00:38, 1788.93it/s]60496it [00:38, 2038.52it/s]59390it [00:37, 2203.38it/s]60712it [00:38, 2001.43it/s]59624it [00:37, 2180.82it/s]59851it [00:38, 2131.12it/s]60921it [00:39, 1635.49it/s]60070it [00:38, 2016.90it/s]61110it [00:39, 1694.21it/s]61314it [00:39, 1780.65it/s]60321it [00:38, 2080.93it/s]64539it [00:38, 291.67it/s] 61515it [00:39, 1840.18it/s]60533it [00:38, 2001.54it/s]64763it [00:38, 374.59it/s]61750it [00:39, 1902.10it/s]60741it [00:38, 1957.60it/s]64988it [00:38, 480.40it/s]62038it [00:39, 2167.79it/s]61032it [00:38, 2212.65it/s]65193it [00:38, 597.25it/s]62446it [00:39, 2703.84it/s]61444it [00:38, 2740.60it/s]65477it [00:38, 814.08it/s]62800it [00:39, 2935.29it/s]61748it [00:38, 2592.39it/s]65704it [00:39, 976.44it/s]63101it [00:39, 2769.90it/s]62015it [00:38, 2281.04it/s]65923it [00:39, 1138.14it/s]63385it [00:40, 2617.70it/s]62254it [00:39, 2161.62it/s]66137it [00:39, 1180.79it/s]63653it [00:40, 2248.86it/s]62478it [00:39, 2063.92it/s]66416it [00:39, 1470.97it/s]63890it [00:40, 2132.20it/s]66763it [00:39, 1859.92it/s]62690it [00:39, 1896.73it/s]64270it [00:40, 2505.37it/s]67085it [00:39, 2165.05it/s]62885it [00:39, 1897.78it/s]67499it [00:39, 2643.11it/s]63127it [00:39, 2033.41it/s]67853it [00:39, 2873.18it/s]63335it [00:39, 1862.91it/s]68195it [00:39, 3018.26it/s]63527it [00:39, 1857.88it/s]68526it [00:40, 3014.81it/s]63717it [00:39, 1815.83it/s]68848it [00:40, 2729.94it/s]63901it [00:40, 1777.81it/s]69140it [00:40, 2715.76it/s]64160it [00:40, 1977.87it/s]69425it [00:40, 2604.78it/s]69695it [00:40, 2431.64it/s]70010it [00:40, 2617.09it/s]70332it [00:40, 2779.46it/s]70618it [00:40, 2637.88it/s]70888it [00:40, 2624.43it/s]71155it [00:41, 2430.11it/s]71426it [00:41, 2498.51it/s]71681it [00:41, 2466.48it/s]71931it [00:41, 2059.41it/s]72150it [00:41, 1795.63it/s]72343it [00:41, 1706.86it/s]72531it [00:41, 1738.72it/s]72712it [00:42, 1700.14it/s]64491it [00:42, 175.65it/s] 64809it [00:42, 266.49it/s]72887it [00:42, 1342.38it/s]65160it [00:42, 400.78it/s]73128it [00:42, 1581.47it/s]65410it [00:42, 502.37it/s]73441it [00:42, 1957.38it/s]65633it [00:42, 622.42it/s]73658it [00:42, 1815.77it/s]65851it [00:42, 747.64it/s]73856it [00:42, 1713.64it/s]66055it [00:42, 886.85it/s]74039it [00:42, 1740.48it/s]66398it [00:43, 1240.59it/s]74222it [00:42, 1715.86it/s]66725it [00:43, 1561.57it/s]74400it [00:43, 1531.01it/s]66988it [00:43, 1710.66it/s]74593it [00:43, 1630.40it/s]67260it [00:43, 1920.44it/s]74948it [00:43, 2136.74it/s]67664it [00:43, 2406.26it/s]64532it [00:44, 242.43it/s] 68003it [00:43, 2647.62it/s]75174it [00:43, 2036.51it/s]68316it [00:43, 2724.29it/s]64718it [00:44, 291.11it/s]75387it [00:43, 1840.16it/s]65017it [00:44, 414.34it/s]75650it [00:43, 2040.37it/s]68623it [00:43, 2175.21it/s]65351it [00:44, 593.44it/s]75885it [00:43, 2122.59it/s]65770it [00:44, 882.74it/s]76105it [00:43, 2008.83it/s]68882it [00:44, 1943.99it/s]66107it [00:44, 1138.84it/s]76374it [00:43, 2191.93it/s]69123it [00:44, 2045.31it/s]76636it [00:44, 2310.88it/s]66415it [00:44, 1339.70it/s]69363it [00:44, 2127.27it/s]76916it [00:44, 2424.97it/s]64360it [00:44, 172.37it/s] 69596it [00:44, 1871.11it/s]66703it [00:45, 1413.02it/s]77163it [00:44, 2437.69it/s]64606it [00:44, 249.10it/s]69822it [00:44, 1962.48it/s]77566it [00:44, 2899.77it/s]64821it [00:44, 336.03it/s]66954it [00:45, 1365.37it/s]70034it [00:44, 1942.48it/s]65131it [00:44, 505.53it/s]67202it [00:45, 1549.63it/s]77860it [00:44, 2203.38it/s]70239it [00:44, 1826.21it/s]65457it [00:44, 727.00it/s]67455it [00:45, 1729.64it/s]70430it [00:44, 1789.40it/s]65872it [00:44, 1079.74it/s]78108it [00:44, 2080.10it/s]67700it [00:45, 1826.21it/s]70645it [00:44, 1883.05it/s]66174it [00:44, 1320.52it/s]78336it [00:44, 2125.77it/s]68050it [00:45, 2204.33it/s]70883it [00:45, 2017.24it/s]78634it [00:44, 2343.28it/s]66472it [00:44, 1533.19it/s]68312it [00:45, 2267.75it/s]71090it [00:45, 2014.25it/s]66783it [00:44, 1811.19it/s]78940it [00:45, 2455.20it/s]71386it [00:45, 2280.84it/s]67114it [00:44, 2112.82it/s]68569it [00:45, 1960.71it/s]79196it [00:45, 2232.67it/s]71764it [00:45, 2710.97it/s]79505it [00:45, 2452.57it/s]67418it [00:45, 1875.93it/s]68793it [00:46, 1697.51it/s]72040it [00:45, 2446.95it/s]72315it [00:45, 2520.17it/s]68987it [00:46, 1655.53it/s]79761it [00:45, 1990.02it/s]67697it [00:45, 1908.58it/s]69170it [00:46, 1694.73it/s]67989it [00:45, 2124.03it/s]79981it [00:45, 1971.42it/s]72574it [00:45, 1972.94it/s]68365it [00:45, 2476.44it/s]80250it [00:45, 2040.38it/s]69380it [00:46, 1604.54it/s]69626it [00:46, 1797.75it/s]72793it [00:46, 1673.41it/s]68652it [00:45, 2205.26it/s]80465it [00:45, 1850.71it/s]69882it [00:46, 1989.66it/s]72982it [00:46, 1689.28it/s]80669it [00:45, 1853.28it/s]68904it [00:45, 2081.22it/s]70093it [00:46, 1897.55it/s]69134it [00:45, 2102.12it/s]80861it [00:46, 1713.59it/s]73166it [00:46, 1446.92it/s]70291it [00:46, 1793.51it/s]81137it [00:46, 1822.65it/s]69360it [00:46, 1946.28it/s]73326it [00:46, 1426.74it/s]70531it [00:47, 1891.70it/s]69594it [00:46, 2041.34it/s]81411it [00:46, 2026.83it/s]73569it [00:46, 1654.55it/s]70726it [00:47, 1809.50it/s]81675it [00:46, 2185.07it/s]69829it [00:46, 2093.41it/s]73764it [00:46, 1726.63it/s]71060it [00:47, 2147.27it/s]81977it [00:46, 2394.61it/s]74035it [00:46, 1983.91it/s]70047it [00:46, 1971.42it/s]71498it [00:47, 2743.60it/s]82401it [00:46, 2903.41it/s]74261it [00:46, 2002.21it/s]70251it [00:46, 1824.46it/s]71862it [00:47, 2990.59it/s]82817it [00:46, 3108.81it/s]70465it [00:46, 1895.69it/s]72171it [00:47, 2931.77it/s]74470it [00:47, 1630.19it/s]83212it [00:46, 3342.67it/s]70660it [00:46, 1900.59it/s]72518it [00:47, 3082.42it/s]74649it [00:47, 1495.19it/s]70856it [00:46, 1914.41it/s]72832it [00:47, 3059.60it/s]83552it [00:47, 2787.47it/s]74820it [00:47, 1531.46it/s]73158it [00:47, 3116.75it/s]71056it [00:46, 1815.23it/s]74983it [00:47, 1499.66it/s]83850it [00:47, 2283.77it/s]71241it [00:47, 1524.41it/s]73473it [00:48, 2470.15it/s]75140it [00:47, 1483.61it/s]71535it [00:47, 1872.19it/s]84104it [00:47, 1994.37it/s]73802it [00:48, 2672.87it/s]75418it [00:47, 1824.03it/s]71766it [00:47, 1960.70it/s]74220it [00:48, 3066.07it/s]75801it [00:47, 2370.57it/s]84325it [00:47, 1757.24it/s]71973it [00:47, 1812.45it/s]76049it [00:47, 2167.13it/s]74547it [00:48, 2742.80it/s]72164it [00:47, 1715.01it/s]74841it [00:48, 2638.72it/s]84518it [00:47, 1395.17it/s]76277it [00:48, 1798.74it/s]72342it [00:47, 1652.80it/s]75139it [00:48, 2701.20it/s]84712it [00:47, 1484.81it/s]76555it [00:48, 2008.39it/s]72523it [00:47, 1650.97it/s]84879it [00:47, 1519.91it/s]75420it [00:48, 2469.56it/s]76805it [00:48, 2129.85it/s]72736it [00:47, 1695.95it/s]75705it [00:48, 2525.96it/s]85046it [00:48, 1455.55it/s]77032it [00:48, 1992.12it/s]72908it [00:48, 1561.00it/s]85333it [00:48, 1767.61it/s]75966it [00:49, 2155.73it/s]77242it [00:48, 1885.96it/s]73084it [00:48, 1591.65it/s]85524it [00:48, 1634.04it/s]76195it [00:49, 2140.00it/s]77439it [00:48, 1775.44it/s]73246it [00:48, 1597.18it/s]76465it [00:49, 2258.16it/s]85698it [00:48, 1505.45it/s]77689it [00:48, 1931.65it/s]73408it [00:48, 1513.24it/s]76734it [00:49, 2371.73it/s]85906it [00:48, 1643.58it/s]73719it [00:48, 1944.71it/s]77889it [00:48, 1678.38it/s]86210it [00:48, 1971.15it/s]76979it [00:49, 2222.96it/s]74144it [00:48, 2586.62it/s]78159it [00:48, 1927.85it/s]86493it [00:48, 2196.85it/s]77219it [00:49, 2245.76it/s]74500it [00:48, 2863.23it/s]78498it [00:49, 2306.12it/s]86723it [00:48, 2218.45it/s]77497it [00:49, 2390.14it/s]74868it [00:48, 3097.74it/s]78743it [00:49, 2240.32it/s]87017it [00:49, 2397.07it/s]77741it [00:49, 2334.82it/s]75257it [00:48, 3212.29it/s]87437it [00:49, 2909.01it/s]75643it [00:48, 3396.85it/s]78978it [00:49, 1877.82it/s]77978it [00:50, 1855.09it/s]87735it [00:49, 2809.13it/s]79312it [00:49, 2230.09it/s]75987it [00:49, 3129.54it/s]78181it [00:50, 1815.77it/s]79564it [00:49, 2302.90it/s]76307it [00:49, 3023.94it/s]78533it [00:50, 2236.98it/s]79882it [00:49, 2536.58it/s]76614it [00:49, 3031.08it/s]78791it [00:50, 2326.24it/s]80202it [00:49, 2718.58it/s]76937it [00:49, 2945.73it/s]79037it [00:50, 2131.44it/s]80485it [00:49, 2622.65it/s]77264it [00:49, 3034.90it/s]80796it [00:49, 2757.17it/s]79262it [00:50, 2057.36it/s]77571it [00:49, 3038.76it/s]81120it [00:50, 2894.31it/s]79610it [00:50, 2427.44it/s]77877it [00:49, 2946.91it/s]79879it [00:50, 2496.21it/s]81415it [00:50, 2663.18it/s]78174it [00:49, 2769.42it/s]80138it [00:50, 2509.77it/s]81769it [00:50, 2903.51it/s]80396it [00:51, 2490.77it/s]78454it [00:50, 2392.09it/s]82083it [00:50, 2969.77it/s]80650it [00:51, 2437.54it/s]82422it [00:50, 3089.26it/s]78703it [00:50, 2317.23it/s]81071it [00:51, 2940.34it/s]82736it [00:50, 2973.00it/s]78942it [00:50, 2213.39it/s]83060it [00:50, 3047.15it/s]79168it [00:50, 1799.29it/s]81370it [00:51, 1995.37it/s]83368it [00:50, 2494.77it/s]79438it [00:50, 2006.13it/s]81695it [00:51, 2268.46it/s]83636it [00:51, 2459.04it/s]79701it [00:50, 2120.89it/s]81963it [00:51, 2067.45it/s]79926it [00:50, 2124.95it/s]83895it [00:51, 2271.21it/s]80148it [00:50, 2146.31it/s]84174it [00:51, 2400.96it/s]82201it [00:51, 1745.65it/s]84448it [00:51, 2489.37it/s]80370it [00:51, 1985.92it/s]84705it [00:51, 2377.66it/s]82404it [00:52, 1634.82it/s]80640it [00:51, 2161.50it/s]84949it [00:51, 2385.15it/s]82679it [00:52, 1865.54it/s]80886it [00:51, 2223.01it/s]85192it [00:51, 2334.24it/s]81137it [00:51, 2153.41it/s]82888it [00:52, 1796.23it/s]81379it [00:51, 2201.52it/s]85429it [00:51, 2150.13it/s]83127it [00:52, 1918.20it/s]81687it [00:51, 2443.87it/s]83449it [00:52, 2242.51it/s]85649it [00:51, 2154.97it/s]83799it [00:52, 2571.00it/s]81936it [00:51, 2426.73it/s]85868it [00:52, 2043.05it/s]84072it [00:52, 2501.91it/s]86075it [00:52, 2042.76it/s]82182it [00:51, 2252.80it/s]84485it [00:52, 2941.75it/s]82412it [00:51, 2243.72it/s]86282it [00:52, 1706.39it/s]84791it [00:52, 2891.12it/s]82812it [00:51, 2733.28it/s]86463it [00:52, 1652.41it/s]85118it [00:53, 2996.21it/s]83164it [00:52, 2955.40it/s]86635it [00:52, 1578.09it/s]83586it [00:52, 3283.48it/s]86800it [00:52, 1585.67it/s]85425it [00:53, 2278.05it/s]83919it [00:52, 2815.21it/s]85695it [00:53, 2376.95it/s]87016it [00:52, 1622.32it/s]87210it [00:52, 1689.02it/s]85957it [00:53, 2240.49it/s]84215it [00:52, 2173.11it/s]87382it [00:53, 1590.12it/s]86198it [00:53, 2162.92it/s]84536it [00:52, 2401.71it/s]87568it [00:53, 1661.24it/s]86468it [00:53, 2160.14it/s]84806it [00:52, 2397.28it/s]87737it [00:53, 1657.83it/s]86740it [00:53, 2272.79it/s]85066it [00:52, 2050.74it/s]87905it [00:53, 1607.58it/s]86999it [00:53, 2354.65it/s]85292it [00:53, 2008.66it/s]87241it [00:54, 2278.43it/s]87474it [00:54, 2247.61it/s]85507it [00:53, 1691.12it/s]87702it [00:54, 2124.44it/s]85846it [00:53, 2065.56it/s]86177it [00:53, 2077.94it/s]87918it [00:54, 1656.73it/s]86402it [00:53, 1850.42it/s]86601it [00:53, 1730.70it/s]86868it [00:53, 1944.16it/s]87076it [00:54, 1833.59it/s]87337it [00:54, 1993.21it/s]87573it [00:54, 2065.20it/s]88022it [00:54, 182.22it/s] 87857it [00:54, 2121.66it/s]88225it [00:54, 227.22it/s]88412it [00:54, 280.87it/s]88632it [00:54, 370.63it/s]88832it [00:54, 472.64it/s]89067it [00:55, 626.76it/s]89299it [00:55, 804.32it/s]89538it [00:55, 1004.48it/s]89786it [00:55, 1234.68it/s]90171it [00:55, 1711.43it/s]90482it [00:55, 2000.16it/s]90767it [00:55, 2104.69it/s]91039it [00:55, 2154.49it/s]91298it [00:55, 2242.03it/s]91699it [00:55, 2697.31it/s]92057it [00:56, 2931.03it/s]92374it [00:56, 2148.73it/s]92637it [00:56, 2254.06it/s]92899it [00:56, 2241.23it/s]93149it [00:56, 2171.86it/s]93384it [00:56, 2022.25it/s]93600it [00:56, 2020.21it/s]93811it [00:57, 1698.15it/s]88067it [00:57, 136.94it/s] 94041it [00:57, 1837.13it/s]88329it [00:57, 218.22it/s]88489it [00:57, 278.97it/s]94238it [00:57, 1668.13it/s]88644it [00:57, 354.59it/s]94456it [00:57, 1778.85it/s]88798it [00:57, 443.34it/s]94644it [00:57, 1745.70it/s]88947it [00:57, 546.24it/s]94826it [00:57, 1663.59it/s]89185it [00:57, 755.27it/s]95079it [00:57, 1887.55it/s]89605it [00:58, 1239.01it/s]95275it [00:57, 1750.40it/s]90001it [00:58, 1705.48it/s]95457it [00:58, 1763.10it/s]90277it [00:58, 1909.58it/s]95638it [00:58, 1586.64it/s]90552it [00:58, 1820.92it/s]95850it [00:58, 1723.15it/s]90794it [00:58, 1827.08it/s]96029it [00:58, 1594.07it/s]96198it [00:58, 1562.98it/s]91019it [00:58, 1772.08it/s]88101it [00:59, 142.57it/s] 96373it [00:58, 1597.08it/s]91274it [00:58, 1948.45it/s]88264it [00:59, 182.51it/s]96633it [00:58, 1868.85it/s]91495it [00:58, 1917.53it/s]88547it [00:59, 280.11it/s]96885it [00:58, 2046.47it/s]91743it [00:59, 2057.53it/s]88767it [00:59, 370.80it/s]97095it [00:58, 2027.64it/s]91965it [00:59, 2037.29it/s]88942it [00:59, 452.16it/s]97301it [00:59, 1931.99it/s]92180it [00:59, 1920.46it/s]89123it [00:59, 566.84it/s]97627it [00:59, 2300.86it/s]92419it [00:59, 2043.12it/s]89291it [01:00, 682.91it/s]97862it [00:59, 1981.35it/s]92631it [00:59, 1823.45it/s]89495it [01:00, 858.71it/s]89684it [01:00, 1014.95it/s]92823it [00:59, 1770.27it/s]98071it [00:59, 1517.31it/s]89957it [01:00, 1329.81it/s]93006it [00:59, 1725.14it/s]98267it [00:59, 1609.87it/s]93292it [00:59, 2005.26it/s]90163it [01:00, 1295.71it/s]98568it [00:59, 1940.59it/s]93641it [00:59, 2406.99it/s]90345it [01:00, 1369.57it/s]98847it [00:59, 2094.86it/s]93890it [01:00, 2298.66it/s]90520it [01:00, 1418.46it/s]99093it [00:59, 2164.42it/s]88075it [00:59, 142.64it/s] 90779it [01:00, 1696.01it/s]94127it [01:00, 2037.38it/s]99337it [01:00, 2210.90it/s]88326it [00:59, 200.24it/s]94542it [01:00, 2577.75it/s]90976it [01:00, 1738.50it/s]99586it [01:00, 2264.20it/s]88700it [01:00, 319.27it/s]91234it [01:01, 1929.66it/s]99820it [01:00, 2268.19it/s]94816it [01:00, 1984.29it/s]100204it [01:00, 2713.88it/s]91443it [01:01, 1707.37it/s]88939it [01:00, 399.42it/s]100482it [01:00, 2453.86it/s]95045it [01:00, 1859.18it/s]89296it [01:00, 585.71it/s]91629it [01:01, 1654.50it/s]89559it [01:00, 741.64it/s]91805it [01:01, 1620.95it/s]100736it [01:00, 2237.86it/s]95252it [01:00, 1706.51it/s]91975it [01:01, 1610.16it/s]95486it [01:00, 1800.88it/s]100969it [01:00, 1996.50it/s]89806it [01:00, 863.12it/s]95714it [01:01, 1914.22it/s]90026it [01:00, 1013.09it/s]101227it [01:00, 2113.00it/s]92141it [01:01, 1371.84it/s]90281it [01:00, 1233.75it/s]95918it [01:01, 1785.87it/s]101447it [01:01, 2015.44it/s]92287it [01:01, 1246.42it/s]90567it [01:00, 1510.20it/s]96202it [01:01, 2048.42it/s]92489it [01:01, 1428.18it/s]90880it [01:01, 1825.78it/s]101655it [01:01, 1658.40it/s]96419it [01:01, 2011.42it/s]92676it [01:02, 1532.62it/s]96823it [01:01, 2523.16it/s]101834it [01:01, 1601.41it/s]91145it [01:01, 1840.28it/s]92865it [01:02, 1626.80it/s]97103it [01:01, 2597.60it/s]102051it [01:01, 1736.80it/s]91387it [01:01, 1727.88it/s]102234it [01:01, 1752.69it/s]93035it [01:02, 1326.50it/s]97371it [01:01, 2286.20it/s]91610it [01:01, 1832.32it/s]102487it [01:01, 1958.35it/s]93278it [01:02, 1589.99it/s]97612it [01:01, 2226.94it/s]91874it [01:01, 2023.31it/s]102746it [01:01, 2103.94it/s]93579it [01:02, 1951.42it/s]92126it [01:01, 2065.92it/s]97843it [01:01, 2071.39it/s]102972it [01:01, 2145.80it/s]93792it [01:02, 1866.71it/s]92366it [01:01, 2151.71it/s]98057it [01:02, 1899.11it/s]93992it [01:02, 1874.61it/s]103192it [01:01, 1844.72it/s]92597it [01:01, 2158.39it/s]98303it [01:02, 2039.22it/s]94222it [01:02, 1989.06it/s]103447it [01:02, 2026.29it/s]92850it [01:01, 2226.94it/s]98522it [01:02, 2062.86it/s]94462it [01:02, 2059.50it/s]103660it [01:02, 1952.52it/s]98789it [01:02, 2226.27it/s]93081it [01:02, 1892.19it/s]103887it [01:02, 1876.79it/s]99017it [01:02, 2084.80it/s]94674it [01:03, 1635.43it/s]93284it [01:02, 1758.46it/s]104109it [01:02, 1953.28it/s]94879it [01:03, 1720.32it/s]99265it [01:02, 2156.50it/s]93584it [01:02, 2064.31it/s]104336it [01:02, 2011.06it/s]95147it [01:03, 1963.38it/s]99500it [01:02, 2208.67it/s]93862it [01:02, 2249.67it/s]104556it [01:02, 2062.75it/s]99725it [01:02, 2212.43it/s]95488it [01:03, 2305.40it/s]94216it [01:02, 2599.16it/s]95824it [01:03, 2590.83it/s]104766it [01:02, 1807.08it/s]94568it [01:02, 2855.73it/s]99949it [01:03, 1808.32it/s]105178it [01:02, 2408.69it/s]94865it [01:02, 2840.23it/s]96095it [01:03, 2279.44it/s]100143it [01:03, 1754.19it/s]105503it [01:02, 2635.40it/s]95204it [01:02, 2996.92it/s]96338it [01:03, 2057.16it/s]100328it [01:03, 1709.18it/s]95510it [01:02, 2950.89it/s]100526it [01:03, 1776.92it/s]95872it [01:03, 3142.62it/s]96557it [01:04, 1877.02it/s]105779it [01:03, 1739.40it/s]100768it [01:03, 1922.24it/s]106075it [01:03, 1990.52it/s]96190it [01:03, 2690.41it/s]96755it [01:04, 1625.54it/s]100965it [01:03, 1911.73it/s]106423it [01:03, 2326.97it/s]101271it [01:03, 2232.15it/s]96929it [01:04, 1597.65it/s]106852it [01:03, 2806.45it/s]96473it [01:03, 2150.27it/s]101576it [01:03, 2464.55it/s]97168it [01:04, 1766.67it/s]96873it [01:03, 2570.91it/s]107247it [01:03, 3023.69it/s]101987it [01:03, 2940.90it/s]97410it [01:04, 1898.46it/s]97215it [01:03, 2777.99it/s]107649it [01:03, 3287.78it/s]97627it [01:04, 1967.03it/s]102286it [01:04, 2550.06it/s]97609it [01:03, 3078.89it/s]108070it [01:03, 3538.94it/s]102554it [01:04, 2534.48it/s]97831it [01:04, 1774.21it/s]97943it [01:03, 3147.39it/s]108444it [01:04, 3140.74it/s]98016it [01:04, 1764.92it/s]102816it [01:04, 2294.17it/s]98275it [01:03, 2781.38it/s]108780it [01:04, 2886.11it/s]98228it [01:04, 1829.87it/s]103055it [01:04, 1948.28it/s]98438it [01:05, 1902.08it/s]98572it [01:04, 2469.57it/s]103302it [01:04, 2067.77it/s]98749it [01:05, 2239.78it/s]109086it [01:04, 2222.24it/s]98837it [01:04, 2303.80it/s]103563it [01:04, 2172.62it/s]98978it [01:05, 2167.19it/s]99112it [01:04, 2410.87it/s]109341it [01:04, 2039.50it/s]99220it [01:05, 2238.23it/s]103886it [01:04, 2268.18it/s]99365it [01:04, 2357.78it/s]109615it [01:04, 2189.13it/s]104120it [01:04, 2169.86it/s]99447it [01:05, 1981.73it/s]99616it [01:04, 2359.41it/s]109856it [01:04, 2147.18it/s]104382it [01:05, 2284.72it/s]99688it [01:05, 2064.89it/s]99882it [01:04, 2439.78it/s]110086it [01:04, 2089.09it/s]104616it [01:05, 2297.23it/s]99958it [01:05, 2236.50it/s]100134it [01:04, 2461.63it/s]110305it [01:05, 1971.59it/s]100188it [01:05, 2181.26it/s]100384it [01:04, 2268.98it/s]104850it [01:05, 1940.89it/s]110549it [01:05, 2087.66it/s]100509it [01:05, 2462.90it/s]100618it [01:05, 2286.93it/s]105107it [01:05, 2097.42it/s]110765it [01:05, 1862.38it/s]100971it [01:05, 2632.57it/s]100761it [01:06, 2209.91it/s]105408it [01:05, 2337.40it/s]111073it [01:05, 2165.14it/s]101240it [01:05, 2596.97it/s]100990it [01:06, 2098.89it/s]105653it [01:05, 2169.85it/s]111307it [01:05, 2180.28it/s]101233it [01:06, 2185.88it/s]101504it [01:05, 2172.56it/s]105880it [01:05, 1984.55it/s]101457it [01:06, 2119.44it/s]101903it [01:05, 2631.30it/s]106087it [01:05, 1832.16it/s]111534it [01:05, 1606.54it/s]101844it [01:06, 2597.71it/s]102252it [01:05, 2857.90it/s]111722it [01:05, 1597.09it/s]106278it [01:06, 1606.60it/s]102112it [01:06, 2365.88it/s]111925it [01:05, 1696.29it/s]102554it [01:05, 2249.30it/s]102358it [01:06, 2207.15it/s]106447it [01:06, 1438.46it/s]112119it [01:06, 1756.24it/s]102943it [01:05, 2631.54it/s]106661it [01:06, 1599.27it/s]112307it [01:06, 1678.08it/s]102586it [01:06, 1915.11it/s]103238it [01:06, 2405.18it/s]106930it [01:06, 1867.32it/s]112537it [01:06, 1831.78it/s]102788it [01:07, 1720.41it/s]103504it [01:06, 2396.25it/s]107178it [01:06, 2026.83it/s]112812it [01:06, 2077.84it/s]103799it [01:06, 2534.16it/s]102969it [01:07, 1679.13it/s]107392it [01:06, 2002.63it/s]113120it [01:06, 2355.58it/s]107632it [01:06, 2111.33it/s]103143it [01:07, 1614.90it/s]113364it [01:06, 2184.84it/s]104068it [01:06, 2158.58it/s]107880it [01:06, 2214.36it/s]103418it [01:07, 1869.37it/s]113591it [01:06, 2163.71it/s]108107it [01:06, 2152.10it/s]103706it [01:07, 2131.10it/s]104303it [01:06, 1868.42it/s]113874it [01:06, 2346.48it/s]104057it [01:07, 2503.11it/s]104567it [01:06, 2040.98it/s]108327it [01:07, 1922.24it/s]104887it [01:06, 2322.66it/s]104353it [01:07, 2521.00it/s]108526it [01:07, 1828.81it/s]114115it [01:07, 1749.94it/s]105139it [01:06, 2218.61it/s]104612it [01:07, 2377.59it/s]114320it [01:07, 1811.32it/s]108714it [01:07, 1679.91it/s]104856it [01:08, 2289.42it/s]114559it [01:07, 1946.14it/s]105375it [01:07, 1989.26it/s]108926it [01:07, 1662.89it/s]114807it [01:07, 2015.59it/s]105090it [01:08, 2016.02it/s]109156it [01:07, 1808.26it/s]105587it [01:07, 1663.57it/s]115047it [01:07, 2116.52it/s]105301it [01:08, 2024.30it/s]109425it [01:07, 1979.63it/s]115453it [01:07, 2648.23it/s]105769it [01:07, 1656.69it/s]105509it [01:08, 1811.29it/s]115767it [01:07, 2753.20it/s]106014it [01:07, 1844.42it/s]109628it [01:07, 1711.07it/s]105729it [01:08, 1908.22it/s]106311it [01:07, 2128.12it/s]116058it [01:07, 2760.32it/s]109808it [01:07, 1697.14it/s]105975it [01:08, 2052.99it/s]106652it [01:07, 2458.41it/s]110104it [01:08, 2020.09it/s]116341it [01:07, 2659.80it/s]106265it [01:08, 2283.27it/s]110528it [01:08, 2615.12it/s]106912it [01:07, 2298.15it/s]116612it [01:08, 2368.17it/s]106501it [01:08, 2251.75it/s]110803it [01:08, 2521.09it/s]107153it [01:07, 2148.01it/s]116858it [01:08, 2283.27it/s]106874it [01:08, 2641.24it/s]107377it [01:08, 2119.37it/s]117122it [01:08, 2376.32it/s]107143it [01:09, 2458.73it/s]111065it [01:08, 2102.76it/s]107595it [01:08, 2095.68it/s]107395it [01:09, 2373.54it/s]111293it [01:08, 1661.10it/s]107809it [01:08, 1531.14it/s]107637it [01:09, 1893.46it/s]111484it [01:08, 1486.48it/s]107844it [01:09, 1862.49it/s]107986it [01:08, 1513.44it/s]111778it [01:08, 1789.02it/s]108088it [01:09, 1981.25it/s]108178it [01:08, 1585.64it/s]112163it [01:09, 2261.21it/s]108340it [01:09, 2106.38it/s]108463it [01:08, 1894.88it/s]112496it [01:09, 2520.91it/s]108869it [01:08, 2456.24it/s]108574it [01:09, 2153.86it/s]112805it [01:09, 2631.10it/s]109217it [01:08, 2732.39it/s]108797it [01:09, 1850.40it/s]109508it [01:09, 2744.17it/s]113090it [01:09, 2273.38it/s]108994it [01:10, 1864.14it/s]109191it [01:10, 1890.83it/s]109795it [01:09, 2138.69it/s]113340it [01:09, 1917.19it/s]109387it [01:10, 1793.92it/s]110081it [01:09, 2278.82it/s]113626it [01:09, 2126.63it/s]109673it [01:10, 2053.15it/s]110368it [01:09, 2422.93it/s]113869it [01:09, 2199.60it/s]114107it [01:09, 2167.16it/s]110630it [01:09, 2339.28it/s]109884it [01:10, 1702.05it/s]110878it [01:09, 2353.56it/s]110099it [01:10, 1810.03it/s]114337it [01:10, 1938.64it/s]111123it [01:09, 2230.41it/s]110292it [01:10, 1804.88it/s]114596it [01:10, 2097.34it/s]110565it [01:10, 2049.26it/s]114818it [01:10, 2061.64it/s]111354it [01:09, 1853.55it/s]110779it [01:10, 2068.16it/s]115219it [01:10, 2577.42it/s]111067it [01:11, 2170.85it/s]115530it [01:10, 2670.70it/s]111554it [01:10, 1558.88it/s]111352it [01:11, 2317.69it/s]115807it [01:10, 2596.05it/s]111796it [01:10, 1737.96it/s]116129it [01:10, 2768.13it/s]111587it [01:11, 2161.64it/s]112071it [01:10, 1977.61it/s]111919it [01:11, 2474.34it/s]116412it [01:10, 2648.26it/s]112423it [01:10, 2367.71it/s]112812it [01:10, 2770.79it/s]112288it [01:11, 2777.65it/s]116682it [01:11, 2241.43it/s]112637it [01:11, 2975.82it/s]113109it [01:10, 2708.12it/s]116990it [01:11, 2448.47it/s]117248it [01:11, 2328.97it/s]112940it [01:11, 2327.00it/s]113394it [01:10, 2146.92it/s]113198it [01:11, 2222.18it/s]113636it [01:11, 1937.99it/s]113438it [01:12, 2040.64it/s]113851it [01:11, 1942.11it/s]113656it [01:12, 2065.07it/s]114060it [01:11, 1976.50it/s]113889it [01:12, 2120.77it/s]114346it [01:11, 2200.97it/s]114112it [01:12, 2148.64it/s]114583it [01:11, 2213.59it/s]114474it [01:12, 2551.58it/s]114737it [01:12, 2420.04it/s]114813it [01:11, 1660.52it/s]115005it [01:11, 1521.48it/s]114986it [01:12, 2006.38it/s]115176it [01:11, 1556.78it/s]115202it [01:12, 1834.43it/s]115434it [01:12, 1782.34it/s]115438it [01:13, 1950.88it/s]115634it [01:12, 1811.99it/s]115645it [01:13, 1970.42it/s]115826it [01:12, 1704.86it/s]115851it [01:13, 1850.25it/s]116005it [01:12, 1703.13it/s]116043it [01:13, 1719.15it/s]116244it [01:12, 1832.39it/s]116221it [01:13, 1587.79it/s]116486it [01:12, 1970.09it/s]116481it [01:13, 1840.15it/s]116894it [01:12, 2552.51it/s]116681it [01:13, 1839.30it/s]117158it [01:12, 2330.19it/s]116871it [01:13, 1793.37it/s]117293it [01:14, 2448.58it/s]117397it [01:14, 139.15it/s] 117569it [01:14, 173.02it/s]117740it [01:14, 218.68it/s]118145it [01:14, 374.70it/s]118386it [01:14, 464.13it/s]118645it [01:15, 612.12it/s]118868it [01:15, 717.30it/s]119066it [01:15, 851.10it/s]119261it [01:15, 980.26it/s]119449it [01:15, 1101.04it/s]119631it [01:15, 1227.39it/s]119812it [01:15, 1297.96it/s]119985it [01:15, 1175.97it/s]120134it [01:16, 1194.32it/s]120438it [01:16, 1595.09it/s]120726it [01:16, 1904.53it/s]120944it [01:16, 1912.09it/s]121155it [01:16, 1853.78it/s]121354it [01:16, 1705.38it/s]121596it [01:16, 1755.32it/s]121876it [01:16, 2000.64it/s]122134it [01:17, 2142.89it/s]122371it [01:17, 2203.98it/s]122598it [01:17, 2096.62it/s]122855it [01:17, 2226.06it/s]123179it [01:17, 2509.05it/s]123436it [01:17, 2343.43it/s]123676it [01:17, 2261.10it/s]123907it [01:17, 2078.26it/s]124120it [01:17, 2068.11it/s]124374it [01:18, 2149.68it/s]124592it [01:18, 1904.54it/s]124789it [01:18, 1727.57it/s]124988it [01:18, 1789.08it/s]125277it [01:18, 2076.93it/s]117491it [01:18, 114.90it/s] 125493it [01:18, 1999.78it/s]117737it [01:18, 156.74it/s]125699it [01:18, 1990.91it/s]117938it [01:19, 202.80it/s]125903it [01:18, 1721.32it/s]118134it [01:19, 258.69it/s]126169it [01:19, 1943.69it/s]118311it [01:19, 328.00it/s]126432it [01:19, 2103.60it/s]118487it [01:19, 411.29it/s]126651it [01:19, 2062.88it/s]118704it [01:19, 550.02it/s]127055it [01:19, 2602.59it/s]118884it [01:19, 666.18it/s]127325it [01:19, 2495.16it/s]119075it [01:19, 819.61it/s]127582it [01:19, 2237.04it/s]119314it [01:19, 1056.25it/s]127835it [01:19, 2312.43it/s]119545it [01:19, 1278.93it/s]128074it [01:19, 2192.53it/s]119751it [01:20, 1380.13it/s]128300it [01:19, 1991.35it/s]119946it [01:20, 1260.15it/s]128506it [01:20, 1693.49it/s]120114it [01:20, 1315.71it/s]120339it [01:20, 1524.05it/s]128705it [01:20, 1720.67it/s]120601it [01:20, 1788.97it/s]128885it [01:20, 1579.98it/s]117400it [01:20, 114.72it/s] 120807it [01:20, 1826.24it/s]129110it [01:20, 1742.30it/s]117658it [01:20, 159.68it/s]121009it [01:20, 1745.49it/s]129311it [01:20, 1785.12it/s]117917it [01:20, 221.50it/s]121263it [01:20, 1943.57it/s]129502it [01:20, 1817.21it/s]118199it [01:20, 312.61it/s]121536it [01:20, 2151.32it/s]129694it [01:20, 1838.92it/s]118442it [01:20, 413.01it/s]121762it [01:21, 2145.16it/s]129882it [01:20, 1763.18it/s]118703it [01:20, 552.98it/s]122174it [01:21, 2698.87it/s]130084it [01:21, 1832.82it/s]118992it [01:20, 747.81it/s]122516it [01:21, 2903.21it/s]130416it [01:21, 2230.73it/s]119248it [01:21, 891.93it/s]122840it [01:21, 2999.92it/s]130659it [01:21, 2244.03it/s]119478it [01:21, 1004.00it/s]130886it [01:21, 2100.51it/s]123146it [01:21, 2339.88it/s]119809it [01:21, 1337.21it/s]131099it [01:21, 1949.01it/s]123406it [01:21, 1993.70it/s]120053it [01:21, 1442.10it/s]131298it [01:21, 1790.77it/s]123686it [01:21, 2126.55it/s]120279it [01:21, 1502.15it/s]131551it [01:21, 1981.50it/s]120541it [01:21, 1727.04it/s]123920it [01:22, 1982.35it/s]131755it [01:21, 1802.97it/s]117548it [01:22, 99.89it/s]  120883it [01:21, 2104.83it/s]131942it [01:21, 1786.92it/s]117816it [01:22, 140.29it/s]124134it [01:22, 1807.75it/s]121206it [01:21, 2376.98it/s]132125it [01:22, 1729.87it/s]118053it [01:22, 189.34it/s]124374it [01:22, 1944.88it/s]121485it [01:21, 2178.69it/s]124769it [01:22, 2444.23it/s]132399it [01:22, 1986.46it/s]118260it [01:22, 246.01it/s]125118it [01:22, 2691.28it/s]118502it [01:23, 336.47it/s]132603it [01:22, 1859.00it/s]121735it [01:22, 2009.81it/s]118764it [01:23, 464.27it/s]132794it [01:22, 1744.51it/s]121960it [01:22, 1869.53it/s]125403it [01:22, 2126.67it/s]119076it [01:23, 648.03it/s]133048it [01:22, 1952.94it/s]122164it [01:22, 1687.15it/s]119311it [01:23, 799.40it/s]125644it [01:22, 1975.72it/s]133249it [01:22, 1639.08it/s]122435it [01:22, 1862.55it/s]125862it [01:22, 1718.70it/s]119537it [01:23, 873.14it/s]122855it [01:22, 2420.90it/s]133425it [01:22, 1495.86it/s]126085it [01:23, 1829.60it/s]119784it [01:23, 1081.50it/s]123237it [01:22, 2768.51it/s]133584it [01:22, 1497.82it/s]126347it [01:23, 2015.77it/s]120040it [01:23, 1314.06it/s]123536it [01:22, 2685.17it/s]133741it [01:23, 1438.89it/s]120463it [01:23, 1848.91it/s]123946it [01:22, 3060.48it/s]126565it [01:23, 1822.53it/s]120803it [01:24, 2171.07it/s]133890it [01:23, 1279.54it/s]124289it [01:23, 3161.64it/s]121211it [01:24, 2606.93it/s]134120it [01:23, 1526.77it/s]126761it [01:23, 1559.43it/s]124628it [01:23, 3224.55it/s]121597it [01:24, 2821.49it/s]126934it [01:23, 1560.78it/s]134282it [01:23, 1418.68it/s]124960it [01:23, 2997.53it/s]122017it [01:24, 3167.68it/s]127101it [01:23, 1556.36it/s]134514it [01:23, 1645.14it/s]125269it [01:23, 2867.67it/s]122409it [01:24, 3367.16it/s]127377it [01:23, 1857.96it/s]134769it [01:23, 1882.90it/s]125563it [01:23, 2661.91it/s]122779it [01:24, 3398.00it/s]127721it [01:23, 2274.17it/s]134967it [01:23, 1818.26it/s]125836it [01:23, 2489.34it/s]123142it [01:24, 3154.69it/s]127962it [01:24, 2164.01it/s]135186it [01:23, 1917.38it/s]126104it [01:23, 2537.30it/s]135426it [01:24, 1969.75it/s]128189it [01:24, 2062.91it/s]123477it [01:24, 2670.34it/s]126363it [01:23, 2286.65it/s]135627it [01:24, 1866.96it/s]123769it [01:24, 2676.50it/s]128403it [01:24, 1825.89it/s]126635it [01:24, 2205.36it/s]128674it [01:24, 2045.75it/s]135818it [01:24, 1562.15it/s]126998it [01:24, 2564.37it/s]129014it [01:24, 2399.50it/s]124054it [01:25, 2119.56it/s]136035it [01:24, 1710.16it/s]127399it [01:24, 2946.35it/s]129344it [01:24, 2643.92it/s]124294it [01:25, 2129.84it/s]136217it [01:24, 1728.67it/s]127751it [01:24, 3103.24it/s]129744it [01:24, 3023.25it/s]124527it [01:25, 2126.45it/s]136398it [01:24, 1621.04it/s]128150it [01:24, 3351.95it/s]130057it [01:24, 2715.93it/s]124754it [01:25, 2003.97it/s]136585it [01:24, 1668.77it/s]128495it [01:24, 3377.96it/s]128839it [01:24, 3272.27it/s]130342it [01:25, 2149.46it/s]136757it [01:24, 1372.72it/s]124965it [01:25, 1628.20it/s]130597it [01:25, 2240.36it/s]129172it [01:24, 2776.72it/s]136967it [01:25, 1545.83it/s]125144it [01:25, 1631.26it/s]137317it [01:25, 2042.92it/s]125429it [01:25, 1916.77it/s]130842it [01:25, 2122.14it/s]129466it [01:24, 2526.91it/s]137556it [01:25, 2076.29it/s]125637it [01:26, 1938.76it/s]131069it [01:25, 1846.10it/s]129733it [01:25, 2097.44it/s]137776it [01:25, 1896.22it/s]125843it [01:26, 1681.07it/s]131285it [01:25, 1918.44it/s]129985it [01:25, 2191.90it/s]138126it [01:25, 2308.39it/s]126130it [01:26, 1968.33it/s]131502it [01:25, 1966.47it/s]138490it [01:25, 2666.06it/s]130221it [01:25, 2043.33it/s]126381it [01:26, 2106.64it/s]131709it [01:25, 1885.76it/s]130494it [01:25, 2208.08it/s]138771it [01:25, 2501.55it/s]126711it [01:26, 2425.62it/s]131955it [01:25, 2033.87it/s]127127it [01:26, 2900.38it/s]130835it [01:25, 2394.48it/s]139033it [01:25, 2392.31it/s]132221it [01:26, 2203.30it/s]139281it [01:25, 2391.53it/s]127430it [01:26, 2765.28it/s]132448it [01:26, 2129.88it/s]131084it [01:25, 2035.99it/s]139526it [01:26, 2364.75it/s]127717it [01:26, 2488.99it/s]131302it [01:25, 1999.01it/s]132666it [01:26, 1744.71it/s]139798it [01:26, 2461.58it/s]131540it [01:26, 2083.50it/s]127978it [01:26, 2300.21it/s]132869it [01:26, 1781.29it/s]131757it [01:26, 2069.80it/s]140048it [01:26, 1963.91it/s]133114it [01:26, 1950.71it/s]132018it [01:26, 2212.58it/s]128218it [01:27, 1823.83it/s]133448it [01:26, 2320.21it/s]140262it [01:26, 1702.49it/s]132293it [01:26, 2328.15it/s]133798it [01:26, 2643.87it/s]128421it [01:27, 1655.76it/s]140469it [01:26, 1785.57it/s]132531it [01:26, 2241.56it/s]134074it [01:26, 2608.53it/s]128604it [01:27, 1628.28it/s]140712it [01:26, 1943.59it/s]132792it [01:26, 2314.66it/s]134343it [01:26, 2381.99it/s]128879it [01:27, 1887.04it/s]140965it [01:26, 2095.32it/s]133051it [01:26, 2390.98it/s]134659it [01:27, 2588.54it/s]141226it [01:26, 2229.38it/s]133331it [01:26, 2507.20it/s]129157it [01:27, 1912.88it/s]135035it [01:27, 2844.92it/s]141611it [01:26, 2679.93it/s]129484it [01:27, 2242.43it/s]133584it [01:26, 2309.82it/s]135442it [01:27, 3183.25it/s]141906it [01:27, 2755.31it/s]129893it [01:27, 2718.55it/s]133877it [01:26, 2480.45it/s]135768it [01:27, 2901.94it/s]130183it [01:28, 2678.32it/s]142190it [01:27, 2414.78it/s]134195it [01:27, 2638.57it/s]142592it [01:27, 2837.27it/s]134594it [01:27, 3022.99it/s]136068it [01:27, 2557.04it/s]130464it [01:28, 2188.38it/s]134987it [01:27, 3282.96it/s]142891it [01:27, 2658.60it/s]136337it [01:27, 2563.83it/s]130705it [01:28, 2062.89it/s]135320it [01:27, 2924.21it/s]143169it [01:27, 2174.14it/s]136603it [01:27, 2069.85it/s]130927it [01:28, 1980.03it/s]135623it [01:27, 2590.67it/s]136947it [01:27, 2384.88it/s]131158it [01:28, 2054.17it/s]143408it [01:27, 2085.18it/s]137345it [01:28, 2779.20it/s]131384it [01:28, 2105.49it/s]135895it [01:27, 2317.71it/s]143631it [01:27, 1892.42it/s]137659it [01:28, 2872.23it/s]131602it [01:28, 2036.68it/s]136139it [01:27, 2127.82it/s]137965it [01:28, 2754.59it/s]143832it [01:28, 1685.86it/s]131902it [01:28, 2294.28it/s]136399it [01:27, 2240.00it/s]138254it [01:28, 2784.12it/s]132269it [01:29, 2674.47it/s]144011it [01:28, 1663.38it/s]136654it [01:28, 2318.57it/s]132592it [01:29, 2830.46it/s]144214it [01:28, 1700.87it/s]138543it [01:28, 1962.80it/s]132882it [01:29, 2473.62it/s]144389it [01:28, 1556.19it/s]136894it [01:28, 1692.65it/s]138904it [01:28, 2317.69it/s]144567it [01:28, 1581.94it/s]137281it [01:28, 2162.48it/s]133142it [01:29, 2129.37it/s]139258it [01:28, 2605.51it/s]144877it [01:28, 1968.94it/s]137547it [01:28, 2278.38it/s]139636it [01:28, 2899.59it/s]145100it [01:28, 2034.29it/s]137805it [01:28, 1975.99it/s]133371it [01:29, 1518.13it/s]145311it [01:28, 1724.56it/s]139958it [01:29, 2285.09it/s]133556it [01:29, 1493.46it/s]145553it [01:29, 1897.08it/s]138030it [01:28, 1726.29it/s]140228it [01:29, 2246.62it/s]145815it [01:29, 2041.81it/s]133728it [01:29, 1481.13it/s]138225it [01:29, 1667.77it/s]140481it [01:29, 2069.29it/s]133987it [01:30, 1702.85it/s]146029it [01:29, 2022.91it/s]138553it [01:29, 2032.95it/s]146302it [01:29, 2214.90it/s]134196it [01:30, 1785.40it/s]138845it [01:29, 2250.12it/s]140709it [01:29, 1861.38it/s]134434it [01:30, 1924.23it/s]146531it [01:29, 2184.01it/s]141047it [01:29, 2200.25it/s]139091it [01:29, 2075.87it/s]134639it [01:30, 1944.21it/s]146781it [01:29, 2271.47it/s]141429it [01:29, 2591.90it/s]139315it [01:29, 2006.95it/s]134843it [01:30, 1855.66it/s]141723it [01:29, 2679.95it/s]147012it [01:29, 1861.11it/s]139527it [01:29, 1910.35it/s]135036it [01:30, 1860.73it/s]142011it [01:30, 2462.63it/s]139795it [01:29, 2104.41it/s]147213it [01:29, 1790.47it/s]135227it [01:30, 1787.45it/s]147422it [01:29, 1832.42it/s]140015it [01:29, 1946.74it/s]142274it [01:30, 2193.31it/s]135414it [01:30, 1798.35it/s]147613it [01:30, 1773.59it/s]140218it [01:29, 1855.39it/s]135597it [01:30, 1743.86it/s]142509it [01:30, 2127.00it/s]147914it [01:30, 2098.99it/s]140418it [01:30, 1891.16it/s]135877it [01:31, 1844.85it/s]142732it [01:30, 1933.43it/s]148263it [01:30, 2477.79it/s]140626it [01:30, 1913.10it/s]143108it [01:30, 2374.66it/s]136161it [01:31, 2046.69it/s]148542it [01:30, 2564.47it/s]140855it [01:30, 2016.05it/s]143435it [01:30, 2588.78it/s]136367it [01:31, 1983.32it/s]148806it [01:30, 2446.89it/s]141116it [01:30, 2182.08it/s]143827it [01:30, 2943.67it/s]136566it [01:31, 1888.40it/s]149143it [01:30, 2702.39it/s]141338it [01:30, 2136.84it/s]144231it [01:30, 3239.30it/s]149420it [01:30, 2679.27it/s]136756it [01:31, 1560.60it/s]141555it [01:30, 1833.31it/s]144568it [01:31, 2907.52it/s]149828it [01:30, 3077.32it/s]137036it [01:31, 1853.57it/s]141747it [01:30, 1797.31it/s]150154it [01:30, 3129.53it/s]144874it [01:31, 2843.52it/s]137293it [01:31, 2036.17it/s]141949it [01:30, 1854.09it/s]142142it [01:30, 1847.41it/s]137509it [01:31, 1888.94it/s]145169it [01:31, 2213.10it/s]150471it [01:31, 2244.88it/s]142331it [01:31, 1633.45it/s]150733it [01:31, 2253.88it/s]137708it [01:32, 1654.73it/s]145417it [01:31, 2026.58it/s]142536it [01:31, 1739.74it/s]137971it [01:32, 1889.64it/s]145685it [01:31, 2172.23it/s]150985it [01:31, 2021.64it/s]138205it [01:32, 2003.80it/s]142716it [01:31, 1619.94it/s]145921it [01:31, 2075.23it/s]151209it [01:31, 1959.57it/s]142884it [01:31, 1631.35it/s]138417it [01:32, 1793.33it/s]151467it [01:31, 2105.85it/s]146142it [01:31, 1948.02it/s]143297it [01:31, 2309.31it/s]151714it [01:31, 2197.36it/s]138608it [01:32, 1670.89it/s]143578it [01:31, 2448.60it/s]146346it [01:32, 1792.38it/s]138823it [01:32, 1789.38it/s]143839it [01:31, 2494.56it/s]146597it [01:32, 1965.18it/s]151945it [01:31, 1941.89it/s]139011it [01:32, 1692.57it/s]152190it [01:32, 2067.47it/s]144095it [01:31, 2276.09it/s]146803it [01:32, 1728.12it/s]139240it [01:32, 1826.08it/s]152424it [01:32, 2098.77it/s]146986it [01:32, 1651.83it/s]139502it [01:33, 2038.57it/s]152642it [01:32, 2091.36it/s]144331it [01:32, 1804.44it/s]139768it [01:33, 2210.01it/s]147158it [01:32, 1582.45it/s]152862it [01:32, 2121.14it/s]144737it [01:32, 2331.22it/s]140053it [01:33, 2390.85it/s]147456it [01:32, 1915.62it/s]153079it [01:32, 2096.70it/s]145115it [01:32, 2640.79it/s]147681it [01:32, 2002.32it/s]153380it [01:32, 2354.26it/s]140298it [01:33, 2120.36it/s]145403it [01:32, 2462.07it/s]147906it [01:32, 2068.21it/s]153619it [01:32, 2345.70it/s]140567it [01:33, 2269.41it/s]145668it [01:32, 2316.52it/s]153864it [01:32, 2375.49it/s]148119it [01:32, 1891.48it/s]140803it [01:33, 2024.01it/s]145934it [01:32, 2400.66it/s]148315it [01:33, 1717.98it/s]146288it [01:32, 2699.34it/s]154104it [01:32, 1862.77it/s]141016it [01:33, 1584.29it/s]146696it [01:32, 3075.06it/s]154309it [01:33, 1794.87it/s]148494it [01:33, 1397.39it/s]147036it [01:32, 3164.32it/s]141198it [01:33, 1612.80it/s]148647it [01:33, 1366.54it/s]141458it [01:34, 1846.15it/s]147362it [01:33, 2816.64it/s]148880it [01:33, 1593.97it/s]141660it [01:34, 1837.11it/s]147657it [01:33, 2563.61it/s]149120it [01:33, 1797.08it/s]141877it [01:34, 1923.25it/s]149335it [01:33, 1889.48it/s]142132it [01:34, 2026.05it/s]147926it [01:33, 2164.37it/s]149555it [01:33, 1973.94it/s]142342it [01:34, 1972.77it/s]148188it [01:33, 2269.55it/s]149835it [01:33, 2187.26it/s]142544it [01:34, 1849.04it/s]148430it [01:33, 2249.95it/s]150176it [01:34, 2534.73it/s]142750it [01:34, 1905.06it/s]150436it [01:34, 1962.28it/s]142945it [01:34, 1761.34it/s]148666it [01:33, 1731.03it/s]150785it [01:34, 2327.01it/s]143157it [01:34, 1832.49it/s]148876it [01:33, 1779.34it/s]151103it [01:34, 2544.27it/s]143395it [01:35, 1980.55it/s]149126it [01:34, 1948.15it/s]151447it [01:34, 2783.08it/s]149338it [01:34, 1927.07it/s]143597it [01:35, 1706.56it/s]151829it [01:34, 3068.94it/s]149543it [01:34, 1762.82it/s]143777it [01:35, 1585.06it/s]152151it [01:34, 3042.45it/s]149748it [01:34, 1833.85it/s]152492it [01:34, 3145.77it/s]143943it [01:35, 1464.57it/s]150020it [01:34, 2058.44it/s]144204it [01:35, 1729.00it/s]150235it [01:34, 1807.29it/s]144386it [01:35, 1726.73it/s]152815it [01:35, 2174.75it/s]150427it [01:34, 1796.49it/s]144565it [01:35, 1635.16it/s]153079it [01:35, 2128.98it/s]150651it [01:34, 1911.16it/s]144734it [01:35, 1616.83it/s]153369it [01:35, 2303.18it/s]150860it [01:35, 1874.12it/s]144951it [01:35, 1764.31it/s]153627it [01:35, 2132.10it/s]145132it [01:36, 1664.20it/s]151053it [01:35, 1520.58it/s]153871it [01:35, 2176.75it/s]145302it [01:36, 1573.41it/s]151263it [01:35, 1646.03it/s]154229it [01:35, 2530.26it/s]151606it [01:35, 2093.38it/s]145463it [01:36, 1454.07it/s]151835it [01:35, 2112.45it/s]145782it [01:36, 1903.47it/s]152078it [01:35, 2178.96it/s]145991it [01:36, 1953.80it/s]152305it [01:35, 2171.01it/s]146247it [01:36, 2097.74it/s]152535it [01:35, 2205.80it/s]146473it [01:36, 2142.56it/s]152761it [01:35, 2142.34it/s]146692it [01:36, 1955.08it/s]153024it [01:36, 2279.08it/s]146897it [01:37, 1979.81it/s]153305it [01:36, 2426.65it/s]147100it [01:37, 1864.43it/s]153551it [01:36, 2156.47it/s]147291it [01:37, 1815.14it/s]153774it [01:36, 2047.12it/s]147589it [01:37, 2101.08it/s]154002it [01:36, 2108.89it/s]147803it [01:37, 2085.54it/s]154262it [01:36, 2243.61it/s]148014it [01:37, 2089.96it/s]148237it [01:37, 2129.37it/s]148477it [01:37, 1973.39it/s]148749it [01:37, 2174.10it/s]148974it [01:38, 2192.74it/s]149218it [01:38, 2261.49it/s]149447it [01:38, 2244.78it/s]149674it [01:38, 2194.55it/s]149895it [01:38, 1959.81it/s]150157it [01:38, 1930.44it/s]150355it [01:38, 1852.13it/s]150594it [01:38, 1990.29it/s]150797it [01:38, 1917.67it/s]150992it [01:39, 1614.15it/s]151163it [01:39, 1463.75it/s]151317it [01:39, 1466.23it/s]151517it [01:39, 1589.64it/s]151688it [01:39, 1608.18it/s]151854it [01:39, 1527.57it/s]152238it [01:39, 2146.03it/s]152463it [01:39, 2017.96it/s]152677it [01:40, 1783.80it/s]152880it [01:40, 1844.28it/s]153078it [01:40, 1860.40it/s]153340it [01:40, 2065.81it/s]153553it [01:40, 1962.89it/s]153778it [01:40, 1991.57it/s]153981it [01:40, 1848.17it/s]154237it [01:40, 2017.23it/s]154501it [01:40, 90.95it/s]  154677it [01:40, 119.95it/s]154852it [01:41, 159.44it/s]155008it [01:41, 202.65it/s]155264it [01:41, 305.86it/s]155592it [01:41, 483.54it/s]155909it [01:41, 694.61it/s]156186it [01:41, 903.91it/s]156504it [01:41, 1190.46it/s]156806it [01:41, 1469.38it/s]157088it [01:41, 1568.06it/s]157341it [01:42, 1442.26it/s]157605it [01:42, 1660.39it/s]157833it [01:42, 1651.80it/s]158042it [01:42, 1541.70it/s]158318it [01:42, 1800.18it/s]158561it [01:42, 1928.32it/s]158780it [01:42, 1846.67it/s]159009it [01:43, 1915.67it/s]159248it [01:43, 2037.44it/s]159477it [01:43, 2104.30it/s]159733it [01:43, 2195.01it/s]159965it [01:43, 2167.48it/s]160187it [01:43, 1704.47it/s]160376it [01:43, 1693.47it/s]160558it [01:43, 1623.87it/s]160729it [01:43, 1619.88it/s]160899it [01:44, 1627.68it/s]161081it [01:44, 1662.49it/s]161251it [01:44, 1533.54it/s]161509it [01:44, 1768.91it/s]161690it [01:44, 1607.49it/s]161856it [01:44, 1599.58it/s]162113it [01:44, 1858.11it/s]162437it [01:44, 2238.56it/s]162668it [01:45, 2054.38it/s]162881it [01:45, 1771.32it/s]163109it [01:45, 1878.63it/s]163307it [01:45, 1833.87it/s]163497it [01:45, 1624.10it/s]163702it [01:45, 1729.19it/s]164100it [01:45, 2315.62it/s]164505it [01:45, 2682.37it/s]164905it [01:45, 3038.37it/s]165308it [01:46, 3313.42it/s]165650it [01:46, 3342.05it/s]166075it [01:46, 3602.70it/s]154229it [01:46, 2530.26it/s]154425it [01:46, 80.85it/s]  166441it [01:46, 3271.15it/s]154608it [01:46, 103.06it/s]154889it [01:46, 151.28it/s]166778it [01:46, 2754.28it/s]155264it [01:46, 239.82it/s]155543it [01:46, 321.68it/s]167073it [01:46, 1932.34it/s]155795it [01:47, 409.70it/s]167364it [01:46, 2122.73it/s]156124it [01:47, 581.53it/s]167618it [01:47, 2211.08it/s]154262it [01:46, 2243.61it/s]154425it [01:46, 69.18it/s]  156412it [01:47, 761.94it/s]154763it [01:47, 113.15it/s]156676it [01:47, 924.24it/s]167872it [01:47, 1794.29it/s]155047it [01:47, 163.91it/s]156922it [01:47, 1077.38it/s]168085it [01:47, 1779.25it/s]168286it [01:47, 1754.36it/s]155316it [01:47, 222.84it/s]157153it [01:47, 1126.61it/s]168477it [01:47, 1755.20it/s]155598it [01:47, 312.98it/s]157353it [01:47, 1253.31it/s]155869it [01:47, 426.22it/s]168664it [01:47, 1633.41it/s]157550it [01:48, 1297.24it/s]156121it [01:47, 550.84it/s]157784it [01:48, 1459.15it/s]168836it [01:47, 1400.96it/s]156519it [01:47, 831.11it/s]157994it [01:48, 1593.63it/s]169043it [01:48, 1536.01it/s]156873it [01:47, 1114.29it/s]158243it [01:48, 1802.62it/s]169288it [01:48, 1700.87it/s]157179it [01:48, 1304.15it/s]158480it [01:48, 1905.54it/s]169545it [01:48, 1802.77it/s]157461it [01:48, 1517.14it/s]169732it [01:48, 1790.29it/s]158694it [01:48, 1571.37it/s]157738it [01:48, 1521.18it/s]170030it [01:48, 2101.64it/s]158904it [01:48, 1670.58it/s]170385it [01:48, 2404.93it/s]159091it [01:48, 1682.46it/s]157978it [01:48, 1382.93it/s]170631it [01:48, 2312.44it/s]159308it [01:48, 1785.68it/s]158220it [01:48, 1553.88it/s]159498it [01:49, 1723.56it/s]158548it [01:48, 1896.65it/s]170867it [01:48, 1848.77it/s]159679it [01:49, 1659.94it/s]158794it [01:48, 1981.71it/s]171068it [01:49, 1861.26it/s]160080it [01:49, 2278.22it/s]171406it [01:49, 2238.49it/s]159034it [01:49, 1808.99it/s]160405it [01:49, 2543.92it/s]171810it [01:49, 2710.73it/s]159246it [01:49, 1855.96it/s]160735it [01:49, 2755.13it/s]172137it [01:49, 2862.88it/s]159464it [01:49, 1805.42it/s]172539it [01:49, 3186.97it/s]161020it [01:49, 2003.21it/s]172870it [01:49, 3036.52it/s]159661it [01:49, 1630.51it/s]173222it [01:49, 3170.67it/s]159837it [01:49, 1546.35it/s]173547it [01:49, 3052.62it/s]161256it [01:49, 1573.56it/s]160059it [01:49, 1685.51it/s]161451it [01:50, 1612.84it/s]173859it [01:49, 2404.16it/s]160237it [01:49, 1348.54it/s]161640it [01:50, 1547.18it/s]174124it [01:50, 2295.00it/s]161894it [01:50, 1766.72it/s]160388it [01:49, 1310.66it/s]174433it [01:50, 2485.79it/s]160654it [01:50, 1572.56it/s]162092it [01:50, 1610.38it/s]174780it [01:50, 2735.95it/s]160824it [01:50, 1548.75it/s]162269it [01:50, 1572.59it/s]175070it [01:50, 2522.14it/s]162533it [01:50, 1829.69it/s]160987it [01:50, 1345.59it/s]175336it [01:50, 2368.92it/s]162819it [01:50, 2092.82it/s]161174it [01:50, 1468.41it/s]154237it [01:51, 2017.23it/s]154427it [01:51, 66.52it/s]  163043it [01:50, 2017.59it/s]175583it [01:50, 2033.72it/s]161552it [01:50, 2047.73it/s]154570it [01:51, 84.11it/s]163446it [01:51, 2552.62it/s]161927it [01:50, 2495.83it/s]175800it [01:50, 1932.64it/s]154758it [01:51, 116.68it/s]163789it [01:51, 2792.75it/s]176002it [01:50, 1873.56it/s]154953it [01:51, 162.77it/s]164143it [01:51, 2974.78it/s]162196it [01:50, 2061.51it/s]176195it [01:51, 1826.12it/s]155163it [01:51, 230.12it/s]164450it [01:51, 2725.40it/s]176424it [01:51, 1942.83it/s]162428it [01:51, 1783.79it/s]155357it [01:51, 310.94it/s]176726it [01:51, 2229.66it/s]164733it [01:51, 2505.28it/s]162737it [01:51, 2076.05it/s]155748it [01:52, 538.69it/s]177063it [01:51, 2533.30it/s]165063it [01:51, 2711.07it/s]156106it [01:52, 774.71it/s]162971it [01:51, 1841.67it/s]165405it [01:51, 2901.39it/s]177324it [01:51, 2411.28it/s]156369it [01:52, 916.52it/s]165704it [01:51, 2855.92it/s]163177it [01:51, 1754.97it/s]177621it [01:51, 2553.83it/s]156635it [01:52, 1131.35it/s]163433it [01:51, 1941.69it/s]165996it [01:51, 2819.59it/s]177882it [01:51, 2545.77it/s]156894it [01:52, 1346.37it/s]178141it [01:51, 2441.43it/s]163643it [01:51, 1814.38it/s]157142it [01:52, 1508.36it/s]166283it [01:52, 2178.17it/s]178389it [01:51, 2441.36it/s]163850it [01:51, 1876.26it/s]178653it [01:52, 2475.73it/s]164078it [01:51, 1980.90it/s]157381it [01:52, 1513.62it/s]166525it [01:52, 1814.01it/s]164297it [01:51, 2009.44it/s]157632it [01:52, 1709.69it/s]178903it [01:52, 2328.83it/s]166805it [01:52, 2003.77it/s]157856it [01:53, 1784.72it/s]179139it [01:52, 2017.87it/s]164504it [01:52, 1664.58it/s]167030it [01:52, 2024.43it/s]158276it [01:53, 2357.38it/s]179389it [01:52, 2138.42it/s]167313it [01:52, 2189.38it/s]164684it [01:52, 1618.05it/s]158554it [01:53, 2437.55it/s]179625it [01:52, 2170.04it/s]167619it [01:52, 2412.82it/s]164855it [01:52, 1515.17it/s]158829it [01:53, 2377.87it/s]179866it [01:52, 2219.99it/s]167875it [01:52, 2409.21it/s]165039it [01:52, 1586.48it/s]159254it [01:53, 2868.53it/s]168126it [01:52, 2195.18it/s]165258it [01:52, 1743.04it/s]159595it [01:53, 3013.85it/s]180093it [01:52, 1727.71it/s]168365it [01:53, 2221.75it/s]160001it [01:53, 3304.33it/s]165439it [01:52, 1542.20it/s]180309it [01:52, 1803.18it/s]165668it [01:52, 1731.51it/s]168595it [01:53, 1951.01it/s]160346it [01:53, 2655.70it/s]180505it [01:53, 1527.67it/s]165851it [01:52, 1629.29it/s]168800it [01:53, 1759.22it/s]160642it [01:54, 2411.67it/s]180674it [01:53, 1485.45it/s]166109it [01:53, 1866.54it/s]169042it [01:53, 1917.99it/s]160907it [01:54, 2377.82it/s]180921it [01:53, 1665.54it/s]166319it [01:53, 1928.43it/s]169255it [01:53, 1836.56it/s]181121it [01:53, 1747.43it/s]161161it [01:54, 2292.86it/s]166519it [01:53, 1841.91it/s]181336it [01:53, 1851.55it/s]169446it [01:53, 1654.18it/s]161405it [01:54, 2328.87it/s]166822it [01:53, 2166.22it/s]181719it [01:53, 2359.96it/s]161670it [01:54, 2405.30it/s]167145it [01:53, 2463.42it/s]169619it [01:53, 1297.14it/s]167503it [01:53, 2782.43it/s]161918it [01:54, 2332.66it/s]181965it [01:53, 2180.86it/s]167862it [01:53, 2994.06it/s]169764it [01:54, 1250.46it/s]182192it [01:53, 1946.63it/s]162156it [01:54, 2046.58it/s]169996it [01:54, 1486.09it/s]168167it [01:53, 2666.81it/s]182397it [01:54, 1897.26it/s]162370it [01:54, 1946.61it/s]170281it [01:54, 1814.53it/s]168444it [01:53, 2502.57it/s]182616it [01:54, 1971.59it/s]162610it [01:54, 2060.46it/s]170481it [01:54, 1814.77it/s]182882it [01:54, 2155.90it/s]168703it [01:54, 2397.36it/s]162826it [01:55, 1860.48it/s]170737it [01:54, 2009.17it/s]183104it [01:54, 1987.66it/s]163019it [01:55, 1815.53it/s]170950it [01:54, 1939.62it/s]168949it [01:54, 1999.19it/s]183309it [01:54, 1842.43it/s]163206it [01:55, 1755.45it/s]169205it [01:54, 2130.93it/s]171153it [01:54, 1876.61it/s]183499it [01:54, 1856.37it/s]169431it [01:54, 2144.09it/s]163385it [01:55, 1634.30it/s]183696it [01:54, 1886.01it/s]171347it [01:54, 1567.45it/s]163552it [01:55, 1561.72it/s]171641it [01:55, 1898.43it/s]169655it [01:54, 1745.86it/s]163909it [01:55, 2080.36it/s]171853it [01:55, 1954.40it/s]183888it [01:54, 1437.36it/s]169933it [01:54, 1957.66it/s]164305it [01:55, 2587.39it/s]172064it [01:55, 1985.71it/s]170174it [01:54, 2066.79it/s]184050it [01:55, 1395.58it/s]164576it [01:55, 2421.72it/s]172273it [01:55, 2007.39it/s]184254it [01:55, 1548.47it/s]170395it [01:55, 1809.45it/s]164829it [01:56, 2215.11it/s]172481it [01:55, 1745.33it/s]184422it [01:55, 1461.14it/s]165060it [01:56, 2131.81it/s]170591it [01:55, 1563.93it/s]184635it [01:55, 1628.65it/s]172666it [01:55, 1624.95it/s]165289it [01:56, 2156.85it/s]170777it [01:55, 1620.46it/s]184808it [01:55, 1614.86it/s]172855it [01:55, 1690.66it/s]170981it [01:55, 1698.53it/s]185048it [01:55, 1824.55it/s]173031it [01:55, 1643.62it/s]165510it [01:56, 1699.63it/s]171224it [01:55, 1732.89it/s]173294it [01:55, 1905.87it/s]185238it [01:55, 1548.47it/s]171459it [01:55, 1886.45it/s]165698it [01:56, 1561.92it/s]173559it [01:56, 2094.07it/s]185405it [01:55, 1437.54it/s]165966it [01:56, 1816.45it/s]171739it [01:55, 2099.03it/s]185689it [01:56, 1757.84it/s]173775it [01:56, 1715.95it/s]166198it [01:56, 1940.08it/s]172012it [01:55, 2268.92it/s]185964it [01:56, 2012.04it/s]173984it [01:56, 1793.36it/s]166441it [01:56, 2067.10it/s]172246it [01:55, 2189.02it/s]174209it [01:56, 1910.03it/s]166722it [01:57, 2251.77it/s]186178it [01:56, 1922.27it/s]172619it [01:56, 2614.10it/s]174472it [01:56, 2102.91it/s]167004it [01:57, 2399.53it/s]172964it [01:56, 2849.98it/s]186380it [01:56, 1871.27it/s]173316it [01:56, 3037.00it/s]174692it [01:56, 1867.56it/s]186574it [01:56, 1715.94it/s]167252it [01:57, 1981.00it/s]173625it [01:56, 2954.00it/s]174958it [01:56, 2045.26it/s]186777it [01:56, 1796.09it/s]167468it [01:57, 1908.22it/s]175187it [01:56, 2110.54it/s]187147it [01:56, 2306.76it/s]167687it [01:57, 1920.72it/s]173925it [01:56, 2206.74it/s]187484it [01:56, 2600.59it/s]175406it [01:57, 1850.19it/s]187875it [01:56, 2842.14it/s]167888it [01:57, 1660.53it/s]174176it [01:56, 2105.66it/s]168125it [01:57, 1830.18it/s]175602it [01:57, 1407.36it/s]174408it [01:56, 1804.53it/s]168341it [01:57, 1909.42it/s]175834it [01:57, 1582.89it/s]188165it [01:57, 1839.73it/s]174608it [01:57, 1828.47it/s]168542it [01:58, 1766.27it/s]176084it [01:57, 1788.55it/s]174816it [01:57, 1882.60it/s]188397it [01:57, 1878.03it/s]168727it [01:58, 1698.94it/s]176284it [01:57, 1756.91it/s]175196it [01:57, 2366.29it/s]188620it [01:57, 1884.06it/s]169126it [01:58, 2286.08it/s]176499it [01:57, 1817.12it/s]175499it [01:57, 2541.33it/s]188865it [01:57, 1998.51it/s]169546it [01:58, 2687.23it/s]189091it [01:57, 2003.14it/s]176692it [01:57, 1597.22it/s]175768it [01:57, 2022.70it/s]169824it [01:58, 2395.29it/s]176899it [01:57, 1675.62it/s]189306it [01:57, 1863.24it/s]175997it [01:57, 1932.11it/s]170075it [01:58, 2391.40it/s]189640it [01:57, 2232.29it/s]176224it [01:57, 2011.10it/s]177104it [01:58, 1495.42it/s]170322it [01:58, 2318.27it/s]189879it [01:58, 2265.07it/s]176440it [01:57, 2035.48it/s]177312it [01:58, 1632.16it/s]170559it [01:58, 2278.85it/s]190181it [01:58, 2469.23it/s]176843it [01:57, 2561.15it/s]177633it [01:58, 2027.41it/s]170972it [01:58, 2779.32it/s]190469it [01:58, 2581.91it/s]177197it [01:58, 2827.96it/s]177944it [01:58, 2225.84it/s]190741it [01:58, 2618.85it/s]171258it [01:59, 2375.53it/s]177496it [01:58, 2871.30it/s]178269it [01:58, 2497.54it/s]191040it [01:58, 2724.52it/s]171511it [01:59, 2391.25it/s]178596it [01:58, 2708.21it/s]177793it [01:58, 2600.64it/s]191362it [01:58, 2868.82it/s]171761it [01:59, 2413.45it/s]178877it [01:58, 2505.76it/s]191658it [01:58, 2895.17it/s]178064it [01:58, 2278.18it/s]172010it [01:59, 2369.65it/s]192073it [01:58, 3266.78it/s]178313it [01:58, 2322.16it/s]179137it [01:58, 2119.92it/s]172253it [01:59, 2011.19it/s]192402it [01:58, 2879.10it/s]178577it [01:58, 2405.03it/s]179365it [01:59, 2116.36it/s]172487it [01:59, 2088.92it/s]192700it [01:58, 2829.93it/s]178826it [01:58, 2313.99it/s]179588it [01:59, 1872.40it/s]192990it [01:59, 2687.99it/s]172706it [01:59, 1595.68it/s]179064it [01:58, 1907.72it/s]179787it [01:59, 1547.34it/s]179278it [01:59, 1925.33it/s]193265it [01:59, 2117.93it/s]172889it [02:00, 1433.67it/s]179557it [01:59, 2142.53it/s]179957it [01:59, 1416.97it/s]173050it [02:00, 1444.47it/s]179895it [01:59, 2468.88it/s]193498it [01:59, 1743.39it/s]180109it [01:59, 1375.30it/s]173207it [02:00, 1401.04it/s]180301it [01:59, 2904.81it/s]180324it [01:59, 1551.40it/s]173401it [02:00, 1529.07it/s]193695it [01:59, 1580.77it/s]180605it [01:59, 2563.26it/s]193870it [01:59, 1597.14it/s]173563it [02:00, 1479.98it/s]180489it [01:59, 1286.34it/s]180880it [01:59, 2611.59it/s]173718it [02:00, 1424.42it/s]180659it [02:00, 1378.22it/s]194042it [01:59, 1449.69it/s]181153it [01:59, 2401.38it/s]194289it [02:00, 1678.81it/s]180809it [02:00, 1327.38it/s]173865it [02:00, 1202.86it/s]181404it [01:59, 2189.56it/s]194471it [02:00, 1700.15it/s]180950it [02:00, 1322.94it/s]174103it [02:00, 1480.70it/s]181731it [02:00, 2459.51it/s]194658it [02:00, 1743.28it/s]181104it [02:00, 1362.30it/s]174323it [02:01, 1652.67it/s]182012it [02:00, 2506.55it/s]194841it [02:00, 1740.79it/s]181280it [02:00, 1464.13it/s]174573it [02:01, 1868.14it/s]195118it [02:00, 2022.09it/s]181472it [02:00, 1589.05it/s]182272it [02:00, 2106.33it/s]174771it [02:01, 1688.70it/s]195327it [02:00, 1966.40it/s]181779it [02:00, 1992.94it/s]182521it [02:00, 2199.04it/s]195571it [02:00, 2086.38it/s]174951it [02:01, 1629.58it/s]182772it [02:00, 2278.85it/s]181984it [02:00, 1762.89it/s]195784it [02:00, 2091.20it/s]175159it [02:01, 1712.10it/s]182169it [02:00, 1729.05it/s]196133it [02:00, 2492.81it/s]175337it [02:01, 1627.34it/s]183011it [02:00, 1809.33it/s]182350it [02:01, 1741.05it/s]196418it [02:00, 2595.05it/s]183283it [02:00, 2020.02it/s]182608it [02:01, 1971.17it/s]175505it [02:01, 1446.84it/s]183507it [02:00, 1903.57it/s]175671it [02:01, 1433.12it/s]196681it [02:01, 1987.18it/s]182810it [02:01, 1743.26it/s]183754it [02:01, 2041.99it/s]175819it [02:02, 1439.78it/s]197025it [02:01, 2339.23it/s]183001it [02:01, 1785.63it/s]184084it [02:01, 2368.67it/s]175966it [02:02, 1425.97it/s]183264it [02:01, 2010.98it/s]197284it [02:01, 2292.46it/s]184491it [02:01, 2828.68it/s]183522it [02:01, 2149.26it/s]176111it [02:02, 1373.36it/s]197531it [02:01, 2154.37it/s]184790it [02:01, 2553.39it/s]176445it [02:02, 1902.01it/s]197760it [02:01, 2066.65it/s]183743it [02:01, 1794.78it/s]176858it [02:02, 2521.15it/s]185061it [02:01, 2544.48it/s]198048it [02:01, 2273.11it/s]177179it [02:02, 2640.49it/s]185327it [02:01, 2529.54it/s]183936it [02:01, 1552.36it/s]198300it [02:01, 2338.73it/s]177450it [02:02, 2453.01it/s]185588it [02:01, 2374.71it/s]184221it [02:02, 1854.35it/s]198542it [02:01, 2300.02it/s]184447it [02:02, 1955.07it/s]177702it [02:02, 2389.04it/s]185832it [02:01, 2279.15it/s]198778it [02:02, 2267.74it/s]184664it [02:02, 1953.63it/s]186065it [02:01, 2192.79it/s]199009it [02:02, 2250.00it/s]177946it [02:02, 2155.43it/s]184935it [02:02, 2155.17it/s]186288it [02:02, 2186.36it/s]199313it [02:02, 2467.83it/s]178168it [02:03, 2159.96it/s]185349it [02:02, 2703.19it/s]199616it [02:02, 2628.80it/s]186509it [02:02, 2128.24it/s]178482it [02:03, 2422.95it/s]178754it [02:03, 2504.45it/s]186724it [02:02, 2090.31it/s]199882it [02:02, 2305.76it/s]185631it [02:02, 2126.33it/s]186934it [02:02, 2066.88it/s]179010it [02:03, 2365.00it/s]200122it [02:02, 2201.29it/s]185871it [02:02, 2036.78it/s]187184it [02:02, 2156.84it/s]179345it [02:03, 2634.38it/s]200349it [02:02, 2191.06it/s]186119it [02:02, 2142.83it/s]179615it [02:03, 2643.57it/s]187401it [02:02, 1876.82it/s]200573it [02:02, 1869.40it/s]186349it [02:03, 1965.08it/s]179884it [02:03, 2445.79it/s]187603it [02:02, 1913.93it/s]186570it [02:03, 2001.34it/s]187839it [02:02, 2006.34it/s]180134it [02:03, 2271.71it/s]186943it [02:03, 2447.29it/s]180367it [02:03, 2100.42it/s]188044it [02:03, 1676.94it/s]188269it [02:03, 1810.61it/s]187202it [02:03, 1714.85it/s]180583it [02:04, 1622.87it/s]188540it [02:03, 2043.29it/s]187464it [02:03, 1903.20it/s]180791it [02:04, 1722.52it/s]188792it [02:03, 2158.15it/s]187690it [02:03, 1947.88it/s]180980it [02:04, 1746.54it/s]189017it [02:03, 2089.02it/s]187932it [02:03, 2063.01it/s]181246it [02:04, 1977.72it/s]188159it [02:03, 2028.45it/s]189233it [02:03, 1716.20it/s]181457it [02:04, 1931.94it/s]189448it [02:03, 1821.27it/s]181722it [02:04, 2087.02it/s]188376it [02:04, 1813.12it/s]189704it [02:03, 1888.56it/s]188646it [02:04, 1977.62it/s]181938it [02:04, 1654.37it/s]189902it [02:03, 1805.73it/s]188921it [02:04, 2174.28it/s]182146it [02:05, 1753.98it/s]189298it [02:04, 2600.90it/s]190164it [02:04, 2014.16it/s]182420it [02:05, 1980.37it/s]189642it [02:04, 2832.06it/s]190374it [02:04, 1866.08it/s]182633it [02:05, 1909.51it/s]189937it [02:04, 2607.07it/s]190227it [02:04, 2684.36it/s]190568it [02:04, 1378.47it/s]182835it [02:05, 1590.09it/s]190728it [02:04, 1404.66it/s]183009it [02:05, 1523.11it/s]190504it [02:04, 2105.85it/s]191008it [02:04, 1726.05it/s]183204it [02:05, 1624.57it/s]191277it [02:04, 1911.03it/s]183376it [02:05, 1516.75it/s]190739it [02:05, 1679.85it/s]191485it [02:04, 1746.97it/s]183605it [02:05, 1709.36it/s]190985it [02:05, 1827.25it/s]191734it [02:05, 1921.86it/s]183826it [02:06, 1775.92it/s]191194it [02:05, 1660.08it/s]191940it [02:05, 1936.52it/s]184243it [02:06, 2411.65it/s]191379it [02:05, 1580.36it/s]184507it [02:06, 2418.58it/s]192143it [02:05, 1566.07it/s]191558it [02:05, 1627.97it/s]184758it [02:06, 1984.81it/s]191793it [02:05, 1793.13it/s]192317it [02:05, 1365.47it/s]192051it [02:05, 1976.26it/s]192519it [02:05, 1497.75it/s]184975it [02:06, 1688.68it/s]192280it [02:06, 2059.52it/s]192759it [02:05, 1696.78it/s]185163it [02:06, 1688.11it/s]192494it [02:06, 2014.30it/s]192964it [02:05, 1781.81it/s]185404it [02:06, 1840.87it/s]192701it [02:06, 2014.80it/s]193154it [02:05, 1596.12it/s]192907it [02:06, 1985.49it/s]185601it [02:06, 1547.91it/s]193341it [02:06, 1664.36it/s]185771it [02:07, 1560.66it/s]193109it [02:06, 1934.34it/s]193571it [02:06, 1827.02it/s]193305it [02:06, 1678.15it/s]193763it [02:06, 1708.30it/s]185938it [02:07, 1300.91it/s]193480it [02:06, 1625.21it/s]186206it [02:07, 1595.09it/s]193941it [02:06, 1523.96it/s]193657it [02:06, 1659.33it/s]194345it [02:06, 2151.40it/s]186384it [02:07, 1517.18it/s]193827it [02:06, 1603.04it/s]186634it [02:07, 1752.34it/s]194744it [02:06, 2530.47it/s]193990it [02:07, 1602.01it/s]186832it [02:07, 1799.60it/s]195132it [02:06, 2886.34it/s]194268it [02:07, 1899.00it/s]187105it [02:07, 2005.93it/s]195533it [02:06, 3193.58it/s]194538it [02:07, 2123.88it/s]195872it [02:06, 3248.39it/s]194754it [02:07, 2047.75it/s]194967it [02:07, 2070.26it/s]187315it [02:08, 1466.35it/s]196207it [02:07, 2550.50it/s]195227it [02:07, 2221.08it/s]187558it [02:08, 1676.83it/s]187863it [02:08, 2003.67it/s]195452it [02:07, 2033.25it/s]196492it [02:07, 2062.13it/s]188214it [02:08, 2385.67it/s]195660it [02:07, 1809.90it/s]196732it [02:07, 2091.70it/s]188499it [02:08, 2505.79it/s]195906it [02:07, 1907.79it/s]188770it [02:08, 2493.02it/s]196966it [02:07, 1778.99it/s]196169it [02:08, 2088.97it/s]197190it [02:07, 1873.06it/s]189033it [02:08, 2149.20it/s]196424it [02:08, 1984.55it/s]189292it [02:08, 2259.28it/s]197397it [02:07, 1684.31it/s]196662it [02:08, 2057.48it/s]189533it [02:08, 2251.25it/s]196915it [02:08, 2182.56it/s]197581it [02:08, 1587.75it/s]189768it [02:09, 2200.54it/s]197264it [02:08, 2472.45it/s]197750it [02:08, 1506.32it/s]189997it [02:09, 2183.87it/s]197660it [02:08, 2882.93it/s]198011it [02:08, 1752.96it/s]197991it [02:08, 2937.18it/s]190221it [02:09, 1703.29it/s]198197it [02:08, 1693.81it/s]190487it [02:09, 1926.49it/s]198374it [02:08, 1439.86it/s]198289it [02:08, 2021.77it/s]190699it [02:09, 1862.30it/s]190967it [02:09, 2067.84it/s]198528it [02:08, 1311.60it/s]198532it [02:09, 1910.73it/s]191214it [02:09, 2170.41it/s]198792it [02:08, 1616.74it/s]198762it [02:09, 1990.72it/s]191531it [02:09, 2443.75it/s]199100it [02:08, 1978.21it/s]199430it [02:09, 2286.03it/s]191875it [02:10, 2681.86it/s]198984it [02:09, 1640.06it/s]199245it [02:09, 1847.40it/s]199675it [02:09, 2154.06it/s]192151it [02:10, 2386.69it/s]199472it [02:09, 1929.21it/s]199903it [02:09, 2049.97it/s]192401it [02:10, 2255.26it/s]199685it [02:09, 1968.62it/s]192649it [02:10, 2312.95it/s]200117it [02:09, 1818.94it/s]199898it [02:09, 2009.83it/s]192897it [02:10, 2334.18it/s]200344it [02:09, 1867.24it/s]200311it [02:09, 2584.18it/s]193136it [02:10, 2048.30it/s]200593it [02:10, 2608.15it/s]200538it [02:09, 1552.32it/s]193406it [02:10, 2213.65it/s]193683it [02:10, 2362.23it/s]193928it [02:10, 2361.47it/s]194256it [02:11, 2617.48it/s]194536it [02:11, 2665.95it/s]194877it [02:11, 2881.61it/s]195169it [02:11, 2596.26it/s]195437it [02:11, 2105.08it/s]195684it [02:11, 2190.09it/s]195919it [02:11, 2064.32it/s]196137it [02:11, 2055.68it/s]196388it [02:12, 2172.86it/s]196613it [02:12, 1999.75it/s]196830it [02:12, 2043.97it/s]197040it [02:12, 2054.80it/s]197266it [02:12, 2053.43it/s]197475it [02:12, 1832.25it/s]197708it [02:12, 1951.84it/s]197924it [02:12, 1993.60it/s]198128it [02:12, 1997.21it/s]198388it [02:13, 2166.50it/s]198608it [02:13, 1843.56it/s]198882it [02:13, 2073.91it/s]199101it [02:13, 1959.61it/s]199306it [02:13, 1771.56it/s]199492it [02:13, 1580.47it/s]199658it [02:13, 1593.19it/s]199858it [02:13, 1696.73it/s]200034it [02:13, 1708.35it/s]200416it [02:14, 2284.21it/s]200653it [02:14, 2171.84it/s]200573it [02:15, 1869.40it/s]200695it [02:15, 54.52it/s]  201027it [02:15, 90.97it/s]201365it [02:15, 142.59it/s]201659it [02:15, 203.93it/s]201982it [02:15, 295.84it/s]202371it [02:15, 445.28it/s]202699it [02:15, 604.23it/s]203082it [02:15, 843.41it/s]203423it [02:16, 903.88it/s]203694it [02:16, 1064.26it/s]203951it [02:16, 1226.55it/s]203998it [02:16, 1493.97it/s]
200538it [02:21, 1552.32it/s]200694it [02:21, 55.09it/s]  200938it [02:21, 82.50it/s]201129it [02:21, 112.01it/s]201395it [02:22, 169.37it/s]201673it [02:22, 250.62it/s]202085it [02:22, 415.57it/s]202396it [02:22, 570.20it/s]202805it [02:22, 836.94it/s]203182it [02:22, 1123.44it/s]203528it [02:22, 1409.55it/s]203872it [02:22, 1636.60it/s]203998it [02:22, 1428.48it/s]
2022-07-29 14:58:54 | INFO | root | success load 203998 data
2022-07-29 14:58:54 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-29 14:58:54 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-29 14:58:54 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-29 14:58:54 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-29 14:58:54 | INFO | transformer.tokenization_utils | loading file None
2022-07-29 14:58:54 | INFO | transformer.tokenization_utils | loading file None
2022-07-29 14:58:54 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
200593it [02:23, 2608.15it/s]200694it [02:23, 56.86it/s]  200953it [02:23, 83.49it/s]201220it [02:23, 121.16it/s]201568it [02:23, 190.34it/s]201922it [02:23, 285.89it/s]202220it [02:23, 384.95it/s]202491it [02:23, 487.69it/s]202729it [02:24, 614.36it/s]202967it [02:24, 768.58it/s]203205it [02:24, 942.36it/s]203440it [02:24, 1009.76it/s]203678it [02:24, 1211.87it/s]203935it [02:24, 1410.88it/s]203998it [02:24, 1409.31it/s]
200653it [02:26, 2171.84it/s]200696it [02:26, 48.32it/s]  201003it [02:26, 81.79it/s]201252it [02:26, 119.13it/s]201495it [02:27, 167.95it/s]201718it [02:27, 226.30it/s]202077it [02:27, 362.11it/s]202375it [02:27, 504.49it/s]202733it [02:27, 726.52it/s]203115it [02:27, 1015.70it/s]203439it [02:27, 1277.52it/s]203788it [02:27, 1595.94it/s]203998it [02:27, 1379.17it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-29 15:09:18 | INFO | train_inner | epoch 001:    100 / 797 loss=nan, nll_loss=11.952, mask_ins=7.596, word_ins_ml=12.465, word_reposition=4.672, kpe=nan, ppl=nan, wps=1376.2, ups=0.16, wpb=8409.1, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=22.987, clip=19, loss_scale=128, train_wall=494, wall=766
2022-07-29 15:19:08 | INFO | train_inner | epoch 001:    200 / 797 loss=nan, nll_loss=10.692, mask_ins=4.813, word_ins_ml=11.356, word_reposition=4.032, kpe=nan, ppl=nan, wps=1427, ups=0.17, wpb=8425.9, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=18.791, clip=0, loss_scale=128, train_wall=473, wall=1357
2022-07-29 15:28:51 | INFO | train_inner | epoch 001:    300 / 797 loss=nan, nll_loss=10.548, mask_ins=2.475, word_ins_ml=11.233, word_reposition=2.591, kpe=nan, ppl=nan, wps=1444, ups=0.17, wpb=8419.1, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=6.64, clip=0, loss_scale=128, train_wall=468, wall=1940
2022-07-29 15:38:28 | INFO | train_inner | epoch 001:    400 / 797 loss=15.55, nll_loss=10.353, mask_ins=2.088, word_ins_ml=11.065, word_reposition=1.378, kpe=1.02, ppl=47991.1, wps=1467.3, ups=0.17, wpb=8464, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=3.943, clip=0, loss_scale=128, train_wall=463, wall=2517
2022-07-29 15:48:04 | INFO | train_inner | epoch 001:    500 / 797 loss=14.779, nll_loss=10.072, mask_ins=1.91, word_ins_ml=10.822, word_reposition=1.068, kpe=0.979, ppl=28112.3, wps=1464.5, ups=0.17, wpb=8439.5, bsz=256, num_updates=500, lr=5.009e-05, gnorm=3.315, clip=0, loss_scale=128, train_wall=466, wall=3093
2022-07-29 15:57:41 | INFO | train_inner | epoch 001:    600 / 797 loss=14.333, nll_loss=9.721, mask_ins=1.885, word_ins_ml=10.518, word_reposition=0.984, kpe=0.946, ppl=20630.6, wps=1463.8, ups=0.17, wpb=8444.1, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=3.059, clip=0, loss_scale=242, train_wall=466, wall=3670
2022-07-29 16:07:16 | INFO | train_inner | epoch 001:    700 / 797 loss=nan, nll_loss=9.407, mask_ins=1.872, word_ins_ml=10.244, word_reposition=0.988, kpe=nan, ppl=nan, wps=1467.6, ups=0.17, wpb=8430.9, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=3.032, clip=0, loss_scale=256, train_wall=464, wall=4244
2022-07-29 16:17:17 | INFO | train | epoch 001 | loss nan | nll_loss 10.238 | mask_ins 3.068 | word_ins_ml 10.967 | word_reposition 2.093 | kpe nan | ppl nan | wps 1434.1 | ups 0.17 | wpb 8439.5 | bsz 256 | num_updates 797 | lr 7.97841e-05 | gnorm 8.081 | clip 2.4 | loss_scale 174 | train_wall 3787 | wall 4846
2022-07-29 16:19:52 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 15.324 | nll_loss 10.76 | mask_ins 1.88 | word_ins_ml 11.537 | word_reposition 0.86 | kpe 1.046 | ppl 41011.4 | wps 2130.6 | wpb 934 | bsz 32 | num_updates 797
2022-07-29 16:20:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_best.pt (epoch 1 @ 797 updates, score 15.324) (writing took 7.808473566547036 seconds)
2022-07-29 16:20:17 | INFO | train_inner | epoch 002:      3 / 797 loss=nan, nll_loss=9.121, mask_ins=1.872, word_ins_ml=9.998, word_reposition=0.995, kpe=nan, ppl=nan, wps=1085.1, ups=0.13, wpb=8474, bsz=255.7, num_updates=800, lr=8.0084e-05, gnorm=2.723, clip=0, loss_scale=256, train_wall=505, wall=5025
2022-07-29 16:29:42 | INFO | train_inner | epoch 002:    103 / 797 loss=13.375, nll_loss=8.776, mask_ins=1.79, word_ins_ml=9.698, word_reposition=1.009, kpe=0.879, ppl=10626.2, wps=1485.7, ups=0.18, wpb=8400.8, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=2.968, clip=0, loss_scale=256, train_wall=455, wall=5591
2022-07-29 16:39:04 | INFO | train_inner | epoch 002:    203 / 797 loss=13.054, nll_loss=8.494, mask_ins=1.726, word_ins_ml=9.452, word_reposition=1.008, kpe=0.868, ppl=8503.3, wps=1500.7, ups=0.18, wpb=8431, bsz=256, num_updates=1000, lr=0.00010008, gnorm=2.668, clip=0, loss_scale=256, train_wall=453, wall=6153
2022-07-29 16:48:33 | INFO | train_inner | epoch 002:    303 / 797 loss=12.841, nll_loss=8.279, mask_ins=1.709, word_ins_ml=9.266, word_reposition=1.007, kpe=0.86, ppl=7336.19, wps=1488.4, ups=0.18, wpb=8466.6, bsz=256, num_updates=1100, lr=0.000110078, gnorm=2.637, clip=0, loss_scale=453, train_wall=458, wall=6722
2022-07-29 16:57:56 | INFO | train_inner | epoch 002:    403 / 797 loss=nan, nll_loss=8.061, mask_ins=1.682, word_ins_ml=9.076, word_reposition=0.991, kpe=nan, ppl=nan, wps=1497.1, ups=0.18, wpb=8433.2, bsz=256, num_updates=1200, lr=0.000120076, gnorm=2.588, clip=0, loss_scale=512, train_wall=455, wall=7285
2022-07-29 17:07:19 | INFO | train_inner | epoch 002:    503 / 797 loss=nan, nll_loss=7.882, mask_ins=1.669, word_ins_ml=8.92, word_reposition=0.98, kpe=nan, ppl=nan, wps=1501.2, ups=0.18, wpb=8454.7, bsz=256, num_updates=1300, lr=0.000130074, gnorm=2.365, clip=0, loss_scale=512, train_wall=452, wall=7848
2022-07-29 17:16:44 | INFO | train_inner | epoch 002:    603 / 797 loss=12.21, nll_loss=7.696, mask_ins=1.647, word_ins_ml=8.757, word_reposition=0.974, kpe=0.831, ppl=4737.25, wps=1479.7, ups=0.18, wpb=8358.5, bsz=256, num_updates=1400, lr=0.000140072, gnorm=2.358, clip=0, loss_scale=512, train_wall=454, wall=8413
2022-07-29 17:26:11 | INFO | train_inner | epoch 002:    703 / 797 loss=nan, nll_loss=7.571, mask_ins=1.65, word_ins_ml=8.648, word_reposition=0.976, kpe=nan, ppl=nan, wps=1504.8, ups=0.18, wpb=8531.8, bsz=256, num_updates=1500, lr=0.00015007, gnorm=2.299, clip=0, loss_scale=512, train_wall=457, wall=8980
2022-07-29 17:35:01 | INFO | train | epoch 002 | loss nan | nll_loss 8.027 | mask_ins 1.688 | word_ins_ml 9.046 | word_reposition 0.988 | kpe nan | ppl nan | wps 1442.3 | ups 0.17 | wpb 8439.5 | bsz 256 | num_updates 1594 | lr 0.000159468 | gnorm 2.518 | clip 0 | loss_scale 477 | train_wall 3624 | wall 9510
2022-07-29 17:37:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 14.811 | nll_loss 10.151 | mask_ins 1.689 | word_ins_ml 11.048 | word_reposition 1.027 | kpe 1.047 | ppl 28753.8 | wps 2140.7 | wpb 934 | bsz 32 | num_updates 1594 | best_loss 14.811
2022-07-29 17:37:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_best.pt (epoch 2 @ 1594 updates, score 14.811) (writing took 9.670721291564405 seconds)
2022-07-29 17:38:18 | INFO | train_inner | epoch 003:      6 / 797 loss=11.889, nll_loss=7.385, mask_ins=1.622, word_ins_ml=8.485, word_reposition=0.956, kpe=0.825, ppl=3792.62, wps=1160.1, ups=0.14, wpb=8434.5, bsz=255.7, num_updates=1600, lr=0.000160068, gnorm=2.235, clip=0, loss_scale=845, train_wall=453, wall=9707
2022-07-29 17:48:37 | INFO | train_inner | epoch 003:    106 / 797 loss=11.709, nll_loss=7.215, mask_ins=1.616, word_ins_ml=8.337, word_reposition=0.95, kpe=0.806, ppl=3346.84, wps=1365.8, ups=0.16, wpb=8456, bsz=256, num_updates=1700, lr=0.000170066, gnorm=2.215, clip=0, loss_scale=1024, train_wall=505, wall=10326
2022-07-29 17:58:02 | INFO | train_inner | epoch 003:    206 / 797 loss=nan, nll_loss=7.05, mask_ins=1.61, word_ins_ml=8.194, word_reposition=0.935, kpe=nan, ppl=nan, wps=1486.8, ups=0.18, wpb=8391.6, bsz=256, num_updates=1800, lr=0.000180064, gnorm=2.144, clip=0, loss_scale=1024, train_wall=456, wall=10891
2022-07-29 18:07:26 | INFO | train_inner | epoch 003:    306 / 797 loss=11.418, nll_loss=6.936, mask_ins=1.586, word_ins_ml=8.096, word_reposition=0.933, kpe=0.804, ppl=2735.65, wps=1496.8, ups=0.18, wpb=8442.5, bsz=256, num_updates=1900, lr=0.000190062, gnorm=2.115, clip=0, loss_scale=1024, train_wall=455, wall=11455
2022-07-29 18:17:05 | INFO | train_inner | epoch 003:    406 / 797 loss=11.317, nll_loss=6.802, mask_ins=1.602, word_ins_ml=7.979, word_reposition=0.936, kpe=0.799, ppl=2551.84, wps=1466.3, ups=0.17, wpb=8496.8, bsz=256, num_updates=2000, lr=0.00020006, gnorm=2.124, clip=0, loss_scale=1024, train_wall=468, wall=12034
2022-07-29 18:26:30 | INFO | train_inner | epoch 003:    506 / 797 loss=nan, nll_loss=6.672, mask_ins=1.59, word_ins_ml=7.867, word_reposition=0.908, kpe=nan, ppl=nan, wps=1487.3, ups=0.18, wpb=8402.1, bsz=256, num_updates=2100, lr=0.000210058, gnorm=2.109, clip=0, loss_scale=1567, train_wall=455, wall=12599
2022-07-29 18:35:56 | INFO | train_inner | epoch 003:    606 / 797 loss=nan, nll_loss=6.574, mask_ins=1.593, word_ins_ml=7.782, word_reposition=0.923, kpe=nan, ppl=nan, wps=1503.2, ups=0.18, wpb=8499.8, bsz=256, num_updates=2200, lr=0.000220056, gnorm=2.121, clip=0, loss_scale=2048, train_wall=456, wall=13164
2022-07-29 18:45:23 | INFO | train_inner | epoch 003:    706 / 797 loss=10.961, nll_loss=6.447, mask_ins=1.587, word_ins_ml=7.673, word_reposition=0.91, kpe=0.792, ppl=1993.7, wps=1492.1, ups=0.18, wpb=8463.6, bsz=256, num_updates=2300, lr=0.000230054, gnorm=2.073, clip=0, loss_scale=2048, train_wall=457, wall=13732
2022-07-29 18:53:59 | INFO | train | epoch 003 | loss nan | nll_loss 6.759 | mask_ins 1.596 | word_ins_ml 7.942 | word_reposition 0.923 | kpe nan | ppl nan | wps 1419.6 | ups 0.17 | wpb 8439.5 | bsz 256 | num_updates 2391 | lr 0.000239152 | gnorm 2.116 | clip 0 | loss_scale 1466 | train_wall 3695 | wall 14248
2022-07-29 18:56:31 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 14.58 | nll_loss 9.925 | mask_ins 1.656 | word_ins_ml 10.891 | word_reposition 0.927 | kpe 1.106 | ppl 24494.9 | wps 2171.8 | wpb 934 | bsz 32 | num_updates 2391 | best_loss 14.58
2022-07-29 18:56:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_best.pt (epoch 3 @ 2391 updates, score 14.58) (writing took 9.802057762630284 seconds)
2022-07-29 18:57:31 | INFO | train_inner | epoch 004:      9 / 797 loss=10.781, nll_loss=6.285, mask_ins=1.573, word_ins_ml=7.532, word_reposition=0.891, kpe=0.786, ppl=1759.98, wps=1151.5, ups=0.14, wpb=8378.2, bsz=255.7, num_updates=2400, lr=0.000240052, gnorm=2.004, clip=0, loss_scale=2048, train_wall=455, wall=14459
2022-07-29 19:07:07 | INFO | train_inner | epoch 004:    109 / 797 loss=10.614, nll_loss=6.15, mask_ins=1.547, word_ins_ml=7.416, word_reposition=0.88, kpe=0.771, ppl=1566.69, wps=1463.5, ups=0.17, wpb=8441.2, bsz=256, num_updates=2500, lr=0.00025005, gnorm=2.015, clip=0, loss_scale=2048, train_wall=468, wall=15036
2022-07-29 19:17:12 | INFO | train_inner | epoch 004:    209 / 797 loss=10.5, nll_loss=6.03, mask_ins=1.554, word_ins_ml=7.312, word_reposition=0.867, kpe=0.768, ppl=1448.43, wps=1390.6, ups=0.17, wpb=8412.1, bsz=256, num_updates=2600, lr=0.000260048, gnorm=2.053, clip=0, loss_scale=2888, train_wall=491, wall=15641
2022-07-29 19:26:34 | INFO | train_inner | epoch 004:    309 / 797 loss=nan, nll_loss=5.952, mask_ins=1.549, word_ins_ml=7.245, word_reposition=0.865, kpe=nan, ppl=nan, wps=1504.9, ups=0.18, wpb=8458.9, bsz=256, num_updates=2700, lr=0.000270046, gnorm=1.911, clip=0, loss_scale=4096, train_wall=453, wall=16203
2022-07-29 19:35:59 | INFO | train_inner | epoch 004:    409 / 797 loss=nan, nll_loss=5.87, mask_ins=1.526, word_ins_ml=7.175, word_reposition=0.857, kpe=nan, ppl=nan, wps=1494.5, ups=0.18, wpb=8436.1, bsz=256, num_updates=2800, lr=0.000280044, gnorm=1.88, clip=0, loss_scale=4096, train_wall=454, wall=16768
2022-07-29 19:42:14 | INFO | train_inner | epoch 004:    509 / 797 loss=nan, nll_loss=5.765, mask_ins=1.523, word_ins_ml=7.085, word_reposition=0.843, kpe=nan, ppl=nan, wps=2242.3, ups=0.27, wpb=8413.3, bsz=256, num_updates=2900, lr=0.000290042, gnorm=1.918, clip=0, loss_scale=4096, train_wall=316, wall=17143
2022-07-29 19:47:10 | INFO | train_inner | epoch 004:    609 / 797 loss=nan, nll_loss=5.7, mask_ins=1.526, word_ins_ml=7.029, word_reposition=0.853, kpe=nan, ppl=nan, wps=2868.3, ups=0.34, wpb=8497.6, bsz=256, num_updates=3000, lr=0.00030004, gnorm=1.929, clip=0, loss_scale=4096, train_wall=262, wall=17439
2022-07-29 19:52:04 | INFO | train_inner | epoch 004:    709 / 797 loss=10.074, nll_loss=5.631, mask_ins=1.514, word_ins_ml=6.971, word_reposition=0.823, kpe=0.767, ppl=1077.94, wps=2868.5, ups=0.34, wpb=8418.1, bsz=256, num_updates=3100, lr=0.000310038, gnorm=1.892, clip=0, loss_scale=5284, train_wall=260, wall=17733
2022-07-29 19:56:21 | INFO | train | epoch 004 | loss nan | nll_loss 5.842 | mask_ins 1.532 | word_ins_ml 7.151 | word_reposition 0.853 | kpe nan | ppl nan | wps 1797.6 | ups 0.21 | wpb 8439.5 | bsz 256 | num_updates 3188 | lr 0.000318836 | gnorm 1.938 | clip 0 | loss_scale 4266 | train_wall 2971 | wall 17989
2022-07-29 19:57:21 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 14.28 | nll_loss 9.564 | mask_ins 1.617 | word_ins_ml 10.568 | word_reposition 0.926 | kpe 1.169 | ppl 19894.2 | wps 5472.2 | wpb 934 | bsz 32 | num_updates 3188 | best_loss 14.28
2022-07-29 19:57:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_best.pt (epoch 4 @ 3188 updates, score 14.28) (writing took 5.6583146918565035 seconds)
2022-07-29 19:58:02 | INFO | train_inner | epoch 005:     12 / 797 loss=nan, nll_loss=5.578, mask_ins=1.512, word_ins_ml=6.925, word_reposition=0.833, kpe=nan, ppl=nan, wps=2355.8, ups=0.28, wpb=8441.5, bsz=255.7, num_updates=3200, lr=0.000320036, gnorm=1.876, clip=0, loss_scale=8192, train_wall=258, wall=18091
2022-07-29 20:02:55 | INFO | train_inner | epoch 005:    112 / 797 loss=9.885, nll_loss=5.457, mask_ins=1.499, word_ins_ml=6.82, word_reposition=0.816, kpe=0.749, ppl=945.31, wps=2892.1, ups=0.34, wpb=8481, bsz=256, num_updates=3300, lr=0.000330034, gnorm=1.839, clip=0, loss_scale=8192, train_wall=259, wall=18384
2022-07-29 20:08:15 | INFO | train_inner | epoch 005:    212 / 797 loss=nan, nll_loss=5.372, mask_ins=1.501, word_ins_ml=6.747, word_reposition=0.813, kpe=nan, ppl=nan, wps=2635.2, ups=0.31, wpb=8421.3, bsz=256, num_updates=3400, lr=0.000340032, gnorm=1.905, clip=0, loss_scale=8192, train_wall=285, wall=18704
2022-07-29 20:13:08 | INFO | train_inner | epoch 005:    312 / 797 loss=9.774, nll_loss=5.334, mask_ins=1.503, word_ins_ml=6.715, word_reposition=0.808, kpe=0.748, ppl=875.63, wps=2881.8, ups=0.34, wpb=8439.5, bsz=256, num_updates=3500, lr=0.00035003, gnorm=1.858, clip=0, loss_scale=8192, train_wall=259, wall=18997
2022-07-29 20:18:01 | INFO | train_inner | epoch 005:    412 / 797 loss=nan, nll_loss=5.262, mask_ins=1.486, word_ins_ml=6.652, word_reposition=0.794, kpe=nan, ppl=nan, wps=2872.2, ups=0.34, wpb=8415.5, bsz=256, num_updates=3600, lr=0.000360028, gnorm=1.852, clip=0, loss_scale=9585, train_wall=259, wall=19290
2022-07-29 20:21:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-29 20:22:58 | INFO | train_inner | epoch 005:    513 / 797 loss=9.673, nll_loss=5.24, mask_ins=1.489, word_ins_ml=6.633, word_reposition=0.798, kpe=0.753, ppl=816.26, wps=2837.9, ups=0.34, wpb=8426.9, bsz=256, num_updates=3700, lr=0.000370026, gnorm=1.819, clip=0, loss_scale=13870, train_wall=262, wall=19587
2022-07-29 20:27:51 | INFO | train_inner | epoch 005:    613 / 797 loss=9.596, nll_loss=5.188, mask_ins=1.469, word_ins_ml=6.588, word_reposition=0.791, kpe=0.748, ppl=773.87, wps=2883.8, ups=0.34, wpb=8449.5, bsz=256, num_updates=3800, lr=0.000380024, gnorm=1.735, clip=0, loss_scale=8192, train_wall=259, wall=19880
2022-07-29 20:32:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-29 20:32:46 | INFO | train_inner | epoch 005:    714 / 797 loss=9.551, nll_loss=5.144, mask_ins=1.472, word_ins_ml=6.55, word_reposition=0.777, kpe=0.751, ppl=749.89, wps=2853.7, ups=0.34, wpb=8435.6, bsz=256, num_updates=3900, lr=0.000390022, gnorm=1.865, clip=0, loss_scale=7786, train_wall=261, wall=20175
2022-07-29 20:36:49 | INFO | train | epoch 005 | loss nan | nll_loss 5.272 | mask_ins 1.489 | word_ins_ml 6.66 | word_reposition 0.798 | kpe nan | ppl nan | wps 2763 | ups 0.33 | wpb 8439.8 | bsz 256 | num_updates 3983 | lr 0.00039832 | gnorm 1.832 | clip 0 | loss_scale 8608 | train_wall 2091 | wall 20418
2022-07-29 20:37:50 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 14.362 | nll_loss 9.58 | mask_ins 1.593 | word_ins_ml 10.599 | word_reposition 1.026 | kpe 1.144 | ppl 21061.2 | wps 5449.6 | wpb 934 | bsz 32 | num_updates 3983 | best_loss 14.28
2022-07-29 20:37:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 5 @ 3983 updates, score 14.362) (writing took 3.5661409683525562 seconds)
2022-07-29 20:38:43 | INFO | train_inner | epoch 006:     17 / 797 loss=nan, nll_loss=5.106, mask_ins=1.478, word_ins_ml=6.517, word_reposition=0.775, kpe=nan, ppl=nan, wps=2373.1, ups=0.28, wpb=8458.3, bsz=255.7, num_updates=4000, lr=0.00040002, gnorm=1.799, clip=0, loss_scale=4096, train_wall=258, wall=20532
2022-07-29 20:43:38 | INFO | train_inner | epoch 006:    117 / 797 loss=nan, nll_loss=5.008, mask_ins=1.45, word_ins_ml=6.432, word_reposition=0.777, kpe=nan, ppl=nan, wps=2884.2, ups=0.34, wpb=8498.5, bsz=256, num_updates=4100, lr=0.000410018, gnorm=1.793, clip=0, loss_scale=4096, train_wall=261, wall=20826
2022-07-29 20:48:31 | INFO | train_inner | epoch 006:    217 / 797 loss=9.318, nll_loss=4.946, mask_ins=1.441, word_ins_ml=6.378, word_reposition=0.764, kpe=0.735, ppl=638.11, wps=2884.7, ups=0.34, wpb=8461.6, bsz=256, num_updates=4200, lr=0.000420016, gnorm=1.793, clip=0, loss_scale=4096, train_wall=259, wall=21120
2022-07-29 20:53:44 | INFO | train_inner | epoch 006:    317 / 797 loss=nan, nll_loss=4.92, mask_ins=1.435, word_ins_ml=6.354, word_reposition=0.754, kpe=nan, ppl=nan, wps=2695.1, ups=0.32, wpb=8428.8, bsz=256, num_updates=4300, lr=0.000430014, gnorm=1.744, clip=0, loss_scale=4096, train_wall=279, wall=21432
2022-07-29 20:58:38 | INFO | train_inner | epoch 006:    417 / 797 loss=nan, nll_loss=4.912, mask_ins=1.437, word_ins_ml=6.348, word_reposition=0.774, kpe=nan, ppl=nan, wps=2887.4, ups=0.34, wpb=8500.6, bsz=256, num_updates=4400, lr=0.000440012, gnorm=1.697, clip=0, loss_scale=4096, train_wall=260, wall=21727
2022-07-29 21:03:31 | INFO | train_inner | epoch 006:    517 / 797 loss=9.155, nll_loss=4.814, mask_ins=1.412, word_ins_ml=6.261, word_reposition=0.746, kpe=0.736, ppl=569.93, wps=2867.2, ups=0.34, wpb=8403, bsz=256, num_updates=4500, lr=0.00045001, gnorm=1.742, clip=0, loss_scale=8110, train_wall=259, wall=22020
2022-07-29 21:08:24 | INFO | train_inner | epoch 006:    617 / 797 loss=9.152, nll_loss=4.818, mask_ins=1.407, word_ins_ml=6.265, word_reposition=0.744, kpe=0.736, ppl=568.98, wps=2868.1, ups=0.34, wpb=8399.6, bsz=256, num_updates=4600, lr=0.000460008, gnorm=1.731, clip=0, loss_scale=8192, train_wall=259, wall=22313
2022-07-29 21:13:16 | INFO | train_inner | epoch 006:    717 / 797 loss=9.139, nll_loss=4.79, mask_ins=1.408, word_ins_ml=6.239, word_reposition=0.749, kpe=0.742, ppl=563.77, wps=2891.8, ups=0.34, wpb=8439.4, bsz=256, num_updates=4700, lr=0.000470006, gnorm=1.647, clip=0, loss_scale=8192, train_wall=258, wall=22605
2022-07-29 21:17:09 | INFO | train | epoch 006 | loss nan | nll_loss 4.873 | mask_ins 1.425 | word_ins_ml 6.313 | word_reposition 0.757 | kpe nan | ppl nan | wps 2779.9 | ups 0.33 | wpb 8439.5 | bsz 256 | num_updates 4780 | lr 0.000478004 | gnorm 1.734 | clip 0 | loss_scale 6039 | train_wall 2085 | wall 22837
2022-07-29 21:18:09 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 14.476 | nll_loss 9.651 | mask_ins 1.627 | word_ins_ml 10.657 | word_reposition 0.943 | kpe 1.249 | ppl 22795 | wps 5473.6 | wpb 934 | bsz 32 | num_updates 4780 | best_loss 14.28
2022-07-29 21:18:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 6 @ 4780 updates, score 14.476) (writing took 3.634148009121418 seconds)
2022-07-29 21:19:11 | INFO | train_inner | epoch 007:     20 / 797 loss=9.042, nll_loss=4.707, mask_ins=1.4, word_ins_ml=6.167, word_reposition=0.738, kpe=0.737, ppl=527.11, wps=2358.2, ups=0.28, wpb=8383.9, bsz=255.7, num_updates=4800, lr=0.000480004, gnorm=1.712, clip=0, loss_scale=8192, train_wall=258, wall=22960
2022-07-29 21:24:04 | INFO | train_inner | epoch 007:    120 / 797 loss=nan, nll_loss=4.662, mask_ins=1.393, word_ins_ml=6.129, word_reposition=0.742, kpe=nan, ppl=nan, wps=2891.6, ups=0.34, wpb=8467.6, bsz=256, num_updates=4900, lr=0.000490002, gnorm=1.695, clip=0, loss_scale=8192, train_wall=259, wall=23253
2022-07-29 21:28:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-29 21:29:01 | INFO | train_inner | epoch 007:    221 / 797 loss=nan, nll_loss=4.668, mask_ins=1.39, word_ins_ml=6.133, word_reposition=0.742, kpe=nan, ppl=nan, wps=2858.2, ups=0.34, wpb=8471.6, bsz=256, num_updates=5000, lr=0.0005, gnorm=1.758, clip=0, loss_scale=13626, train_wall=262, wall=23549
2022-07-29 21:33:54 | INFO | train_inner | epoch 007:    321 / 797 loss=8.956, nll_loss=4.655, mask_ins=1.37, word_ins_ml=6.121, word_reposition=0.733, kpe=0.732, ppl=496.7, wps=2896.8, ups=0.34, wpb=8500.2, bsz=256, num_updates=5100, lr=0.000495074, gnorm=1.623, clip=0, loss_scale=8192, train_wall=259, wall=23843
2022-07-29 21:39:13 | INFO | train_inner | epoch 007:    421 / 797 loss=nan, nll_loss=4.614, mask_ins=1.37, word_ins_ml=6.086, word_reposition=0.729, kpe=nan, ppl=nan, wps=2633.2, ups=0.31, wpb=8399.7, bsz=256, num_updates=5200, lr=0.00049029, gnorm=1.674, clip=0, loss_scale=8192, train_wall=285, wall=24162
2022-07-29 21:44:07 | INFO | train_inner | epoch 007:    521 / 797 loss=8.887, nll_loss=4.604, mask_ins=1.358, word_ins_ml=6.077, word_reposition=0.724, kpe=0.728, ppl=473.57, wps=2868.5, ups=0.34, wpb=8424.6, bsz=256, num_updates=5300, lr=0.000485643, gnorm=1.633, clip=0, loss_scale=8192, train_wall=260, wall=24455
2022-07-29 21:47:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-29 21:49:02 | INFO | train_inner | epoch 007:    622 / 797 loss=8.855, nll_loss=4.563, mask_ins=1.363, word_ins_ml=6.039, word_reposition=0.724, kpe=0.729, ppl=463.11, wps=2837.9, ups=0.34, wpb=8391.5, bsz=256, num_updates=5400, lr=0.000481125, gnorm=1.656, clip=0, loss_scale=7138, train_wall=262, wall=24751
2022-07-29 21:53:57 | INFO | train_inner | epoch 007:    722 / 797 loss=8.822, nll_loss=4.551, mask_ins=1.34, word_ins_ml=6.029, word_reposition=0.718, kpe=0.734, ppl=452.47, wps=2867.3, ups=0.34, wpb=8454.9, bsz=256, num_updates=5500, lr=0.000476731, gnorm=1.6, clip=0, loss_scale=4096, train_wall=261, wall=25046
2022-07-29 21:57:37 | INFO | train | epoch 007 | loss nan | nll_loss 4.607 | mask_ins 1.369 | word_ins_ml 6.079 | word_reposition 0.728 | kpe nan | ppl nan | wps 2763.2 | ups 0.33 | wpb 8440.1 | bsz 256 | num_updates 5575 | lr 0.000473514 | gnorm 1.661 | clip 0 | loss_scale 7848 | train_wall 2093 | wall 25266
2022-07-29 21:58:37 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 14.381 | nll_loss 9.54 | mask_ins 1.612 | word_ins_ml 10.564 | word_reposition 0.98 | kpe 1.225 | ppl 21331.9 | wps 5487.1 | wpb 934 | bsz 32 | num_updates 5575 | best_loss 14.28
2022-07-29 21:58:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 7 @ 5575 updates, score 14.381) (writing took 3.638562323525548 seconds)
2022-07-29 21:59:54 | INFO | train_inner | epoch 008:     25 / 797 loss=nan, nll_loss=4.471, mask_ins=1.356, word_ins_ml=5.958, word_reposition=0.701, kpe=nan, ppl=nan, wps=2357, ups=0.28, wpb=8404.5, bsz=255.7, num_updates=5600, lr=0.000472456, gnorm=1.636, clip=0, loss_scale=4096, train_wall=259, wall=25403
2022-07-29 22:04:48 | INFO | train_inner | epoch 008:    125 / 797 loss=nan, nll_loss=4.474, mask_ins=1.356, word_ins_ml=5.961, word_reposition=0.721, kpe=nan, ppl=nan, wps=2890.2, ups=0.34, wpb=8497, bsz=256, num_updates=5700, lr=0.000468293, gnorm=1.587, clip=0, loss_scale=4096, train_wall=260, wall=25697
2022-07-29 22:09:41 | INFO | train_inner | epoch 008:    225 / 797 loss=8.659, nll_loss=4.417, mask_ins=1.335, word_ins_ml=5.911, word_reposition=0.704, kpe=0.71, ppl=404.16, wps=2872.3, ups=0.34, wpb=8414, bsz=256, num_updates=5800, lr=0.000464238, gnorm=1.614, clip=0, loss_scale=4096, train_wall=259, wall=25990
2022-07-29 22:14:33 | INFO | train_inner | epoch 008:    325 / 797 loss=8.595, nll_loss=4.365, mask_ins=1.335, word_ins_ml=5.864, word_reposition=0.687, kpe=0.709, ppl=386.68, wps=2854.9, ups=0.34, wpb=8340.1, bsz=256, num_updates=5900, lr=0.000460287, gnorm=1.612, clip=0, loss_scale=4669, train_wall=258, wall=26282
2022-07-29 22:19:51 | INFO | train_inner | epoch 008:    425 / 797 loss=8.649, nll_loss=4.419, mask_ins=1.325, word_ins_ml=5.912, word_reposition=0.693, kpe=0.719, ppl=401.44, wps=2674, ups=0.31, wpb=8503.3, bsz=256, num_updates=6000, lr=0.000456435, gnorm=1.579, clip=0, loss_scale=8192, train_wall=284, wall=26600
2022-07-29 22:24:45 | INFO | train_inner | epoch 008:    525 / 797 loss=8.57, nll_loss=4.343, mask_ins=1.32, word_ins_ml=5.844, word_reposition=0.697, kpe=0.709, ppl=380.09, wps=2866.2, ups=0.34, wpb=8419.1, bsz=256, num_updates=6100, lr=0.000452679, gnorm=1.568, clip=0, loss_scale=8192, train_wall=259, wall=26893
2022-07-29 22:29:39 | INFO | train_inner | epoch 008:    625 / 797 loss=nan, nll_loss=4.361, mask_ins=1.317, word_ins_ml=5.861, word_reposition=0.692, kpe=nan, ppl=nan, wps=2868.7, ups=0.34, wpb=8430.9, bsz=256, num_updates=6200, lr=0.000449013, gnorm=1.612, clip=0, loss_scale=8192, train_wall=260, wall=27187
2022-07-29 22:34:31 | INFO | train_inner | epoch 008:    725 / 797 loss=nan, nll_loss=4.366, mask_ins=1.311, word_ins_ml=5.865, word_reposition=0.691, kpe=nan, ppl=nan, wps=2891, ups=0.34, wpb=8466, bsz=256, num_updates=6300, lr=0.000445435, gnorm=1.544, clip=0, loss_scale=8192, train_wall=259, wall=27480
2022-07-29 22:38:03 | INFO | train | epoch 008 | loss nan | nll_loss 4.388 | mask_ins 1.327 | word_ins_ml 5.885 | word_reposition 0.696 | kpe nan | ppl nan | wps 2772.6 | ups 0.33 | wpb 8439.5 | bsz 256 | num_updates 6372 | lr 0.000442912 | gnorm 1.589 | clip 0 | loss_scale 6594 | train_wall 2091 | wall 27692
2022-07-29 22:39:04 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 14.214 | nll_loss 9.406 | mask_ins 1.607 | word_ins_ml 10.437 | word_reposition 0.972 | kpe 1.198 | ppl 18999.4 | wps 5441.7 | wpb 934 | bsz 32 | num_updates 6372 | best_loss 14.214
2022-07-29 22:39:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_best.pt (epoch 8 @ 6372 updates, score 14.214) (writing took 5.440590561367571 seconds)
2022-07-29 22:40:31 | INFO | train_inner | epoch 009:     28 / 797 loss=8.53, nll_loss=4.33, mask_ins=1.305, word_ins_ml=5.831, word_reposition=0.686, kpe=0.707, ppl=369.58, wps=2352.9, ups=0.28, wpb=8460.1, bsz=255.7, num_updates=6400, lr=0.000441942, gnorm=1.597, clip=0, loss_scale=8356, train_wall=260, wall=27840
2022-07-29 22:41:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-29 22:45:27 | INFO | train_inner | epoch 009:    129 / 797 loss=8.442, nll_loss=4.245, mask_ins=1.301, word_ins_ml=5.757, word_reposition=0.69, kpe=0.694, ppl=347.78, wps=2862.9, ups=0.34, wpb=8485.3, bsz=256, num_updates=6500, lr=0.000438529, gnorm=1.567, clip=0, loss_scale=10382, train_wall=262, wall=28136
2022-07-29 22:50:21 | INFO | train_inner | epoch 009:    229 / 797 loss=nan, nll_loss=4.262, mask_ins=1.31, word_ins_ml=5.772, word_reposition=0.685, kpe=nan, ppl=nan, wps=2901.8, ups=0.34, wpb=8514.5, bsz=256, num_updates=6600, lr=0.000435194, gnorm=1.612, clip=0, loss_scale=8192, train_wall=259, wall=28430
2022-07-29 22:55:13 | INFO | train_inner | epoch 009:    329 / 797 loss=nan, nll_loss=4.247, mask_ins=1.298, word_ins_ml=5.758, word_reposition=0.689, kpe=nan, ppl=nan, wps=2891.5, ups=0.34, wpb=8457.1, bsz=256, num_updates=6700, lr=0.000431934, gnorm=1.595, clip=0, loss_scale=8192, train_wall=258, wall=28722
2022-07-29 23:00:05 | INFO | train_inner | epoch 009:    429 / 797 loss=8.408, nll_loss=4.221, mask_ins=1.297, word_ins_ml=5.736, word_reposition=0.677, kpe=0.698, ppl=339.61, wps=2879, ups=0.34, wpb=8402.6, bsz=256, num_updates=6800, lr=0.000428746, gnorm=1.535, clip=0, loss_scale=8192, train_wall=258, wall=29014
2022-07-29 23:05:23 | INFO | train_inner | epoch 009:    529 / 797 loss=8.415, nll_loss=4.227, mask_ins=1.303, word_ins_ml=5.74, word_reposition=0.675, kpe=0.697, ppl=341.24, wps=2658.7, ups=0.31, wpb=8456.3, bsz=256, num_updates=6900, lr=0.000425628, gnorm=1.554, clip=0, loss_scale=8192, train_wall=284, wall=29332
2022-07-29 23:07:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-29 23:10:20 | INFO | train_inner | epoch 009:    630 / 797 loss=nan, nll_loss=4.188, mask_ins=1.286, word_ins_ml=5.706, word_reposition=0.673, kpe=nan, ppl=nan, wps=2846.4, ups=0.34, wpb=8442.5, bsz=256, num_updates=7000, lr=0.000422577, gnorm=1.552, clip=0, loss_scale=8922, train_wall=262, wall=29629
2022-07-29 23:15:14 | INFO | train_inner | epoch 009:    730 / 797 loss=nan, nll_loss=4.153, mask_ins=1.277, word_ins_ml=5.675, word_reposition=0.66, kpe=nan, ppl=nan, wps=2837.2, ups=0.34, wpb=8350.1, bsz=256, num_updates=7100, lr=0.000419591, gnorm=1.567, clip=0, loss_scale=8192, train_wall=260, wall=29923
2022-07-29 23:18:30 | INFO | train | epoch 009 | loss nan | nll_loss 4.217 | mask_ins 1.294 | word_ins_ml 5.732 | word_reposition 0.677 | kpe nan | ppl nan | wps 2764.7 | ups 0.33 | wpb 8440.2 | bsz 256 | num_updates 7167 | lr 0.000417625 | gnorm 1.568 | clip 0 | loss_scale 8583 | train_wall 2089 | wall 30119
2022-07-29 23:19:30 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 14.362 | nll_loss 9.528 | mask_ins 1.59 | word_ins_ml 10.543 | word_reposition 1.014 | kpe 1.215 | ppl 21054.5 | wps 5494.3 | wpb 934 | bsz 32 | num_updates 7167 | best_loss 14.214
2022-07-29 23:19:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 9 @ 7167 updates, score 14.362) (writing took 3.5857257172465324 seconds)
2022-07-29 23:21:10 | INFO | train_inner | epoch 010:     33 / 797 loss=nan, nll_loss=4.141, mask_ins=1.274, word_ins_ml=5.663, word_reposition=0.661, kpe=nan, ppl=nan, wps=2355.7, ups=0.28, wpb=8393.2, bsz=255.7, num_updates=7200, lr=0.000416667, gnorm=1.514, clip=0, loss_scale=8192, train_wall=259, wall=30279
2022-07-29 23:26:05 | INFO | train_inner | epoch 010:    133 / 797 loss=8.241, nll_loss=4.1, mask_ins=1.269, word_ins_ml=5.628, word_reposition=0.665, kpe=0.678, ppl=302.44, wps=2866.2, ups=0.34, wpb=8433.5, bsz=256, num_updates=7300, lr=0.000413803, gnorm=1.554, clip=0, loss_scale=8192, train_wall=260, wall=30573
2022-07-29 23:30:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-29 23:31:00 | INFO | train_inner | epoch 010:    234 / 797 loss=8.239, nll_loss=4.088, mask_ins=1.281, word_ins_ml=5.617, word_reposition=0.659, kpe=0.682, ppl=302.2, wps=2873.7, ups=0.34, wpb=8500.1, bsz=256, num_updates=7400, lr=0.000410997, gnorm=1.593, clip=0, loss_scale=7705, train_wall=262, wall=30869
2022-07-29 23:35:53 | INFO | train_inner | epoch 010:    334 / 797 loss=8.216, nll_loss=4.074, mask_ins=1.271, word_ins_ml=5.605, word_reposition=0.659, kpe=0.68, ppl=297.27, wps=2862.4, ups=0.34, wpb=8386.1, bsz=256, num_updates=7500, lr=0.000408248, gnorm=1.558, clip=0, loss_scale=4096, train_wall=259, wall=31162
2022-07-29 23:40:46 | INFO | train_inner | epoch 010:    434 / 797 loss=8.206, nll_loss=4.071, mask_ins=1.266, word_ins_ml=5.601, word_reposition=0.66, kpe=0.679, ppl=295.39, wps=2873.5, ups=0.34, wpb=8392.5, bsz=256, num_updates=7600, lr=0.000405554, gnorm=1.523, clip=0, loss_scale=4096, train_wall=258, wall=31454
2022-07-29 23:45:39 | INFO | train_inner | epoch 010:    534 / 797 loss=nan, nll_loss=4.058, mask_ins=1.268, word_ins_ml=5.59, word_reposition=0.66, kpe=nan, ppl=nan, wps=2885, ups=0.34, wpb=8468, bsz=256, num_updates=7700, lr=0.000402911, gnorm=1.553, clip=0, loss_scale=4096, train_wall=260, wall=31748
2022-07-29 23:50:57 | INFO | train_inner | epoch 010:    634 / 797 loss=nan, nll_loss=4.073, mask_ins=1.262, word_ins_ml=5.602, word_reposition=0.664, kpe=nan, ppl=nan, wps=2665, ups=0.31, wpb=8467.9, bsz=256, num_updates=7800, lr=0.00040032, gnorm=1.527, clip=0, loss_scale=4096, train_wall=283, wall=32066
2022-07-29 23:55:50 | INFO | train_inner | epoch 010:    734 / 797 loss=nan, nll_loss=4.059, mask_ins=1.275, word_ins_ml=5.589, word_reposition=0.657, kpe=nan, ppl=nan, wps=2887.2, ups=0.34, wpb=8468.2, bsz=256, num_updates=7900, lr=0.000397779, gnorm=1.528, clip=0, loss_scale=4096, train_wall=259, wall=32359
2022-07-29 23:58:54 | INFO | train | epoch 010 | loss nan | nll_loss 4.074 | mask_ins 1.269 | word_ins_ml 5.604 | word_reposition 0.66 | kpe nan | ppl nan | wps 2771.1 | ups 0.33 | wpb 8439.5 | bsz 256 | num_updates 7963 | lr 0.000396202 | gnorm 1.546 | clip 0 | loss_scale 5561 | train_wall 2090 | wall 32543
2022-07-29 23:59:55 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 14.379 | nll_loss 9.486 | mask_ins 1.619 | word_ins_ml 10.495 | word_reposition 0.997 | kpe 1.269 | ppl 21307.5 | wps 5483.7 | wpb 934 | bsz 32 | num_updates 7963 | best_loss 14.214
2022-07-29 23:59:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 10 @ 7963 updates, score 14.379) (writing took 3.4539222000166774 seconds)
2022-07-30 00:01:46 | INFO | train_inner | epoch 011:     37 / 797 loss=nan, nll_loss=4.016, mask_ins=1.256, word_ins_ml=5.552, word_reposition=0.648, kpe=nan, ppl=nan, wps=2352.6, ups=0.28, wpb=8381, bsz=255.7, num_updates=8000, lr=0.000395285, gnorm=1.541, clip=0, loss_scale=8192, train_wall=259, wall=32715
2022-07-30 00:04:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-30 00:06:43 | INFO | train_inner | epoch 011:    138 / 797 loss=8.081, nll_loss=3.978, mask_ins=1.25, word_ins_ml=5.518, word_reposition=0.651, kpe=0.663, ppl=270.88, wps=2850.3, ups=0.34, wpb=8446.6, bsz=256, num_updates=8100, lr=0.000392837, gnorm=1.534, clip=0, loss_scale=6245, train_wall=262, wall=33011
2022-07-30 00:11:36 | INFO | train_inner | epoch 011:    238 / 797 loss=8.076, nll_loss=3.97, mask_ins=1.249, word_ins_ml=5.511, word_reposition=0.65, kpe=0.666, ppl=269.93, wps=2870.2, ups=0.34, wpb=8411.9, bsz=256, num_updates=8200, lr=0.000390434, gnorm=1.571, clip=0, loss_scale=4096, train_wall=259, wall=33305
2022-07-30 00:16:28 | INFO | train_inner | epoch 011:    338 / 797 loss=8.057, nll_loss=3.963, mask_ins=1.235, word_ins_ml=5.505, word_reposition=0.647, kpe=0.67, ppl=266.38, wps=2889.8, ups=0.34, wpb=8448.4, bsz=256, num_updates=8300, lr=0.000388075, gnorm=1.521, clip=0, loss_scale=4096, train_wall=258, wall=33597
2022-07-30 00:21:21 | INFO | train_inner | epoch 011:    438 / 797 loss=8.082, nll_loss=3.969, mask_ins=1.249, word_ins_ml=5.51, word_reposition=0.651, kpe=0.672, ppl=270.96, wps=2900.3, ups=0.34, wpb=8486.6, bsz=256, num_updates=8400, lr=0.000385758, gnorm=1.575, clip=0, loss_scale=4096, train_wall=259, wall=33889
2022-07-30 00:26:14 | INFO | train_inner | epoch 011:    538 / 797 loss=8.11, nll_loss=4.016, mask_ins=1.245, word_ins_ml=5.551, word_reposition=0.641, kpe=0.674, ppl=276.32, wps=2894.4, ups=0.34, wpb=8486.4, bsz=256, num_updates=8500, lr=0.000383482, gnorm=1.583, clip=0, loss_scale=4096, train_wall=259, wall=34183
2022-07-30 00:31:25 | INFO | train_inner | epoch 011:    638 / 797 loss=nan, nll_loss=3.973, mask_ins=1.248, word_ins_ml=5.513, word_reposition=0.644, kpe=nan, ppl=nan, wps=2711.4, ups=0.32, wpb=8444.1, bsz=256, num_updates=8600, lr=0.000381246, gnorm=1.534, clip=0, loss_scale=5571, train_wall=277, wall=34494
2022-07-30 00:36:27 | INFO | train_inner | epoch 011:    738 / 797 loss=nan, nll_loss=3.939, mask_ins=1.235, word_ins_ml=5.483, word_reposition=0.636, kpe=nan, ppl=nan, wps=2784.2, ups=0.33, wpb=8383.2, bsz=256, num_updates=8700, lr=0.000379049, gnorm=1.572, clip=0, loss_scale=8192, train_wall=267, wall=34795
2022-07-30 00:37:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-30 00:39:20 | INFO | train | epoch 011 | loss nan | nll_loss 3.969 | mask_ins 1.245 | word_ins_ml 5.51 | word_reposition 0.645 | kpe nan | ppl nan | wps 2766.1 | ups 0.33 | wpb 8439.8 | bsz 256 | num_updates 8758 | lr 0.000377792 | gnorm 1.553 | clip 0 | loss_scale 5381 | train_wall 2090 | wall 34969
2022-07-30 00:40:20 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 14.456 | nll_loss 9.536 | mask_ins 1.609 | word_ins_ml 10.55 | word_reposition 1.031 | kpe 1.267 | ppl 22473.6 | wps 5468.2 | wpb 934 | bsz 32 | num_updates 8758 | best_loss 14.214
2022-07-30 00:40:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 11 @ 8758 updates, score 14.456) (writing took 3.7349746711552143 seconds)
2022-07-30 00:42:28 | INFO | train_inner | epoch 012:     42 / 797 loss=nan, nll_loss=3.888, mask_ins=1.238, word_ins_ml=5.437, word_reposition=0.644, kpe=nan, ppl=nan, wps=2337.3, ups=0.28, wpb=8441, bsz=255.7, num_updates=8800, lr=0.000376889, gnorm=1.563, clip=0, loss_scale=5069, train_wall=263, wall=35156
2022-07-30 00:47:21 | INFO | train_inner | epoch 012:    142 / 797 loss=nan, nll_loss=3.896, mask_ins=1.227, word_ins_ml=5.445, word_reposition=0.639, kpe=nan, ppl=nan, wps=2878.2, ups=0.34, wpb=8434.6, bsz=256, num_updates=8900, lr=0.000374766, gnorm=1.567, clip=0, loss_scale=4096, train_wall=259, wall=35449
2022-07-30 00:52:15 | INFO | train_inner | epoch 012:    242 / 797 loss=7.918, nll_loss=3.86, mask_ins=1.227, word_ins_ml=5.413, word_reposition=0.627, kpe=0.652, ppl=241.8, wps=2867.2, ups=0.34, wpb=8431.3, bsz=256, num_updates=9000, lr=0.000372678, gnorm=1.578, clip=0, loss_scale=4096, train_wall=260, wall=35744
2022-07-30 00:57:09 | INFO | train_inner | epoch 012:    342 / 797 loss=7.941, nll_loss=3.871, mask_ins=1.232, word_ins_ml=5.422, word_reposition=0.633, kpe=0.654, ppl=245.82, wps=2866.1, ups=0.34, wpb=8422.5, bsz=256, num_updates=9100, lr=0.000370625, gnorm=1.559, clip=0, loss_scale=4096, train_wall=260, wall=36037
2022-07-30 01:02:03 | INFO | train_inner | epoch 012:    442 / 797 loss=7.92, nll_loss=3.848, mask_ins=1.232, word_ins_ml=5.402, word_reposition=0.632, kpe=0.655, ppl=242.23, wps=2864.8, ups=0.34, wpb=8419.9, bsz=256, num_updates=9200, lr=0.000368605, gnorm=1.559, clip=0, loss_scale=4096, train_wall=260, wall=36331
2022-07-30 01:06:54 | INFO | train_inner | epoch 012:    542 / 797 loss=nan, nll_loss=3.861, mask_ins=1.235, word_ins_ml=5.412, word_reposition=0.631, kpe=nan, ppl=nan, wps=2896.1, ups=0.34, wpb=8434.8, bsz=256, num_updates=9300, lr=0.000366618, gnorm=1.619, clip=0, loss_scale=6758, train_wall=257, wall=36623
2022-07-30 01:11:45 | INFO | train_inner | epoch 012:    642 / 797 loss=nan, nll_loss=3.859, mask_ins=1.223, word_ins_ml=5.411, word_reposition=0.632, kpe=nan, ppl=nan, wps=2893, ups=0.34, wpb=8433, bsz=256, num_updates=9400, lr=0.000364662, gnorm=1.592, clip=0, loss_scale=8192, train_wall=258, wall=36914
2022-07-30 01:17:08 | INFO | train_inner | epoch 012:    742 / 797 loss=7.952, nll_loss=3.889, mask_ins=1.221, word_ins_ml=5.437, word_reposition=0.63, kpe=0.664, ppl=247.58, wps=2635.6, ups=0.31, wpb=8492, bsz=256, num_updates=9500, lr=0.000362738, gnorm=1.567, clip=0, loss_scale=8192, train_wall=288, wall=37236
2022-07-30 01:19:49 | INFO | train | epoch 012 | loss nan | nll_loss 3.868 | mask_ins 1.228 | word_ins_ml 5.419 | word_reposition 0.633 | kpe nan | ppl nan | wps 2769.1 | ups 0.33 | wpb 8439.5 | bsz 256 | num_updates 9555 | lr 0.000361693 | gnorm 1.58 | clip 0 | loss_scale 5741 | train_wall 2094 | wall 37398
2022-07-30 01:20:49 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 14.495 | nll_loss 9.504 | mask_ins 1.632 | word_ins_ml 10.525 | word_reposition 1.062 | kpe 1.277 | ppl 23098 | wps 5471.5 | wpb 934 | bsz 32 | num_updates 9555 | best_loss 14.214
2022-07-30 01:20:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 12 @ 9555 updates, score 14.495) (writing took 3.5554858036339283 seconds)
2022-07-30 01:23:04 | INFO | train_inner | epoch 013:     45 / 797 loss=7.908, nll_loss=3.841, mask_ins=1.226, word_ins_ml=5.395, word_reposition=0.633, kpe=0.653, ppl=240.24, wps=2369.2, ups=0.28, wpb=8455.1, bsz=255.7, num_updates=9600, lr=0.000360844, gnorm=1.603, clip=0, loss_scale=8192, train_wall=259, wall=37593
2022-07-30 01:27:58 | INFO | train_inner | epoch 013:    145 / 797 loss=7.806, nll_loss=3.772, mask_ins=1.214, word_ins_ml=5.333, word_reposition=0.617, kpe=0.641, ppl=223.74, wps=2872.4, ups=0.34, wpb=8440, bsz=256, num_updates=9700, lr=0.000358979, gnorm=1.606, clip=0, loss_scale=8192, train_wall=260, wall=37887
2022-07-30 01:29:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-30 01:32:54 | INFO | train_inner | epoch 013:    246 / 797 loss=7.845, nll_loss=3.802, mask_ins=1.214, word_ins_ml=5.36, word_reposition=0.625, kpe=0.645, ppl=229.84, wps=2855.9, ups=0.34, wpb=8449.1, bsz=256, num_updates=9800, lr=0.000357143, gnorm=1.573, clip=0, loss_scale=5069, train_wall=262, wall=38183
2022-07-30 01:37:47 | INFO | train_inner | epoch 013:    346 / 797 loss=nan, nll_loss=3.776, mask_ins=1.209, word_ins_ml=5.337, word_reposition=0.625, kpe=nan, ppl=nan, wps=2871.3, ups=0.34, wpb=8404.7, bsz=256, num_updates=9900, lr=0.000355335, gnorm=1.567, clip=0, loss_scale=4096, train_wall=259, wall=38476
2022-07-30 01:42:47 | INFO | train_inner | epoch 013:    446 / 797 loss=7.846, nll_loss=3.816, mask_ins=1.214, word_ins_ml=5.372, word_reposition=0.618, kpe=0.643, ppl=230.13, wps=2797.8, ups=0.33, wpb=8407, bsz=256, num_updates=10000, lr=0.000353553, gnorm=1.573, clip=0, loss_scale=4096, train_wall=265, wall=38776
2022-07-30 01:47:42 | INFO | train_inner | epoch 013:    546 / 797 loss=7.828, nll_loss=3.771, mask_ins=1.216, word_ins_ml=5.332, word_reposition=0.631, kpe=0.649, ppl=227.21, wps=2872.9, ups=0.34, wpb=8464.5, bsz=256, num_updates=10100, lr=0.000351799, gnorm=1.598, clip=0, loss_scale=4096, train_wall=260, wall=39071
2022-07-30 01:52:34 | INFO | train_inner | epoch 013:    646 / 797 loss=nan, nll_loss=3.782, mask_ins=1.212, word_ins_ml=5.341, word_reposition=0.621, kpe=nan, ppl=nan, wps=2895.1, ups=0.34, wpb=8456.8, bsz=256, num_updates=10200, lr=0.00035007, gnorm=1.591, clip=0, loss_scale=4096, train_wall=258, wall=39363
2022-07-30 01:57:27 | INFO | train_inner | epoch 013:    746 / 797 loss=nan, nll_loss=3.808, mask_ins=1.207, word_ins_ml=5.363, word_reposition=0.625, kpe=nan, ppl=nan, wps=2891.2, ups=0.34, wpb=8478.6, bsz=256, num_updates=10300, lr=0.000348367, gnorm=1.625, clip=0, loss_scale=6758, train_wall=259, wall=39656
2022-07-30 02:00:17 | INFO | train | epoch 013 | loss nan | nll_loss 3.785 | mask_ins 1.212 | word_ins_ml 5.345 | word_reposition 0.623 | kpe nan | ppl nan | wps 2767.2 | ups 0.33 | wpb 8440.6 | bsz 256 | num_updates 10351 | lr 0.000347507 | gnorm 1.589 | clip 0 | loss_scale 5561 | train_wall 2092 | wall 39826
2022-07-30 02:01:18 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 14.462 | nll_loss 9.539 | mask_ins 1.629 | word_ins_ml 10.557 | word_reposition 1.01 | kpe 1.266 | ppl 22562.5 | wps 5447.7 | wpb 934 | bsz 32 | num_updates 10351 | best_loss 14.214
2022-07-30 02:01:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 13 @ 10351 updates, score 14.462) (writing took 3.5987853603437543 seconds)
2022-07-30 02:03:47 | INFO | train_inner | epoch 014:     49 / 797 loss=nan, nll_loss=3.709, mask_ins=1.202, word_ins_ml=5.277, word_reposition=0.616, kpe=nan, ppl=nan, wps=2212.9, ups=0.26, wpb=8405.6, bsz=255.7, num_updates=10400, lr=0.000346688, gnorm=1.588, clip=0, loss_scale=8192, train_wall=282, wall=40036
2022-07-30 02:08:41 | INFO | train_inner | epoch 014:    149 / 797 loss=7.749, nll_loss=3.728, mask_ins=1.206, word_ins_ml=5.294, word_reposition=0.615, kpe=0.634, ppl=215.14, wps=2905, ups=0.34, wpb=8544.6, bsz=256, num_updates=10500, lr=0.000345033, gnorm=1.573, clip=0, loss_scale=8192, train_wall=260, wall=40330
2022-07-30 02:13:34 | INFO | train_inner | epoch 014:    249 / 797 loss=7.768, nll_loss=3.755, mask_ins=1.206, word_ins_ml=5.317, word_reposition=0.616, kpe=0.629, ppl=217.98, wps=2878, ups=0.34, wpb=8431.7, bsz=256, num_updates=10600, lr=0.000343401, gnorm=1.59, clip=0, loss_scale=8192, train_wall=259, wall=40623
2022-07-30 02:18:28 | INFO | train_inner | epoch 014:    349 / 797 loss=nan, nll_loss=3.728, mask_ins=1.222, word_ins_ml=5.294, word_reposition=0.616, kpe=nan, ppl=nan, wps=2882.2, ups=0.34, wpb=8451.2, bsz=256, num_updates=10700, lr=0.000341793, gnorm=1.589, clip=0, loss_scale=8192, train_wall=259, wall=40916
2022-07-30 02:21:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-30 02:23:23 | INFO | train_inner | epoch 014:    450 / 797 loss=nan, nll_loss=3.743, mask_ins=1.204, word_ins_ml=5.306, word_reposition=0.619, kpe=nan, ppl=nan, wps=2842.7, ups=0.34, wpb=8405.5, bsz=256, num_updates=10800, lr=0.000340207, gnorm=1.618, clip=0, loss_scale=8516, train_wall=261, wall=41212
2022-07-30 02:28:18 | INFO | train_inner | epoch 014:    550 / 797 loss=nan, nll_loss=3.661, mask_ins=1.184, word_ins_ml=5.233, word_reposition=0.609, kpe=nan, ppl=nan, wps=2837.7, ups=0.34, wpb=8360, bsz=256, num_updates=10900, lr=0.000338643, gnorm=1.6, clip=0, loss_scale=8192, train_wall=261, wall=41507
2022-07-30 02:33:12 | INFO | train_inner | epoch 014:    650 / 797 loss=7.753, nll_loss=3.729, mask_ins=1.2, word_ins_ml=5.293, word_reposition=0.62, kpe=0.64, ppl=215.74, wps=2871.9, ups=0.34, wpb=8456.7, bsz=256, num_updates=11000, lr=0.0003371, gnorm=1.596, clip=0, loss_scale=8192, train_wall=261, wall=41801
2022-07-30 02:38:06 | INFO | train_inner | epoch 014:    750 / 797 loss=nan, nll_loss=3.699, mask_ins=1.2, word_ins_ml=5.267, word_reposition=0.615, kpe=nan, ppl=nan, wps=2878.9, ups=0.34, wpb=8468.1, bsz=256, num_updates=11100, lr=0.000335578, gnorm=1.583, clip=0, loss_scale=8192, train_wall=260, wall=42095
2022-07-30 02:40:24 | INFO | train | epoch 014 | loss nan | nll_loss 3.717 | mask_ins 1.204 | word_ins_ml 5.283 | word_reposition 0.616 | kpe nan | ppl nan | wps 2791.7 | ups 0.33 | wpb 8440.2 | bsz 256 | num_updates 11147 | lr 0.00033487 | gnorm 1.595 | clip 0 | loss_scale 8233 | train_wall 2070 | wall 42232
2022-07-30 02:41:24 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 14.452 | nll_loss 9.502 | mask_ins 1.625 | word_ins_ml 10.521 | word_reposition 1.002 | kpe 1.303 | ppl 22404.6 | wps 5460.4 | wpb 934 | bsz 32 | num_updates 11147 | best_loss 14.214
2022-07-30 02:41:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 14 @ 11147 updates, score 14.452) (writing took 3.347257921472192 seconds)
2022-07-30 02:44:03 | INFO | train_inner | epoch 015:     53 / 797 loss=7.648, nll_loss=3.637, mask_ins=1.197, word_ins_ml=5.211, word_reposition=0.615, kpe=0.624, ppl=200.55, wps=2344, ups=0.28, wpb=8361.8, bsz=255.7, num_updates=11200, lr=0.000334077, gnorm=1.599, clip=0, loss_scale=8192, train_wall=259, wall=42452
2022-07-30 02:47:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-30 02:49:20 | INFO | train_inner | epoch 015:    154 / 797 loss=7.638, nll_loss=3.642, mask_ins=1.188, word_ins_ml=5.217, word_reposition=0.612, kpe=0.622, ppl=199.17, wps=2675.5, ups=0.32, wpb=8463, bsz=256, num_updates=11300, lr=0.000332595, gnorm=1.582, clip=0, loss_scale=8354, train_wall=281, wall=42768
2022-07-30 02:54:13 | INFO | train_inner | epoch 015:    254 / 797 loss=nan, nll_loss=3.648, mask_ins=1.191, word_ins_ml=5.222, word_reposition=0.613, kpe=nan, ppl=nan, wps=2880.2, ups=0.34, wpb=8438.1, bsz=256, num_updates=11400, lr=0.000331133, gnorm=1.595, clip=0, loss_scale=8192, train_wall=259, wall=43061
2022-07-30 02:59:06 | INFO | train_inner | epoch 015:    354 / 797 loss=nan, nll_loss=3.666, mask_ins=1.198, word_ins_ml=5.238, word_reposition=0.607, kpe=nan, ppl=nan, wps=2898, ups=0.34, wpb=8497.1, bsz=256, num_updates=11500, lr=0.00032969, gnorm=1.592, clip=0, loss_scale=8192, train_wall=259, wall=43354
2022-07-30 03:02:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-30 03:04:01 | INFO | train_inner | epoch 015:    455 / 797 loss=7.603, nll_loss=3.606, mask_ins=1.184, word_ins_ml=5.184, word_reposition=0.61, kpe=0.624, ppl=194.37, wps=2848.6, ups=0.34, wpb=8420.7, bsz=256, num_updates=11600, lr=0.000328266, gnorm=1.624, clip=0, loss_scale=6975, train_wall=261, wall=43650
2022-07-30 03:08:56 | INFO | train_inner | epoch 015:    555 / 797 loss=7.618, nll_loss=3.641, mask_ins=1.176, word_ins_ml=5.215, word_reposition=0.601, kpe=0.626, ppl=196.43, wps=2863.8, ups=0.34, wpb=8438.1, bsz=256, num_updates=11700, lr=0.00032686, gnorm=1.62, clip=0, loss_scale=4096, train_wall=260, wall=43945
2022-07-30 03:13:51 | INFO | train_inner | epoch 015:    655 / 797 loss=nan, nll_loss=3.645, mask_ins=1.187, word_ins_ml=5.218, word_reposition=0.609, kpe=nan, ppl=nan, wps=2869, ups=0.34, wpb=8464.4, bsz=256, num_updates=11800, lr=0.000325472, gnorm=1.611, clip=0, loss_scale=4096, train_wall=261, wall=44240
2022-07-30 03:18:44 | INFO | train_inner | epoch 015:    755 / 797 loss=nan, nll_loss=3.646, mask_ins=1.181, word_ins_ml=5.218, word_reposition=0.608, kpe=nan, ppl=nan, wps=2874.5, ups=0.34, wpb=8415.2, bsz=256, num_updates=11900, lr=0.000324102, gnorm=1.627, clip=0, loss_scale=4096, train_wall=259, wall=44533
2022-07-30 03:20:47 | INFO | train | epoch 015 | loss nan | nll_loss 3.638 | mask_ins 1.186 | word_ins_ml 5.213 | word_reposition 0.609 | kpe nan | ppl nan | wps 2769.1 | ups 0.33 | wpb 8440.1 | bsz 256 | num_updates 11942 | lr 0.000323531 | gnorm 1.608 | clip 0 | loss_scale 6301 | train_wall 2088 | wall 44655
2022-07-30 03:21:47 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 14.567 | nll_loss 9.576 | mask_ins 1.619 | word_ins_ml 10.589 | word_reposition 1.04 | kpe 1.318 | ppl 24264.2 | wps 5494.1 | wpb 934 | bsz 32 | num_updates 11942 | best_loss 14.214
2022-07-30 03:21:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 15 @ 11942 updates, score 14.567) (writing took 3.4950676197186112 seconds)
2022-07-30 03:24:40 | INFO | train_inner | epoch 016:     58 / 797 loss=nan, nll_loss=3.62, mask_ins=1.177, word_ins_ml=5.196, word_reposition=0.606, kpe=nan, ppl=nan, wps=2360.6, ups=0.28, wpb=8413.2, bsz=255.7, num_updates=12000, lr=0.000322749, gnorm=1.626, clip=0, loss_scale=4096, train_wall=259, wall=44889
2022-07-30 03:30:00 | INFO | train_inner | epoch 016:    158 / 797 loss=nan, nll_loss=3.587, mask_ins=1.154, word_ins_ml=5.166, word_reposition=0.591, kpe=nan, ppl=nan, wps=2642.1, ups=0.31, wpb=8442.5, bsz=256, num_updates=12100, lr=0.000321412, gnorm=1.624, clip=0, loss_scale=4833, train_wall=286, wall=45208
2022-07-30 03:31:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-30 03:34:57 | INFO | train_inner | epoch 016:    259 / 797 loss=7.548, nll_loss=3.569, mask_ins=1.183, word_ins_ml=5.151, word_reposition=0.6, kpe=0.615, ppl=187.2, wps=2839.5, ups=0.34, wpb=8442.6, bsz=256, num_updates=12200, lr=0.000320092, gnorm=1.622, clip=0, loss_scale=5678, train_wall=262, wall=45506
2022-07-30 03:39:52 | INFO | train_inner | epoch 016:    359 / 797 loss=7.533, nll_loss=3.562, mask_ins=1.172, word_ins_ml=5.145, word_reposition=0.604, kpe=0.611, ppl=185.23, wps=2867.5, ups=0.34, wpb=8444.4, bsz=256, num_updates=12300, lr=0.000318788, gnorm=1.607, clip=0, loss_scale=4096, train_wall=260, wall=45800
2022-07-30 03:44:44 | INFO | train_inner | epoch 016:    459 / 797 loss=7.525, nll_loss=3.567, mask_ins=1.175, word_ins_ml=5.149, word_reposition=0.588, kpe=0.613, ppl=184.14, wps=2871.2, ups=0.34, wpb=8408.8, bsz=256, num_updates=12400, lr=0.0003175, gnorm=1.668, clip=0, loss_scale=4096, train_wall=259, wall=46093
2022-07-30 03:49:39 | INFO | train_inner | epoch 016:    559 / 797 loss=7.544, nll_loss=3.566, mask_ins=1.174, word_ins_ml=5.146, word_reposition=0.606, kpe=0.618, ppl=186.66, wps=2871.9, ups=0.34, wpb=8466.8, bsz=256, num_updates=12500, lr=0.000316228, gnorm=1.626, clip=0, loss_scale=4096, train_wall=261, wall=46388
2022-07-30 03:54:34 | INFO | train_inner | epoch 016:    659 / 797 loss=nan, nll_loss=3.591, mask_ins=1.165, word_ins_ml=5.169, word_reposition=0.6, kpe=nan, ppl=nan, wps=2868.1, ups=0.34, wpb=8441.5, bsz=256, num_updates=12600, lr=0.00031497, gnorm=1.622, clip=0, loss_scale=4096, train_wall=260, wall=46682
2022-07-30 03:59:30 | INFO | train_inner | epoch 016:    759 / 797 loss=nan, nll_loss=3.593, mask_ins=1.174, word_ins_ml=5.171, word_reposition=0.604, kpe=nan, ppl=nan, wps=2857.7, ups=0.34, wpb=8457.1, bsz=256, num_updates=12700, lr=0.000313728, gnorm=1.623, clip=0, loss_scale=6144, train_wall=261, wall=46978
2022-07-30 04:01:21 | INFO | train | epoch 016 | loss nan | nll_loss 3.58 | mask_ins 1.172 | word_ins_ml 5.16 | word_reposition 0.6 | kpe nan | ppl nan | wps 2759.3 | ups 0.33 | wpb 8440 | bsz 256 | num_updates 12738 | lr 0.00031326 | gnorm 1.626 | clip 0 | loss_scale 4841 | train_wall 2099 | wall 47090
2022-07-30 04:02:22 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 14.543 | nll_loss 9.566 | mask_ins 1.615 | word_ins_ml 10.58 | word_reposition 1.031 | kpe 1.317 | ppl 23869.2 | wps 5431 | wpb 934 | bsz 32 | num_updates 12738 | best_loss 14.214
2022-07-30 04:02:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 16 @ 12738 updates, score 14.543) (writing took 3.257001342251897 seconds)
2022-07-30 04:05:28 | INFO | train_inner | epoch 017:     62 / 797 loss=7.484, nll_loss=3.535, mask_ins=1.162, word_ins_ml=5.12, word_reposition=0.595, kpe=0.607, ppl=179.01, wps=2353.9, ups=0.28, wpb=8427.5, bsz=255.7, num_updates=12800, lr=0.0003125, gnorm=1.625, clip=0, loss_scale=8192, train_wall=259, wall=47336
2022-07-30 04:10:22 | INFO | train_inner | epoch 017:    162 / 797 loss=7.417, nll_loss=3.492, mask_ins=1.15, word_ins_ml=5.081, word_reposition=0.584, kpe=0.602, ppl=170.87, wps=2868.4, ups=0.34, wpb=8432.1, bsz=256, num_updates=12900, lr=0.000311286, gnorm=1.632, clip=0, loss_scale=8192, train_wall=260, wall=47630
2022-07-30 04:15:42 | INFO | train_inner | epoch 017:    262 / 797 loss=7.476, nll_loss=3.522, mask_ins=1.166, word_ins_ml=5.108, word_reposition=0.596, kpe=0.605, ppl=178.02, wps=2622.6, ups=0.31, wpb=8416.1, bsz=256, num_updates=13000, lr=0.000310087, gnorm=1.644, clip=0, loss_scale=8192, train_wall=286, wall=47951
2022-07-30 04:20:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-30 04:20:39 | INFO | train_inner | epoch 017:    363 / 797 loss=nan, nll_loss=3.515, mask_ins=1.163, word_ins_ml=5.102, word_reposition=0.597, kpe=nan, ppl=nan, wps=2848, ups=0.34, wpb=8457, bsz=256, num_updates=13100, lr=0.000308901, gnorm=1.66, clip=0, loss_scale=7705, train_wall=262, wall=48248
2022-07-30 04:25:35 | INFO | train_inner | epoch 017:    463 / 797 loss=7.52, nll_loss=3.554, mask_ins=1.174, word_ins_ml=5.136, word_reposition=0.6, kpe=0.609, ppl=183.54, wps=2873.9, ups=0.34, wpb=8489.3, bsz=256, num_updates=13200, lr=0.000307729, gnorm=1.632, clip=0, loss_scale=4096, train_wall=261, wall=48544
2022-07-30 04:30:30 | INFO | train_inner | epoch 017:    563 / 797 loss=nan, nll_loss=3.546, mask_ins=1.16, word_ins_ml=5.128, word_reposition=0.596, kpe=nan, ppl=nan, wps=2875.8, ups=0.34, wpb=8488.9, bsz=256, num_updates=13300, lr=0.00030657, gnorm=1.633, clip=0, loss_scale=4096, train_wall=261, wall=48839
2022-07-30 04:35:25 | INFO | train_inner | epoch 017:    663 / 797 loss=nan, nll_loss=3.5, mask_ins=1.162, word_ins_ml=5.088, word_reposition=0.593, kpe=nan, ppl=nan, wps=2837.1, ups=0.34, wpb=8361.7, bsz=256, num_updates=13400, lr=0.000305424, gnorm=1.679, clip=0, loss_scale=4096, train_wall=260, wall=49133
2022-07-30 04:40:18 | INFO | train_inner | epoch 017:    763 / 797 loss=nan, nll_loss=3.514, mask_ins=1.164, word_ins_ml=5.1, word_reposition=0.592, kpe=nan, ppl=nan, wps=2886.5, ups=0.34, wpb=8460.9, bsz=256, num_updates=13500, lr=0.00030429, gnorm=1.63, clip=0, loss_scale=4096, train_wall=259, wall=49427
2022-07-30 04:41:57 | INFO | train | epoch 017 | loss nan | nll_loss 3.518 | mask_ins 1.162 | word_ins_ml 5.104 | word_reposition 0.594 | kpe nan | ppl nan | wps 2758.9 | ups 0.33 | wpb 8440.2 | bsz 256 | num_updates 13534 | lr 0.000303908 | gnorm 1.644 | clip 0 | loss_scale 5900 | train_wall 2096 | wall 49525
2022-07-30 04:42:57 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 14.612 | nll_loss 9.616 | mask_ins 1.627 | word_ins_ml 10.621 | word_reposition 1.036 | kpe 1.327 | ppl 25033.4 | wps 5483.3 | wpb 934 | bsz 32 | num_updates 13534 | best_loss 14.214
2022-07-30 04:43:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 17 @ 13534 updates, score 14.612) (writing took 3.680399026721716 seconds)
2022-07-30 04:46:13 | INFO | train_inner | epoch 018:     66 / 797 loss=7.403, nll_loss=3.464, mask_ins=1.16, word_ins_ml=5.057, word_reposition=0.591, kpe=0.596, ppl=169.28, wps=2369.9, ups=0.28, wpb=8425.8, bsz=255.7, num_updates=13600, lr=0.00030317, gnorm=1.677, clip=0, loss_scale=4096, train_wall=258, wall=49782
2022-07-30 04:49:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-30 04:51:10 | INFO | train_inner | epoch 018:    167 / 797 loss=nan, nll_loss=3.483, mask_ins=1.167, word_ins_ml=5.072, word_reposition=0.591, kpe=nan, ppl=nan, wps=2861.8, ups=0.34, wpb=8481.2, bsz=256, num_updates=13700, lr=0.000302061, gnorm=1.668, clip=0, loss_scale=7097, train_wall=262, wall=50079
2022-07-30 04:56:03 | INFO | train_inner | epoch 018:    267 / 797 loss=7.35, nll_loss=3.44, mask_ins=1.134, word_ins_ml=5.035, word_reposition=0.587, kpe=0.595, ppl=163.11, wps=2869.8, ups=0.34, wpb=8419, bsz=256, num_updates=13800, lr=0.000300965, gnorm=1.652, clip=0, loss_scale=4096, train_wall=260, wall=50372
2022-07-30 05:01:22 | INFO | train_inner | epoch 018:    367 / 797 loss=nan, nll_loss=3.479, mask_ins=1.15, word_ins_ml=5.069, word_reposition=0.579, kpe=nan, ppl=nan, wps=2636.9, ups=0.31, wpb=8397.9, bsz=256, num_updates=13900, lr=0.00029988, gnorm=1.679, clip=0, loss_scale=4096, train_wall=284, wall=50690
2022-07-30 05:06:14 | INFO | train_inner | epoch 018:    467 / 797 loss=7.393, nll_loss=3.472, mask_ins=1.144, word_ins_ml=5.062, word_reposition=0.589, kpe=0.599, ppl=168.09, wps=2894.7, ups=0.34, wpb=8474.5, bsz=256, num_updates=14000, lr=0.000298807, gnorm=1.669, clip=0, loss_scale=4096, train_wall=259, wall=50983
2022-07-30 05:11:09 | INFO | train_inner | epoch 018:    567 / 797 loss=nan, nll_loss=3.488, mask_ins=1.152, word_ins_ml=5.076, word_reposition=0.579, kpe=nan, ppl=nan, wps=2854.7, ups=0.34, wpb=8421.3, bsz=256, num_updates=14100, lr=0.000297746, gnorm=1.667, clip=0, loss_scale=4096, train_wall=260, wall=51278
2022-07-30 05:16:05 | INFO | train_inner | epoch 018:    667 / 797 loss=7.395, nll_loss=3.476, mask_ins=1.137, word_ins_ml=5.065, word_reposition=0.588, kpe=0.604, ppl=168.26, wps=2848.2, ups=0.34, wpb=8423.7, bsz=256, num_updates=14200, lr=0.000296695, gnorm=1.681, clip=0, loss_scale=4710, train_wall=261, wall=51574
2022-07-30 05:21:02 | INFO | train_inner | epoch 018:    767 / 797 loss=7.445, nll_loss=3.514, mask_ins=1.15, word_ins_ml=5.099, word_reposition=0.592, kpe=0.605, ppl=174.29, wps=2849.4, ups=0.34, wpb=8464.8, bsz=256, num_updates=14300, lr=0.000295656, gnorm=1.648, clip=0, loss_scale=8192, train_wall=262, wall=51871
2022-07-30 05:22:31 | INFO | train | epoch 018 | loss nan | nll_loss 3.476 | mask_ins 1.149 | word_ins_ml 5.066 | word_reposition 0.587 | kpe nan | ppl nan | wps 2759.5 | ups 0.33 | wpb 8439.8 | bsz 256 | num_updates 14330 | lr 0.000295347 | gnorm 1.667 | clip 0 | loss_scale 5222 | train_wall 2097 | wall 51960
2022-07-30 05:23:32 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 14.601 | nll_loss 9.572 | mask_ins 1.633 | word_ins_ml 10.587 | word_reposition 1.057 | kpe 1.323 | ppl 24847.2 | wps 5427.3 | wpb 934 | bsz 32 | num_updates 14330 | best_loss 14.214
2022-07-30 05:23:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 18 @ 14330 updates, score 14.601) (writing took 3.2726685144007206 seconds)
2022-07-30 05:23:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-30 05:27:04 | INFO | train_inner | epoch 019:     71 / 797 loss=nan, nll_loss=3.416, mask_ins=1.146, word_ins_ml=5.013, word_reposition=0.585, kpe=nan, ppl=nan, wps=2316.1, ups=0.28, wpb=8389.6, bsz=255.7, num_updates=14400, lr=0.000294628, gnorm=1.686, clip=0, loss_scale=5515, train_wall=263, wall=52233
2022-07-30 05:32:01 | INFO | train_inner | epoch 019:    171 / 797 loss=7.346, nll_loss=3.432, mask_ins=1.151, word_ins_ml=5.027, word_reposition=0.581, kpe=0.587, ppl=162.66, wps=2852.9, ups=0.34, wpb=8444.7, bsz=256, num_updates=14500, lr=0.00029361, gnorm=1.694, clip=0, loss_scale=4096, train_wall=261, wall=52529
2022-07-30 05:36:58 | INFO | train_inner | epoch 019:    271 / 797 loss=nan, nll_loss=3.436, mask_ins=1.133, word_ins_ml=5.03, word_reposition=0.586, kpe=nan, ppl=nan, wps=2843.4, ups=0.34, wpb=8452.1, bsz=256, num_updates=14600, lr=0.000292603, gnorm=1.67, clip=0, loss_scale=4096, train_wall=262, wall=52827
2022-07-30 05:42:17 | INFO | train_inner | epoch 019:    371 / 797 loss=nan, nll_loss=3.438, mask_ins=1.149, word_ins_ml=5.032, word_reposition=0.584, kpe=nan, ppl=nan, wps=2634.4, ups=0.31, wpb=8415.6, bsz=256, num_updates=14700, lr=0.000291606, gnorm=1.689, clip=0, loss_scale=4096, train_wall=284, wall=53146
2022-07-30 05:47:20 | INFO | train_inner | epoch 019:    471 / 797 loss=7.342, nll_loss=3.434, mask_ins=1.138, word_ins_ml=5.027, word_reposition=0.587, kpe=0.589, ppl=162.24, wps=2778.1, ups=0.33, wpb=8404.1, bsz=256, num_updates=14800, lr=0.000290619, gnorm=1.686, clip=0, loss_scale=4096, train_wall=268, wall=53448
2022-07-30 05:52:15 | INFO | train_inner | epoch 019:    571 / 797 loss=nan, nll_loss=3.439, mask_ins=1.141, word_ins_ml=5.033, word_reposition=0.59, kpe=nan, ppl=nan, wps=2870.1, ups=0.34, wpb=8464.5, bsz=256, num_updates=14900, lr=0.000289642, gnorm=1.664, clip=0, loss_scale=6308, train_wall=260, wall=53743
2022-07-30 05:56:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-30 05:57:14 | INFO | train_inner | epoch 019:    672 / 797 loss=7.334, nll_loss=3.415, mask_ins=1.138, word_ins_ml=5.011, word_reposition=0.592, kpe=0.593, ppl=161.38, wps=2834.7, ups=0.33, wpb=8474.4, bsz=256, num_updates=15000, lr=0.000288675, gnorm=1.668, clip=0, loss_scale=7908, train_wall=264, wall=54042
2022-07-30 06:02:09 | INFO | train_inner | epoch 019:    772 / 797 loss=nan, nll_loss=3.424, mask_ins=1.141, word_ins_ml=5.018, word_reposition=0.582, kpe=nan, ppl=nan, wps=2867.9, ups=0.34, wpb=8477.1, bsz=256, num_updates=15100, lr=0.000287718, gnorm=1.666, clip=0, loss_scale=4096, train_wall=260, wall=54338
2022-07-30 06:03:22 | INFO | train | epoch 019 | loss nan | nll_loss 3.426 | mask_ins 1.141 | word_ins_ml 5.021 | word_reposition 0.586 | kpe nan | ppl nan | wps 2737.2 | ups 0.32 | wpb 8439.5 | bsz 256 | num_updates 15125 | lr 0.00028748 | gnorm 1.678 | clip 0 | loss_scale 4882 | train_wall 2110 | wall 54411
2022-07-30 06:04:23 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 14.616 | nll_loss 9.591 | mask_ins 1.628 | word_ins_ml 10.6 | word_reposition 1.048 | kpe 1.34 | ppl 25104 | wps 5461.2 | wpb 934 | bsz 32 | num_updates 15125 | best_loss 14.214
2022-07-30 06:04:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 19 @ 15125 updates, score 14.616) (writing took 3.752851984463632 seconds)
2022-07-30 06:08:08 | INFO | train_inner | epoch 020:     75 / 797 loss=nan, nll_loss=3.341, mask_ins=1.141, word_ins_ml=4.945, word_reposition=0.579, kpe=nan, ppl=nan, wps=2349.8, ups=0.28, wpb=8420.1, bsz=255.7, num_updates=15200, lr=0.00028677, gnorm=1.685, clip=0, loss_scale=4096, train_wall=259, wall=54696
2022-07-30 06:13:03 | INFO | train_inner | epoch 020:    175 / 797 loss=7.209, nll_loss=3.339, mask_ins=1.121, word_ins_ml=4.943, word_reposition=0.57, kpe=0.575, ppl=147.95, wps=2845.7, ups=0.34, wpb=8396, bsz=256, num_updates=15300, lr=0.000285831, gnorm=1.707, clip=0, loss_scale=4096, train_wall=260, wall=54991
2022-07-30 06:17:58 | INFO | train_inner | epoch 020:    275 / 797 loss=7.271, nll_loss=3.378, mask_ins=1.133, word_ins_ml=4.978, word_reposition=0.578, kpe=0.582, ppl=154.49, wps=2870.2, ups=0.34, wpb=8482.6, bsz=256, num_updates=15400, lr=0.000284901, gnorm=1.727, clip=0, loss_scale=4096, train_wall=261, wall=55287
2022-07-30 06:22:54 | INFO | train_inner | epoch 020:    375 / 797 loss=7.278, nll_loss=3.389, mask_ins=1.13, word_ins_ml=4.987, word_reposition=0.576, kpe=0.584, ppl=155.17, wps=2851.6, ups=0.34, wpb=8448.6, bsz=256, num_updates=15500, lr=0.000283981, gnorm=1.7, clip=0, loss_scale=4096, train_wall=262, wall=55583
2022-07-30 06:28:14 | INFO | train_inner | epoch 020:    475 / 797 loss=nan, nll_loss=3.367, mask_ins=1.126, word_ins_ml=4.968, word_reposition=0.573, kpe=nan, ppl=nan, wps=2636.4, ups=0.31, wpb=8424.2, bsz=256, num_updates=15600, lr=0.000283069, gnorm=1.73, clip=0, loss_scale=7987, train_wall=285, wall=55903
2022-07-30 06:32:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-30 06:33:13 | INFO | train_inner | epoch 020:    576 / 797 loss=nan, nll_loss=3.346, mask_ins=1.124, word_ins_ml=4.949, word_reposition=0.578, kpe=nan, ppl=nan, wps=2818.2, ups=0.33, wpb=8417.4, bsz=256, num_updates=15700, lr=0.000282166, gnorm=1.695, clip=0, loss_scale=7381, train_wall=264, wall=56201
2022-07-30 06:38:09 | INFO | train_inner | epoch 020:    676 / 797 loss=nan, nll_loss=3.391, mask_ins=1.144, word_ins_ml=4.989, word_reposition=0.583, kpe=nan, ppl=nan, wps=2843.3, ups=0.34, wpb=8426.3, bsz=256, num_updates=15800, lr=0.000281272, gnorm=1.702, clip=0, loss_scale=4096, train_wall=262, wall=56498
2022-07-30 06:43:04 | INFO | train_inner | epoch 020:    776 / 797 loss=nan, nll_loss=3.356, mask_ins=1.131, word_ins_ml=4.958, word_reposition=0.575, kpe=nan, ppl=nan, wps=2884.7, ups=0.34, wpb=8511.2, bsz=256, num_updates=15900, lr=0.000280386, gnorm=1.681, clip=0, loss_scale=4096, train_wall=260, wall=56793
2022-07-30 06:44:06 | INFO | train | epoch 020 | loss nan | nll_loss 3.36 | mask_ins 1.131 | word_ins_ml 4.962 | word_reposition 0.576 | kpe nan | ppl nan | wps 2749.3 | ups 0.33 | wpb 8439.4 | bsz 256 | num_updates 15921 | lr 0.000280201 | gnorm 1.705 | clip 0 | loss_scale 5001 | train_wall 2101 | wall 56855
2022-07-30 06:45:06 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 14.609 | nll_loss 9.591 | mask_ins 1.643 | word_ins_ml 10.599 | word_reposition 1.038 | kpe 1.329 | ppl 24993.4 | wps 5452.3 | wpb 934 | bsz 32 | num_updates 15921 | best_loss 14.214
2022-07-30 06:45:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 20 @ 15921 updates, score 14.609) (writing took 3.5067071709781885 seconds)
2022-07-30 06:49:03 | INFO | train_inner | epoch 021:     79 / 797 loss=7.182, nll_loss=3.296, mask_ins=1.126, word_ins_ml=4.905, word_reposition=0.58, kpe=0.572, ppl=145.19, wps=2353.4, ups=0.28, wpb=8434.8, bsz=255.7, num_updates=16000, lr=0.000279508, gnorm=1.712, clip=0, loss_scale=4096, train_wall=259, wall=57151
2022-07-30 06:53:58 | INFO | train_inner | epoch 021:    179 / 797 loss=nan, nll_loss=3.371, mask_ins=1.14, word_ins_ml=4.972, word_reposition=0.582, kpe=nan, ppl=nan, wps=2874.2, ups=0.34, wpb=8504.8, bsz=256, num_updates=16100, lr=0.000278639, gnorm=1.72, clip=0, loss_scale=4096, train_wall=261, wall=57447
2022-07-30 06:58:53 | INFO | train_inner | epoch 021:    279 / 797 loss=7.174, nll_loss=3.307, mask_ins=1.12, word_ins_ml=4.915, word_reposition=0.57, kpe=0.57, ppl=144.45, wps=2856.6, ups=0.34, wpb=8403.1, bsz=256, num_updates=16200, lr=0.000277778, gnorm=1.718, clip=0, loss_scale=4424, train_wall=260, wall=57741
2022-07-30 07:03:48 | INFO | train_inner | epoch 021:    379 / 797 loss=7.205, nll_loss=3.326, mask_ins=1.126, word_ins_ml=4.932, word_reposition=0.571, kpe=0.577, ppl=147.53, wps=2853.8, ups=0.34, wpb=8438, bsz=256, num_updates=16300, lr=0.000276924, gnorm=1.729, clip=0, loss_scale=8192, train_wall=261, wall=58037
2022-07-30 07:04:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-30 07:08:46 | INFO | train_inner | epoch 021:    480 / 797 loss=7.198, nll_loss=3.327, mask_ins=1.126, word_ins_ml=4.932, word_reposition=0.568, kpe=0.572, ppl=146.83, wps=2817, ups=0.34, wpb=8398, bsz=256, num_updates=16400, lr=0.000276079, gnorm=1.754, clip=0, loss_scale=4542, train_wall=263, wall=58335
2022-07-30 07:14:13 | INFO | train_inner | epoch 021:    580 / 797 loss=7.23, nll_loss=3.337, mask_ins=1.129, word_ins_ml=4.941, word_reposition=0.583, kpe=0.578, ppl=150.11, wps=2581.5, ups=0.31, wpb=8427.5, bsz=256, num_updates=16500, lr=0.000275241, gnorm=1.725, clip=0, loss_scale=4096, train_wall=291, wall=58662
2022-07-30 07:19:08 | INFO | train_inner | epoch 021:    680 / 797 loss=nan, nll_loss=3.359, mask_ins=1.127, word_ins_ml=4.96, word_reposition=0.579, kpe=nan, ppl=nan, wps=2868.6, ups=0.34, wpb=8464.7, bsz=256, num_updates=16600, lr=0.000274411, gnorm=1.705, clip=0, loss_scale=4096, train_wall=260, wall=58957
2022-07-30 07:24:04 | INFO | train_inner | epoch 021:    780 / 797 loss=nan, nll_loss=3.334, mask_ins=1.122, word_ins_ml=4.938, word_reposition=0.574, kpe=nan, ppl=nan, wps=2850.4, ups=0.34, wpb=8432.2, bsz=256, num_updates=16700, lr=0.000273588, gnorm=1.74, clip=0, loss_scale=4096, train_wall=260, wall=59253
2022-07-30 07:24:54 | INFO | train | epoch 021 | loss nan | nll_loss 3.333 | mask_ins 1.127 | word_ins_ml 4.938 | word_reposition 0.576 | kpe nan | ppl nan | wps 2744.6 | ups 0.33 | wpb 8439.8 | bsz 256 | num_updates 16717 | lr 0.000273449 | gnorm 1.724 | clip 0 | loss_scale 4708 | train_wall 2106 | wall 59302
2022-07-30 07:25:55 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 14.658 | nll_loss 9.595 | mask_ins 1.644 | word_ins_ml 10.609 | word_reposition 1.02 | kpe 1.385 | ppl 25849.7 | wps 5384.2 | wpb 934 | bsz 32 | num_updates 16717 | best_loss 14.214
2022-07-30 07:25:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased_XSum/checkpoint_last.pt (epoch 21 @ 16717 updates, score 14.658) (writing took 3.4650008091703057 seconds)
2022-07-30 07:30:07 | INFO | train_inner | epoch 022:     83 / 797 loss=nan, nll_loss=3.244, mask_ins=1.111, word_ins_ml=4.859, word_reposition=0.563, kpe=nan, ppl=nan, wps=2308.3, ups=0.28, wpb=8380.5, bsz=255.7, num_updates=16800, lr=0.000272772, gnorm=1.707, clip=0, loss_scale=4096, train_wall=262, wall=59616
2022-07-30 07:35:06 | INFO | train_inner | epoch 022:    183 / 797 loss=nan, nll_loss=3.287, mask_ins=1.106, word_ins_ml=4.896, word_reposition=0.565, kpe=nan, ppl=nan, wps=2808.8, ups=0.33, wpb=8398.1, bsz=256, num_updates=16900, lr=0.000271964, gnorm=1.725, clip=0, loss_scale=7291, train_wall=264, wall=59915
2022-07-30 07:40:00 | INFO | train_inner | epoch 022:    283 / 797 loss=nan, nll_loss=3.301, mask_ins=1.118, word_ins_ml=4.909, word_reposition=0.568, kpe=nan, ppl=nan, wps=2876.5, ups=0.34, wpb=8463.5, bsz=256, num_updates=17000, lr=0.000271163, gnorm=1.723, clip=0, loss_scale=8192, train_wall=259, wall=60209
2022-07-30 07:44:58 | INFO | train_inner | epoch 022:    383 / 797 loss=7.151, nll_loss=3.269, mask_ins=1.126, word_ins_ml=4.881, word_reposition=0.579, kpe=0.565, ppl=142.12, wps=2827.6, ups=0.34, wpb=8422.9, bsz=256, num_updates=17100, lr=0.000270369, gnorm=1.738, clip=0, loss_scale=8192, train_wall=263, wall=60507
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
