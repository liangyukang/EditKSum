nohup: ignoring input
2022-07-25 16:46:13 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:14932
2022-07-25 16:46:13 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:14932
2022-07-25 16:46:13 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:14932
2022-07-25 16:46:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-25 16:46:13 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:14932
2022-07-25 16:46:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-25 16:46:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-25 16:46:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-25 16:46:14 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-25 16:46:14 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-25 16:46:14 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-25 16:46:14 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-25 16:46:14 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-25 16:46:14 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-25 16:46:14 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-25 16:46:14 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-25 16:46:18 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:14932', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='keyword', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-25 16:46:18 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-25 16:46:18 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-07-25 16:46:18 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.source
2022-07-25 16:46:18 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.target
2022-07-25 16:46:18 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]395it [00:00, 3941.98it/s]389it [00:00, 3881.11it/s]395it [00:00, 3948.06it/s]387it [00:00, 3860.56it/s]778it [00:00, 3534.25it/s]790it [00:00, 3533.96it/s]774it [00:00, 3487.17it/s]790it [00:00, 3468.72it/s]1143it [00:00, 3581.90it/s]1159it [00:00, 3600.68it/s]1126it [00:00, 3497.96it/s]1157it [00:00, 3551.36it/s]1521it [00:00, 3440.58it/s]1515it [00:00, 3414.29it/s]1503it [00:00, 3314.45it/s]1478it [00:00, 3229.88it/s]1880it [00:00, 3492.90it/s]1867it [00:00, 3340.48it/s]1838it [00:00, 3295.54it/s]1804it [00:00, 3204.41it/s]2231it [00:00, 3491.89it/s]2214it [00:00, 3380.17it/s]2179it [00:00, 3332.15it/s]2145it [00:00, 3270.19it/s]2514it [00:00, 3235.15it/s]2553it [00:00, 3221.74it/s]2582it [00:00, 3276.44it/s]2474it [00:00, 3153.83it/s]2870it [00:00, 3332.95it/s]2938it [00:00, 3361.02it/s]2832it [00:00, 3281.98it/s]2877it [00:00, 3194.31it/s]3205it [00:00, 3301.95it/s]3205it [00:00, 3218.05it/s]3277it [00:00, 3320.38it/s]3162it [00:00, 3234.26it/s]3599it [00:01, 3490.44it/s]3561it [00:01, 3319.12it/s]3654it [00:01, 3453.25it/s]3553it [00:01, 3434.00it/s]3950it [00:01, 3439.39it/s]4002it [00:01, 3403.72it/s]3918it [00:01, 3306.15it/s]3918it [00:01, 3380.89it/s]4341it [00:01, 3577.87it/s]4377it [00:01, 3505.30it/s]4313it [00:01, 3494.52it/s]4303it [00:01, 3516.01it/s]4724it [00:01, 3652.18it/s]4688it [00:01, 3569.22it/s]4682it [00:01, 3595.00it/s]4757it [00:01, 3590.95it/s]5091it [00:01, 3550.70it/s]5046it [00:01, 3459.80it/s]5118it [00:01, 3437.00it/s]5043it [00:01, 3437.48it/s]5467it [00:01, 3610.88it/s]5424it [00:01, 3552.81it/s]5487it [00:01, 3508.73it/s]5419it [00:01, 3529.88it/s]5830it [00:01, 3438.50it/s]5781it [00:01, 3404.86it/s]5840it [00:01, 3368.63it/s]5774it [00:01, 3377.14it/s]6182it [00:01, 3459.29it/s]6130it [00:01, 3428.80it/s]6186it [00:01, 3394.37it/s]6123it [00:01, 3408.30it/s]6475it [00:01, 3292.01it/s]6466it [00:01, 3275.84it/s]6530it [00:02, 2149.39it/s]6528it [00:02, 1887.21it/s]6882it [00:02, 2428.92it/s]6807it [00:02, 2033.04it/s]6888it [00:02, 2206.09it/s]6796it [00:02, 1867.86it/s]7242it [00:02, 2691.89it/s]7166it [00:02, 2343.94it/s]7248it [00:02, 2498.96it/s]7155it [00:02, 2189.59it/s]7560it [00:02, 2752.78it/s]7458it [00:02, 2463.70it/s]7562it [00:02, 2610.06it/s]7451it [00:02, 2351.51it/s]7911it [00:02, 2943.58it/s]7814it [00:02, 2725.57it/s]7907it [00:02, 2815.90it/s]7809it [00:02, 2634.59it/s]8233it [00:02, 2945.89it/s]8161it [00:02, 2913.99it/s]8227it [00:02, 2862.48it/s]8166it [00:02, 2866.04it/s]8591it [00:02, 3112.58it/s]8482it [00:02, 2928.49it/s]8586it [00:02, 3054.63it/s]8491it [00:02, 2899.47it/s]8941it [00:02, 3218.04it/s]8833it [00:02, 3084.86it/s]8947it [00:02, 3206.40it/s]8840it [00:02, 3055.48it/s]9275it [00:02, 3096.78it/s]9158it [00:02, 3060.99it/s]9284it [00:03, 3140.70it/s]9166it [00:03, 3042.00it/s]9623it [00:03, 3201.82it/s]9509it [00:03, 3184.72it/s]9642it [00:03, 3257.65it/s]9521it [00:03, 3181.84it/s]9951it [00:03, 3144.75it/s]9867it [00:03, 3141.96it/s]9862it [00:03, 3244.99it/s]9977it [00:03, 3184.81it/s]10282it [00:03, 3190.48it/s]10217it [00:03, 3241.11it/s]10335it [00:03, 3295.50it/s]10195it [00:03, 3061.40it/s]10622it [00:03, 3248.93it/s]10557it [00:03, 3238.23it/s]10670it [00:03, 3307.28it/s]10520it [00:03, 3111.75it/s]10950it [00:03, 3045.93it/s]10885it [00:03, 2987.10it/s]11005it [00:03, 3051.40it/s]10837it [00:03, 2938.84it/s]11263it [00:03, 3067.02it/s]11191it [00:03, 2984.21it/s]11317it [00:03, 3014.70it/s]11150it [00:03, 2990.76it/s]11505it [00:03, 3026.57it/s]11573it [00:03, 2793.99it/s]11454it [00:03, 2921.63it/s]11623it [00:03, 2885.09it/s]11893it [00:03, 2902.98it/s]11811it [00:03, 2919.64it/s]11977it [00:03, 3064.86it/s]11750it [00:03, 2884.49it/s]12252it [00:03, 3094.37it/s]12170it [00:03, 3108.28it/s]12340it [00:03, 3222.64it/s]12103it [00:03, 3067.61it/s]12567it [00:04, 3060.03it/s]12484it [00:04, 3029.45it/s]12413it [00:04, 3027.74it/s]12666it [00:04, 3158.14it/s]12925it [00:04, 3206.56it/s]12844it [00:04, 3190.95it/s]12764it [00:04, 3166.17it/s]13018it [00:04, 3255.91it/s]13249it [00:04, 3100.54it/s]13188it [00:04, 3262.67it/s]13368it [00:04, 3139.13it/s]
2022-07-25 16:46:22 | INFO | root | success load 13368 data
2022-07-25 16:46:22 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-25 16:46:22 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-25 16:46:22 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-25 16:46:22 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-25 16:46:22 | INFO | transformer.tokenization_utils | loading file None
2022-07-25 16:46:22 | INFO | transformer.tokenization_utils | loading file None
2022-07-25 16:46:22 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
13120it [00:04, 3281.05it/s]13346it [00:04, 3182.64it/s]13368it [00:04, 3113.39it/s]
13368it [00:04, 3102.61it/s]
13368it [00:04, 3064.10it/s]
2022-07-25 16:46:23 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-25 16:46:23 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-25 16:46:23 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-25 16:46:26 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-25 16:46:26 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-25 16:46:26 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-25 16:46:26 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-25 16:46:26 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-25 16:46:28 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-25 16:46:28 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-07-25 16:46:28 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-07-25 16:46:28 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-25 16:46:28 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 142456835)
2022-07-25 16:46:28 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-07-25 16:46:28 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-25 16:46:35 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-25 16:46:35 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-25 16:46:35 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt
2022-07-25 16:46:35 | INFO | fairseq.trainer | loading train data for epoch 1
2022-07-25 16:46:35 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.source
2022-07-25 16:46:35 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.target
2022-07-25 16:46:35 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]374it [00:00, 3737.00it/s]363it [00:00, 3625.09it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]748it [00:00, 3455.03it/s]726it [00:00, 3408.33it/s]371it [00:00, 3700.77it/s]368it [00:00, 3672.69it/s]1122it [00:00, 3579.21it/s]1100it [00:00, 3551.30it/s]736it [00:00, 3457.34it/s]742it [00:00, 3469.98it/s]1482it [00:00, 3387.75it/s]1468it [00:00, 3599.68it/s]1112it [00:00, 3589.54it/s]1120it [00:00, 3605.44it/s]1871it [00:00, 3559.00it/s]1829it [00:00, 3469.68it/s]1482it [00:00, 3413.29it/s]1480it [00:00, 3412.87it/s]2249it [00:00, 3628.82it/s]2203it [00:00, 3558.39it/s]1863it [00:00, 3547.63it/s]1860it [00:00, 3543.26it/s]2614it [00:00, 3465.89it/s]2560it [00:00, 3435.45it/s]2241it [00:00, 3623.74it/s]2235it [00:00, 3609.88it/s]2991it [00:00, 3555.67it/s]2935it [00:00, 3531.31it/s]2598it [00:00, 3466.83it/s]2605it [00:00, 3465.96it/s]3349it [00:00, 3421.79it/s]3290it [00:00, 3414.46it/s]2963it [00:00, 3520.57it/s]2983it [00:00, 3559.93it/s]3719it [00:01, 3494.61it/s]3658it [00:01, 3492.63it/s]3317it [00:00, 3410.01it/s]3341it [00:00, 3434.56it/s]4071it [00:01, 3393.81it/s]4009it [00:01, 3382.02it/s]3685it [00:01, 3489.03it/s]3713it [00:01, 3517.88it/s]4437it [00:01, 3468.94it/s]4383it [00:01, 3485.18it/s]4036it [00:01, 3370.00it/s]4067it [00:01, 3404.53it/s]4804it [00:01, 3525.70it/s]4753it [00:01, 3548.08it/s]4402it [00:01, 3453.51it/s]4442it [00:01, 3503.49it/s]5158it [00:01, 3402.78it/s]5110it [00:01, 3411.48it/s]4772it [00:01, 3523.69it/s]4814it [00:01, 3566.54it/s]5520it [00:01, 3465.13it/s]5477it [00:01, 3483.24it/s]5126it [00:01, 3399.44it/s]5173it [00:01, 3438.20it/s]5868it [00:01, 3368.85it/s]5827it [00:01, 3362.95it/s]5495it [00:01, 3481.78it/s]5521it [00:01, 3449.57it/s]6234it [00:01, 3451.26it/s]6199it [00:01, 3463.23it/s]5868it [00:01, 3356.78it/s]5845it [00:01, 3366.19it/s]6581it [00:01, 3269.37it/s]6547it [00:01, 3306.44it/s]6241it [00:01, 3463.29it/s]6206it [00:01, 3434.18it/s]6919it [00:02, 3292.53it/s]6880it [00:02, 3305.49it/s]6551it [00:01, 3024.50it/s]6589it [00:01, 2864.33it/s]7213it [00:02, 3310.89it/s]7251it [00:02, 3148.71it/s]6872it [00:02, 3073.60it/s]6893it [00:02, 2817.15it/s]7187it [00:02, 2901.75it/s]7220it [00:02, 2927.78it/s]7569it [00:03, 935.26it/s] 7546it [00:03, 918.46it/s] 7939it [00:03, 1229.15it/s]7523it [00:03, 965.24it/s] 7484it [00:03, 942.66it/s] 7916it [00:03, 1205.51it/s]7897it [00:03, 1276.74it/s]7853it [00:03, 1247.03it/s]8268it [00:03, 1489.86it/s]8269it [00:03, 1482.14it/s]8227it [00:03, 1587.27it/s]8640it [00:03, 1840.55it/s]8269it [00:03, 1577.39it/s]8640it [00:03, 1823.16it/s]8541it [00:03, 1840.23it/s]9014it [00:03, 2189.06it/s]8644it [00:03, 1928.25it/s]9014it [00:03, 2167.15it/s]8906it [00:03, 2182.33it/s]9344it [00:03, 2389.70it/s]9022it [00:03, 2275.43it/s]9347it [00:03, 2377.30it/s]9227it [00:03, 2388.81it/s]9717it [00:03, 2692.24it/s]9353it [00:03, 2466.29it/s]9722it [00:03, 2683.09it/s]9601it [00:03, 2702.06it/s]10056it [00:03, 2808.91it/s]9732it [00:03, 2770.60it/s]10064it [00:03, 2812.43it/s]9949it [00:03, 2833.77it/s]10431it [00:03, 3048.06it/s]10074it [00:03, 2874.68it/s]10439it [00:03, 3047.49it/s]10324it [00:03, 3068.84it/s]10449it [00:03, 3099.51it/s]10789it [00:04, 3063.42it/s]10789it [00:04, 3081.61it/s]10700it [00:03, 3253.39it/s]11167it [00:04, 3254.81it/s]10796it [00:03, 3107.10it/s]11164it [00:04, 3260.33it/s]11054it [00:04, 3235.85it/s]11546it [00:04, 3401.92it/s]11177it [00:04, 3296.96it/s]11540it [00:04, 3397.47it/s]11434it [00:04, 3391.02it/s]11556it [00:04, 3432.80it/s]11903it [00:04, 3340.40it/s]11897it [00:04, 3339.25it/s]11788it [00:04, 3325.31it/s]12285it [00:04, 3472.94it/s]11915it [00:04, 3356.75it/s]12276it [00:04, 3466.33it/s]12170it [00:04, 3463.15it/s]12642it [00:04, 3403.18it/s]12303it [00:04, 3503.48it/s]12632it [00:04, 3403.61it/s]12525it [00:04, 3406.31it/s]13026it [00:04, 3526.58it/s]12662it [00:04, 3402.38it/s]13006it [00:04, 3499.44it/s]12905it [00:04, 3517.89it/s]13384it [00:04, 3438.69it/s]13053it [00:04, 3544.43it/s]13361it [00:04, 3449.04it/s]13286it [00:04, 3599.94it/s]13745it [00:04, 3487.20it/s]13413it [00:04, 3464.20it/s]13737it [00:04, 3538.54it/s]14118it [00:04, 3553.81it/s]13650it [00:04, 3384.13it/s]13771it [00:04, 3435.43it/s]14097it [00:04, 3553.92it/s]13994it [00:04, 3369.91it/s]14476it [00:05, 3329.49it/s]14118it [00:04, 3266.84it/s]14455it [00:05, 3314.36it/s]14819it [00:05, 3357.07it/s]14335it [00:05, 3212.64it/s]14448it [00:05, 3126.33it/s]14798it [00:05, 3345.82it/s]14672it [00:05, 3256.24it/s]15158it [00:05, 3198.59it/s]14779it [00:05, 3175.81it/s]15136it [00:05, 3125.30it/s]15492it [00:05, 3236.52it/s]15001it [00:05, 3058.55it/s]15099it [00:05, 3139.94it/s]15508it [00:05, 3289.08it/s]15376it [00:05, 3247.94it/s]15829it [00:05, 3213.21it/s]15463it [00:05, 3281.11it/s]15842it [00:05, 3258.21it/s]15754it [00:05, 3396.71it/s]16195it [00:05, 3339.43it/s]15829it [00:05, 3242.05it/s]16212it [00:05, 3382.26it/s]16586it [00:05, 3504.25it/s]16098it [00:05, 3313.81it/s]16202it [00:05, 3378.74it/s]16585it [00:05, 3481.27it/s]16485it [00:05, 3472.13it/s]16599it [00:05, 3542.66it/s]16936it [00:06, 1026.63it/s]16939it [00:06, 957.14it/s] 16956it [00:06, 1073.34it/s]16836it [00:06, 1017.45it/s]17316it [00:06, 1329.97it/s]17317it [00:06, 1245.80it/s]17345it [00:06, 1387.75it/s]17228it [00:06, 1330.14it/s]17636it [00:06, 1582.41it/s]17639it [00:06, 1497.07it/s]17673it [00:06, 1647.82it/s]17580it [00:06, 1605.08it/s]18021it [00:06, 1949.16it/s]18021it [00:07, 1854.78it/s]18066it [00:06, 2022.40it/s]17974it [00:06, 1975.23it/s]18399it [00:07, 2292.48it/s]18408it [00:07, 2217.84it/s]18419it [00:06, 2281.92it/s]18361it [00:07, 2324.70it/s]18741it [00:07, 2488.79it/s]18752it [00:07, 2427.51it/s]18802it [00:07, 2609.24it/s]18710it [00:07, 2532.74it/s]19113it [00:07, 2769.57it/s]19127it [00:07, 2721.76it/s]19188it [00:07, 2898.05it/s]19099it [00:07, 2839.97it/s]19459it [00:07, 2890.96it/s]19475it [00:07, 2848.45it/s]19548it [00:07, 3003.02it/s]19455it [00:07, 2956.96it/s]19819it [00:07, 3071.74it/s]19851it [00:07, 3078.65it/s]19932it [00:07, 3217.34it/s]19836it [00:07, 3173.62it/s]20165it [00:07, 3118.50it/s]20202it [00:07, 3094.72it/s]20293it [00:07, 3232.83it/s]20194it [00:07, 3190.60it/s]20531it [00:07, 3265.61it/s]20563it [00:07, 3231.55it/s]20671it [00:07, 3381.22it/s]20557it [00:07, 3308.92it/s]20908it [00:07, 3405.13it/s]20925it [00:07, 3337.58it/s]21030it [00:07, 3354.89it/s]20920it [00:07, 3397.58it/s]21264it [00:07, 3278.83it/s]21380it [00:07, 3320.22it/s]21276it [00:07, 3184.20it/s]21604it [00:07, 3182.10it/s]21276it [00:07, 2937.80it/s]21608it [00:08, 3219.77it/s]21722it [00:07, 3326.22it/s]21612it [00:07, 3045.09it/s]21931it [00:08, 3024.87it/s]21940it [00:08, 3078.44it/s]22062it [00:08, 3172.50it/s]22266it [00:08, 3112.34it/s]21934it [00:08, 2964.31it/s]22271it [00:08, 3140.77it/s]22386it [00:08, 3123.82it/s]22301it [00:08, 3152.69it/s]22619it [00:08, 3109.90it/s]22618it [00:08, 3159.83it/s]22705it [00:08, 3141.42it/s]22627it [00:08, 3163.67it/s]23001it [00:08, 3305.89it/s]22975it [00:08, 3274.76it/s]23080it [00:08, 3314.02it/s]23014it [00:08, 3363.29it/s]23359it [00:08, 3382.66it/s]23347it [00:08, 3402.60it/s]23451it [00:08, 3428.53it/s]23389it [00:08, 3473.35it/s]23701it [00:08, 3309.57it/s]23691it [00:08, 3329.51it/s]23797it [00:08, 3341.65it/s]24069it [00:08, 3414.61it/s]23742it [00:08, 3372.04it/s]24069it [00:08, 3459.61it/s]24168it [00:08, 3445.22it/s]24120it [00:08, 3487.67it/s]24413it [00:08, 3339.76it/s]24418it [00:08, 3368.41it/s]24515it [00:08, 3355.17it/s]24778it [00:08, 3426.57it/s]24473it [00:08, 3385.79it/s]24790it [00:09, 3467.42it/s]24886it [00:08, 3456.26it/s]24848it [00:08, 3490.05it/s]25139it [00:09, 3328.68it/s]25139it [00:09, 3354.94it/s]25234it [00:08, 3350.79it/s]25500it [00:09, 3407.32it/s]25200it [00:09, 3376.09it/s]25515it [00:09, 3470.35it/s]25605it [00:09, 3452.69it/s]25877it [00:09, 3511.44it/s]25572it [00:09, 3473.39it/s]25892it [00:09, 3556.19it/s]25980it [00:09, 3365.80it/s]25950it [00:09, 3561.08it/s]26230it [00:09, 3375.80it/s]26250it [00:09, 3424.49it/s]26359it [00:09, 3483.50it/s]26604it [00:09, 3478.70it/s]26308it [00:09, 3421.93it/s]26618it [00:09, 3496.62it/s]26732it [00:09, 3552.70it/s]26681it [00:09, 3509.19it/s]26954it [00:09, 3338.21it/s]26970it [00:09, 3386.55it/s]27089it [00:09, 3436.74it/s]27034it [00:09, 3411.41it/s]27336it [00:09, 3474.74it/s]27358it [00:09, 3526.33it/s]27461it [00:09, 3516.07it/s]27419it [00:09, 3534.67it/s]27686it [00:09, 3348.63it/s]27713it [00:09, 3417.53it/s]27815it [00:09, 3417.86it/s]27775it [00:09, 3434.33it/s]28065it [00:09, 3471.29it/s]28088it [00:09, 3512.48it/s]28191it [00:09, 3514.71it/s]28152it [00:09, 3530.11it/s]28429it [00:10, 3513.53it/s]28459it [00:10, 3566.99it/s]28544it [00:09, 3394.84it/s]28507it [00:09, 3404.65it/s]28886it [00:11, 828.21it/s] 29256it [00:11, 1087.86it/s]28850it [00:11, 762.75it/s] 29565it [00:11, 1316.73it/s]28818it [00:11, 720.01it/s] 28783it [00:11, 688.16it/s] 29223it [00:11, 1010.80it/s]29945it [00:11, 1665.02it/s]29182it [00:11, 947.03it/s]29146it [00:11, 909.42it/s]29525it [00:11, 1225.01it/s]30263it [00:11, 1913.98it/s]29485it [00:11, 1155.19it/s]29456it [00:11, 1122.00it/s]29899it [00:11, 1556.75it/s]30643it [00:11, 2275.79it/s]29864it [00:11, 1484.24it/s]29829it [00:11, 1439.30it/s]31019it [00:11, 2592.85it/s]30249it [00:11, 1835.62it/s]30237it [00:11, 1823.95it/s]30202it [00:11, 1778.57it/s]30624it [00:11, 2184.78it/s]31366it [00:11, 2748.71it/s]30573it [00:12, 2071.48it/s]30536it [00:12, 1990.78it/s]30996it [00:11, 2501.63it/s]31740it [00:11, 2993.63it/s]30946it [00:12, 2403.76it/s]30904it [00:12, 2319.81it/s]31341it [00:12, 2653.65it/s]32090it [00:12, 3051.86it/s]31288it [00:12, 2578.73it/s]31238it [00:12, 2502.58it/s]31715it [00:12, 2915.54it/s]32467it [00:12, 3242.32it/s]31661it [00:12, 2850.96it/s]31612it [00:12, 2791.97it/s]32062it [00:12, 2961.40it/s]32819it [00:12, 3208.10it/s]32007it [00:12, 2931.62it/s]31953it [00:12, 2886.22it/s]32435it [00:12, 3161.73it/s]33190it [00:12, 3345.19it/s]32385it [00:12, 3151.31it/s]32328it [00:12, 3109.88it/s]33564it [00:12, 3455.54it/s]32782it [00:12, 3132.19it/s]32741it [00:12, 3261.42it/s]32691it [00:12, 3249.08it/s]33921it [00:12, 3391.57it/s]33153it [00:12, 3288.66it/s]33092it [00:12, 3214.56it/s]33042it [00:12, 3171.96it/s]34302it [00:12, 3510.81it/s]33521it [00:12, 3397.09it/s]33459it [00:12, 3341.09it/s]33407it [00:12, 3302.17it/s]34660it [00:12, 3401.92it/s]33873it [00:12, 3319.47it/s]33807it [00:12, 3280.65it/s]33752it [00:12, 3260.33it/s]35032it [00:12, 3491.16it/s]34252it [00:12, 3451.05it/s]34188it [00:13, 3430.14it/s]34130it [00:13, 3404.88it/s]35385it [00:12, 3379.31it/s]34604it [00:12, 3351.00it/s]34539it [00:13, 3344.01it/s]34479it [00:13, 3335.78it/s]35761it [00:13, 3486.75it/s]34969it [00:13, 3435.01it/s]34908it [00:13, 3441.62it/s]34838it [00:13, 3407.80it/s]35317it [00:13, 3331.74it/s]36129it [00:13, 3397.30it/s]35281it [00:13, 3523.55it/s]35210it [00:13, 3496.15it/s]35688it [00:13, 3439.09it/s]36499it [00:13, 3481.04it/s]35637it [00:13, 3402.86it/s]35563it [00:13, 3384.31it/s]36056it [00:13, 3506.86it/s]36874it [00:13, 3557.19it/s]35985it [00:13, 3424.45it/s]35931it [00:13, 3468.02it/s]37232it [00:13, 3439.57it/s]36409it [00:13, 3251.38it/s]36330it [00:13, 3217.07it/s]36281it [00:13, 3288.31it/s]37583it [00:13, 3458.29it/s]36755it [00:13, 3308.65it/s]36660it [00:13, 3239.51it/s]36614it [00:13, 3284.90it/s]37931it [00:13, 3222.70it/s]37090it [00:13, 3032.06it/s]36987it [00:13, 2973.20it/s]36945it [00:13, 3271.24it/s]38270it [00:13, 3267.50it/s]37401it [00:13, 3051.89it/s]37290it [00:14, 2885.46it/s]37274it [00:14, 3091.27it/s]38600it [00:13, 3233.44it/s]37729it [00:13, 3113.52it/s]37627it [00:14, 3016.60it/s]37619it [00:14, 3189.97it/s]38926it [00:14, 3191.75it/s]38045it [00:14, 3091.97it/s]37933it [00:14, 3025.70it/s]37941it [00:14, 3160.22it/s]39303it [00:14, 3357.50it/s]38417it [00:14, 3272.04it/s]38308it [00:14, 3232.14it/s]38317it [00:14, 3330.07it/s]39641it [00:14, 3295.93it/s]38747it [00:14, 3213.80it/s]38649it [00:14, 3155.48it/s]38652it [00:14, 3191.64it/s]40020it [00:14, 3436.33it/s]39127it [00:14, 3380.85it/s]39030it [00:14, 3340.15it/s]39030it [00:14, 3356.42it/s]40366it [00:14, 3361.13it/s]39490it [00:14, 3335.91it/s]39405it [00:14, 3456.15it/s]39406it [00:14, 3472.46it/s]40725it [00:14, 3425.90it/s]39869it [00:14, 3465.61it/s]39753it [00:14, 3355.84it/s]39756it [00:14, 3368.85it/s]41098it [00:14, 3513.93it/s]40251it [00:14, 3567.87it/s]40124it [00:14, 3456.89it/s]40119it [00:14, 3441.87it/s]41451it [00:14, 3415.43it/s]40610it [00:14, 3422.04it/s]40472it [00:14, 3368.74it/s]40465it [00:14, 3339.17it/s]41823it [00:14, 3502.07it/s]40989it [00:14, 3525.23it/s]40846it [00:15, 3473.40it/s]40830it [00:15, 3427.17it/s]42175it [00:14, 3408.70it/s]41344it [00:15, 3425.79it/s]41195it [00:15, 3382.01it/s]41175it [00:15, 3345.26it/s]42552it [00:15, 3512.45it/s]41719it [00:15, 3516.73it/s]41568it [00:15, 3479.94it/s]41538it [00:15, 3425.32it/s]42905it [00:15, 3402.40it/s]42073it [00:15, 3388.79it/s]41941it [00:15, 3551.90it/s]41904it [00:15, 3492.96it/s]43270it [00:15, 3469.14it/s]42451it [00:15, 3500.17it/s]42298it [00:15, 3442.57it/s]42255it [00:15, 3391.11it/s]43639it [00:15, 3530.92it/s]42828it [00:15, 3576.22it/s]42673it [00:15, 3531.32it/s]42630it [00:15, 3493.85it/s]43188it [00:15, 3440.27it/s]43028it [00:15, 3386.85it/s]42981it [00:15, 3352.98it/s]43554it [00:15, 3501.38it/s]43398it [00:15, 3473.99it/s]43347it [00:15, 3438.06it/s]43748it [00:15, 3361.45it/s]43693it [00:15, 3334.90it/s]43994it [00:16, 671.05it/s] 44370it [00:17, 898.64it/s]44674it [00:17, 1102.56it/s]45052it [00:17, 1423.20it/s]45428it [00:17, 1764.36it/s]43906it [00:17, 597.04it/s] 44086it [00:17, 619.35it/s] 45765it [00:17, 2013.56it/s]44029it [00:17, 602.17it/s] 44283it [00:17, 806.09it/s]44458it [00:17, 835.25it/s]46136it [00:17, 2344.46it/s]44395it [00:17, 811.81it/s]44599it [00:17, 1007.62it/s]44774it [00:17, 1045.76it/s]46478it [00:17, 2526.24it/s]44705it [00:17, 1015.92it/s]44970it [00:17, 1303.06it/s]45152it [00:17, 1358.65it/s]46846it [00:17, 2795.63it/s]45081it [00:17, 1326.00it/s]45345it [00:17, 1633.18it/s]45469it [00:17, 1610.03it/s]47190it [00:17, 2902.36it/s]45439it [00:18, 1608.69it/s]45681it [00:17, 1888.39it/s]45839it [00:18, 1958.82it/s]47562it [00:17, 3113.31it/s]45801it [00:18, 1936.32it/s]46050it [00:18, 2222.21it/s]46208it [00:18, 2288.46it/s]47936it [00:18, 3281.92it/s]46165it [00:18, 2257.77it/s]46391it [00:18, 2407.49it/s]46550it [00:18, 2488.48it/s]48291it [00:18, 3240.17it/s]46504it [00:18, 2451.72it/s]46764it [00:18, 2705.76it/s]46919it [00:18, 2767.34it/s]48664it [00:18, 3374.13it/s]46869it [00:18, 2726.23it/s]47120it [00:18, 2825.63it/s]47264it [00:18, 2876.41it/s]49016it [00:18, 3310.68it/s]47210it [00:18, 2829.30it/s]47500it [00:18, 3070.78it/s]47636it [00:18, 3092.12it/s]49386it [00:18, 3418.27it/s]47580it [00:18, 3052.21it/s]47870it [00:18, 3235.57it/s]47983it [00:18, 3100.48it/s]49736it [00:18, 3338.84it/s]47948it [00:18, 3219.59it/s]48225it [00:18, 3220.43it/s]48358it [00:18, 3276.96it/s]50109it [00:18, 3448.65it/s]48299it [00:18, 3196.62it/s]48598it [00:18, 3359.82it/s]48732it [00:18, 3405.38it/s]50480it [00:18, 3355.31it/s]48662it [00:18, 3316.38it/s]48951it [00:18, 3312.26it/s]49088it [00:19, 3347.95it/s]50850it [00:18, 3451.00it/s]49009it [00:19, 3287.31it/s]49326it [00:18, 3433.90it/s]49460it [00:19, 3451.76it/s]51227it [00:19, 3542.29it/s]49376it [00:19, 3395.14it/s]49679it [00:19, 3342.28it/s]49814it [00:19, 3372.81it/s]51584it [00:19, 3410.99it/s]49724it [00:19, 3326.51it/s]50051it [00:19, 3448.90it/s]50185it [00:19, 3468.43it/s]51951it [00:19, 3484.63it/s]50085it [00:19, 3404.93it/s]50410it [00:19, 3480.52it/s]50537it [00:19, 3274.81it/s]52302it [00:19, 3379.04it/s]50452it [00:19, 3480.34it/s]50762it [00:19, 3258.59it/s]50899it [00:19, 3370.80it/s]52664it [00:19, 3446.76it/s]50804it [00:19, 3309.04it/s]51103it [00:19, 3299.72it/s]51241it [00:19, 3051.82it/s]53011it [00:19, 3231.50it/s]51139it [00:19, 3318.34it/s]51437it [00:19, 3080.58it/s]51555it [00:19, 2957.04it/s]53338it [00:19, 3169.30it/s]51474it [00:19, 3110.71it/s]51755it [00:19, 3106.37it/s]51880it [00:19, 3034.05it/s]53658it [00:19, 3149.68it/s]51802it [00:19, 3155.85it/s]52070it [00:19, 3112.53it/s]52189it [00:20, 2992.68it/s]53975it [00:19, 3003.83it/s]52160it [00:20, 3135.60it/s]52384it [00:19, 3093.94it/s]52560it [00:20, 3192.05it/s]54340it [00:20, 3183.31it/s]52521it [00:20, 3267.66it/s]52750it [00:20, 3255.77it/s]52929it [00:20, 3333.42it/s]54680it [00:20, 3171.31it/s]52895it [00:20, 3400.33it/s]53078it [00:20, 3198.93it/s]53266it [00:20, 3271.23it/s]55050it [00:20, 3320.42it/s]53238it [00:20, 3310.50it/s]53446it [00:20, 3337.01it/s]53632it [00:20, 3382.34it/s]55413it [00:20, 3409.43it/s]53603it [00:20, 3406.14it/s]53816it [00:20, 3441.90it/s]53973it [00:20, 3330.20it/s]55756it [00:20, 3276.38it/s]53946it [00:20, 3303.30it/s]54162it [00:20, 3350.55it/s]54350it [00:20, 3456.93it/s]56135it [00:20, 3420.75it/s]54324it [00:20, 3438.04it/s]54534it [00:20, 3455.58it/s]54698it [00:20, 3347.25it/s]56480it [00:20, 3340.14it/s]54680it [00:20, 3346.87it/s]54881it [00:20, 3350.69it/s]55065it [00:20, 3439.47it/s]56854it [00:20, 3453.57it/s]55045it [00:20, 3431.41it/s]55244it [00:20, 3429.87it/s]55430it [00:20, 3499.25it/s]57202it [00:20, 3368.73it/s]55392it [00:20, 3442.36it/s]55589it [00:20, 3326.53it/s]55782it [00:21, 3384.92it/s]57577it [00:20, 3477.10it/s]55738it [00:21, 3324.18it/s]55954it [00:20, 3418.45it/s]56155it [00:21, 3483.50it/s]57950it [00:21, 3549.44it/s]56112it [00:21, 3441.10it/s]56321it [00:21, 3490.63it/s]56505it [00:21, 3393.06it/s]58307it [00:21, 3410.16it/s]56458it [00:21, 3340.11it/s]56672it [00:21, 3375.78it/s]56878it [00:21, 3489.41it/s]58670it [00:21, 3463.41it/s]56819it [00:21, 3417.04it/s]57046it [00:21, 3479.91it/s]57229it [00:21, 3399.56it/s]59018it [00:21, 3353.21it/s]57192it [00:21, 3505.93it/s]57396it [00:21, 3373.16it/s]57602it [00:21, 3494.13it/s]59381it [00:21, 3431.31it/s]57544it [00:21, 3392.59it/s]57764it [00:21, 3460.24it/s]57972it [00:21, 3551.74it/s]59726it [00:21, 3327.07it/s]57911it [00:21, 3470.81it/s]58112it [00:21, 3352.74it/s]58329it [00:21, 3391.65it/s]60093it [00:21, 3424.18it/s]58477it [00:21, 3435.69it/s]58260it [00:21, 3325.41it/s]58700it [00:21, 3481.17it/s]60458it [00:21, 3488.49it/s]58842it [00:21, 3496.19it/s]58624it [00:21, 3414.01it/s]59050it [00:22, 3362.15it/s]60809it [00:21, 3386.21it/s]59193it [00:21, 3367.27it/s]58968it [00:22, 3308.61it/s]59417it [00:22, 3447.86it/s]61179it [00:22, 3476.30it/s]59556it [00:22, 3442.00it/s]59331it [00:22, 3400.11it/s]59764it [00:22, 3338.77it/s]61528it [00:22, 3270.96it/s]59682it [00:22, 3431.25it/s]59902it [00:22, 3290.94it/s]60118it [00:22, 3394.50it/s]61899it [00:22, 3393.74it/s]60027it [00:22, 3330.39it/s]60251it [00:22, 3346.08it/s]60474it [00:22, 3440.52it/s]60365it [00:22, 3343.65it/s]62242it [00:22, 3158.51it/s]60588it [00:22, 3167.62it/s]60820it [00:22, 3232.57it/s]60701it [00:22, 3144.54it/s]60908it [00:22, 3076.65it/s]61148it [00:22, 3243.46it/s]61019it [00:22, 3080.21it/s]61234it [00:22, 3127.75it/s]61475it [00:22, 3039.68it/s]61329it [00:22, 2921.93it/s]61549it [00:22, 2978.78it/s]61783it [00:22, 3007.57it/s]61624it [00:22, 2918.28it/s]61921it [00:22, 3185.65it/s]62141it [00:22, 3168.14it/s]61994it [00:23, 3137.42it/s]62243it [00:22, 3155.03it/s]62563it [00:24, 492.15it/s] 62937it [00:24, 678.72it/s]63249it [00:24, 863.99it/s]63602it [00:24, 1124.57it/s]62461it [00:24, 507.91it/s] 63976it [00:24, 1445.12it/s]62827it [00:25, 701.21it/s]62561it [00:24, 488.60it/s] 64302it [00:24, 1697.03it/s]63148it [00:25, 899.99it/s]62919it [00:25, 672.56it/s]64672it [00:25, 2044.72it/s]63506it [00:25, 1175.42it/s]63186it [00:25, 828.28it/s]65006it [00:25, 2263.86it/s]63858it [00:25, 1475.38it/s]63533it [00:25, 1093.92it/s]65358it [00:25, 2536.91it/s]64177it [00:25, 1705.97it/s]62311it [00:25, 413.12it/s] 63843it [00:25, 1345.66it/s]65692it [00:25, 2586.16it/s]64485it [00:25, 1938.77it/s]62647it [00:25, 563.61it/s]64139it [00:25, 1508.59it/s]66019it [00:25, 2750.81it/s]64816it [00:25, 2214.22it/s]62968it [00:25, 743.54it/s]64455it [00:25, 1790.08it/s]66338it [00:25, 2811.33it/s]63246it [00:25, 920.41it/s]65129it [00:25, 2325.28it/s]64781it [00:25, 2077.87it/s]66650it [00:25, 2730.16it/s]63604it [00:25, 1217.93it/s]65491it [00:25, 2628.88it/s]65079it [00:25, 2265.97it/s]67020it [00:25, 2983.70it/s]63963it [00:25, 1544.60it/s]65809it [00:25, 2745.17it/s]65437it [00:25, 2577.75it/s]67347it [00:25, 3014.54it/s]64280it [00:26, 1800.40it/s]66178it [00:26, 2993.13it/s]65752it [00:25, 2689.18it/s]67721it [00:26, 3214.66it/s]64642it [00:26, 2143.81it/s]66509it [00:26, 3022.02it/s]66126it [00:26, 2964.85it/s]68092it [00:26, 3354.54it/s]64970it [00:26, 2351.61it/s]66879it [00:26, 3207.61it/s]66488it [00:26, 3143.24it/s]68436it [00:26, 3283.39it/s]65328it [00:26, 2633.84it/s]67247it [00:26, 3340.03it/s]66827it [00:26, 3135.08it/s]68811it [00:26, 3414.43it/s]65669it [00:26, 2747.43it/s]67594it [00:26, 3268.33it/s]67199it [00:26, 3298.86it/s]69158it [00:26, 3315.91it/s]66029it [00:26, 2964.47it/s]67968it [00:26, 3400.55it/s]67543it [00:26, 3236.96it/s]69516it [00:26, 3390.80it/s]66395it [00:26, 3149.97it/s]68316it [00:26, 3310.42it/s]67917it [00:26, 3379.64it/s]66740it [00:26, 3130.48it/s]69868it [00:26, 3290.12it/s]68675it [00:26, 3389.43it/s]68263it [00:26, 3303.64it/s]67102it [00:26, 3264.28it/s]70241it [00:26, 3413.86it/s]69028it [00:26, 3306.98it/s]68631it [00:26, 3410.80it/s]70613it [00:26, 3501.95it/s]67444it [00:26, 3218.76it/s]69395it [00:27, 3408.84it/s]68998it [00:26, 3483.50it/s]67810it [00:27, 3341.42it/s]70966it [00:26, 3366.97it/s]69757it [00:27, 3467.57it/s]69350it [00:27, 3293.07it/s]68175it [00:27, 3429.13it/s]71333it [00:27, 3452.21it/s]70106it [00:27, 3368.93it/s]69714it [00:27, 3390.06it/s]68525it [00:27, 3295.30it/s]71681it [00:27, 3350.74it/s]70473it [00:27, 3455.64it/s]70057it [00:27, 3316.96it/s]68892it [00:27, 3401.71it/s]72051it [00:27, 3450.46it/s]70821it [00:27, 3341.69it/s]70426it [00:27, 3422.27it/s]72398it [00:27, 3341.19it/s]69237it [00:27, 3286.89it/s]71182it [00:27, 3417.87it/s]70771it [00:27, 3325.51it/s]69600it [00:27, 3383.52it/s]72776it [00:27, 3464.99it/s]71544it [00:27, 3475.73it/s]71129it [00:27, 3390.15it/s]73159it [00:27, 3568.76it/s]69942it [00:27, 3294.41it/s]71893it [00:27, 3341.23it/s]71498it [00:27, 3476.44it/s]70309it [00:27, 3401.49it/s]73518it [00:27, 3444.35it/s]72253it [00:27, 3415.01it/s]71848it [00:27, 3372.44it/s]70678it [00:27, 3483.24it/s]73898it [00:27, 3545.46it/s]72597it [00:27, 3334.14it/s]72213it [00:27, 3452.07it/s]74255it [00:27, 3417.13it/s]72964it [00:28, 3429.20it/s]71029it [00:28, 3326.09it/s]72560it [00:27, 3242.50it/s]74617it [00:28, 3473.45it/s]71390it [00:28, 3406.05it/s]73309it [00:28, 3207.77it/s]72913it [00:28, 3312.61it/s]74966it [00:28, 3267.85it/s]73649it [00:28, 3253.25it/s]71733it [00:28, 3156.61it/s]73247it [00:28, 3108.13it/s]73984it [00:28, 3278.18it/s]75296it [00:28, 3233.44it/s]72055it [00:28, 3174.00it/s]73577it [00:28, 3160.80it/s]72376it [00:28, 3182.26it/s]75622it [00:28, 3107.18it/s]74314it [00:28, 3115.72it/s]73897it [00:28, 3169.40it/s]72697it [00:28, 3032.74it/s]75935it [00:28, 2971.77it/s]74629it [00:28, 2966.74it/s]74217it [00:28, 3057.03it/s]73064it [00:28, 3211.40it/s]76289it [00:28, 3127.07it/s]74932it [00:28, 2983.70it/s]74587it [00:28, 3237.88it/s]73389it [00:28, 3143.54it/s]76605it [00:28, 3108.87it/s]75294it [00:28, 3162.03it/s]74914it [00:28, 3194.71it/s]73767it [00:28, 3323.16it/s]76973it [00:28, 3270.02it/s]75661it [00:28, 3306.22it/s]75283it [00:28, 3336.28it/s]77347it [00:28, 3404.33it/s]74102it [00:28, 3254.39it/s]75994it [00:29, 3237.90it/s]75651it [00:28, 3434.72it/s]74468it [00:29, 3370.08it/s]76355it [00:29, 3343.16it/s]77690it [00:28, 3319.40it/s]75996it [00:29, 3328.44it/s]74824it [00:29, 3423.48it/s]78058it [00:29, 3418.61it/s]76691it [00:29, 3262.29it/s]76354it [00:29, 3401.04it/s]75168it [00:29, 3301.87it/s]77048it [00:29, 3350.85it/s]78402it [00:29, 3336.00it/s]76696it [00:29, 3302.03it/s]75529it [00:29, 3367.98it/s]77415it [00:29, 3442.29it/s]78768it [00:29, 3427.17it/s]77058it [00:29, 3392.96it/s]75868it [00:29, 3267.71it/s]77761it [00:29, 3331.03it/s]79112it [00:29, 3320.46it/s]77427it [00:29, 3477.79it/s]76226it [00:29, 3356.40it/s]78126it [00:29, 3421.84it/s]79479it [00:29, 3419.65it/s]77776it [00:29, 3361.72it/s]76580it [00:29, 3408.51it/s]79849it [00:29, 3501.01it/s]78470it [00:29, 3311.96it/s]78142it [00:29, 3447.05it/s]76922it [00:29, 3289.12it/s]78831it [00:29, 3397.15it/s]80201it [00:29, 3382.47it/s]78489it [00:29, 3354.41it/s]77279it [00:29, 3368.36it/s]80575it [00:29, 3485.42it/s]79173it [00:29, 3309.56it/s]78840it [00:29, 3398.99it/s]77618it [00:30, 3277.29it/s]79534it [00:30, 3395.23it/s]80926it [00:29, 3354.10it/s]79182it [00:29, 3307.75it/s]77973it [00:30, 3355.40it/s]79903it [00:30, 3480.58it/s]81289it [00:30, 3432.33it/s]79546it [00:30, 3401.60it/s]78310it [00:30, 3270.21it/s]80253it [00:30, 3359.36it/s]81634it [00:30, 3327.60it/s]79916it [00:30, 3487.19it/s]78675it [00:30, 3378.26it/s]80623it [00:30, 3455.42it/s]82003it [00:30, 3430.59it/s]80266it [00:30, 3362.85it/s]79031it [00:30, 3429.70it/s]82371it [00:30, 3500.86it/s]80971it [00:30, 3341.03it/s]80636it [00:30, 3458.42it/s]79375it [00:30, 3308.53it/s]81334it [00:30, 3423.12it/s]82723it [00:30, 3393.58it/s]80984it [00:30, 3352.28it/s]79732it [00:30, 3383.06it/s]83090it [00:30, 3472.69it/s]81678it [00:30, 3317.31it/s]81348it [00:30, 3433.24it/s]80072it [00:30, 3275.06it/s]82035it [00:30, 3389.61it/s]83439it [00:30, 3359.07it/s]81693it [00:30, 3320.41it/s]80433it [00:30, 3371.09it/s]82391it [00:30, 3436.71it/s]83805it [00:30, 3443.28it/s]82047it [00:30, 3383.22it/s]80790it [00:30, 3275.81it/s]84151it [00:30, 3336.81it/s]82736it [00:31, 3223.21it/s]82400it [00:30, 3421.12it/s]81120it [00:31, 3242.83it/s]84487it [00:30, 3226.30it/s]83062it [00:31, 3129.16it/s]82744it [00:31, 3202.80it/s]81446it [00:31, 3190.94it/s]83378it [00:31, 3002.55it/s]83068it [00:31, 3173.06it/s]84812it [00:31, 2777.93it/s]81766it [00:31, 3010.37it/s]83681it [00:31, 2935.90it/s]83388it [00:31, 3024.28it/s]85101it [00:31, 2722.24it/s]82095it [00:31, 3088.07it/s]83976it [00:31, 2902.84it/s]83722it [00:31, 3111.90it/s]85434it [00:31, 2880.23it/s]82460it [00:31, 3246.33it/s]84282it [00:31, 2945.23it/s]84083it [00:31, 3253.41it/s]85788it [00:31, 3059.13it/s]82787it [00:31, 3176.43it/s]84639it [00:31, 3123.49it/s]84411it [00:31, 3210.15it/s]83152it [00:31, 3310.82it/s]84989it [00:31, 3129.62it/s]84779it [00:31, 3345.63it/s]83485it [00:31, 3211.66it/s]85344it [00:31, 3248.34it/s]85116it [00:31, 3266.26it/s]83853it [00:31, 3343.88it/s]85712it [00:31, 3372.50it/s]85480it [00:31, 3370.22it/s]84190it [00:32, 3221.72it/s]85830it [00:31, 3286.03it/s]84559it [00:32, 3353.70it/s]84922it [00:32, 3432.46it/s]85267it [00:32, 3277.50it/s]85629it [00:32, 3372.48it/s]86101it [00:34, 366.98it/s] 86456it [00:34, 512.03it/s]86754it [00:34, 662.88it/s]87122it [00:34, 905.05it/s]87498it [00:34, 1198.88it/s]86051it [00:34, 375.58it/s] 86161it [00:34, 388.19it/s] 87822it [00:34, 1450.65it/s]86421it [00:34, 523.49it/s]86528it [00:34, 539.22it/s]88188it [00:34, 1790.48it/s]86737it [00:35, 678.49it/s]86827it [00:34, 690.45it/s]88521it [00:34, 2040.42it/s]87097it [00:35, 907.46it/s]87192it [00:34, 930.09it/s]88882it [00:35, 2357.54it/s]87465it [00:35, 1186.35it/s]87559it [00:35, 1214.32it/s]89254it [00:35, 2662.33it/s]87791it [00:35, 1436.54it/s]87884it [00:35, 1467.02it/s]89601it [00:35, 2772.06it/s]88156it [00:35, 1770.87it/s]88248it [00:35, 1802.78it/s]89966it [00:35, 2992.36it/s]88490it [00:35, 2023.84it/s]88581it [00:35, 2053.17it/s]90310it [00:35, 3001.32it/s]88858it [00:35, 2354.80it/s]88947it [00:35, 2379.14it/s]90664it [00:35, 3143.64it/s]89224it [00:35, 2644.38it/s]89285it [00:35, 2549.84it/s]91003it [00:35, 3098.85it/s]89571it [00:35, 2761.04it/s]89637it [00:35, 2780.40it/s]91357it [00:35, 3219.92it/s]89935it [00:35, 2981.33it/s]90003it [00:35, 3003.13it/s]85969it [00:35, 313.45it/s] 91713it [00:35, 3314.19it/s]90279it [00:36, 2987.08it/s]90348it [00:35, 3017.59it/s]86343it [00:36, 439.81it/s]92054it [00:35, 3218.60it/s]90631it [00:36, 3127.97it/s]90700it [00:36, 3151.81it/s]86694it [00:36, 593.43it/s]92407it [00:36, 3302.96it/s]90969it [00:36, 3083.58it/s]91039it [00:36, 3103.97it/s]86998it [00:36, 756.65it/s]92743it [00:36, 3206.97it/s]91322it [00:36, 3205.27it/s]91394it [00:36, 3225.98it/s]87367it [00:36, 1011.87it/s]93099it [00:36, 3305.20it/s]91673it [00:36, 3289.13it/s]91744it [00:36, 3303.38it/s]87690it [00:36, 1251.88it/s]93455it [00:36, 3377.87it/s]92012it [00:36, 3191.36it/s]88062it [00:36, 1586.87it/s]92084it [00:36, 3199.10it/s]93796it [00:36, 3257.60it/s]92362it [00:36, 3278.50it/s]92432it [00:36, 3278.25it/s]88420it [00:36, 1871.71it/s]94154it [00:36, 3347.70it/s]88786it [00:36, 2203.06it/s]92696it [00:36, 3074.95it/s]92766it [00:36, 3064.37it/s]94491it [00:36, 3221.77it/s]93010it [00:36, 3071.41it/s]89127it [00:36, 2388.11it/s]93079it [00:36, 2902.86it/s]94816it [00:36, 3224.53it/s]93322it [00:37, 3035.45it/s]89456it [00:37, 2437.29it/s]93375it [00:36, 2791.62it/s]95141it [00:36, 3002.54it/s]93629it [00:37, 2887.79it/s]89773it [00:37, 2605.61it/s]93658it [00:37, 2734.88it/s]95453it [00:37, 3032.54it/s]93943it [00:37, 2956.83it/s]90087it [00:37, 2737.66it/s]93974it [00:37, 2850.11it/s]95765it [00:37, 3055.45it/s]94279it [00:37, 3070.05it/s]90398it [00:37, 2787.40it/s]96073it [00:37, 3007.72it/s]94298it [00:37, 2857.95it/s]94589it [00:37, 3022.25it/s]90754it [00:37, 2994.34it/s]96431it [00:37, 3170.13it/s]94653it [00:37, 3049.80it/s]94939it [00:37, 3158.09it/s]91075it [00:37, 2995.84it/s]96790it [00:37, 3290.09it/s]95004it [00:37, 3179.39it/s]95257it [00:37, 3086.32it/s]91430it [00:37, 3148.59it/s]95325it [00:37, 3117.38it/s]97121it [00:37, 3194.10it/s]95604it [00:37, 3196.73it/s]91780it [00:37, 3094.29it/s]95675it [00:37, 3226.95it/s]97480it [00:37, 3308.02it/s]95951it [00:37, 3275.24it/s]92134it [00:37, 3218.64it/s]96000it [00:37, 3141.23it/s]97813it [00:37, 3198.29it/s]96280it [00:37, 3147.64it/s]92487it [00:37, 3306.37it/s]96352it [00:37, 3250.05it/s]98168it [00:37, 3297.23it/s]96629it [00:38, 3245.97it/s]92823it [00:38, 3219.32it/s]96704it [00:37, 3328.19it/s]98500it [00:37, 3187.27it/s]96956it [00:38, 3148.82it/s]93172it [00:38, 3294.38it/s]97039it [00:38, 3209.48it/s]98857it [00:38, 3294.31it/s]97306it [00:38, 3247.13it/s]93505it [00:38, 3199.19it/s]97392it [00:38, 3300.98it/s]99214it [00:38, 3371.76it/s]97655it [00:38, 3316.41it/s]93861it [00:38, 3299.99it/s]97724it [00:38, 3201.60it/s]99553it [00:38, 3247.98it/s]97988it [00:38, 3185.66it/s]94216it [00:38, 3369.97it/s]98076it [00:38, 3291.79it/s]99900it [00:38, 3310.54it/s]98336it [00:38, 3268.30it/s]94555it [00:38, 3235.43it/s]98426it [00:38, 3349.86it/s]100233it [00:38, 3200.61it/s]98665it [00:38, 3159.87it/s]94908it [00:38, 3319.17it/s]98763it [00:38, 3229.03it/s]100587it [00:38, 3296.94it/s]99014it [00:38, 3251.89it/s]95242it [00:38, 3215.65it/s]99104it [00:38, 3280.60it/s]100941it [00:38, 3364.80it/s]99341it [00:38, 3146.30it/s]95595it [00:38, 3305.19it/s]99434it [00:38, 3179.67it/s]101279it [00:38, 3237.14it/s]99683it [00:39, 3223.60it/s]95948it [00:38, 3368.33it/s]99785it [00:38, 3273.97it/s]101630it [00:38, 3314.83it/s]100032it [00:39, 3298.63it/s]96287it [00:39, 3226.85it/s]100138it [00:39, 3346.73it/s]101964it [00:39, 3200.08it/s]100364it [00:39, 3167.86it/s]96641it [00:39, 3315.25it/s]100474it [00:39, 3223.24it/s]102319it [00:39, 3298.53it/s]100711it [00:39, 3253.84it/s]96975it [00:39, 3208.70it/s]100824it [00:39, 3302.30it/s]102672it [00:39, 3364.18it/s]101039it [00:39, 3137.61it/s]97333it [00:39, 3313.98it/s]101156it [00:39, 3197.33it/s]103010it [00:39, 3252.93it/s]101384it [00:39, 3226.46it/s]97667it [00:39, 3199.75it/s]101503it [00:39, 3275.04it/s]103367it [00:39, 3343.18it/s]101719it [00:39, 3261.97it/s]98024it [00:39, 3303.20it/s]101841it [00:39, 3305.19it/s]103703it [00:39, 3227.69it/s]98379it [00:39, 3373.22it/s]102047it [00:39, 3048.44it/s]104062it [00:39, 3329.41it/s]102173it [00:39, 3088.52it/s]102367it [00:39, 3088.61it/s]98718it [00:39, 3165.35it/s]102486it [00:39, 3094.87it/s]104397it [00:39, 2863.82it/s]102679it [00:39, 3056.82it/s]99038it [00:39, 3145.20it/s]102798it [00:39, 2937.18it/s]104710it [00:39, 2933.27it/s]102987it [00:40, 2900.67it/s]103095it [00:39, 2921.93it/s]99355it [00:40, 2871.32it/s]105013it [00:40, 2950.54it/s]103299it [00:40, 2961.47it/s]103390it [00:40, 2884.84it/s]99671it [00:40, 2949.10it/s]105315it [00:40, 2896.91it/s]103598it [00:40, 2930.55it/s]100015it [00:40, 3085.18it/s]103680it [00:40, 2859.30it/s]105668it [00:40, 3072.48it/s]103948it [00:40, 3093.41it/s]104033it [00:40, 3050.87it/s]100328it [00:40, 3030.36it/s]106024it [00:40, 3210.72it/s]104300it [00:40, 3216.39it/s]100677it [00:40, 3161.03it/s]104379it [00:40, 3028.89it/s]106349it [00:40, 3129.93it/s]104624it [00:40, 3123.00it/s]104731it [00:40, 3166.86it/s]101020it [00:40, 3110.53it/s]106707it [00:40, 3257.45it/s]104977it [00:40, 3238.68it/s]105083it [00:40, 3266.52it/s]101373it [00:40, 3228.37it/s]107036it [00:40, 3166.56it/s]105303it [00:40, 3134.89it/s]101723it [00:40, 3305.46it/s]105412it [00:40, 3166.20it/s]107388it [00:40, 3266.69it/s]105654it [00:40, 3242.52it/s]105762it [00:40, 3261.76it/s]102056it [00:40, 3196.46it/s]107737it [00:40, 3182.17it/s]105999it [00:41, 3300.56it/s]102411it [00:41, 3296.05it/s]106090it [00:40, 3162.13it/s]108093it [00:40, 3289.37it/s]106331it [00:41, 3191.80it/s]106442it [00:41, 3263.92it/s]102743it [00:41, 3198.09it/s]108448it [00:41, 3363.16it/s]106684it [00:41, 3288.59it/s]106795it [00:41, 3340.60it/s]103095it [00:41, 3288.75it/s]108786it [00:41, 3250.79it/s]107015it [00:41, 3177.67it/s]103446it [00:41, 3352.47it/s]107131it [00:41, 3220.77it/s]109140it [00:41, 3331.32it/s]107368it [00:41, 3276.56it/s]107481it [00:41, 3299.57it/s]103783it [00:41, 3211.59it/s]109475it [00:41, 3226.88it/s]107721it [00:41, 3347.76it/s]104138it [00:41, 3307.08it/s]107813it [00:41, 3196.90it/s]109833it [00:41, 3327.01it/s]108058it [00:41, 3223.95it/s]108165it [00:41, 3289.25it/s]104471it [00:41, 3206.82it/s]110190it [00:41, 3395.86it/s]108411it [00:41, 3309.86it/s]108510it [00:41, 3334.26it/s]104827it [00:41, 3307.35it/s]110531it [00:41, 3248.30it/s]108744it [00:41, 3206.76it/s]105170it [00:41, 3341.42it/s]108845it [00:41, 3235.61it/s]110887it [00:41, 3331.57it/s]109097it [00:41, 3280.06it/s]109197it [00:41, 3316.19it/s]105506it [00:41, 3161.26it/s]111222it [00:41, 3226.73it/s]109427it [00:42, 3174.22it/s]109530it [00:41, 3206.99it/s]105861it [00:42, 3271.09it/s]111574it [00:42, 3308.99it/s]109777it [00:42, 3266.34it/s]109878it [00:42, 3284.15it/s]106191it [00:42, 3185.40it/s]111928it [00:42, 3373.34it/s]110128it [00:42, 3336.76it/s]110227it [00:42, 3343.02it/s]106545it [00:42, 3286.20it/s]112267it [00:42, 3239.80it/s]110463it [00:42, 3214.02it/s]106890it [00:42, 3332.57it/s]110563it [00:42, 3221.84it/s]112622it [00:42, 3326.91it/s]110817it [00:42, 3306.08it/s]110910it [00:42, 3292.92it/s]107225it [00:42, 3222.44it/s]112957it [00:42, 3207.97it/s]111150it [00:42, 3067.28it/s]107579it [00:42, 3312.54it/s]111241it [00:42, 3082.69it/s]113313it [00:42, 3307.76it/s]111466it [00:42, 3091.57it/s]111568it [00:42, 3134.07it/s]107912it [00:42, 3152.63it/s]113646it [00:42, 3097.31it/s]111779it [00:42, 3096.07it/s]111885it [00:42, 3121.14it/s]108230it [00:42, 3072.98it/s]113960it [00:42, 3105.86it/s]112091it [00:42, 2908.01it/s]108543it [00:42, 3087.15it/s]112199it [00:42, 2951.07it/s]114274it [00:42, 3021.60it/s]112393it [00:43, 2937.33it/s]112510it [00:42, 2993.70it/s]108854it [00:43, 2941.70it/s]114579it [00:42, 2936.09it/s]112733it [00:43, 3067.41it/s]112812it [00:43, 2961.85it/s]109208it [00:43, 3107.57it/s]114930it [00:43, 3095.21it/s]113043it [00:43, 3027.85it/s]113154it [00:43, 3092.92it/s]109522it [00:43, 3048.26it/s]115282it [00:43, 3214.58it/s]113398it [00:43, 3176.85it/s]113505it [00:43, 3213.62it/s]109882it [00:43, 3204.30it/s]113718it [00:43, 3102.48it/s]113828it [00:43, 3141.20it/s]110241it [00:43, 3313.97it/s]114076it [00:43, 3238.47it/s]114179it [00:43, 3246.90it/s]110575it [00:43, 3214.92it/s]114433it [00:43, 3333.03it/s]114505it [00:43, 3164.10it/s]110933it [00:43, 3319.64it/s]114768it [00:43, 3209.41it/s]114852it [00:43, 3250.59it/s]111267it [00:43, 3198.81it/s]115119it [00:43, 3295.69it/s]115205it [00:43, 3331.10it/s]111620it [00:43, 3293.11it/s]111952it [00:44, 3192.99it/s]112308it [00:44, 3296.63it/s]112654it [00:44, 3343.61it/s]112990it [00:44, 3228.30it/s]113349it [00:44, 3330.86it/s]113684it [00:44, 3232.81it/s]114041it [00:44, 3329.09it/s]114395it [00:44, 3387.86it/s]114735it [00:44, 3275.75it/s]115090it [00:44, 3354.22it/s]115606it [00:46, 304.93it/s] 115964it [00:46, 428.57it/s]116252it [00:46, 554.01it/s]116614it [00:46, 762.78it/s]116975it [00:46, 1014.49it/s]117293it [00:47, 1248.39it/s]117649it [00:47, 1565.52it/s]115540it [00:47, 307.30it/s] 117974it [00:47, 1814.33it/s]115897it [00:47, 429.03it/s]118333it [00:47, 2146.60it/s]116207it [00:47, 562.95it/s]118691it [00:47, 2449.17it/s]116557it [00:47, 760.44it/s]115451it [00:47, 280.50it/s] 116912it [00:47, 1004.94it/s]119029it [00:47, 2585.67it/s]115807it [00:47, 392.44it/s]119383it [00:47, 2817.40it/s]117229it [00:47, 1237.79it/s]116163it [00:47, 539.70it/s]117582it [00:47, 1550.31it/s]119718it [00:47, 2856.61it/s]116460it [00:47, 690.69it/s]117906it [00:47, 1800.85it/s]120072it [00:47, 3034.86it/s]116805it [00:48, 915.98it/s]118258it [00:48, 2121.88it/s]120405it [00:48, 2969.62it/s]117116it [00:48, 1138.75it/s]118610it [00:48, 2416.38it/s]120762it [00:48, 3130.26it/s]117471it [00:48, 1449.98it/s]118945it [00:48, 2543.84it/s]121116it [00:48, 3243.09it/s]117818it [00:48, 1764.66it/s]119283it [00:48, 2746.21it/s]121453it [00:48, 3170.57it/s]118147it [00:48, 1957.23it/s]119610it [00:48, 2756.39it/s]121779it [00:48, 3132.93it/s]118469it [00:48, 2208.38it/s]119925it [00:48, 2857.98it/s]122099it [00:48, 2964.55it/s]118783it [00:48, 2315.87it/s]120243it [00:48, 2943.93it/s]122421it [00:48, 3033.11it/s]119094it [00:48, 2499.67it/s]120558it [00:48, 2855.64it/s]119410it [00:48, 2663.81it/s]122729it [00:48, 2993.08it/s]115427it [00:48, 269.39it/s] 120898it [00:48, 3003.88it/s]119716it [00:49, 2694.64it/s]123032it [00:48, 2898.34it/s]115781it [00:49, 375.11it/s]121246it [00:48, 3135.62it/s]120073it [00:49, 2928.29it/s]123379it [00:49, 3057.66it/s]116136it [00:49, 515.26it/s]121569it [00:49, 3084.55it/s]123729it [00:49, 3183.71it/s]120406it [00:49, 2948.67it/s]116434it [00:49, 660.95it/s]121922it [00:49, 3211.38it/s]120764it [00:49, 3122.49it/s]124050it [00:49, 3113.03it/s]116792it [00:49, 888.81it/s]122249it [00:49, 3142.87it/s]121122it [00:49, 3251.19it/s]124405it [00:49, 3238.41it/s]117107it [00:49, 1107.40it/s]122607it [00:49, 3267.33it/s]124731it [00:49, 3166.15it/s]121457it [00:49, 3166.89it/s]117464it [00:49, 1413.70it/s]122937it [00:49, 3174.45it/s]125086it [00:49, 3275.10it/s]121811it [00:49, 3270.77it/s]117819it [00:49, 1734.87it/s]123288it [00:49, 3269.35it/s]125437it [00:49, 3341.62it/s]122144it [00:49, 3169.75it/s]118151it [00:49, 1972.80it/s]123639it [00:49, 3338.42it/s]125773it [00:49, 3240.60it/s]122500it [00:49, 3278.58it/s]118498it [00:49, 2269.10it/s]123975it [00:49, 3227.09it/s]126132it [00:49, 3340.84it/s]122854it [00:49, 3352.46it/s]118827it [00:50, 2433.29it/s]124327it [00:49, 3310.96it/s]126468it [00:49, 3229.77it/s]123192it [00:50, 3207.37it/s]119178it [00:50, 2685.21it/s]124660it [00:50, 3202.52it/s]126820it [00:50, 3310.59it/s]123547it [00:50, 3302.93it/s]119526it [00:50, 2884.89it/s]125015it [00:50, 3301.82it/s]127153it [00:50, 3186.14it/s]123880it [00:50, 3196.34it/s]119860it [00:50, 2881.96it/s]125366it [00:50, 3361.35it/s]127501it [00:50, 3267.42it/s]124234it [00:50, 3293.88it/s]120213it [00:50, 3052.44it/s]125704it [00:50, 3242.18it/s]127847it [00:50, 3321.39it/s]124589it [00:50, 3365.54it/s]120543it [00:50, 3031.91it/s]126049it [00:50, 3301.11it/s]128181it [00:50, 3199.96it/s]124928it [00:50, 3240.76it/s]120895it [00:50, 3166.13it/s]126381it [00:50, 3197.35it/s]128532it [00:50, 3288.69it/s]125279it [00:50, 3317.21it/s]121249it [00:50, 3107.55it/s]126730it [00:50, 3279.77it/s]128863it [00:50, 3191.91it/s]125613it [00:50, 3205.56it/s]121596it [00:50, 3207.08it/s]127083it [00:50, 3350.81it/s]129213it [00:50, 3278.58it/s]125971it [00:50, 3310.83it/s]121945it [00:50, 3285.52it/s]127420it [00:50, 3242.28it/s]129556it [00:50, 3320.82it/s]126304it [00:51, 3182.64it/s]122280it [00:51, 3181.07it/s]127775it [00:50, 3329.76it/s]129890it [00:50, 3202.87it/s]126660it [00:51, 3289.03it/s]122637it [00:51, 3288.87it/s]128110it [00:51, 3225.92it/s]130239it [00:51, 3283.60it/s]127016it [00:51, 3366.75it/s]122970it [00:51, 3159.03it/s]128459it [00:51, 3301.57it/s]130569it [00:51, 3173.52it/s]127355it [00:51, 3152.06it/s]123321it [00:51, 3258.03it/s]128803it [00:51, 3341.64it/s]130915it [00:51, 3254.98it/s]127701it [00:51, 3236.49it/s]123661it [00:51, 3296.24it/s]129139it [00:51, 3102.83it/s]131242it [00:51, 3176.05it/s]128028it [00:51, 3036.27it/s]123993it [00:51, 3055.19it/s]129454it [00:51, 3100.91it/s]131561it [00:51, 2979.99it/s]128339it [00:51, 3054.33it/s]124312it [00:51, 3089.35it/s]129767it [00:51, 2961.82it/s]131864it [00:51, 2993.39it/s]128660it [00:51, 3096.17it/s]130077it [00:51, 3000.03it/s]124625it [00:51, 2748.56it/s]132166it [00:51, 2864.45it/s]128972it [00:51, 2954.99it/s]130419it [00:51, 3117.68it/s]124974it [00:51, 2942.49it/s]132516it [00:51, 3031.10it/s]129311it [00:52, 3076.16it/s]130733it [00:51, 3078.84it/s]125316it [00:52, 3071.50it/s]132866it [00:51, 3157.70it/s]129647it [00:52, 3013.51it/s]131091it [00:52, 3221.43it/s]125631it [00:52, 3024.65it/s]133185it [00:52, 3099.76it/s]130006it [00:52, 3174.73it/s]131415it [00:52, 3140.53it/s]125975it [00:52, 3140.43it/s]133542it [00:52, 3220.49it/s]130363it [00:52, 3287.45it/s]131770it [00:52, 3257.17it/s]126294it [00:52, 3077.51it/s]133866it [00:52, 3133.60it/s]130695it [00:52, 3189.42it/s]132125it [00:52, 3340.16it/s]126648it [00:52, 3208.44it/s]134225it [00:52, 3261.87it/s]131050it [00:52, 3291.43it/s]132461it [00:52, 3232.34it/s]127002it [00:52, 3303.68it/s]134577it [00:52, 3334.86it/s]131382it [00:52, 3191.36it/s]132818it [00:52, 3329.60it/s]127335it [00:52, 3177.17it/s]134912it [00:52, 3222.18it/s]131736it [00:52, 3291.03it/s]133153it [00:52, 3226.08it/s]127656it [00:52, 3167.88it/s]135262it [00:52, 3302.05it/s]132093it [00:52, 3370.99it/s]133510it [00:52, 3323.48it/s]127975it [00:52, 3060.69it/s]135594it [00:52, 3198.46it/s]132432it [00:52, 3242.37it/s]133848it [00:52, 3223.02it/s]128327it [00:53, 3189.74it/s]135946it [00:52, 3286.19it/s]132775it [00:53, 3295.17it/s]134203it [00:52, 3315.17it/s]128677it [00:53, 3278.24it/s]136300it [00:53, 3357.94it/s]133107it [00:53, 3190.96it/s]134558it [00:53, 3382.20it/s]129007it [00:53, 3146.93it/s]136637it [00:53, 3253.56it/s]133459it [00:53, 3283.21it/s]134898it [00:53, 3268.70it/s]129354it [00:53, 3236.72it/s]136989it [00:53, 3329.97it/s]133817it [00:53, 3367.63it/s]135251it [00:53, 3341.97it/s]129680it [00:53, 3131.29it/s]137324it [00:53, 3203.06it/s]134156it [00:53, 3241.18it/s]135587it [00:53, 3248.61it/s]130026it [00:53, 3223.41it/s]137673it [00:53, 3283.51it/s]134510it [00:53, 3325.73it/s]135945it [00:53, 3341.92it/s]130372it [00:53, 3290.46it/s]138025it [00:53, 3350.41it/s]134845it [00:53, 3221.94it/s]136299it [00:53, 3398.06it/s]130703it [00:53, 3164.00it/s]138362it [00:53, 3213.24it/s]135200it [00:53, 3314.20it/s]136640it [00:53, 3284.75it/s]131048it [00:53, 3244.11it/s]138709it [00:53, 3285.89it/s]135533it [00:53, 3214.24it/s]136997it [00:53, 3364.97it/s]131375it [00:53, 3132.14it/s]139040it [00:53, 3205.82it/s]135889it [00:54, 3310.75it/s]137335it [00:53, 3253.16it/s]131729it [00:54, 3246.47it/s]139390it [00:53, 3288.98it/s]136232it [00:54, 3344.03it/s]137692it [00:54, 3342.54it/s]132081it [00:54, 3324.49it/s]139726it [00:54, 3199.94it/s]136568it [00:54, 3240.90it/s]138048it [00:54, 3168.81it/s]132415it [00:54, 3193.36it/s]140085it [00:54, 3310.21it/s]136912it [00:54, 3297.10it/s]138395it [00:54, 3251.85it/s]132767it [00:54, 3286.68it/s]140442it [00:54, 3383.35it/s]137243it [00:54, 3162.29it/s]138731it [00:54, 3282.23it/s]133098it [00:54, 3127.05it/s]140782it [00:54, 3127.04it/s]137561it [00:54, 2953.94it/s]139062it [00:54, 3056.41it/s]133414it [00:54, 3123.90it/s]141100it [00:54, 3137.12it/s]137863it [00:54, 2971.33it/s]139379it [00:54, 3085.67it/s]133729it [00:54, 3115.27it/s]141417it [00:54, 2966.13it/s]138163it [00:54, 2858.38it/s]139693it [00:54, 3098.55it/s]134042it [00:54, 2903.50it/s]141735it [00:54, 3024.86it/s]138482it [00:54, 2948.76it/s]140006it [00:54, 3002.96it/s]134394it [00:54, 3072.79it/s]142090it [00:54, 3173.35it/s]138831it [00:54, 3102.87it/s]140354it [00:54, 3137.37it/s]134705it [00:55, 3041.96it/s]142411it [00:54, 3088.68it/s]139144it [00:55, 3047.19it/s]140670it [00:54, 3094.50it/s]135062it [00:55, 3192.86it/s]142768it [00:55, 3225.91it/s]139503it [00:55, 3203.31it/s]141032it [00:55, 3244.16it/s]135414it [00:55, 3286.89it/s]143093it [00:55, 3092.47it/s]139826it [00:55, 3139.17it/s]141395it [00:55, 3354.50it/s]135745it [00:55, 3200.11it/s]143450it [00:55, 3226.96it/s]140186it [00:55, 3270.90it/s]141732it [00:55, 3258.57it/s]136067it [00:55, 3192.06it/s]143807it [00:55, 3325.58it/s]140542it [00:55, 3353.18it/s]142094it [00:55, 3360.66it/s]136388it [00:55, 3122.33it/s]144142it [00:55, 3166.40it/s]140879it [00:55, 3252.50it/s]142432it [00:55, 3262.67it/s]136745it [00:55, 3250.33it/s]144501it [00:55, 3285.43it/s]141240it [00:55, 3353.62it/s]142793it [00:55, 3360.90it/s]137101it [00:55, 3340.48it/s]144833it [00:55, 3210.63it/s]141577it [00:55, 3249.56it/s]143131it [00:55, 3266.02it/s]137437it [00:55, 3191.80it/s]145189it [00:55, 3308.69it/s]141936it [00:55, 3345.61it/s]143494it [00:55, 3369.81it/s]137792it [00:55, 3293.73it/s]145536it [00:55, 3349.33it/s]142272it [00:56, 3244.68it/s]143857it [00:55, 3443.54it/s]138124it [00:56, 3203.64it/s]145873it [00:55, 3260.34it/s]142622it [00:56, 3315.57it/s]144203it [00:56, 3321.67it/s]138477it [00:56, 3296.12it/s]146221it [00:56, 3322.62it/s]142981it [00:56, 3394.62it/s]144566it [00:56, 3407.96it/s]138827it [00:56, 3354.04it/s]146555it [00:56, 3223.33it/s]143322it [00:56, 3281.15it/s]144909it [00:56, 3281.75it/s]139164it [00:56, 3249.02it/s]146905it [00:56, 3301.26it/s]143682it [00:56, 3371.23it/s]145274it [00:56, 3384.30it/s]139509it [00:56, 3304.67it/s]147261it [00:56, 3376.00it/s]144021it [00:56, 3263.20it/s]145615it [00:56, 3292.23it/s]139841it [00:56, 3212.05it/s]147600it [00:56, 3248.22it/s]144378it [00:56, 3350.78it/s]145981it [00:56, 3395.16it/s]140188it [00:56, 3284.05it/s]147957it [00:56, 3340.16it/s]144742it [00:56, 3434.44it/s]146342it [00:56, 3457.13it/s]140547it [00:56, 3370.88it/s]148293it [00:56, 3227.78it/s]145087it [00:56, 3312.56it/s]146689it [00:56, 3329.18it/s]140886it [00:56, 3239.19it/s]148649it [00:56, 3321.26it/s]145448it [00:56, 3395.55it/s]147052it [00:56, 3415.27it/s]141244it [00:57, 3336.80it/s]148983it [00:56, 3201.60it/s]145790it [00:57, 3251.58it/s]147396it [00:56, 3302.74it/s]141580it [00:57, 3206.70it/s]149338it [00:57, 3300.16it/s]146142it [00:57, 3326.03it/s]147744it [00:57, 3351.74it/s]141934it [00:57, 3300.42it/s]149688it [00:57, 3357.90it/s]146477it [00:57, 3128.11it/s]148081it [00:57, 3263.71it/s]142266it [00:57, 3128.47it/s]150026it [00:57, 3151.57it/s]146794it [00:57, 3102.60it/s]142582it [00:57, 3041.17it/s]148409it [00:57, 2898.40it/s]150345it [00:57, 3091.37it/s]147107it [00:57, 3058.56it/s]148726it [00:57, 2970.40it/s]142889it [00:57, 2834.25it/s]150657it [00:57, 2938.16it/s]147415it [00:57, 2937.28it/s]149030it [00:57, 2791.36it/s]143176it [00:57, 2746.27it/s]150979it [00:57, 3014.49it/s]147726it [00:57, 2985.39it/s]149359it [00:57, 2916.32it/s]143525it [00:57, 2948.10it/s]151325it [00:57, 3139.07it/s]148077it [00:57, 3134.72it/s]149714it [00:57, 3091.28it/s]143881it [00:57, 3118.98it/s]151642it [00:57, 3101.19it/s]148393it [00:57, 3094.14it/s]150029it [00:57, 3049.55it/s]144197it [00:58, 3059.67it/s]151995it [00:57, 3223.07it/s]148747it [00:58, 3221.93it/s]150388it [00:57, 3202.72it/s]144558it [00:58, 3189.17it/s]152326it [00:58, 3165.67it/s]149071it [00:58, 3139.86it/s]150712it [00:58, 3145.03it/s]144880it [00:58, 3130.82it/s]149427it [00:58, 3259.32it/s]151070it [00:58, 3269.30it/s]145231it [00:58, 3235.50it/s]149786it [00:58, 3355.02it/s]151427it [00:58, 3356.41it/s]145591it [00:58, 3339.58it/s]150123it [00:58, 3246.20it/s]151765it [00:58, 3249.21it/s]145927it [00:58, 3203.44it/s]150480it [00:58, 3337.35it/s]152123it [00:58, 3344.09it/s]146285it [00:58, 3309.29it/s]150816it [00:58, 3236.16it/s]151175it [00:58, 3336.27it/s]146618it [00:58, 3166.02it/s]146977it [00:58, 3285.88it/s]151511it [00:58, 3230.36it/s]151873it [00:58, 3339.83it/s]147308it [00:58, 3147.31it/s]152232it [00:59, 3410.22it/s]147663it [00:59, 3260.90it/s]148024it [00:59, 3360.55it/s]148363it [00:59, 3214.29it/s]148721it [00:59, 3316.52it/s]149056it [00:59, 3183.87it/s]149414it [00:59, 3293.71it/s]149769it [00:59, 3367.30it/s]150108it [00:59, 3255.80it/s]150461it [00:59, 3332.60it/s]150796it [01:00, 3233.93it/s]151145it [01:00, 3306.42it/s]151481it [01:00, 3320.88it/s]151815it [01:00, 3069.55it/s]152132it [01:00, 3095.08it/s]152460it [01:03, 232.78it/s] 152761it [01:03, 310.89it/s]153078it [01:03, 421.35it/s]153371it [01:03, 552.53it/s]152575it [01:03, 238.59it/s] 153731it [01:03, 764.28it/s]152937it [01:03, 334.65it/s]154077it [01:03, 997.28it/s]153238it [01:03, 438.96it/s]154442it [01:03, 1297.67it/s]153600it [01:04, 606.78it/s]154798it [01:03, 1612.59it/s]153951it [01:04, 810.26it/s]155132it [01:04, 1872.33it/s]154272it [01:04, 1024.08it/s]155495it [01:04, 2204.30it/s]154635it [01:04, 1322.45it/s]155832it [01:04, 2400.82it/s]154965it [01:04, 1582.06it/s]156195it [01:04, 2681.95it/s]155329it [01:04, 1922.81it/s]156559it [01:04, 2917.19it/s]155687it [01:04, 2239.93it/s]152644it [01:04, 161.92it/s] 156905it [01:04, 2965.77it/s]156029it [01:04, 2422.95it/s]153007it [01:04, 233.46it/s]157271it [01:04, 3148.88it/s]156391it [01:04, 2697.23it/s]153296it [01:04, 309.50it/s]157615it [01:04, 3136.58it/s]156731it [01:04, 2800.15it/s]153656it [01:04, 438.78it/s]157979it [01:04, 3275.43it/s]157092it [01:05, 3006.35it/s]154013it [01:04, 605.18it/s]158322it [01:04, 3221.03it/s]157437it [01:05, 3035.36it/s]154330it [01:05, 783.24it/s]158689it [01:05, 3345.59it/s]157803it [01:05, 3203.02it/s]154684it [01:05, 1034.67it/s]159058it [01:05, 3443.60it/s]158157it [01:05, 3296.57it/s]155009it [01:05, 1277.26it/s]159409it [01:05, 3343.62it/s]155364it [01:05, 1595.94it/s]158502it [01:05, 3238.45it/s]159778it [01:05, 3440.99it/s]155727it [01:05, 1935.04it/s]158868it [01:05, 3356.53it/s]160126it [01:05, 3328.46it/s]159212it [01:05, 3286.51it/s]156066it [01:05, 2154.27it/s]160479it [01:05, 3383.31it/s]159570it [01:05, 3370.20it/s]156427it [01:05, 2461.23it/s]160820it [01:05, 3202.95it/s]159920it [01:05, 3405.79it/s]156764it [01:05, 2587.10it/s]161177it [01:05, 3305.77it/s]157097it [01:05, 2765.77it/s]160264it [01:06, 3170.08it/s]161511it [01:05, 2978.03it/s]160587it [01:06, 3176.27it/s]157424it [01:05, 2792.62it/s]161817it [01:06, 2905.83it/s]160909it [01:06, 3052.62it/s]157739it [01:06, 2719.57it/s]162142it [01:06, 2998.86it/s]161239it [01:06, 3120.04it/s]152445it [01:06, 178.10it/s] 158047it [01:06, 2812.07it/s]162477it [01:06, 2961.68it/s]161595it [01:06, 3244.31it/s]152792it [01:06, 253.13it/s]158347it [01:06, 2839.77it/s]162845it [01:06, 3159.00it/s]153152it [01:06, 358.65it/s]161922it [01:06, 3201.45it/s]158698it [01:06, 3025.18it/s]163212it [01:06, 3302.84it/s]153443it [01:06, 468.58it/s]162290it [01:06, 3337.90it/s]159064it [01:06, 3203.90it/s]163546it [01:06, 3245.37it/s]153803it [01:06, 650.13it/s]162626it [01:06, 3252.76it/s]159393it [01:06, 3136.26it/s]163914it [01:06, 3367.93it/s]154114it [01:06, 835.13it/s]162991it [01:06, 3367.07it/s]159752it [01:06, 3264.42it/s]164254it [01:06, 3288.59it/s]154476it [01:06, 1108.33it/s]163330it [01:06, 3287.50it/s]160084it [01:06, 3158.62it/s]164621it [01:06, 3397.23it/s]154826it [01:07, 1402.41it/s]163702it [01:07, 3410.38it/s]160445it [01:06, 3286.98it/s]164997it [01:07, 3340.32it/s]155157it [01:07, 1665.68it/s]164073it [01:07, 3497.05it/s]160796it [01:07, 3189.17it/s]165366it [01:07, 3438.51it/s]155507it [01:07, 1984.14it/s]164424it [01:07, 3379.29it/s]161157it [01:07, 3307.20it/s]165736it [01:07, 3513.28it/s]155838it [01:07, 2205.27it/s]164784it [01:07, 3442.06it/s]161509it [01:07, 3367.61it/s]166089it [01:07, 3407.04it/s]156188it [01:07, 2486.23it/s]165130it [01:07, 3355.54it/s]161849it [01:07, 3269.13it/s]166432it [01:07, 3397.30it/s]156546it [01:07, 2744.82it/s]165498it [01:07, 3448.11it/s]162200it [01:07, 3335.76it/s]166773it [01:07, 3334.83it/s]156885it [01:07, 2806.33it/s]165845it [01:07, 3362.59it/s]162536it [01:07, 3251.37it/s]167142it [01:07, 3436.95it/s]157245it [01:07, 3011.29it/s]166223it [01:07, 3480.57it/s]162888it [01:07, 3326.53it/s]167516it [01:07, 3525.02it/s]166598it [01:07, 3558.55it/s]157581it [01:07, 3004.03it/s]163251it [01:07, 3412.76it/s]167870it [01:07, 3401.28it/s]157940it [01:07, 3162.89it/s]166955it [01:08, 3414.18it/s]163594it [01:07, 3285.20it/s]168244it [01:07, 3498.74it/s]158278it [01:08, 3105.08it/s]167326it [01:08, 3496.91it/s]163953it [01:07, 3370.54it/s]168596it [01:08, 3394.84it/s]158637it [01:08, 3239.37it/s]167678it [01:08, 3390.40it/s]164292it [01:08, 3267.11it/s]168963it [01:08, 3473.54it/s]158992it [01:08, 3326.86it/s]168048it [01:08, 3479.02it/s]164651it [01:08, 3359.40it/s]169312it [01:08, 3361.45it/s]159333it [01:08, 3250.83it/s]168398it [01:08, 3385.89it/s]164996it [01:08, 3248.06it/s]169677it [01:08, 3443.46it/s]159680it [01:08, 3311.48it/s]168770it [01:08, 3479.40it/s]165362it [01:08, 3363.31it/s]170038it [01:08, 3331.75it/s]160016it [01:08, 3234.37it/s]169128it [01:08, 3507.86it/s]165732it [01:08, 3460.21it/s]170407it [01:08, 3430.78it/s]160363it [01:08, 3300.88it/s]169480it [01:08, 3365.95it/s]166080it [01:08, 3328.04it/s]170764it [01:08, 3470.38it/s]160724it [01:08, 3388.32it/s]169836it [01:08, 3419.28it/s]166451it [01:08, 3436.22it/s]171113it [01:08, 3282.60it/s]161065it [01:08, 3200.82it/s]170180it [01:08, 3234.68it/s]166797it [01:08, 3227.14it/s]171445it [01:08, 3148.70it/s]161389it [01:09, 3183.63it/s]170511it [01:09, 3254.11it/s]167124it [01:08, 3233.19it/s]170840it [01:09, 3261.92it/s]171763it [01:09, 2966.51it/s]161710it [01:09, 3019.97it/s]167450it [01:09, 3240.07it/s]172091it [01:09, 3051.36it/s]162043it [01:09, 3105.16it/s]171168it [01:09, 3072.55it/s]167776it [01:09, 3113.31it/s]172459it [01:09, 3226.73it/s]162408it [01:09, 3254.71it/s]171533it [01:09, 3233.89it/s]168148it [01:09, 3283.75it/s]172786it [01:09, 3181.30it/s]162737it [01:09, 3213.99it/s]171860it [01:09, 3189.67it/s]168479it [01:09, 3213.98it/s]173154it [01:09, 3322.90it/s]163105it [01:09, 3348.28it/s]172230it [01:09, 3335.14it/s]168846it [01:09, 3343.97it/s]163442it [01:09, 3268.37it/s]173489it [01:09, 3233.44it/s]172566it [01:09, 3279.82it/s]169196it [01:09, 3266.92it/s]163803it [01:09, 3366.56it/s]173857it [01:09, 3359.75it/s]172939it [01:09, 3408.02it/s]169564it [01:09, 3384.74it/s]174226it [01:09, 3453.03it/s]173308it [01:09, 3489.58it/s]164158it [01:09, 3286.50it/s]169919it [01:09, 3431.23it/s]174573it [01:09, 3359.00it/s]164526it [01:09, 3398.60it/s]173659it [01:10, 3348.91it/s]170264it [01:09, 3330.71it/s]174932it [01:09, 3423.86it/s]164898it [01:10, 3490.13it/s]174026it [01:10, 3439.07it/s]170629it [01:10, 3421.19it/s]175276it [01:10, 3335.97it/s]165249it [01:10, 3277.13it/s]174372it [01:10, 3342.95it/s]170973it [01:10, 3312.44it/s]175639it [01:10, 3420.11it/s]165619it [01:10, 3396.02it/s]174747it [01:10, 3457.31it/s]171306it [01:10, 3311.81it/s]175983it [01:10, 3314.94it/s]165962it [01:10, 3322.79it/s]175095it [01:10, 3363.55it/s]171673it [01:10, 3413.99it/s]176344it [01:10, 3398.76it/s]166334it [01:10, 3433.62it/s]175463it [01:10, 3454.36it/s]172016it [01:10, 3319.43it/s]176713it [01:10, 3483.26it/s]175822it [01:10, 3491.80it/s]166680it [01:10, 3359.31it/s]172384it [01:10, 3423.54it/s]177063it [01:10, 3346.36it/s]167048it [01:10, 3449.78it/s]176173it [01:10, 3368.26it/s]172728it [01:10, 3315.74it/s]177431it [01:10, 3439.89it/s]167398it [01:10, 3462.36it/s]176540it [01:10, 3453.80it/s]173105it [01:10, 3444.08it/s]177777it [01:10, 3317.90it/s]167746it [01:10, 3354.95it/s]176887it [01:10, 3366.71it/s]173451it [01:10, 3248.78it/s]178156it [01:10, 3451.79it/s]168117it [01:11, 3455.42it/s]177253it [01:11, 3449.25it/s]173816it [01:10, 3353.71it/s]178504it [01:11, 3337.81it/s]168464it [01:11, 3360.16it/s]177600it [01:11, 3347.96it/s]174187it [01:11, 3453.06it/s]178867it [01:11, 3420.14it/s]168833it [01:11, 3454.78it/s]177964it [01:11, 3429.53it/s]174535it [01:11, 3367.83it/s]179230it [01:11, 3480.41it/s]169193it [01:11, 3495.21it/s]178339it [01:11, 3522.30it/s]174896it [01:11, 3436.68it/s]179580it [01:11, 3363.67it/s]169544it [01:11, 3371.84it/s]178693it [01:11, 3408.58it/s]175242it [01:11, 3310.89it/s]179938it [01:11, 3425.26it/s]169908it [01:11, 3447.63it/s]179058it [01:11, 3478.11it/s]175607it [01:11, 3406.97it/s]170255it [01:11, 3347.43it/s]180282it [01:11, 3231.80it/s]179408it [01:11, 3252.40it/s]175950it [01:11, 3302.97it/s]170595it [01:11, 3360.69it/s]180623it [01:11, 3280.33it/s]179756it [01:11, 3313.85it/s]176282it [01:11, 3297.31it/s]170933it [01:11, 3136.79it/s]180958it [01:11, 3115.65it/s]180091it [01:11, 3244.11it/s]176613it [01:11, 3251.85it/s]181280it [01:11, 3144.01it/s]171250it [01:12, 3083.46it/s]180418it [01:12, 3104.93it/s]176939it [01:11, 3064.36it/s]181597it [01:11, 3143.06it/s]171577it [01:12, 3134.35it/s]180731it [01:12, 3001.23it/s]177265it [01:12, 3118.61it/s]181913it [01:12, 3082.65it/s]171893it [01:12, 3056.26it/s]181039it [01:12, 3021.01it/s]177596it [01:12, 3095.50it/s]182287it [01:12, 3269.57it/s]172260it [01:12, 3229.10it/s]181405it [01:12, 3202.43it/s]177962it [01:12, 3256.02it/s]172585it [01:12, 3196.10it/s]182638it [01:12, 3224.95it/s]181775it [01:12, 3344.25it/s]178336it [01:12, 3394.40it/s]172957it [01:12, 3347.37it/s]183008it [01:12, 3359.65it/s]182112it [01:12, 3289.10it/s]178678it [01:12, 3304.44it/s]173317it [01:12, 3418.94it/s]183384it [01:12, 3475.70it/s]182475it [01:12, 3386.68it/s]179048it [01:12, 3417.72it/s]173661it [01:12, 3325.65it/s]183734it [01:12, 3359.92it/s]182815it [01:12, 3337.57it/s]179392it [01:12, 3291.47it/s]174028it [01:12, 3415.20it/s]184106it [01:12, 3462.15it/s]183186it [01:12, 3446.15it/s]179766it [01:12, 3410.72it/s]174371it [01:12, 3302.36it/s]184454it [01:12, 3361.15it/s]183532it [01:12, 3360.10it/s]180116it [01:12, 3319.48it/s]174744it [01:13, 3425.14it/s]184818it [01:12, 3440.25it/s]183903it [01:13, 3460.55it/s]180493it [01:12, 3447.98it/s]184282it [01:13, 3555.16it/s]175089it [01:13, 3334.28it/s]185164it [01:13, 3345.82it/s]180858it [01:13, 3505.11it/s]185506it [01:13, 3364.81it/s]184639it [01:13, 3418.81it/s]175424it [01:13, 3189.11it/s]181210it [01:13, 3380.41it/s]185878it [01:13, 3466.33it/s]185017it [01:13, 3517.84it/s]175787it [01:13, 3310.91it/s]181576it [01:13, 3459.30it/s]186226it [01:13, 3370.17it/s]176121it [01:13, 3240.86it/s]185371it [01:13, 3390.99it/s]181924it [01:13, 3351.89it/s]186602it [01:13, 3481.96it/s]176484it [01:13, 3349.53it/s]185743it [01:13, 3483.17it/s]182295it [01:13, 3453.44it/s]186952it [01:13, 3410.15it/s]176821it [01:13, 3295.91it/s]186094it [01:13, 3401.18it/s]182642it [01:13, 3363.84it/s]187324it [01:13, 3499.61it/s]177178it [01:13, 3366.64it/s]186471it [01:13, 3506.76it/s]183010it [01:13, 3452.45it/s]177546it [01:13, 3456.31it/s]187678it [01:13, 3378.14it/s]186837it [01:13, 3430.78it/s]183384it [01:13, 3535.86it/s]188055it [01:13, 3490.21it/s]177893it [01:13, 3370.46it/s]187210it [01:14, 3516.07it/s]183739it [01:13, 3383.09it/s]188429it [01:13, 3562.58it/s]178265it [01:14, 3470.00it/s]187584it [01:14, 3579.55it/s]184106it [01:14, 3463.82it/s]188787it [01:14, 3429.20it/s]178614it [01:14, 3346.07it/s]187944it [01:14, 3474.71it/s]184455it [01:14, 3369.91it/s]189149it [01:14, 3483.59it/s]178984it [01:14, 3445.09it/s]188316it [01:14, 3544.76it/s]184826it [01:14, 3466.47it/s]189499it [01:14, 3371.25it/s]179330it [01:14, 3338.77it/s]188672it [01:14, 3441.45it/s]185175it [01:14, 3347.34it/s]189862it [01:14, 3444.36it/s]179704it [01:14, 3452.18it/s]189018it [01:14, 3430.30it/s]185538it [01:14, 3427.08it/s]180063it [01:14, 3490.86it/s]190208it [01:14, 3237.19it/s]189362it [01:14, 3274.40it/s]185916it [01:14, 3528.62it/s]190538it [01:14, 3234.33it/s]180414it [01:14, 3230.09it/s]189692it [01:14, 2983.93it/s]186271it [01:14, 3264.69it/s]190864it [01:14, 3222.95it/s]190024it [01:14, 3072.24it/s]180742it [01:14, 2902.09it/s]186604it [01:14, 3282.78it/s]191188it [01:14, 3040.59it/s]190336it [01:15, 2933.59it/s]186936it [01:14, 3139.59it/s]181041it [01:15, 2754.20it/s]191520it [01:14, 3116.57it/s]190661it [01:15, 3018.25it/s]187275it [01:14, 3208.69it/s]181363it [01:15, 2874.54it/s]191878it [01:15, 3086.63it/s]191024it [01:15, 3189.99it/s]187636it [01:15, 3323.00it/s]181721it [01:15, 3066.03it/s]192251it [01:15, 3264.01it/s]191347it [01:15, 3154.31it/s]182034it [01:15, 3078.43it/s]187971it [01:15, 3267.39it/s]192630it [01:15, 3413.60it/s]191706it [01:15, 3276.90it/s]182409it [01:15, 3268.40it/s]188343it [01:15, 3396.47it/s]192974it [01:15, 3327.78it/s]192036it [01:15, 3252.01it/s]182740it [01:15, 3224.73it/s]188685it [01:15, 3328.55it/s]193344it [01:15, 3433.11it/s]192410it [01:15, 3391.60it/s]183113it [01:15, 3369.24it/s]189039it [01:15, 3387.96it/s]193690it [01:15, 3323.54it/s]192751it [01:15, 3343.96it/s]183478it [01:15, 3309.14it/s]189379it [01:15, 3299.90it/s]194069it [01:15, 3455.13it/s]193123it [01:15, 3452.19it/s]183852it [01:15, 3429.93it/s]189751it [01:15, 3419.50it/s]193488it [01:15, 3509.78it/s]194417it [01:15, 3346.25it/s]184215it [01:15, 3486.90it/s]190127it [01:15, 3516.45it/s]194792it [01:15, 3460.63it/s]193840it [01:16, 3406.32it/s]184566it [01:16, 3387.80it/s]190480it [01:15, 3354.88it/s]195178it [01:15, 3575.93it/s]194207it [01:16, 3482.66it/s]184940it [01:16, 3487.41it/s]190848it [01:16, 3446.83it/s]195538it [01:16, 3421.13it/s]194557it [01:16, 3352.64it/s]185291it [01:16, 3374.32it/s]191195it [01:16, 3327.60it/s]195904it [01:16, 3486.72it/s]194926it [01:16, 3448.83it/s]185648it [01:16, 3421.37it/s]191567it [01:16, 3438.55it/s]195273it [01:16, 3399.19it/s]196255it [01:16, 3356.07it/s]185998it [01:16, 3346.28it/s]191913it [01:16, 3315.60it/s]195642it [01:16, 3483.28it/s]196631it [01:16, 3470.31it/s]186377it [01:16, 3473.38it/s]192284it [01:16, 3425.71it/s]196003it [01:16, 3519.96it/s]196981it [01:16, 3348.62it/s]186768it [01:16, 3595.25it/s]192661it [01:16, 3524.47it/s]196356it [01:16, 3384.29it/s]197355it [01:16, 3459.60it/s]187129it [01:16, 3418.44it/s]193016it [01:16, 3409.18it/s]196731it [01:16, 3489.00it/s]197721it [01:16, 3517.05it/s]187505it [01:16, 3514.65it/s]193375it [01:16, 3460.07it/s]197082it [01:16, 3397.32it/s]198075it [01:16, 3426.91it/s]187859it [01:17, 3420.64it/s]193723it [01:16, 3352.75it/s]197457it [01:17, 3496.70it/s]198444it [01:16, 3501.04it/s]188222it [01:17, 3478.57it/s]194105it [01:16, 3486.68it/s]197809it [01:17, 3401.26it/s]188572it [01:17, 3393.27it/s]194456it [01:17, 3389.32it/s]198182it [01:17, 3493.70it/s]188938it [01:17, 3468.45it/s]194809it [01:17, 3429.32it/s]198546it [01:17, 3535.19it/s]189308it [01:17, 3535.12it/s]195197it [01:17, 3558.32it/s]189663it [01:17, 3373.66it/s]195555it [01:17, 3426.33it/s]195900it [01:17, 3422.38it/s]190003it [01:17, 3325.49it/s]190337it [01:17, 3185.08it/s]196244it [01:17, 2945.92it/s]190658it [01:17, 3138.89it/s]196551it [01:17, 2819.55it/s]190974it [01:17, 3072.91it/s]196842it [01:17, 2770.88it/s]191283it [01:18, 3021.12it/s]197126it [01:18, 2780.26it/s]191642it [01:18, 3181.37it/s]197503it [01:18, 3052.85it/s]191962it [01:18, 3185.42it/s]197819it [01:18, 3081.92it/s]192324it [01:18, 3311.61it/s]198199it [01:18, 3287.58it/s]192708it [01:18, 3466.97it/s]198564it [01:18, 3392.77it/s]193056it [01:18, 3379.19it/s]193425it [01:18, 3469.42it/s]193773it [01:18, 3354.48it/s]194155it [01:18, 3487.92it/s]194506it [01:19, 3408.46it/s]194883it [01:19, 3510.15it/s]195238it [01:19, 3374.74it/s]195614it [01:19, 3484.70it/s]195983it [01:19, 3543.05it/s]196339it [01:19, 3422.97it/s]196716it [01:19, 3520.10it/s]197070it [01:19, 3427.90it/s]197450it [01:19, 3534.75it/s]197805it [01:19, 3427.82it/s]198178it [01:20, 3507.14it/s]198560it [01:20, 3596.57it/s]198796it [01:22, 198.08it/s] 199170it [01:22, 279.57it/s]199506it [01:22, 376.78it/s]199865it [01:23, 516.21it/s]200216it [01:23, 690.89it/s]200541it [01:23, 878.59it/s]200862it [01:23, 1107.31it/s]201178it [01:23, 1358.86it/s]201494it [01:23, 1583.40it/s]201859it [01:23, 1938.41it/s]202179it [01:23, 2153.30it/s]198901it [01:24, 176.18it/s] 202569it [01:23, 2531.06it/s]199280it [01:24, 250.12it/s]202905it [01:24, 2708.47it/s]199599it [01:24, 333.60it/s]203286it [01:24, 2979.49it/s]199963it [01:24, 462.48it/s]203657it [01:24, 3172.16it/s]200331it [01:24, 632.14it/s]204011it [01:24, 3167.60it/s]200664it [01:24, 816.79it/s]204386it [01:24, 3326.44it/s]201034it [01:24, 1077.66it/s]204739it [01:24, 3278.45it/s]201373it [01:24, 1323.10it/s]205111it [01:24, 3400.22it/s]201745it [01:24, 1655.34it/s]205462it [01:24, 3326.40it/s]202086it [01:24, 1922.16it/s]205837it [01:24, 3446.45it/s]202473it [01:25, 2289.77it/s]206218it [01:24, 3550.20it/s]202858it [01:25, 2620.52it/s]206578it [01:25, 3442.63it/s]203218it [01:25, 2761.12it/s]206952it [01:25, 3525.76it/s]203577it [01:25, 2962.64it/s]207308it [01:25, 3407.63it/s]203928it [01:25, 2998.38it/s]207681it [01:25, 3498.12it/s]204299it [01:25, 3185.10it/s]198907it [01:25, 159.57it/s] 208034it [01:25, 3407.65it/s]199289it [01:25, 230.18it/s]204647it [01:25, 3163.39it/s]208416it [01:25, 3525.19it/s]199609it [01:25, 309.83it/s]205018it [01:25, 3312.94it/s]208771it [01:25, 3419.57it/s]199966it [01:25, 430.09it/s]205385it [01:25, 3259.90it/s]209152it [01:25, 3530.95it/s]200337it [01:25, 594.31it/s]205748it [01:25, 3361.94it/s]209520it [01:25, 3571.69it/s]200668it [01:25, 772.29it/s]206125it [01:26, 3476.24it/s]201039it [01:26, 1027.03it/s]209879it [01:26, 3333.28it/s]206480it [01:26, 3272.80it/s]210235it [01:26, 3396.67it/s]201378it [01:26, 1268.07it/s]206815it [01:26, 3284.66it/s]201705it [01:26, 1530.99it/s]210578it [01:26, 3149.20it/s]207149it [01:26, 3107.47it/s]210912it [01:26, 3199.96it/s]202029it [01:26, 1755.43it/s]207478it [01:26, 3157.48it/s]211238it [01:26, 3214.68it/s]202372it [01:26, 2059.59it/s]207799it [01:26, 3170.76it/s]202717it [01:26, 2346.25it/s]211563it [01:26, 3119.23it/s]208119it [01:26, 3132.03it/s]203044it [01:26, 2531.70it/s]211938it [01:26, 3295.89it/s]208504it [01:26, 3337.43it/s]203413it [01:26, 2813.69it/s]212271it [01:26, 3246.45it/s]208840it [01:26, 3286.44it/s]203750it [01:26, 2894.77it/s]212645it [01:26, 3388.01it/s]209220it [01:27, 3433.16it/s]204114it [01:26, 3090.84it/s]212986it [01:26, 3283.78it/s]209584it [01:27, 3492.14it/s]204485it [01:27, 3259.41it/s]213351it [01:27, 3387.64it/s]198921it [01:27, 167.62it/s] 209935it [01:27, 3335.30it/s]204834it [01:27, 3219.83it/s]213716it [01:27, 3456.89it/s]199296it [01:27, 236.11it/s]210300it [01:27, 3423.57it/s]205210it [01:27, 3369.31it/s]214064it [01:27, 3385.02it/s]199606it [01:27, 312.27it/s]210645it [01:27, 3326.15it/s]214429it [01:27, 3460.43it/s]205560it [01:27, 3267.42it/s]199964it [01:27, 431.85it/s]211013it [01:27, 3426.44it/s]205936it [01:27, 3405.03it/s]214777it [01:27, 3351.71it/s]200328it [01:27, 591.21it/s]211358it [01:27, 3325.22it/s]215148it [01:27, 3454.21it/s]206284it [01:27, 3328.01it/s]200657it [01:27, 766.71it/s]211725it [01:27, 3422.29it/s]206654it [01:27, 3432.70it/s]215495it [01:27, 3356.92it/s]201013it [01:27, 1007.14it/s]212096it [01:27, 3505.21it/s]207018it [01:27, 3492.35it/s]215868it [01:27, 3463.87it/s]201346it [01:27, 1250.31it/s]212448it [01:28, 3356.84it/s]216238it [01:27, 3531.19it/s]207371it [01:27, 3384.33it/s]201717it [01:28, 1580.15it/s]212815it [01:28, 3445.99it/s]207743it [01:28, 3478.95it/s]216593it [01:28, 3418.99it/s]202056it [01:28, 1849.99it/s]213162it [01:28, 3337.29it/s]216974it [01:28, 3531.42it/s]208094it [01:28, 3386.83it/s]202433it [01:28, 2204.42it/s]213529it [01:28, 3430.69it/s]208465it [01:28, 3476.85it/s]202808it [01:28, 2526.80it/s]217329it [01:28, 3426.74it/s]213874it [01:28, 3333.22it/s]217713it [01:28, 3543.96it/s]208815it [01:28, 3378.42it/s]203162it [01:28, 2693.56it/s]214241it [01:28, 3429.60it/s]209194it [01:28, 3494.08it/s]203530it [01:28, 2932.38it/s]218069it [01:28, 3462.97it/s]214596it [01:28, 3461.98it/s]209558it [01:28, 3534.36it/s]218444it [01:28, 3543.40it/s]203881it [01:28, 2967.69it/s]214944it [01:28, 3340.25it/s]204249it [01:28, 3152.77it/s]209913it [01:28, 3376.94it/s]218827it [01:28, 3481.10it/s]215319it [01:28, 3455.98it/s]210278it [01:28, 3454.26it/s]219198it [01:28, 3545.60it/s]204596it [01:28, 3143.95it/s]215667it [01:28, 3318.71it/s]219564it [01:28, 3577.55it/s]204964it [01:28, 3289.71it/s]210626it [01:28, 3343.05it/s]216011it [01:29, 3351.74it/s]205323it [01:29, 3373.36it/s]210985it [01:28, 3404.13it/s]219923it [01:28, 3363.48it/s]216348it [01:29, 3177.19it/s]220264it [01:29, 3375.44it/s]211327it [01:29, 3175.55it/s]205673it [01:29, 3085.29it/s]216669it [01:29, 3160.16it/s]211649it [01:29, 3175.05it/s]220604it [01:29, 3223.47it/s]217006it [01:29, 3216.96it/s]205995it [01:29, 2884.73it/s]211970it [01:29, 3179.38it/s]220934it [01:29, 3244.61it/s]217330it [01:29, 3072.91it/s]206295it [01:29, 2832.94it/s]221277it [01:29, 3292.52it/s]212290it [01:29, 3060.06it/s]217706it [01:29, 3260.57it/s]206661it [01:29, 3052.69it/s]221608it [01:29, 3284.26it/s]212656it [01:29, 3229.58it/s]218035it [01:29, 3245.89it/s]207036it [01:29, 3244.88it/s]221983it [01:29, 3418.99it/s]212982it [01:29, 3185.75it/s]218406it [01:29, 3377.56it/s]207368it [01:29, 3214.81it/s]222326it [01:29, 3347.89it/s]213349it [01:29, 3324.93it/s]218790it [01:29, 3510.19it/s]207733it [01:29, 3336.62it/s]222702it [01:29, 3466.48it/s]213702it [01:29, 3384.30it/s]219143it [01:30, 3397.76it/s]208071it [01:29, 3295.29it/s]223050it [01:29, 3369.70it/s]214042it [01:29, 3312.00it/s]219526it [01:30, 3514.76it/s]208454it [01:30, 3448.85it/s]223420it [01:30, 3463.32it/s]214411it [01:30, 3419.62it/s]219880it [01:30, 3415.49it/s]208802it [01:30, 3373.36it/s]223790it [01:30, 3532.38it/s]214755it [01:30, 3305.22it/s]220267it [01:30, 3543.88it/s]209169it [01:30, 3458.99it/s]224145it [01:30, 3433.37it/s]215115it [01:30, 3384.77it/s]209532it [01:30, 3506.44it/s]220623it [01:30, 3462.50it/s]224512it [01:30, 3500.36it/s]215465it [01:30, 3302.69it/s]220996it [01:30, 3536.48it/s]209884it [01:30, 3386.45it/s]224864it [01:30, 3390.09it/s]215836it [01:30, 3419.36it/s]210247it [01:30, 3456.50it/s]221351it [01:30, 3411.29it/s]225227it [01:30, 3457.42it/s]216210it [01:30, 3510.28it/s]221746it [01:30, 3565.12it/s]210595it [01:30, 3333.44it/s]225574it [01:30, 3361.25it/s]216563it [01:30, 3370.50it/s]222116it [01:30, 3602.69it/s]210962it [01:30, 3427.86it/s]225939it [01:30, 3443.97it/s]216937it [01:30, 3475.10it/s]211307it [01:30, 3332.41it/s]222478it [01:30, 3448.82it/s]226306it [01:30, 3507.26it/s]217287it [01:30, 3364.51it/s]211677it [01:31, 3436.79it/s]222852it [01:31, 3530.87it/s]226658it [01:30, 3389.91it/s]217672it [01:30, 3502.63it/s]212039it [01:31, 3489.08it/s]223207it [01:31, 3394.85it/s]227017it [01:31, 3439.29it/s]218025it [01:31, 3402.45it/s]212390it [01:31, 3380.53it/s]223567it [01:31, 3451.10it/s]227363it [01:31, 3339.93it/s]218398it [01:31, 3495.76it/s]212760it [01:31, 3471.27it/s]223914it [01:31, 3351.27it/s]227726it [01:31, 3422.98it/s]218787it [01:31, 3610.21it/s]213109it [01:31, 3362.34it/s]224286it [01:31, 3449.10it/s]228070it [01:31, 3317.27it/s]219150it [01:31, 3472.92it/s]213468it [01:31, 3425.39it/s]224653it [01:31, 3511.54it/s]228439it [01:31, 3422.01it/s]219526it [01:31, 3551.44it/s]213812it [01:31, 3327.57it/s]225006it [01:31, 3385.03it/s]228808it [01:31, 3498.90it/s]219883it [01:31, 3456.01it/s]214183it [01:31, 3435.97it/s]225373it [01:31, 3465.72it/s]229160it [01:31, 3369.27it/s]220273it [01:31, 3583.33it/s]214548it [01:31, 3496.77it/s]225722it [01:31, 3225.47it/s]229524it [01:31, 3445.76it/s]220633it [01:31, 3473.15it/s]214899it [01:31, 3353.19it/s]226069it [01:32, 3292.43it/s]229871it [01:31, 3309.80it/s]220996it [01:31, 3507.17it/s]215247it [01:32, 3388.23it/s]226402it [01:32, 3057.04it/s]230204it [01:32, 3267.10it/s]221348it [01:32, 3283.94it/s]215588it [01:32, 3150.99it/s]226724it [01:32, 3100.52it/s]230532it [01:32, 3177.01it/s]221695it [01:32, 3334.63it/s]215908it [01:32, 3164.12it/s]227038it [01:32, 3088.63it/s]230851it [01:32, 3008.88it/s]222032it [01:32, 3294.39it/s]216228it [01:32, 3165.25it/s]227350it [01:32, 2965.80it/s]231212it [01:32, 3174.75it/s]222364it [01:32, 3188.84it/s]216547it [01:32, 3126.89it/s]227705it [01:32, 3127.97it/s]231533it [01:32, 3144.06it/s]222741it [01:32, 3351.84it/s]216923it [01:32, 3308.31it/s]228066it [01:32, 3103.84it/s]231904it [01:32, 3304.52it/s]223079it [01:32, 3258.53it/s]217256it [01:32, 3239.77it/s]228432it [01:32, 3259.11it/s]232267it [01:32, 3241.61it/s]223442it [01:32, 3362.49it/s]217628it [01:32, 3377.63it/s]228797it [01:32, 3368.76it/s]232633it [01:32, 3359.42it/s]223810it [01:32, 3452.90it/s]217987it [01:32, 3320.13it/s]229137it [01:33, 3272.89it/s]233003it [01:32, 3457.20it/s]224157it [01:32, 3362.23it/s]218363it [01:33, 3445.33it/s]229514it [01:33, 3412.94it/s]233351it [01:32, 3372.71it/s]224527it [01:33, 3459.18it/s]218753it [01:33, 3574.91it/s]229858it [01:33, 3288.61it/s]233719it [01:33, 3460.86it/s]224875it [01:33, 3322.53it/s]219112it [01:33, 3421.55it/s]230220it [01:33, 3382.49it/s]234067it [01:33, 3360.59it/s]225244it [01:33, 3426.84it/s]219494it [01:33, 3535.43it/s]230580it [01:33, 3445.35it/s]234436it [01:33, 3453.56it/s]225589it [01:33, 3315.94it/s]219850it [01:33, 3434.39it/s]230927it [01:33, 3332.62it/s]234787it [01:33, 3337.61it/s]225950it [01:33, 3399.71it/s]220227it [01:33, 3525.41it/s]231298it [01:33, 3439.65it/s]235155it [01:33, 3435.12it/s]226302it [01:33, 3434.02it/s]220582it [01:33, 3445.37it/s]231644it [01:33, 3317.82it/s]235518it [01:33, 3491.28it/s]226647it [01:33, 3317.77it/s]220961it [01:33, 3542.36it/s]231998it [01:33, 3380.99it/s]235869it [01:33, 3368.71it/s]227004it [01:33, 3389.65it/s]221343it [01:33, 3620.57it/s]232338it [01:33, 3278.46it/s]236237it [01:33, 3456.88it/s]227345it [01:33, 3282.05it/s]221707it [01:33, 3483.60it/s]232706it [01:34, 3391.81it/s]236585it [01:33, 3340.97it/s]227700it [01:33, 3356.58it/s]222078it [01:34, 3546.51it/s]233074it [01:34, 3472.93it/s]236940it [01:34, 3400.23it/s]228061it [01:34, 3428.43it/s]222435it [01:34, 3398.67it/s]233423it [01:34, 3359.13it/s]237304it [01:34, 3467.85it/s]228406it [01:34, 3309.64it/s]222809it [01:34, 3495.37it/s]233791it [01:34, 3449.14it/s]237653it [01:34, 3348.20it/s]228768it [01:34, 3398.29it/s]223161it [01:34, 3346.44it/s]234138it [01:34, 3310.23it/s]238016it [01:34, 3427.48it/s]229110it [01:34, 3278.09it/s]223523it [01:34, 3421.81it/s]234501it [01:34, 3401.32it/s]238361it [01:34, 3312.59it/s]229477it [01:34, 3389.77it/s]223868it [01:34, 3320.80it/s]234843it [01:34, 3287.70it/s]238721it [01:34, 3394.14it/s]229818it [01:34, 3296.20it/s]224238it [01:34, 3428.33it/s]235196it [01:34, 3356.02it/s]230169it [01:34, 3356.55it/s]239062it [01:34, 3188.43it/s]224592it [01:34, 3459.21it/s]235545it [01:34, 3392.78it/s]230520it [01:34, 3398.28it/s]239407it [01:34, 3253.19it/s]224940it [01:34, 3268.44it/s]235886it [01:35, 3141.46it/s]239735it [01:34, 3258.11it/s]230861it [01:34, 3176.69it/s]225270it [01:35, 3259.00it/s]236205it [01:35, 3144.34it/s]231188it [01:35, 3202.34it/s]240063it [01:35, 3069.40it/s]225598it [01:35, 3074.65it/s]236523it [01:35, 2948.44it/s]240384it [01:35, 3106.77it/s]231511it [01:35, 3000.76it/s]225909it [01:35, 3078.71it/s]236856it [01:35, 3051.69it/s]240698it [01:35, 3037.26it/s]231865it [01:35, 3148.67it/s]226269it [01:35, 3226.66it/s]237222it [01:35, 3222.09it/s]241067it [01:35, 3221.24it/s]232229it [01:35, 3285.57it/s]226594it [01:35, 3168.30it/s]237548it [01:35, 3176.68it/s]241437it [01:35, 3353.41it/s]232562it [01:35, 3216.91it/s]226946it [01:35, 3268.94it/s]237913it [01:35, 3311.92it/s]241775it [01:35, 3294.56it/s]232919it [01:35, 3316.95it/s]227275it [01:35, 3166.41it/s]238247it [01:35, 3238.66it/s]242140it [01:35, 3395.83it/s]233253it [01:35, 3245.19it/s]227635it [01:35, 3290.07it/s]238600it [01:35, 3322.10it/s]242481it [01:35, 3288.35it/s]233619it [01:35, 3363.96it/s]227994it [01:35, 3376.61it/s]238965it [01:35, 3416.95it/s]242846it [01:35, 3390.67it/s]233958it [01:35, 3265.97it/s]228334it [01:36, 3244.67it/s]239309it [01:36, 3317.09it/s]243187it [01:35, 3310.52it/s]234319it [01:35, 3363.71it/s]228697it [01:36, 3348.31it/s]239677it [01:36, 3419.51it/s]243554it [01:36, 3413.18it/s]234680it [01:36, 3433.55it/s]229034it [01:36, 3254.61it/s]240021it [01:36, 3314.04it/s]243923it [01:36, 3492.38it/s]235025it [01:36, 3317.16it/s]229398it [01:36, 3364.67it/s]240384it [01:36, 3403.54it/s]244274it [01:36, 3365.07it/s]235388it [01:36, 3405.03it/s]229747it [01:36, 3250.56it/s]240726it [01:36, 3282.91it/s]244636it [01:36, 3437.67it/s]235730it [01:36, 3260.95it/s]230108it [01:36, 3351.77it/s]241092it [01:36, 3390.38it/s]244982it [01:36, 3335.75it/s]236092it [01:36, 3361.74it/s]230466it [01:36, 3417.26it/s]241463it [01:36, 3482.25it/s]245346it [01:36, 3421.52it/s]236454it [01:36, 3435.92it/s]230810it [01:36, 3293.42it/s]241813it [01:36, 3377.48it/s]245707it [01:36, 3319.30it/s]236800it [01:36, 3312.19it/s]231171it [01:36, 3383.80it/s]242179it [01:36, 3458.35it/s]246072it [01:36, 3411.35it/s]237154it [01:36, 3375.30it/s]231512it [01:36, 3276.61it/s]242527it [01:37, 3350.05it/s]246436it [01:36, 3476.54it/s]237494it [01:36, 3264.93it/s]231874it [01:37, 3374.38it/s]242883it [01:37, 3410.28it/s]246786it [01:37, 3364.26it/s]237856it [01:37, 3365.42it/s]232238it [01:37, 3450.12it/s]243226it [01:37, 3321.91it/s]247154it [01:37, 3453.10it/s]238195it [01:37, 3242.50it/s]232585it [01:37, 3280.23it/s]243597it [01:37, 3432.87it/s]247501it [01:37, 3350.45it/s]238556it [01:37, 3344.80it/s]232947it [01:37, 3375.21it/s]243964it [01:37, 3501.61it/s]247867it [01:37, 3438.39it/s]238916it [01:37, 3410.03it/s]233287it [01:37, 3288.70it/s]244316it [01:37, 3380.05it/s]248227it [01:37, 3257.28it/s]239259it [01:37, 3297.53it/s]233651it [01:37, 3387.47it/s]244671it [01:37, 3428.96it/s]248562it [01:37, 3280.87it/s]239612it [01:37, 3362.36it/s]233992it [01:37, 3258.99it/s]245016it [01:37, 3209.72it/s]248914it [01:37, 3348.21it/s]239950it [01:37, 3209.78it/s]234330it [01:37, 3290.96it/s]245341it [01:37, 3210.26it/s]249251it [01:37, 3135.90it/s]240274it [01:37, 3191.59it/s]234661it [01:37, 3234.35it/s]245665it [01:37, 3213.43it/s]249569it [01:37, 3139.66it/s]240595it [01:37, 3177.19it/s]234986it [01:38, 3062.90it/s]245989it [01:38, 3039.36it/s]249889it [01:37, 3155.24it/s]240914it [01:37, 2949.89it/s]235295it [01:38, 3068.96it/s]246322it [01:38, 3119.24it/s]250207it [01:38, 2946.46it/s]241270it [01:38, 3118.13it/s]235626it [01:38, 3022.08it/s]246637it [01:38, 3105.40it/s]250578it [01:38, 3156.87it/s]241586it [01:38, 3085.63it/s]235991it [01:38, 3197.45it/s]247007it [01:38, 3276.46it/s]250898it [01:38, 3130.22it/s]241954it [01:38, 3254.02it/s]236354it [01:38, 3319.59it/s]247367it [01:38, 3369.85it/s]251262it [01:38, 3275.20it/s]242310it [01:38, 3340.17it/s]236688it [01:38, 3218.72it/s]247706it [01:38, 3282.51it/s]251593it [01:38, 3208.10it/s]242647it [01:38, 3248.09it/s]237056it [01:38, 3350.22it/s]248069it [01:38, 3382.19it/s]251954it [01:38, 3323.71it/s]243010it [01:38, 3357.07it/s]237393it [01:38, 3259.01it/s]248409it [01:38, 3285.39it/s]252324it [01:38, 3432.25it/s]243348it [01:38, 3264.28it/s]237755it [01:38, 3361.87it/s]248777it [01:38, 3396.77it/s]252669it [01:38, 3316.37it/s]243707it [01:38, 3357.17it/s]238113it [01:38, 3424.47it/s]249119it [01:39, 3299.15it/s]253039it [01:38, 3425.09it/s]244045it [01:38, 3259.95it/s]238457it [01:39, 3311.51it/s]249477it [01:39, 3377.94it/s]253384it [01:39, 3323.85it/s]244408it [01:39, 3365.26it/s]238821it [01:39, 3404.15it/s]249838it [01:39, 3443.67it/s]253753it [01:39, 3427.49it/s]244774it [01:39, 3448.89it/s]239163it [01:39, 3278.08it/s]250184it [01:39, 3351.54it/s]254098it [01:39, 3400.13it/s]245121it [01:39, 3296.71it/s]239527it [01:39, 3379.25it/s]250555it [01:39, 3452.81it/s]245486it [01:39, 3396.61it/s]254440it [01:39, 3211.14it/s]239867it [01:39, 3292.81it/s]250902it [01:39, 3348.94it/s]254788it [01:39, 3285.49it/s]245828it [01:39, 3287.45it/s]240230it [01:39, 3388.85it/s]251269it [01:39, 3439.39it/s]255119it [01:39, 3213.76it/s]246192it [01:39, 3387.61it/s]240583it [01:39, 3429.57it/s]251615it [01:39, 3316.40it/s]255484it [01:39, 3337.76it/s]246546it [01:39, 3253.31it/s]240928it [01:39, 3317.24it/s]251979it [01:39, 3407.47it/s]255820it [01:39, 3232.78it/s]246910it [01:39, 3361.85it/s]241296it [01:39, 3413.04it/s]252348it [01:39, 3488.73it/s]247274it [01:39, 3439.90it/s]241639it [01:40, 3316.45it/s]252699it [01:40, 3361.16it/s]247620it [01:40, 3291.88it/s]241996it [01:40, 3387.13it/s]253060it [01:40, 3430.35it/s]247986it [01:40, 3394.95it/s]242346it [01:40, 3298.90it/s]253405it [01:40, 3332.51it/s]248328it [01:40, 3282.55it/s]242709it [01:40, 3391.36it/s]253760it [01:40, 3394.22it/s]248691it [01:40, 3380.04it/s]243072it [01:40, 3458.25it/s]254106it [01:40, 3264.84it/s]249047it [01:40, 3431.71it/s]243419it [01:40, 3320.40it/s]254458it [01:40, 3336.30it/s]249392it [01:40, 3322.63it/s]243784it [01:40, 3414.52it/s]254813it [01:40, 3395.68it/s]249726it [01:40, 3322.46it/s]255154it [01:40, 3181.77it/s]244128it [01:40, 2632.79it/s]250060it [01:40, 3101.17it/s]255479it [01:40, 3198.78it/s]244422it [01:40, 2706.42it/s]250374it [01:40, 3100.30it/s]255802it [01:41, 3024.09it/s]244714it [01:41, 2680.55it/s]250687it [01:40, 2824.90it/s]245005it [01:41, 2739.66it/s]250986it [01:41, 2867.93it/s]245364it [01:41, 2971.00it/s]251351it [01:41, 3083.92it/s]245707it [01:41, 3005.43it/s]251665it [01:41, 3028.27it/s]246063it [01:41, 3160.07it/s]252030it [01:41, 3204.06it/s]246428it [01:41, 3298.01it/s]252397it [01:41, 3336.28it/s]246763it [01:41, 3226.52it/s]252734it [01:41, 3240.06it/s]247129it [01:41, 3350.39it/s]253093it [01:41, 3339.29it/s]247468it [01:41, 3241.17it/s]253430it [01:41, 3253.27it/s]247836it [01:42, 3366.23it/s]253797it [01:41, 3371.33it/s]248201it [01:42, 3446.69it/s]254136it [01:42, 3266.13it/s]248548it [01:42, 3332.35it/s]254492it [01:42, 3348.23it/s]248899it [01:42, 3382.20it/s]254856it [01:42, 3431.11it/s]249239it [01:42, 3299.71it/s]255201it [01:42, 3315.90it/s]249604it [01:42, 3400.17it/s]255568it [01:42, 3415.87it/s]249946it [01:42, 3303.25it/s]250314it [01:42, 3410.62it/s]250677it [01:42, 3467.86it/s]251025it [01:42, 3369.14it/s]251388it [01:43, 3444.46it/s]251734it [01:43, 3332.71it/s]252104it [01:43, 3438.01it/s]252450it [01:43, 3305.56it/s]252817it [01:43, 3407.77it/s]253169it [01:43, 3439.55it/s]253515it [01:43, 3180.17it/s]253840it [01:43, 3197.76it/s]254164it [01:43, 3045.77it/s]254484it [01:44, 3087.82it/s]254842it [01:44, 3226.18it/s]255168it [01:44, 3182.61it/s]255536it [01:44, 3325.03it/s]256145it [01:47, 135.41it/s] 256513it [01:47, 194.94it/s]256807it [01:48, 259.25it/s]257177it [01:48, 370.68it/s]257535it [01:48, 508.79it/s]257890it [01:48, 688.16it/s]258261it [01:48, 922.99it/s]258597it [01:48, 1156.56it/s]258963it [01:48, 1466.85it/s]256108it [01:48, 134.04it/s] 259302it [01:48, 1728.15it/s]256469it [01:49, 193.99it/s]259671it [01:48, 2070.57it/s]256769it [01:49, 261.46it/s]260030it [01:49, 2373.76it/s]257138it [01:49, 375.32it/s]260377it [01:49, 2532.67it/s]257482it [01:49, 514.12it/s]260728it [01:49, 2761.11it/s]257796it [01:49, 667.27it/s]261067it [01:49, 2764.97it/s]258127it [01:49, 876.45it/s]261393it [01:49, 2890.43it/s]258437it [01:49, 1080.31it/s]261724it [01:49, 3001.27it/s]258757it [01:49, 1344.17it/s]262049it [01:49, 2902.10it/s]259066it [01:49, 1607.71it/s]262416it [01:49, 3110.01it/s]259370it [01:49, 1800.02it/s]262742it [01:49, 3084.84it/s]259735it [01:50, 2164.12it/s]263114it [01:49, 3262.26it/s]260054it [01:50, 2355.77it/s]263449it [01:50, 3203.63it/s]260417it [01:50, 2655.49it/s]263811it [01:50, 3320.43it/s]260742it [01:50, 2783.02it/s]264179it [01:50, 3423.32it/s]261064it [01:50, 2814.43it/s]264526it [01:50, 3324.64it/s]261432it [01:50, 3045.49it/s]264885it [01:50, 3391.23it/s]261761it [01:50, 3066.82it/s]265227it [01:50, 3316.55it/s]262131it [01:50, 3243.75it/s]265598it [01:50, 3428.08it/s]262501it [01:50, 3373.59it/s]265943it [01:50, 3305.25it/s]262849it [01:51, 3297.90it/s]266314it [01:50, 3419.28it/s]263207it [01:51, 3377.94it/s]266687it [01:51, 3507.73it/s]263551it [01:51, 3304.55it/s]267040it [01:51, 3365.55it/s]263921it [01:51, 3418.17it/s]267411it [01:51, 3462.47it/s]264267it [01:51, 3339.61it/s]267760it [01:51, 3361.18it/s]264642it [01:51, 3457.41it/s]268124it [01:51, 3439.78it/s]265012it [01:51, 3525.64it/s]268470it [01:51, 3346.00it/s]265367it [01:51, 3392.99it/s]268842it [01:51, 3452.95it/s]265739it [01:51, 3486.87it/s]269201it [01:51, 3491.97it/s]255912it [01:51, 122.50it/s] 266090it [01:51, 3383.78it/s]269552it [01:51, 3364.75it/s]256273it [01:51, 173.67it/s]266460it [01:52, 3474.51it/s]269923it [01:51, 3463.07it/s]256632it [01:51, 243.81it/s]255871it [01:52, 141.68it/s] 266810it [01:52, 3285.49it/s]256934it [01:52, 322.69it/s]270271it [01:52, 3207.96it/s]256228it [01:52, 201.40it/s]267169it [01:52, 3369.98it/s]257290it [01:52, 449.20it/s]270619it [01:52, 3283.33it/s]256583it [01:52, 282.70it/s]267509it [01:52, 3348.74it/s]270951it [01:52, 3264.68it/s]256880it [01:52, 371.61it/s]257607it [01:52, 575.86it/s]267846it [01:52, 3117.19it/s]257200it [01:52, 500.45it/s]271280it [01:52, 3026.27it/s]257917it [01:52, 748.27it/s]268172it [01:52, 3156.27it/s]257521it [01:52, 665.98it/s]271604it [01:52, 3083.49it/s]258245it [01:52, 973.30it/s]268491it [01:52, 3033.23it/s]257826it [01:52, 850.99it/s]271917it [01:52, 3026.34it/s]258547it [01:52, 1198.08it/s]268810it [01:52, 3076.64it/s]258192it [01:52, 1137.96it/s]272273it [01:52, 3176.05it/s]258909it [01:52, 1532.47it/s]269175it [01:52, 3237.83it/s]258510it [01:52, 1392.68it/s]272639it [01:52, 3314.12it/s]259227it [01:52, 1785.95it/s]269502it [01:53, 3194.09it/s]258880it [01:53, 1746.53it/s]259587it [01:52, 2127.96it/s]272973it [01:52, 3243.16it/s]269868it [01:53, 3326.63it/s]259216it [01:53, 2007.20it/s]259948it [01:53, 2442.83it/s]273326it [01:53, 3324.85it/s]270203it [01:53, 3266.45it/s]259569it [01:53, 2314.73it/s]273661it [01:53, 3249.42it/s]260285it [01:53, 2589.71it/s]270569it [01:53, 3378.82it/s]259936it [01:53, 2617.68it/s]274028it [01:53, 3369.64it/s]260640it [01:53, 2824.54it/s]270936it [01:53, 3463.39it/s]260280it [01:53, 2745.34it/s]260975it [01:53, 2878.84it/s]274367it [01:53, 3253.12it/s]271284it [01:53, 3345.93it/s]260643it [01:53, 2967.42it/s]261338it [01:53, 3075.39it/s]274731it [01:53, 3363.36it/s]271651it [01:53, 3439.17it/s]260986it [01:53, 3011.78it/s]261699it [01:53, 3220.70it/s]275098it [01:53, 3451.46it/s]271997it [01:53, 3329.24it/s]261346it [01:53, 3168.84it/s]262043it [01:53, 3168.43it/s]275445it [01:53, 3317.30it/s]272364it [01:53, 3426.02it/s]261713it [01:53, 3307.62it/s]262400it [01:53, 3278.60it/s]275807it [01:53, 3402.71it/s]272709it [01:53, 3311.47it/s]262062it [01:54, 3253.26it/s]276149it [01:53, 3314.54it/s]262740it [01:53, 3201.65it/s]273076it [01:54, 3412.49it/s]262427it [01:54, 3364.15it/s]263097it [01:54, 3305.46it/s]276502it [01:54, 3375.70it/s]273441it [01:54, 3480.50it/s]262774it [01:54, 3282.29it/s]263434it [01:54, 3208.54it/s]276856it [01:54, 3275.75it/s]273791it [01:54, 3367.42it/s]263132it [01:54, 3365.45it/s]263795it [01:54, 3322.13it/s]277225it [01:54, 3391.17it/s]274159it [01:54, 3456.56it/s]263474it [01:54, 3282.73it/s]264158it [01:54, 3410.31it/s]277577it [01:54, 3426.44it/s]274507it [01:54, 3311.53it/s]263842it [01:54, 3394.30it/s]264503it [01:54, 3292.45it/s]277921it [01:54, 3316.36it/s]274874it [01:54, 3410.92it/s]264211it [01:54, 3478.35it/s]264861it [01:54, 3372.18it/s]278282it [01:54, 3399.31it/s]275218it [01:54, 3307.63it/s]264562it [01:54, 3365.49it/s]265201it [01:54, 3263.55it/s]278624it [01:54, 3266.50it/s]275589it [01:54, 3420.51it/s]264917it [01:54, 3418.01it/s]265564it [01:54, 3366.11it/s]278991it [01:54, 3379.86it/s]275954it [01:54, 3486.07it/s]265261it [01:54, 3325.16it/s]265924it [01:54, 3432.03it/s]279354it [01:54, 3449.71it/s]276305it [01:55, 3358.98it/s]265632it [01:55, 3435.63it/s]266269it [01:54, 3306.72it/s]279701it [01:54, 3273.22it/s]276656it [01:55, 3400.06it/s]265978it [01:55, 3334.06it/s]266630it [01:55, 3391.61it/s]280050it [01:55, 3333.71it/s]276998it [01:55, 3116.46it/s]266335it [01:55, 3399.86it/s]266971it [01:55, 3205.27it/s]280386it [01:55, 3134.40it/s]266677it [01:55, 3320.45it/s]277315it [01:55, 2791.67it/s]267295it [01:55, 3213.37it/s]280710it [01:55, 3163.65it/s]277617it [01:55, 2849.61it/s]267011it [01:55, 3127.92it/s]281032it [01:55, 3177.34it/s]267619it [01:55, 3026.74it/s]267343it [01:55, 3180.15it/s]277909it [01:55, 2762.90it/s]267925it [01:55, 3021.38it/s]281352it [01:55, 2963.86it/s]278268it [01:55, 2985.37it/s]267664it [01:55, 3106.10it/s]268286it [01:55, 3186.13it/s]281716it [01:55, 3150.08it/s]278573it [01:55, 3001.36it/s]268037it [01:55, 3281.82it/s]268608it [01:55, 3136.78it/s]282036it [01:55, 3110.77it/s]278941it [01:55, 3193.82it/s]268400it [01:55, 3382.07it/s]268967it [01:55, 3266.43it/s]282389it [01:55, 3229.20it/s]279305it [01:56, 3317.52it/s]268741it [01:56, 3299.39it/s]269296it [01:55, 3180.67it/s]282736it [01:55, 3175.72it/s]269110it [01:56, 3411.56it/s]279640it [01:56, 3188.25it/s]269658it [01:56, 3306.13it/s]283100it [01:56, 3305.91it/s]280005it [01:56, 3312.66it/s]269453it [01:56, 3312.78it/s]270026it [01:56, 3414.96it/s]283452it [01:56, 3365.17it/s]280339it [01:56, 3236.79it/s]269825it [01:56, 3429.89it/s]270369it [01:56, 3270.37it/s]283791it [01:56, 3270.70it/s]280708it [01:56, 3365.68it/s]270170it [01:56, 3317.48it/s]270741it [01:56, 3397.34it/s]284158it [01:56, 3385.64it/s]281055it [01:56, 3283.64it/s]270537it [01:56, 3416.39it/s]271083it [01:56, 3247.47it/s]284499it [01:56, 3254.39it/s]281423it [01:56, 3396.56it/s]270902it [01:56, 3483.74it/s]271448it [01:56, 3360.30it/s]284867it [01:56, 3374.96it/s]281790it [01:56, 3473.65it/s]271252it [01:56, 3359.57it/s]271803it [01:56, 3414.06it/s]285232it [01:56, 3454.45it/s]282139it [01:56, 3349.88it/s]271615it [01:56, 3436.48it/s]272147it [01:56, 3285.97it/s]285580it [01:56, 3314.41it/s]282496it [01:56, 3372.22it/s]271961it [01:56, 3308.97it/s]272503it [01:56, 3361.60it/s]285948it [01:56, 3418.56it/s]272326it [01:57, 3404.54it/s]282835it [01:57, 3271.99it/s]272842it [01:56, 3248.07it/s]286292it [01:56, 3312.32it/s]283201it [01:57, 3381.86it/s]272669it [01:57, 3307.91it/s]273202it [01:57, 3346.94it/s]286654it [01:57, 3399.68it/s]283566it [01:57, 3458.03it/s]273036it [01:57, 3409.97it/s]273539it [01:57, 3242.83it/s]286996it [01:57, 3310.86it/s]287112it [01:57, 2448.98it/s]
273401it [01:57, 3479.32it/s]283914it [01:57, 3334.84it/s]273899it [01:57, 3342.64it/s]284280it [01:57, 3427.15it/s]273751it [01:57, 3342.73it/s]274256it [01:57, 3407.03it/s]274123it [01:57, 3449.60it/s]284625it [01:57, 3315.00it/s]274599it [01:57, 3282.09it/s]284993it [01:57, 3416.99it/s]274470it [01:57, 3339.35it/s]274954it [01:57, 3356.44it/s]274834it [01:57, 3422.93it/s]285337it [01:57, 3314.62it/s]275292it [01:57, 3243.03it/s]285699it [01:57, 3400.43it/s]275178it [01:57, 3326.90it/s]275647it [01:57, 3330.07it/s]286056it [01:58, 3446.85it/s]275536it [01:58, 3397.30it/s]276012it [01:57, 3422.04it/s]275901it [01:58, 3469.11it/s]286402it [01:58, 3239.53it/s]276356it [01:58, 3291.85it/s]286730it [01:58, 3242.75it/s]276250it [01:58, 3216.30it/s]276688it [01:58, 3273.53it/s]287057it [01:58, 2924.74it/s]276576it [01:58, 2920.20it/s]277017it [01:58, 3086.15it/s]287112it [01:58, 2424.76it/s]
2022-07-25 16:48:34 | INFO | root | success load 287112 data
2022-07-25 16:48:34 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-25 16:48:34 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-25 16:48:34 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-25 16:48:34 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-25 16:48:34 | INFO | transformer.tokenization_utils | loading file None
2022-07-25 16:48:34 | INFO | transformer.tokenization_utils | loading file None
2022-07-25 16:48:34 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
277329it [01:58, 3092.12it/s]276876it [01:58, 2595.48it/s]277665it [01:58, 3166.26it/s]277189it [01:58, 2728.31it/s]277984it [01:58, 3097.23it/s]277541it [01:58, 2934.79it/s]278347it [01:58, 3247.93it/s]277844it [01:58, 2922.30it/s]278674it [01:58, 3125.75it/s]278209it [01:58, 3124.39it/s]279041it [01:58, 3279.50it/s]278536it [01:59, 3103.03it/s]279375it [01:58, 3207.91it/s]278890it [01:59, 3225.42it/s]279743it [01:59, 3342.37it/s]279257it [01:59, 3351.48it/s]280082it [01:59, 3354.16it/s]279596it [01:59, 3257.30it/s]280419it [01:59, 3258.55it/s]279957it [01:59, 3358.65it/s]280781it [01:59, 3361.16it/s]280296it [01:59, 3251.49it/s]281119it [01:59, 3241.40it/s]280636it [01:59, 3287.28it/s]281481it [01:59, 3348.70it/s]281005it [01:59, 3403.19it/s]281835it [01:59, 3403.23it/s]281347it [01:59, 3295.69it/s]282177it [01:59, 3297.81it/s]281711it [01:59, 3393.02it/s]282532it [01:59, 3369.83it/s]282052it [02:00, 3291.79it/s]282871it [02:00, 3273.07it/s]282402it [02:00, 3350.26it/s]283225it [02:00, 3339.65it/s]282739it [02:00, 3221.68it/s]283575it [02:00, 3246.17it/s]283101it [02:00, 3334.66it/s]283931it [02:00, 3334.47it/s]283461it [02:00, 3409.97it/s]284299it [02:00, 3433.78it/s]283804it [02:00, 3279.99it/s]284644it [02:00, 3299.19it/s]284136it [02:00, 3290.53it/s]285010it [02:00, 3402.32it/s]284467it [02:00, 3211.51it/s]285353it [02:00, 3278.00it/s]284833it [02:00, 3338.62it/s]285720it [02:00, 3388.78it/s]285193it [02:01, 3414.18it/s]286066it [02:00, 3408.16it/s]285536it [02:01, 3200.45it/s]286409it [02:01, 3152.41it/s]285860it [02:01, 3165.91it/s]286732it [02:01, 3173.40it/s]286179it [02:01, 2959.17it/s]287053it [02:01, 3004.58it/s]287112it [02:01, 2365.86it/s]
286501it [02:01, 3030.45it/s]286861it [02:01, 3190.13it/s]287112it [02:01, 2359.84it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-25 16:53:31 | INFO | train_inner | epoch 001:    100 / 1122 loss=nan, nll_loss=11.482, mask_ins=7.604, word_ins_ml=12.045, word_reposition=6.294, kpe=nan, ppl=nan, wps=7104.1, ups=0.35, wpb=20527, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=25.792, clip=22, loss_scale=128, train_wall=249, wall=416
2022-07-25 16:58:19 | INFO | train_inner | epoch 001:    200 / 1122 loss=22.256, nll_loss=10.118, mask_ins=4.739, word_ins_ml=10.834, word_reposition=5.282, kpe=1.401, ppl=5.00787e+06, wps=7155.9, ups=0.35, wpb=20583.2, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=21.292, clip=0, loss_scale=128, train_wall=242, wall=703
2022-07-25 17:03:05 | INFO | train_inner | epoch 001:    300 / 1122 loss=16.548, nll_loss=9.609, mask_ins=1.932, word_ins_ml=10.375, word_reposition=2.975, kpe=1.266, ppl=95819, wps=7175.2, ups=0.35, wpb=20561.3, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=6.882, clip=0, loss_scale=128, train_wall=241, wall=990
2022-07-25 17:07:52 | INFO | train_inner | epoch 001:    400 / 1122 loss=14.786, nll_loss=8.854, mask_ins=1.587, word_ins_ml=9.716, word_reposition=2.287, kpe=1.196, ppl=28258.3, wps=7179.3, ups=0.35, wpb=20576.5, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=3.518, clip=0, loss_scale=128, train_wall=241, wall=1277
2022-07-25 17:12:38 | INFO | train_inner | epoch 001:    500 / 1122 loss=14.151, nll_loss=8.338, mask_ins=1.544, word_ins_ml=9.267, word_reposition=2.188, kpe=1.152, ppl=18191.3, wps=7169, ups=0.35, wpb=20523.5, bsz=256, num_updates=500, lr=5.009e-05, gnorm=3.494, clip=0, loss_scale=128, train_wall=241, wall=1563
2022-07-25 17:17:38 | INFO | train_inner | epoch 001:    600 / 1122 loss=13.59, nll_loss=7.923, mask_ins=1.442, word_ins_ml=8.905, word_reposition=2.122, kpe=1.12, ppl=12334.9, wps=6841.4, ups=0.33, wpb=20491.4, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=3.452, clip=0, loss_scale=242, train_wall=254, wall=1862
2022-07-25 17:22:26 | INFO | train_inner | epoch 001:    700 / 1122 loss=13.197, nll_loss=7.608, mask_ins=1.382, word_ins_ml=8.634, word_reposition=2.081, kpe=1.1, ppl=9392.72, wps=7131.5, ups=0.35, wpb=20542.5, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=3.123, clip=0, loss_scale=256, train_wall=242, wall=2150
2022-07-25 17:27:12 | INFO | train_inner | epoch 001:    800 / 1122 loss=12.883, nll_loss=7.34, mask_ins=1.352, word_ins_ml=8.403, word_reposition=2.041, kpe=1.086, ppl=7553.78, wps=7183.6, ups=0.35, wpb=20579, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=2.847, clip=0, loss_scale=256, train_wall=241, wall=2437
2022-07-25 17:31:59 | INFO | train_inner | epoch 001:    900 / 1122 loss=12.601, nll_loss=7.118, mask_ins=1.328, word_ins_ml=8.212, word_reposition=1.995, kpe=1.066, ppl=6212.5, wps=7130.3, ups=0.35, wpb=20464, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=2.75, clip=0, loss_scale=256, train_wall=241, wall=2724
2022-07-25 17:36:49 | INFO | train_inner | epoch 001:   1000 / 1122 loss=12.378, nll_loss=6.901, mask_ins=1.316, word_ins_ml=8.022, word_reposition=1.982, kpe=1.058, ppl=5323.36, wps=7111.8, ups=0.35, wpb=20597.8, bsz=256, num_updates=1000, lr=0.00010008, gnorm=2.539, clip=0, loss_scale=256, train_wall=243, wall=3014
2022-07-25 17:41:35 | INFO | train_inner | epoch 001:   1100 / 1122 loss=12.18, nll_loss=6.724, mask_ins=1.305, word_ins_ml=7.869, word_reposition=1.962, kpe=1.044, ppl=4641.74, wps=7145.2, ups=0.35, wpb=20473.7, bsz=256, num_updates=1100, lr=0.000110078, gnorm=2.521, clip=0, loss_scale=453, train_wall=241, wall=3300
2022-07-25 17:42:37 | INFO | train | epoch 001 | loss nan | nll_loss 8.331 | mask_ins 2.302 | word_ins_ml 9.269 | word_reposition 2.821 | kpe nan | ppl nan | wps 7116.3 | ups 0.35 | wpb 20520.3 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 7.022 | clip 2 | loss_scale 220 | train_wall 2729 | wall 3362
2022-07-25 17:43:57 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 15.885 | nll_loss 8.909 | mask_ins 1.594 | word_ins_ml 9.866 | word_reposition 3.104 | kpe 1.321 | ppl 60523.7 | wps 12420.6 | wpb 2367.6 | bsz 32 | num_updates 1122
2022-07-25 17:44:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_best.pt (epoch 1 @ 1122 updates, score 15.885) (writing took 3.3883197735995054 seconds)
2022-07-25 17:47:44 | INFO | train_inner | epoch 002:     78 / 1122 loss=11.974, nll_loss=6.545, mask_ins=1.292, word_ins_ml=7.712, word_reposition=1.932, kpe=1.037, ppl=4021.75, wps=5520.5, ups=0.27, wpb=20333.3, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=2.485, clip=0, loss_scale=512, train_wall=241, wall=3668
2022-07-25 17:52:30 | INFO | train_inner | epoch 002:    178 / 1122 loss=11.777, nll_loss=6.351, mask_ins=1.284, word_ins_ml=7.542, word_reposition=1.928, kpe=1.023, ppl=3509.71, wps=7181.5, ups=0.35, wpb=20587.3, bsz=256, num_updates=1300, lr=0.000130074, gnorm=2.169, clip=0, loss_scale=512, train_wall=242, wall=3955
2022-07-25 17:57:19 | INFO | train_inner | epoch 002:    278 / 1122 loss=11.55, nll_loss=6.174, mask_ins=1.269, word_ins_ml=7.387, word_reposition=1.877, kpe=1.016, ppl=2997.92, wps=7146.4, ups=0.35, wpb=20599.8, bsz=256, num_updates=1400, lr=0.000140072, gnorm=2.26, clip=0, loss_scale=512, train_wall=243, wall=4243
2022-07-25 18:02:05 | INFO | train_inner | epoch 002:    378 / 1122 loss=11.39, nll_loss=6.004, mask_ins=1.266, word_ins_ml=7.238, word_reposition=1.876, kpe=1.01, ppl=2684.4, wps=7109.1, ups=0.35, wpb=20347.3, bsz=256, num_updates=1500, lr=0.00015007, gnorm=2.138, clip=0, loss_scale=512, train_wall=241, wall=4530
2022-07-25 18:06:52 | INFO | train_inner | epoch 002:    478 / 1122 loss=11.163, nll_loss=5.805, mask_ins=1.253, word_ins_ml=7.064, word_reposition=1.839, kpe=1.007, ppl=2292.44, wps=7172.8, ups=0.35, wpb=20567.7, bsz=256, num_updates=1600, lr=0.000160068, gnorm=2.15, clip=0, loss_scale=845, train_wall=242, wall=4816
2022-07-25 18:11:38 | INFO | train_inner | epoch 002:    578 / 1122 loss=11.047, nll_loss=5.662, mask_ins=1.255, word_ins_ml=6.939, word_reposition=1.843, kpe=1.01, ppl=2115.4, wps=7161.3, ups=0.35, wpb=20536.9, bsz=256, num_updates=1700, lr=0.000170066, gnorm=2.167, clip=0, loss_scale=1024, train_wall=242, wall=5103
2022-07-25 18:16:25 | INFO | train_inner | epoch 002:    678 / 1122 loss=10.709, nll_loss=5.355, mask_ins=1.239, word_ins_ml=6.671, word_reposition=1.787, kpe=1.012, ppl=1673.48, wps=7142.1, ups=0.35, wpb=20477.4, bsz=256, num_updates=1800, lr=0.000180064, gnorm=2.195, clip=0, loss_scale=1024, train_wall=243, wall=5390
2022-07-25 18:21:38 | INFO | train_inner | epoch 002:    778 / 1122 loss=nan, nll_loss=5.088, mask_ins=1.233, word_ins_ml=6.438, word_reposition=1.754, kpe=nan, ppl=nan, wps=6577.6, ups=0.32, wpb=20576, bsz=256, num_updates=1900, lr=0.000190062, gnorm=2.292, clip=0, loss_scale=1024, train_wall=267, wall=5703
2022-07-25 18:26:26 | INFO | train_inner | epoch 002:    878 / 1122 loss=10.158, nll_loss=4.807, mask_ins=1.227, word_ins_ml=6.192, word_reposition=1.727, kpe=1.013, ppl=1142.63, wps=7108.1, ups=0.35, wpb=20447.7, bsz=256, num_updates=2000, lr=0.00020006, gnorm=2.277, clip=0, loss_scale=1024, train_wall=242, wall=5990
2022-07-25 18:31:14 | INFO | train_inner | epoch 002:    978 / 1122 loss=9.922, nll_loss=4.603, mask_ins=1.212, word_ins_ml=6.012, word_reposition=1.688, kpe=1.01, ppl=969.93, wps=7127, ups=0.35, wpb=20513.5, bsz=256, num_updates=2100, lr=0.000210058, gnorm=2.276, clip=0, loss_scale=1567, train_wall=243, wall=6278
2022-07-25 18:36:03 | INFO | train_inner | epoch 002:   1078 / 1122 loss=nan, nll_loss=4.433, mask_ins=1.174, word_ins_ml=5.864, word_reposition=1.657, kpe=nan, ppl=nan, wps=7152.1, ups=0.35, wpb=20708.1, bsz=256, num_updates=2200, lr=0.000220056, gnorm=2.187, clip=0, loss_scale=2048, train_wall=244, wall=6568
2022-07-25 18:38:09 | INFO | train | epoch 002 | loss nan | nll_loss 5.462 | mask_ins 1.241 | word_ins_ml 6.765 | word_reposition 1.802 | kpe nan | ppl nan | wps 6911.9 | ups 0.34 | wpb 20521 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 2.233 | clip 0 | loss_scale 1015 | train_wall 2742 | wall 6693
2022-07-25 18:39:28 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 15.349 | nll_loss 7.872 | mask_ins 1.667 | word_ins_ml 8.987 | word_reposition 3.244 | kpe 1.451 | ppl 41741.1 | wps 12398.1 | wpb 2367.6 | bsz 32 | num_updates 2244 | best_loss 15.349
2022-07-25 18:39:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_best.pt (epoch 2 @ 2244 updates, score 15.349) (writing took 18.685841595754027 seconds)
2022-07-25 18:42:27 | INFO | train_inner | epoch 003:     56 / 1122 loss=nan, nll_loss=4.31, mask_ins=1.128, word_ins_ml=5.756, word_reposition=1.667, kpe=nan, ppl=nan, wps=5314.8, ups=0.26, wpb=20387.7, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=2.226, clip=0, loss_scale=2048, train_wall=241, wall=6951
2022-07-25 18:47:14 | INFO | train_inner | epoch 003:    156 / 1122 loss=9.336, nll_loss=4.174, mask_ins=1.086, word_ins_ml=5.636, word_reposition=1.617, kpe=0.997, ppl=646.21, wps=7131.1, ups=0.35, wpb=20466.9, bsz=256, num_updates=2400, lr=0.000240052, gnorm=2.063, clip=0, loss_scale=2048, train_wall=242, wall=7238
2022-07-25 18:52:01 | INFO | train_inner | epoch 003:    256 / 1122 loss=9.218, nll_loss=4.101, mask_ins=1.049, word_ins_ml=5.572, word_reposition=1.599, kpe=0.997, ppl=595.69, wps=7168.9, ups=0.35, wpb=20590.4, bsz=256, num_updates=2500, lr=0.00025005, gnorm=1.962, clip=0, loss_scale=2048, train_wall=242, wall=7526
2022-07-25 18:56:47 | INFO | train_inner | epoch 003:    356 / 1122 loss=9.06, nll_loss=3.997, mask_ins=1.02, word_ins_ml=5.481, word_reposition=1.567, kpe=0.993, ppl=533.9, wps=7174, ups=0.35, wpb=20552.9, bsz=256, num_updates=2600, lr=0.000260048, gnorm=1.931, clip=0, loss_scale=2888, train_wall=242, wall=7812
2022-07-25 19:01:34 | INFO | train_inner | epoch 003:    456 / 1122 loss=9.007, nll_loss=3.924, mask_ins=1.006, word_ins_ml=5.416, word_reposition=1.59, kpe=0.996, ppl=514.34, wps=7110.9, ups=0.35, wpb=20384, bsz=256, num_updates=2700, lr=0.000270046, gnorm=1.853, clip=0, loss_scale=4096, train_wall=241, wall=8099
2022-07-25 19:06:22 | INFO | train_inner | epoch 003:    556 / 1122 loss=8.972, nll_loss=3.899, mask_ins=0.989, word_ins_ml=5.395, word_reposition=1.598, kpe=0.989, ppl=502.02, wps=7122.5, ups=0.35, wpb=20480.9, bsz=256, num_updates=2800, lr=0.000280044, gnorm=1.75, clip=0, loss_scale=4096, train_wall=242, wall=8386
2022-07-25 19:11:08 | INFO | train_inner | epoch 003:    656 / 1122 loss=8.874, nll_loss=3.841, mask_ins=0.972, word_ins_ml=5.342, word_reposition=1.568, kpe=0.991, ppl=469.1, wps=7188, ups=0.35, wpb=20612.3, bsz=256, num_updates=2900, lr=0.000290042, gnorm=1.737, clip=0, loss_scale=4096, train_wall=242, wall=8673
2022-07-25 19:15:56 | INFO | train_inner | epoch 003:    756 / 1122 loss=8.791, nll_loss=3.795, mask_ins=0.957, word_ins_ml=5.301, word_reposition=1.545, kpe=0.987, ppl=442.83, wps=7162.1, ups=0.35, wpb=20597.8, bsz=256, num_updates=3000, lr=0.00030004, gnorm=1.68, clip=0, loss_scale=4096, train_wall=242, wall=8961
2022-07-25 19:20:42 | INFO | train_inner | epoch 003:    856 / 1122 loss=8.757, nll_loss=3.746, mask_ins=0.951, word_ins_ml=5.258, word_reposition=1.563, kpe=0.984, ppl=432.53, wps=7199.7, ups=0.35, wpb=20609.8, bsz=256, num_updates=3100, lr=0.000310038, gnorm=1.676, clip=0, loss_scale=5284, train_wall=241, wall=9247
2022-07-25 19:26:08 | INFO | train_inner | epoch 003:    956 / 1122 loss=nan, nll_loss=3.698, mask_ins=0.937, word_ins_ml=5.214, word_reposition=1.535, kpe=nan, ppl=nan, wps=6314.7, ups=0.31, wpb=20572.9, bsz=256, num_updates=3200, lr=0.000320036, gnorm=1.602, clip=0, loss_scale=8192, train_wall=280, wall=9573
2022-07-25 19:31:01 | INFO | train_inner | epoch 003:   1056 / 1122 loss=8.664, nll_loss=3.69, mask_ins=0.938, word_ins_ml=5.207, word_reposition=1.536, kpe=0.982, ppl=405.6, wps=7014.1, ups=0.34, wpb=20512.4, bsz=256, num_updates=3300, lr=0.000330034, gnorm=1.636, clip=0, loss_scale=8192, train_wall=245, wall=9865
2022-07-25 19:34:08 | INFO | train | epoch 003 | loss nan | nll_loss 3.89 | mask_ins 0.993 | word_ins_ml 5.386 | word_reposition 1.572 | kpe nan | ppl nan | wps 6853.5 | ups 0.33 | wpb 20521.3 | bsz 255.8 | num_updates 3366 | lr 0.000336633 | gnorm 1.795 | clip 0 | loss_scale 4598 | train_wall 2751 | wall 10053
2022-07-25 19:35:28 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 14.188 | nll_loss 7.4 | mask_ins 1.562 | word_ins_ml 8.584 | word_reposition 2.674 | kpe 1.369 | ppl 18667.5 | wps 12416.7 | wpb 2367.6 | bsz 32 | num_updates 3366 | best_loss 14.188
2022-07-25 19:35:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_best.pt (epoch 3 @ 3366 updates, score 14.188) (writing took 28.51495749875903 seconds)
2022-07-25 19:37:34 | INFO | train_inner | epoch 004:     34 / 1122 loss=8.503, nll_loss=3.588, mask_ins=0.917, word_ins_ml=5.116, word_reposition=1.49, kpe=0.98, ppl=362.69, wps=5175.1, ups=0.25, wpb=20354.1, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=1.645, clip=0, loss_scale=8192, train_wall=240, wall=10259
2022-07-25 19:42:21 | INFO | train_inner | epoch 004:    134 / 1122 loss=8.442, nll_loss=3.538, mask_ins=0.91, word_ins_ml=5.073, word_reposition=1.49, kpe=0.97, ppl=347.84, wps=7123.7, ups=0.35, wpb=20472.6, bsz=256, num_updates=3500, lr=0.00035003, gnorm=1.57, clip=0, loss_scale=8192, train_wall=242, wall=10546
2022-07-25 19:47:08 | INFO | train_inner | epoch 004:    234 / 1122 loss=8.39, nll_loss=3.489, mask_ins=0.9, word_ins_ml=5.029, word_reposition=1.491, kpe=0.97, ppl=335.51, wps=7188.4, ups=0.35, wpb=20634.1, bsz=256, num_updates=3600, lr=0.000360028, gnorm=1.528, clip=0, loss_scale=9585, train_wall=242, wall=10833
2022-07-25 19:51:53 | INFO | train_inner | epoch 004:    334 / 1122 loss=8.306, nll_loss=3.45, mask_ins=0.885, word_ins_ml=4.993, word_reposition=1.457, kpe=0.97, ppl=316.46, wps=7178.6, ups=0.35, wpb=20411.2, bsz=256, num_updates=3700, lr=0.000370026, gnorm=1.483, clip=0, loss_scale=16384, train_wall=241, wall=11117
2022-07-25 19:56:40 | INFO | train_inner | epoch 004:    434 / 1122 loss=8.231, nll_loss=3.398, mask_ins=0.872, word_ins_ml=4.946, word_reposition=1.449, kpe=0.964, ppl=300.52, wps=7183.7, ups=0.35, wpb=20666.8, bsz=256, num_updates=3800, lr=0.000380024, gnorm=1.468, clip=0, loss_scale=16384, train_wall=242, wall=11405
2022-07-25 19:57:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-25 20:01:30 | INFO | train_inner | epoch 004:    535 / 1122 loss=8.25, nll_loss=3.375, mask_ins=0.875, word_ins_ml=4.926, word_reposition=1.478, kpe=0.971, ppl=304.35, wps=7066.7, ups=0.35, wpb=20478.8, bsz=256, num_updates=3900, lr=0.000390022, gnorm=1.514, clip=0, loss_scale=9409, train_wall=245, wall=11695
2022-07-25 20:06:19 | INFO | train_inner | epoch 004:    635 / 1122 loss=nan, nll_loss=3.33, mask_ins=0.861, word_ins_ml=4.885, word_reposition=1.441, kpe=nan, ppl=nan, wps=7107.8, ups=0.35, wpb=20519.3, bsz=256, num_updates=4000, lr=0.00040002, gnorm=1.521, clip=0, loss_scale=8192, train_wall=241, wall=11984
2022-07-25 20:11:06 | INFO | train_inner | epoch 004:    735 / 1122 loss=8.119, nll_loss=3.316, mask_ins=0.854, word_ins_ml=4.873, word_reposition=1.422, kpe=0.97, ppl=278.01, wps=7186.4, ups=0.35, wpb=20609.6, bsz=256, num_updates=4100, lr=0.000410018, gnorm=1.438, clip=0, loss_scale=8192, train_wall=242, wall=12270
2022-07-25 20:15:52 | INFO | train_inner | epoch 004:    835 / 1122 loss=nan, nll_loss=3.296, mask_ins=0.851, word_ins_ml=4.853, word_reposition=1.427, kpe=nan, ppl=nan, wps=7127.9, ups=0.35, wpb=20446.8, bsz=256, num_updates=4200, lr=0.000420016, gnorm=1.405, clip=0, loss_scale=8192, train_wall=242, wall=12557
2022-07-25 20:20:38 | INFO | train_inner | epoch 004:    935 / 1122 loss=8.098, nll_loss=3.295, mask_ins=0.851, word_ins_ml=4.852, word_reposition=1.426, kpe=0.969, ppl=274, wps=7215.3, ups=0.35, wpb=20633.8, bsz=256, num_updates=4300, lr=0.000430014, gnorm=1.379, clip=0, loss_scale=8192, train_wall=241, wall=12843
2022-07-25 20:25:26 | INFO | train_inner | epoch 004:   1035 / 1122 loss=8.068, nll_loss=3.265, mask_ins=0.846, word_ins_ml=4.825, word_reposition=1.431, kpe=0.967, ppl=268.43, wps=7113.6, ups=0.35, wpb=20465.7, bsz=256, num_updates=4400, lr=0.000440012, gnorm=1.42, clip=0, loss_scale=14254, train_wall=241, wall=13131
2022-07-25 20:30:24 | INFO | train | epoch 004 | loss nan | nll_loss 3.369 | mask_ins 0.869 | word_ins_ml 4.92 | word_reposition 1.447 | kpe nan | ppl nan | wps 6814.5 | ups 0.33 | wpb 20518.9 | bsz 255.8 | num_updates 4487 | lr 0.00044871 | gnorm 1.475 | clip 0 | loss_scale 11061 | train_wall 2760 | wall 13428
2022-07-25 20:31:44 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 14.246 | nll_loss 7.17 | mask_ins 1.556 | word_ins_ml 8.382 | word_reposition 2.746 | kpe 1.562 | ppl 19430.5 | wps 12352.8 | wpb 2367.6 | bsz 32 | num_updates 4487 | best_loss 14.188
2022-07-25 20:32:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 4 @ 4487 updates, score 14.246) (writing took 19.468810617923737 seconds)
2022-07-25 20:32:41 | INFO | train_inner | epoch 005:     13 / 1122 loss=7.983, nll_loss=3.216, mask_ins=0.837, word_ins_ml=4.78, word_reposition=1.392, kpe=0.974, ppl=252.97, wps=4687.2, ups=0.23, wpb=20374.8, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=1.43, clip=0, loss_scale=16384, train_wall=289, wall=13566
2022-07-25 20:37:27 | INFO | train_inner | epoch 005:    113 / 1122 loss=7.927, nll_loss=3.153, mask_ins=0.832, word_ins_ml=4.726, word_reposition=1.41, kpe=0.96, ppl=243.43, wps=7201.9, ups=0.35, wpb=20631.6, bsz=256, num_updates=4600, lr=0.000460008, gnorm=1.351, clip=0, loss_scale=16384, train_wall=241, wall=13852
2022-07-25 20:42:13 | INFO | train_inner | epoch 005:    213 / 1122 loss=7.869, nll_loss=3.125, mask_ins=0.825, word_ins_ml=4.7, word_reposition=1.383, kpe=0.962, ppl=233.83, wps=7177.1, ups=0.35, wpb=20520.3, bsz=256, num_updates=4700, lr=0.000470006, gnorm=1.325, clip=0, loss_scale=16384, train_wall=242, wall=14138
2022-07-25 20:46:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-25 20:47:03 | INFO | train_inner | epoch 005:    314 / 1122 loss=7.874, nll_loss=3.134, mask_ins=0.828, word_ins_ml=4.707, word_reposition=1.372, kpe=0.966, ppl=234.54, wps=7098.5, ups=0.35, wpb=20566.2, bsz=256, num_updates=4800, lr=0.000480004, gnorm=1.412, clip=0, loss_scale=14518, train_wall=244, wall=14428
2022-07-25 20:51:51 | INFO | train_inner | epoch 005:    414 / 1122 loss=nan, nll_loss=3.147, mask_ins=0.83, word_ins_ml=4.718, word_reposition=1.386, kpe=nan, ppl=nan, wps=7119.9, ups=0.35, wpb=20499.2, bsz=256, num_updates=4900, lr=0.000490002, gnorm=1.333, clip=0, loss_scale=8192, train_wall=241, wall=14716
2022-07-25 20:56:37 | INFO | train_inner | epoch 005:    514 / 1122 loss=7.862, nll_loss=3.108, mask_ins=0.827, word_ins_ml=4.683, word_reposition=1.393, kpe=0.96, ppl=232.61, wps=7120.6, ups=0.35, wpb=20352.1, bsz=256, num_updates=5000, lr=0.0005, gnorm=1.357, clip=0, loss_scale=8192, train_wall=241, wall=15001
2022-07-25 21:01:24 | INFO | train_inner | epoch 005:    614 / 1122 loss=7.872, nll_loss=3.149, mask_ins=0.824, word_ins_ml=4.718, word_reposition=1.368, kpe=0.962, ppl=234.33, wps=7138.1, ups=0.35, wpb=20507.6, bsz=256, num_updates=5100, lr=0.000495074, gnorm=1.288, clip=0, loss_scale=8192, train_wall=242, wall=15289
2022-07-25 21:06:11 | INFO | train_inner | epoch 005:    714 / 1122 loss=7.865, nll_loss=3.113, mask_ins=0.825, word_ins_ml=4.687, word_reposition=1.392, kpe=0.961, ppl=233.13, wps=7146.8, ups=0.35, wpb=20526, bsz=256, num_updates=5200, lr=0.00049029, gnorm=1.347, clip=0, loss_scale=8192, train_wall=242, wall=15576
2022-07-25 21:10:59 | INFO | train_inner | epoch 005:    814 / 1122 loss=7.831, nll_loss=3.105, mask_ins=0.821, word_ins_ml=4.679, word_reposition=1.374, kpe=0.958, ppl=227.76, wps=7177.6, ups=0.35, wpb=20658.1, bsz=256, num_updates=5300, lr=0.000485643, gnorm=1.28, clip=0, loss_scale=9093, train_wall=241, wall=15864
2022-07-25 21:15:47 | INFO | train_inner | epoch 005:    914 / 1122 loss=7.805, nll_loss=3.076, mask_ins=0.821, word_ins_ml=4.652, word_reposition=1.376, kpe=0.956, ppl=223.68, wps=7142.9, ups=0.35, wpb=20546.5, bsz=256, num_updates=5400, lr=0.000481125, gnorm=1.239, clip=0, loss_scale=16384, train_wall=242, wall=16151
2022-07-25 21:17:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-25 21:20:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-25 21:20:39 | INFO | train_inner | epoch 005:   1016 / 1122 loss=nan, nll_loss=3.078, mask_ins=0.813, word_ins_ml=4.653, word_reposition=1.368, kpe=nan, ppl=nan, wps=7011, ups=0.34, wpb=20484.4, bsz=256, num_updates=5500, lr=0.000476731, gnorm=1.266, clip=0, loss_scale=10561, train_wall=247, wall=16444
2022-07-25 21:20:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-25 21:25:29 | INFO | train_inner | epoch 005:   1117 / 1122 loss=7.751, nll_loss=3.057, mask_ins=0.808, word_ins_ml=4.634, word_reposition=1.353, kpe=0.956, ppl=215.43, wps=7095.5, ups=0.34, wpb=20582.2, bsz=256, num_updates=5600, lr=0.000472456, gnorm=1.234, clip=0, loss_scale=2170, train_wall=244, wall=16734
2022-07-25 21:25:42 | INFO | train | epoch 005 | loss nan | nll_loss 3.115 | mask_ins 0.824 | word_ins_ml 4.688 | word_reposition 1.381 | kpe nan | ppl nan | wps 6913.1 | ups 0.34 | wpb 20520.7 | bsz 255.8 | num_updates 5605 | lr 0.000472245 | gnorm 1.318 | clip 0 | loss_scale 10773 | train_wall 2710 | wall 16747
2022-07-25 21:27:02 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 13.599 | nll_loss 6.926 | mask_ins 1.559 | word_ins_ml 8.179 | word_reposition 2.452 | kpe 1.409 | ppl 12406.5 | wps 12396.5 | wpb 2367.6 | bsz 32 | num_updates 5605 | best_loss 13.599
2022-07-25 21:27:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_best.pt (epoch 5 @ 5605 updates, score 13.599) (writing took 24.56847008317709 seconds)
2022-07-25 21:32:02 | INFO | train_inner | epoch 006:     95 / 1122 loss=7.67, nll_loss=2.978, mask_ins=0.808, word_ins_ml=4.565, word_reposition=1.358, kpe=0.939, ppl=203.65, wps=5178, ups=0.25, wpb=20334.7, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=1.288, clip=0, loss_scale=2048, train_wall=242, wall=17126
2022-07-25 21:37:26 | INFO | train_inner | epoch 006:    195 / 1122 loss=7.635, nll_loss=2.952, mask_ins=0.805, word_ins_ml=4.541, word_reposition=1.347, kpe=0.942, ppl=198.8, wps=6347.4, ups=0.31, wpb=20602.5, bsz=256, num_updates=5800, lr=0.000464238, gnorm=1.208, clip=0, loss_scale=2048, train_wall=278, wall=17451
2022-07-25 21:40:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-25 21:42:16 | INFO | train_inner | epoch 006:    296 / 1122 loss=nan, nll_loss=2.968, mask_ins=0.807, word_ins_ml=4.555, word_reposition=1.351, kpe=nan, ppl=nan, wps=7095.4, ups=0.34, wpb=20587.3, bsz=256, num_updates=5900, lr=0.000460287, gnorm=1.506, clip=0, loss_scale=1663, train_wall=244, wall=17741
2022-07-25 21:47:02 | INFO | train_inner | epoch 006:    396 / 1122 loss=7.618, nll_loss=2.933, mask_ins=0.803, word_ins_ml=4.523, word_reposition=1.344, kpe=0.949, ppl=196.49, wps=7200.6, ups=0.35, wpb=20576.6, bsz=256, num_updates=6000, lr=0.000456435, gnorm=1.439, clip=0, loss_scale=1024, train_wall=240, wall=18027
2022-07-25 21:51:48 | INFO | train_inner | epoch 006:    496 / 1122 loss=7.623, nll_loss=2.942, mask_ins=0.804, word_ins_ml=4.53, word_reposition=1.341, kpe=0.948, ppl=197.14, wps=7163.6, ups=0.35, wpb=20499.7, bsz=256, num_updates=6100, lr=0.000452679, gnorm=1.485, clip=0, loss_scale=1024, train_wall=241, wall=18313
2022-07-25 21:56:35 | INFO | train_inner | epoch 006:    596 / 1122 loss=7.593, nll_loss=2.943, mask_ins=0.798, word_ins_ml=4.531, word_reposition=1.318, kpe=0.946, ppl=193.04, wps=7099.3, ups=0.35, wpb=20363.4, bsz=256, num_updates=6200, lr=0.000449013, gnorm=1.405, clip=0, loss_scale=1024, train_wall=242, wall=18600
2022-07-25 22:01:22 | INFO | train_inner | epoch 006:    696 / 1122 loss=7.595, nll_loss=2.942, mask_ins=0.8, word_ins_ml=4.53, word_reposition=1.328, kpe=0.938, ppl=193.39, wps=7194.9, ups=0.35, wpb=20637, bsz=256, num_updates=6300, lr=0.000445435, gnorm=1.224, clip=0, loss_scale=1024, train_wall=241, wall=18887
2022-07-25 22:06:09 | INFO | train_inner | epoch 006:    796 / 1122 loss=nan, nll_loss=2.904, mask_ins=0.797, word_ins_ml=4.495, word_reposition=1.344, kpe=nan, ppl=nan, wps=7147.5, ups=0.35, wpb=20487.4, bsz=256, num_updates=6400, lr=0.000441942, gnorm=1.264, clip=0, loss_scale=1290, train_wall=241, wall=19173
2022-07-25 22:10:54 | INFO | train_inner | epoch 006:    896 / 1122 loss=7.571, nll_loss=2.922, mask_ins=0.794, word_ins_ml=4.51, word_reposition=1.328, kpe=0.938, ppl=190.19, wps=7231.5, ups=0.35, wpb=20650.1, bsz=256, num_updates=6500, lr=0.000438529, gnorm=1.225, clip=0, loss_scale=2048, train_wall=242, wall=19459
2022-07-25 22:15:41 | INFO | train_inner | epoch 006:    996 / 1122 loss=7.575, nll_loss=2.908, mask_ins=0.799, word_ins_ml=4.498, word_reposition=1.339, kpe=0.939, ppl=190.69, wps=7131.9, ups=0.35, wpb=20479.6, bsz=256, num_updates=6600, lr=0.000435194, gnorm=1.28, clip=0, loss_scale=2048, train_wall=242, wall=19746
2022-07-25 22:20:28 | INFO | train_inner | epoch 006:   1096 / 1122 loss=7.553, nll_loss=2.907, mask_ins=0.796, word_ins_ml=4.497, word_reposition=1.326, kpe=0.934, ppl=187.76, wps=7155.1, ups=0.35, wpb=20507.7, bsz=256, num_updates=6700, lr=0.000431934, gnorm=1.185, clip=0, loss_scale=2048, train_wall=242, wall=20033
2022-07-25 22:21:41 | INFO | train | epoch 006 | loss nan | nll_loss 2.934 | mask_ins 0.8 | word_ins_ml 4.523 | word_reposition 1.337 | kpe nan | ppl nan | wps 6848 | ups 0.33 | wpb 20519.4 | bsz 255.8 | num_updates 6726 | lr 0.000431099 | gnorm 1.316 | clip 0 | loss_scale 1581 | train_wall 2744 | wall 20106
2022-07-25 22:23:01 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 13.759 | nll_loss 6.968 | mask_ins 1.553 | word_ins_ml 8.222 | word_reposition 2.551 | kpe 1.434 | ppl 13866 | wps 12349.8 | wpb 2367.6 | bsz 32 | num_updates 6726 | best_loss 13.599
2022-07-25 22:23:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 6 @ 6726 updates, score 13.759) (writing took 9.781358980573714 seconds)
2022-07-25 22:26:45 | INFO | train_inner | epoch 007:     74 / 1122 loss=7.421, nll_loss=2.809, mask_ins=0.784, word_ins_ml=4.41, word_reposition=1.303, kpe=0.924, ppl=171.43, wps=5396.3, ups=0.27, wpb=20322.2, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=1.227, clip=0, loss_scale=2048, train_wall=242, wall=20409
2022-07-25 22:31:33 | INFO | train_inner | epoch 007:    174 / 1122 loss=7.39, nll_loss=2.786, mask_ins=0.781, word_ins_ml=4.39, word_reposition=1.299, kpe=0.92, ppl=167.77, wps=7137.6, ups=0.35, wpb=20588.8, bsz=256, num_updates=6900, lr=0.000425628, gnorm=1.185, clip=0, loss_scale=2335, train_wall=243, wall=20698
2022-07-25 22:36:20 | INFO | train_inner | epoch 007:    274 / 1122 loss=7.505, nll_loss=2.869, mask_ins=0.791, word_ins_ml=4.463, word_reposition=1.327, kpe=0.923, ppl=181.59, wps=7194.9, ups=0.35, wpb=20653.7, bsz=256, num_updates=7000, lr=0.000422577, gnorm=1.242, clip=0, loss_scale=4096, train_wall=242, wall=20985
2022-07-25 22:36:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-25 22:36:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-25 22:36:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-25 22:42:05 | INFO | train_inner | epoch 007:    377 / 1122 loss=7.411, nll_loss=2.813, mask_ins=0.78, word_ins_ml=4.412, word_reposition=1.299, kpe=0.918, ppl=170.13, wps=5952.5, ups=0.29, wpb=20497.1, bsz=256, num_updates=7100, lr=0.000419591, gnorm=1.185, clip=0, loss_scale=666, train_wall=297, wall=21329
2022-07-25 22:46:50 | INFO | train_inner | epoch 007:    477 / 1122 loss=nan, nll_loss=2.794, mask_ins=0.779, word_ins_ml=4.396, word_reposition=1.295, kpe=nan, ppl=nan, wps=7180.8, ups=0.35, wpb=20524.7, bsz=256, num_updates=7200, lr=0.000416667, gnorm=1.165, clip=0, loss_scale=512, train_wall=242, wall=21615
2022-07-25 22:51:37 | INFO | train_inner | epoch 007:    577 / 1122 loss=7.406, nll_loss=2.792, mask_ins=0.787, word_ins_ml=4.394, word_reposition=1.305, kpe=0.92, ppl=169.65, wps=7137.8, ups=0.35, wpb=20453.1, bsz=256, num_updates=7300, lr=0.000413803, gnorm=1.185, clip=0, loss_scale=512, train_wall=241, wall=21902
2022-07-25 22:56:23 | INFO | train_inner | epoch 007:    677 / 1122 loss=7.427, nll_loss=2.8, mask_ins=0.787, word_ins_ml=4.4, word_reposition=1.319, kpe=0.922, ppl=172.15, wps=7171.1, ups=0.35, wpb=20519.5, bsz=256, num_updates=7400, lr=0.000410997, gnorm=1.168, clip=0, loss_scale=512, train_wall=241, wall=22188
2022-07-25 23:01:09 | INFO | train_inner | epoch 007:    777 / 1122 loss=7.36, nll_loss=2.754, mask_ins=0.778, word_ins_ml=4.36, word_reposition=1.306, kpe=0.915, ppl=164.23, wps=7182.3, ups=0.35, wpb=20558.8, bsz=256, num_updates=7500, lr=0.000408248, gnorm=1.168, clip=0, loss_scale=512, train_wall=242, wall=22474
2022-07-25 23:05:55 | INFO | train_inner | epoch 007:    877 / 1122 loss=7.428, nll_loss=2.821, mask_ins=0.783, word_ins_ml=4.418, word_reposition=1.308, kpe=0.919, ppl=172.26, wps=7179.9, ups=0.35, wpb=20536.3, bsz=256, num_updates=7600, lr=0.000405554, gnorm=1.149, clip=0, loss_scale=942, train_wall=241, wall=22760
2022-07-25 23:10:40 | INFO | train_inner | epoch 007:    977 / 1122 loss=7.348, nll_loss=2.759, mask_ins=0.774, word_ins_ml=4.363, word_reposition=1.292, kpe=0.918, ppl=162.91, wps=7238.1, ups=0.35, wpb=20602.1, bsz=256, num_updates=7700, lr=0.000402911, gnorm=1.154, clip=0, loss_scale=1024, train_wall=241, wall=23045
2022-07-25 23:15:25 | INFO | train_inner | epoch 007:   1077 / 1122 loss=7.366, nll_loss=2.772, mask_ins=0.779, word_ins_ml=4.374, word_reposition=1.294, kpe=0.918, ppl=164.93, wps=7202.6, ups=0.35, wpb=20515.4, bsz=256, num_updates=7800, lr=0.00040032, gnorm=1.162, clip=0, loss_scale=1024, train_wall=241, wall=23330
2022-07-25 23:17:33 | INFO | train | epoch 007 | loss nan | nll_loss 2.794 | mask_ins 0.782 | word_ins_ml 4.396 | word_reposition 1.304 | kpe nan | ppl nan | wps 6850.1 | ups 0.33 | wpb 20518.9 | bsz 255.8 | num_updates 7845 | lr 0.000399171 | gnorm 1.181 | clip 0 | loss_scale 1259 | train_wall 2759 | wall 23458
2022-07-25 23:18:53 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 13.739 | nll_loss 6.866 | mask_ins 1.559 | word_ins_ml 8.14 | word_reposition 2.607 | kpe 1.432 | ppl 13671 | wps 12367.8 | wpb 2367.6 | bsz 32 | num_updates 7845 | best_loss 13.599
2022-07-25 23:19:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 7 @ 7845 updates, score 13.739) (writing took 15.27151220291853 seconds)
2022-07-25 23:21:46 | INFO | train_inner | epoch 008:     55 / 1122 loss=7.339, nll_loss=2.747, mask_ins=0.783, word_ins_ml=4.352, word_reposition=1.296, kpe=0.908, ppl=161.95, wps=5346.3, ups=0.26, wpb=20388.8, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=1.212, clip=0, loss_scale=1024, train_wall=241, wall=23711
2022-07-25 23:26:35 | INFO | train_inner | epoch 008:    155 / 1122 loss=nan, nll_loss=2.665, mask_ins=0.772, word_ins_ml=4.28, word_reposition=1.272, kpe=nan, ppl=nan, wps=7094.8, ups=0.35, wpb=20508.8, bsz=256, num_updates=8000, lr=0.000395285, gnorm=1.186, clip=0, loss_scale=1024, train_wall=244, wall=24000
2022-07-25 23:31:22 | INFO | train_inner | epoch 008:    255 / 1122 loss=7.236, nll_loss=2.665, mask_ins=0.773, word_ins_ml=4.28, word_reposition=1.283, kpe=0.901, ppl=150.79, wps=7158.1, ups=0.35, wpb=20506.7, bsz=256, num_updates=8100, lr=0.000392837, gnorm=1.179, clip=0, loss_scale=1761, train_wall=242, wall=24286
2022-07-25 23:36:10 | INFO | train_inner | epoch 008:    355 / 1122 loss=7.245, nll_loss=2.674, mask_ins=0.771, word_ins_ml=4.286, word_reposition=1.28, kpe=0.908, ppl=151.74, wps=7142.7, ups=0.35, wpb=20550.8, bsz=256, num_updates=8200, lr=0.000390434, gnorm=1.268, clip=0, loss_scale=2048, train_wall=242, wall=24574
2022-07-25 23:40:57 | INFO | train_inner | epoch 008:    455 / 1122 loss=7.24, nll_loss=2.668, mask_ins=0.77, word_ins_ml=4.282, word_reposition=1.289, kpe=0.9, ppl=151.18, wps=7155.2, ups=0.35, wpb=20539.7, bsz=256, num_updates=8300, lr=0.000388075, gnorm=1.169, clip=0, loss_scale=2048, train_wall=241, wall=24861
2022-07-25 23:46:30 | INFO | train_inner | epoch 008:    555 / 1122 loss=7.251, nll_loss=2.688, mask_ins=0.77, word_ins_ml=4.299, word_reposition=1.277, kpe=0.905, ppl=152.35, wps=6166.8, ups=0.3, wpb=20568.3, bsz=256, num_updates=8400, lr=0.000385758, gnorm=1.17, clip=0, loss_scale=2048, train_wall=289, wall=25195
2022-07-25 23:51:16 | INFO | train_inner | epoch 008:    655 / 1122 loss=7.249, nll_loss=2.687, mask_ins=0.77, word_ins_ml=4.298, word_reposition=1.276, kpe=0.905, ppl=152.12, wps=7217.7, ups=0.35, wpb=20618.5, bsz=256, num_updates=8500, lr=0.000383482, gnorm=1.176, clip=0, loss_scale=2048, train_wall=241, wall=25480
2022-07-25 23:56:02 | INFO | train_inner | epoch 008:    755 / 1122 loss=7.225, nll_loss=2.665, mask_ins=0.769, word_ins_ml=4.279, word_reposition=1.274, kpe=0.904, ppl=149.6, wps=7167.9, ups=0.35, wpb=20488.6, bsz=256, num_updates=8600, lr=0.000381246, gnorm=1.185, clip=0, loss_scale=3277, train_wall=241, wall=25766
2022-07-26 00:00:48 | INFO | train_inner | epoch 008:    855 / 1122 loss=nan, nll_loss=2.712, mask_ins=0.772, word_ins_ml=4.319, word_reposition=1.287, kpe=nan, ppl=nan, wps=7183.1, ups=0.35, wpb=20560.7, bsz=256, num_updates=8700, lr=0.000379049, gnorm=1.177, clip=0, loss_scale=4096, train_wall=240, wall=26053
2022-07-26 00:05:34 | INFO | train_inner | epoch 008:    955 / 1122 loss=7.209, nll_loss=2.661, mask_ins=0.767, word_ins_ml=4.273, word_reposition=1.266, kpe=0.903, ppl=147.96, wps=7181, ups=0.35, wpb=20571.2, bsz=256, num_updates=8800, lr=0.000376889, gnorm=1.142, clip=0, loss_scale=4096, train_wall=242, wall=26339
2022-07-26 00:10:21 | INFO | train_inner | epoch 008:   1055 / 1122 loss=7.29, nll_loss=2.712, mask_ins=0.774, word_ins_ml=4.319, word_reposition=1.292, kpe=0.905, ppl=156.51, wps=7098.6, ups=0.35, wpb=20349.1, bsz=256, num_updates=8900, lr=0.000374766, gnorm=1.185, clip=0, loss_scale=4096, train_wall=240, wall=26626
2022-07-26 00:13:32 | INFO | train | epoch 008 | loss nan | nll_loss 2.687 | mask_ins 0.772 | word_ins_ml 4.298 | word_reposition 1.282 | kpe nan | ppl nan | wps 6854.4 | ups 0.33 | wpb 20521.1 | bsz 255.8 | num_updates 8967 | lr 0.000373363 | gnorm 1.187 | clip 0 | loss_scale 2660 | train_wall 2757 | wall 26817
2022-07-26 00:14:52 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 13.63 | nll_loss 6.889 | mask_ins 1.547 | word_ins_ml 8.165 | word_reposition 2.524 | kpe 1.394 | ppl 12677.7 | wps 12337.6 | wpb 2367.6 | bsz 32 | num_updates 8967 | best_loss 13.599
2022-07-26 00:15:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 8 @ 8967 updates, score 13.63) (writing took 12.08872798550874 seconds)
2022-07-26 00:16:38 | INFO | train_inner | epoch 009:     33 / 1122 loss=7.282, nll_loss=2.705, mask_ins=0.776, word_ins_ml=4.313, word_reposition=1.293, kpe=0.899, ppl=155.58, wps=5382.3, ups=0.26, wpb=20313.4, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=1.217, clip=0, loss_scale=4096, train_wall=241, wall=27003
2022-07-26 00:20:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-26 00:21:26 | INFO | train_inner | epoch 009:    134 / 1122 loss=7.137, nll_loss=2.597, mask_ins=0.765, word_ins_ml=4.218, word_reposition=1.268, kpe=0.887, ppl=140.79, wps=7174.9, ups=0.35, wpb=20662.9, bsz=256, num_updates=9100, lr=0.000370625, gnorm=1.168, clip=0, loss_scale=4867, train_wall=244, wall=27291
2022-07-26 00:23:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-26 00:26:19 | INFO | train_inner | epoch 009:    235 / 1122 loss=7.078, nll_loss=2.582, mask_ins=0.757, word_ins_ml=4.203, word_reposition=1.231, kpe=0.887, ppl=135.12, wps=6999.5, ups=0.34, wpb=20480.4, bsz=256, num_updates=9200, lr=0.000368605, gnorm=1.18, clip=0, loss_scale=2940, train_wall=246, wall=27584
2022-07-26 00:31:05 | INFO | train_inner | epoch 009:    335 / 1122 loss=7.078, nll_loss=2.57, mask_ins=0.76, word_ins_ml=4.193, word_reposition=1.232, kpe=0.892, ppl=135.07, wps=7160, ups=0.35, wpb=20452.7, bsz=256, num_updates=9300, lr=0.000366618, gnorm=1.217, clip=0, loss_scale=2048, train_wall=241, wall=27869
2022-07-26 00:35:51 | INFO | train_inner | epoch 009:    435 / 1122 loss=nan, nll_loss=2.586, mask_ins=0.763, word_ins_ml=4.207, word_reposition=1.267, kpe=nan, ppl=nan, wps=7138.3, ups=0.35, wpb=20462.9, bsz=256, num_updates=9400, lr=0.000364662, gnorm=1.17, clip=0, loss_scale=2048, train_wall=241, wall=28156
2022-07-26 00:40:38 | INFO | train_inner | epoch 009:    535 / 1122 loss=7.106, nll_loss=2.595, mask_ins=0.756, word_ins_ml=4.215, word_reposition=1.246, kpe=0.889, ppl=137.8, wps=7174.9, ups=0.35, wpb=20555.1, bsz=256, num_updates=9500, lr=0.000362738, gnorm=1.206, clip=0, loss_scale=2048, train_wall=241, wall=28443
2022-07-26 00:45:24 | INFO | train_inner | epoch 009:    635 / 1122 loss=7.108, nll_loss=2.597, mask_ins=0.758, word_ins_ml=4.216, word_reposition=1.247, kpe=0.887, ppl=137.99, wps=7175.4, ups=0.35, wpb=20546.7, bsz=256, num_updates=9600, lr=0.000360844, gnorm=1.153, clip=0, loss_scale=2048, train_wall=242, wall=28729
2022-07-26 00:50:48 | INFO | train_inner | epoch 009:    735 / 1122 loss=7.124, nll_loss=2.596, mask_ins=0.761, word_ins_ml=4.215, word_reposition=1.259, kpe=0.89, ppl=139.54, wps=6354, ups=0.31, wpb=20558.9, bsz=256, num_updates=9700, lr=0.000358979, gnorm=1.167, clip=0, loss_scale=2970, train_wall=278, wall=29052
2022-07-26 00:55:33 | INFO | train_inner | epoch 009:    835 / 1122 loss=7.164, nll_loss=2.624, mask_ins=0.765, word_ins_ml=4.24, word_reposition=1.268, kpe=0.891, ppl=143.43, wps=7183.1, ups=0.35, wpb=20498, bsz=256, num_updates=9800, lr=0.000357143, gnorm=1.177, clip=0, loss_scale=4096, train_wall=241, wall=29338
2022-07-26 01:00:19 | INFO | train_inner | epoch 009:    935 / 1122 loss=7.078, nll_loss=2.569, mask_ins=0.756, word_ins_ml=4.191, word_reposition=1.245, kpe=0.887, ppl=135.1, wps=7194.5, ups=0.35, wpb=20583.1, bsz=256, num_updates=9900, lr=0.000355335, gnorm=1.16, clip=0, loss_scale=4096, train_wall=241, wall=29624
2022-07-26 01:05:06 | INFO | train_inner | epoch 009:   1035 / 1122 loss=nan, nll_loss=2.63, mask_ins=0.765, word_ins_ml=4.245, word_reposition=1.263, kpe=nan, ppl=nan, wps=7155.8, ups=0.35, wpb=20519.6, bsz=256, num_updates=10000, lr=0.000353553, gnorm=1.169, clip=0, loss_scale=4096, train_wall=242, wall=29911
2022-07-26 01:09:16 | INFO | train | epoch 009 | loss nan | nll_loss 2.598 | mask_ins 0.761 | word_ins_ml 4.217 | word_reposition 1.253 | kpe nan | ppl nan | wps 6873.4 | ups 0.33 | wpb 20522.1 | bsz 255.8 | num_updates 10087 | lr 0.000352025 | gnorm 1.183 | clip 0 | loss_scale 3231 | train_wall 2746 | wall 30161
2022-07-26 01:10:36 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 13.619 | nll_loss 6.883 | mask_ins 1.549 | word_ins_ml 8.161 | word_reposition 2.496 | kpe 1.412 | ppl 12578.5 | wps 12406.7 | wpb 2367.6 | bsz 32 | num_updates 10087 | best_loss 13.599
2022-07-26 01:10:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 9 @ 10087 updates, score 13.619) (writing took 22.836641473695636 seconds)
2022-07-26 01:11:36 | INFO | train_inner | epoch 010:     13 / 1122 loss=7.152, nll_loss=2.635, mask_ins=0.764, word_ins_ml=4.249, word_reposition=1.254, kpe=0.885, ppl=142.26, wps=5247.2, ups=0.26, wpb=20466, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=1.264, clip=0, loss_scale=4096, train_wall=241, wall=30301
2022-07-26 01:16:23 | INFO | train_inner | epoch 010:    113 / 1122 loss=6.951, nll_loss=2.478, mask_ins=0.749, word_ins_ml=4.111, word_reposition=1.218, kpe=0.874, ppl=123.74, wps=7177.9, ups=0.35, wpb=20579.3, bsz=256, num_updates=10200, lr=0.00035007, gnorm=1.167, clip=0, loss_scale=5448, train_wall=241, wall=30587
2022-07-26 01:17:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-26 01:21:11 | INFO | train_inner | epoch 010:    214 / 1122 loss=6.978, nll_loss=2.493, mask_ins=0.75, word_ins_ml=4.124, word_reposition=1.234, kpe=0.87, ppl=126.04, wps=7133.7, ups=0.35, wpb=20534.9, bsz=256, num_updates=10300, lr=0.000348367, gnorm=1.157, clip=0, loss_scale=5353, train_wall=243, wall=30875
2022-07-26 01:26:00 | INFO | train_inner | epoch 010:    314 / 1122 loss=6.994, nll_loss=2.507, mask_ins=0.755, word_ins_ml=4.136, word_reposition=1.229, kpe=0.874, ppl=127.46, wps=7116.7, ups=0.35, wpb=20607.2, bsz=256, num_updates=10400, lr=0.000346688, gnorm=1.182, clip=0, loss_scale=4096, train_wall=244, wall=31165
2022-07-26 01:30:47 | INFO | train_inner | epoch 010:    414 / 1122 loss=6.978, nll_loss=2.497, mask_ins=0.751, word_ins_ml=4.127, word_reposition=1.222, kpe=0.879, ppl=126.09, wps=7178.4, ups=0.35, wpb=20562.6, bsz=256, num_updates=10500, lr=0.000345033, gnorm=1.174, clip=0, loss_scale=4096, train_wall=242, wall=31451
2022-07-26 01:35:34 | INFO | train_inner | epoch 010:    514 / 1122 loss=7.035, nll_loss=2.525, mask_ins=0.755, word_ins_ml=4.152, word_reposition=1.259, kpe=0.869, ppl=131.12, wps=7148.2, ups=0.35, wpb=20513.3, bsz=256, num_updates=10600, lr=0.000343401, gnorm=1.185, clip=0, loss_scale=4096, train_wall=242, wall=31738
2022-07-26 01:40:21 | INFO | train_inner | epoch 010:    614 / 1122 loss=nan, nll_loss=2.574, mask_ins=0.761, word_ins_ml=4.196, word_reposition=1.255, kpe=nan, ppl=nan, wps=7199.4, ups=0.35, wpb=20699.5, bsz=256, num_updates=10700, lr=0.000341793, gnorm=1.203, clip=0, loss_scale=4096, train_wall=241, wall=32026
2022-07-26 01:45:08 | INFO | train_inner | epoch 010:    714 / 1122 loss=7.032, nll_loss=2.535, mask_ins=0.755, word_ins_ml=4.161, word_reposition=1.239, kpe=0.877, ppl=130.87, wps=7111.2, ups=0.35, wpb=20397, bsz=256, num_updates=10800, lr=0.000340207, gnorm=1.181, clip=0, loss_scale=6472, train_wall=241, wall=32313
2022-07-26 01:49:55 | INFO | train_inner | epoch 010:    814 / 1122 loss=nan, nll_loss=2.534, mask_ins=0.754, word_ins_ml=4.159, word_reposition=1.23, kpe=nan, ppl=nan, wps=7161.5, ups=0.35, wpb=20530, bsz=256, num_updates=10900, lr=0.000338643, gnorm=1.199, clip=0, loss_scale=8192, train_wall=241, wall=32599
2022-07-26 01:55:18 | INFO | train_inner | epoch 010:    914 / 1122 loss=7.001, nll_loss=2.515, mask_ins=0.75, word_ins_ml=4.142, word_reposition=1.229, kpe=0.88, ppl=128.08, wps=6363.3, ups=0.31, wpb=20540.4, bsz=256, num_updates=11000, lr=0.0003371, gnorm=1.174, clip=0, loss_scale=8192, train_wall=283, wall=32922
2022-07-26 01:59:53 | INFO | train_inner | epoch 010:   1014 / 1122 loss=7.064, nll_loss=2.547, mask_ins=0.758, word_ins_ml=4.171, word_reposition=1.255, kpe=0.88, ppl=133.79, wps=7434.5, ups=0.36, wpb=20461.9, bsz=256, num_updates=11100, lr=0.000335578, gnorm=1.189, clip=0, loss_scale=8192, train_wall=236, wall=33197
2022-07-26 02:04:28 | INFO | train_inner | epoch 010:   1114 / 1122 loss=6.987, nll_loss=2.503, mask_ins=0.748, word_ins_ml=4.131, word_reposition=1.23, kpe=0.878, ppl=126.85, wps=7446.1, ups=0.36, wpb=20472.6, bsz=256, num_updates=11200, lr=0.000334077, gnorm=1.166, clip=0, loss_scale=8192, train_wall=236, wall=33472
2022-07-26 02:04:49 | INFO | train | epoch 010 | loss nan | nll_loss 2.52 | mask_ins 0.753 | word_ins_ml 4.147 | word_reposition 1.236 | kpe nan | ppl nan | wps 6902.9 | ups 0.34 | wpb 20521.6 | bsz 255.8 | num_updates 11208 | lr 0.000333957 | gnorm 1.184 | clip 0 | loss_scale 6031 | train_wall 2739 | wall 33494
2022-07-26 02:06:05 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 13.462 | nll_loss 6.704 | mask_ins 1.549 | word_ins_ml 7.998 | word_reposition 2.535 | kpe 1.379 | ppl 11280.4 | wps 13072.6 | wpb 2367.6 | bsz 32 | num_updates 11208 | best_loss 13.462
2022-07-26 02:06:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_best.pt (epoch 10 @ 11208 updates, score 13.462) (writing took 24.913606191053987 seconds)
2022-07-26 02:09:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-26 02:10:45 | INFO | train_inner | epoch 011:     93 / 1122 loss=6.963, nll_loss=2.486, mask_ins=0.753, word_ins_ml=4.117, word_reposition=1.234, kpe=0.859, ppl=124.75, wps=5404.3, ups=0.26, wpb=20394.5, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=1.205, clip=0, loss_scale=10220, train_wall=237, wall=33850
2022-07-26 02:15:20 | INFO | train_inner | epoch 011:    193 / 1122 loss=nan, nll_loss=2.414, mask_ins=0.746, word_ins_ml=4.053, word_reposition=1.223, kpe=nan, ppl=nan, wps=7445.1, ups=0.36, wpb=20469.8, bsz=256, num_updates=11400, lr=0.000331133, gnorm=1.208, clip=0, loss_scale=8192, train_wall=235, wall=34125
2022-07-26 02:19:54 | INFO | train_inner | epoch 011:    293 / 1122 loss=nan, nll_loss=2.409, mask_ins=0.742, word_ins_ml=4.048, word_reposition=1.202, kpe=nan, ppl=nan, wps=7480.4, ups=0.36, wpb=20531, bsz=256, num_updates=11500, lr=0.00032969, gnorm=1.193, clip=0, loss_scale=8192, train_wall=235, wall=34399
2022-07-26 02:23:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-26 02:24:32 | INFO | train_inner | epoch 011:    394 / 1122 loss=6.854, nll_loss=2.418, mask_ins=0.742, word_ins_ml=4.056, word_reposition=1.196, kpe=0.86, ppl=115.67, wps=7404.8, ups=0.36, wpb=20568.8, bsz=256, num_updates=11600, lr=0.000328266, gnorm=1.192, clip=0, loss_scale=7584, train_wall=238, wall=34677
2022-07-26 02:29:07 | INFO | train_inner | epoch 011:    494 / 1122 loss=6.923, nll_loss=2.465, mask_ins=0.75, word_ins_ml=4.098, word_reposition=1.209, kpe=0.865, ppl=121.32, wps=7511.6, ups=0.36, wpb=20651.1, bsz=256, num_updates=11700, lr=0.00032686, gnorm=1.201, clip=0, loss_scale=4096, train_wall=236, wall=34952
2022-07-26 02:33:42 | INFO | train_inner | epoch 011:    594 / 1122 loss=6.93, nll_loss=2.481, mask_ins=0.745, word_ins_ml=4.111, word_reposition=1.213, kpe=0.861, ppl=121.91, wps=7476.2, ups=0.36, wpb=20543.4, bsz=256, num_updates=11800, lr=0.000325472, gnorm=1.197, clip=0, loss_scale=4096, train_wall=236, wall=35227
2022-07-26 02:38:17 | INFO | train_inner | epoch 011:    694 / 1122 loss=6.946, nll_loss=2.47, mask_ins=0.75, word_ins_ml=4.102, word_reposition=1.226, kpe=0.868, ppl=123.33, wps=7467.1, ups=0.36, wpb=20527.8, bsz=256, num_updates=11900, lr=0.000324102, gnorm=1.231, clip=0, loss_scale=4096, train_wall=236, wall=35502
2022-07-26 02:42:55 | INFO | train_inner | epoch 011:    794 / 1122 loss=6.886, nll_loss=2.431, mask_ins=0.746, word_ins_ml=4.067, word_reposition=1.204, kpe=0.869, ppl=118.24, wps=7322.6, ups=0.36, wpb=20332.7, bsz=256, num_updates=12000, lr=0.000322749, gnorm=1.203, clip=0, loss_scale=4096, train_wall=237, wall=35779
2022-07-26 02:47:34 | INFO | train_inner | epoch 011:    894 / 1122 loss=6.937, nll_loss=2.465, mask_ins=0.745, word_ins_ml=4.097, word_reposition=1.232, kpe=0.863, ppl=122.52, wps=7376.5, ups=0.36, wpb=20629.2, bsz=256, num_updates=12100, lr=0.000321412, gnorm=1.183, clip=0, loss_scale=4219, train_wall=238, wall=36059
2022-07-26 02:52:20 | INFO | train_inner | epoch 011:    994 / 1122 loss=6.956, nll_loss=2.487, mask_ins=0.749, word_ins_ml=4.116, word_reposition=1.224, kpe=0.867, ppl=124.19, wps=7181.2, ups=0.35, wpb=20487, bsz=256, num_updates=12200, lr=0.000320092, gnorm=1.172, clip=0, loss_scale=8192, train_wall=243, wall=36344
2022-07-26 02:54:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-26 02:57:36 | INFO | train_inner | epoch 011:   1095 / 1122 loss=6.886, nll_loss=2.434, mask_ins=0.742, word_ins_ml=4.07, word_reposition=1.211, kpe=0.864, ppl=118.3, wps=6508.3, ups=0.32, wpb=20575.9, bsz=256, num_updates=12300, lr=0.000318788, gnorm=1.185, clip=0, loss_scale=5597, train_wall=276, wall=36660
2022-07-26 02:58:50 | INFO | train | epoch 011 | loss nan | nll_loss 2.451 | mask_ins 0.746 | word_ins_ml 4.085 | word_reposition 1.216 | kpe nan | ppl nan | wps 7084.5 | ups 0.35 | wpb 20522.7 | bsz 255.8 | num_updates 12327 | lr 0.000318439 | gnorm 1.198 | clip 0 | loss_scale 6173 | train_wall 2693 | wall 36735
2022-07-26 03:00:07 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 13.597 | nll_loss 6.895 | mask_ins 1.542 | word_ins_ml 8.176 | word_reposition 2.489 | kpe 1.39 | ppl 12391.4 | wps 12849.9 | wpb 2367.6 | bsz 32 | num_updates 12327 | best_loss 13.462
2022-07-26 03:00:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 11 @ 12327 updates, score 13.597) (writing took 12.12417475041002 seconds)
2022-07-26 03:03:46 | INFO | train_inner | epoch 012:     73 / 1122 loss=6.869, nll_loss=2.427, mask_ins=0.745, word_ins_ml=4.064, word_reposition=1.207, kpe=0.854, ppl=116.93, wps=5528.7, ups=0.27, wpb=20456.1, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=1.259, clip=0, loss_scale=4096, train_wall=240, wall=37030
2022-07-26 03:08:22 | INFO | train_inner | epoch 012:    173 / 1122 loss=6.81, nll_loss=2.375, mask_ins=0.742, word_ins_ml=4.019, word_reposition=1.204, kpe=0.846, ppl=112.22, wps=7408.9, ups=0.36, wpb=20488.9, bsz=256, num_updates=12500, lr=0.000316228, gnorm=1.185, clip=0, loss_scale=4096, train_wall=237, wall=37307
2022-07-26 03:12:59 | INFO | train_inner | epoch 012:    273 / 1122 loss=6.819, nll_loss=2.4, mask_ins=0.738, word_ins_ml=4.04, word_reposition=1.196, kpe=0.845, ppl=112.93, wps=7422.5, ups=0.36, wpb=20508.5, bsz=256, num_updates=12600, lr=0.00031497, gnorm=1.197, clip=0, loss_scale=4096, train_wall=237, wall=37583
2022-07-26 03:17:39 | INFO | train_inner | epoch 012:    373 / 1122 loss=6.776, nll_loss=2.342, mask_ins=0.737, word_ins_ml=3.988, word_reposition=1.2, kpe=0.851, ppl=109.6, wps=7344, ups=0.36, wpb=20607.2, bsz=256, num_updates=12700, lr=0.000313728, gnorm=1.199, clip=0, loss_scale=4096, train_wall=239, wall=37864
2022-07-26 03:22:23 | INFO | train_inner | epoch 012:    473 / 1122 loss=6.825, nll_loss=2.4, mask_ins=0.741, word_ins_ml=4.039, word_reposition=1.192, kpe=0.852, ppl=113.36, wps=7209.3, ups=0.35, wpb=20492.2, bsz=256, num_updates=12800, lr=0.0003125, gnorm=1.193, clip=0, loss_scale=6226, train_wall=240, wall=38148
2022-07-26 03:27:05 | INFO | train_inner | epoch 012:    573 / 1122 loss=6.849, nll_loss=2.427, mask_ins=0.739, word_ins_ml=4.063, word_reposition=1.196, kpe=0.851, ppl=115.26, wps=7312.4, ups=0.35, wpb=20611.6, bsz=256, num_updates=12900, lr=0.000311286, gnorm=1.223, clip=0, loss_scale=8192, train_wall=240, wall=38430
2022-07-26 03:31:43 | INFO | train_inner | epoch 012:    673 / 1122 loss=6.779, nll_loss=2.363, mask_ins=0.734, word_ins_ml=4.007, word_reposition=1.187, kpe=0.851, ppl=109.86, wps=7330.3, ups=0.36, wpb=20374.1, bsz=256, num_updates=13000, lr=0.000310087, gnorm=1.2, clip=0, loss_scale=8192, train_wall=238, wall=38708
2022-07-26 03:36:18 | INFO | train_inner | epoch 012:    773 / 1122 loss=6.818, nll_loss=2.382, mask_ins=0.738, word_ins_ml=4.023, word_reposition=1.204, kpe=0.853, ppl=112.85, wps=7494.8, ups=0.36, wpb=20613.2, bsz=256, num_updates=13100, lr=0.000308901, gnorm=1.22, clip=0, loss_scale=8192, train_wall=236, wall=38983
2022-07-26 03:40:53 | INFO | train_inner | epoch 012:    873 / 1122 loss=nan, nll_loss=2.413, mask_ins=0.744, word_ins_ml=4.051, word_reposition=1.204, kpe=nan, ppl=nan, wps=7476.9, ups=0.36, wpb=20559, bsz=256, num_updates=13200, lr=0.000307729, gnorm=1.222, clip=0, loss_scale=8192, train_wall=235, wall=39258
2022-07-26 03:44:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-26 03:45:35 | INFO | train_inner | epoch 012:    974 / 1122 loss=6.801, nll_loss=2.377, mask_ins=0.738, word_ins_ml=4.019, word_reposition=1.194, kpe=0.851, ppl=111.48, wps=7281.5, ups=0.36, wpb=20502.9, bsz=256, num_updates=13300, lr=0.00030657, gnorm=1.206, clip=0, loss_scale=8922, train_wall=242, wall=39540
2022-07-26 03:50:10 | INFO | train_inner | epoch 012:   1074 / 1122 loss=nan, nll_loss=2.421, mask_ins=0.739, word_ins_ml=4.057, word_reposition=1.188, kpe=nan, ppl=nan, wps=7459.6, ups=0.36, wpb=20498.5, bsz=256, num_updates=13400, lr=0.000305424, gnorm=1.188, clip=0, loss_scale=8192, train_wall=235, wall=39814
2022-07-26 03:52:21 | INFO | train | epoch 012 | loss nan | nll_loss 2.393 | mask_ins 0.74 | word_ins_ml 4.033 | word_reposition 1.198 | kpe nan | ppl nan | wps 7165.1 | ups 0.35 | wpb 20520.1 | bsz 255.8 | num_updates 13448 | lr 0.000304878 | gnorm 1.208 | clip 0 | loss_scale 6721 | train_wall 2668 | wall 39946
2022-07-26 03:53:36 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 13.73 | nll_loss 6.79 | mask_ins 1.56 | word_ins_ml 8.087 | word_reposition 2.615 | kpe 1.469 | ppl 13590.5 | wps 13108.2 | wpb 2367.6 | bsz 32 | num_updates 13448 | best_loss 13.462
2022-07-26 03:53:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 12 @ 13448 updates, score 13.73) (writing took 20.35354639403522 seconds)
2022-07-26 03:56:53 | INFO | train_inner | epoch 013:     52 / 1122 loss=6.775, nll_loss=2.364, mask_ins=0.735, word_ins_ml=4.007, word_reposition=1.192, kpe=0.84, ppl=109.5, wps=5048.6, ups=0.25, wpb=20381.7, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=1.255, clip=0, loss_scale=8192, train_wall=269, wall=40218
2022-07-26 04:01:40 | INFO | train_inner | epoch 013:    152 / 1122 loss=6.696, nll_loss=2.31, mask_ins=0.731, word_ins_ml=3.96, word_reposition=1.171, kpe=0.834, ppl=103.7, wps=7178.7, ups=0.35, wpb=20591.3, bsz=256, num_updates=13600, lr=0.00030317, gnorm=1.209, clip=0, loss_scale=8192, train_wall=248, wall=40505
2022-07-26 04:06:16 | INFO | train_inner | epoch 013:    252 / 1122 loss=6.761, nll_loss=2.357, mask_ins=0.736, word_ins_ml=4.002, word_reposition=1.188, kpe=0.835, ppl=108.45, wps=7421.5, ups=0.36, wpb=20469, bsz=256, num_updates=13700, lr=0.000302061, gnorm=1.219, clip=0, loss_scale=8192, train_wall=236, wall=40781
2022-07-26 04:10:50 | INFO | train_inner | epoch 013:    352 / 1122 loss=nan, nll_loss=2.355, mask_ins=0.737, word_ins_ml=4, word_reposition=1.19, kpe=nan, ppl=nan, wps=7487, ups=0.36, wpb=20547.9, bsz=256, num_updates=13800, lr=0.000300965, gnorm=1.235, clip=0, loss_scale=9830, train_wall=235, wall=41055
2022-07-26 04:12:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-26 04:15:28 | INFO | train_inner | epoch 013:    453 / 1122 loss=6.789, nll_loss=2.346, mask_ins=0.741, word_ins_ml=3.992, word_reposition=1.22, kpe=0.836, ppl=110.59, wps=7415.5, ups=0.36, wpb=20571.1, bsz=256, num_updates=13900, lr=0.00029988, gnorm=1.225, clip=0, loss_scale=10625, train_wall=238, wall=41333
2022-07-26 04:20:03 | INFO | train_inner | epoch 013:    553 / 1122 loss=6.723, nll_loss=2.335, mask_ins=0.729, word_ins_ml=3.981, word_reposition=1.174, kpe=0.839, ppl=105.65, wps=7465, ups=0.36, wpb=20537.5, bsz=256, num_updates=14000, lr=0.000298807, gnorm=1.204, clip=0, loss_scale=8192, train_wall=236, wall=41608
2022-07-26 04:24:38 | INFO | train_inner | epoch 013:    653 / 1122 loss=6.713, nll_loss=2.319, mask_ins=0.733, word_ins_ml=3.967, word_reposition=1.174, kpe=0.839, ppl=104.91, wps=7445.6, ups=0.36, wpb=20453.5, bsz=256, num_updates=14100, lr=0.000297746, gnorm=1.232, clip=0, loss_scale=8192, train_wall=236, wall=41882
2022-07-26 04:29:12 | INFO | train_inner | epoch 013:    753 / 1122 loss=nan, nll_loss=2.35, mask_ins=0.736, word_ins_ml=3.995, word_reposition=1.195, kpe=nan, ppl=nan, wps=7481.3, ups=0.36, wpb=20537.9, bsz=256, num_updates=14200, lr=0.000296695, gnorm=1.229, clip=0, loss_scale=8192, train_wall=235, wall=42157
2022-07-26 04:33:47 | INFO | train_inner | epoch 013:    853 / 1122 loss=6.762, nll_loss=2.352, mask_ins=0.737, word_ins_ml=3.996, word_reposition=1.19, kpe=0.839, ppl=108.55, wps=7469.6, ups=0.36, wpb=20518.9, bsz=256, num_updates=14300, lr=0.000295656, gnorm=1.238, clip=0, loss_scale=8192, train_wall=236, wall=42432
2022-07-26 04:36:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-26 04:38:24 | INFO | train_inner | epoch 013:    954 / 1122 loss=6.7, nll_loss=2.309, mask_ins=0.73, word_ins_ml=3.958, word_reposition=1.169, kpe=0.843, ppl=103.97, wps=7401.4, ups=0.36, wpb=20477.6, bsz=256, num_updates=14400, lr=0.000294628, gnorm=1.251, clip=0, loss_scale=9733, train_wall=237, wall=42708
2022-07-26 04:42:58 | INFO | train_inner | epoch 013:   1054 / 1122 loss=6.707, nll_loss=2.336, mask_ins=0.725, word_ins_ml=3.981, word_reposition=1.162, kpe=0.838, ppl=104.46, wps=7520.1, ups=0.36, wpb=20651, bsz=256, num_updates=14500, lr=0.00029361, gnorm=1.221, clip=0, loss_scale=8192, train_wall=235, wall=42983
2022-07-26 04:46:04 | INFO | train | epoch 013 | loss nan | nll_loss 2.339 | mask_ins 0.734 | word_ins_ml 3.985 | word_reposition 1.184 | kpe nan | ppl nan | wps 7130.2 | ups 0.35 | wpb 20521.2 | bsz 255.8 | num_updates 14568 | lr 0.000292924 | gnorm 1.231 | clip 0 | loss_scale 8696 | train_wall 2688 | wall 43169
2022-07-26 04:47:20 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 13.327 | nll_loss 6.599 | mask_ins 1.526 | word_ins_ml 7.897 | word_reposition 2.495 | kpe 1.41 | ppl 10279.5 | wps 13081.3 | wpb 2367.6 | bsz 32 | num_updates 14568 | best_loss 13.327
2022-07-26 04:47:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_best.pt (epoch 13 @ 14568 updates, score 13.327) (writing took 19.83655118662864 seconds)
2022-07-26 04:49:07 | INFO | train_inner | epoch 014:     32 / 1122 loss=6.747, nll_loss=2.343, mask_ins=0.734, word_ins_ml=3.988, word_reposition=1.183, kpe=0.843, ppl=107.42, wps=5525.3, ups=0.27, wpb=20393.2, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=1.287, clip=0, loss_scale=8192, train_wall=235, wall=43352
2022-07-26 04:53:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-26 04:53:46 | INFO | train_inner | epoch 014:    133 / 1122 loss=6.629, nll_loss=2.258, mask_ins=0.729, word_ins_ml=3.914, word_reposition=1.164, kpe=0.822, ppl=98.96, wps=7370.4, ups=0.36, wpb=20518.6, bsz=256, num_updates=14700, lr=0.000291606, gnorm=1.231, clip=0, loss_scale=7908, train_wall=239, wall=43630
2022-07-26 04:58:54 | INFO | train_inner | epoch 014:    233 / 1122 loss=6.612, nll_loss=2.254, mask_ins=0.724, word_ins_ml=3.909, word_reposition=1.155, kpe=0.823, ppl=97.8, wps=6671.7, ups=0.32, wpb=20546.5, bsz=256, num_updates=14800, lr=0.000290619, gnorm=1.24, clip=0, loss_scale=4096, train_wall=269, wall=43938
2022-07-26 05:03:42 | INFO | train_inner | epoch 014:    333 / 1122 loss=nan, nll_loss=2.269, mask_ins=0.73, word_ins_ml=3.922, word_reposition=1.182, kpe=nan, ppl=nan, wps=7140.5, ups=0.35, wpb=20556.6, bsz=256, num_updates=14900, lr=0.000289642, gnorm=1.27, clip=0, loss_scale=4096, train_wall=248, wall=44226
2022-07-26 05:08:17 | INFO | train_inner | epoch 014:    433 / 1122 loss=6.683, nll_loss=2.31, mask_ins=0.725, word_ins_ml=3.959, word_reposition=1.173, kpe=0.825, ppl=102.75, wps=7454.1, ups=0.36, wpb=20535.9, bsz=256, num_updates=15000, lr=0.000288675, gnorm=1.23, clip=0, loss_scale=4096, train_wall=236, wall=44502
2022-07-26 05:12:54 | INFO | train_inner | epoch 014:    533 / 1122 loss=6.589, nll_loss=2.244, mask_ins=0.718, word_ins_ml=3.9, word_reposition=1.152, kpe=0.819, ppl=96.29, wps=7400.1, ups=0.36, wpb=20488.9, bsz=256, num_updates=15100, lr=0.000287718, gnorm=1.26, clip=0, loss_scale=4096, train_wall=237, wall=44779
2022-07-26 05:17:30 | INFO | train_inner | epoch 014:    633 / 1122 loss=6.655, nll_loss=2.286, mask_ins=0.726, word_ins_ml=3.938, word_reposition=1.163, kpe=0.827, ppl=100.78, wps=7426.5, ups=0.36, wpb=20496.4, bsz=256, num_updates=15200, lr=0.00028677, gnorm=1.279, clip=0, loss_scale=4096, train_wall=236, wall=45055
2022-07-26 05:22:05 | INFO | train_inner | epoch 014:    733 / 1122 loss=6.698, nll_loss=2.311, mask_ins=0.731, word_ins_ml=3.959, word_reposition=1.18, kpe=0.828, ppl=103.86, wps=7515.2, ups=0.36, wpb=20652.7, bsz=256, num_updates=15300, lr=0.000285831, gnorm=1.262, clip=0, loss_scale=7987, train_wall=235, wall=45330
2022-07-26 05:26:41 | INFO | train_inner | epoch 014:    833 / 1122 loss=6.737, nll_loss=2.347, mask_ins=0.735, word_ins_ml=3.991, word_reposition=1.179, kpe=0.832, ppl=106.68, wps=7397.6, ups=0.36, wpb=20413.6, bsz=256, num_updates=15400, lr=0.000284901, gnorm=1.239, clip=0, loss_scale=8192, train_wall=236, wall=45605
2022-07-26 05:31:16 | INFO | train_inner | epoch 014:    933 / 1122 loss=6.701, nll_loss=2.312, mask_ins=0.73, word_ins_ml=3.96, word_reposition=1.176, kpe=0.834, ppl=104.04, wps=7449.7, ups=0.36, wpb=20508.8, bsz=256, num_updates=15500, lr=0.000283981, gnorm=1.249, clip=0, loss_scale=8192, train_wall=236, wall=45881
2022-07-26 05:35:51 | INFO | train_inner | epoch 014:   1033 / 1122 loss=6.705, nll_loss=2.304, mask_ins=0.733, word_ins_ml=3.954, word_reposition=1.187, kpe=0.832, ppl=104.33, wps=7477.7, ups=0.36, wpb=20538.5, bsz=256, num_updates=15600, lr=0.000283069, gnorm=1.254, clip=0, loss_scale=8192, train_wall=235, wall=46155
2022-07-26 05:39:55 | INFO | train | epoch 014 | loss nan | nll_loss 2.292 | mask_ins 0.728 | word_ins_ml 3.943 | word_reposition 1.17 | kpe nan | ppl nan | wps 7119.8 | ups 0.35 | wpb 20522 | bsz 255.8 | num_updates 15689 | lr 0.000282265 | gnorm 1.259 | clip 0 | loss_scale 6323 | train_wall 2694 | wall 46400
2022-07-26 05:41:11 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 13.798 | nll_loss 6.839 | mask_ins 1.56 | word_ins_ml 8.131 | word_reposition 2.681 | kpe 1.427 | ppl 14243.5 | wps 13098.6 | wpb 2367.6 | bsz 32 | num_updates 15689 | best_loss 13.327
2022-07-26 05:41:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 14 @ 15689 updates, score 13.798) (writing took 13.41109758708626 seconds)
2022-07-26 05:41:55 | INFO | train_inner | epoch 015:     11 / 1122 loss=nan, nll_loss=2.319, mask_ins=0.733, word_ins_ml=3.966, word_reposition=1.171, kpe=nan, ppl=nan, wps=5621.9, ups=0.27, wpb=20457.8, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=1.337, clip=0, loss_scale=8192, train_wall=236, wall=46519
2022-07-26 05:43:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-26 05:46:33 | INFO | train_inner | epoch 015:    112 / 1122 loss=6.56, nll_loss=2.234, mask_ins=0.719, word_ins_ml=3.892, word_reposition=1.14, kpe=0.81, ppl=94.33, wps=7423.7, ups=0.36, wpb=20646.1, bsz=256, num_updates=15800, lr=0.000281272, gnorm=1.225, clip=0, loss_scale=8760, train_wall=238, wall=46797
2022-07-26 05:51:13 | INFO | train_inner | epoch 015:    212 / 1122 loss=6.598, nll_loss=2.245, mask_ins=0.724, word_ins_ml=3.902, word_reposition=1.166, kpe=0.806, ppl=96.84, wps=7332, ups=0.36, wpb=20581.2, bsz=256, num_updates=15900, lr=0.000280386, gnorm=1.255, clip=0, loss_scale=8192, train_wall=242, wall=47078
2022-07-26 05:56:40 | INFO | train_inner | epoch 015:    312 / 1122 loss=nan, nll_loss=2.209, mask_ins=0.718, word_ins_ml=3.869, word_reposition=1.141, kpe=nan, ppl=nan, wps=6287.3, ups=0.31, wpb=20538.6, bsz=256, num_updates=16000, lr=0.000279508, gnorm=1.281, clip=0, loss_scale=8192, train_wall=287, wall=47405
2022-07-26 06:02:08 | INFO | train_inner | epoch 015:    412 / 1122 loss=6.57, nll_loss=2.231, mask_ins=0.721, word_ins_ml=3.888, word_reposition=1.147, kpe=0.814, ppl=95.01, wps=6261.5, ups=0.3, wpb=20532.3, bsz=256, num_updates=16100, lr=0.000278639, gnorm=1.252, clip=0, loss_scale=8192, train_wall=287, wall=47733
2022-07-26 06:06:44 | INFO | train_inner | epoch 015:    512 / 1122 loss=6.581, nll_loss=2.25, mask_ins=0.721, word_ins_ml=3.906, word_reposition=1.137, kpe=0.817, ppl=95.71, wps=7420.4, ups=0.36, wpb=20478.8, bsz=256, num_updates=16200, lr=0.000277778, gnorm=1.272, clip=0, loss_scale=8192, train_wall=236, wall=48009
2022-07-26 06:08:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-26 06:11:22 | INFO | train_inner | epoch 015:    613 / 1122 loss=nan, nll_loss=2.258, mask_ins=0.726, word_ins_ml=3.912, word_reposition=1.156, kpe=nan, ppl=nan, wps=7390.5, ups=0.36, wpb=20529.2, bsz=256, num_updates=16300, lr=0.000276924, gnorm=1.262, clip=0, loss_scale=8273, train_wall=238, wall=48287
2022-07-26 06:15:58 | INFO | train_inner | epoch 015:    713 / 1122 loss=6.602, nll_loss=2.247, mask_ins=0.726, word_ins_ml=3.903, word_reposition=1.158, kpe=0.816, ppl=97.12, wps=7413.7, ups=0.36, wpb=20477.7, bsz=256, num_updates=16400, lr=0.000276079, gnorm=1.283, clip=0, loss_scale=8192, train_wall=237, wall=48563
2022-07-26 06:20:33 | INFO | train_inner | epoch 015:    813 / 1122 loss=6.601, nll_loss=2.256, mask_ins=0.721, word_ins_ml=3.91, word_reposition=1.148, kpe=0.822, ppl=97.08, wps=7459.6, ups=0.36, wpb=20537.8, bsz=256, num_updates=16500, lr=0.000275241, gnorm=1.246, clip=0, loss_scale=8192, train_wall=236, wall=48838
2022-07-26 06:25:10 | INFO | train_inner | epoch 015:    913 / 1122 loss=6.626, nll_loss=2.264, mask_ins=0.73, word_ins_ml=3.918, word_reposition=1.162, kpe=0.816, ppl=98.78, wps=7420.1, ups=0.36, wpb=20492.6, bsz=256, num_updates=16600, lr=0.000274411, gnorm=1.27, clip=0, loss_scale=8192, train_wall=236, wall=49114
2022-07-26 06:29:45 | INFO | train_inner | epoch 015:   1013 / 1122 loss=6.619, nll_loss=2.257, mask_ins=0.726, word_ins_ml=3.911, word_reposition=1.163, kpe=0.819, ppl=98.26, wps=7436.1, ups=0.36, wpb=20505, bsz=256, num_updates=16700, lr=0.000273588, gnorm=1.269, clip=0, loss_scale=8192, train_wall=236, wall=49390
2022-07-26 06:34:21 | INFO | train_inner | epoch 015:   1113 / 1122 loss=6.637, nll_loss=2.273, mask_ins=0.725, word_ins_ml=3.925, word_reposition=1.169, kpe=0.818, ppl=99.5, wps=7470.1, ups=0.36, wpb=20621.4, bsz=256, num_updates=16800, lr=0.000272772, gnorm=1.279, clip=0, loss_scale=12534, train_wall=236, wall=49666
2022-07-26 06:34:45 | INFO | train | epoch 015 | loss nan | nll_loss 2.247 | mask_ins 0.723 | word_ins_ml 3.903 | word_reposition 1.153 | kpe nan | ppl nan | wps 6986.4 | ups 0.34 | wpb 20519.8 | bsz 255.8 | num_updates 16809 | lr 0.000272699 | gnorm 1.267 | clip 0 | loss_scale 8703 | train_wall 2756 | wall 49690
2022-07-26 06:36:01 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 13.846 | nll_loss 6.868 | mask_ins 1.576 | word_ins_ml 8.165 | word_reposition 2.644 | kpe 1.461 | ppl 14720.9 | wps 13081.6 | wpb 2367.6 | bsz 32 | num_updates 16809 | best_loss 13.327
2022-07-26 06:36:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 15 @ 16809 updates, score 13.846) (writing took 16.86883298587054 seconds)
2022-07-26 06:37:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-26 06:40:31 | INFO | train_inner | epoch 016:     92 / 1122 loss=nan, nll_loss=2.174, mask_ins=0.717, word_ins_ml=3.839, word_reposition=1.125, kpe=nan, ppl=nan, wps=5486, ups=0.27, wpb=20272.7, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=1.328, clip=0, loss_scale=10220, train_wall=237, wall=50036
2022-07-26 06:45:08 | INFO | train_inner | epoch 016:    192 / 1122 loss=6.528, nll_loss=2.207, mask_ins=0.723, word_ins_ml=3.867, word_reposition=1.139, kpe=0.798, ppl=92.26, wps=7396.9, ups=0.36, wpb=20486.3, bsz=256, num_updates=17000, lr=0.000271163, gnorm=1.277, clip=0, loss_scale=8192, train_wall=237, wall=50313
2022-07-26 06:49:43 | INFO | train_inner | epoch 016:    292 / 1122 loss=6.478, nll_loss=2.174, mask_ins=0.714, word_ins_ml=3.839, word_reposition=1.127, kpe=0.798, ppl=89.15, wps=7514.1, ups=0.36, wpb=20648.8, bsz=256, num_updates=17100, lr=0.000270369, gnorm=1.276, clip=0, loss_scale=8192, train_wall=235, wall=50587
2022-07-26 06:51:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-26 06:54:21 | INFO | train_inner | epoch 016:    393 / 1122 loss=6.516, nll_loss=2.203, mask_ins=0.716, word_ins_ml=3.864, word_reposition=1.134, kpe=0.802, ppl=91.52, wps=7355.6, ups=0.36, wpb=20436.1, bsz=256, num_updates=17200, lr=0.000269582, gnorm=1.271, clip=0, loss_scale=5353, train_wall=238, wall=50865
2022-07-26 06:58:56 | INFO | train_inner | epoch 016:    493 / 1122 loss=6.502, nll_loss=2.19, mask_ins=0.716, word_ins_ml=3.852, word_reposition=1.13, kpe=0.804, ppl=90.62, wps=7489.8, ups=0.36, wpb=20622.6, bsz=256, num_updates=17300, lr=0.000268802, gnorm=1.242, clip=0, loss_scale=4096, train_wall=236, wall=51141
2022-07-26 07:04:19 | INFO | train_inner | epoch 016:    593 / 1122 loss=6.547, nll_loss=2.212, mask_ins=0.721, word_ins_ml=3.871, word_reposition=1.15, kpe=0.804, ppl=93.48, wps=6365.7, ups=0.31, wpb=20549.5, bsz=256, num_updates=17400, lr=0.000268028, gnorm=1.269, clip=0, loss_scale=4096, train_wall=283, wall=51463
2022-07-26 07:08:55 | INFO | train_inner | epoch 016:    693 / 1122 loss=6.497, nll_loss=2.181, mask_ins=0.715, word_ins_ml=3.844, word_reposition=1.133, kpe=0.806, ppl=90.35, wps=7456.9, ups=0.36, wpb=20586, bsz=256, num_updates=17500, lr=0.000267261, gnorm=1.258, clip=0, loss_scale=4096, train_wall=236, wall=51739
2022-07-26 07:13:31 | INFO | train_inner | epoch 016:    793 / 1122 loss=6.53, nll_loss=2.204, mask_ins=0.721, word_ins_ml=3.864, word_reposition=1.137, kpe=0.807, ppl=92.4, wps=7430.1, ups=0.36, wpb=20518.4, bsz=256, num_updates=17600, lr=0.000266501, gnorm=1.277, clip=0, loss_scale=4096, train_wall=236, wall=52016
2022-07-26 07:18:06 | INFO | train_inner | epoch 016:    893 / 1122 loss=6.581, nll_loss=2.245, mask_ins=0.721, word_ins_ml=3.9, word_reposition=1.151, kpe=0.81, ppl=95.76, wps=7464, ups=0.36, wpb=20568.2, bsz=256, num_updates=17700, lr=0.000265747, gnorm=1.318, clip=0, loss_scale=6472, train_wall=236, wall=52291
2022-07-26 07:22:43 | INFO | train_inner | epoch 016:    993 / 1122 loss=6.543, nll_loss=2.221, mask_ins=0.716, word_ins_ml=3.879, word_reposition=1.138, kpe=0.81, ppl=93.26, wps=7419.6, ups=0.36, wpb=20510.4, bsz=256, num_updates=17800, lr=0.000264999, gnorm=1.302, clip=0, loss_scale=8192, train_wall=236, wall=52568
2022-07-26 07:24:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-26 07:27:21 | INFO | train_inner | epoch 016:   1094 / 1122 loss=nan, nll_loss=2.221, mask_ins=0.717, word_ins_ml=3.879, word_reposition=1.139, kpe=nan, ppl=nan, wps=7363.2, ups=0.36, wpb=20508.9, bsz=256, num_updates=17900, lr=0.000264258, gnorm=1.295, clip=0, loss_scale=5515, train_wall=238, wall=52846
2022-07-26 07:28:37 | INFO | train | epoch 016 | loss nan | nll_loss 2.205 | mask_ins 0.718 | word_ins_ml 3.865 | word_reposition 1.137 | kpe nan | ppl nan | wps 7103.5 | ups 0.35 | wpb 20519.8 | bsz 255.8 | num_updates 17928 | lr 0.000264052 | gnorm 1.283 | clip 0 | loss_scale 6097 | train_wall 2695 | wall 52922
2022-07-26 07:29:53 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 13.606 | nll_loss 6.713 | mask_ins 1.562 | word_ins_ml 8.004 | word_reposition 2.591 | kpe 1.449 | ppl 12465.5 | wps 13140.5 | wpb 2367.6 | bsz 32 | num_updates 17928 | best_loss 13.327
2022-07-26 07:30:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 16 @ 17928 updates, score 13.606) (writing took 12.168343643657863 seconds)
2022-07-26 07:33:23 | INFO | train_inner | epoch 017:     72 / 1122 loss=6.477, nll_loss=2.174, mask_ins=0.716, word_ins_ml=3.838, word_reposition=1.131, kpe=0.792, ppl=89.08, wps=5651.4, ups=0.28, wpb=20446.3, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=1.353, clip=0, loss_scale=4096, train_wall=235, wall=53208
2022-07-26 07:38:18 | INFO | train_inner | epoch 017:    172 / 1122 loss=6.439, nll_loss=2.165, mask_ins=0.711, word_ins_ml=3.83, word_reposition=1.113, kpe=0.785, ppl=86.74, wps=6951.3, ups=0.34, wpb=20498.3, bsz=256, num_updates=18100, lr=0.000262794, gnorm=1.267, clip=0, loss_scale=4096, train_wall=248, wall=53503
2022-07-26 07:43:02 | INFO | train_inner | epoch 017:    272 / 1122 loss=6.44, nll_loss=2.144, mask_ins=0.715, word_ins_ml=3.811, word_reposition=1.128, kpe=0.786, ppl=86.84, wps=7226.7, ups=0.35, wpb=20482.1, bsz=256, num_updates=18200, lr=0.000262071, gnorm=1.274, clip=0, loss_scale=4096, train_wall=241, wall=53786
2022-07-26 07:47:37 | INFO | train_inner | epoch 017:    372 / 1122 loss=6.476, nll_loss=2.165, mask_ins=0.715, word_ins_ml=3.83, word_reposition=1.139, kpe=0.793, ppl=89.04, wps=7422.5, ups=0.36, wpb=20436.9, bsz=256, num_updates=18300, lr=0.000261354, gnorm=1.299, clip=0, loss_scale=4096, train_wall=236, wall=54062
2022-07-26 07:50:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-26 07:52:15 | INFO | train_inner | epoch 017:    473 / 1122 loss=6.471, nll_loss=2.179, mask_ins=0.716, word_ins_ml=3.842, word_reposition=1.124, kpe=0.789, ppl=88.72, wps=7366.8, ups=0.36, wpb=20473.3, bsz=256, num_updates=18400, lr=0.000260643, gnorm=1.288, clip=0, loss_scale=5029, train_wall=238, wall=54340
2022-07-26 07:56:50 | INFO | train_inner | epoch 017:    573 / 1122 loss=6.447, nll_loss=2.166, mask_ins=0.711, word_ins_ml=3.83, word_reposition=1.115, kpe=0.79, ppl=87.22, wps=7461.3, ups=0.36, wpb=20500.5, bsz=256, num_updates=18500, lr=0.000259938, gnorm=1.291, clip=0, loss_scale=4096, train_wall=235, wall=54614
2022-07-26 08:01:25 | INFO | train_inner | epoch 017:    673 / 1122 loss=6.422, nll_loss=2.145, mask_ins=0.707, word_ins_ml=3.812, word_reposition=1.107, kpe=0.796, ppl=85.74, wps=7476.1, ups=0.36, wpb=20622.9, bsz=256, num_updates=18600, lr=0.000259238, gnorm=1.307, clip=0, loss_scale=4096, train_wall=236, wall=54890
2022-07-26 08:06:46 | INFO | train_inner | epoch 017:    773 / 1122 loss=6.443, nll_loss=2.172, mask_ins=0.708, word_ins_ml=3.836, word_reposition=1.106, kpe=0.794, ppl=86.99, wps=6478.4, ups=0.31, wpb=20743.5, bsz=256, num_updates=18700, lr=0.000258544, gnorm=1.303, clip=0, loss_scale=4096, train_wall=281, wall=55210
2022-07-26 08:11:21 | INFO | train_inner | epoch 017:    873 / 1122 loss=6.487, nll_loss=2.196, mask_ins=0.713, word_ins_ml=3.857, word_reposition=1.121, kpe=0.796, ppl=89.67, wps=7439.3, ups=0.36, wpb=20464.5, bsz=256, num_updates=18800, lr=0.000257855, gnorm=1.321, clip=0, loss_scale=4096, train_wall=235, wall=55485
2022-07-26 08:15:56 | INFO | train_inner | epoch 017:    973 / 1122 loss=6.486, nll_loss=2.192, mask_ins=0.71, word_ins_ml=3.853, word_reposition=1.126, kpe=0.797, ppl=89.66, wps=7448.7, ups=0.36, wpb=20488.1, bsz=256, num_updates=18900, lr=0.000257172, gnorm=1.296, clip=0, loss_scale=4915, train_wall=236, wall=55761
2022-07-26 08:20:30 | INFO | train_inner | epoch 017:   1073 / 1122 loss=nan, nll_loss=2.183, mask_ins=0.715, word_ins_ml=3.845, word_reposition=1.144, kpe=nan, ppl=nan, wps=7492.6, ups=0.36, wpb=20544.9, bsz=256, num_updates=19000, lr=0.000256495, gnorm=1.297, clip=0, loss_scale=8192, train_wall=235, wall=56035
2022-07-26 08:22:43 | INFO | train | epoch 017 | loss nan | nll_loss 2.167 | mask_ins 0.712 | word_ins_ml 3.831 | word_reposition 1.122 | kpe nan | ppl nan | wps 7086.4 | ups 0.35 | wpb 20519.2 | bsz 255.8 | num_updates 19049 | lr 0.000256164 | gnorm 1.3 | clip 0 | loss_scale 4797 | train_wall 2706 | wall 56168
2022-07-26 08:23:59 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 13.891 | nll_loss 6.793 | mask_ins 1.57 | word_ins_ml 8.09 | word_reposition 2.681 | kpe 1.55 | ppl 15186.5 | wps 13092.9 | wpb 2367.6 | bsz 32 | num_updates 19049 | best_loss 13.327
2022-07-26 08:24:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 17 @ 19049 updates, score 13.891) (writing took 16.181842871941626 seconds)
2022-07-26 08:26:35 | INFO | train_inner | epoch 018:     51 / 1122 loss=nan, nll_loss=2.12, mask_ins=0.708, word_ins_ml=3.79, word_reposition=1.102, kpe=nan, ppl=nan, wps=5570, ups=0.27, wpb=20316.4, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=1.338, clip=0, loss_scale=8192, train_wall=234, wall=56399
2022-07-26 08:31:09 | INFO | train_inner | epoch 018:    151 / 1122 loss=6.357, nll_loss=2.118, mask_ins=0.702, word_ins_ml=3.788, word_reposition=1.095, kpe=0.772, ppl=81.99, wps=7495.8, ups=0.36, wpb=20556.5, bsz=256, num_updates=19200, lr=0.000255155, gnorm=1.306, clip=0, loss_scale=8192, train_wall=235, wall=56674
2022-07-26 08:35:44 | INFO | train_inner | epoch 018:    251 / 1122 loss=6.354, nll_loss=2.101, mask_ins=0.706, word_ins_ml=3.772, word_reposition=1.1, kpe=0.775, ppl=81.81, wps=7488, ups=0.36, wpb=20582.3, bsz=256, num_updates=19300, lr=0.000254493, gnorm=1.311, clip=0, loss_scale=8192, train_wall=236, wall=56949
2022-07-26 08:40:19 | INFO | train_inner | epoch 018:    351 / 1122 loss=6.399, nll_loss=2.135, mask_ins=0.71, word_ins_ml=3.803, word_reposition=1.108, kpe=0.777, ppl=84.37, wps=7443.5, ups=0.36, wpb=20476.3, bsz=256, num_updates=19400, lr=0.000253837, gnorm=1.292, clip=0, loss_scale=8847, train_wall=236, wall=57224
2022-07-26 08:41:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-26 08:44:57 | INFO | train_inner | epoch 018:    452 / 1122 loss=6.396, nll_loss=2.129, mask_ins=0.708, word_ins_ml=3.797, word_reposition=1.114, kpe=0.778, ppl=84.23, wps=7407.3, ups=0.36, wpb=20573.8, bsz=256, num_updates=19500, lr=0.000253185, gnorm=1.316, clip=0, loss_scale=9409, train_wall=238, wall=57501
2022-07-26 08:49:31 | INFO | train_inner | epoch 018:    552 / 1122 loss=6.387, nll_loss=2.13, mask_ins=0.708, word_ins_ml=3.798, word_reposition=1.099, kpe=0.783, ppl=83.68, wps=7445.9, ups=0.36, wpb=20442.1, bsz=256, num_updates=19600, lr=0.000252538, gnorm=1.328, clip=0, loss_scale=8192, train_wall=235, wall=57776
2022-07-26 08:54:06 | INFO | train_inner | epoch 018:    652 / 1122 loss=6.384, nll_loss=2.143, mask_ins=0.703, word_ins_ml=3.809, word_reposition=1.089, kpe=0.783, ppl=83.54, wps=7440.6, ups=0.36, wpb=20443.7, bsz=256, num_updates=19700, lr=0.000251896, gnorm=1.302, clip=0, loss_scale=8192, train_wall=235, wall=58051
2022-07-26 08:58:41 | INFO | train_inner | epoch 018:    752 / 1122 loss=nan, nll_loss=2.143, mask_ins=0.707, word_ins_ml=3.809, word_reposition=1.111, kpe=nan, ppl=nan, wps=7496.9, ups=0.36, wpb=20637.2, bsz=256, num_updates=19800, lr=0.000251259, gnorm=1.319, clip=0, loss_scale=8192, train_wall=236, wall=58326
2022-07-26 09:03:17 | INFO | train_inner | epoch 018:    852 / 1122 loss=6.403, nll_loss=2.132, mask_ins=0.708, word_ins_ml=3.8, word_reposition=1.112, kpe=0.783, ppl=84.61, wps=7501.8, ups=0.36, wpb=20693, bsz=256, num_updates=19900, lr=0.000250627, gnorm=1.307, clip=0, loss_scale=8192, train_wall=236, wall=58602
2022-07-26 09:04:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-26 09:08:34 | INFO | train_inner | epoch 018:    953 / 1122 loss=6.397, nll_loss=2.125, mask_ins=0.709, word_ins_ml=3.794, word_reposition=1.108, kpe=0.786, ppl=84.27, wps=6464.1, ups=0.32, wpb=20465.8, bsz=256, num_updates=20000, lr=0.00025, gnorm=1.318, clip=0, loss_scale=9003, train_wall=276, wall=58919
2022-07-26 09:13:11 | INFO | train_inner | epoch 018:   1053 / 1122 loss=6.425, nll_loss=2.137, mask_ins=0.713, word_ins_ml=3.804, word_reposition=1.123, kpe=0.785, ppl=85.93, wps=7410.4, ups=0.36, wpb=20518.3, bsz=256, num_updates=20100, lr=0.000249377, gnorm=1.303, clip=0, loss_scale=8192, train_wall=238, wall=59195
2022-07-26 09:16:21 | INFO | train | epoch 018 | loss nan | nll_loss 2.129 | mask_ins 0.708 | word_ins_ml 3.798 | word_reposition 1.105 | kpe nan | ppl nan | wps 7143.6 | ups 0.35 | wpb 20521.5 | bsz 255.8 | num_updates 20169 | lr 0.00024895 | gnorm 1.314 | clip 0 | loss_scale 8433 | train_wall 2684 | wall 59386
2022-07-26 09:17:37 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 13.828 | nll_loss 6.833 | mask_ins 1.589 | word_ins_ml 8.122 | word_reposition 2.668 | kpe 1.449 | ppl 14546.7 | wps 13050.1 | wpb 2367.6 | bsz 32 | num_updates 20169 | best_loss 13.327
2022-07-26 09:17:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 18 @ 20169 updates, score 13.828) (writing took 16.230569902807474 seconds)
2022-07-26 09:19:18 | INFO | train_inner | epoch 019:     31 / 1122 loss=6.383, nll_loss=2.124, mask_ins=0.709, word_ins_ml=3.793, word_reposition=1.103, kpe=0.779, ppl=83.48, wps=5530.9, ups=0.27, wpb=20308.2, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=1.358, clip=0, loss_scale=8192, train_wall=236, wall=59563
2022-07-26 09:23:54 | INFO | train_inner | epoch 019:    131 / 1122 loss=6.314, nll_loss=2.076, mask_ins=0.705, word_ins_ml=3.75, word_reposition=1.1, kpe=0.759, ppl=79.55, wps=7356.3, ups=0.36, wpb=20331.6, bsz=256, num_updates=20300, lr=0.000248146, gnorm=1.31, clip=0, loss_scale=8192, train_wall=237, wall=59839
2022-07-26 09:28:34 | INFO | train_inner | epoch 019:    231 / 1122 loss=nan, nll_loss=2.093, mask_ins=0.705, word_ins_ml=3.765, word_reposition=1.105, kpe=nan, ppl=nan, wps=7341.6, ups=0.36, wpb=20543.5, bsz=256, num_updates=20400, lr=0.000247537, gnorm=1.301, clip=0, loss_scale=8192, train_wall=239, wall=60119
2022-07-26 09:30:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-26 09:33:14 | INFO | train_inner | epoch 019:    332 / 1122 loss=6.344, nll_loss=2.107, mask_ins=0.703, word_ins_ml=3.778, word_reposition=1.098, kpe=0.764, ppl=81.21, wps=7327.2, ups=0.36, wpb=20515.8, bsz=256, num_updates=20500, lr=0.000246932, gnorm=1.327, clip=0, loss_scale=8354, train_wall=240, wall=60399
2022-07-26 09:36:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-26 09:36:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-26 09:37:56 | INFO | train_inner | epoch 019:    434 / 1122 loss=6.349, nll_loss=2.112, mask_ins=0.707, word_ins_ml=3.783, word_reposition=1.092, kpe=0.767, ppl=81.49, wps=7222.1, ups=0.35, wpb=20390, bsz=256, num_updates=20600, lr=0.000246332, gnorm=1.359, clip=0, loss_scale=6003, train_wall=242, wall=60681
2022-07-26 09:42:32 | INFO | train_inner | epoch 019:    534 / 1122 loss=6.359, nll_loss=2.106, mask_ins=0.704, word_ins_ml=3.777, word_reposition=1.105, kpe=0.772, ppl=82.06, wps=7501.9, ups=0.36, wpb=20659.2, bsz=256, num_updates=20700, lr=0.000245737, gnorm=1.308, clip=0, loss_scale=2048, train_wall=236, wall=60957
2022-07-26 09:47:08 | INFO | train_inner | epoch 019:    634 / 1122 loss=6.331, nll_loss=2.102, mask_ins=0.7, word_ins_ml=3.773, word_reposition=1.091, kpe=0.768, ppl=80.53, wps=7476.8, ups=0.36, wpb=20619.6, bsz=256, num_updates=20800, lr=0.000245145, gnorm=1.319, clip=0, loss_scale=2048, train_wall=237, wall=61232
2022-07-26 09:51:43 | INFO | train_inner | epoch 019:    734 / 1122 loss=6.348, nll_loss=2.092, mask_ins=0.705, word_ins_ml=3.764, word_reposition=1.11, kpe=0.769, ppl=81.47, wps=7509.6, ups=0.36, wpb=20709.2, bsz=256, num_updates=20900, lr=0.000244558, gnorm=1.345, clip=0, loss_scale=2048, train_wall=236, wall=61508
2022-07-26 09:56:17 | INFO | train_inner | epoch 019:    834 / 1122 loss=6.367, nll_loss=2.101, mask_ins=0.706, word_ins_ml=3.772, word_reposition=1.107, kpe=0.781, ppl=82.52, wps=7502, ups=0.36, wpb=20558.9, bsz=256, num_updates=21000, lr=0.000243975, gnorm=1.39, clip=0, loss_scale=2048, train_wall=235, wall=61782
2022-07-26 10:00:52 | INFO | train_inner | epoch 019:    934 / 1122 loss=nan, nll_loss=2.107, mask_ins=0.703, word_ins_ml=3.777, word_reposition=1.097, kpe=nan, ppl=nan, wps=7451.7, ups=0.36, wpb=20432.8, bsz=256, num_updates=21100, lr=0.000243396, gnorm=1.352, clip=0, loss_scale=2396, train_wall=235, wall=62056
2022-07-26 10:05:27 | INFO | train_inner | epoch 019:   1034 / 1122 loss=6.353, nll_loss=2.092, mask_ins=0.706, word_ins_ml=3.764, word_reposition=1.107, kpe=0.776, ppl=81.77, wps=7494, ups=0.36, wpb=20620.4, bsz=256, num_updates=21200, lr=0.000242821, gnorm=1.354, clip=0, loss_scale=4096, train_wall=236, wall=62332
2022-07-26 10:10:14 | INFO | train | epoch 019 | loss nan | nll_loss 2.098 | mask_ins 0.705 | word_ins_ml 3.77 | word_reposition 1.101 | kpe nan | ppl nan | wps 7102.3 | ups 0.35 | wpb 20518.1 | bsz 255.8 | num_updates 21288 | lr 0.000242319 | gnorm 1.341 | clip 0 | loss_scale 4614 | train_wall 2698 | wall 62618
2022-07-26 10:11:29 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 13.802 | nll_loss 6.836 | mask_ins 1.553 | word_ins_ml 8.134 | word_reposition 2.628 | kpe 1.487 | ppl 14279.8 | wps 13088.7 | wpb 2367.6 | bsz 32 | num_updates 21288 | best_loss 13.327
2022-07-26 10:11:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 19 @ 21288 updates, score 13.802) (writing took 15.669541316106915 seconds)
2022-07-26 10:12:18 | INFO | train_inner | epoch 020:     12 / 1122 loss=6.346, nll_loss=2.096, mask_ins=0.705, word_ins_ml=3.768, word_reposition=1.101, kpe=0.773, ppl=81.37, wps=4970.1, ups=0.24, wpb=20413.8, bsz=253.8, num_updates=21300, lr=0.000242251, gnorm=1.382, clip=0, loss_scale=4096, train_wall=281, wall=62742
2022-07-26 10:16:52 | INFO | train_inner | epoch 020:    112 / 1122 loss=nan, nll_loss=2.056, mask_ins=0.699, word_ins_ml=3.733, word_reposition=1.087, kpe=nan, ppl=nan, wps=7467.5, ups=0.36, wpb=20531.2, bsz=256, num_updates=21400, lr=0.000241684, gnorm=1.33, clip=0, loss_scale=4096, train_wall=236, wall=63017
2022-07-26 10:21:27 | INFO | train_inner | epoch 020:    212 / 1122 loss=6.241, nll_loss=2.051, mask_ins=0.695, word_ins_ml=3.728, word_reposition=1.066, kpe=0.752, ppl=75.66, wps=7482.1, ups=0.36, wpb=20515.2, bsz=256, num_updates=21500, lr=0.000241121, gnorm=1.319, clip=0, loss_scale=4096, train_wall=235, wall=63291
2022-07-26 10:26:03 | INFO | train_inner | epoch 020:    312 / 1122 loss=6.27, nll_loss=2.067, mask_ins=0.698, word_ins_ml=3.742, word_reposition=1.076, kpe=0.754, ppl=77.15, wps=7454.6, ups=0.36, wpb=20617.9, bsz=256, num_updates=21600, lr=0.000240563, gnorm=1.342, clip=0, loss_scale=4301, train_wall=237, wall=63568
2022-07-26 10:30:42 | INFO | train_inner | epoch 020:    412 / 1122 loss=nan, nll_loss=2.078, mask_ins=0.703, word_ins_ml=3.752, word_reposition=1.1, kpe=nan, ppl=nan, wps=7389.4, ups=0.36, wpb=20584.1, bsz=256, num_updates=21700, lr=0.000240008, gnorm=1.346, clip=0, loss_scale=8192, train_wall=238, wall=63847
2022-07-26 10:35:20 | INFO | train_inner | epoch 020:    512 / 1122 loss=6.294, nll_loss=2.067, mask_ins=0.702, word_ins_ml=3.742, word_reposition=1.091, kpe=0.758, ppl=78.45, wps=7378.3, ups=0.36, wpb=20491.9, bsz=256, num_updates=21800, lr=0.000239457, gnorm=1.355, clip=0, loss_scale=8192, train_wall=238, wall=64124
2022-07-26 10:39:57 | INFO | train_inner | epoch 020:    612 / 1122 loss=6.291, nll_loss=2.066, mask_ins=0.7, word_ins_ml=3.741, word_reposition=1.089, kpe=0.76, ppl=78.28, wps=7437.1, ups=0.36, wpb=20615.6, bsz=256, num_updates=21900, lr=0.000238909, gnorm=1.348, clip=0, loss_scale=8192, train_wall=237, wall=64401
2022-07-26 10:44:32 | INFO | train_inner | epoch 020:    712 / 1122 loss=6.268, nll_loss=2.049, mask_ins=0.697, word_ins_ml=3.726, word_reposition=1.081, kpe=0.763, ppl=77.04, wps=7402.3, ups=0.36, wpb=20391.8, bsz=256, num_updates=22000, lr=0.000238366, gnorm=1.377, clip=0, loss_scale=8192, train_wall=236, wall=64677
2022-07-26 10:49:09 | INFO | train_inner | epoch 020:    812 / 1122 loss=6.28, nll_loss=2.054, mask_ins=0.698, word_ins_ml=3.73, word_reposition=1.091, kpe=0.76, ppl=77.71, wps=7415.4, ups=0.36, wpb=20495.4, bsz=256, num_updates=22100, lr=0.000237826, gnorm=1.345, clip=0, loss_scale=8192, train_wall=237, wall=64953
2022-07-26 10:50:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-26 10:53:48 | INFO | train_inner | epoch 020:    913 / 1122 loss=6.345, nll_loss=2.105, mask_ins=0.704, word_ins_ml=3.775, word_reposition=1.103, kpe=0.763, ppl=81.3, wps=7380, ups=0.36, wpb=20596.2, bsz=256, num_updates=22200, lr=0.000237289, gnorm=1.351, clip=0, loss_scale=9246, train_wall=239, wall=65232
2022-07-26 10:58:24 | INFO | train_inner | epoch 020:   1013 / 1122 loss=6.295, nll_loss=2.068, mask_ins=0.701, word_ins_ml=3.742, word_reposition=1.089, kpe=0.763, ppl=78.51, wps=7436.9, ups=0.36, wpb=20556.6, bsz=256, num_updates=22300, lr=0.000236757, gnorm=1.371, clip=0, loss_scale=8192, train_wall=237, wall=65509
2022-07-26 11:02:59 | INFO | train_inner | epoch 020:   1113 / 1122 loss=6.373, nll_loss=2.11, mask_ins=0.711, word_ins_ml=3.78, word_reposition=1.115, kpe=0.767, ppl=82.86, wps=7441.6, ups=0.36, wpb=20473.6, bsz=256, num_updates=22400, lr=0.000236228, gnorm=1.368, clip=0, loss_scale=8192, train_wall=236, wall=65784
2022-07-26 11:03:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-26 11:03:23 | INFO | train | epoch 020 | loss nan | nll_loss 2.07 | mask_ins 0.701 | word_ins_ml 3.744 | word_reposition 1.09 | kpe nan | ppl nan | wps 7205.8 | ups 0.35 | wpb 20521.2 | bsz 255.8 | num_updates 22408 | lr 0.000236186 | gnorm 1.353 | clip 0 | loss_scale 7159 | train_wall 2657 | wall 65808
2022-07-26 11:04:39 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 13.883 | nll_loss 6.814 | mask_ins 1.587 | word_ins_ml 8.108 | word_reposition 2.7 | kpe 1.487 | ppl 15104 | wps 13084.4 | wpb 2367.6 | bsz 32 | num_updates 22408 | best_loss 13.327
2022-07-26 11:04:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_withoutnoise_cased/checkpoint_last.pt (epoch 20 @ 22408 updates, score 13.883) (writing took 11.023476622067392 seconds)
2022-07-26 11:09:03 | INFO | train_inner | epoch 021:     92 / 1122 loss=6.19, nll_loss=2.016, mask_ins=0.692, word_ins_ml=3.698, word_reposition=1.063, kpe=0.737, ppl=73, wps=5636.2, ups=0.28, wpb=20480.4, bsz=253.8, num_updates=22500, lr=0.000235702, gnorm=1.374, clip=0, loss_scale=4380, train_wall=237, wall=66147
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
train.sh: line 43: nstraint: command not found
