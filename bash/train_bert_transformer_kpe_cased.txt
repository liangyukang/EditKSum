nohup: ignoring input
2022-07-09 16:01:15 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:16293
2022-07-09 16:01:15 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:16293
2022-07-09 16:01:16 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:16293
2022-07-09 16:01:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-09 16:01:16 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:16293
2022-07-09 16:01:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-09 16:01:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-09 16:01:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-09 16:01:16 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-09 16:01:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-09 16:01:16 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-09 16:01:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-09 16:01:16 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-09 16:01:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-09 16:01:16 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-09 16:01:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-09 16:01:20 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:16293', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_transformer_kpe_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='6,6,6', layers_num='6,6,6', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='/data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, no_share_maskpredictor=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert', decoder='transformer', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-09 16:01:20 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-09 16:01:20 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
start load cached examples valid ...
0it [00:00, ?it/s]2022-07-09 16:01:20 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.source
2022-07-09 16:01:20 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.target
2022-07-09 16:01:20 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]383it [00:00, 3827.92it/s]382it [00:00, 3817.28it/s]369it [00:00, 3678.06it/s]374it [00:00, 3735.80it/s]766it [00:00, 3456.37it/s]737it [00:00, 3422.00it/s]764it [00:00, 3426.81it/s]748it [00:00, 3446.32it/s]1115it [00:00, 3447.21it/s]1099it [00:00, 3505.42it/s]1121it [00:00, 3484.71it/s]1095it [00:00, 3450.64it/s]1461it [00:00, 3325.26it/s]1451it [00:00, 3304.01it/s]1441it [00:00, 3317.25it/s]1472it [00:00, 3312.20it/s]1843it [00:00, 3494.38it/s]1838it [00:00, 3493.86it/s]1829it [00:00, 3510.26it/s]1852it [00:00, 3476.93it/s]2230it [00:00, 3617.72it/s]2225it [00:00, 3615.37it/s]2201it [00:00, 3578.79it/s]2218it [00:00, 3362.32it/s]2594it [00:00, 3463.41it/s]2561it [00:00, 3515.02it/s]2589it [00:00, 3516.44it/s]2603it [00:00, 3509.77it/s]2989it [00:00, 3610.29it/s]2957it [00:00, 3651.08it/s]2965it [00:00, 3590.32it/s]2988it [00:00, 3610.36it/s]3324it [00:00, 3554.39it/s]3353it [00:00, 3503.34it/s]3326it [00:00, 3497.58it/s]3352it [00:00, 3472.23it/s]3708it [00:01, 3638.67it/s]3716it [00:01, 3540.18it/s]3707it [00:01, 3588.60it/s]3727it [00:01, 3551.64it/s]4072it [00:01, 3454.49it/s]4068it [00:01, 3494.87it/s]4073it [00:01, 3504.51it/s]4085it [00:01, 3440.22it/s]4449it [00:01, 3545.75it/s]4429it [00:01, 3527.79it/s]4453it [00:01, 3589.65it/s]4452it [00:01, 3505.04it/s]4805it [00:01, 3433.86it/s]4783it [00:01, 3421.73it/s]4814it [00:01, 3482.73it/s]4805it [00:01, 3375.19it/s]5165it [00:01, 3480.72it/s]5162it [00:01, 3527.30it/s]5193it [00:01, 3570.62it/s]5156it [00:01, 3411.71it/s]5531it [00:01, 3531.61it/s]5516it [00:01, 3527.73it/s]5566it [00:01, 3616.44it/s]5519it [00:01, 3474.09it/s]5886it [00:01, 3357.02it/s]5870it [00:01, 3369.90it/s]5929it [00:01, 3410.18it/s]5868it [00:01, 3297.00it/s]6222it [00:01, 3411.51it/s]6224it [00:01, 3358.64it/s]6285it [00:01, 3451.66it/s]6214it [00:01, 3343.08it/s]6562it [00:02, 2042.03it/s]6565it [00:02, 1989.75it/s]6633it [00:02, 2006.88it/s]6551it [00:02, 1933.74it/s]6894it [00:02, 2297.74it/s]6914it [00:02, 2280.72it/s]6985it [00:02, 2297.92it/s]6876it [00:02, 2186.65it/s]7241it [00:02, 2558.35it/s]7246it [00:02, 2506.39it/s]7335it [00:02, 2557.23it/s]7219it [00:02, 2453.04it/s]7546it [00:02, 2637.57it/s]7551it [00:02, 2609.44it/s]7648it [00:02, 2657.78it/s]7520it [00:02, 2541.43it/s]7892it [00:02, 2844.38it/s]7899it [00:02, 2827.88it/s]7997it [00:02, 2864.84it/s]7862it [00:02, 2759.06it/s]8206it [00:02, 2769.87it/s]8215it [00:02, 2851.32it/s]8318it [00:02, 2871.13it/s]8171it [00:02, 2737.02it/s]8554it [00:02, 2955.86it/s]8546it [00:02, 2974.45it/s]8670it [00:02, 3044.29it/s]8517it [00:02, 2927.08it/s]8901it [00:02, 3096.41it/s]8895it [00:02, 3117.18it/s]9023it [00:02, 3178.13it/s]8860it [00:02, 3063.20it/s]9224it [00:02, 3005.90it/s]9221it [00:02, 3030.84it/s]9355it [00:02, 3121.56it/s]9181it [00:03, 2959.28it/s]9572it [00:03, 3136.05it/s]9570it [00:03, 3157.54it/s]9710it [00:03, 3240.01it/s]9525it [00:03, 3090.85it/s]9894it [00:03, 3072.96it/s]9894it [00:03, 3094.60it/s]10042it [00:03, 3134.07it/s]9848it [00:03, 3020.11it/s]10240it [00:03, 3180.96it/s]10242it [00:03, 3202.34it/s]10394it [00:03, 3240.59it/s]10193it [00:03, 3139.54it/s]10571it [00:03, 3217.04it/s]10573it [00:03, 3231.45it/s]10723it [00:03, 3163.51it/s]10517it [00:03, 3167.54it/s]10896it [00:03, 3127.75it/s]10900it [00:03, 3146.44it/s]11072it [00:03, 3256.36it/s]10838it [00:03, 3065.55it/s]11241it [00:03, 3220.18it/s]11247it [00:03, 3237.68it/s]11409it [00:03, 3287.61it/s]11179it [00:03, 3162.68it/s]11566it [00:03, 3087.73it/s]11573it [00:03, 3108.10it/s]11740it [00:03, 3203.66it/s]11508it [00:03, 3196.99it/s]11912it [00:03, 3192.57it/s]11922it [00:03, 3215.10it/s]12093it [00:03, 3297.72it/s]11830it [00:03, 3085.69it/s]12261it [00:03, 3278.29it/s]12272it [00:03, 3295.89it/s]12425it [00:03, 3204.76it/s]12173it [00:03, 3183.80it/s]12591it [00:04, 3174.04it/s]12604it [00:04, 3192.47it/s]12778it [00:04, 3297.31it/s]12494it [00:04, 3085.77it/s]12929it [00:04, 3230.81it/s]12950it [00:04, 3267.40it/s]13112it [00:04, 3309.07it/s]12818it [00:04, 3129.77it/s]13368it [00:04, 3182.60it/s]
13254it [00:04, 3126.70it/s]13279it [00:04, 3169.51it/s]13368it [00:04, 3148.14it/s]
13368it [00:04, 3141.07it/s]
13158it [00:04, 3207.86it/s]13368it [00:04, 3083.44it/s]
2022-07-09 16:01:25 | INFO | root | success load 13368 data
2022-07-09 16:01:25 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-09 16:01:25 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-09 16:01:25 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-09 16:01:25 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-09 16:01:25 | INFO | transformer.tokenization_utils | loading file None
2022-07-09 16:01:25 | INFO | transformer.tokenization_utils | loading file None
2022-07-09 16:01:25 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
2022-07-09 16:01:25 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-09 16:01:25 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-09 16:01:25 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-09 16:01:27 | INFO | transformer.modeling_utils | Weights of BertEncoder not initialized from pretrained model: ['kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-09 16:01:27 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 676
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.encoder_attn.k_proj.weight', 'decoder.layers_msk.0.encoder_attn.k_proj.bias', 'decoder.layers_msk.0.encoder_attn.v_proj.weight', 'decoder.layers_msk.0.encoder_attn.v_proj.bias', 'decoder.layers_msk.0.encoder_attn.q_proj.weight', 'decoder.layers_msk.0.encoder_attn.q_proj.bias', 'decoder.layers_msk.0.encoder_attn.out_proj.weight', 'decoder.layers_msk.0.encoder_attn.out_proj.bias', 'decoder.layers_msk.0.encoder_attn_layer_norm.weight', 'decoder.layers_msk.0.encoder_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.encoder_attn.k_proj.weight', 'decoder.layers_msk.1.encoder_attn.k_proj.bias', 'decoder.layers_msk.1.encoder_attn.v_proj.weight', 'decoder.layers_msk.1.encoder_attn.v_proj.bias', 'decoder.layers_msk.1.encoder_attn.q_proj.weight', 'decoder.layers_msk.1.encoder_attn.q_proj.bias', 'decoder.layers_msk.1.encoder_attn.out_proj.weight', 'decoder.layers_msk.1.encoder_attn.out_proj.bias', 'decoder.layers_msk.1.encoder_attn_layer_norm.weight', 'decoder.layers_msk.1.encoder_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.encoder_attn.k_proj.weight', 'decoder.layers_msk.2.encoder_attn.k_proj.bias', 'decoder.layers_msk.2.encoder_attn.v_proj.weight', 'decoder.layers_msk.2.encoder_attn.v_proj.bias', 'decoder.layers_msk.2.encoder_attn.q_proj.weight', 'decoder.layers_msk.2.encoder_attn.q_proj.bias', 'decoder.layers_msk.2.encoder_attn.out_proj.weight', 'decoder.layers_msk.2.encoder_attn.out_proj.bias', 'decoder.layers_msk.2.encoder_attn_layer_norm.weight', 'decoder.layers_msk.2.encoder_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.encoder_attn.k_proj.weight', 'decoder.layers_msk.3.encoder_attn.k_proj.bias', 'decoder.layers_msk.3.encoder_attn.v_proj.weight', 'decoder.layers_msk.3.encoder_attn.v_proj.bias', 'decoder.layers_msk.3.encoder_attn.q_proj.weight', 'decoder.layers_msk.3.encoder_attn.q_proj.bias', 'decoder.layers_msk.3.encoder_attn.out_proj.weight', 'decoder.layers_msk.3.encoder_attn.out_proj.bias', 'decoder.layers_msk.3.encoder_attn_layer_norm.weight', 'decoder.layers_msk.3.encoder_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.encoder_attn.k_proj.weight', 'decoder.layers_msk.4.encoder_attn.k_proj.bias', 'decoder.layers_msk.4.encoder_attn.v_proj.weight', 'decoder.layers_msk.4.encoder_attn.v_proj.bias', 'decoder.layers_msk.4.encoder_attn.q_proj.weight', 'decoder.layers_msk.4.encoder_attn.q_proj.bias', 'decoder.layers_msk.4.encoder_attn.out_proj.weight', 'decoder.layers_msk.4.encoder_attn.out_proj.bias', 'decoder.layers_msk.4.encoder_attn_layer_norm.weight', 'decoder.layers_msk.4.encoder_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.encoder_attn.k_proj.weight', 'decoder.layers_msk.5.encoder_attn.k_proj.bias', 'decoder.layers_msk.5.encoder_attn.v_proj.weight', 'decoder.layers_msk.5.encoder_attn.v_proj.bias', 'decoder.layers_msk.5.encoder_attn.q_proj.weight', 'decoder.layers_msk.5.encoder_attn.q_proj.bias', 'decoder.layers_msk.5.encoder_attn.out_proj.weight', 'decoder.layers_msk.5.encoder_attn.out_proj.bias', 'decoder.layers_msk.5.encoder_attn_layer_norm.weight', 'decoder.layers_msk.5.encoder_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 496
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 676
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.encoder_attn.k_proj.weight', 'decoder.layers_msk.0.encoder_attn.k_proj.bias', 'decoder.layers_msk.0.encoder_attn.v_proj.weight', 'decoder.layers_msk.0.encoder_attn.v_proj.bias', 'decoder.layers_msk.0.encoder_attn.q_proj.weight', 'decoder.layers_msk.0.encoder_attn.q_proj.bias', 'decoder.layers_msk.0.encoder_attn.out_proj.weight', 'decoder.layers_msk.0.encoder_attn.out_proj.bias', 'decoder.layers_msk.0.encoder_attn_layer_norm.weight', 'decoder.layers_msk.0.encoder_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.encoder_attn.k_proj.weight', 'decoder.layers_msk.1.encoder_attn.k_proj.bias', 'decoder.layers_msk.1.encoder_attn.v_proj.weight', 'decoder.layers_msk.1.encoder_attn.v_proj.bias', 'decoder.layers_msk.1.encoder_attn.q_proj.weight', 'decoder.layers_msk.1.encoder_attn.q_proj.bias', 'decoder.layers_msk.1.encoder_attn.out_proj.weight', 'decoder.layers_msk.1.encoder_attn.out_proj.bias', 'decoder.layers_msk.1.encoder_attn_layer_norm.weight', 'decoder.layers_msk.1.encoder_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.encoder_attn.k_proj.weight', 'decoder.layers_msk.2.encoder_attn.k_proj.bias', 'decoder.layers_msk.2.encoder_attn.v_proj.weight', 'decoder.layers_msk.2.encoder_attn.v_proj.bias', 'decoder.layers_msk.2.encoder_attn.q_proj.weight', 'decoder.layers_msk.2.encoder_attn.q_proj.bias', 'decoder.layers_msk.2.encoder_attn.out_proj.weight', 'decoder.layers_msk.2.encoder_attn.out_proj.bias', 'decoder.layers_msk.2.encoder_attn_layer_norm.weight', 'decoder.layers_msk.2.encoder_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.encoder_attn.k_proj.weight', 'decoder.layers_msk.3.encoder_attn.k_proj.bias', 'decoder.layers_msk.3.encoder_attn.v_proj.weight', 'decoder.layers_msk.3.encoder_attn.v_proj.bias', 'decoder.layers_msk.3.encoder_attn.q_proj.weight', 'decoder.layers_msk.3.encoder_attn.q_proj.bias', 'decoder.layers_msk.3.encoder_attn.out_proj.weight', 'decoder.layers_msk.3.encoder_attn.out_proj.bias', 'decoder.layers_msk.3.encoder_attn_layer_norm.weight', 'decoder.layers_msk.3.encoder_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.encoder_attn.k_proj.weight', 'decoder.layers_msk.4.encoder_attn.k_proj.bias', 'decoder.layers_msk.4.encoder_attn.v_proj.weight', 'decoder.layers_msk.4.encoder_attn.v_proj.bias', 'decoder.layers_msk.4.encoder_attn.q_proj.weight', 'decoder.layers_msk.4.encoder_attn.q_proj.bias', 'decoder.layers_msk.4.encoder_attn.out_proj.weight', 'decoder.layers_msk.4.encoder_attn.out_proj.bias', 'decoder.layers_msk.4.encoder_attn_layer_norm.weight', 'decoder.layers_msk.4.encoder_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.encoder_attn.k_proj.weight', 'decoder.layers_msk.5.encoder_attn.k_proj.bias', 'decoder.layers_msk.5.encoder_attn.v_proj.weight', 'decoder.layers_msk.5.encoder_attn.v_proj.bias', 'decoder.layers_msk.5.encoder_attn.q_proj.weight', 'decoder.layers_msk.5.encoder_attn.q_proj.bias', 'decoder.layers_msk.5.encoder_attn.out_proj.weight', 'decoder.layers_msk.5.encoder_attn.out_proj.bias', 'decoder.layers_msk.5.encoder_attn_layer_norm.weight', 'decoder.layers_msk.5.encoder_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 496
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 676
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.encoder_attn.k_proj.weight', 'decoder.layers_msk.0.encoder_attn.k_proj.bias', 'decoder.layers_msk.0.encoder_attn.v_proj.weight', 'decoder.layers_msk.0.encoder_attn.v_proj.bias', 'decoder.layers_msk.0.encoder_attn.q_proj.weight', 'decoder.layers_msk.0.encoder_attn.q_proj.bias', 'decoder.layers_msk.0.encoder_attn.out_proj.weight', 'decoder.layers_msk.0.encoder_attn.out_proj.bias', 'decoder.layers_msk.0.encoder_attn_layer_norm.weight', 'decoder.layers_msk.0.encoder_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.encoder_attn.k_proj.weight', 'decoder.layers_msk.1.encoder_attn.k_proj.bias', 'decoder.layers_msk.1.encoder_attn.v_proj.weight', 'decoder.layers_msk.1.encoder_attn.v_proj.bias', 'decoder.layers_msk.1.encoder_attn.q_proj.weight', 'decoder.layers_msk.1.encoder_attn.q_proj.bias', 'decoder.layers_msk.1.encoder_attn.out_proj.weight', 'decoder.layers_msk.1.encoder_attn.out_proj.bias', 'decoder.layers_msk.1.encoder_attn_layer_norm.weight', 'decoder.layers_msk.1.encoder_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.encoder_attn.k_proj.weight', 'decoder.layers_msk.2.encoder_attn.k_proj.bias', 'decoder.layers_msk.2.encoder_attn.v_proj.weight', 'decoder.layers_msk.2.encoder_attn.v_proj.bias', 'decoder.layers_msk.2.encoder_attn.q_proj.weight', 'decoder.layers_msk.2.encoder_attn.q_proj.bias', 'decoder.layers_msk.2.encoder_attn.out_proj.weight', 'decoder.layers_msk.2.encoder_attn.out_proj.bias', 'decoder.layers_msk.2.encoder_attn_layer_norm.weight', 'decoder.layers_msk.2.encoder_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.encoder_attn.k_proj.weight', 'decoder.layers_msk.3.encoder_attn.k_proj.bias', 'decoder.layers_msk.3.encoder_attn.v_proj.weight', 'decoder.layers_msk.3.encoder_attn.v_proj.bias', 'decoder.layers_msk.3.encoder_attn.q_proj.weight', 'decoder.layers_msk.3.encoder_attn.q_proj.bias', 'decoder.layers_msk.3.encoder_attn.out_proj.weight', 'decoder.layers_msk.3.encoder_attn.out_proj.bias', 'decoder.layers_msk.3.encoder_attn_layer_norm.weight', 'decoder.layers_msk.3.encoder_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.encoder_attn.k_proj.weight', 'decoder.layers_msk.4.encoder_attn.k_proj.bias', 'decoder.layers_msk.4.encoder_attn.v_proj.weight', 'decoder.layers_msk.4.encoder_attn.v_proj.bias', 'decoder.layers_msk.4.encoder_attn.q_proj.weight', 'decoder.layers_msk.4.encoder_attn.q_proj.bias', 'decoder.layers_msk.4.encoder_attn.out_proj.weight', 'decoder.layers_msk.4.encoder_attn.out_proj.bias', 'decoder.layers_msk.4.encoder_attn_layer_norm.weight', 'decoder.layers_msk.4.encoder_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.encoder_attn.k_proj.weight', 'decoder.layers_msk.5.encoder_attn.k_proj.bias', 'decoder.layers_msk.5.encoder_attn.v_proj.weight', 'decoder.layers_msk.5.encoder_attn.v_proj.bias', 'decoder.layers_msk.5.encoder_attn.q_proj.weight', 'decoder.layers_msk.5.encoder_attn.q_proj.bias', 'decoder.layers_msk.5.encoder_attn.out_proj.weight', 'decoder.layers_msk.5.encoder_attn.out_proj.bias', 'decoder.layers_msk.5.encoder_attn_layer_norm.weight', 'decoder.layers_msk.5.encoder_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 496
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
start load cached examples train ...
0it [00:00, ?it/s]374it [00:00, 3480.18it/s]Trained parameters: len 676
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.encoder_attn.k_proj.weight', 'decoder.layers_msk.0.encoder_attn.k_proj.bias', 'decoder.layers_msk.0.encoder_attn.v_proj.weight', 'decoder.layers_msk.0.encoder_attn.v_proj.bias', 'decoder.layers_msk.0.encoder_attn.q_proj.weight', 'decoder.layers_msk.0.encoder_attn.q_proj.bias', 'decoder.layers_msk.0.encoder_attn.out_proj.weight', 'decoder.layers_msk.0.encoder_attn.out_proj.bias', 'decoder.layers_msk.0.encoder_attn_layer_norm.weight', 'decoder.layers_msk.0.encoder_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.encoder_attn.k_proj.weight', 'decoder.layers_msk.1.encoder_attn.k_proj.bias', 'decoder.layers_msk.1.encoder_attn.v_proj.weight', 'decoder.layers_msk.1.encoder_attn.v_proj.bias', 'decoder.layers_msk.1.encoder_attn.q_proj.weight', 'decoder.layers_msk.1.encoder_attn.q_proj.bias', 'decoder.layers_msk.1.encoder_attn.out_proj.weight', 'decoder.layers_msk.1.encoder_attn.out_proj.bias', 'decoder.layers_msk.1.encoder_attn_layer_norm.weight', 'decoder.layers_msk.1.encoder_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.encoder_attn.k_proj.weight', 'decoder.layers_msk.2.encoder_attn.k_proj.bias', 'decoder.layers_msk.2.encoder_attn.v_proj.weight', 'decoder.layers_msk.2.encoder_attn.v_proj.bias', 'decoder.layers_msk.2.encoder_attn.q_proj.weight', 'decoder.layers_msk.2.encoder_attn.q_proj.bias', 'decoder.layers_msk.2.encoder_attn.out_proj.weight', 'decoder.layers_msk.2.encoder_attn.out_proj.bias', 'decoder.layers_msk.2.encoder_attn_layer_norm.weight', 'decoder.layers_msk.2.encoder_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.encoder_attn.k_proj.weight', 'decoder.layers_msk.3.encoder_attn.k_proj.bias', 'decoder.layers_msk.3.encoder_attn.v_proj.weight', 'decoder.layers_msk.3.encoder_attn.v_proj.bias', 'decoder.layers_msk.3.encoder_attn.q_proj.weight', 'decoder.layers_msk.3.encoder_attn.q_proj.bias', 'decoder.layers_msk.3.encoder_attn.out_proj.weight', 'decoder.layers_msk.3.encoder_attn.out_proj.bias', 'decoder.layers_msk.3.encoder_attn_layer_norm.weight', 'decoder.layers_msk.3.encoder_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.encoder_attn.k_proj.weight', 'decoder.layers_msk.4.encoder_attn.k_proj.bias', 'decoder.layers_msk.4.encoder_attn.v_proj.weight', 'decoder.layers_msk.4.encoder_attn.v_proj.bias', 'decoder.layers_msk.4.encoder_attn.q_proj.weight', 'decoder.layers_msk.4.encoder_attn.q_proj.bias', 'decoder.layers_msk.4.encoder_attn.out_proj.weight', 'decoder.layers_msk.4.encoder_attn.out_proj.bias', 'decoder.layers_msk.4.encoder_attn_layer_norm.weight', 'decoder.layers_msk.4.encoder_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.encoder_attn.k_proj.weight', 'decoder.layers_msk.5.encoder_attn.k_proj.bias', 'decoder.layers_msk.5.encoder_attn.v_proj.weight', 'decoder.layers_msk.5.encoder_attn.v_proj.bias', 'decoder.layers_msk.5.encoder_attn.q_proj.weight', 'decoder.layers_msk.5.encoder_attn.q_proj.bias', 'decoder.layers_msk.5.encoder_attn.out_proj.weight', 'decoder.layers_msk.5.encoder_attn.out_proj.bias', 'decoder.layers_msk.5.encoder_attn_layer_norm.weight', 'decoder.layers_msk.5.encoder_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 496
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
2022-07-09 16:01:29 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoder(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): EditorTransformerDecoder(
    (embed_tokens): Embedding(28996, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
    (embed_mask_ins): Embedding(256, 1536)
    (layers_msk): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layers_reposition): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-07-09 16:01:29 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-09 16:01:29 | INFO | fairseq_cli.train | num. model params: 273563651 (num. trained: 273563651)
2022-07-09 16:01:29 | INFO | fairseq_cli.train | num. Encoder model params: 108705539 (Encoder num. trained: 108705539)
2022-07-09 16:01:29 | INFO | fairseq_cli.train | num. Decoder model params: 164858112 (Decoder num. trained: 164858112)

start load cached examples train ...
0it [00:00, ?it/s]749it [00:00, 3629.24it/s]
start load cached examples train ...
0it [00:00, ?it/s]374it [00:00, 3433.43it/s]2022-07-09 16:01:29 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-09 16:01:29 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-09 16:01:29 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt
2022-07-09 16:01:29 | INFO | fairseq.trainer | loading train data for epoch 1
1125it [00:00, 3687.11it/s]364it [00:00, 3639.50it/s]754it [00:00, 3639.59it/s]1495it [00:00, 3447.67it/s]728it [00:00, 3486.72it/s]2022-07-09 16:01:29 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.source
1129it [00:00, 3685.25it/s]2022-07-09 16:01:30 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.target
2022-07-09 16:01:30 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]1883it [00:00, 3595.13it/s]1100it [00:00, 3588.65it/s]1499it [00:00, 3475.38it/s]370it [00:00, 3690.34it/s]2245it [00:00, 3488.01it/s]1460it [00:00, 3363.30it/s]1884it [00:00, 3603.59it/s]740it [00:00, 3492.86it/s]2621it [00:00, 3572.16it/s]1839it [00:00, 3508.32it/s]2247it [00:00, 3469.23it/s]1108it [00:00, 3574.95it/s]2980it [00:00, 3480.76it/s]2192it [00:00, 3356.36it/s]2622it [00:00, 3555.36it/s]1467it [00:00, 3349.44it/s]3360it [00:00, 3575.67it/s]2561it [00:00, 3457.06it/s]2980it [00:00, 3447.48it/s]1843it [00:00, 3487.96it/s]3720it [00:01, 3582.23it/s]2909it [00:00, 3361.58it/s]3359it [00:00, 3548.66it/s]2194it [00:00, 3359.99it/s]4080it [00:01, 3480.76it/s]3269it [00:00, 3432.61it/s]3733it [00:01, 3603.64it/s]2568it [00:00, 3475.94it/s]4459it [00:01, 3569.56it/s]3643it [00:01, 3522.21it/s]4095it [00:01, 3469.89it/s]2918it [00:00, 3355.15it/s]4818it [00:01, 3457.01it/s]3997it [00:01, 3394.49it/s]4472it [00:01, 3556.75it/s]3294it [00:00, 3473.27it/s]5193it [00:01, 3541.44it/s]4372it [00:01, 3495.28it/s]4830it [00:01, 3432.16it/s]3663it [00:01, 3536.12it/s]5549it [00:01, 3452.10it/s]4724it [00:01, 3343.31it/s]5203it [00:01, 3515.64it/s]4019it [00:01, 3390.01it/s]5926it [00:01, 3541.35it/s]5093it [00:01, 3441.74it/s]5557it [00:01, 3404.83it/s]4394it [00:01, 3493.06it/s]6282it [00:01, 3452.79it/s]5440it [00:01, 3338.92it/s]5923it [00:01, 3476.87it/s]4746it [00:01, 3335.11it/s]6664it [00:01, 3557.02it/s]5810it [00:01, 3441.48it/s]6273it [00:01, 3385.19it/s]5114it [00:01, 3430.87it/s]7038it [00:01, 3608.87it/s]6168it [00:01, 3478.96it/s]6654it [00:01, 3504.50it/s]5460it [00:01, 3309.22it/s]7400it [00:02, 3504.74it/s]6518it [00:01, 3363.33it/s]7026it [00:01, 3564.64it/s]5831it [00:01, 3421.65it/s]7776it [00:02, 3576.68it/s]6885it [00:02, 3445.67it/s]7384it [00:02, 3453.49it/s]6202it [00:01, 3502.91it/s]7232it [00:02, 3352.03it/s]7749it [00:02, 3508.33it/s]6555it [00:01, 3348.15it/s]7585it [00:02, 3397.81it/s]6923it [00:02, 3440.90it/s]7935it [00:02, 3313.35it/s]7270it [00:02, 3338.40it/s]7630it [00:02, 3411.28it/s]7973it [00:02, 3309.92it/s]8135it [00:02, 1184.97it/s]8102it [00:02, 1279.70it/s]8513it [00:03, 1500.19it/s]8482it [00:03, 1611.60it/s]8844it [00:03, 1758.96it/s]8844it [00:03, 1890.50it/s]9227it [00:03, 2118.53it/s]8268it [00:03, 1177.80it/s]9213it [00:03, 2216.13it/s]9595it [00:03, 2428.08it/s]8638it [00:03, 1497.61it/s]9595it [00:03, 2545.75it/s]9936it [00:03, 2619.49it/s]8949it [00:03, 1744.38it/s]9936it [00:03, 2708.42it/s]10316it [00:03, 2900.00it/s]9320it [00:03, 2097.07it/s]10315it [00:03, 2970.61it/s]8306it [00:03, 1030.13it/s]10666it [00:03, 3003.68it/s]9684it [00:03, 2323.63it/s]10664it [00:03, 3029.54it/s]8677it [00:03, 1331.89it/s]11045it [00:03, 3209.63it/s]10063it [00:03, 2644.40it/s]11044it [00:03, 3231.76it/s]8976it [00:03, 1561.54it/s]11399it [00:03, 3233.54it/s]10440it [00:03, 2911.13it/s]9345it [00:03, 1912.80it/s]11396it [00:03, 3237.45it/s]11779it [00:04, 3387.91it/s]10786it [00:03, 2971.64it/s]11773it [00:03, 3384.08it/s]9683it [00:03, 2158.39it/s]12161it [00:04, 3508.67it/s]11159it [00:03, 3167.47it/s]12152it [00:04, 3497.16it/s]10051it [00:03, 2480.26it/s]12525it [00:04, 3463.08it/s]11506it [00:04, 3137.66it/s]10427it [00:03, 2777.21it/s]12513it [00:04, 3423.29it/s]12909it [00:04, 3570.96it/s]11879it [00:04, 3298.56it/s]12897it [00:04, 3539.70it/s]10770it [00:03, 2866.81it/s]13273it [00:04, 3521.04it/s]12226it [00:04, 3267.67it/s]11137it [00:04, 3072.07it/s]13258it [00:04, 3477.30it/s]13665it [00:04, 3635.92it/s]12609it [00:04, 3424.86it/s]13650it [00:04, 3603.97it/s]11481it [00:04, 3093.14it/s]14033it [00:04, 3514.48it/s]12987it [00:04, 3525.72it/s]14015it [00:04, 3522.16it/s]11853it [00:04, 3236.39it/s]14430it [00:04, 3642.86it/s]13347it [00:04, 3446.60it/s]14398it [00:04, 3608.68it/s]12203it [00:04, 3216.68it/s]14798it [00:04, 3550.67it/s]13732it [00:04, 3560.60it/s]14762it [00:04, 3510.32it/s]12586it [00:04, 3386.11it/s]15192it [00:04, 3660.24it/s]14093it [00:04, 3467.88it/s]15154it [00:04, 3628.11it/s]12956it [00:04, 3474.09it/s]15565it [00:05, 3536.74it/s]14456it [00:04, 3512.78it/s]15534it [00:04, 3676.84it/s]13312it [00:04, 3417.05it/s]15948it [00:05, 3620.18it/s]14810it [00:04, 3435.09it/s]13694it [00:04, 3532.02it/s]15904it [00:05, 3521.41it/s]16331it [00:05, 3679.10it/s]15196it [00:05, 3555.73it/s]16276it [00:05, 3576.09it/s]14052it [00:04, 3412.03it/s]16701it [00:05, 3587.04it/s]15565it [00:05, 3427.58it/s]16636it [00:05, 3501.72it/s]14440it [00:04, 3543.05it/s]17085it [00:05, 3657.67it/s]15923it [00:05, 3468.85it/s]17031it [00:05, 3629.23it/s]14798it [00:05, 3438.23it/s]17453it [00:05, 3560.23it/s]16293it [00:05, 3533.28it/s]15171it [00:05, 3520.02it/s]17396it [00:05, 3510.89it/s]17842it [00:05, 3653.77it/s]16648it [00:05, 3453.72it/s]15545it [00:05, 3583.53it/s]17776it [00:05, 3592.21it/s]17035it [00:05, 3570.85it/s]18137it [00:05, 3506.01it/s]15906it [00:05, 3453.96it/s]17394it [00:05, 3421.27it/s]16257it [00:05, 3467.85it/s]17779it [00:05, 3541.06it/s]16606it [00:05, 3401.89it/s]18136it [00:05, 3453.38it/s]16995it [00:05, 3542.59it/s]17351it [00:05, 3436.36it/s]17714it [00:05, 3489.83it/s]18084it [00:06, 3397.65it/s]18209it [00:06, 1145.73it/s]18603it [00:06, 1468.19it/s]18987it [00:06, 1803.99it/s]18489it [00:06, 1061.33it/s]19317it [00:06, 2054.97it/s]18870it [00:06, 1365.13it/s]19701it [00:06, 2400.02it/s]19191it [00:06, 1616.42it/s]20047it [00:07, 2589.40it/s]19577it [00:06, 1981.58it/s]20430it [00:07, 2878.51it/s]19905it [00:07, 2214.31it/s]20783it [00:07, 2987.99it/s]18483it [00:07, 883.84it/s] 20272it [00:07, 2521.61it/s]21167it [00:07, 3208.17it/s]18857it [00:07, 1154.61it/s]20653it [00:07, 2819.90it/s]19159it [00:07, 1377.02it/s]21524it [00:07, 3235.67it/s]21004it [00:07, 2929.16it/s]19541it [00:07, 1732.03it/s]21911it [00:07, 3409.31it/s]18426it [00:07, 918.36it/s] 21383it [00:07, 3151.26it/s]19861it [00:07, 1975.44it/s]22292it [00:07, 3519.90it/s]18780it [00:07, 1177.60it/s]21737it [00:07, 3146.42it/s]20231it [00:07, 2313.95it/s]22659it [00:07, 3486.90it/s]19087it [00:07, 1411.74it/s]22117it [00:07, 3322.05it/s]20605it [00:07, 2622.73it/s]23036it [00:07, 3567.32it/s]19464it [00:07, 1766.01it/s]22470it [00:07, 3305.53it/s]20950it [00:07, 2765.21it/s]23401it [00:07, 3473.67it/s]19833it [00:07, 2039.31it/s]22858it [00:07, 3465.38it/s]21326it [00:07, 3013.16it/s]23778it [00:08, 3556.88it/s]20189it [00:07, 2337.54it/s]23216it [00:07, 3398.09it/s]21675it [00:07, 3042.41it/s]24138it [00:08, 3483.17it/s]20558it [00:07, 2632.89it/s]23594it [00:08, 3503.96it/s]22052it [00:08, 3235.67it/s]24520it [00:08, 3579.80it/s]20897it [00:07, 2744.22it/s]23953it [00:08, 3527.50it/s]22402it [00:08, 3231.63it/s]24881it [00:08, 3479.64it/s]21268it [00:07, 2984.24it/s]24311it [00:08, 3414.01it/s]22775it [00:08, 3367.84it/s]25262it [00:08, 3573.61it/s]21610it [00:08, 2988.71it/s]24681it [00:08, 3495.16it/s]23153it [00:08, 3483.20it/s]25645it [00:08, 3643.37it/s]21983it [00:08, 3185.03it/s]25034it [00:08, 3392.86it/s]23512it [00:08, 3375.12it/s]26011it [00:08, 3503.99it/s]22353it [00:08, 3177.27it/s]25407it [00:08, 3489.16it/s]23878it [00:08, 3455.38it/s]26393it [00:08, 3592.13it/s]22734it [00:08, 3349.82it/s]25759it [00:08, 3400.15it/s]24230it [00:08, 3372.11it/s]26755it [00:08, 3487.04it/s]23089it [00:08, 3404.55it/s]26114it [00:08, 3441.12it/s]24602it [00:08, 3470.99it/s]27140it [00:09, 3589.57it/s]23440it [00:08, 3299.49it/s]26485it [00:08, 3513.48it/s]24953it [00:08, 3368.58it/s]27501it [00:09, 3507.84it/s]23809it [00:08, 3408.63it/s]26838it [00:09, 3405.82it/s]25319it [00:08, 3449.19it/s]27885it [00:09, 3600.62it/s]24156it [00:08, 3315.29it/s]27218it [00:09, 3518.01it/s]25694it [00:09, 3536.05it/s]28247it [00:09, 3501.41it/s]24508it [00:08, 3372.49it/s]27572it [00:09, 3433.60it/s]26050it [00:09, 3412.30it/s]28616it [00:09, 3553.42it/s]24873it [00:08, 3284.17it/s]27951it [00:09, 3535.84it/s]26414it [00:09, 3477.65it/s]28993it [00:09, 3615.51it/s]25240it [00:09, 3391.97it/s]28306it [00:09, 3368.61it/s]26764it [00:09, 3369.30it/s]29356it [00:09, 3496.98it/s]25610it [00:09, 3478.82it/s]28678it [00:09, 3466.11it/s]27142it [00:09, 3484.69it/s]29736it [00:09, 3583.71it/s]25961it [00:09, 3317.34it/s]29049it [00:09, 3535.35it/s]27493it [00:09, 3377.79it/s]30096it [00:09, 3482.41it/s]26327it [00:09, 3411.73it/s]29405it [00:09, 3422.28it/s]27870it [00:09, 3489.81it/s]30475it [00:09, 3570.26it/s]26671it [00:09, 3312.06it/s]29778it [00:09, 3507.98it/s]28235it [00:09, 3390.71it/s]27042it [00:09, 3423.03it/s]30131it [00:09, 3407.26it/s]28603it [00:09, 3472.39it/s]27394it [00:09, 3315.39it/s]30491it [00:10, 3462.45it/s]28962it [00:10, 3505.60it/s]27765it [00:09, 3426.18it/s]29314it [00:10, 3400.55it/s]28139it [00:09, 3514.18it/s]29683it [00:10, 3481.66it/s]28493it [00:10, 3365.67it/s]30033it [00:10, 3361.06it/s]28861it [00:10, 3453.06it/s]30405it [00:10, 3455.02it/s]29209it [00:10, 3295.46it/s]30755it [00:10, 3361.95it/s]29570it [00:10, 3381.56it/s]29914it [00:10, 3281.06it/s]30834it [00:11, 911.35it/s] 30282it [00:10, 3393.45it/s]31217it [00:11, 1191.21it/s]30649it [00:10, 3472.49it/s]31598it [00:11, 1506.82it/s]31922it [00:11, 1761.08it/s]32309it [00:11, 2126.18it/s]30839it [00:11, 754.24it/s] 32651it [00:11, 2351.08it/s]31215it [00:11, 1003.18it/s]33026it [00:11, 2656.56it/s]31590it [00:11, 1293.65it/s]33374it [00:11, 2797.14it/s]31905it [00:11, 1535.93it/s]33750it [00:11, 3036.61it/s]32285it [00:11, 1892.77it/s]34135it [00:11, 3248.73it/s]31093it [00:11, 789.79it/s] 32619it [00:11, 2113.12it/s]34496it [00:12, 3259.34it/s]31462it [00:11, 1043.59it/s]32985it [00:12, 2428.32it/s]34873it [00:12, 3398.57it/s]31772it [00:12, 1272.03it/s]33344it [00:12, 2607.52it/s]35232it [00:12, 3358.91it/s]32152it [00:12, 1618.24it/s]33719it [00:12, 2878.71it/s]35610it [00:12, 3476.26it/s]32504it [00:12, 1875.08it/s]34097it [00:12, 3106.68it/s]35968it [00:12, 3410.89it/s]32870it [00:12, 2204.86it/s]34451it [00:12, 3139.97it/s]36346it [00:12, 3514.96it/s]33241it [00:12, 2518.42it/s]34800it [00:12, 3234.49it/s]36704it [00:12, 3392.71it/s]33582it [00:12, 2665.28it/s]35146it [00:12, 3210.66it/s]37084it [00:12, 3505.84it/s]30998it [00:12, 622.08it/s] 33948it [00:12, 2906.95it/s]35515it [00:12, 3344.24it/s]37459it [00:12, 3575.96it/s]31366it [00:12, 834.14it/s]34290it [00:12, 2979.17it/s]35864it [00:12, 3294.17it/s]37820it [00:13, 3467.63it/s]31669it [00:12, 1031.84it/s]34660it [00:12, 3169.50it/s]36236it [00:12, 3414.77it/s]38204it [00:13, 3572.64it/s]32029it [00:12, 1324.63it/s]35024it [00:12, 3152.61it/s]36603it [00:13, 3487.92it/s]32403it [00:12, 1662.51it/s]38564it [00:13, 3432.09it/s]35397it [00:13, 3308.00it/s]36957it [00:13, 3344.52it/s]38945it [00:13, 3537.63it/s]32733it [00:12, 1903.08it/s]35759it [00:13, 3393.83it/s]37329it [00:13, 3448.85it/s]39302it [00:13, 3469.86it/s]33072it [00:12, 2128.97it/s]36110it [00:13, 3310.03it/s]37678it [00:13, 3346.11it/s]39669it [00:13, 3526.87it/s]33386it [00:13, 2319.63it/s]36478it [00:13, 3412.19it/s]38048it [00:13, 3445.45it/s]40050it [00:13, 3607.89it/s]33758it [00:13, 2639.25it/s]36826it [00:13, 3280.44it/s]38396it [00:13, 3355.21it/s]34130it [00:13, 2904.30it/s]40413it [00:13, 3505.97it/s]37194it [00:13, 3389.88it/s]38743it [00:13, 3386.94it/s]40788it [00:13, 3574.65it/s]34472it [00:13, 2957.31it/s]37544it [00:13, 3299.50it/s]39116it [00:13, 3485.32it/s]41147it [00:13, 3477.66it/s]34836it [00:13, 3137.35it/s]37896it [00:13, 3360.93it/s]39466it [00:13, 3399.66it/s]41529it [00:14, 3575.51it/s]35178it [00:13, 3117.01it/s]38276it [00:13, 3485.79it/s]39837it [00:14, 3488.62it/s]41888it [00:14, 3466.87it/s]35545it [00:13, 3268.87it/s]38627it [00:14, 3328.16it/s]40188it [00:14, 3395.57it/s]42257it [00:14, 3530.98it/s]35887it [00:13, 3214.48it/s]39006it [00:14, 3458.57it/s]40557it [00:14, 3478.58it/s]42612it [00:14, 3453.21it/s]36252it [00:13, 3334.85it/s]39355it [00:14, 3377.85it/s]40907it [00:14, 3340.48it/s]42991it [00:14, 3549.02it/s]36594it [00:14, 3356.58it/s]39709it [00:14, 3423.70it/s]41283it [00:14, 3460.02it/s]43365it [00:14, 3603.66it/s]36936it [00:14, 3262.90it/s]40064it [00:14, 3330.89it/s]41656it [00:14, 3536.36it/s]43727it [00:14, 3489.10it/s]37302it [00:14, 3374.77it/s]40438it [00:14, 3446.67it/s]42012it [00:14, 3423.73it/s]44112it [00:14, 3591.74it/s]37644it [00:14, 3261.53it/s]40807it [00:14, 3516.93it/s]42385it [00:14, 3511.74it/s]44473it [00:14, 3503.61it/s]38000it [00:14, 3346.45it/s]41161it [00:14, 3403.88it/s]42738it [00:14, 3415.75it/s]44854it [00:15, 3590.68it/s]38368it [00:14, 3440.47it/s]41518it [00:14, 3449.29it/s]43096it [00:14, 3462.94it/s]45215it [00:15, 3450.64it/s]38715it [00:14, 3306.34it/s]41865it [00:14, 3351.19it/s]43444it [00:15, 3356.85it/s]45594it [00:15, 3541.07it/s]39081it [00:14, 3406.59it/s]42238it [00:15, 3458.30it/s]43817it [00:15, 3461.69it/s]45950it [00:15, 3442.97it/s]39424it [00:14, 3267.77it/s]42586it [00:15, 3370.35it/s]44194it [00:15, 3549.24it/s]39789it [00:14, 3374.11it/s]42940it [00:15, 3417.38it/s]44551it [00:15, 3444.82it/s]40129it [00:15, 3282.84it/s]43310it [00:15, 3497.61it/s]44929it [00:15, 3540.04it/s]40496it [00:15, 3390.87it/s]43661it [00:15, 3376.59it/s]45285it [00:15, 3431.62it/s]40844it [00:15, 3416.59it/s]44040it [00:15, 3494.89it/s]45660it [00:15, 3522.03it/s]41188it [00:15, 3321.68it/s]44391it [00:15, 3366.54it/s]41556it [00:15, 3423.56it/s]44761it [00:15, 3461.31it/s]41900it [00:15, 3313.52it/s]45109it [00:15, 3381.48it/s]42271it [00:15, 3427.29it/s]45464it [00:16, 3429.63it/s]42616it [00:15, 3295.81it/s]45835it [00:16, 3509.98it/s]42982it [00:15, 3399.09it/s]43349it [00:16, 3476.20it/s]43699it [00:16, 3300.35it/s]44069it [00:16, 3411.37it/s]46296it [00:16, 693.37it/s] 44413it [00:16, 3314.27it/s]46672it [00:16, 926.60it/s]44781it [00:16, 3417.02it/s]46977it [00:17, 1134.64it/s]45125it [00:16, 3322.47it/s]47361it [00:17, 1466.74it/s]45474it [00:16, 3368.29it/s]47693it [00:17, 1733.20it/s]45841it [00:16, 3454.66it/s]48075it [00:17, 2097.75it/s]48459it [00:17, 2445.92it/s]48811it [00:17, 2647.12it/s]49190it [00:17, 2917.68it/s]46014it [00:17, 591.66it/s] 49546it [00:17, 3015.99it/s]46383it [00:17, 793.67it/s]49928it [00:17, 3225.95it/s]46738it [00:17, 1028.53it/s]47049it [00:17, 1254.28it/s]50286it [00:17, 3190.14it/s]47425it [00:17, 1590.34it/s]50663it [00:18, 3347.39it/s]47754it [00:18, 1847.92it/s]51049it [00:18, 3489.37it/s]48124it [00:18, 2190.61it/s]51413it [00:18, 3410.06it/s]46188it [00:18, 545.31it/s] 48500it [00:18, 2518.67it/s]51791it [00:18, 3513.04it/s]46558it [00:18, 738.02it/s]48849it [00:18, 2659.58it/s]52151it [00:18, 3437.49it/s]46853it [00:18, 913.37it/s]49221it [00:18, 2914.51it/s]52527it [00:18, 3527.62it/s]47232it [00:18, 1208.54it/s]49568it [00:18, 2982.96it/s]52885it [00:18, 3450.63it/s]47604it [00:18, 1529.43it/s]49938it [00:18, 3171.16it/s]53261it [00:18, 3538.65it/s]47936it [00:18, 1785.37it/s]50286it [00:18, 3157.01it/s]48310it [00:18, 2134.56it/s]53618it [00:18, 3334.01it/s]50656it [00:18, 3304.07it/s]53991it [00:19, 3443.68it/s]48650it [00:18, 2331.70it/s]51016it [00:18, 3386.93it/s]54366it [00:19, 3530.22it/s]49025it [00:18, 2644.49it/s]51367it [00:19, 3311.29it/s]46188it [00:18, 533.98it/s] 54722it [00:19, 3422.22it/s]49373it [00:19, 2771.92it/s]51736it [00:19, 3418.21it/s]46551it [00:18, 721.78it/s]55090it [00:19, 3494.23it/s]49750it [00:19, 3020.61it/s]52085it [00:19, 3332.92it/s]46852it [00:18, 903.19it/s]50105it [00:19, 3149.88it/s]55442it [00:19, 3378.88it/s]52453it [00:19, 3427.60it/s]47207it [00:19, 1173.56it/s]55808it [00:19, 3459.02it/s]50453it [00:19, 3140.91it/s]47576it [00:19, 1493.58it/s]52800it [00:19, 3337.53it/s]50823it [00:19, 3291.71it/s]56156it [00:19, 3381.18it/s]47902it [00:19, 1746.25it/s]53168it [00:19, 3433.28it/s]56526it [00:19, 3470.41it/s]51170it [00:19, 3254.77it/s]48270it [00:19, 2090.99it/s]53535it [00:19, 3500.58it/s]56880it [00:19, 3490.54it/s]51539it [00:19, 3375.20it/s]48604it [00:19, 2303.73it/s]53888it [00:19, 3383.78it/s]57231it [00:19, 3411.44it/s]51893it [00:19, 3263.05it/s]48957it [00:19, 2575.14it/s]54264it [00:19, 3491.88it/s]57604it [00:20, 3501.18it/s]52263it [00:19, 3385.10it/s]49324it [00:19, 2837.04it/s]54616it [00:20, 3380.91it/s]57956it [00:20, 3406.48it/s]52634it [00:19, 3475.31it/s]49668it [00:19, 2910.01it/s]54980it [00:20, 3454.85it/s]58320it [00:20, 3471.25it/s]52987it [00:20, 3365.60it/s]50035it [00:19, 3107.73it/s]55328it [00:20, 3329.85it/s]58669it [00:20, 3379.46it/s]53356it [00:20, 3457.23it/s]50379it [00:19, 3046.90it/s]55677it [00:20, 3375.09it/s]59033it [00:20, 3454.34it/s]53705it [00:20, 3337.49it/s]50745it [00:20, 3211.49it/s]56047it [00:20, 3466.65it/s]59396it [00:20, 3504.99it/s]54075it [00:20, 3440.67it/s]51084it [00:20, 3177.00it/s]56396it [00:20, 3362.73it/s]59748it [00:20, 3387.73it/s]54422it [00:20, 3326.73it/s]51451it [00:20, 3312.41it/s]56764it [00:20, 3453.79it/s]60097it [00:20, 3417.16it/s]54788it [00:20, 3421.41it/s]51817it [00:20, 3409.92it/s]57111it [00:20, 3358.72it/s]60440it [00:20, 3330.89it/s]55151it [00:20, 3480.70it/s]57482it [00:20, 3459.57it/s]52166it [00:20, 3257.08it/s]60811it [00:21, 3439.62it/s]55501it [00:20, 3342.55it/s]52529it [00:20, 3359.74it/s]57830it [00:20, 3353.15it/s]61157it [00:21, 3355.29it/s]55856it [00:20, 3399.83it/s]58194it [00:21, 3434.16it/s]52871it [00:20, 3279.25it/s]61524it [00:21, 3443.79it/s]56198it [00:21, 3317.75it/s]58557it [00:21, 3490.36it/s]53234it [00:20, 3378.72it/s]61897it [00:21, 3525.79it/s]56565it [00:21, 3417.71it/s]58908it [00:21, 3364.41it/s]53576it [00:20, 3225.17it/s]62251it [00:21, 3419.13it/s]56933it [00:21, 3330.41it/s]59264it [00:21, 3420.03it/s]53944it [00:21, 3351.32it/s]62619it [00:21, 3494.15it/s]57298it [00:21, 3419.49it/s]54314it [00:21, 3450.57it/s]59608it [00:21, 3299.80it/s]62970it [00:21, 3345.06it/s]57667it [00:21, 3495.24it/s]59977it [00:21, 3408.88it/s]54662it [00:21, 3329.68it/s]63343it [00:21, 3453.36it/s]58018it [00:21, 3378.67it/s]55023it [00:21, 3409.12it/s]60320it [00:21, 3301.10it/s]63691it [00:21, 3351.59it/s]58379it [00:21, 3443.17it/s]60689it [00:21, 3411.28it/s]64061it [00:21, 3450.96it/s]55367it [00:21, 3235.66it/s]58725it [00:21, 3252.28it/s]61059it [00:21, 3494.88it/s]64427it [00:22, 3508.63it/s]55723it [00:21, 3325.69it/s]59087it [00:21, 3354.00it/s]61410it [00:21, 3381.83it/s]64780it [00:22, 3412.92it/s]56092it [00:21, 3428.56it/s]59449it [00:21, 3429.16it/s]61775it [00:22, 3458.38it/s]65149it [00:22, 3492.04it/s]56438it [00:21, 3303.17it/s]59795it [00:22, 3318.96it/s]62123it [00:22, 3362.90it/s]56802it [00:21, 3396.94it/s]60157it [00:22, 3404.51it/s]62489it [00:22, 3446.93it/s]57144it [00:22, 3253.50it/s]60500it [00:22, 3308.19it/s]62836it [00:22, 3353.66it/s]57510it [00:22, 3367.60it/s]60833it [00:22, 3308.53it/s]63212it [00:22, 3468.79it/s]57850it [00:22, 3259.48it/s]61166it [00:22, 3242.75it/s]63580it [00:22, 3528.23it/s]58209it [00:22, 3353.11it/s]61529it [00:22, 3353.37it/s]63934it [00:22, 3406.76it/s]58549it [00:22, 3366.50it/s]61899it [00:22, 3451.89it/s]64304it [00:22, 3490.65it/s]58888it [00:22, 3257.57it/s]62246it [00:22, 3345.49it/s]64655it [00:22, 3380.44it/s]59248it [00:22, 3355.03it/s]62610it [00:22, 3430.47it/s]65025it [00:23, 3470.86it/s]59586it [00:22, 3242.71it/s]62955it [00:23, 3255.72it/s]65374it [00:23, 3358.59it/s]59946it [00:22, 3344.57it/s]63327it [00:23, 3385.19it/s]60291it [00:22, 3372.83it/s]63668it [00:23, 3283.52it/s]60630it [00:23, 3261.74it/s]64036it [00:23, 3395.65it/s]60993it [00:23, 3361.31it/s]64399it [00:23, 3462.57it/s]61331it [00:23, 3272.96it/s]64747it [00:23, 3307.56it/s]61668it [00:23, 3299.22it/s]65112it [00:23, 3403.08it/s]61999it [00:23, 3218.57it/s]62367it [00:23, 3350.08it/s]62729it [00:23, 3426.58it/s]63073it [00:23, 3293.97it/s]65500it [00:24, 520.29it/s] 63419it [00:23, 3341.23it/s]65873it [00:24, 707.83it/s]63755it [00:24, 3226.27it/s]66226it [00:24, 925.24it/s]64118it [00:24, 3341.73it/s]66537it [00:24, 1141.77it/s]64477it [00:24, 3412.66it/s]66910it [00:24, 1462.55it/s]64820it [00:24, 3239.67it/s]67238it [00:24, 1726.33it/s]65178it [00:24, 3335.32it/s]67610it [00:24, 2077.40it/s]67949it [00:25, 2317.34it/s]68319it [00:25, 2622.36it/s]68693it [00:25, 2885.00it/s]69046it [00:25, 2966.22it/s]65712it [00:25, 513.84it/s] 69401it [00:25, 3118.15it/s]66083it [00:25, 701.77it/s]66385it [00:25, 881.25it/s]69747it [00:25, 3129.51it/s]66756it [00:25, 1162.86it/s]70125it [00:25, 3306.94it/s]67083it [00:25, 1415.35it/s]70474it [00:25, 3269.85it/s]67444it [00:25, 1744.92it/s]70843it [00:25, 3387.67it/s]67817it [00:25, 2095.44it/s]71212it [00:25, 3472.74it/s]68157it [00:25, 2312.13it/s]71567it [00:26, 3372.24it/s]65455it [00:25, 467.16it/s] 68525it [00:26, 2612.09it/s]71939it [00:26, 3470.79it/s]65821it [00:26, 637.85it/s]68867it [00:26, 2743.01it/s]72291it [00:26, 3365.71it/s]66188it [00:26, 853.14it/s]69235it [00:26, 2976.82it/s]72646it [00:26, 3416.35it/s]66494it [00:26, 1054.66it/s]69602it [00:26, 3157.62it/s]72991it [00:26, 3359.34it/s]66847it [00:26, 1342.01it/s]69953it [00:26, 3154.35it/s]73361it [00:26, 3455.97it/s]67166it [00:26, 1593.91it/s]70322it [00:26, 3300.01it/s]73737it [00:26, 3543.23it/s]67530it [00:26, 1937.05it/s]70671it [00:26, 3215.07it/s]74093it [00:26, 3451.37it/s]67903it [00:26, 2283.11it/s]71034it [00:26, 3328.61it/s]74464it [00:26, 3525.09it/s]68246it [00:26, 2466.89it/s]71377it [00:26, 3259.37it/s]74818it [00:27, 3415.54it/s]68595it [00:26, 2703.17it/s]71748it [00:26, 3385.89it/s]75185it [00:27, 3488.34it/s]68932it [00:26, 2803.66it/s]72115it [00:27, 3466.82it/s]75536it [00:27, 3334.92it/s]69298it [00:27, 3023.33it/s]72467it [00:27, 3363.16it/s]75905it [00:27, 3435.58it/s]69638it [00:27, 3031.99it/s]72835it [00:27, 3452.96it/s]76267it [00:27, 3487.97it/s]69994it [00:27, 3173.90it/s]73184it [00:27, 3371.99it/s]65514it [00:27, 401.21it/s] 76618it [00:27, 3376.00it/s]70361it [00:27, 3310.67it/s]73557it [00:27, 3474.63it/s]65875it [00:27, 552.48it/s]76983it [00:27, 3453.75it/s]70707it [00:27, 3217.75it/s]73907it [00:27, 3387.12it/s]66236it [00:27, 745.33it/s]77330it [00:27, 3360.93it/s]71065it [00:27, 3317.76it/s]74261it [00:27, 3429.16it/s]66537it [00:27, 926.60it/s]77699it [00:27, 3453.81it/s]71405it [00:27, 3205.74it/s]74630it [00:27, 3504.95it/s]66899it [00:27, 1209.44it/s]78046it [00:27, 3355.05it/s]71771it [00:27, 3326.90it/s]74982it [00:27, 3380.02it/s]67214it [00:27, 1453.67it/s]78400it [00:28, 3408.04it/s]72121it [00:27, 3253.53it/s]75344it [00:28, 3441.45it/s]67572it [00:27, 1784.83it/s]78767it [00:28, 3482.94it/s]72484it [00:28, 3358.77it/s]75690it [00:28, 3343.33it/s]67922it [00:27, 2048.17it/s]79117it [00:28, 3367.34it/s]72836it [00:28, 3404.50it/s]76050it [00:28, 3415.32it/s]68272it [00:27, 2338.39it/s]79482it [00:28, 3447.93it/s]73179it [00:28, 3322.53it/s]68633it [00:27, 2623.07it/s]76393it [00:28, 3313.84it/s]79829it [00:28, 3357.99it/s]73550it [00:28, 3432.66it/s]76756it [00:28, 3402.73it/s]68971it [00:28, 2733.31it/s]80196it [00:28, 3447.73it/s]73896it [00:28, 3347.57it/s]77117it [00:28, 3461.23it/s]69329it [00:28, 2946.85it/s]80543it [00:28, 3364.80it/s]74248it [00:28, 3397.15it/s]77465it [00:28, 3359.35it/s]69666it [00:28, 2964.33it/s]80914it [00:28, 3462.62it/s]74615it [00:28, 3476.25it/s]77813it [00:28, 3393.38it/s]70013it [00:28, 3099.92it/s]81278it [00:28, 3514.35it/s]74964it [00:28, 3348.99it/s]70373it [00:28, 3238.47it/s]78154it [00:28, 3310.33it/s]81631it [00:29, 3358.55it/s]75326it [00:28, 3425.03it/s]78524it [00:28, 3420.40it/s]70714it [00:28, 3185.67it/s]81997it [00:29, 3442.09it/s]75670it [00:28, 3309.98it/s]71067it [00:28, 3282.33it/s]78868it [00:29, 3314.30it/s]82343it [00:29, 3355.35it/s]76029it [00:29, 3389.37it/s]79235it [00:29, 3416.26it/s]71405it [00:28, 3195.41it/s]82709it [00:29, 3441.82it/s]76370it [00:29, 3240.32it/s]79599it [00:29, 3479.70it/s]71749it [00:28, 3264.44it/s]83055it [00:29, 3348.66it/s]76734it [00:29, 3352.85it/s]72110it [00:29, 3362.81it/s]79949it [00:29, 3377.51it/s]83424it [00:29, 3446.34it/s]77091it [00:29, 3413.86it/s]80314it [00:29, 3447.60it/s]72451it [00:29, 3253.52it/s]83792it [00:29, 3513.85it/s]77435it [00:29, 3312.15it/s]80660it [00:29, 3363.09it/s]72813it [00:29, 3356.87it/s]84145it [00:29, 3400.86it/s]77782it [00:29, 3355.97it/s]81023it [00:29, 3437.81it/s]73152it [00:29, 3273.58it/s]84498it [00:29, 3438.00it/s]78119it [00:29, 3262.94it/s]73515it [00:29, 3374.34it/s]81368it [00:29, 3286.68it/s]84843it [00:29, 3341.68it/s]78484it [00:29, 3373.45it/s]81732it [00:29, 3386.70it/s]73855it [00:29, 3236.84it/s]85211it [00:30, 3437.43it/s]78842it [00:29, 3272.58it/s]82099it [00:29, 3467.32it/s]74222it [00:29, 3358.59it/s]85564it [00:30, 3346.59it/s]79192it [00:30, 3328.39it/s]74581it [00:29, 3425.12it/s]82448it [00:30, 3357.36it/s]85929it [00:30, 3432.37it/s]79551it [00:30, 3401.41it/s]82816it [00:30, 3448.05it/s]86305it [00:30, 3525.24it/s]74926it [00:29, 3290.15it/s]79893it [00:30, 3306.52it/s]83163it [00:30, 3340.06it/s]75285it [00:29, 3373.73it/s]86659it [00:30, 3413.27it/s]80255it [00:30, 3395.02it/s]83529it [00:30, 3429.25it/s]87024it [00:30, 3479.18it/s]75625it [00:30, 3217.50it/s]80596it [00:30, 3267.06it/s]83884it [00:30, 3325.34it/s]75980it [00:30, 3310.80it/s]87374it [00:30, 3383.84it/s]80957it [00:30, 3364.81it/s]84253it [00:30, 3427.11it/s]87732it [00:30, 3437.74it/s]76323it [00:30, 3196.71it/s]81318it [00:30, 3433.65it/s]84619it [00:30, 3492.68it/s]88084it [00:30, 3347.89it/s]76681it [00:30, 3304.29it/s]81663it [00:30, 3307.14it/s]84970it [00:30, 3377.88it/s]88459it [00:31, 3461.30it/s]77034it [00:30, 3368.85it/s]82011it [00:30, 3355.68it/s]85315it [00:30, 3396.56it/s]88828it [00:31, 3527.68it/s]77373it [00:30, 3259.29it/s]82349it [00:30, 3257.24it/s]85656it [00:31, 3313.67it/s]89182it [00:31, 3420.68it/s]77717it [00:30, 3310.14it/s]82712it [00:31, 3363.20it/s]86022it [00:31, 3413.09it/s]89550it [00:31, 3495.08it/s]78050it [00:30, 3210.72it/s]86392it [00:31, 3495.40it/s]83050it [00:31, 3265.35it/s]78412it [00:30, 3325.65it/s]83399it [00:31, 3327.85it/s]86743it [00:31, 3378.84it/s]78769it [00:31, 3395.03it/s]83763it [00:31, 3417.22it/s]87093it [00:31, 3413.10it/s]79110it [00:31, 3257.62it/s]84106it [00:31, 3302.10it/s]87436it [00:31, 3324.57it/s]79469it [00:31, 3351.27it/s]84470it [00:31, 3398.34it/s]87805it [00:31, 3429.86it/s]79806it [00:31, 3201.59it/s]84812it [00:31, 3250.39it/s]88150it [00:31, 3342.28it/s]80163it [00:31, 3303.60it/s]85174it [00:31, 3352.97it/s]88523it [00:31, 3454.02it/s]80523it [00:31, 3221.74it/s]85536it [00:31, 3428.09it/s]88883it [00:31, 3476.10it/s]80884it [00:31, 3328.28it/s]85881it [00:32, 3305.18it/s]89232it [00:32, 3373.04it/s]81239it [00:31, 3390.57it/s]86252it [00:32, 3419.82it/s]89601it [00:32, 3464.34it/s]81580it [00:31, 3223.42it/s]86596it [00:32, 3275.16it/s]81941it [00:32, 3331.34it/s]86956it [00:32, 3367.26it/s]82277it [00:32, 3227.34it/s]87295it [00:32, 3278.08it/s]82635it [00:32, 3325.40it/s]87661it [00:32, 3386.37it/s]82994it [00:32, 3400.16it/s]88028it [00:32, 3466.32it/s]83336it [00:32, 3273.41it/s]88377it [00:32, 3311.08it/s]83680it [00:32, 3320.02it/s]88745it [00:32, 3415.89it/s]84014it [00:32, 3228.25it/s]89089it [00:32, 3320.01it/s]84372it [00:32, 3321.01it/s]89451it [00:33, 3404.37it/s]84722it [00:32, 3225.70it/s]89794it [00:33, 3265.82it/s]85082it [00:32, 3329.86it/s]85425it [00:33, 3357.12it/s]85762it [00:33, 3188.46it/s]86123it [00:33, 3305.92it/s]86456it [00:33, 3202.38it/s]89901it [00:33, 424.39it/s] 86813it [00:33, 3306.23it/s]90266it [00:34, 579.65it/s]87170it [00:33, 3381.14it/s]90625it [00:34, 773.35it/s]90932it [00:34, 965.89it/s]87510it [00:33, 3231.33it/s]91276it [00:34, 1231.04it/s]87871it [00:33, 3339.09it/s]91593it [00:34, 1479.03it/s]88208it [00:33, 3233.73it/s]91953it [00:34, 1814.85it/s]88573it [00:34, 3349.74it/s]92312it [00:34, 2142.46it/s]88923it [00:34, 3245.03it/s]92649it [00:34, 2347.55it/s]89267it [00:34, 3299.76it/s]93010it [00:34, 2632.44it/s]89623it [00:34, 3372.84it/s]93348it [00:34, 2746.85it/s]93710it [00:35, 2966.79it/s]89949it [00:34, 395.15it/s] 94048it [00:35, 2990.28it/s]90298it [00:35, 535.50it/s]94394it [00:35, 3115.65it/s]90652it [00:35, 718.27it/s]94752it [00:35, 3243.69it/s]90953it [00:35, 899.97it/s]95093it [00:35, 3187.72it/s]91308it [00:35, 1171.14it/s]95449it [00:35, 3292.57it/s]91624it [00:35, 1414.23it/s]95787it [00:35, 3216.99it/s]91981it [00:35, 1743.75it/s]96144it [00:35, 3317.22it/s]92336it [00:35, 2067.37it/s]96503it [00:35, 3394.76it/s]92670it [00:35, 2274.10it/s]96847it [00:35, 3291.22it/s]93027it [00:35, 2560.20it/s]97183it [00:36, 3309.24it/s]93361it [00:36, 2653.14it/s]97517it [00:36, 3238.94it/s]93718it [00:36, 2881.26it/s]97875it [00:36, 3335.48it/s]94050it [00:36, 2896.36it/s]98232it [00:36, 3401.22it/s]94410it [00:36, 3082.46it/s]98574it [00:36, 3292.68it/s]94766it [00:36, 3212.58it/s]98931it [00:36, 3371.88it/s]90123it [00:36, 333.08it/s] 95105it [00:36, 3146.56it/s]99270it [00:36, 3274.70it/s]90477it [00:36, 460.25it/s]95459it [00:36, 3254.83it/s]99628it [00:36, 3360.31it/s]90766it [00:36, 590.45it/s]95795it [00:36, 3169.50it/s]99966it [00:36, 3263.53it/s]91119it [00:36, 799.44it/s]96151it [00:36, 3279.22it/s]100305it [00:37, 3297.89it/s]91471it [00:36, 1050.25it/s]96508it [00:36, 3360.51it/s]100663it [00:37, 3378.47it/s]91786it [00:36, 1276.65it/s]96849it [00:37, 3220.74it/s]101002it [00:37, 3272.90it/s]92138it [00:37, 1592.57it/s]97203it [00:37, 3309.92it/s]101359it [00:37, 3357.38it/s]92457it [00:37, 1831.27it/s]97538it [00:37, 3214.35it/s]101696it [00:37, 3253.31it/s]92811it [00:37, 2156.32it/s]97894it [00:37, 3311.69it/s]102054it [00:37, 3345.11it/s]93164it [00:37, 2449.73it/s]98232it [00:37, 3209.81it/s]102411it [00:37, 3409.23it/s]93497it [00:37, 2580.01it/s]98588it [00:37, 3308.86it/s]102754it [00:37, 3296.40it/s]93830it [00:37, 2763.06it/s]98943it [00:37, 3377.49it/s]103111it [00:37, 3373.09it/s]94155it [00:37, 2819.07it/s]99283it [00:37, 3261.76it/s]103450it [00:37, 3253.89it/s]94510it [00:37, 3012.58it/s]99639it [00:37, 3344.83it/s]103806it [00:38, 3340.15it/s]94862it [00:37, 3150.38it/s]89962it [00:37, 325.26it/s] 99976it [00:38, 3208.53it/s]104142it [00:38, 3250.86it/s]95197it [00:37, 3087.89it/s]90301it [00:37, 443.35it/s]100331it [00:38, 3303.05it/s]104497it [00:38, 3336.10it/s]95534it [00:38, 3165.67it/s]90649it [00:37, 601.82it/s]100684it [00:38, 3366.98it/s]104854it [00:38, 3403.87it/s]95861it [00:38, 3095.06it/s]90943it [00:37, 762.65it/s]101023it [00:38, 3255.71it/s]105196it [00:38, 3286.92it/s]96213it [00:38, 3214.41it/s]91291it [00:38, 1007.20it/s]101377it [00:38, 3336.81it/s]105550it [00:38, 3358.97it/s]96551it [00:38, 3137.23it/s]91600it [00:38, 1232.87it/s]101713it [00:38, 3216.81it/s]105888it [00:38, 3260.47it/s]96901it [00:38, 3231.33it/s]91947it [00:38, 1543.26it/s]102069it [00:38, 3314.13it/s]106245it [00:38, 3349.36it/s]97253it [00:38, 3312.05it/s]92282it [00:38, 1839.52it/s]102424it [00:38, 3380.56it/s]106591it [00:38, 3379.03it/s]97588it [00:38, 3206.50it/s]92602it [00:38, 2057.91it/s]102764it [00:38, 3257.44it/s]106930it [00:39, 3278.66it/s]97939it [00:38, 3291.96it/s]92952it [00:38, 2361.06it/s]103105it [00:38, 3300.05it/s]107286it [00:39, 3358.27it/s]98271it [00:38, 3184.28it/s]93275it [00:38, 2494.90it/s]103437it [00:39, 3194.78it/s]107623it [00:39, 3259.15it/s]98611it [00:39, 3243.84it/s]93624it [00:38, 2735.83it/s]103792it [00:39, 3295.52it/s]107982it [00:39, 3353.42it/s]98962it [00:39, 3319.47it/s]93972it [00:38, 2927.11it/s]104124it [00:39, 3194.73it/s]108319it [00:39, 3257.46it/s]99296it [00:39, 3207.40it/s]94303it [00:39, 2894.19it/s]104478it [00:39, 3292.84it/s]108679it [00:39, 3353.64it/s]99647it [00:39, 3294.15it/s]94653it [00:39, 3055.01it/s]104833it [00:39, 3366.49it/s]109039it [00:39, 3425.14it/s]99978it [00:39, 3191.64it/s]94979it [00:39, 3000.42it/s]105171it [00:39, 3236.12it/s]109383it [00:39, 3303.61it/s]100319it [00:39, 3253.66it/s]95326it [00:39, 3129.33it/s]105523it [00:39, 3316.85it/s]109729it [00:39, 3348.24it/s]100671it [00:39, 3330.41it/s]95672it [00:39, 3222.66it/s]105857it [00:39, 3213.50it/s]110066it [00:39, 3243.46it/s]101006it [00:39, 3206.29it/s]96003it [00:39, 3076.45it/s]106201it [00:39, 3277.51it/s]110424it [00:40, 3338.06it/s]101357it [00:39, 3291.67it/s]96352it [00:39, 3191.77it/s]106556it [00:40, 3356.26it/s]110782it [00:40, 3406.24it/s]101688it [00:39, 3109.23it/s]96677it [00:39, 3102.70it/s]106893it [00:40, 3244.04it/s]111124it [00:40, 3294.84it/s]102039it [00:40, 3221.45it/s]97023it [00:39, 3203.82it/s]107246it [00:40, 3326.04it/s]111482it [00:40, 3374.35it/s]102389it [00:40, 3301.23it/s]97372it [00:39, 3284.84it/s]107580it [00:40, 3220.76it/s]111821it [00:40, 3266.22it/s]102722it [00:40, 3190.18it/s]97704it [00:40, 3163.46it/s]107939it [00:40, 3325.91it/s]112181it [00:40, 3361.42it/s]103074it [00:40, 3283.39it/s]98036it [00:40, 3207.52it/s]108299it [00:40, 3404.29it/s]112519it [00:40, 3264.96it/s]103405it [00:40, 3186.17it/s]98359it [00:40, 3114.77it/s]108641it [00:40, 3288.36it/s]112847it [00:40, 3233.24it/s]103735it [00:40, 3184.77it/s]98709it [00:40, 3223.67it/s]109001it [00:40, 3377.69it/s]113207it [00:40, 3337.63it/s]104088it [00:40, 3283.47it/s]99058it [00:40, 3300.22it/s]113542it [00:41, 3243.57it/s]109341it [00:40, 3235.91it/s]104418it [00:40, 3178.01it/s]99390it [00:40, 3178.83it/s]113903it [00:41, 3346.87it/s]109697it [00:40, 3327.44it/s]104771it [00:40, 3277.78it/s]99722it [00:40, 3216.86it/s]114239it [00:41, 3258.27it/s]110032it [00:41, 3224.45it/s]105101it [00:41, 3169.66it/s]100046it [00:40, 3123.07it/s]114599it [00:41, 3356.48it/s]110390it [00:41, 3324.37it/s]105451it [00:41, 3261.90it/s]100395it [00:40, 3227.24it/s]114957it [00:41, 3420.57it/s]110748it [00:41, 3396.88it/s]105791it [00:41, 3170.55it/s]100745it [00:41, 3305.80it/s]115301it [00:41, 3306.37it/s]111090it [00:41, 3278.98it/s]106145it [00:41, 3273.45it/s]101077it [00:41, 3181.08it/s]115660it [00:41, 3386.62it/s]111447it [00:41, 3361.96it/s]106474it [00:41, 3274.92it/s]101425it [00:41, 3264.76it/s]116000it [00:41, 3286.78it/s]111785it [00:41, 3249.60it/s]106803it [00:41, 3180.30it/s]101754it [00:41, 3096.75it/s]116362it [00:41, 3380.82it/s]112145it [00:41, 3347.58it/s]107155it [00:41, 3276.14it/s]102103it [00:41, 3207.16it/s]112504it [00:41, 3416.13it/s]116713it [00:41, 3278.71it/s]107484it [00:41, 3173.09it/s]102432it [00:41, 3109.80it/s]117077it [00:42, 3380.12it/s]112847it [00:41, 3264.13it/s]107840it [00:41, 3281.96it/s]102782it [00:41, 3216.82it/s]117417it [00:42, 3362.78it/s]113207it [00:42, 3357.86it/s]108195it [00:41, 3358.71it/s]103132it [00:41, 3297.01it/s]117755it [00:42, 3274.23it/s]113545it [00:42, 3251.46it/s]108533it [00:42, 3201.74it/s]103464it [00:41, 3139.79it/s]118113it [00:42, 3352.40it/s]113905it [00:42, 3348.55it/s]108890it [00:42, 3305.68it/s]103812it [00:41, 3234.16it/s]118450it [00:42, 3263.05it/s]114242it [00:42, 3253.24it/s]109223it [00:42, 3201.46it/s]118809it [00:42, 3356.48it/s]104138it [00:42, 3126.38it/s]114602it [00:42, 3350.96it/s]109577it [00:42, 3295.56it/s]119163it [00:42, 3407.63it/s]104485it [00:42, 3223.36it/s]114959it [00:42, 3413.95it/s]109929it [00:42, 3358.32it/s]104835it [00:42, 3301.58it/s]119505it [00:42, 3294.48it/s]115302it [00:42, 3292.33it/s]110267it [00:42, 3241.54it/s]119863it [00:42, 3367.20it/s]105167it [00:42, 3165.82it/s]115660it [00:42, 3374.38it/s]110603it [00:42, 3275.43it/s]105501it [00:42, 3213.80it/s]115999it [00:42, 3242.33it/s]110932it [00:42, 3179.46it/s]105825it [00:42, 3109.20it/s]116362it [00:42, 3350.46it/s]111287it [00:42, 3285.77it/s]106175it [00:42, 3220.32it/s]116713it [00:43, 3251.81it/s]111639it [00:43, 3353.43it/s]106525it [00:42, 3300.18it/s]117077it [00:43, 3359.60it/s]111976it [00:43, 3234.21it/s]106857it [00:42, 3176.30it/s]117434it [00:43, 3418.73it/s]112331it [00:43, 3322.92it/s]107192it [00:43, 3219.13it/s]117778it [00:43, 3299.48it/s]112665it [00:43, 3175.00it/s]107516it [00:43, 3125.90it/s]118135it [00:43, 3375.79it/s]113019it [00:43, 3277.75it/s]107868it [00:43, 3236.19it/s]118475it [00:43, 3266.83it/s]113351it [00:43, 3184.96it/s]108218it [00:43, 3312.58it/s]118834it [00:43, 3357.29it/s]113704it [00:43, 3282.43it/s]108551it [00:43, 3186.06it/s]119179it [00:43, 3383.99it/s]114060it [00:43, 3361.69it/s]108904it [00:43, 3283.16it/s]119519it [00:43, 3260.93it/s]114398it [00:43, 3251.43it/s]109234it [00:43, 3124.33it/s]119876it [00:44, 3348.79it/s]114734it [00:43, 3282.54it/s]109584it [00:43, 3230.12it/s]115064it [00:44, 3185.67it/s]109933it [00:43, 3304.49it/s]115417it [00:44, 3284.54it/s]110266it [00:43, 3187.48it/s]115772it [00:44, 3359.89it/s]110616it [00:44, 3275.77it/s]116110it [00:44, 3240.56it/s]110946it [00:44, 3116.93it/s]116468it [00:44, 3336.31it/s]111297it [00:44, 3226.55it/s]116804it [00:44, 3188.78it/s]111644it [00:44, 3295.97it/s]117159it [00:44, 3290.20it/s]111976it [00:44, 3173.40it/s]117512it [00:44, 3358.49it/s]112326it [00:44, 3266.26it/s]117850it [00:44, 3245.35it/s]112655it [00:44, 3150.31it/s]118202it [00:45, 3322.76it/s]112991it [00:44, 3209.18it/s]118536it [00:45, 3178.11it/s]113342it [00:44, 3294.70it/s]118889it [00:45, 3277.72it/s]113673it [00:45, 3169.82it/s]119231it [00:45, 3181.57it/s]114025it [00:45, 3268.35it/s]119582it [00:45, 3273.96it/s]114354it [00:45, 3161.83it/s]119925it [00:45, 3317.50it/s]114704it [00:45, 3216.86it/s]115032it [00:45, 3124.93it/s]120201it [00:46, 340.23it/s] 115382it [00:45, 3229.92it/s]120562it [00:46, 471.85it/s]115732it [00:45, 3298.28it/s]120910it [00:46, 635.45it/s]116064it [00:45, 3173.09it/s]121208it [00:46, 804.15it/s]116417it [00:45, 3273.49it/s]121566it [00:46, 1063.19it/s]116746it [00:46, 3111.73it/s]121882it [00:46, 1302.89it/s]117098it [00:46, 3226.30it/s]122242it [00:46, 1631.42it/s]117446it [00:46, 3297.59it/s]122601it [00:46, 1963.59it/s]117778it [00:46, 3171.05it/s]122937it [00:46, 2198.58it/s]118125it [00:46, 3256.19it/s]123295it [00:46, 2494.80it/s]118453it [00:46, 3101.46it/s]123632it [00:47, 2639.53it/s]118801it [00:46, 3206.32it/s]123993it [00:47, 2872.71it/s]119146it [00:46, 3273.57it/s]124343it [00:47, 2901.86it/s]119476it [00:46, 3137.87it/s]124701it [00:47, 3077.88it/s]119821it [00:46, 3225.68it/s]125058it [00:47, 3211.07it/s]125400it [00:47, 3162.32it/s]125760it [00:47, 3282.93it/s]120213it [00:47, 299.24it/s] 126100it [00:47, 3215.83it/s]120574it [00:47, 417.48it/s]126457it [00:47, 3315.79it/s]120930it [00:47, 570.03it/s]126815it [00:48, 3389.19it/s]121230it [00:47, 726.88it/s]127159it [00:48, 3290.01it/s]121565it [00:48, 949.05it/s]127520it [00:48, 3380.27it/s]121875it [00:48, 1175.08it/s]127861it [00:48, 3278.15it/s]122231it [00:48, 1492.03it/s]128201it [00:48, 3310.97it/s]122590it [00:48, 1826.22it/s]128543it [00:48, 3235.56it/s]122922it [00:48, 2063.19it/s]128901it [00:48, 3334.12it/s]123278it [00:48, 2371.32it/s]129261it [00:48, 3409.60it/s]123611it [00:48, 2524.27it/s]129604it [00:48, 3301.77it/s]123967it [00:48, 2772.89it/s]129963it [00:48, 3383.52it/s]124324it [00:48, 2974.79it/s]130303it [00:49, 3280.72it/s]124664it [00:48, 2981.00it/s]130661it [00:49, 3364.92it/s]125019it [00:49, 3132.80it/s]131019it [00:49, 3426.15it/s]125355it [00:49, 3043.62it/s]131363it [00:49, 3308.29it/s]125711it [00:49, 3183.14it/s]131705it [00:49, 3338.66it/s]126042it [00:49, 3121.75it/s]132041it [00:49, 3254.75it/s]126399it [00:49, 3245.52it/s]120259it [00:49, 276.46it/s] 132397it [00:49, 3340.95it/s]126753it [00:49, 3329.23it/s]120616it [00:49, 386.66it/s]132743it [00:49, 3252.70it/s]127092it [00:49, 3222.93it/s]120967it [00:49, 529.16it/s]133103it [00:49, 3349.81it/s]127451it [00:49, 3324.99it/s]121263it [00:49, 676.52it/s]133462it [00:50, 3417.91it/s]121614it [00:49, 903.84it/s]127787it [00:49, 3210.96it/s]133805it [00:50, 3308.81it/s]128140it [00:50, 3301.58it/s]121925it [00:49, 1118.11it/s]134164it [00:50, 3389.29it/s]128495it [00:50, 3373.22it/s]122278it [00:50, 1423.93it/s]134505it [00:50, 3280.50it/s]122634it [00:50, 1750.88it/s]128835it [00:50, 3212.93it/s]134864it [00:50, 3368.05it/s]122963it [00:50, 1984.55it/s]129191it [00:50, 3310.72it/s]135225it [00:50, 3436.67it/s]123316it [00:50, 2294.05it/s]129525it [00:50, 3211.29it/s]135570it [00:50, 3288.98it/s]123645it [00:50, 2450.12it/s]129882it [00:50, 3311.69it/s]135930it [00:50, 3376.43it/s]123985it [00:50, 2674.00it/s]130223it [00:50, 3211.22it/s]136270it [00:50, 3277.37it/s]124337it [00:50, 2885.70it/s]130579it [00:50, 3308.06it/s]136630it [00:50, 3368.64it/s]124670it [00:50, 2895.63it/s]130933it [00:50, 3374.30it/s]136969it [00:51, 3268.77it/s]125022it [00:50, 3060.48it/s]131272it [00:50, 3261.95it/s]137329it [00:51, 3362.17it/s]125352it [00:50, 3016.09it/s]131624it [00:51, 3334.44it/s]137690it [00:51, 3432.16it/s]125704it [00:51, 3154.44it/s]131959it [00:51, 3233.92it/s]138035it [00:51, 3310.68it/s]132298it [00:51, 3278.40it/s]126033it [00:51, 3061.33it/s]138394it [00:51, 3390.18it/s]132654it [00:51, 3359.49it/s]126386it [00:51, 3190.91it/s]138735it [00:51, 3252.36it/s]126737it [00:51, 3279.41it/s]132992it [00:51, 3250.32it/s]139096it [00:51, 3353.07it/s]120146it [00:51, 248.56it/s] 133348it [00:51, 3339.17it/s]127071it [00:51, 3180.03it/s]139458it [00:51, 3428.67it/s]120497it [00:51, 348.81it/s]127424it [00:51, 3279.43it/s]133684it [00:51, 3240.46it/s]120845it [00:51, 480.51it/s]139803it [00:51, 3317.96it/s]134042it [00:51, 3336.85it/s]127756it [00:51, 3136.28it/s]121136it [00:51, 617.10it/s]140162it [00:52, 3391.83it/s]134398it [00:51, 3400.24it/s]128106it [00:51, 3237.34it/s]121484it [00:51, 831.17it/s]140503it [00:52, 3298.04it/s]128457it [00:51, 3313.18it/s]134740it [00:52, 3279.73it/s]140862it [00:52, 3379.11it/s]121822it [00:51, 1059.83it/s]135098it [00:52, 3364.25it/s]128791it [00:52, 3198.70it/s]122172it [00:51, 1349.82it/s]141202it [00:52, 3290.26it/s]135436it [00:52, 3266.39it/s]129135it [00:52, 3251.54it/s]122516it [00:51, 1654.17it/s]141562it [00:52, 3372.25it/s]135794it [00:52, 3355.26it/s]129462it [00:52, 3154.17it/s]141922it [00:52, 3438.17it/s]122840it [00:52, 1886.93it/s]129814it [00:52, 3257.30it/s]136131it [00:52, 3208.30it/s]123187it [00:52, 2193.31it/s]142267it [00:52, 3289.87it/s]130166it [00:52, 3331.93it/s]136488it [00:52, 3309.53it/s]142623it [00:52, 3367.05it/s]123510it [00:52, 2353.61it/s]136846it [00:52, 3386.36it/s]130501it [00:52, 3207.76it/s]123856it [00:52, 2609.37it/s]142962it [00:52, 3268.56it/s]130851it [00:52, 3290.39it/s]137187it [00:52, 3274.18it/s]124188it [00:52, 2786.01it/s]143322it [00:52, 3361.56it/s]137545it [00:52, 3361.13it/s]131182it [00:52, 3161.83it/s]124514it [00:52, 2820.98it/s]143662it [00:53, 3272.29it/s]131535it [00:52, 3266.19it/s]137883it [00:52, 3257.23it/s]124866it [00:52, 3006.22it/s]144023it [00:53, 3367.23it/s]131889it [00:52, 3344.77it/s]138241it [00:53, 3347.75it/s]144384it [00:53, 3435.73it/s]125192it [00:52, 2975.50it/s]138596it [00:53, 3405.30it/s]132226it [00:53, 3232.88it/s]125544it [00:52, 3125.41it/s]144729it [00:53, 3338.18it/s]132582it [00:53, 3325.65it/s]138938it [00:53, 3286.50it/s]125895it [00:52, 3233.13it/s]145091it [00:53, 3417.04it/s]139297it [00:53, 3372.90it/s]132917it [00:53, 3186.92it/s]126229it [00:53, 3098.61it/s]145434it [00:53, 3280.42it/s]133273it [00:53, 3292.27it/s]139636it [00:53, 3227.67it/s]126579it [00:53, 3209.70it/s]145798it [00:53, 3380.91it/s]139996it [00:53, 3331.69it/s]133605it [00:53, 3198.95it/s]146163it [00:53, 3457.74it/s]126907it [00:53, 3111.67it/s]133963it [00:53, 3305.89it/s]140332it [00:53, 3236.58it/s]127258it [00:53, 3223.58it/s]146511it [00:53, 3335.63it/s]134319it [00:53, 3378.02it/s]140690it [00:53, 3334.06it/s]127611it [00:53, 3310.39it/s]146875it [00:54, 3421.88it/s]141048it [00:53, 3405.26it/s]134659it [00:53, 3221.81it/s]147219it [00:54, 3312.93it/s]127946it [00:53, 3187.93it/s]141390it [00:54, 3290.56it/s]135016it [00:53, 3319.50it/s]147582it [00:54, 3394.81it/s]128297it [00:53, 3278.04it/s]141747it [00:54, 3368.72it/s]135351it [00:54, 3224.61it/s]147923it [00:54, 3303.24it/s]128628it [00:53, 3119.24it/s]142086it [00:54, 3263.82it/s]135710it [00:54, 3328.01it/s]148282it [00:54, 3385.39it/s]128980it [00:53, 3229.77it/s]142443it [00:54, 3349.24it/s]136069it [00:54, 3401.37it/s]148631it [00:54, 3414.38it/s]129332it [00:54, 3311.41it/s]142796it [00:54, 3399.19it/s]136411it [00:54, 3273.84it/s]148974it [00:54, 3311.76it/s]129666it [00:54, 3185.49it/s]143138it [00:54, 3282.28it/s]136754it [00:54, 3317.08it/s]149333it [00:54, 3386.11it/s]130018it [00:54, 3280.18it/s]143478it [00:54, 3314.50it/s]137088it [00:54, 3216.98it/s]149673it [00:54, 3299.90it/s]130349it [00:54, 3178.50it/s]143811it [00:54, 3228.17it/s]137445it [00:54, 3316.24it/s]150033it [00:54, 3380.55it/s]130682it [00:54, 3221.28it/s]144168it [00:54, 3324.49it/s]137781it [00:54, 3216.32it/s]150383it [00:55, 3289.69it/s]131034it [00:54, 3305.69it/s]144502it [00:54, 3235.10it/s]138140it [00:54, 3320.77it/s]150746it [00:55, 3385.88it/s]131366it [00:54, 3176.03it/s]144863it [00:55, 3342.69it/s]138481it [00:55, 3346.34it/s]151106it [00:55, 3446.07it/s]131715it [00:54, 3264.55it/s]145224it [00:55, 3419.64it/s]138817it [00:55, 3243.34it/s]151452it [00:55, 3328.40it/s]132044it [00:54, 3149.33it/s]145568it [00:55, 3306.31it/s]139177it [00:55, 3343.52it/s]151800it [00:55, 3370.21it/s]132397it [00:55, 3256.09it/s]145931it [00:55, 3397.21it/s]139513it [00:55, 3239.41it/s]152139it [00:55, 3281.46it/s]132742it [00:55, 3116.05it/s]146273it [00:55, 3287.55it/s]139874it [00:55, 3344.33it/s]152501it [00:55, 3376.47it/s]133094it [00:55, 3227.56it/s]146630it [00:55, 3367.31it/s]140218it [00:55, 3371.77it/s]152863it [00:55, 3446.97it/s]133446it [00:55, 3308.47it/s]146974it [00:55, 3386.95it/s]140557it [00:55, 3256.86it/s]153209it [00:55, 3336.79it/s]133780it [00:55, 3199.62it/s]147314it [00:55, 3277.85it/s]140915it [00:55, 3349.06it/s]153570it [00:56, 3414.79it/s]134134it [00:55, 3296.07it/s]147674it [00:55, 3368.49it/s]141252it [00:55, 3245.12it/s]153913it [00:56, 3316.39it/s]134466it [00:55, 3179.70it/s]148013it [00:56, 3267.68it/s]141610it [00:55, 3339.72it/s]154278it [00:56, 3409.88it/s]134820it [00:55, 3282.04it/s]148371it [00:56, 3355.68it/s]141954it [00:56, 3367.80it/s]154621it [00:56, 3306.52it/s]135156it [00:55, 3304.44it/s]148708it [00:56, 3247.91it/s]142292it [00:56, 3251.43it/s]154984it [00:56, 3398.70it/s]135488it [00:55, 3190.25it/s]149068it [00:56, 3346.71it/s]142648it [00:56, 3339.28it/s]155336it [00:56, 3431.35it/s]135841it [00:56, 3286.94it/s]149426it [00:56, 3411.91it/s]142984it [00:56, 3229.69it/s]155681it [00:56, 3322.45it/s]136172it [00:56, 3181.73it/s]149769it [00:56, 3301.66it/s]143342it [00:56, 3291.51it/s]156043it [00:56, 3407.76it/s]136526it [00:56, 3282.99it/s]150127it [00:56, 3379.76it/s]143673it [00:56, 3199.65it/s]156386it [00:56, 3314.14it/s]136879it [00:56, 3353.15it/s]150467it [00:56, 3274.21it/s]144032it [00:56, 3308.65it/s]156751it [00:56, 3408.71it/s]137216it [00:56, 3192.58it/s]150808it [00:56, 3311.17it/s]144391it [00:56, 3387.80it/s]157103it [00:57, 3310.39it/s]137571it [00:56, 3292.45it/s]151166it [00:56, 3387.33it/s]144731it [00:56, 3280.20it/s]157471it [00:57, 3416.02it/s]137903it [00:56, 3180.98it/s]151506it [00:57, 3270.59it/s]145075it [00:57, 3324.99it/s]157836it [00:57, 3482.60it/s]138259it [00:56, 3287.81it/s]151866it [00:57, 3364.72it/s]145409it [00:57, 3233.81it/s]138613it [00:56, 3358.89it/s]152204it [00:57, 3266.32it/s]145769it [00:57, 3336.69it/s]138951it [00:57, 3228.55it/s]152565it [00:57, 3363.92it/s]146131it [00:57, 3418.27it/s]139304it [00:57, 3313.71it/s]152903it [00:57, 3263.53it/s]146474it [00:57, 3289.84it/s]139638it [00:57, 3155.73it/s]153266it [00:57, 3366.39it/s]146820it [00:57, 3336.79it/s]139993it [00:57, 3265.95it/s]153628it [00:57, 3439.42it/s]147155it [00:57, 3236.52it/s]140323it [00:57, 3157.20it/s]153974it [00:57, 3325.09it/s]147513it [00:57, 3334.95it/s]140676it [00:57, 3261.76it/s]154321it [00:57, 3366.47it/s]147862it [00:57, 3237.30it/s]141030it [00:57, 3339.69it/s]154659it [00:58, 3270.29it/s]148219it [00:57, 3331.25it/s]141366it [00:57, 3218.01it/s]155022it [00:58, 3373.76it/s]148562it [00:58, 3358.38it/s]141703it [00:57, 3254.27it/s]155386it [00:58, 3451.09it/s]148900it [00:58, 3252.69it/s]142030it [00:57, 3161.02it/s]155733it [00:58, 3329.65it/s]149258it [00:58, 3346.28it/s]142384it [00:58, 3267.86it/s]156096it [00:58, 3414.11it/s]149595it [00:58, 3240.84it/s]142737it [00:58, 3343.74it/s]156439it [00:58, 3305.83it/s]149953it [00:58, 3336.82it/s]143073it [00:58, 3210.84it/s]156805it [00:58, 3405.52it/s]150311it [00:58, 3406.97it/s]143426it [00:58, 3301.79it/s]157148it [00:58, 3303.48it/s]150653it [00:58, 3249.38it/s]143758it [00:58, 3189.68it/s]157517it [00:58, 3412.34it/s]151011it [00:58, 3341.71it/s]144095it [00:58, 3241.16it/s]157878it [00:58, 3468.41it/s]151348it [00:58, 3230.96it/s]144449it [00:58, 3327.21it/s]151707it [00:59, 3332.99it/s]144784it [00:58, 3221.80it/s]152062it [00:59, 3239.04it/s]145142it [00:58, 3323.12it/s]152411it [00:59, 3308.05it/s]145476it [00:59, 3220.54it/s]152769it [00:59, 3383.63it/s]145833it [00:59, 3318.26it/s]153109it [00:59, 3281.84it/s]146178it [00:59, 3355.01it/s]153467it [00:59, 3366.33it/s]146515it [00:59, 3221.94it/s]153806it [00:59, 3270.24it/s]146873it [00:59, 3324.09it/s]154154it [00:59, 3330.38it/s]147208it [00:59, 3206.95it/s]154514it [00:59, 3407.44it/s]147567it [00:59, 3314.06it/s]154856it [00:59, 3299.69it/s]147901it [00:59, 3202.31it/s]155217it [01:00, 3389.04it/s]148243it [00:59, 3250.76it/s]155558it [01:00, 3245.66it/s]148597it [00:59, 3334.02it/s]155917it [01:00, 3341.23it/s]148932it [01:00, 3211.49it/s]156262it [01:00, 3250.59it/s]149288it [01:00, 3309.96it/s]156624it [01:00, 3354.78it/s]149621it [01:00, 3200.83it/s]156989it [01:00, 3439.87it/s]149976it [01:00, 3300.23it/s]157335it [01:00, 3284.56it/s]150332it [01:00, 3373.59it/s]157701it [01:00, 3391.30it/s]150671it [01:00, 3202.42it/s]151024it [01:00, 3294.21it/s]151356it [01:00, 3191.84it/s]151712it [01:00, 3294.39it/s]158186it [01:01, 263.29it/s] 152063it [01:01, 3178.99it/s]158556it [01:01, 368.49it/s]152418it [01:01, 3281.42it/s]158864it [01:01, 482.07it/s]152758it [01:01, 3312.89it/s]159240it [01:01, 667.15it/s]153092it [01:01, 3191.42it/s]159609it [01:01, 892.78it/s]153448it [01:01, 3296.16it/s]159942it [01:02, 1120.46it/s]153780it [01:01, 3190.40it/s]160311it [01:02, 1429.50it/s]154140it [01:01, 3306.73it/s]160650it [01:02, 1697.17it/s]154499it [01:01, 3386.07it/s]161007it [01:02, 2018.20it/s]154840it [01:01, 3254.40it/s]161371it [01:02, 2270.29it/s]155182it [01:02, 3300.52it/s]161742it [01:02, 2578.74it/s]155514it [01:02, 3188.53it/s]162117it [01:02, 2852.91it/s]155873it [01:02, 3300.94it/s]162471it [01:02, 2940.37it/s]156232it [01:02, 3383.04it/s]162842it [01:02, 3138.95it/s]156572it [01:02, 3245.80it/s]163193it [01:03, 3153.16it/s]156932it [01:02, 3346.73it/s]163567it [01:03, 3312.18it/s]157269it [01:02, 3195.19it/s]163918it [01:03, 3269.91it/s]157630it [01:02, 3310.07it/s]164283it [01:03, 3373.96it/s]157964it [01:02, 3199.77it/s]164655it [01:03, 3472.06it/s]165010it [01:03, 3407.19it/s]165380it [01:03, 3489.32it/s]158227it [01:03, 247.76it/s] 165734it [01:03, 3403.19it/s]158593it [01:03, 346.68it/s]166112it [01:03, 3510.50it/s]158896it [01:03, 453.37it/s]166466it [01:03, 3419.00it/s]159267it [01:03, 628.38it/s]166845it [01:04, 3524.68it/s]159629it [01:03, 841.58it/s]167216it [01:04, 3578.18it/s]159957it [01:04, 1060.51it/s]167576it [01:04, 3439.37it/s]160320it [01:04, 1358.51it/s]167951it [01:04, 3528.00it/s]160654it [01:04, 1618.89it/s]168306it [01:04, 3430.62it/s]161018it [01:04, 1956.27it/s]168681it [01:04, 3521.93it/s]161372it [01:04, 2197.65it/s]169035it [01:04, 3419.97it/s]161723it [01:04, 2474.38it/s]169404it [01:04, 3497.48it/s]162094it [01:04, 2758.88it/s]169772it [01:04, 3395.88it/s]162440it [01:04, 2850.88it/s]170146it [01:04, 3492.82it/s]162809it [01:04, 3066.00it/s]170517it [01:05, 3554.16it/s]163155it [01:04, 3076.31it/s]170874it [01:05, 3449.80it/s]163525it [01:05, 3244.45it/s]171232it [01:05, 3487.00it/s]163892it [01:05, 3201.45it/s]171582it [01:05, 3390.52it/s]164265it [01:05, 3346.50it/s]171951it [01:05, 3476.00it/s]164631it [01:05, 3433.49it/s]172300it [01:05, 3379.90it/s]164983it [01:05, 3357.90it/s]172677it [01:05, 3489.94it/s]165331it [01:05, 3391.68it/s]173053it [01:05, 3567.91it/s]165675it [01:05, 3312.10it/s]173411it [01:05, 3453.94it/s]166051it [01:05, 3439.84it/s]173781it [01:06, 3524.33it/s]166412it [01:05, 3340.25it/s]158043it [01:05, 220.14it/s] 174135it [01:06, 3428.30it/s]166793it [01:06, 3472.30it/s]158409it [01:06, 309.72it/s]174503it [01:06, 3498.75it/s]167160it [01:06, 3528.96it/s]158776it [01:06, 430.23it/s]174855it [01:06, 3388.87it/s]167515it [01:06, 3414.10it/s]159081it [01:06, 556.39it/s]175231it [01:06, 3494.00it/s]167882it [01:06, 3485.26it/s]159447it [01:06, 757.89it/s]175596it [01:06, 3538.03it/s]168233it [01:06, 3387.07it/s]159767it [01:06, 960.58it/s]175951it [01:06, 3431.92it/s]168606it [01:06, 3484.92it/s]160131it [01:06, 1249.01it/s]176319it [01:06, 3502.64it/s]168957it [01:06, 3334.23it/s]160497it [01:06, 1569.94it/s]176671it [01:06, 3408.74it/s]169326it [01:06, 3435.06it/s]160838it [01:06, 1829.53it/s]177039it [01:06, 3485.61it/s]169693it [01:06, 3500.24it/s]161187it [01:06, 2132.70it/s]177389it [01:07, 3394.34it/s]170045it [01:06, 3377.61it/s]161523it [01:06, 2336.34it/s]177745it [01:07, 3441.12it/s]170414it [01:07, 3464.33it/s]161895it [01:07, 2646.39it/s]178121it [01:07, 3532.07it/s]170763it [01:07, 3360.33it/s]162235it [01:07, 2757.98it/s]178476it [01:07, 3423.81it/s]171130it [01:07, 3448.10it/s]162606it [01:07, 2998.59it/s]178854it [01:07, 3526.71it/s]171477it [01:07, 3306.52it/s]162974it [01:07, 3178.20it/s]179208it [01:07, 3417.16it/s]171845it [01:07, 3412.06it/s]163324it [01:07, 3122.12it/s]179577it [01:07, 3495.38it/s]172215it [01:07, 3494.34it/s]163692it [01:07, 3273.22it/s]179928it [01:07, 3388.65it/s]172567it [01:07, 3390.71it/s]164037it [01:07, 3221.56it/s]180311it [01:07, 3515.04it/s]172937it [01:07, 3478.48it/s]164408it [01:07, 3356.12it/s]180677it [01:08, 3556.45it/s]173287it [01:07, 3369.12it/s]164754it [01:07, 3274.35it/s]181034it [01:08, 3461.80it/s]173655it [01:08, 3457.98it/s]165131it [01:07, 3412.32it/s]181390it [01:08, 3489.38it/s]174003it [01:08, 3313.79it/s]165482it [01:08, 3439.42it/s]181740it [01:08, 3390.95it/s]174374it [01:08, 3424.36it/s]165830it [01:08, 3355.86it/s]182116it [01:08, 3495.14it/s]174749it [01:08, 3516.90it/s]166203it [01:08, 3463.50it/s]182467it [01:08, 3405.26it/s]175103it [01:08, 3399.17it/s]166552it [01:08, 3364.23it/s]182849it [01:08, 3523.09it/s]175471it [01:08, 3477.13it/s]158287it [01:08, 200.41it/s] 166927it [01:08, 3474.92it/s]183212it [01:08, 3410.32it/s]175821it [01:08, 3363.44it/s]158643it [01:08, 283.40it/s]167277it [01:08, 3363.25it/s]183587it [01:08, 3505.12it/s]176175it [01:08, 3411.47it/s]158937it [01:08, 373.95it/s]167635it [01:08, 3423.57it/s]183961it [01:08, 3571.95it/s]176518it [01:08, 3312.47it/s]159301it [01:08, 525.55it/s]168006it [01:08, 3505.39it/s]184320it [01:09, 3471.36it/s]176892it [01:08, 3434.63it/s]159662it [01:08, 717.37it/s]168358it [01:08, 3400.37it/s]184682it [01:09, 3511.29it/s]177258it [01:09, 3497.83it/s]159981it [01:08, 913.14it/s]168726it [01:09, 3480.48it/s]185035it [01:09, 3421.26it/s]177610it [01:09, 3375.73it/s]160322it [01:08, 1172.01it/s]169076it [01:09, 3366.75it/s]185399it [01:09, 3483.89it/s]177982it [01:09, 3472.04it/s]160643it [01:08, 1422.24it/s]169445it [01:09, 3458.68it/s]185749it [01:09, 3386.74it/s]178331it [01:09, 3380.01it/s]161001it [01:09, 1755.52it/s]169793it [01:09, 3326.65it/s]186133it [01:09, 3516.50it/s]161364it [01:09, 2093.71it/s]178671it [01:09, 3312.10it/s]170164it [01:09, 3435.25it/s]186505it [01:09, 3575.29it/s]179012it [01:09, 3254.63it/s]161702it [01:09, 2298.17it/s]170534it [01:09, 3509.87it/s]186864it [01:09, 3510.59it/s]179377it [01:09, 3365.79it/s]162067it [01:09, 2597.80it/s]187237it [01:09, 3572.71it/s]170887it [01:09, 3391.76it/s]179750it [01:09, 3469.72it/s]162405it [01:09, 2675.61it/s]171252it [01:09, 3463.30it/s]187596it [01:10, 3461.93it/s]180099it [01:09, 3354.68it/s]162770it [01:09, 2917.38it/s]187965it [01:10, 3527.63it/s]171600it [01:09, 3349.25it/s]180480it [01:10, 3483.90it/s]163106it [01:09, 2947.23it/s]171959it [01:09, 3417.49it/s]188319it [01:10, 3426.54it/s]180830it [01:10, 3380.82it/s]163471it [01:09, 3132.59it/s]188698it [01:10, 3529.42it/s]172303it [01:10, 3314.04it/s]181197it [01:10, 3463.74it/s]163832it [01:09, 3261.76it/s]189067it [01:10, 3575.96it/s]172677it [01:10, 3434.68it/s]181545it [01:10, 3345.24it/s]164177it [01:10, 3195.29it/s]173050it [01:10, 3520.26it/s]189426it [01:10, 3468.58it/s]181916it [01:10, 3447.09it/s]164539it [01:10, 3313.80it/s]189795it [01:10, 3532.56it/s]173404it [01:10, 3396.67it/s]182295it [01:10, 3545.78it/s]164881it [01:10, 3202.43it/s]173771it [01:10, 3475.04it/s]190150it [01:10, 3443.94it/s]182652it [01:10, 3391.33it/s]165247it [01:10, 3330.78it/s]190521it [01:10, 3519.55it/s]174121it [01:10, 3346.26it/s]183020it [01:10, 3471.13it/s]165587it [01:10, 3246.19it/s]190874it [01:10, 3413.68it/s]174498it [01:10, 3467.01it/s]183370it [01:10, 3385.46it/s]165958it [01:10, 3375.96it/s]191249it [01:11, 3509.56it/s]174847it [01:10, 3345.89it/s]183740it [01:10, 3473.10it/s]166327it [01:10, 3465.91it/s]191610it [01:11, 3538.16it/s]175221it [01:10, 3456.37it/s]184089it [01:11, 3374.23it/s]166677it [01:10, 3353.37it/s]191965it [01:11, 3452.44it/s]175584it [01:11, 3504.44it/s]184465it [01:11, 3482.67it/s]167026it [01:10, 3390.91it/s]192338it [01:11, 3530.52it/s]175936it [01:11, 3365.94it/s]184837it [01:11, 3551.01it/s]167368it [01:10, 3282.78it/s]192692it [01:11, 3454.65it/s]176303it [01:11, 3450.63it/s]185194it [01:11, 3408.59it/s]167735it [01:11, 3393.44it/s]193068it [01:11, 3541.00it/s]176650it [01:11, 3343.14it/s]185554it [01:11, 3461.29it/s]168093it [01:11, 3292.67it/s]193423it [01:11, 3428.54it/s]177019it [01:11, 3441.91it/s]185902it [01:11, 3376.85it/s]168462it [01:11, 3404.96it/s]193798it [01:11, 3519.26it/s]177365it [01:11, 3342.05it/s]186280it [01:11, 3490.87it/s]168829it [01:11, 3480.56it/s]194152it [01:11, 3435.31it/s]177729it [01:11, 3426.46it/s]186631it [01:11, 3398.88it/s]169179it [01:11, 3337.17it/s]194533it [01:11, 3540.97it/s]178089it [01:11, 3474.59it/s]187015it [01:11, 3525.62it/s]169525it [01:11, 3371.22it/s]194895it [01:12, 3563.55it/s]178438it [01:11, 3362.66it/s]187387it [01:12, 3581.10it/s]169864it [01:11, 3262.13it/s]195253it [01:12, 3488.49it/s]178809it [01:11, 3461.41it/s]187747it [01:12, 3422.11it/s]170232it [01:11, 3380.46it/s]195628it [01:12, 3563.07it/s]179157it [01:12, 3351.93it/s]188122it [01:12, 3514.45it/s]170595it [01:11, 3451.44it/s]195986it [01:12, 3437.59it/s]179522it [01:12, 3435.00it/s]188476it [01:12, 3407.70it/s]170942it [01:12, 3326.14it/s]196361it [01:12, 3525.67it/s]179867it [01:12, 3345.18it/s]188850it [01:12, 3501.05it/s]171306it [01:12, 3415.63it/s]196715it [01:12, 3450.75it/s]180244it [01:12, 3466.87it/s]189202it [01:12, 3389.81it/s]197092it [01:12, 3541.52it/s]171650it [01:12, 3249.96it/s]180593it [01:12, 3424.11it/s]189571it [01:12, 3473.26it/s]197472it [01:12, 3616.05it/s]172014it [01:12, 3358.86it/s]180937it [01:12, 3342.35it/s]189932it [01:12, 3379.07it/s]197835it [01:12, 3511.51it/s]172353it [01:12, 3255.56it/s]181303it [01:12, 3432.16it/s]190311it [01:12, 3494.28it/s]198200it [01:13, 3548.89it/s]172719it [01:12, 3368.16it/s]181648it [01:12, 3331.76it/s]190663it [01:12, 3498.37it/s]198556it [01:13, 3458.43it/s]173091it [01:12, 3469.59it/s]182021it [01:12, 3442.72it/s]191015it [01:13, 3386.84it/s]198931it [01:13, 3541.17it/s]173440it [01:12, 3329.34it/s]182371it [01:13, 3346.61it/s]191386it [01:13, 3477.97it/s]199287it [01:13, 3449.70it/s]173788it [01:12, 3370.53it/s]182752it [01:13, 3478.09it/s]191736it [01:13, 3379.92it/s]199669it [01:13, 3554.36it/s]174127it [01:12, 3275.96it/s]183121it [01:13, 3537.71it/s]192115it [01:13, 3497.22it/s]200026it [01:13, 3416.42it/s]174499it [01:13, 3401.68it/s]183477it [01:13, 3349.47it/s]192467it [01:13, 3395.26it/s]200397it [01:13, 3499.20it/s]174841it [01:13, 3276.42it/s]183845it [01:13, 3441.18it/s]192848it [01:13, 3511.84it/s]200769it [01:13, 3561.92it/s]175210it [01:13, 3393.57it/s]184192it [01:13, 3348.74it/s]193222it [01:13, 3575.70it/s]201127it [01:13, 3443.74it/s]175569it [01:13, 3450.04it/s]184567it [01:13, 3461.79it/s]193581it [01:13, 3376.46it/s]201496it [01:13, 3512.59it/s]175916it [01:13, 3329.81it/s]184916it [01:13, 3369.76it/s]193962it [01:13, 3498.87it/s]201849it [01:14, 3400.90it/s]176261it [01:13, 3361.91it/s]185286it [01:13, 3462.59it/s]194315it [01:14, 3409.58it/s]202231it [01:14, 3518.72it/s]176599it [01:13, 3248.62it/s]185651it [01:13, 3516.18it/s]194688it [01:14, 3499.28it/s]202585it [01:14, 3461.55it/s]176967it [01:13, 3371.21it/s]186004it [01:14, 3364.66it/s]202972it [01:14, 3577.26it/s]195040it [01:14, 3388.80it/s]177332it [01:13, 3269.12it/s]186380it [01:14, 3475.21it/s]203347it [01:14, 3625.23it/s]195429it [01:14, 3530.35it/s]177691it [01:14, 3358.79it/s]186730it [01:14, 3406.62it/s]195804it [01:14, 3592.95it/s]203711it [01:14, 3494.53it/s]178061it [01:14, 3454.88it/s]187103it [01:14, 3498.56it/s]204080it [01:14, 3549.99it/s]196165it [01:14, 3437.20it/s]178409it [01:14, 3292.98it/s]187455it [01:14, 3394.96it/s]196520it [01:14, 3468.71it/s]204437it [01:14, 3442.78it/s]178778it [01:14, 3404.18it/s]187830it [01:14, 3495.88it/s]196869it [01:14, 3408.39it/s]204814it [01:14, 3535.63it/s]188203it [01:14, 3562.05it/s]179121it [01:14, 3281.99it/s]197243it [01:14, 3503.45it/s]179480it [01:14, 3367.33it/s]188561it [01:14, 3404.74it/s]197595it [01:14, 3414.46it/s]188930it [01:14, 3485.98it/s]179852it [01:14, 3286.25it/s]197981it [01:15, 3541.36it/s]180223it [01:14, 3404.53it/s]189281it [01:15, 3376.26it/s]198337it [01:15, 3428.52it/s]180588it [01:14, 3472.21it/s]189648it [01:15, 3459.59it/s]198715it [01:15, 3527.59it/s]189996it [01:15, 3371.81it/s]180938it [01:15, 3311.48it/s]199093it [01:15, 3598.81it/s]190369it [01:15, 3474.00it/s]181300it [01:15, 3397.44it/s]199455it [01:15, 3432.75it/s]190740it [01:15, 3540.76it/s]181643it [01:15, 3287.66it/s]199830it [01:15, 3522.40it/s]191096it [01:15, 3368.40it/s]182014it [01:15, 3405.48it/s]200185it [01:15, 3397.91it/s]191470it [01:15, 3473.18it/s]182372it [01:15, 3297.99it/s]200556it [01:15, 3485.18it/s]191820it [01:15, 3379.48it/s]182749it [01:15, 3431.01it/s]200907it [01:15, 3368.56it/s]192194it [01:15, 3480.58it/s]183096it [01:15, 3441.81it/s]201276it [01:16, 3459.61it/s]192544it [01:15, 3390.74it/s]183442it [01:15, 3330.26it/s]201653it [01:16, 3548.46it/s]192924it [01:16, 3505.60it/s]183807it [01:15, 3421.25it/s]202010it [01:16, 3440.54it/s]193291it [01:16, 3397.57it/s]184151it [01:15, 3309.44it/s]202378it [01:16, 3508.43it/s]193638it [01:16, 3417.89it/s]184519it [01:16, 3412.88it/s]202731it [01:16, 3437.54it/s]194020it [01:16, 3533.80it/s]184890it [01:16, 3497.97it/s]203114it [01:16, 3549.62it/s]194375it [01:16, 3418.72it/s]185242it [01:16, 3320.67it/s]203471it [01:16, 3432.81it/s]194746it [01:16, 3501.14it/s]185596it [01:16, 3382.46it/s]203843it [01:16, 3515.17it/s]195098it [01:16, 3429.00it/s]185937it [01:16, 3289.19it/s]204212it [01:16, 3401.73it/s]195467it [01:16, 3501.87it/s]186307it [01:16, 3406.02it/s]204587it [01:16, 3499.22it/s]195819it [01:16, 3390.78it/s]186650it [01:16, 3298.50it/s]204963it [01:17, 3573.65it/s]196171it [01:17, 3425.30it/s]187033it [01:16, 3449.90it/s]196547it [01:17, 3521.84it/s]187403it [01:16, 3521.85it/s]196901it [01:17, 3436.34it/s]187757it [01:17, 3346.08it/s]197280it [01:17, 3537.30it/s]188129it [01:17, 3450.43it/s]197635it [01:17, 3401.33it/s]188477it [01:17, 3327.76it/s]198022it [01:17, 3533.92it/s]188848it [01:17, 3435.09it/s]198378it [01:17, 3380.98it/s]189194it [01:17, 3300.23it/s]198751it [01:17, 3477.20it/s]189561it [01:17, 3404.71it/s]199127it [01:17, 3556.80it/s]189914it [01:17, 3438.75it/s]199485it [01:17, 3452.22it/s]190260it [01:17, 3345.01it/s]199854it [01:18, 3519.84it/s]190625it [01:17, 3430.56it/s]200208it [01:18, 3390.32it/s]190970it [01:17, 3306.69it/s]200564it [01:18, 3437.15it/s]191334it [01:18, 3401.61it/s]200910it [01:18, 3331.85it/s]191676it [01:18, 3307.38it/s]201279it [01:18, 3433.44it/s]192053it [01:18, 3439.26it/s]201656it [01:18, 3528.37it/s]192403it [01:18, 3455.75it/s]202011it [01:18, 3420.82it/s]192750it [01:18, 3365.54it/s]202394it [01:18, 3536.56it/s]193119it [01:18, 3456.69it/s]202750it [01:18, 3412.92it/s]193466it [01:18, 3333.47it/s]203135it [01:19, 3535.83it/s]193839it [01:18, 3445.80it/s]203491it [01:19, 3419.21it/s]194186it [01:18, 3344.58it/s]203860it [01:19, 3496.18it/s]194543it [01:19, 3407.51it/s]204212it [01:19, 3378.76it/s]194909it [01:19, 3478.85it/s]204572it [01:19, 3439.52it/s]195259it [01:19, 3394.11it/s]204946it [01:19, 3525.25it/s]195629it [01:19, 3481.58it/s]195979it [01:19, 3339.09it/s]196350it [01:19, 3443.20it/s]196697it [01:19, 3309.67it/s]205169it [01:20, 218.27it/s] 197072it [01:19, 3433.13it/s]197443it [01:19, 3510.61it/s]205544it [01:20, 306.54it/s]205927it [01:20, 427.98it/s]197796it [01:19, 3389.14it/s]206248it [01:20, 558.55it/s]198172it [01:20, 3494.52it/s]206629it [01:20, 762.83it/s]198524it [01:20, 3375.48it/s]206966it [01:20, 973.33it/s]198892it [01:20, 3461.60it/s]207337it [01:20, 1260.14it/s]199240it [01:20, 3297.02it/s]207681it [01:20, 1529.33it/s]199619it [01:20, 3434.87it/s]208062it [01:21, 1883.57it/s]199974it [01:20, 3467.74it/s]208450it [01:21, 2246.59it/s]200323it [01:20, 3323.50it/s]208811it [01:21, 2474.52it/s]200688it [01:20, 3413.99it/s]209193it [01:21, 2774.54it/s]201032it [01:20, 3290.06it/s]209553it [01:21, 2882.96it/s]201375it [01:21, 3329.13it/s]209927it [01:21, 3097.96it/s]201710it [01:21, 3240.89it/s]210283it [01:21, 3123.46it/s]202085it [01:21, 3385.77it/s]210657it [01:21, 3288.13it/s]202466it [01:21, 3506.03it/s]211011it [01:21, 3231.98it/s]202819it [01:21, 3392.71it/s]211387it [01:22, 3376.32it/s]203193it [01:21, 3491.95it/s]211763it [01:22, 3483.64it/s]203544it [01:21, 3347.59it/s]212122it [01:22, 3393.63it/s]203889it [01:21, 3376.12it/s]212498it [01:22, 3496.36it/s]204229it [01:21, 3258.41it/s]212854it [01:22, 3411.19it/s]204597it [01:21, 3378.32it/s]213228it [01:22, 3503.07it/s]204964it [01:22, 3460.66it/s]213582it [01:22, 3400.46it/s]213958it [01:22, 3502.76it/s]214319it [01:22, 3533.78it/s]205322it [01:22, 206.60it/s] 214675it [01:22, 3419.01it/s]205690it [01:22, 288.24it/s]215044it [01:23, 3495.85it/s]206006it [01:22, 381.50it/s]215396it [01:23, 3410.99it/s]206385it [01:23, 532.64it/s]215767it [01:23, 3495.31it/s]206760it [01:23, 724.26it/s]216118it [01:23, 3416.39it/s]207097it [01:23, 927.30it/s]216496it [01:23, 3520.48it/s]207472it [01:23, 1210.41it/s]216881it [01:23, 3448.44it/s]207817it [01:23, 1467.89it/s]217260it [01:23, 3544.26it/s]208201it [01:23, 1823.30it/s]217635it [01:23, 3603.25it/s]208550it [01:23, 2089.63it/s]217997it [01:23, 3518.32it/s]208928it [01:23, 2425.31it/s]218376it [01:24, 3595.60it/s]209308it [01:23, 2728.65it/s]218737it [01:24, 3524.32it/s]209669it [01:24, 2840.86it/s]219121it [01:24, 3606.96it/s]210038it [01:24, 3050.28it/s]219483it [01:24, 3523.47it/s]210392it [01:24, 3053.01it/s]219873it [01:24, 3631.97it/s]210764it [01:24, 3228.73it/s]220241it [01:24, 3551.40it/s]211113it [01:24, 3203.93it/s]220637it [01:24, 3668.33it/s]211487it [01:24, 3349.81it/s]221017it [01:24, 3664.31it/s]211842it [01:24, 3298.66it/s]221385it [01:24, 3560.30it/s]212213it [01:24, 3412.81it/s]221784it [01:24, 3683.90it/s]212573it [01:24, 3466.16it/s]222154it [01:25, 3546.38it/s]212926it [01:24, 3377.18it/s]222532it [01:25, 3610.79it/s]213297it [01:25, 3471.86it/s]222895it [01:25, 3486.54it/s]213648it [01:25, 3365.54it/s]223268it [01:25, 3555.16it/s]214022it [01:25, 3471.83it/s]223626it [01:25, 3441.22it/s]214372it [01:25, 3368.72it/s]224008it [01:25, 3547.84it/s]214739it [01:25, 3454.62it/s]224382it [01:25, 3601.79it/s]215100it [01:25, 3498.25it/s]224744it [01:25, 3441.16it/s]215452it [01:25, 3398.77it/s]205300it [01:25, 191.49it/s] 225117it [01:25, 3520.99it/s]215823it [01:25, 3486.91it/s]205674it [01:25, 270.14it/s]225472it [01:26, 3416.77it/s]216174it [01:25, 3395.56it/s]205985it [01:25, 357.28it/s]225840it [01:26, 3491.32it/s]216547it [01:26, 3491.00it/s]206366it [01:25, 502.28it/s]226191it [01:26, 3374.90it/s]206739it [01:26, 684.94it/s]216898it [01:26, 3405.76it/s]226558it [01:26, 3458.48it/s]217275it [01:26, 3510.47it/s]207075it [01:26, 881.04it/s]226919it [01:26, 3500.11it/s]217644it [01:26, 3561.58it/s]207434it [01:26, 1141.44it/s]227271it [01:26, 3388.19it/s]207773it [01:26, 1401.04it/s]218002it [01:26, 3471.12it/s]227638it [01:26, 3468.87it/s]208158it [01:26, 1758.42it/s]218382it [01:26, 3566.36it/s]227987it [01:26, 3364.59it/s]218740it [01:26, 3493.58it/s]208506it [01:26, 2026.07it/s]228342it [01:26, 3415.94it/s]208887it [01:26, 2373.95it/s]219122it [01:26, 3581.18it/s]228685it [01:26, 3334.18it/s]209264it [01:26, 2677.41it/s]219482it [01:26, 3492.27it/s]229055it [01:27, 3439.23it/s]219870it [01:26, 3604.34it/s]209624it [01:26, 2722.59it/s]229431it [01:27, 3530.45it/s]220242it [01:27, 3480.28it/s]209995it [01:26, 2961.39it/s]229786it [01:27, 3426.30it/s]220636it [01:27, 3610.51it/s]210342it [01:27, 3004.68it/s]230154it [01:27, 3499.19it/s]221014it [01:27, 3659.27it/s]210714it [01:27, 3191.21it/s]230506it [01:27, 3386.31it/s]221382it [01:27, 3542.35it/s]211061it [01:27, 3163.50it/s]230879it [01:27, 3482.52it/s]221782it [01:27, 3665.59it/s]211420it [01:27, 3279.97it/s]231229it [01:27, 3401.65it/s]211794it [01:27, 3407.57it/s]222151it [01:27, 3518.47it/s]231584it [01:27, 3435.82it/s]222512it [01:27, 3542.82it/s]212146it [01:27, 3324.30it/s]231956it [01:27, 3515.92it/s]212520it [01:27, 3441.79it/s]222868it [01:27, 3375.42it/s]232309it [01:28, 3400.58it/s]223242it [01:27, 3477.43it/s]212871it [01:27, 3345.49it/s]232681it [01:28, 3491.20it/s]213241it [01:27, 3445.07it/s]223602it [01:28, 3369.04it/s]233032it [01:28, 3393.72it/s]223980it [01:28, 3484.44it/s]213590it [01:28, 3305.39it/s]233405it [01:28, 3489.64it/s]224356it [01:28, 3563.51it/s]213967it [01:28, 3435.52it/s]233756it [01:28, 3397.26it/s]214338it [01:28, 3514.26it/s]224715it [01:28, 3438.62it/s]234130it [01:28, 3493.71it/s]225072it [01:28, 3470.12it/s]214693it [01:28, 3380.19it/s]234500it [01:28, 3552.13it/s]215062it [01:28, 3467.17it/s]225421it [01:28, 3375.88it/s]234857it [01:28, 3432.60it/s]225790it [01:28, 3463.32it/s]215412it [01:28, 3377.76it/s]235212it [01:28, 3465.68it/s]215773it [01:28, 3441.72it/s]226138it [01:28, 3347.60it/s]235560it [01:28, 3356.80it/s]226507it [01:28, 3443.54it/s]216119it [01:28, 3360.23it/s]235926it [01:29, 3442.61it/s]226865it [01:28, 3482.01it/s]216495it [01:28, 3474.48it/s]236272it [01:29, 3351.29it/s]216878it [01:28, 3576.15it/s]227215it [01:29, 3367.29it/s]236638it [01:29, 3439.67it/s]205312it [01:28, 171.01it/s] 227569it [01:29, 3415.91it/s]217237it [01:29, 3446.56it/s]237010it [01:29, 3521.08it/s]205681it [01:28, 241.95it/s]217619it [01:29, 3553.65it/s]227912it [01:29, 3313.79it/s]237364it [01:29, 3398.04it/s]205986it [01:28, 320.59it/s]228279it [01:29, 3413.62it/s]217977it [01:29, 3436.48it/s]237730it [01:29, 3473.04it/s]206363it [01:29, 453.28it/s]218353it [01:29, 3527.21it/s]228642it [01:29, 3319.41it/s]206714it [01:29, 612.46it/s]238079it [01:29, 3373.98it/s]218708it [01:29, 3446.79it/s]229010it [01:29, 3421.42it/s]207040it [01:29, 792.02it/s]238428it [01:29, 3404.96it/s]219089it [01:29, 3550.65it/s]229385it [01:29, 3514.34it/s]207412it [01:29, 1054.45it/s]238770it [01:29, 3315.42it/s]219446it [01:29, 3456.66it/s]229738it [01:29, 3375.28it/s]207748it [01:29, 1301.67it/s]239135it [01:29, 3410.72it/s]219821it [01:29, 3537.62it/s]230106it [01:29, 3460.25it/s]208126it [01:29, 1643.73it/s]239499it [01:30, 3474.94it/s]220211it [01:29, 3642.31it/s]230454it [01:30, 3347.79it/s]208481it [01:29, 1919.54it/s]239848it [01:30, 3374.19it/s]220577it [01:30, 3545.05it/s]230820it [01:30, 3435.71it/s]208858it [01:29, 2266.06it/s]240215it [01:30, 3457.12it/s]220956it [01:30, 3614.95it/s]231166it [01:30, 3358.67it/s]209214it [01:29, 2538.73it/s]240562it [01:30, 3355.25it/s]221319it [01:30, 3497.42it/s]231535it [01:30, 3452.91it/s]240930it [01:30, 3448.43it/s]209564it [01:30, 2669.47it/s]221713it [01:30, 3622.90it/s]231905it [01:30, 3524.81it/s]209931it [01:30, 2910.93it/s]241277it [01:30, 3363.83it/s]222077it [01:30, 3466.21it/s]232259it [01:30, 3367.66it/s]241633it [01:30, 3419.06it/s]210277it [01:30, 2949.07it/s]222445it [01:30, 3525.09it/s]232628it [01:30, 3457.89it/s]242007it [01:30, 3510.80it/s]210646it [01:30, 3142.91it/s]222800it [01:30, 3423.02it/s]232976it [01:30, 3355.39it/s]242360it [01:30, 3404.26it/s]211001it [01:30, 3110.27it/s]223172it [01:30, 3506.65it/s]233346it [01:30, 3452.92it/s]242722it [01:31, 3464.63it/s]211351it [01:30, 3213.59it/s]223539it [01:30, 3553.79it/s]233693it [01:30, 3351.36it/s]243070it [01:31, 3364.87it/s]211717it [01:30, 3335.76it/s]223896it [01:30, 3435.63it/s]234063it [01:31, 3451.17it/s]243441it [01:31, 3463.33it/s]212063it [01:30, 3258.32it/s]224261it [01:31, 3495.62it/s]234433it [01:31, 3522.32it/s]243789it [01:31, 3371.21it/s]212430it [01:30, 3372.56it/s]224612it [01:31, 3381.55it/s]234787it [01:31, 3365.55it/s]244159it [01:31, 3463.91it/s]212774it [01:30, 3278.43it/s]224982it [01:31, 3472.51it/s]235154it [01:31, 3450.58it/s]244525it [01:31, 3518.50it/s]213142it [01:31, 3392.08it/s]225331it [01:31, 3364.79it/s]235501it [01:31, 3345.38it/s]244878it [01:31, 3400.89it/s]213507it [01:31, 3465.15it/s]225701it [01:31, 3451.96it/s]235865it [01:31, 3428.23it/s]245242it [01:31, 3467.66it/s]213857it [01:31, 3302.15it/s]226065it [01:31, 3505.46it/s]236210it [01:31, 3328.88it/s]245590it [01:31, 3333.95it/s]214221it [01:31, 3391.65it/s]226417it [01:31, 3345.35it/s]236575it [01:31, 3419.14it/s]245957it [01:31, 3427.97it/s]214564it [01:31, 3285.97it/s]226783it [01:31, 3435.00it/s]236932it [01:31, 3461.79it/s]246302it [01:32, 3339.97it/s]214926it [01:31, 3379.69it/s]227129it [01:31, 3306.43it/s]237280it [01:32, 3344.95it/s]246672it [01:32, 3442.19it/s]215267it [01:31, 3291.45it/s]227493it [01:32, 3399.50it/s]237645it [01:32, 3431.28it/s]247042it [01:32, 3515.35it/s]215635it [01:31, 3401.67it/s]227835it [01:32, 3297.50it/s]237990it [01:32, 3325.68it/s]247395it [01:32, 3401.07it/s]215977it [01:31, 3402.06it/s]228190it [01:32, 3367.73it/s]238352it [01:32, 3408.86it/s]247759it [01:32, 3468.20it/s]216319it [01:32, 3320.13it/s]228558it [01:32, 3456.51it/s]238717it [01:32, 3478.10it/s]248108it [01:32, 3366.42it/s]216688it [01:32, 3426.01it/s]228906it [01:32, 3342.68it/s]239066it [01:32, 3356.41it/s]248476it [01:32, 3454.64it/s]217032it [01:32, 3345.77it/s]229271it [01:32, 3427.93it/s]239418it [01:32, 3402.44it/s]248823it [01:32, 3361.43it/s]217400it [01:32, 3441.60it/s]229616it [01:32, 3358.30it/s]239760it [01:32, 3313.26it/s]249180it [01:32, 3418.82it/s]217746it [01:32, 3370.56it/s]229980it [01:32, 3439.24it/s]240122it [01:32, 3401.34it/s]249548it [01:33, 3494.29it/s]218126it [01:32, 3492.71it/s]240464it [01:32, 3310.28it/s]230326it [01:32, 3295.95it/s]249899it [01:33, 3384.10it/s]218488it [01:32, 3529.82it/s]240829it [01:33, 3407.84it/s]230692it [01:32, 3398.44it/s]250272it [01:33, 3474.91it/s]218842it [01:32, 3426.70it/s]241199it [01:33, 3491.81it/s]231069it [01:33, 3504.88it/s]250621it [01:33, 3395.80it/s]219222it [01:32, 3532.46it/s]241550it [01:33, 3382.40it/s]231422it [01:33, 3386.69it/s]250992it [01:33, 3484.29it/s]219577it [01:32, 3427.42it/s]241912it [01:33, 3450.18it/s]231790it [01:33, 3468.69it/s]251342it [01:33, 3378.67it/s]219961it [01:33, 3544.84it/s]242259it [01:33, 3341.77it/s]232139it [01:33, 3353.41it/s]251710it [01:33, 3463.94it/s]220317it [01:33, 3438.29it/s]242626it [01:33, 3435.47it/s]232496it [01:33, 3415.26it/s]252082it [01:33, 3537.59it/s]220705it [01:33, 3563.11it/s]242971it [01:33, 3334.38it/s]232841it [01:33, 3310.24it/s]252437it [01:33, 3414.70it/s]221080it [01:33, 3617.19it/s]243342it [01:33, 3431.57it/s]233215it [01:33, 3432.23it/s]252788it [01:33, 3441.41it/s]221443it [01:33, 3495.69it/s]243709it [01:33, 3500.50it/s]233586it [01:33, 3510.16it/s]253134it [01:34, 3346.95it/s]221831it [01:33, 3604.99it/s]244061it [01:34, 3349.70it/s]233939it [01:33, 3392.47it/s]253500it [01:34, 3436.21it/s]222194it [01:33, 3414.84it/s]244425it [01:34, 3430.87it/s]234311it [01:34, 3479.54it/s]253845it [01:34, 3345.58it/s]222563it [01:33, 3492.16it/s]244770it [01:34, 3330.05it/s]254211it [01:34, 3436.07it/s]234661it [01:34, 3322.90it/s]222915it [01:33, 3357.64it/s]245136it [01:34, 3423.83it/s]254579it [01:34, 3506.85it/s]235031it [01:34, 3429.01it/s]223284it [01:34, 3451.13it/s]245480it [01:34, 3322.91it/s]254931it [01:34, 3389.82it/s]235377it [01:34, 3324.70it/s]223632it [01:34, 3323.68it/s]245847it [01:34, 3421.84it/s]255297it [01:34, 3466.84it/s]235741it [01:34, 3412.98it/s]224007it [01:34, 3442.87it/s]246213it [01:34, 3488.58it/s]236108it [01:34, 3485.63it/s]255645it [01:34, 3375.02it/s]224354it [01:34, 3308.10it/s]255997it [01:34, 3415.31it/s]246564it [01:34, 3342.44it/s]236459it [01:34, 3331.22it/s]224688it [01:34, 3225.31it/s]246934it [01:34, 3443.16it/s]236826it [01:34, 3425.13it/s]256362it [01:35, 3345.28it/s]225053it [01:34, 3342.97it/s]247281it [01:34, 3345.66it/s]256734it [01:35, 3451.49it/s]237171it [01:34, 3317.15it/s]225390it [01:34, 3243.36it/s]247650it [01:35, 3442.12it/s]257106it [01:35, 3528.42it/s]237534it [01:35, 3404.28it/s]225756it [01:34, 3360.92it/s]247996it [01:35, 3341.26it/s]257461it [01:35, 3400.45it/s]237881it [01:35, 3297.75it/s]226114it [01:34, 3422.58it/s]248365it [01:35, 3438.57it/s]257832it [01:35, 3480.74it/s]238246it [01:35, 3396.15it/s]226458it [01:35, 3294.39it/s]248732it [01:35, 3499.82it/s]238597it [01:35, 3428.73it/s]258182it [01:35, 3384.49it/s]226812it [01:35, 3363.12it/s]249084it [01:35, 3357.11it/s]258555it [01:35, 3480.97it/s]238942it [01:35, 3312.33it/s]227150it [01:35, 3253.26it/s]249454it [01:35, 3454.70it/s]239306it [01:35, 3405.73it/s]258905it [01:35, 3395.86it/s]227508it [01:35, 3345.93it/s]259262it [01:35, 3445.51it/s]249802it [01:35, 3345.15it/s]239649it [01:35, 3300.35it/s]227845it [01:35, 3239.86it/s]259635it [01:35, 3527.93it/s]250174it [01:35, 3451.46it/s]240016it [01:35, 3403.79it/s]228185it [01:35, 3285.56it/s]259989it [01:36, 3426.86it/s]250521it [01:35, 3366.84it/s]240379it [01:35, 3469.40it/s]228548it [01:35, 3384.96it/s]260358it [01:36, 3500.79it/s]250894it [01:36, 3470.33it/s]240728it [01:35, 3318.06it/s]228888it [01:35, 3268.81it/s]251260it [01:36, 3523.90it/s]260710it [01:36, 3401.37it/s]241097it [01:36, 3423.40it/s]229250it [01:35, 3367.34it/s]261086it [01:36, 3503.67it/s]251614it [01:36, 3367.16it/s]241442it [01:36, 3327.76it/s]229589it [01:35, 3287.66it/s]251981it [01:36, 3453.62it/s]261438it [01:36, 3412.27it/s]241812it [01:36, 3431.81it/s]229954it [01:36, 3390.92it/s]261814it [01:36, 3511.00it/s]252329it [01:36, 3358.91it/s]242157it [01:36, 3324.46it/s]230315it [01:36, 3453.28it/s]262184it [01:36, 3563.39it/s]252694it [01:36, 3441.46it/s]242526it [01:36, 3427.43it/s]230662it [01:36, 3286.24it/s]262542it [01:36, 3447.30it/s]253040it [01:36, 3344.27it/s]242881it [01:36, 3460.45it/s]231031it [01:36, 3399.15it/s]262902it [01:36, 3489.82it/s]253410it [01:36, 3445.13it/s]243229it [01:36, 3350.99it/s]231374it [01:36, 3295.89it/s]253769it [01:36, 3485.26it/s]263253it [01:37, 3391.25it/s]243598it [01:36, 3446.23it/s]231738it [01:36, 3393.01it/s]263625it [01:37, 3484.88it/s]254119it [01:36, 3365.88it/s]243945it [01:36, 3340.94it/s]232080it [01:36, 3275.56it/s]254485it [01:37, 3449.62it/s]263975it [01:37, 3397.80it/s]244310it [01:37, 3429.08it/s]232445it [01:36, 3381.21it/s]254832it [01:37, 3342.19it/s]244655it [01:37, 3288.10it/s]232790it [01:36, 3399.59it/s]255202it [01:37, 3442.57it/s]245021it [01:37, 3386.87it/s]233132it [01:37, 3294.90it/s]255548it [01:37, 3337.27it/s]245386it [01:37, 3461.11it/s]233500it [01:37, 3404.47it/s]255922it [01:37, 3450.83it/s]245734it [01:37, 3348.60it/s]233842it [01:37, 3293.67it/s]256282it [01:37, 3493.88it/s]246101it [01:37, 3440.18it/s]234211it [01:37, 3406.21it/s]256633it [01:37, 3387.27it/s]246447it [01:37, 3326.56it/s]234554it [01:37, 3280.20it/s]257002it [01:37, 3474.22it/s]246806it [01:37, 3400.01it/s]234900it [01:37, 3329.69it/s]257351it [01:37, 3359.39it/s]247148it [01:37, 3302.78it/s]235265it [01:37, 3420.23it/s]257721it [01:38, 3455.39it/s]247517it [01:37, 3413.17it/s]235609it [01:37, 3289.82it/s]258069it [01:38, 3365.04it/s]247886it [01:38, 3492.83it/s]235972it [01:37, 3385.72it/s]258441it [01:38, 3466.23it/s]248237it [01:38, 3366.73it/s]236313it [01:37, 3272.76it/s]258803it [01:38, 3508.38it/s]248601it [01:38, 3444.55it/s]236675it [01:38, 3369.55it/s]259155it [01:38, 3402.03it/s]248947it [01:38, 3320.29it/s]237041it [01:38, 3452.12it/s]259524it [01:38, 3483.44it/s]249316it [01:38, 3425.77it/s]237388it [01:38, 3269.53it/s]259874it [01:38, 3380.60it/s]249661it [01:38, 3319.63it/s]237746it [01:38, 3357.09it/s]260251it [01:38, 3491.53it/s]250033it [01:38, 3433.44it/s]238085it [01:38, 3254.18it/s]260602it [01:38, 3376.06it/s]250405it [01:38, 3514.29it/s]238442it [01:38, 3337.33it/s]260966it [01:38, 3450.96it/s]250758it [01:38, 3396.89it/s]238778it [01:38, 3236.45it/s]261340it [01:39, 3533.94it/s]251113it [01:39, 3438.42it/s]239138it [01:38, 3338.43it/s]261695it [01:39, 3417.57it/s]251459it [01:39, 3340.63it/s]239497it [01:38, 3409.02it/s]262069it [01:39, 3508.76it/s]251823it [01:39, 3424.45it/s]239840it [01:39, 3251.80it/s]262422it [01:39, 3393.06it/s]252167it [01:39, 3334.83it/s]240197it [01:39, 3341.20it/s]262795it [01:39, 3489.31it/s]252534it [01:39, 3429.03it/s]240534it [01:39, 3244.61it/s]263146it [01:39, 3380.73it/s]252902it [01:39, 3501.92it/s]240894it [01:39, 3345.39it/s]263502it [01:39, 3429.10it/s]253254it [01:39, 3345.98it/s]241242it [01:39, 3256.93it/s]263878it [01:39, 3522.58it/s]253624it [01:39, 3446.69it/s]241608it [01:39, 3369.82it/s]253971it [01:39, 3340.25it/s]241979it [01:39, 3465.37it/s]254334it [01:39, 3422.25it/s]242328it [01:39, 3289.37it/s]254681it [01:40, 3319.77it/s]242692it [01:39, 3387.22it/s]255035it [01:40, 3380.34it/s]243034it [01:39, 3274.70it/s]255402it [01:40, 3463.15it/s]243402it [01:40, 3389.74it/s]255750it [01:40, 3357.94it/s]243762it [01:40, 3278.81it/s]256120it [01:40, 3455.09it/s]244128it [01:40, 3384.98it/s]256467it [01:40, 3352.51it/s]244472it [01:40, 3400.36it/s]256839it [01:40, 3455.66it/s]244814it [01:40, 3275.35it/s]257193it [01:40, 3478.44it/s]245177it [01:40, 3374.51it/s]257542it [01:40, 3353.01it/s]245517it [01:40, 3268.33it/s]257908it [01:41, 3439.53it/s]245875it [01:40, 3355.17it/s]258254it [01:41, 3338.65it/s]246241it [01:40, 3441.33it/s]258626it [01:41, 3447.48it/s]246587it [01:41, 3311.91it/s]258973it [01:41, 3359.16it/s]246935it [01:41, 3359.66it/s]259326it [01:41, 3408.11it/s]247273it [01:41, 3255.09it/s]259699it [01:41, 3500.44it/s]247636it [01:41, 3362.24it/s]260051it [01:41, 3386.69it/s]247974it [01:41, 3252.94it/s]260421it [01:41, 3469.23it/s]248338it [01:41, 3362.73it/s]260770it [01:41, 3373.50it/s]248700it [01:41, 3435.62it/s]261141it [01:41, 3468.51it/s]249045it [01:41, 3315.40it/s]261490it [01:42, 3333.55it/s]249392it [01:41, 3358.67it/s]261864it [01:42, 3446.86it/s]249730it [01:41, 3247.60it/s]262234it [01:42, 3517.72it/s]250098it [01:42, 3370.52it/s]262588it [01:42, 3400.18it/s]250467it [01:42, 3462.75it/s]262959it [01:42, 3486.56it/s]250815it [01:42, 3338.77it/s]263310it [01:42, 3333.69it/s]251181it [01:42, 3428.75it/s]263683it [01:42, 3445.90it/s]251526it [01:42, 3251.71it/s]251887it [01:42, 3350.94it/s]252225it [01:42, 3272.82it/s]252585it [01:42, 3365.97it/s]252952it [01:42, 3451.50it/s]253299it [01:43, 3315.77it/s]253663it [01:43, 3408.39it/s]254006it [01:43, 3239.42it/s]254368it [01:43, 3346.46it/s]254706it [01:43, 3230.92it/s]264316it [01:43, 168.62it/s] 255069it [01:43, 3341.81it/s]264689it [01:44, 239.92it/s]255431it [01:43, 3419.60it/s]264979it [01:44, 313.87it/s]265359it [01:44, 447.02it/s]255775it [01:43, 3294.50it/s]265675it [01:44, 587.36it/s]256140it [01:43, 3393.38it/s]266051it [01:44, 805.74it/s]256482it [01:44, 3234.41it/s]266428it [01:44, 1070.93it/s]256849it [01:44, 3354.90it/s]266774it [01:44, 1329.51it/s]257202it [01:44, 3247.61it/s]267149it [01:44, 1663.79it/s]257563it [01:44, 3349.20it/s]267498it [01:44, 1936.98it/s]257930it [01:44, 3440.24it/s]267876it [01:44, 2283.87it/s]258277it [01:44, 3312.97it/s]268228it [01:45, 2498.64it/s]258645it [01:44, 3416.76it/s]268607it [01:45, 2793.13it/s]258989it [01:44, 3270.45it/s]268962it [01:45, 2978.07it/s]259355it [01:44, 3378.46it/s]269316it [01:45, 3037.18it/s]259722it [01:44, 3277.12it/s]269693it [01:45, 3231.50it/s]260091it [01:45, 3390.75it/s]270047it [01:45, 3227.65it/s]260456it [01:45, 3463.77it/s]270426it [01:45, 3381.11it/s]260805it [01:45, 3337.52it/s]270781it [01:45, 3315.80it/s]261158it [01:45, 3389.98it/s]271154it [01:45, 3430.50it/s]261499it [01:45, 3288.24it/s]271527it [01:46, 3515.22it/s]261869it [01:45, 3405.22it/s]271886it [01:46, 3421.20it/s]262234it [01:45, 3474.10it/s]272255it [01:46, 3497.68it/s]262583it [01:45, 3342.32it/s]272609it [01:46, 3392.21it/s]262949it [01:45, 3432.97it/s]272978it [01:46, 3475.85it/s]263295it [01:46, 3308.19it/s]273329it [01:46, 3373.79it/s]263645it [01:46, 3361.37it/s]273705it [01:46, 3482.39it/s]263983it [01:46, 3261.48it/s]274071it [01:46, 3398.21it/s]274444it [01:46, 3491.71it/s]274817it [01:46, 3558.39it/s]275175it [01:47, 3449.36it/s]275549it [01:47, 3531.34it/s]275904it [01:47, 3427.79it/s]276266it [01:47, 3481.91it/s]264232it [01:47, 154.57it/s] 276616it [01:47, 3382.93it/s]264593it [01:47, 216.78it/s]276989it [01:47, 3480.66it/s]264902it [01:47, 288.81it/s]277361it [01:47, 3546.48it/s]265277it [01:47, 408.47it/s]277717it [01:47, 3428.86it/s]265654it [01:47, 566.67it/s]278081it [01:47, 3488.86it/s]265988it [01:47, 738.49it/s]278432it [01:48, 3388.35it/s]266363it [01:47, 986.13it/s]278805it [01:48, 3484.76it/s]266705it [01:48, 1231.44it/s]279155it [01:48, 3389.72it/s]267068it [01:48, 1542.31it/s]279504it [01:48, 3355.59it/s]267411it [01:48, 1813.55it/s]279873it [01:48, 3449.93it/s]267784it [01:48, 2159.52it/s]280220it [01:48, 3353.26it/s]268163it [01:48, 2492.87it/s]280589it [01:48, 3449.08it/s]268519it [01:48, 2666.57it/s]280936it [01:48, 3369.89it/s]268891it [01:48, 2917.21it/s]281306it [01:48, 3464.88it/s]269244it [01:48, 2979.87it/s]281654it [01:48, 3367.31it/s]269606it [01:48, 3144.98it/s]282026it [01:49, 3466.67it/s]269954it [01:49, 3155.98it/s]282396it [01:49, 3533.55it/s]270328it [01:49, 3314.24it/s]282751it [01:49, 3346.12it/s]270700it [01:49, 3427.84it/s]283120it [01:49, 3442.05it/s]271056it [01:49, 3347.47it/s]283467it [01:49, 3347.00it/s]271423it [01:49, 3438.00it/s]283838it [01:49, 3448.53it/s]271774it [01:49, 3345.51it/s]284185it [01:49, 3361.47it/s]272135it [01:49, 3419.60it/s]284555it [01:49, 3457.30it/s]272482it [01:49, 3331.64it/s]284918it [01:49, 3505.83it/s]272852it [01:49, 3435.70it/s]285270it [01:50, 3395.52it/s]273221it [01:49, 3507.96it/s]285639it [01:50, 3480.13it/s]273574it [01:50, 3394.49it/s]285989it [01:50, 3386.09it/s]273944it [01:50, 3481.16it/s]286346it [01:50, 3437.98it/s]274294it [01:50, 3344.71it/s]286691it [01:50, 3363.92it/s]274663it [01:50, 3443.08it/s]287069it [01:50, 3484.03it/s]287112it [01:50, 2596.54it/s]
275010it [01:50, 3353.60it/s]275384it [01:50, 3464.19it/s]264030it [01:50, 147.23it/s] 275751it [01:50, 3363.69it/s]264392it [01:50, 207.20it/s]276123it [01:50, 3464.33it/s]264762it [01:50, 291.30it/s]276495it [01:50, 3535.72it/s]265071it [01:50, 384.35it/s]276851it [01:51, 3384.96it/s]265447it [01:50, 537.38it/s]277223it [01:51, 3479.07it/s]265775it [01:51, 702.03it/s]277573it [01:51, 3372.58it/s]266149it [01:51, 944.22it/s]277945it [01:51, 3470.93it/s]266510it [01:51, 1198.17it/s]278294it [01:51, 3362.11it/s]266871it [01:51, 1501.15it/s]278668it [01:51, 3469.24it/s]267247it [01:51, 1844.72it/s]279041it [01:51, 3536.54it/s]267598it [01:51, 2100.33it/s]279397it [01:51, 3384.23it/s]267977it [01:51, 2437.93it/s]279770it [01:51, 3480.51it/s]268329it [01:51, 2610.64it/s]280121it [01:51, 3366.51it/s]268703it [01:51, 2876.50it/s]280493it [01:52, 3466.21it/s]269054it [01:52, 2941.14it/s]280842it [01:52, 3367.05it/s]269412it [01:52, 3105.20it/s]281211it [01:52, 3449.83it/s]269786it [01:52, 3274.38it/s]281563it [01:52, 3469.73it/s]270139it [01:52, 3245.97it/s]281912it [01:52, 3336.79it/s]270506it [01:52, 3363.26it/s]282252it [01:52, 3352.90it/s]270856it [01:52, 3282.17it/s]282589it [01:52, 3209.20it/s]271225it [01:52, 3396.12it/s]282952it [01:52, 3326.56it/s]271573it [01:52, 3298.25it/s]283311it [01:52, 3240.47it/s]271932it [01:52, 3379.49it/s]283680it [01:53, 3365.78it/s]272300it [01:52, 3465.38it/s]284035it [01:53, 3416.06it/s]272650it [01:53, 3345.21it/s]284379it [01:53, 3320.06it/s]273020it [01:53, 3437.12it/s]284745it [01:53, 3416.80it/s]273367it [01:53, 3331.21it/s]285089it [01:53, 3314.92it/s]273736it [01:53, 3432.90it/s]285460it [01:53, 3427.32it/s]274082it [01:53, 3336.76it/s]285828it [01:53, 3500.45it/s]274435it [01:53, 3389.63it/s]286180it [01:53, 3380.03it/s]274803it [01:53, 3472.57it/s]286536it [01:53, 3430.68it/s]275152it [01:53, 3357.24it/s]286881it [01:53, 3338.98it/s]275525it [01:53, 3462.56it/s]287112it [01:54, 2517.74it/s]
275873it [01:54, 3344.63it/s]276241it [01:54, 3439.43it/s]276590it [01:54, 3333.85it/s]276946it [01:54, 3397.42it/s]277319it [01:54, 3492.81it/s]277670it [01:54, 3373.29it/s]278042it [01:54, 3471.95it/s]278391it [01:54, 3352.79it/s]264311it [01:54, 132.78it/s] 278760it [01:54, 3448.71it/s]264665it [01:54, 188.42it/s]279110it [01:54, 3345.17it/s]264964it [01:54, 252.29it/s]279464it [01:55, 3400.31it/s]265332it [01:54, 360.38it/s]279831it [01:55, 3478.01it/s]265670it [01:55, 488.40it/s]280181it [01:55, 3352.94it/s]266037it [01:55, 672.15it/s]280550it [01:55, 3448.43it/s]266405it [01:55, 901.41it/s]280897it [01:55, 3349.74it/s]266742it [01:55, 1129.51it/s]281262it [01:55, 3434.22it/s]267111it [01:55, 1440.99it/s]281630it [01:55, 3327.18it/s]267450it [01:55, 1702.29it/s]281982it [01:55, 3379.53it/s]267820it [01:55, 2045.83it/s]282349it [01:55, 3462.65it/s]268190it [01:55, 2287.35it/s]282697it [01:56, 3340.44it/s]268559it [01:55, 2586.96it/s]283062it [01:56, 3428.27it/s]268914it [01:55, 2811.32it/s]283407it [01:56, 3313.54it/s]269261it [01:56, 2875.23it/s]283773it [01:56, 3412.14it/s]269628it [01:56, 3077.40it/s]284123it [01:56, 3437.56it/s]269972it [01:56, 3077.36it/s]284469it [01:56, 3328.51it/s]270338it [01:56, 3234.57it/s]284837it [01:56, 3429.25it/s]270693it [01:56, 3321.12it/s]285182it [01:56, 3321.23it/s]271040it [01:56, 3235.58it/s]285550it [01:56, 3417.89it/s]271407it [01:56, 3355.54it/s]285894it [01:56, 3324.06it/s]271751it [01:56, 3252.53it/s]286247it [01:57, 3381.93it/s]272113it [01:56, 3356.19it/s]286616it [01:57, 3469.04it/s]272454it [01:57, 3257.45it/s]286965it [01:57, 3364.90it/s]287112it [01:57, 2446.93it/s]
272804it [01:57, 3325.22it/s]273168it [01:57, 3415.46it/s]273512it [01:57, 3297.98it/s]273880it [01:57, 3406.67it/s]274223it [01:57, 3294.14it/s]274576it [01:57, 3361.24it/s]274914it [01:57, 3253.75it/s]275287it [01:57, 3387.92it/s]275653it [01:57, 3464.64it/s]276002it [01:58, 3339.18it/s]276367it [01:58, 3427.50it/s]276712it [01:58, 3273.91it/s]277080it [01:58, 3386.46it/s]277430it [01:58, 3275.37it/s]277798it [01:58, 3387.98it/s]278162it [01:58, 3459.70it/s]278510it [01:58, 3304.45it/s]278876it [01:58, 3405.32it/s]279219it [01:59, 3292.49it/s]279585it [01:59, 3395.20it/s]279940it [01:59, 3439.44it/s]280286it [01:59, 3225.87it/s]280632it [01:59, 3289.27it/s]280964it [01:59, 3205.36it/s]281329it [01:59, 3329.34it/s]281665it [01:59, 3225.37it/s]282029it [01:59, 3342.93it/s]282381it [01:59, 3391.56it/s]282722it [02:00, 3275.43it/s]283087it [02:00, 3380.72it/s]283427it [02:00, 3260.23it/s]283792it [02:00, 3371.50it/s]284150it [02:00, 3263.95it/s]284501it [02:00, 3331.41it/s]284868it [02:00, 3427.42it/s]285213it [02:00, 3303.23it/s]285580it [02:00, 3407.72it/s]285923it [02:01, 3295.90it/s]286277it [02:01, 3363.76it/s]286647it [02:01, 3458.84it/s]286995it [02:01, 3342.05it/s]287112it [02:01, 2365.12it/s]
2022-07-09 16:03:31 | INFO | root | success load 287112 data
2022-07-09 16:03:31 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-09 16:03:31 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-09 16:03:31 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-09 16:03:31 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-09 16:03:31 | INFO | transformer.tokenization_utils | loading file None
2022-07-09 16:03:31 | INFO | transformer.tokenization_utils | loading file None
2022-07-09 16:03:31 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-09 16:07:55 | INFO | train_inner | epoch 001:    100 / 1122 loss=nan, nll_loss=13.985, mask_ins=7.369, word_ins_ml=14.147, word_reposition=2.474, kpe=nan, ppl=nan, wps=7873.1, ups=0.38, wpb=20527, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=21.347, clip=1, loss_scale=128, train_wall=224, wall=386
2022-07-09 16:12:16 | INFO | train_inner | epoch 001:    200 / 1122 loss=19.37, nll_loss=12.183, mask_ins=4.043, word_ins_ml=12.534, word_reposition=1.506, kpe=1.287, ppl=677733, wps=7873.6, ups=0.38, wpb=20583.2, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=18.833, clip=0, loss_scale=128, train_wall=224, wall=647
2022-07-09 16:16:38 | INFO | train_inner | epoch 001:    300 / 1122 loss=16.498, nll_loss=11.273, mask_ins=2.195, word_ins_ml=11.741, word_reposition=1.44, kpe=1.121, ppl=92558.1, wps=7855.1, ups=0.38, wpb=20561.3, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=4.154, clip=0, loss_scale=128, train_wall=224, wall=909
2022-07-09 16:20:59 | INFO | train_inner | epoch 001:    400 / 1122 loss=15.833, nll_loss=10.862, mask_ins=1.919, word_ins_ml=11.418, word_reposition=1.426, kpe=1.07, ppl=58354.9, wps=7872.8, ups=0.38, wpb=20576.5, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=2.148, clip=0, loss_scale=128, train_wall=224, wall=1170
2022-07-09 16:25:20 | INFO | train_inner | epoch 001:    500 / 1122 loss=15.627, nll_loss=10.751, mask_ins=1.845, word_ins_ml=11.339, word_reposition=1.406, kpe=1.037, ppl=50603.2, wps=7864.2, ups=0.38, wpb=20523.5, bsz=256, num_updates=500, lr=5.009e-05, gnorm=1.791, clip=0, loss_scale=128, train_wall=223, wall=1431
2022-07-09 16:29:40 | INFO | train_inner | epoch 001:    600 / 1122 loss=15.515, nll_loss=10.687, mask_ins=1.832, word_ins_ml=11.286, word_reposition=1.386, kpe=1.011, ppl=46832.2, wps=7894.9, ups=0.39, wpb=20491.4, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=1.833, clip=0, loss_scale=242, train_wall=222, wall=1691
2022-07-09 16:34:15 | INFO | train_inner | epoch 001:    700 / 1122 loss=15.452, nll_loss=10.634, mask_ins=1.834, word_ins_ml=11.242, word_reposition=1.385, kpe=0.991, ppl=44818.6, wps=7468.5, ups=0.36, wpb=20542.5, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=1.705, clip=0, loss_scale=256, train_wall=237, wall=1966
2022-07-09 16:38:39 | INFO | train_inner | epoch 001:    800 / 1122 loss=15.381, nll_loss=10.561, mask_ins=1.845, word_ins_ml=11.178, word_reposition=1.376, kpe=0.981, ppl=42661.8, wps=7804.3, ups=0.38, wpb=20579, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=1.857, clip=0, loss_scale=256, train_wall=226, wall=2229
2022-07-09 16:43:00 | INFO | train_inner | epoch 001:    900 / 1122 loss=15.286, nll_loss=10.493, mask_ins=1.831, word_ins_ml=11.12, word_reposition=1.372, kpe=0.963, ppl=39953.6, wps=7838.2, ups=0.38, wpb=20464, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=1.692, clip=0, loss_scale=256, train_wall=223, wall=2490
2022-07-09 16:47:19 | INFO | train_inner | epoch 001:   1000 / 1122 loss=15.233, nll_loss=10.427, mask_ins=1.847, word_ins_ml=11.064, word_reposition=1.365, kpe=0.956, ppl=38520.1, wps=7934.8, ups=0.39, wpb=20597.8, bsz=256, num_updates=1000, lr=0.00010008, gnorm=1.743, clip=0, loss_scale=256, train_wall=222, wall=2750
2022-07-09 16:51:40 | INFO | train_inner | epoch 001:   1100 / 1122 loss=15.142, nll_loss=10.358, mask_ins=1.834, word_ins_ml=11.005, word_reposition=1.358, kpe=0.945, ppl=36162, wps=7847.8, ups=0.38, wpb=20473.7, bsz=256, num_updates=1100, lr=0.000110078, gnorm=1.716, clip=0, loss_scale=453, train_wall=223, wall=3011
2022-07-09 16:52:37 | INFO | train | epoch 001 | loss nan | nll_loss 11.095 | mask_ins 2.567 | word_ins_ml 11.63 | word_reposition 1.497 | kpe nan | ppl nan | wps 7823.4 | ups 0.38 | wpb 20520.3 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 5.283 | clip 0.1 | loss_scale 220 | train_wall 2520 | wall 3068
2022-07-09 16:53:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 16.074 | nll_loss 10.449 | mask_ins 2.282 | word_ins_ml 11.109 | word_reposition 1.312 | kpe 1.37 | ppl 68967.8 | wps 15194.7 | wpb 2367.6 | bsz 32 | num_updates 1122
2022-07-09 16:53:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_best.pt (epoch 1 @ 1122 updates, score 16.074) (writing took 7.099319727160037 seconds)
2022-07-09 16:57:12 | INFO | train_inner | epoch 002:     78 / 1122 loss=15.051, nll_loss=10.281, mask_ins=1.837, word_ins_ml=10.942, word_reposition=1.347, kpe=0.925, ppl=33955.4, wps=6125.7, ups=0.3, wpb=20333.3, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=1.814, clip=0, loss_scale=512, train_wall=222, wall=3343
2022-07-09 17:01:34 | INFO | train_inner | epoch 002:    178 / 1122 loss=14.984, nll_loss=10.217, mask_ins=1.83, word_ins_ml=10.887, word_reposition=1.354, kpe=0.913, ppl=32407.1, wps=7879.1, ups=0.38, wpb=20587.3, bsz=256, num_updates=1300, lr=0.000130074, gnorm=1.638, clip=0, loss_scale=512, train_wall=224, wall=3604
2022-07-09 17:05:57 | INFO | train_inner | epoch 002:    278 / 1122 loss=14.924, nll_loss=10.167, mask_ins=1.814, word_ins_ml=10.845, word_reposition=1.355, kpe=0.909, ppl=31079.6, wps=7826.4, ups=0.38, wpb=20599.8, bsz=256, num_updates=1400, lr=0.000140072, gnorm=1.619, clip=0, loss_scale=512, train_wall=225, wall=3867
2022-07-09 17:10:20 | INFO | train_inner | epoch 002:    378 / 1122 loss=14.869, nll_loss=10.113, mask_ins=1.825, word_ins_ml=10.799, word_reposition=1.335, kpe=0.909, ppl=29923.9, wps=7741.7, ups=0.38, wpb=20347.3, bsz=256, num_updates=1500, lr=0.00015007, gnorm=1.741, clip=0, loss_scale=512, train_wall=225, wall=4130
2022-07-09 17:14:43 | INFO | train_inner | epoch 002:    478 / 1122 loss=14.814, nll_loss=10.056, mask_ins=1.825, word_ins_ml=10.752, word_reposition=1.331, kpe=0.906, ppl=28811.5, wps=7805.8, ups=0.38, wpb=20567.7, bsz=256, num_updates=1600, lr=0.000160068, gnorm=1.623, clip=0, loss_scale=845, train_wall=226, wall=4394
2022-07-09 17:19:04 | INFO | train_inner | epoch 002:    578 / 1122 loss=14.791, nll_loss=10.012, mask_ins=1.825, word_ins_ml=10.715, word_reposition=1.342, kpe=0.91, ppl=28355, wps=7878.6, ups=0.38, wpb=20536.9, bsz=256, num_updates=1700, lr=0.000170066, gnorm=1.554, clip=0, loss_scale=1024, train_wall=223, wall=4654
2022-07-09 17:23:26 | INFO | train_inner | epoch 002:    678 / 1122 loss=14.761, nll_loss=9.966, mask_ins=1.834, word_ins_ml=10.676, word_reposition=1.342, kpe=0.909, ppl=27772.2, wps=7798.1, ups=0.38, wpb=20477.4, bsz=256, num_updates=1800, lr=0.000180064, gnorm=1.6, clip=0, loss_scale=1024, train_wall=225, wall=4917
2022-07-09 17:28:15 | INFO | train_inner | epoch 002:    778 / 1122 loss=nan, nll_loss=9.927, mask_ins=1.822, word_ins_ml=10.642, word_reposition=1.327, kpe=nan, ppl=nan, wps=7139.1, ups=0.35, wpb=20576, bsz=256, num_updates=1900, lr=0.000190062, gnorm=1.591, clip=0, loss_scale=1024, train_wall=250, wall=5205
2022-07-09 17:32:35 | INFO | train_inner | epoch 002:    878 / 1122 loss=14.662, nll_loss=9.892, mask_ins=1.824, word_ins_ml=10.612, word_reposition=1.322, kpe=0.904, ppl=25923.6, wps=7859, ups=0.38, wpb=20447.7, bsz=256, num_updates=2000, lr=0.00020006, gnorm=1.635, clip=0, loss_scale=1024, train_wall=222, wall=5465
2022-07-09 17:36:57 | INFO | train_inner | epoch 002:    978 / 1122 loss=14.634, nll_loss=9.859, mask_ins=1.814, word_ins_ml=10.585, word_reposition=1.332, kpe=0.903, ppl=25431.3, wps=7827.3, ups=0.38, wpb=20513.5, bsz=256, num_updates=2100, lr=0.000210058, gnorm=1.617, clip=0, loss_scale=1567, train_wall=224, wall=5727
2022-07-09 17:41:18 | INFO | train_inner | epoch 002:   1078 / 1122 loss=nan, nll_loss=9.824, mask_ins=1.825, word_ins_ml=10.555, word_reposition=1.323, kpe=nan, ppl=nan, wps=7923.9, ups=0.38, wpb=20708.1, bsz=256, num_updates=2200, lr=0.000220056, gnorm=1.546, clip=0, loss_scale=2048, train_wall=224, wall=5989
2022-07-09 17:43:12 | INFO | train | epoch 002 | loss nan | nll_loss 10.014 | mask_ins 1.825 | word_ins_ml 10.716 | word_reposition 1.337 | kpe nan | ppl nan | wps 7586.6 | ups 0.37 | wpb 20521 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 1.618 | clip 0 | loss_scale 1015 | train_wall 2541 | wall 6103
2022-07-09 17:44:17 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 15.628 | nll_loss 10.025 | mask_ins 2.159 | word_ins_ml 10.758 | word_reposition 1.234 | kpe 1.476 | ppl 50645.9 | wps 15153.3 | wpb 2367.6 | bsz 32 | num_updates 2244 | best_loss 15.628
2022-07-09 17:44:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_best.pt (epoch 2 @ 2244 updates, score 15.628) (writing took 10.574489641934633 seconds)
2022-07-09 17:46:55 | INFO | train_inner | epoch 003:     56 / 1122 loss=nan, nll_loss=9.771, mask_ins=1.822, word_ins_ml=10.509, word_reposition=1.327, kpe=nan, ppl=nan, wps=6055, ups=0.3, wpb=20387.7, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=1.583, clip=0, loss_scale=2048, train_wall=224, wall=6326
2022-07-09 17:51:16 | INFO | train_inner | epoch 003:    156 / 1122 loss=14.46, nll_loss=9.717, mask_ins=1.823, word_ins_ml=10.462, word_reposition=1.324, kpe=0.851, ppl=22530.6, wps=7839.1, ups=0.38, wpb=20466.9, bsz=256, num_updates=2400, lr=0.000240052, gnorm=1.591, clip=0, loss_scale=2048, train_wall=223, wall=6587
2022-07-09 17:55:37 | INFO | train_inner | epoch 003:    256 / 1122 loss=14.443, nll_loss=9.704, mask_ins=1.811, word_ins_ml=10.452, word_reposition=1.321, kpe=0.859, ppl=22265.8, wps=7875.7, ups=0.38, wpb=20590.4, bsz=256, num_updates=2500, lr=0.00025005, gnorm=1.665, clip=0, loss_scale=2048, train_wall=223, wall=6848
2022-07-09 18:00:01 | INFO | train_inner | epoch 003:    356 / 1122 loss=14.431, nll_loss=9.672, mask_ins=1.824, word_ins_ml=10.425, word_reposition=1.32, kpe=0.863, ppl=22090, wps=7808.8, ups=0.38, wpb=20552.9, bsz=256, num_updates=2600, lr=0.000260048, gnorm=1.724, clip=0, loss_scale=2888, train_wall=226, wall=7111
2022-07-09 18:04:24 | INFO | train_inner | epoch 003:    456 / 1122 loss=14.392, nll_loss=9.641, mask_ins=1.81, word_ins_ml=10.397, word_reposition=1.309, kpe=0.875, ppl=21495.5, wps=7743.7, ups=0.38, wpb=20384, bsz=256, num_updates=2700, lr=0.000270046, gnorm=1.637, clip=0, loss_scale=4096, train_wall=225, wall=7375
2022-07-09 18:08:47 | INFO | train_inner | epoch 003:    556 / 1122 loss=14.393, nll_loss=9.633, mask_ins=1.816, word_ins_ml=10.391, word_reposition=1.312, kpe=0.873, ppl=21511.2, wps=7782.7, ups=0.38, wpb=20480.9, bsz=256, num_updates=2800, lr=0.000280044, gnorm=1.553, clip=0, loss_scale=4096, train_wall=226, wall=7638
2022-07-09 18:13:11 | INFO | train_inner | epoch 003:    656 / 1122 loss=14.389, nll_loss=9.618, mask_ins=1.823, word_ins_ml=10.379, word_reposition=1.308, kpe=0.879, ppl=21456, wps=7800.4, ups=0.38, wpb=20612.3, bsz=256, num_updates=2900, lr=0.000290042, gnorm=1.644, clip=0, loss_scale=4096, train_wall=227, wall=7902
2022-07-09 18:17:49 | INFO | train_inner | epoch 003:    756 / 1122 loss=14.377, nll_loss=9.59, mask_ins=1.825, word_ins_ml=10.355, word_reposition=1.314, kpe=0.883, ppl=21274.7, wps=7426.5, ups=0.36, wpb=20597.8, bsz=256, num_updates=3000, lr=0.00030004, gnorm=1.555, clip=0, loss_scale=4096, train_wall=240, wall=8179
2022-07-09 18:22:14 | INFO | train_inner | epoch 003:    856 / 1122 loss=14.359, nll_loss=9.565, mask_ins=1.827, word_ins_ml=10.332, word_reposition=1.315, kpe=0.885, ppl=21020, wps=7763.1, ups=0.38, wpb=20609.8, bsz=256, num_updates=3100, lr=0.000310038, gnorm=1.56, clip=0, loss_scale=5284, train_wall=228, wall=8445
2022-07-09 18:26:39 | INFO | train_inner | epoch 003:    956 / 1122 loss=nan, nll_loss=9.546, mask_ins=1.822, word_ins_ml=10.316, word_reposition=1.306, kpe=nan, ppl=nan, wps=7780.2, ups=0.38, wpb=20572.9, bsz=256, num_updates=3200, lr=0.000320036, gnorm=1.659, clip=0, loss_scale=8192, train_wall=227, wall=8709
2022-07-09 18:31:03 | INFO | train_inner | epoch 003:   1056 / 1122 loss=14.322, nll_loss=9.534, mask_ins=1.826, word_ins_ml=10.305, word_reposition=1.299, kpe=0.892, ppl=20475.9, wps=7747.5, ups=0.38, wpb=20512.4, bsz=256, num_updates=3300, lr=0.000330034, gnorm=1.542, clip=0, loss_scale=8192, train_wall=228, wall=8974
2022-07-09 18:33:57 | INFO | train | epoch 003 | loss nan | nll_loss 9.622 | mask_ins 1.822 | word_ins_ml 10.381 | word_reposition 1.314 | kpe nan | ppl nan | wps 7560.8 | ups 0.37 | wpb 20521.3 | bsz 255.8 | num_updates 3366 | lr 0.000336633 | gnorm 1.626 | clip 0 | loss_scale 4598 | train_wall 2548 | wall 9148
2022-07-09 18:35:02 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 15.37 | nll_loss 9.748 | mask_ins 2.087 | word_ins_ml 10.524 | word_reposition 1.301 | kpe 1.459 | ppl 42350.3 | wps 15217.7 | wpb 2367.6 | bsz 32 | num_updates 3366 | best_loss 15.37
2022-07-09 18:35:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_best.pt (epoch 3 @ 3366 updates, score 15.37) (writing took 10.504364599473774 seconds)
2022-07-09 18:36:42 | INFO | train_inner | epoch 004:     34 / 1122 loss=14.295, nll_loss=9.473, mask_ins=1.83, word_ins_ml=10.251, word_reposition=1.338, kpe=0.875, ppl=20102.9, wps=6005.2, ups=0.3, wpb=20354.1, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=1.781, clip=0, loss_scale=8192, train_wall=226, wall=9313
2022-07-09 18:38:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-09 18:41:09 | INFO | train_inner | epoch 004:    135 / 1122 loss=14.14, nll_loss=9.273, mask_ins=1.822, word_ins_ml=10.074, word_reposition=1.396, kpe=0.848, ppl=18049.7, wps=7668.7, ups=0.37, wpb=20490.6, bsz=256, num_updates=3500, lr=0.00035003, gnorm=2.053, clip=0, loss_scale=5353, train_wall=229, wall=9580
2022-07-09 18:45:33 | INFO | train_inner | epoch 004:    235 / 1122 loss=14.036, nll_loss=9.148, mask_ins=1.808, word_ins_ml=9.965, word_reposition=1.415, kpe=0.848, ppl=16799.4, wps=7809.3, ups=0.38, wpb=20607.3, bsz=256, num_updates=3600, lr=0.000360028, gnorm=2.21, clip=0, loss_scale=4096, train_wall=227, wall=9844
2022-07-09 18:49:57 | INFO | train_inner | epoch 004:    335 / 1122 loss=14.011, nll_loss=9.056, mask_ins=1.831, word_ins_ml=9.885, word_reposition=1.434, kpe=0.86, ppl=16504.7, wps=7736.8, ups=0.38, wpb=20432.2, bsz=256, num_updates=3700, lr=0.000370026, gnorm=2.421, clip=0, loss_scale=4096, train_wall=227, wall=10108
2022-07-09 18:50:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 18:54:24 | INFO | train_inner | epoch 004:    436 / 1122 loss=13.984, nll_loss=9.003, mask_ins=1.826, word_ins_ml=9.84, word_reposition=1.442, kpe=0.876, ppl=16202.9, wps=7743, ups=0.38, wpb=20643.4, bsz=256, num_updates=3800, lr=0.000380024, gnorm=2.913, clip=0, loss_scale=2210, train_wall=229, wall=10375
2022-07-09 18:58:47 | INFO | train_inner | epoch 004:    536 / 1122 loss=13.906, nll_loss=8.913, mask_ins=1.812, word_ins_ml=9.761, word_reposition=1.462, kpe=0.871, ppl=15351.1, wps=7776, ups=0.38, wpb=20484.3, bsz=256, num_updates=3900, lr=0.000390022, gnorm=2.7, clip=0, loss_scale=2048, train_wall=226, wall=10638
2022-07-09 19:03:11 | INFO | train_inner | epoch 004:    636 / 1122 loss=nan, nll_loss=8.857, mask_ins=1.815, word_ins_ml=9.712, word_reposition=1.458, kpe=nan, ppl=nan, wps=7784.6, ups=0.38, wpb=20511, bsz=256, num_updates=4000, lr=0.00040002, gnorm=2.579, clip=0, loss_scale=2048, train_wall=226, wall=10902
2022-07-09 19:07:35 | INFO | train_inner | epoch 004:    736 / 1122 loss=13.836, nll_loss=8.818, mask_ins=1.809, word_ins_ml=9.678, word_reposition=1.463, kpe=0.885, ppl=14620.1, wps=7827.4, ups=0.38, wpb=20631.1, bsz=256, num_updates=4100, lr=0.000410018, gnorm=2.481, clip=0, loss_scale=2048, train_wall=226, wall=11165
2022-07-09 19:12:23 | INFO | train_inner | epoch 004:    836 / 1122 loss=nan, nll_loss=8.768, mask_ins=1.808, word_ins_ml=9.635, word_reposition=1.452, kpe=nan, ppl=nan, wps=7080.5, ups=0.35, wpb=20431.8, bsz=256, num_updates=4200, lr=0.000420016, gnorm=2.468, clip=0, loss_scale=2048, train_wall=251, wall=11454
2022-07-09 19:16:49 | INFO | train_inner | epoch 004:    936 / 1122 loss=13.782, nll_loss=8.71, mask_ins=1.814, word_ins_ml=9.584, word_reposition=1.484, kpe=0.901, ppl=14089.6, wps=7767.4, ups=0.38, wpb=20641.5, bsz=256, num_updates=4300, lr=0.000430014, gnorm=2.735, clip=0, loss_scale=3707, train_wall=228, wall=11720
2022-07-09 19:21:14 | INFO | train_inner | epoch 004:   1036 / 1122 loss=13.703, nll_loss=8.624, mask_ins=1.807, word_ins_ml=9.509, word_reposition=1.489, kpe=0.899, ppl=13339.2, wps=7703.6, ups=0.38, wpb=20455.6, bsz=256, num_updates=4400, lr=0.000440012, gnorm=2.55, clip=0, loss_scale=4096, train_wall=228, wall=11985
2022-07-09 19:25:02 | INFO | train | epoch 004 | loss nan | nll_loss 8.901 | mask_ins 1.815 | word_ins_ml 9.75 | word_reposition 1.45 | kpe nan | ppl nan | wps 7498.9 | ups 0.37 | wpb 20518.5 | bsz 255.8 | num_updates 4486 | lr 0.00044861 | gnorm 2.525 | clip 0 | loss_scale 3399 | train_wall 2569 | wall 12213
2022-07-09 19:26:07 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 15.807 | nll_loss 9.522 | mask_ins 2.292 | word_ins_ml 10.368 | word_reposition 1.52 | kpe 1.626 | ppl 57319.6 | wps 15138 | wpb 2367.6 | bsz 32 | num_updates 4486 | best_loss 15.37
2022-07-09 19:26:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 4 @ 4486 updates, score 15.807) (writing took 6.031683110632002 seconds)
2022-07-09 19:26:50 | INFO | train_inner | epoch 005:     14 / 1122 loss=13.622, nll_loss=8.505, mask_ins=1.812, word_ins_ml=9.405, word_reposition=1.494, kpe=0.912, ppl=12611.1, wps=6072, ups=0.3, wpb=20383.7, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=3.087, clip=0, loss_scale=4096, train_wall=227, wall=12321
2022-07-09 19:31:15 | INFO | train_inner | epoch 005:    114 / 1122 loss=13.366, nll_loss=8.225, mask_ins=1.809, word_ins_ml=9.16, word_reposition=1.536, kpe=0.862, ppl=10556.8, wps=7778.5, ups=0.38, wpb=20625.6, bsz=256, num_updates=4600, lr=0.000460008, gnorm=3.228, clip=0, loss_scale=4096, train_wall=228, wall=12586
2022-07-09 19:35:40 | INFO | train_inner | epoch 005:    214 / 1122 loss=13.256, nll_loss=8.073, mask_ins=1.803, word_ins_ml=9.027, word_reposition=1.55, kpe=0.876, ppl=9781.97, wps=7763.3, ups=0.38, wpb=20524.8, bsz=256, num_updates=4700, lr=0.000470006, gnorm=3.515, clip=0, loss_scale=4096, train_wall=227, wall=12850
2022-07-09 19:37:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-09 19:37:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-09 19:37:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-09 19:40:12 | INFO | train_inner | epoch 005:    317 / 1122 loss=13.169, nll_loss=7.921, mask_ins=1.821, word_ins_ml=8.895, word_reposition=1.558, kpe=0.894, ppl=9208.6, wps=7539.1, ups=0.37, wpb=20565.7, bsz=256, num_updates=4800, lr=0.000480004, gnorm=3.65, clip=0, loss_scale=2068, train_wall=234, wall=13123
2022-07-09 19:44:37 | INFO | train_inner | epoch 005:    417 / 1122 loss=nan, nll_loss=7.759, mask_ins=1.808, word_ins_ml=8.755, word_reposition=1.565, kpe=nan, ppl=nan, wps=7748.3, ups=0.38, wpb=20468.4, bsz=256, num_updates=4900, lr=0.000490002, gnorm=3.656, clip=0, loss_scale=1024, train_wall=227, wall=13387
2022-07-09 19:49:01 | INFO | train_inner | epoch 005:    517 / 1122 loss=12.843, nll_loss=7.559, mask_ins=1.802, word_ins_ml=8.58, word_reposition=1.559, kpe=0.902, ppl=7348.44, wps=7709.1, ups=0.38, wpb=20383.8, bsz=256, num_updates=5000, lr=0.0005, gnorm=3.759, clip=0, loss_scale=1024, train_wall=227, wall=13652
2022-07-09 19:53:26 | INFO | train_inner | epoch 005:    617 / 1122 loss=12.679, nll_loss=7.385, mask_ins=1.779, word_ins_ml=8.429, word_reposition=1.558, kpe=0.913, ppl=6559.5, wps=7747.3, ups=0.38, wpb=20491.5, bsz=256, num_updates=5100, lr=0.000495074, gnorm=3.78, clip=0, loss_scale=1024, train_wall=227, wall=13916
2022-07-09 19:57:49 | INFO | train_inner | epoch 005:    717 / 1122 loss=12.54, nll_loss=7.168, mask_ins=1.808, word_ins_ml=8.241, word_reposition=1.572, kpe=0.919, ppl=5953.77, wps=7793.1, ups=0.38, wpb=20532.4, bsz=256, num_updates=5200, lr=0.00049029, gnorm=3.949, clip=0, loss_scale=1024, train_wall=226, wall=14180
2022-07-09 19:59:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-09 20:02:40 | INFO | train_inner | epoch 005:    818 / 1122 loss=12.324, nll_loss=6.933, mask_ins=1.802, word_ins_ml=8.037, word_reposition=1.56, kpe=0.925, ppl=5127.81, wps=7095, ups=0.34, wpb=20651.4, bsz=256, num_updates=5300, lr=0.000485643, gnorm=3.897, clip=0, loss_scale=684, train_wall=253, wall=14471
2022-07-09 20:07:06 | INFO | train_inner | epoch 005:    918 / 1122 loss=12.044, nll_loss=6.625, mask_ins=1.789, word_ins_ml=7.77, word_reposition=1.557, kpe=0.929, ppl=4222.5, wps=7735.4, ups=0.38, wpb=20562.1, bsz=256, num_updates=5400, lr=0.000481125, gnorm=4.057, clip=0, loss_scale=512, train_wall=229, wall=14737
2022-07-09 20:11:30 | INFO | train_inner | epoch 005:   1018 / 1122 loss=nan, nll_loss=6.358, mask_ins=1.795, word_ins_ml=7.538, word_reposition=1.537, kpe=nan, ppl=nan, wps=7731.4, ups=0.38, wpb=20455.8, bsz=256, num_updates=5500, lr=0.000476731, gnorm=4.12, clip=0, loss_scale=512, train_wall=227, wall=15001
2022-07-09 20:15:55 | INFO | train_inner | epoch 005:   1118 / 1122 loss=11.557, nll_loss=6.084, mask_ins=1.8, word_ins_ml=7.3, word_reposition=1.524, kpe=0.932, ppl=3012.45, wps=7773.3, ups=0.38, wpb=20588.5, bsz=256, num_updates=5600, lr=0.000472456, gnorm=4.224, clip=0, loss_scale=512, train_wall=227, wall=15266
2022-07-09 20:16:05 | INFO | train | epoch 005 | loss nan | nll_loss 7.291 | mask_ins 1.802 | word_ins_ml 8.349 | word_reposition 1.552 | kpe nan | ppl nan | wps 7488.7 | ups 0.36 | wpb 20518.6 | bsz 255.8 | num_updates 5604 | lr 0.000472287 | gnorm 3.814 | clip 0 | loss_scale 1536 | train_wall 2572 | wall 15276
2022-07-09 20:17:10 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 14.172 | nll_loss 8.032 | mask_ins 2.036 | word_ins_ml 9.075 | word_reposition 1.591 | kpe 1.47 | ppl 18454.3 | wps 15165.3 | wpb 2367.6 | bsz 32 | num_updates 5604 | best_loss 14.172
2022-07-09 20:17:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_best.pt (epoch 5 @ 5604 updates, score 14.172) (writing took 10.419668027199805 seconds)
2022-07-09 20:21:36 | INFO | train_inner | epoch 006:     96 / 1122 loss=11.255, nll_loss=5.796, mask_ins=1.805, word_ins_ml=7.05, word_reposition=1.502, kpe=0.897, ppl=2443.39, wps=5978.7, ups=0.29, wpb=20336, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=4.731, clip=1, loss_scale=512, train_wall=227, wall=15606
2022-07-09 20:25:59 | INFO | train_inner | epoch 006:    196 / 1122 loss=11.035, nll_loss=5.585, mask_ins=1.784, word_ins_ml=6.868, word_reposition=1.475, kpe=0.908, ppl=2098.28, wps=7810.5, ups=0.38, wpb=20603.5, bsz=256, num_updates=5800, lr=0.000464238, gnorm=4.633, clip=1, loss_scale=794, train_wall=226, wall=15870
2022-07-09 20:30:23 | INFO | train_inner | epoch 006:    296 / 1122 loss=nan, nll_loss=5.415, mask_ins=1.795, word_ins_ml=6.722, word_reposition=1.461, kpe=nan, ppl=nan, wps=7808.6, ups=0.38, wpb=20594.7, bsz=256, num_updates=5900, lr=0.000460287, gnorm=4.844, clip=0, loss_scale=1024, train_wall=226, wall=16134
2022-07-09 20:34:47 | INFO | train_inner | epoch 006:    396 / 1122 loss=10.679, nll_loss=5.206, mask_ins=1.791, word_ins_ml=6.54, word_reposition=1.437, kpe=0.911, ppl=1639.87, wps=7799.7, ups=0.38, wpb=20576.6, bsz=256, num_updates=6000, lr=0.000456435, gnorm=4.473, clip=0, loss_scale=1024, train_wall=227, wall=16398
2022-07-09 20:39:11 | INFO | train_inner | epoch 006:    496 / 1122 loss=10.546, nll_loss=5.06, mask_ins=1.802, word_ins_ml=6.414, word_reposition=1.419, kpe=0.912, ppl=1495.48, wps=7765.7, ups=0.38, wpb=20499.7, bsz=256, num_updates=6100, lr=0.000452679, gnorm=4.376, clip=0, loss_scale=1024, train_wall=226, wall=16662
2022-07-09 20:43:35 | INFO | train_inner | epoch 006:    596 / 1122 loss=10.33, nll_loss=4.863, mask_ins=1.789, word_ins_ml=6.242, word_reposition=1.384, kpe=0.914, ppl=1286.86, wps=7712.5, ups=0.38, wpb=20363.4, bsz=256, num_updates=6200, lr=0.000449013, gnorm=4.178, clip=0, loss_scale=1024, train_wall=226, wall=16926
2022-07-09 20:47:58 | INFO | train_inner | epoch 006:    696 / 1122 loss=10.176, nll_loss=4.705, mask_ins=1.788, word_ins_ml=6.105, word_reposition=1.367, kpe=0.915, ppl=1156.51, wps=7840.5, ups=0.38, wpb=20637, bsz=256, num_updates=6300, lr=0.000445435, gnorm=4.186, clip=0, loss_scale=1464, train_wall=226, wall=17189
2022-07-09 20:52:47 | INFO | train_inner | epoch 006:    796 / 1122 loss=nan, nll_loss=4.493, mask_ins=1.782, word_ins_ml=5.919, word_reposition=1.341, kpe=nan, ppl=nan, wps=7082.9, ups=0.35, wpb=20487.4, bsz=256, num_updates=6400, lr=0.000441942, gnorm=4.061, clip=0, loss_scale=2048, train_wall=252, wall=17478
2022-07-09 20:57:12 | INFO | train_inner | epoch 006:    896 / 1122 loss=9.829, nll_loss=4.364, mask_ins=1.787, word_ins_ml=5.806, word_reposition=1.32, kpe=0.916, ppl=909.68, wps=7801.6, ups=0.38, wpb=20650.1, bsz=256, num_updates=6500, lr=0.000438529, gnorm=3.943, clip=0, loss_scale=2048, train_wall=226, wall=17743
2022-07-09 20:58:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-09 20:58:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-09 21:01:40 | INFO | train_inner | epoch 006:    998 / 1122 loss=9.72, nll_loss=4.277, mask_ins=1.782, word_ins_ml=5.729, word_reposition=1.278, kpe=0.931, ppl=843.32, wps=7636.9, ups=0.37, wpb=20481.8, bsz=256, num_updates=6600, lr=0.000435194, gnorm=5.077, clip=1, loss_scale=848, train_wall=230, wall=18011
2022-07-09 21:06:04 | INFO | train_inner | epoch 006:   1098 / 1122 loss=9.491, nll_loss=4.087, mask_ins=1.773, word_ins_ml=5.562, word_reposition=1.24, kpe=0.915, ppl=719.39, wps=7770.6, ups=0.38, wpb=20511.7, bsz=256, num_updates=6700, lr=0.000431934, gnorm=3.857, clip=0, loss_scale=512, train_wall=226, wall=18275
2022-07-09 21:07:07 | INFO | train | epoch 006 | loss nan | nll_loss 4.875 | mask_ins 1.789 | word_ins_ml 6.251 | word_reposition 1.381 | kpe nan | ppl nan | wps 7506.5 | ups 0.37 | wpb 20519.9 | bsz 255.8 | num_updates 6724 | lr 0.000431163 | gnorm 4.385 | clip 0.3 | loss_scale 1109 | train_wall 2563 | wall 18337
2022-07-09 21:08:12 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 13.345 | nll_loss 7.101 | mask_ins 1.906 | word_ins_ml 8.322 | word_reposition 1.583 | kpe 1.534 | ppl 10403.4 | wps 15248.8 | wpb 2367.6 | bsz 32 | num_updates 6724 | best_loss 13.345
2022-07-09 21:08:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_best.pt (epoch 6 @ 6724 updates, score 13.345) (writing took 10.40682052448392 seconds)
2022-07-09 21:11:42 | INFO | train_inner | epoch 007:     76 / 1122 loss=9.391, nll_loss=3.98, mask_ins=1.797, word_ins_ml=5.467, word_reposition=1.232, kpe=0.896, ppl=671.4, wps=6007.6, ups=0.3, wpb=20308.2, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=4.784, clip=1, loss_scale=512, train_wall=225, wall=18613
2022-07-09 21:16:06 | INFO | train_inner | epoch 007:    176 / 1122 loss=9.191, nll_loss=3.803, mask_ins=1.8, word_ins_ml=5.31, word_reposition=1.194, kpe=0.888, ppl=584.35, wps=7792.7, ups=0.38, wpb=20586.9, bsz=256, num_updates=6900, lr=0.000425628, gnorm=3.97, clip=0, loss_scale=512, train_wall=227, wall=18877
2022-07-09 21:20:30 | INFO | train_inner | epoch 007:    276 / 1122 loss=9.125, nll_loss=3.762, mask_ins=1.791, word_ins_ml=5.273, word_reposition=1.172, kpe=0.89, ppl=558.36, wps=7826, ups=0.38, wpb=20641.3, bsz=256, num_updates=7000, lr=0.000422577, gnorm=3.791, clip=0, loss_scale=512, train_wall=226, wall=19141
2022-07-09 21:24:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-09 21:24:56 | INFO | train_inner | epoch 007:    377 / 1122 loss=8.944, nll_loss=3.602, mask_ins=1.792, word_ins_ml=5.131, word_reposition=1.13, kpe=0.891, ppl=492.52, wps=7710.6, ups=0.38, wpb=20522.1, bsz=256, num_updates=7100, lr=0.000419591, gnorm=4.012, clip=0, loss_scale=765, train_wall=228, wall=19407
2022-07-09 21:28:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-09 21:29:21 | INFO | train_inner | epoch 007:    478 / 1122 loss=nan, nll_loss=3.541, mask_ins=1.803, word_ins_ml=5.076, word_reposition=1.113, kpe=nan, ppl=nan, wps=7746.6, ups=0.38, wpb=20527.4, bsz=256, num_updates=7200, lr=0.000416667, gnorm=4.417, clip=1, loss_scale=433, train_wall=227, wall=19672
2022-07-09 21:33:45 | INFO | train_inner | epoch 007:    578 / 1122 loss=8.807, nll_loss=3.481, mask_ins=1.798, word_ins_ml=5.022, word_reposition=1.094, kpe=0.893, ppl=448, wps=7776.9, ups=0.38, wpb=20464.4, bsz=256, num_updates=7300, lr=0.000413803, gnorm=3.504, clip=0, loss_scale=256, train_wall=225, wall=19935
2022-07-09 21:38:08 | INFO | train_inner | epoch 007:    678 / 1122 loss=8.708, nll_loss=3.384, mask_ins=1.801, word_ins_ml=4.936, word_reposition=1.075, kpe=0.896, ppl=418.06, wps=7781.4, ups=0.38, wpb=20512, bsz=256, num_updates=7400, lr=0.000410997, gnorm=3.573, clip=0, loss_scale=256, train_wall=226, wall=20199
2022-07-09 21:42:32 | INFO | train_inner | epoch 007:    778 / 1122 loss=8.662, nll_loss=3.343, mask_ins=1.809, word_ins_ml=4.898, word_reposition=1.064, kpe=0.891, ppl=405.2, wps=7809.9, ups=0.38, wpb=20574.1, bsz=256, num_updates=7500, lr=0.000408248, gnorm=3.527, clip=0, loss_scale=256, train_wall=225, wall=20462
2022-07-09 21:47:19 | INFO | train_inner | epoch 007:    878 / 1122 loss=8.528, nll_loss=3.273, mask_ins=1.771, word_ins_ml=4.835, word_reposition=1.027, kpe=0.895, ppl=369.08, wps=7130.6, ups=0.35, wpb=20520.5, bsz=256, num_updates=7600, lr=0.000405554, gnorm=3.401, clip=0, loss_scale=256, train_wall=250, wall=20750
2022-07-09 21:51:43 | INFO | train_inner | epoch 007:    978 / 1122 loss=8.549, nll_loss=3.267, mask_ins=1.79, word_ins_ml=4.828, word_reposition=1.036, kpe=0.895, ppl=374.6, wps=7817.1, ups=0.38, wpb=20600.8, bsz=256, num_updates=7700, lr=0.000402911, gnorm=3.374, clip=0, loss_scale=305, train_wall=226, wall=21014
2022-07-09 21:56:06 | INFO | train_inner | epoch 007:   1078 / 1122 loss=8.513, nll_loss=3.229, mask_ins=1.796, word_ins_ml=4.793, word_reposition=1.024, kpe=0.9, ppl=365.38, wps=7791.7, ups=0.38, wpb=20515.3, bsz=256, num_updates=7800, lr=0.00040032, gnorm=3.507, clip=0, loss_scale=512, train_wall=226, wall=21277
2022-07-09 21:58:02 | INFO | train | epoch 007 | loss nan | nll_loss 3.492 | mask_ins 1.795 | word_ins_ml 5.031 | word_reposition 1.099 | kpe nan | ppl nan | wps 7523.2 | ups 0.37 | wpb 20520.2 | bsz 255.8 | num_updates 7844 | lr 0.000399196 | gnorm 3.794 | clip 0.2 | loss_scale 418 | train_wall 2557 | wall 21392
2022-07-09 21:59:07 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 12.514 | nll_loss 6.357 | mask_ins 1.876 | word_ins_ml 7.647 | word_reposition 1.514 | kpe 1.477 | ppl 5849.99 | wps 15230.9 | wpb 2367.6 | bsz 32 | num_updates 7844 | best_loss 12.514
2022-07-09 21:59:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_best.pt (epoch 7 @ 7844 updates, score 12.514) (writing took 10.852939808741212 seconds)
2022-07-09 21:59:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-09 21:59:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-09 22:01:51 | INFO | train_inner | epoch 008:     58 / 1122 loss=8.577, nll_loss=3.289, mask_ins=1.796, word_ins_ml=4.845, word_reposition=1.027, kpe=0.908, ppl=381.9, wps=5919.9, ups=0.29, wpb=20386, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=6.43, clip=1, loss_scale=321, train_wall=231, wall=21621
2022-07-09 22:06:14 | INFO | train_inner | epoch 008:    158 / 1122 loss=nan, nll_loss=3.099, mask_ins=1.789, word_ins_ml=4.676, word_reposition=0.987, kpe=nan, ppl=nan, wps=7772.7, ups=0.38, wpb=20511.7, bsz=256, num_updates=8000, lr=0.000395285, gnorm=4.297, clip=1, loss_scale=128, train_wall=226, wall=21885
2022-07-09 22:10:39 | INFO | train_inner | epoch 008:    258 / 1122 loss=8.32, nll_loss=3.104, mask_ins=1.78, word_ins_ml=4.679, word_reposition=0.987, kpe=0.873, ppl=319.58, wps=7755.9, ups=0.38, wpb=20480.2, bsz=256, num_updates=8100, lr=0.000392837, gnorm=5.166, clip=3, loss_scale=128, train_wall=226, wall=22149
2022-07-09 22:15:02 | INFO | train_inner | epoch 008:    358 / 1122 loss=8.246, nll_loss=3.048, mask_ins=1.777, word_ins_ml=4.629, word_reposition=0.97, kpe=0.869, ppl=303.58, wps=7812.7, ups=0.38, wpb=20560, bsz=256, num_updates=8200, lr=0.000390434, gnorm=5.392, clip=1, loss_scale=128, train_wall=226, wall=22412
2022-07-09 22:19:26 | INFO | train_inner | epoch 008:    458 / 1122 loss=8.18, nll_loss=3.001, mask_ins=1.757, word_ins_ml=4.587, word_reposition=0.964, kpe=0.872, ppl=289.95, wps=7781.7, ups=0.38, wpb=20564.6, bsz=256, num_updates=8300, lr=0.000388075, gnorm=3.435, clip=0, loss_scale=128, train_wall=227, wall=22677
2022-07-09 22:23:49 | INFO | train_inner | epoch 008:    558 / 1122 loss=8.157, nll_loss=2.973, mask_ins=1.766, word_ins_ml=4.561, word_reposition=0.955, kpe=0.876, ppl=285.48, wps=7800.6, ups=0.38, wpb=20553.8, bsz=256, num_updates=8400, lr=0.000385758, gnorm=3.61, clip=0, loss_scale=172, train_wall=226, wall=22940
2022-07-09 22:28:13 | INFO | train_inner | epoch 008:    658 / 1122 loss=8.167, nll_loss=3.002, mask_ins=1.757, word_ins_ml=4.586, word_reposition=0.95, kpe=0.875, ppl=287.5, wps=7819.2, ups=0.38, wpb=20603, bsz=256, num_updates=8500, lr=0.000383482, gnorm=3.524, clip=0, loss_scale=256, train_wall=226, wall=23204
2022-07-09 22:32:37 | INFO | train_inner | epoch 008:    758 / 1122 loss=8.088, nll_loss=2.926, mask_ins=1.763, word_ins_ml=4.517, word_reposition=0.933, kpe=0.874, ppl=272.01, wps=7764.9, ups=0.38, wpb=20481.6, bsz=256, num_updates=8600, lr=0.000381246, gnorm=3.234, clip=0, loss_scale=256, train_wall=226, wall=23467
2022-07-09 22:37:24 | INFO | train_inner | epoch 008:    858 / 1122 loss=nan, nll_loss=2.945, mask_ins=1.752, word_ins_ml=4.533, word_reposition=0.934, kpe=nan, ppl=nan, wps=7186.7, ups=0.35, wpb=20618.4, bsz=256, num_updates=8700, lr=0.000379049, gnorm=4.352, clip=1, loss_scale=256, train_wall=249, wall=23754
2022-07-09 22:41:50 | INFO | train_inner | epoch 008:    958 / 1122 loss=8.064, nll_loss=2.905, mask_ins=1.737, word_ins_ml=4.496, word_reposition=0.939, kpe=0.891, ppl=267.58, wps=7716.4, ups=0.38, wpb=20538.3, bsz=256, num_updates=8800, lr=0.000376889, gnorm=4.249, clip=1, loss_scale=256, train_wall=228, wall=24020
2022-07-09 22:46:14 | INFO | train_inner | epoch 008:   1058 / 1122 loss=8.053, nll_loss=2.907, mask_ins=1.742, word_ins_ml=4.498, word_reposition=0.925, kpe=0.889, ppl=265.62, wps=7715.5, ups=0.38, wpb=20347.9, bsz=256, num_updates=8900, lr=0.000374766, gnorm=3.999, clip=0, loss_scale=312, train_wall=226, wall=24284
2022-07-09 22:49:02 | INFO | train | epoch 008 | loss nan | nll_loss 3.002 | mask_ins 1.761 | word_ins_ml 4.586 | word_reposition 0.956 | kpe nan | ppl nan | wps 7510.1 | ups 0.37 | wpb 20520.2 | bsz 255.8 | num_updates 8964 | lr 0.000373426 | gnorm 4.309 | clip 0.7 | loss_scale 218 | train_wall 2563 | wall 24453
2022-07-09 22:50:07 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 12.303 | nll_loss 6.207 | mask_ins 1.819 | word_ins_ml 7.508 | word_reposition 1.509 | kpe 1.467 | ppl 5052.43 | wps 15245.3 | wpb 2367.6 | bsz 32 | num_updates 8964 | best_loss 12.303
2022-07-09 22:50:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_best.pt (epoch 8 @ 8964 updates, score 12.303) (writing took 10.598553525283933 seconds)
2022-07-09 22:51:53 | INFO | train_inner | epoch 009:     36 / 1122 loss=7.983, nll_loss=2.878, mask_ins=1.726, word_ins_ml=4.472, word_reposition=0.916, kpe=0.869, ppl=252.94, wps=5980.2, ups=0.29, wpb=20300.1, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=3.527, clip=0, loss_scale=512, train_wall=226, wall=24624
2022-07-09 22:56:17 | INFO | train_inner | epoch 009:    136 / 1122 loss=7.873, nll_loss=2.791, mask_ins=1.723, word_ins_ml=4.394, word_reposition=0.909, kpe=0.847, ppl=234.4, wps=7829.7, ups=0.38, wpb=20653.5, bsz=256, num_updates=9100, lr=0.000370625, gnorm=3.539, clip=0, loss_scale=512, train_wall=227, wall=24887
2022-07-09 23:00:41 | INFO | train_inner | epoch 009:    236 / 1122 loss=7.872, nll_loss=2.787, mask_ins=1.729, word_ins_ml=4.39, word_reposition=0.901, kpe=0.852, ppl=234.23, wps=7741.8, ups=0.38, wpb=20473.5, bsz=256, num_updates=9200, lr=0.000368605, gnorm=3.562, clip=0, loss_scale=512, train_wall=227, wall=25152
2022-07-09 23:05:05 | INFO | train_inner | epoch 009:    336 / 1122 loss=7.767, nll_loss=2.708, mask_ins=1.711, word_ins_ml=4.319, word_reposition=0.888, kpe=0.849, ppl=217.75, wps=7757.9, ups=0.38, wpb=20468.8, bsz=256, num_updates=9300, lr=0.000366618, gnorm=3.185, clip=0, loss_scale=512, train_wall=226, wall=25416
2022-07-09 23:09:29 | INFO | train_inner | epoch 009:    436 / 1122 loss=nan, nll_loss=2.72, mask_ins=1.699, word_ins_ml=4.328, word_reposition=0.894, kpe=nan, ppl=nan, wps=7762.3, ups=0.38, wpb=20449.6, bsz=256, num_updates=9400, lr=0.000364662, gnorm=3.171, clip=0, loss_scale=563, train_wall=226, wall=25679
2022-07-09 23:12:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-09 23:13:55 | INFO | train_inner | epoch 009:    537 / 1122 loss=7.793, nll_loss=2.75, mask_ins=1.682, word_ins_ml=4.355, word_reposition=0.897, kpe=0.858, ppl=221.76, wps=7709.6, ups=0.37, wpb=20565, bsz=256, num_updates=9500, lr=0.000362738, gnorm=3.589, clip=0, loss_scale=902, train_wall=229, wall=25946
2022-07-09 23:14:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-09 23:18:22 | INFO | train_inner | epoch 009:    638 / 1122 loss=7.769, nll_loss=2.733, mask_ins=1.686, word_ins_ml=4.339, word_reposition=0.891, kpe=0.853, ppl=218.08, wps=7701.9, ups=0.37, wpb=20579.2, bsz=256, num_updates=9600, lr=0.000360844, gnorm=3.341, clip=1, loss_scale=297, train_wall=229, wall=26213
2022-07-09 23:22:45 | INFO | train_inner | epoch 009:    738 / 1122 loss=7.722, nll_loss=2.692, mask_ins=1.686, word_ins_ml=4.301, word_reposition=0.884, kpe=0.851, ppl=211.18, wps=7819.2, ups=0.38, wpb=20545.1, bsz=256, num_updates=9700, lr=0.000358979, gnorm=3.165, clip=0, loss_scale=256, train_wall=225, wall=26476
2022-07-09 23:27:09 | INFO | train_inner | epoch 009:    838 / 1122 loss=7.717, nll_loss=2.724, mask_ins=1.644, word_ins_ml=4.329, word_reposition=0.881, kpe=0.864, ppl=210.47, wps=7776.2, ups=0.38, wpb=20492.9, bsz=256, num_updates=9800, lr=0.000357143, gnorm=3.24, clip=0, loss_scale=256, train_wall=226, wall=26739
2022-07-09 23:31:58 | INFO | train_inner | epoch 009:    938 / 1122 loss=7.627, nll_loss=2.658, mask_ins=1.631, word_ins_ml=4.27, word_reposition=0.873, kpe=0.853, ppl=197.63, wps=7117.4, ups=0.35, wpb=20599.4, bsz=256, num_updates=9900, lr=0.000355335, gnorm=3.126, clip=0, loss_scale=256, train_wall=252, wall=27029
2022-07-09 23:36:22 | INFO | train_inner | epoch 009:   1038 / 1122 loss=nan, nll_loss=2.687, mask_ins=1.604, word_ins_ml=4.295, word_reposition=0.876, kpe=nan, ppl=nan, wps=7766.5, ups=0.38, wpb=20488.6, bsz=256, num_updates=10000, lr=0.000353553, gnorm=3.074, clip=0, loss_scale=256, train_wall=226, wall=27293
2022-07-09 23:40:04 | INFO | train | epoch 009 | loss nan | nll_loss 2.73 | mask_ins 1.673 | word_ins_ml 4.336 | word_reposition 0.889 | kpe nan | ppl nan | wps 7506.5 | ups 0.37 | wpb 20521.9 | bsz 255.8 | num_updates 10084 | lr 0.000352078 | gnorm 3.314 | clip 0.1 | loss_scale 435 | train_wall 2565 | wall 27515
2022-07-09 23:41:09 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 12.09 | nll_loss 6.141 | mask_ins 1.766 | word_ins_ml 7.457 | word_reposition 1.454 | kpe 1.413 | ppl 4360.12 | wps 15197.3 | wpb 2367.6 | bsz 32 | num_updates 10084 | best_loss 12.09
2022-07-09 23:41:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_best.pt (epoch 9 @ 10084 updates, score 12.09) (writing took 10.556424888782203 seconds)
2022-07-09 23:42:02 | INFO | train_inner | epoch 010:     16 / 1122 loss=7.613, nll_loss=2.715, mask_ins=1.572, word_ins_ml=4.319, word_reposition=0.872, kpe=0.85, ppl=195.76, wps=6031.6, ups=0.29, wpb=20499.8, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=3.331, clip=0, loss_scale=443, train_wall=227, wall=27633
2022-07-09 23:46:24 | INFO | train_inner | epoch 010:    116 / 1122 loss=7.359, nll_loss=2.569, mask_ins=1.498, word_ins_ml=4.189, word_reposition=0.861, kpe=0.811, ppl=164.17, wps=7838.3, ups=0.38, wpb=20579.8, bsz=256, num_updates=10200, lr=0.00035007, gnorm=3.159, clip=0, loss_scale=512, train_wall=225, wall=27895
2022-07-09 23:47:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-09 23:49:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-09 23:50:52 | INFO | train_inner | epoch 010:    218 / 1122 loss=7.426, nll_loss=2.656, mask_ins=1.458, word_ins_ml=4.266, word_reposition=0.869, kpe=0.833, ppl=172.02, wps=7683.9, ups=0.37, wpb=20519.6, bsz=256, num_updates=10300, lr=0.000348367, gnorm=4.617, clip=0, loss_scale=257, train_wall=229, wall=28162
2022-07-09 23:55:15 | INFO | train_inner | epoch 010:    318 / 1122 loss=7.31, nll_loss=2.581, mask_ins=1.41, word_ins_ml=4.199, word_reposition=0.868, kpe=0.834, ppl=158.71, wps=7827.2, ups=0.38, wpb=20586.6, bsz=256, num_updates=10400, lr=0.000346688, gnorm=3.616, clip=1, loss_scale=128, train_wall=226, wall=28425
2022-07-09 23:59:37 | INFO | train_inner | epoch 010:    418 / 1122 loss=7.215, nll_loss=2.585, mask_ins=1.341, word_ins_ml=4.201, word_reposition=0.852, kpe=0.82, ppl=148.52, wps=7818.5, ups=0.38, wpb=20554, bsz=256, num_updates=10500, lr=0.000345033, gnorm=3.081, clip=0, loss_scale=128, train_wall=225, wall=28688
2022-07-10 00:04:00 | INFO | train_inner | epoch 010:    518 / 1122 loss=7.175, nll_loss=2.575, mask_ins=1.307, word_ins_ml=4.192, word_reposition=0.854, kpe=0.822, ppl=144.51, wps=7812, ups=0.38, wpb=20524.2, bsz=256, num_updates=10600, lr=0.000343401, gnorm=3.342, clip=0, loss_scale=128, train_wall=225, wall=28951
2022-07-10 00:08:22 | INFO | train_inner | epoch 010:    618 / 1122 loss=nan, nll_loss=2.582, mask_ins=1.291, word_ins_ml=4.198, word_reposition=0.853, kpe=nan, ppl=nan, wps=7901.3, ups=0.38, wpb=20685.2, bsz=256, num_updates=10700, lr=0.000341793, gnorm=3.655, clip=0, loss_scale=128, train_wall=224, wall=29213
2022-07-10 00:12:46 | INFO | train_inner | epoch 010:    718 / 1122 loss=7.162, nll_loss=2.593, mask_ins=1.261, word_ins_ml=4.207, word_reposition=0.852, kpe=0.842, ppl=143.21, wps=7736.3, ups=0.38, wpb=20401.7, bsz=256, num_updates=10800, lr=0.000340207, gnorm=3.668, clip=0, loss_scale=165, train_wall=226, wall=29476
2022-07-10 00:17:10 | INFO | train_inner | epoch 010:    818 / 1122 loss=nan, nll_loss=2.57, mask_ins=1.237, word_ins_ml=4.187, word_reposition=0.852, kpe=nan, ppl=nan, wps=7771.9, ups=0.38, wpb=20566.7, bsz=256, num_updates=10900, lr=0.000338643, gnorm=3.74, clip=0, loss_scale=256, train_wall=227, wall=29741
2022-07-10 00:18:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-10 00:18:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-07-10 00:22:05 | INFO | train_inner | epoch 010:    920 / 1122 loss=7.101, nll_loss=2.566, mask_ins=1.227, word_ins_ml=4.182, word_reposition=0.846, kpe=0.845, ppl=137.3, wps=6974.9, ups=0.34, wpb=20537.3, bsz=256, num_updates=11000, lr=0.0003371, gnorm=4.244, clip=0, loss_scale=119, train_wall=256, wall=30035
2022-07-10 00:26:29 | INFO | train_inner | epoch 010:   1020 / 1122 loss=7.056, nll_loss=2.56, mask_ins=1.2, word_ins_ml=4.176, word_reposition=0.834, kpe=0.846, ppl=133.1, wps=7727.5, ups=0.38, wpb=20448.3, bsz=256, num_updates=11100, lr=0.000335578, gnorm=6.495, clip=7, loss_scale=64, train_wall=227, wall=30300
2022-07-10 00:30:52 | INFO | train_inner | epoch 010:   1120 / 1122 loss=6.959, nll_loss=2.489, mask_ins=1.179, word_ins_ml=4.113, word_reposition=0.835, kpe=0.831, ppl=124.44, wps=7791.9, ups=0.38, wpb=20470.7, bsz=256, num_updates=11200, lr=0.000334077, gnorm=3.282, clip=0, loss_scale=64, train_wall=225, wall=30563
2022-07-10 00:30:56 | INFO | train | epoch 010 | loss nan | nll_loss 2.576 | mask_ins 1.314 | word_ins_ml 4.193 | word_reposition 0.853 | kpe nan | ppl nan | wps 7516.1 | ups 0.37 | wpb 20521.9 | bsz 255.8 | num_updates 11202 | lr 0.000334047 | gnorm 3.896 | clip 0.7 | loss_scale 182 | train_wall 2554 | wall 30567
2022-07-10 00:32:02 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 12.113 | nll_loss 6.185 | mask_ins 1.664 | word_ins_ml 7.511 | word_reposition 1.475 | kpe 1.463 | ppl 4430.11 | wps 15198.8 | wpb 2367.6 | bsz 32 | num_updates 11202 | best_loss 12.09
2022-07-10 00:32:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 10 @ 11202 updates, score 12.113) (writing took 6.185108920559287 seconds)
2022-07-10 00:36:26 | INFO | train_inner | epoch 011:     98 / 1122 loss=6.916, nll_loss=2.503, mask_ins=1.162, word_ins_ml=4.126, word_reposition=0.842, kpe=0.787, ppl=120.78, wps=6109.8, ups=0.3, wpb=20417.5, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=3.631, clip=1, loss_scale=64, train_wall=226, wall=30897
2022-07-10 00:40:50 | INFO | train_inner | epoch 011:    198 / 1122 loss=nan, nll_loss=2.41, mask_ins=1.139, word_ins_ml=4.043, word_reposition=0.819, kpe=nan, ppl=nan, wps=7779.6, ups=0.38, wpb=20494.3, bsz=256, num_updates=11400, lr=0.000331133, gnorm=3.074, clip=0, loss_scale=64, train_wall=226, wall=31160
2022-07-10 00:45:13 | INFO | train_inner | epoch 011:    298 / 1122 loss=nan, nll_loss=2.455, mask_ins=1.136, word_ins_ml=4.082, word_reposition=0.826, kpe=nan, ppl=nan, wps=7791.1, ups=0.38, wpb=20476.2, bsz=256, num_updates=11500, lr=0.00032969, gnorm=2.97, clip=0, loss_scale=102, train_wall=225, wall=31423
2022-07-10 00:49:35 | INFO | train_inner | epoch 011:    398 / 1122 loss=6.784, nll_loss=2.431, mask_ins=1.12, word_ins_ml=4.059, word_reposition=0.816, kpe=0.79, ppl=110.23, wps=7830.3, ups=0.38, wpb=20590.3, bsz=256, num_updates=11600, lr=0.000328266, gnorm=3.097, clip=0, loss_scale=128, train_wall=225, wall=31686
2022-07-10 00:53:59 | INFO | train_inner | epoch 011:    498 / 1122 loss=6.833, nll_loss=2.469, mask_ins=1.111, word_ins_ml=4.093, word_reposition=0.832, kpe=0.798, ppl=114.01, wps=7817.7, ups=0.38, wpb=20620.6, bsz=256, num_updates=11700, lr=0.00032686, gnorm=3.006, clip=0, loss_scale=128, train_wall=226, wall=31950
2022-07-10 00:58:23 | INFO | train_inner | epoch 011:    598 / 1122 loss=6.78, nll_loss=2.431, mask_ins=1.102, word_ins_ml=4.058, word_reposition=0.826, kpe=0.795, ppl=109.92, wps=7814.3, ups=0.38, wpb=20575.7, bsz=256, num_updates=11800, lr=0.000325472, gnorm=2.957, clip=0, loss_scale=128, train_wall=226, wall=32213
2022-07-10 01:02:46 | INFO | train_inner | epoch 011:    698 / 1122 loss=6.762, nll_loss=2.416, mask_ins=1.098, word_ins_ml=4.045, word_reposition=0.817, kpe=0.802, ppl=108.54, wps=7797.5, ups=0.38, wpb=20510.4, bsz=256, num_updates=11900, lr=0.000324102, gnorm=2.932, clip=0, loss_scale=128, train_wall=226, wall=32476
2022-07-10 01:07:09 | INFO | train_inner | epoch 011:    798 / 1122 loss=6.774, nll_loss=2.449, mask_ins=1.081, word_ins_ml=4.074, word_reposition=0.812, kpe=0.807, ppl=109.41, wps=7721.2, ups=0.38, wpb=20341.8, bsz=256, num_updates=12000, lr=0.000322749, gnorm=2.945, clip=0, loss_scale=189, train_wall=226, wall=32740
2022-07-10 01:11:45 | INFO | train_inner | epoch 011:    898 / 1122 loss=6.701, nll_loss=2.39, mask_ins=1.063, word_ins_ml=4.021, word_reposition=0.815, kpe=0.802, ppl=104.08, wps=7459.2, ups=0.36, wpb=20607.2, bsz=256, num_updates=12100, lr=0.000321412, gnorm=2.915, clip=0, loss_scale=256, train_wall=238, wall=33016
2022-07-10 01:16:21 | INFO | train_inner | epoch 011:    998 / 1122 loss=6.726, nll_loss=2.398, mask_ins=1.079, word_ins_ml=4.027, word_reposition=0.812, kpe=0.808, ppl=105.83, wps=7427.5, ups=0.36, wpb=20502.1, bsz=256, num_updates=12200, lr=0.000320092, gnorm=2.968, clip=0, loss_scale=256, train_wall=238, wall=33292
2022-07-10 01:20:44 | INFO | train_inner | epoch 011:   1098 / 1122 loss=6.738, nll_loss=2.424, mask_ins=1.064, word_ins_ml=4.049, word_reposition=0.818, kpe=0.806, ppl=106.75, wps=7819.5, ups=0.38, wpb=20567.2, bsz=256, num_updates=12300, lr=0.000318788, gnorm=3.196, clip=0, loss_scale=256, train_wall=225, wall=33555
2022-07-10 01:21:47 | INFO | train | epoch 011 | loss nan | nll_loss 2.432 | mask_ins 1.104 | word_ins_ml 4.059 | word_reposition 0.821 | kpe nan | ppl nan | wps 7548.7 | ups 0.37 | wpb 20521.7 | bsz 255.8 | num_updates 12324 | lr 0.000318478 | gnorm 3.055 | clip 0.1 | loss_scale 157 | train_wall 2557 | wall 33617
2022-07-10 01:22:52 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 11.635 | nll_loss 5.878 | mask_ins 1.607 | word_ins_ml 7.222 | word_reposition 1.39 | kpe 1.416 | ppl 3180.01 | wps 15142.9 | wpb 2367.6 | bsz 32 | num_updates 12324 | best_loss 11.635
2022-07-10 01:23:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_best.pt (epoch 11 @ 12324 updates, score 11.635) (writing took 10.566776664927602 seconds)
2022-07-10 01:24:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-10 01:26:26 | INFO | train_inner | epoch 012:     77 / 1122 loss=6.632, nll_loss=2.38, mask_ins=1.044, word_ins_ml=4.01, word_reposition=0.807, kpe=0.771, ppl=99.18, wps=5983.6, ups=0.29, wpb=20446.1, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=4.246, clip=2, loss_scale=198, train_wall=228, wall=33897
2022-07-10 01:30:49 | INFO | train_inner | epoch 012:    177 / 1122 loss=6.535, nll_loss=2.309, mask_ins=1.033, word_ins_ml=3.947, word_reposition=0.807, kpe=0.748, ppl=92.74, wps=7794, ups=0.38, wpb=20494.3, bsz=256, num_updates=12500, lr=0.000316228, gnorm=3.04, clip=0, loss_scale=128, train_wall=226, wall=34160
2022-07-10 01:35:12 | INFO | train_inner | epoch 012:    277 / 1122 loss=6.558, nll_loss=2.338, mask_ins=1.036, word_ins_ml=3.972, word_reposition=0.801, kpe=0.749, ppl=94.21, wps=7810.4, ups=0.38, wpb=20509.5, bsz=256, num_updates=12600, lr=0.00031497, gnorm=2.814, clip=0, loss_scale=128, train_wall=225, wall=34422
2022-07-10 01:39:35 | INFO | train_inner | epoch 012:    377 / 1122 loss=6.561, nll_loss=2.343, mask_ins=1.018, word_ins_ml=3.976, word_reposition=0.807, kpe=0.76, ppl=94.44, wps=7837.8, ups=0.38, wpb=20609.1, bsz=256, num_updates=12700, lr=0.000313728, gnorm=2.962, clip=0, loss_scale=128, train_wall=225, wall=34685
2022-07-10 01:43:58 | INFO | train_inner | epoch 012:    477 / 1122 loss=6.571, nll_loss=2.355, mask_ins=1.018, word_ins_ml=3.986, word_reposition=0.802, kpe=0.765, ppl=95.07, wps=7788, ups=0.38, wpb=20496.2, bsz=256, num_updates=12800, lr=0.0003125, gnorm=2.901, clip=0, loss_scale=128, train_wall=226, wall=34948
2022-07-10 01:48:22 | INFO | train_inner | epoch 012:    577 / 1122 loss=6.532, nll_loss=2.308, mask_ins=1.019, word_ins_ml=3.944, word_reposition=0.805, kpe=0.764, ppl=92.53, wps=7789.5, ups=0.38, wpb=20579.3, bsz=256, num_updates=12900, lr=0.000311286, gnorm=2.874, clip=0, loss_scale=172, train_wall=227, wall=35213
2022-07-10 01:52:45 | INFO | train_inner | epoch 012:    677 / 1122 loss=6.506, nll_loss=2.306, mask_ins=1.005, word_ins_ml=3.942, word_reposition=0.796, kpe=0.763, ppl=90.89, wps=7756.9, ups=0.38, wpb=20415.2, bsz=256, num_updates=13000, lr=0.000310087, gnorm=2.81, clip=0, loss_scale=256, train_wall=225, wall=35476
2022-07-10 01:57:09 | INFO | train_inner | epoch 012:    777 / 1122 loss=6.512, nll_loss=2.321, mask_ins=1.001, word_ins_ml=3.954, word_reposition=0.792, kpe=0.765, ppl=91.27, wps=7815.3, ups=0.38, wpb=20592.1, bsz=256, num_updates=13100, lr=0.000308901, gnorm=2.854, clip=0, loss_scale=256, train_wall=226, wall=35739
2022-07-10 02:01:32 | INFO | train_inner | epoch 012:    877 / 1122 loss=nan, nll_loss=2.327, mask_ins=1.011, word_ins_ml=3.96, word_reposition=0.802, kpe=nan, ppl=nan, wps=7803.5, ups=0.38, wpb=20561, bsz=256, num_updates=13200, lr=0.000307729, gnorm=2.871, clip=0, loss_scale=256, train_wall=226, wall=36003
2022-07-10 02:06:28 | INFO | train_inner | epoch 012:    977 / 1122 loss=6.478, nll_loss=2.287, mask_ins=0.994, word_ins_ml=3.924, word_reposition=0.791, kpe=0.769, ppl=89.13, wps=6947.4, ups=0.34, wpb=20520.9, bsz=256, num_updates=13300, lr=0.00030657, gnorm=2.776, clip=0, loss_scale=256, train_wall=257, wall=36298
2022-07-10 02:10:51 | INFO | train_inner | epoch 012:   1077 / 1122 loss=nan, nll_loss=2.321, mask_ins=0.996, word_ins_ml=3.953, word_reposition=0.791, kpe=nan, ppl=nan, wps=7769.9, ups=0.38, wpb=20482.9, bsz=256, num_updates=13400, lr=0.000305424, gnorm=2.715, clip=0, loss_scale=312, train_wall=226, wall=36562
2022-07-10 02:12:49 | INFO | train | epoch 012 | loss nan | nll_loss 2.326 | mask_ins 1.014 | word_ins_ml 3.96 | word_reposition 0.8 | kpe nan | ppl nan | wps 7510.9 | ups 0.37 | wpb 20519.4 | bsz 255.8 | num_updates 13445 | lr 0.000304912 | gnorm 2.983 | clip 0.2 | loss_scale 213 | train_wall 2564 | wall 36680
2022-07-10 02:13:54 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.755 | nll_loss 5.869 | mask_ins 1.612 | word_ins_ml 7.205 | word_reposition 1.442 | kpe 1.495 | ppl 3455.29 | wps 15249.5 | wpb 2367.6 | bsz 32 | num_updates 13445 | best_loss 11.635
2022-07-10 02:14:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 12 @ 13445 updates, score 11.755) (writing took 5.869022922590375 seconds)
2022-07-10 02:16:25 | INFO | train_inner | epoch 013:     55 / 1122 loss=6.418, nll_loss=2.263, mask_ins=0.987, word_ins_ml=3.903, word_reposition=0.787, kpe=0.74, ppl=85.51, wps=6116.1, ups=0.3, wpb=20389.8, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=2.898, clip=0, loss_scale=512, train_wall=226, wall=36895
2022-07-10 02:20:47 | INFO | train_inner | epoch 013:    155 / 1122 loss=6.394, nll_loss=2.275, mask_ins=0.979, word_ins_ml=3.912, word_reposition=0.79, kpe=0.713, ppl=84.11, wps=7829.8, ups=0.38, wpb=20589.8, bsz=256, num_updates=13600, lr=0.00030317, gnorm=2.766, clip=0, loss_scale=512, train_wall=225, wall=37158
2022-07-10 02:25:12 | INFO | train_inner | epoch 013:    255 / 1122 loss=6.403, nll_loss=2.275, mask_ins=0.98, word_ins_ml=3.913, word_reposition=0.786, kpe=0.724, ppl=84.62, wps=7742.6, ups=0.38, wpb=20456.7, bsz=256, num_updates=13700, lr=0.000302061, gnorm=2.876, clip=0, loss_scale=512, train_wall=227, wall=37422
2022-07-10 02:29:35 | INFO | train_inner | epoch 013:    355 / 1122 loss=nan, nll_loss=2.277, mask_ins=0.958, word_ins_ml=3.914, word_reposition=0.783, kpe=nan, ppl=nan, wps=7784.7, ups=0.38, wpb=20524.6, bsz=256, num_updates=13800, lr=0.000300965, gnorm=2.807, clip=0, loss_scale=512, train_wall=226, wall=37686
2022-07-10 02:33:59 | INFO | train_inner | epoch 013:    455 / 1122 loss=6.375, nll_loss=2.245, mask_ins=0.978, word_ins_ml=3.885, word_reposition=0.789, kpe=0.723, ppl=83, wps=7813.9, ups=0.38, wpb=20610.1, bsz=256, num_updates=13900, lr=0.00029988, gnorm=2.77, clip=0, loss_scale=563, train_wall=226, wall=37950
2022-07-10 02:38:23 | INFO | train_inner | epoch 013:    555 / 1122 loss=6.395, nll_loss=2.286, mask_ins=0.962, word_ins_ml=3.92, word_reposition=0.781, kpe=0.731, ppl=84.13, wps=7787.6, ups=0.38, wpb=20514.5, bsz=256, num_updates=14000, lr=0.000298807, gnorm=2.84, clip=0, loss_scale=1024, train_wall=226, wall=38213
2022-07-10 02:42:47 | INFO | train_inner | epoch 013:    655 / 1122 loss=6.357, nll_loss=2.243, mask_ins=0.959, word_ins_ml=3.882, word_reposition=0.784, kpe=0.733, ppl=81.98, wps=7755.7, ups=0.38, wpb=20474.8, bsz=256, num_updates=14100, lr=0.000297746, gnorm=2.88, clip=0, loss_scale=1024, train_wall=227, wall=38477
2022-07-10 02:46:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 02:47:13 | INFO | train_inner | epoch 013:    756 / 1122 loss=nan, nll_loss=2.272, mask_ins=0.952, word_ins_ml=3.907, word_reposition=0.784, kpe=nan, ppl=nan, wps=7699, ups=0.38, wpb=20529.1, bsz=256, num_updates=14200, lr=0.000296695, gnorm=3.078, clip=0, loss_scale=882, train_wall=228, wall=38744
2022-07-10 02:51:37 | INFO | train_inner | epoch 013:    856 / 1122 loss=6.372, nll_loss=2.253, mask_ins=0.96, word_ins_ml=3.89, word_reposition=0.781, kpe=0.74, ppl=82.84, wps=7787, ups=0.38, wpb=20513.8, bsz=256, num_updates=14300, lr=0.000295656, gnorm=3.115, clip=1, loss_scale=512, train_wall=226, wall=39007
2022-07-10 02:56:12 | INFO | train_inner | epoch 013:    956 / 1122 loss=6.333, nll_loss=2.235, mask_ins=0.949, word_ins_ml=3.874, word_reposition=0.777, kpe=0.733, ppl=80.61, wps=7436.8, ups=0.36, wpb=20479.1, bsz=256, num_updates=14400, lr=0.000294628, gnorm=2.702, clip=0, loss_scale=512, train_wall=238, wall=39283
2022-07-10 03:00:50 | INFO | train_inner | epoch 013:   1056 / 1122 loss=6.336, nll_loss=2.235, mask_ins=0.945, word_ins_ml=3.874, word_reposition=0.778, kpe=0.739, ppl=80.76, wps=7438.2, ups=0.36, wpb=20644.2, bsz=256, num_updates=14500, lr=0.00029361, gnorm=2.781, clip=0, loss_scale=512, train_wall=240, wall=39560
2022-07-10 03:03:43 | INFO | train | epoch 013 | loss nan | nll_loss 2.259 | mask_ins 0.963 | word_ins_ml 3.896 | word_reposition 0.783 | kpe nan | ppl nan | wps 7533.2 | ups 0.37 | wpb 20519.5 | bsz 255.8 | num_updates 14566 | lr 0.000292944 | gnorm 2.868 | clip 0.1 | loss_scale 641 | train_wall 2561 | wall 39733
2022-07-10 03:04:48 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 11.851 | nll_loss 5.886 | mask_ins 1.586 | word_ins_ml 7.215 | word_reposition 1.493 | kpe 1.556 | ppl 3694.51 | wps 15255 | wpb 2367.6 | bsz 32 | num_updates 14566 | best_loss 11.635
2022-07-10 03:04:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 13 @ 14566 updates, score 11.851) (writing took 5.746426869183779 seconds)
2022-07-10 03:06:23 | INFO | train_inner | epoch 014:     34 / 1122 loss=6.339, nll_loss=2.257, mask_ins=0.944, word_ins_ml=3.893, word_reposition=0.772, kpe=0.729, ppl=80.93, wps=6125.3, ups=0.3, wpb=20397.3, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=2.968, clip=0, loss_scale=512, train_wall=225, wall=39893
2022-07-10 03:10:46 | INFO | train_inner | epoch 014:    134 / 1122 loss=6.23, nll_loss=2.181, mask_ins=0.945, word_ins_ml=3.826, word_reposition=0.777, kpe=0.682, ppl=75.06, wps=7785.2, ups=0.38, wpb=20495.2, bsz=256, num_updates=14700, lr=0.000291606, gnorm=2.834, clip=0, loss_scale=594, train_wall=226, wall=40157
2022-07-10 03:15:05 | INFO | train_inner | epoch 014:    234 / 1122 loss=6.162, nll_loss=2.139, mask_ins=0.926, word_ins_ml=3.788, word_reposition=0.766, kpe=0.682, ppl=71.6, wps=7921.9, ups=0.39, wpb=20552.3, bsz=256, num_updates=14800, lr=0.000290619, gnorm=2.674, clip=0, loss_scale=1024, train_wall=222, wall=40416
2022-07-10 03:17:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 03:19:29 | INFO | train_inner | epoch 014:    335 / 1122 loss=nan, nll_loss=2.164, mask_ins=0.919, word_ins_ml=3.81, word_reposition=0.77, kpe=nan, ppl=nan, wps=7805.4, ups=0.38, wpb=20559.2, bsz=256, num_updates=14900, lr=0.000289642, gnorm=2.83, clip=0, loss_scale=740, train_wall=225, wall=40679
2022-07-10 03:23:49 | INFO | train_inner | epoch 014:    435 / 1122 loss=6.183, nll_loss=2.149, mask_ins=0.924, word_ins_ml=3.796, word_reposition=0.77, kpe=0.692, ppl=72.63, wps=7881.7, ups=0.38, wpb=20517.6, bsz=256, num_updates=15000, lr=0.000288675, gnorm=2.781, clip=0, loss_scale=512, train_wall=223, wall=40940
2022-07-10 03:28:11 | INFO | train_inner | epoch 014:    535 / 1122 loss=6.185, nll_loss=2.148, mask_ins=0.926, word_ins_ml=3.795, word_reposition=0.773, kpe=0.69, ppl=72.75, wps=7817.3, ups=0.38, wpb=20487.2, bsz=256, num_updates=15100, lr=0.000287718, gnorm=2.818, clip=0, loss_scale=512, train_wall=225, wall=41202
2022-07-10 03:32:32 | INFO | train_inner | epoch 014:    635 / 1122 loss=6.225, nll_loss=2.185, mask_ins=0.928, word_ins_ml=3.828, word_reposition=0.771, kpe=0.698, ppl=74.78, wps=7870.5, ups=0.38, wpb=20528.4, bsz=256, num_updates=15200, lr=0.00028677, gnorm=2.871, clip=0, loss_scale=512, train_wall=223, wall=41463
2022-07-10 03:36:51 | INFO | train_inner | epoch 014:    735 / 1122 loss=6.185, nll_loss=2.159, mask_ins=0.91, word_ins_ml=3.804, word_reposition=0.771, kpe=0.7, ppl=72.77, wps=7950.1, ups=0.39, wpb=20634.6, bsz=256, num_updates=15300, lr=0.000285831, gnorm=2.802, clip=0, loss_scale=512, train_wall=222, wall=41722
2022-07-10 03:41:13 | INFO | train_inner | epoch 014:    835 / 1122 loss=6.245, nll_loss=2.208, mask_ins=0.921, word_ins_ml=3.847, word_reposition=0.768, kpe=0.708, ppl=75.85, wps=7822.7, ups=0.38, wpb=20418.8, bsz=256, num_updates=15400, lr=0.000284901, gnorm=2.79, clip=0, loss_scale=737, train_wall=223, wall=41983
2022-07-10 03:45:34 | INFO | train_inner | epoch 014:    935 / 1122 loss=6.218, nll_loss=2.186, mask_ins=0.918, word_ins_ml=3.827, word_reposition=0.765, kpe=0.707, ppl=74.45, wps=7839.2, ups=0.38, wpb=20504.7, bsz=256, num_updates=15500, lr=0.000283981, gnorm=2.69, clip=0, loss_scale=1024, train_wall=224, wall=42245
2022-07-10 03:50:21 | INFO | train_inner | epoch 014:   1035 / 1122 loss=6.178, nll_loss=2.151, mask_ins=0.906, word_ins_ml=3.796, word_reposition=0.768, kpe=0.709, ppl=72.42, wps=7160.5, ups=0.35, wpb=20540.5, bsz=256, num_updates=15600, lr=0.000283069, gnorm=2.777, clip=0, loss_scale=1024, train_wall=249, wall=42532
2022-07-10 03:54:07 | INFO | train | epoch 014 | loss nan | nll_loss 2.166 | mask_ins 0.922 | word_ins_ml 3.811 | word_reposition 0.77 | kpe nan | ppl nan | wps 7607.1 | ups 0.37 | wpb 20521.4 | bsz 255.8 | num_updates 15687 | lr 0.000282283 | gnorm 2.809 | clip 0 | loss_scale 737 | train_wall 2532 | wall 42757
2022-07-10 03:55:12 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.557 | nll_loss 5.793 | mask_ins 1.529 | word_ins_ml 7.142 | word_reposition 1.339 | kpe 1.547 | ppl 3012.46 | wps 15086.6 | wpb 2367.6 | bsz 32 | num_updates 15687 | best_loss 11.557
2022-07-10 03:55:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_best.pt (epoch 14 @ 15687 updates, score 11.557) (writing took 10.774940002709627 seconds)
2022-07-10 03:55:57 | INFO | train_inner | epoch 015:     13 / 1122 loss=nan, nll_loss=2.132, mask_ins=0.912, word_ins_ml=3.779, word_reposition=0.768, kpe=nan, ppl=nan, wps=6090.7, ups=0.3, wpb=20470.5, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=2.98, clip=0, loss_scale=1024, train_wall=222, wall=42868
2022-07-10 04:00:18 | INFO | train_inner | epoch 015:    113 / 1122 loss=6.089, nll_loss=2.128, mask_ins=0.899, word_ins_ml=3.776, word_reposition=0.766, kpe=0.648, ppl=68.07, wps=7906.7, ups=0.38, wpb=20648.4, bsz=256, num_updates=15800, lr=0.000281272, gnorm=2.804, clip=0, loss_scale=1024, train_wall=223, wall=43129
2022-07-10 04:02:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 04:04:43 | INFO | train_inner | epoch 015:    214 / 1122 loss=6.054, nll_loss=2.097, mask_ins=0.893, word_ins_ml=3.748, word_reposition=0.764, kpe=0.65, ppl=66.45, wps=7777.6, ups=0.38, wpb=20583.6, bsz=256, num_updates=15900, lr=0.000280386, gnorm=2.795, clip=0, loss_scale=710, train_wall=227, wall=43394
2022-07-10 04:05:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-10 04:09:07 | INFO | train_inner | epoch 015:    315 / 1122 loss=nan, nll_loss=2.123, mask_ins=0.898, word_ins_ml=3.771, word_reposition=0.758, kpe=nan, ppl=nan, wps=7781.7, ups=0.38, wpb=20534.8, bsz=256, num_updates=16000, lr=0.000279508, gnorm=4.127, clip=1, loss_scale=294, train_wall=226, wall=43657
2022-07-10 04:13:29 | INFO | train_inner | epoch 015:    415 / 1122 loss=6.093, nll_loss=2.127, mask_ins=0.896, word_ins_ml=3.773, word_reposition=0.762, kpe=0.662, ppl=68.26, wps=7839.9, ups=0.38, wpb=20540.6, bsz=256, num_updates=16100, lr=0.000278639, gnorm=2.86, clip=0, loss_scale=256, train_wall=224, wall=43919
2022-07-10 04:17:48 | INFO | train_inner | epoch 015:    515 / 1122 loss=6.096, nll_loss=2.129, mask_ins=0.892, word_ins_ml=3.775, word_reposition=0.758, kpe=0.671, ppl=68.41, wps=7886.1, ups=0.39, wpb=20479.4, bsz=256, num_updates=16200, lr=0.000277778, gnorm=3.09, clip=0, loss_scale=256, train_wall=222, wall=44179
2022-07-10 04:22:10 | INFO | train_inner | epoch 015:    615 / 1122 loss=nan, nll_loss=2.144, mask_ins=0.894, word_ins_ml=3.788, word_reposition=0.762, kpe=nan, ppl=nan, wps=7849.5, ups=0.38, wpb=20501, bsz=256, num_updates=16300, lr=0.000276924, gnorm=3.013, clip=0, loss_scale=256, train_wall=223, wall=44440
2022-07-10 04:26:32 | INFO | train_inner | epoch 015:    715 / 1122 loss=6.106, nll_loss=2.143, mask_ins=0.886, word_ins_ml=3.788, word_reposition=0.756, kpe=0.676, ppl=68.89, wps=7821.4, ups=0.38, wpb=20502.3, bsz=256, num_updates=16400, lr=0.000276079, gnorm=3.252, clip=0, loss_scale=256, train_wall=224, wall=44702
2022-07-10 04:30:52 | INFO | train_inner | epoch 015:    815 / 1122 loss=6.094, nll_loss=2.126, mask_ins=0.882, word_ins_ml=3.773, word_reposition=0.757, kpe=0.683, ppl=68.32, wps=7901.1, ups=0.38, wpb=20547.3, bsz=256, num_updates=16500, lr=0.000275241, gnorm=3.026, clip=0, loss_scale=445, train_wall=223, wall=44962
2022-07-10 04:35:11 | INFO | train_inner | epoch 015:    915 / 1122 loss=6.09, nll_loss=2.139, mask_ins=0.882, word_ins_ml=3.783, word_reposition=0.757, kpe=0.668, ppl=68.13, wps=7892.2, ups=0.39, wpb=20450.5, bsz=256, num_updates=16600, lr=0.000274411, gnorm=2.768, clip=0, loss_scale=512, train_wall=221, wall=45222
2022-07-10 04:39:43 | INFO | train_inner | epoch 015:   1015 / 1122 loss=6.095, nll_loss=2.13, mask_ins=0.881, word_ins_ml=3.775, word_reposition=0.759, kpe=0.68, ppl=68.35, wps=7539.9, ups=0.37, wpb=20537.3, bsz=256, num_updates=16700, lr=0.000273588, gnorm=2.774, clip=0, loss_scale=512, train_wall=235, wall=45494
2022-07-10 04:44:16 | INFO | train_inner | epoch 015:   1115 / 1122 loss=6.02, nll_loss=2.074, mask_ins=0.865, word_ins_ml=3.725, word_reposition=0.757, kpe=0.674, ppl=64.91, wps=7548.8, ups=0.37, wpb=20598.7, bsz=256, num_updates=16800, lr=0.000272772, gnorm=2.695, clip=0, loss_scale=512, train_wall=235, wall=45767
2022-07-10 04:44:33 | INFO | train | epoch 015 | loss nan | nll_loss 2.124 | mask_ins 0.888 | word_ins_ml 3.771 | word_reposition 0.76 | kpe nan | ppl nan | wps 7593.5 | ups 0.37 | wpb 20520.7 | bsz 255.8 | num_updates 16807 | lr 0.000272716 | gnorm 3.022 | clip 0.1 | loss_scale 465 | train_wall 2527 | wall 45784
2022-07-10 04:45:38 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 11.672 | nll_loss 5.764 | mask_ins 1.55 | word_ins_ml 7.113 | word_reposition 1.456 | kpe 1.552 | ppl 3262.23 | wps 15234.2 | wpb 2367.6 | bsz 32 | num_updates 16807 | best_loss 11.557
2022-07-10 04:45:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 15 @ 16807 updates, score 11.672) (writing took 5.977071980014443 seconds)
2022-07-10 04:49:47 | INFO | train_inner | epoch 016:     93 / 1122 loss=nan, nll_loss=2.093, mask_ins=0.865, word_ins_ml=3.743, word_reposition=0.743, kpe=nan, ppl=nan, wps=6121.5, ups=0.3, wpb=20279.3, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=2.845, clip=0, loss_scale=512, train_wall=223, wall=46098
2022-07-10 04:54:07 | INFO | train_inner | epoch 016:    193 / 1122 loss=5.969, nll_loss=2.095, mask_ins=0.855, word_ins_ml=3.744, word_reposition=0.748, kpe=0.622, ppl=62.65, wps=7883.7, ups=0.38, wpb=20489.7, bsz=256, num_updates=17000, lr=0.000271163, gnorm=2.75, clip=0, loss_scale=829, train_wall=222, wall=46358
2022-07-10 04:56:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 04:58:29 | INFO | train_inner | epoch 016:    294 / 1122 loss=5.958, nll_loss=2.058, mask_ins=0.866, word_ins_ml=3.71, word_reposition=0.755, kpe=0.627, ppl=62.17, wps=7888.2, ups=0.38, wpb=20655.3, bsz=256, num_updates=17100, lr=0.000270369, gnorm=2.921, clip=0, loss_scale=776, train_wall=224, wall=46620
2022-07-10 05:02:50 | INFO | train_inner | epoch 016:    394 / 1122 loss=5.944, nll_loss=2.059, mask_ins=0.858, word_ins_ml=3.711, word_reposition=0.744, kpe=0.631, ppl=61.58, wps=7851.6, ups=0.38, wpb=20438.9, bsz=256, num_updates=17200, lr=0.000269582, gnorm=2.796, clip=0, loss_scale=512, train_wall=223, wall=46880
2022-07-10 05:07:09 | INFO | train_inner | epoch 016:    494 / 1122 loss=5.957, nll_loss=2.063, mask_ins=0.858, word_ins_ml=3.715, word_reposition=0.751, kpe=0.633, ppl=62.13, wps=7939.8, ups=0.38, wpb=20624.1, bsz=256, num_updates=17300, lr=0.000268802, gnorm=2.736, clip=0, loss_scale=512, train_wall=222, wall=47140
2022-07-10 05:11:29 | INFO | train_inner | epoch 016:    594 / 1122 loss=5.932, nll_loss=2.047, mask_ins=0.848, word_ins_ml=3.7, word_reposition=0.746, kpe=0.638, ppl=61.05, wps=7908.1, ups=0.38, wpb=20554.8, bsz=256, num_updates=17400, lr=0.000268028, gnorm=2.797, clip=0, loss_scale=512, train_wall=222, wall=47400
2022-07-10 05:15:50 | INFO | train_inner | epoch 016:    694 / 1122 loss=5.899, nll_loss=2.024, mask_ins=0.838, word_ins_ml=3.68, word_reposition=0.739, kpe=0.641, ppl=59.66, wps=7906, ups=0.38, wpb=20579.4, bsz=256, num_updates=17500, lr=0.000267261, gnorm=2.878, clip=0, loss_scale=512, train_wall=222, wall=47660
2022-07-10 05:20:11 | INFO | train_inner | epoch 016:    794 / 1122 loss=5.979, nll_loss=2.09, mask_ins=0.854, word_ins_ml=3.738, word_reposition=0.746, kpe=0.641, ppl=63.1, wps=7847.2, ups=0.38, wpb=20507.6, bsz=256, num_updates=17600, lr=0.000266501, gnorm=2.819, clip=0, loss_scale=701, train_wall=223, wall=47922
2022-07-10 05:24:30 | INFO | train_inner | epoch 016:    894 / 1122 loss=5.922, nll_loss=2.028, mask_ins=0.848, word_ins_ml=3.682, word_reposition=0.744, kpe=0.647, ppl=60.62, wps=7929.5, ups=0.39, wpb=20575.1, bsz=256, num_updates=17700, lr=0.000265747, gnorm=2.785, clip=0, loss_scale=1024, train_wall=222, wall=48181
2022-07-10 05:28:52 | INFO | train_inner | epoch 016:    994 / 1122 loss=5.95, nll_loss=2.049, mask_ins=0.853, word_ins_ml=3.701, word_reposition=0.746, kpe=0.65, ppl=61.8, wps=7843.3, ups=0.38, wpb=20502, bsz=256, num_updates=17800, lr=0.000264999, gnorm=2.783, clip=0, loss_scale=1024, train_wall=224, wall=48442
2022-07-10 05:33:40 | INFO | train_inner | epoch 016:   1094 / 1122 loss=nan, nll_loss=2.082, mask_ins=0.853, word_ins_ml=3.73, word_reposition=0.749, kpe=nan, ppl=nan, wps=7114.7, ups=0.35, wpb=20513.8, bsz=256, num_updates=17900, lr=0.000264258, gnorm=2.8, clip=0, loss_scale=1024, train_wall=251, wall=48731
2022-07-10 05:34:53 | INFO | train | epoch 016 | loss nan | nll_loss 2.064 | mask_ins 0.854 | word_ins_ml 3.715 | word_reposition 0.746 | kpe nan | ppl nan | wps 7618.8 | ups 0.37 | wpb 20519.7 | bsz 255.8 | num_updates 17928 | lr 0.000264052 | gnorm 2.841 | clip 0.1 | loss_scale 731 | train_wall 2526 | wall 48803
2022-07-10 05:35:58 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 11.635 | nll_loss 5.74 | mask_ins 1.534 | word_ins_ml 7.084 | word_reposition 1.43 | kpe 1.587 | ppl 3179.99 | wps 15116.2 | wpb 2367.6 | bsz 32 | num_updates 17928 | best_loss 11.557
2022-07-10 05:36:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 16 @ 17928 updates, score 11.635) (writing took 5.768264782615006 seconds)
2022-07-10 05:39:11 | INFO | train_inner | epoch 017:     72 / 1122 loss=5.911, nll_loss=2.052, mask_ins=0.85, word_ins_ml=3.703, word_reposition=0.746, kpe=0.611, ppl=60.15, wps=6185.6, ups=0.3, wpb=20446.3, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=3.231, clip=1, loss_scale=1024, train_wall=222, wall=49061
2022-07-10 05:43:32 | INFO | train_inner | epoch 017:    172 / 1122 loss=5.846, nll_loss=2.013, mask_ins=0.846, word_ins_ml=3.669, word_reposition=0.738, kpe=0.593, ppl=57.54, wps=7833.8, ups=0.38, wpb=20498.3, bsz=256, num_updates=18100, lr=0.000262794, gnorm=2.736, clip=0, loss_scale=1280, train_wall=224, wall=49323
2022-07-10 05:47:52 | INFO | train_inner | epoch 017:    272 / 1122 loss=5.848, nll_loss=2.018, mask_ins=0.837, word_ins_ml=3.673, word_reposition=0.737, kpe=0.601, ppl=57.61, wps=7887.5, ups=0.39, wpb=20482.1, bsz=256, num_updates=18200, lr=0.000262071, gnorm=2.84, clip=0, loss_scale=2048, train_wall=222, wall=49583
2022-07-10 05:52:11 | INFO | train_inner | epoch 017:    372 / 1122 loss=5.877, nll_loss=2.04, mask_ins=0.836, word_ins_ml=3.692, word_reposition=0.74, kpe=0.61, ppl=58.79, wps=7886.5, ups=0.39, wpb=20436.9, bsz=256, num_updates=18300, lr=0.000261354, gnorm=2.966, clip=0, loss_scale=2048, train_wall=221, wall=49842
2022-07-10 05:56:31 | INFO | train_inner | epoch 017:    472 / 1122 loss=5.872, nll_loss=2.035, mask_ins=0.836, word_ins_ml=3.688, word_reposition=0.74, kpe=0.608, ppl=58.56, wps=7875.8, ups=0.38, wpb=20478.7, bsz=256, num_updates=18400, lr=0.000260643, gnorm=2.871, clip=0, loss_scale=2048, train_wall=222, wall=50102
2022-07-10 06:00:53 | INFO | train_inner | epoch 017:    572 / 1122 loss=5.842, nll_loss=2.012, mask_ins=0.833, word_ins_ml=3.667, word_reposition=0.733, kpe=0.608, ppl=57.35, wps=7836.1, ups=0.38, wpb=20497.2, bsz=256, num_updates=18500, lr=0.000259938, gnorm=2.76, clip=0, loss_scale=2048, train_wall=223, wall=50363
2022-07-10 06:01:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 06:01:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 06:05:18 | INFO | train_inner | epoch 017:    674 / 1122 loss=5.811, nll_loss=1.983, mask_ins=0.828, word_ins_ml=3.641, word_reposition=0.731, kpe=0.612, ppl=56.16, wps=7767.7, ups=0.38, wpb=20633.9, bsz=256, num_updates=18600, lr=0.000259238, gnorm=2.754, clip=0, loss_scale=788, train_wall=226, wall=50629
2022-07-10 06:09:38 | INFO | train_inner | epoch 017:    774 / 1122 loss=5.837, nll_loss=2.005, mask_ins=0.822, word_ins_ml=3.66, word_reposition=0.741, kpe=0.614, ppl=57.16, wps=7990.6, ups=0.39, wpb=20750.8, bsz=256, num_updates=18700, lr=0.000258544, gnorm=2.738, clip=0, loss_scale=512, train_wall=221, wall=50889
2022-07-10 06:13:58 | INFO | train_inner | epoch 017:    874 / 1122 loss=5.831, nll_loss=1.994, mask_ins=0.831, word_ins_ml=3.65, word_reposition=0.734, kpe=0.616, ppl=56.92, wps=7854.9, ups=0.38, wpb=20449.2, bsz=256, num_updates=18800, lr=0.000257855, gnorm=2.728, clip=0, loss_scale=512, train_wall=222, wall=51149
2022-07-10 06:18:19 | INFO | train_inner | epoch 017:    974 / 1122 loss=5.828, nll_loss=1.99, mask_ins=0.826, word_ins_ml=3.647, word_reposition=0.736, kpe=0.619, ppl=56.8, wps=7859.3, ups=0.38, wpb=20495.8, bsz=256, num_updates=18900, lr=0.000257172, gnorm=2.762, clip=0, loss_scale=512, train_wall=222, wall=51410
2022-07-10 06:22:55 | INFO | train_inner | epoch 017:   1074 / 1122 loss=nan, nll_loss=2.011, mask_ins=0.821, word_ins_ml=3.665, word_reposition=0.733, kpe=nan, ppl=nan, wps=7453.5, ups=0.36, wpb=20549.3, bsz=256, num_updates=19000, lr=0.000256495, gnorm=2.659, clip=0, loss_scale=512, train_wall=238, wall=51686
2022-07-10 06:25:13 | INFO | train | epoch 017 | loss nan | nll_loss 2.01 | mask_ins 0.832 | word_ins_ml 3.665 | word_reposition 0.737 | kpe nan | ppl nan | wps 7610.5 | ups 0.37 | wpb 20520.4 | bsz 255.8 | num_updates 19048 | lr 0.000256171 | gnorm 2.795 | clip 0 | loss_scale 1195 | train_wall 2520 | wall 51823
2022-07-10 06:26:18 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 11.583 | nll_loss 5.709 | mask_ins 1.545 | word_ins_ml 7.051 | word_reposition 1.313 | kpe 1.674 | ppl 3066.82 | wps 15171.2 | wpb 2367.6 | bsz 32 | num_updates 19048 | best_loss 11.557
2022-07-10 06:26:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 17 @ 19048 updates, score 11.583) (writing took 5.711052130907774 seconds)
2022-07-10 06:28:38 | INFO | train_inner | epoch 018:     52 / 1122 loss=nan, nll_loss=1.99, mask_ins=0.819, word_ins_ml=3.647, word_reposition=0.744, kpe=nan, ppl=nan, wps=5918.9, ups=0.29, wpb=20306.8, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=2.997, clip=0, loss_scale=876, train_wall=234, wall=52029
2022-07-10 06:32:56 | INFO | train_inner | epoch 018:    152 / 1122 loss=5.701, nll_loss=1.926, mask_ins=0.816, word_ins_ml=3.59, word_reposition=0.725, kpe=0.57, ppl=52.02, wps=7955.8, ups=0.39, wpb=20561.5, bsz=256, num_updates=19200, lr=0.000255155, gnorm=2.84, clip=0, loss_scale=1024, train_wall=221, wall=52287
2022-07-10 06:37:15 | INFO | train_inner | epoch 018:    252 / 1122 loss=5.703, nll_loss=1.933, mask_ins=0.802, word_ins_ml=3.596, word_reposition=0.735, kpe=0.569, ppl=52.07, wps=7963.8, ups=0.39, wpb=20583.9, bsz=256, num_updates=19300, lr=0.000254493, gnorm=2.653, clip=0, loss_scale=1024, train_wall=221, wall=52546
2022-07-10 06:41:33 | INFO | train_inner | epoch 018:    352 / 1122 loss=5.753, nll_loss=1.966, mask_ins=0.819, word_ins_ml=3.624, word_reposition=0.731, kpe=0.578, ppl=53.92, wps=7942.6, ups=0.39, wpb=20473.8, bsz=256, num_updates=19400, lr=0.000253837, gnorm=2.683, clip=0, loss_scale=1024, train_wall=220, wall=52803
2022-07-10 06:45:52 | INFO | train_inner | epoch 018:    452 / 1122 loss=5.742, nll_loss=1.973, mask_ins=0.805, word_ins_ml=3.63, word_reposition=0.729, kpe=0.577, ppl=53.51, wps=7927.4, ups=0.39, wpb=20570.9, bsz=256, num_updates=19500, lr=0.000253185, gnorm=2.754, clip=0, loss_scale=1024, train_wall=222, wall=53063
2022-07-10 06:50:13 | INFO | train_inner | epoch 018:    552 / 1122 loss=5.757, nll_loss=1.978, mask_ins=0.808, word_ins_ml=3.635, word_reposition=0.73, kpe=0.584, ppl=54.08, wps=7852, ups=0.38, wpb=20442.1, bsz=256, num_updates=19600, lr=0.000252538, gnorm=2.768, clip=0, loss_scale=1628, train_wall=222, wall=53323
2022-07-10 06:54:32 | INFO | train_inner | epoch 018:    652 / 1122 loss=5.748, nll_loss=1.979, mask_ins=0.804, word_ins_ml=3.636, word_reposition=0.722, kpe=0.586, ppl=53.76, wps=7890, ups=0.39, wpb=20443.7, bsz=256, num_updates=19700, lr=0.000251896, gnorm=2.735, clip=0, loss_scale=2048, train_wall=221, wall=53582
2022-07-10 06:58:51 | INFO | train_inner | epoch 018:    752 / 1122 loss=nan, nll_loss=1.983, mask_ins=0.813, word_ins_ml=3.639, word_reposition=0.739, kpe=nan, ppl=nan, wps=7972.7, ups=0.39, wpb=20637.2, bsz=256, num_updates=19800, lr=0.000251259, gnorm=2.751, clip=0, loss_scale=2048, train_wall=221, wall=53841
2022-07-10 07:00:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 07:03:13 | INFO | train_inner | epoch 018:    853 / 1122 loss=5.72, nll_loss=1.94, mask_ins=0.802, word_ins_ml=3.601, word_reposition=0.726, kpe=0.59, ppl=52.7, wps=7890.9, ups=0.38, wpb=20684.8, bsz=256, num_updates=19900, lr=0.000250627, gnorm=2.768, clip=0, loss_scale=1501, train_wall=224, wall=54103
2022-07-10 07:07:33 | INFO | train_inner | epoch 018:    953 / 1122 loss=5.758, nll_loss=1.975, mask_ins=0.8, word_ins_ml=3.631, word_reposition=0.73, kpe=0.597, ppl=54.13, wps=7860.7, ups=0.38, wpb=20469.1, bsz=256, num_updates=20000, lr=0.00025, gnorm=2.908, clip=0, loss_scale=1024, train_wall=222, wall=54364
2022-07-10 07:11:53 | INFO | train_inner | epoch 018:   1053 / 1122 loss=5.79, nll_loss=1.988, mask_ins=0.813, word_ins_ml=3.642, word_reposition=0.738, kpe=0.596, ppl=55.32, wps=7895.5, ups=0.38, wpb=20518.3, bsz=256, num_updates=20100, lr=0.000249377, gnorm=2.829, clip=0, loss_scale=1024, train_wall=222, wall=54624
2022-07-10 07:15:17 | INFO | train | epoch 018 | loss nan | nll_loss 1.967 | mask_ins 0.809 | word_ins_ml 3.625 | word_reposition 0.731 | kpe nan | ppl nan | wps 7656.3 | ups 0.37 | wpb 20520.7 | bsz 255.8 | num_updates 20169 | lr 0.00024895 | gnorm 2.791 | clip 0 | loss_scale 1303 | train_wall 2508 | wall 54828
2022-07-10 07:16:22 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 11.601 | nll_loss 5.705 | mask_ins 1.515 | word_ins_ml 7.054 | word_reposition 1.351 | kpe 1.682 | ppl 3106.86 | wps 15223.3 | wpb 2367.6 | bsz 32 | num_updates 20169 | best_loss 11.557
2022-07-10 07:16:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 18 @ 20169 updates, score 11.601) (writing took 5.21565147023648 seconds)
2022-07-10 07:17:48 | INFO | train_inner | epoch 019:     31 / 1122 loss=5.753, nll_loss=1.974, mask_ins=0.806, word_ins_ml=3.63, word_reposition=0.731, kpe=0.587, ppl=53.94, wps=5722, ups=0.28, wpb=20308.2, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=2.933, clip=0, loss_scale=1024, train_wall=247, wall=54979
2022-07-10 07:22:08 | INFO | train_inner | epoch 019:    131 / 1122 loss=5.652, nll_loss=1.929, mask_ins=0.798, word_ins_ml=3.591, word_reposition=0.719, kpe=0.545, ppl=50.28, wps=7817.6, ups=0.38, wpb=20331.6, bsz=256, num_updates=20300, lr=0.000248146, gnorm=2.747, clip=0, loss_scale=1024, train_wall=222, wall=55239
2022-07-10 07:26:27 | INFO | train_inner | epoch 019:    231 / 1122 loss=nan, nll_loss=1.935, mask_ins=0.794, word_ins_ml=3.596, word_reposition=0.718, kpe=nan, ppl=nan, wps=7915.8, ups=0.39, wpb=20543.5, bsz=256, num_updates=20400, lr=0.000247537, gnorm=2.794, clip=0, loss_scale=1454, train_wall=221, wall=55498
2022-07-10 07:30:46 | INFO | train_inner | epoch 019:    331 / 1122 loss=5.686, nll_loss=1.942, mask_ins=0.801, word_ins_ml=3.602, word_reposition=0.725, kpe=0.558, ppl=51.47, wps=7924.5, ups=0.39, wpb=20526.1, bsz=256, num_updates=20500, lr=0.000246932, gnorm=2.977, clip=0, loss_scale=2048, train_wall=221, wall=55757
2022-07-10 07:35:05 | INFO | train_inner | epoch 019:    431 / 1122 loss=5.655, nll_loss=1.927, mask_ins=0.788, word_ins_ml=3.588, word_reposition=0.721, kpe=0.557, ppl=50.39, wps=7884.6, ups=0.39, wpb=20385.4, bsz=256, num_updates=20600, lr=0.000246332, gnorm=2.821, clip=0, loss_scale=2048, train_wall=220, wall=56016
2022-07-10 07:37:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 07:39:28 | INFO | train_inner | epoch 019:    532 / 1122 loss=5.691, nll_loss=1.953, mask_ins=0.79, word_ins_ml=3.611, word_reposition=0.733, kpe=0.557, ppl=51.67, wps=7863, ups=0.38, wpb=20669.5, bsz=256, num_updates=20700, lr=0.000245737, gnorm=2.758, clip=0, loss_scale=1622, train_wall=225, wall=56279
2022-07-10 07:43:48 | INFO | train_inner | epoch 019:    632 / 1122 loss=5.667, nll_loss=1.936, mask_ins=0.785, word_ins_ml=3.595, word_reposition=0.725, kpe=0.561, ppl=50.81, wps=7922.7, ups=0.38, wpb=20618.1, bsz=256, num_updates=20800, lr=0.000245145, gnorm=2.864, clip=0, loss_scale=1024, train_wall=222, wall=56539
2022-07-10 07:48:09 | INFO | train_inner | epoch 019:    732 / 1122 loss=5.685, nll_loss=1.935, mask_ins=0.793, word_ins_ml=3.595, word_reposition=0.729, kpe=0.567, ppl=51.44, wps=7940.8, ups=0.38, wpb=20698.7, bsz=256, num_updates=20900, lr=0.000244558, gnorm=3.25, clip=0, loss_scale=1024, train_wall=222, wall=56800
2022-07-10 07:52:28 | INFO | train_inner | epoch 019:    832 / 1122 loss=5.694, nll_loss=1.948, mask_ins=0.789, word_ins_ml=3.606, word_reposition=0.727, kpe=0.572, ppl=51.76, wps=7916.3, ups=0.39, wpb=20546.3, bsz=256, num_updates=21000, lr=0.000243975, gnorm=2.81, clip=0, loss_scale=1024, train_wall=221, wall=57059
2022-07-10 07:56:47 | INFO | train_inner | epoch 019:    932 / 1122 loss=nan, nll_loss=1.892, mask_ins=0.78, word_ins_ml=3.556, word_reposition=0.722, kpe=nan, ppl=nan, wps=7906.5, ups=0.39, wpb=20451.4, bsz=256, num_updates=21100, lr=0.000243396, gnorm=2.734, clip=0, loss_scale=1024, train_wall=221, wall=57318
2022-07-10 08:01:08 | INFO | train_inner | epoch 019:   1032 / 1122 loss=5.65, nll_loss=1.915, mask_ins=0.783, word_ins_ml=3.576, word_reposition=0.717, kpe=0.575, ppl=50.23, wps=7906.8, ups=0.38, wpb=20602.5, bsz=256, num_updates=21200, lr=0.000242821, gnorm=2.809, clip=0, loss_scale=1331, train_wall=222, wall=57578
2022-07-10 08:05:13 | INFO | train | epoch 019 | loss nan | nll_loss 1.932 | mask_ins 0.791 | word_ins_ml 3.593 | word_reposition 0.724 | kpe nan | ppl nan | wps 7678.8 | ups 0.37 | wpb 20519.1 | bsz 255.8 | num_updates 21290 | lr 0.000242308 | gnorm 2.859 | clip 0 | loss_scale 1408 | train_wall 2497 | wall 57823
2022-07-10 08:06:18 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.98 | nll_loss 5.805 | mask_ins 1.523 | word_ins_ml 7.14 | word_reposition 1.519 | kpe 1.799 | ppl 4040.07 | wps 15174.3 | wpb 2367.6 | bsz 32 | num_updates 21290 | best_loss 11.557
2022-07-10 08:06:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 19 @ 21290 updates, score 11.98) (writing took 6.316656652837992 seconds)
2022-07-10 08:06:50 | INFO | train_inner | epoch 020:     10 / 1122 loss=5.687, nll_loss=1.935, mask_ins=0.793, word_ins_ml=3.594, word_reposition=0.728, kpe=0.571, ppl=51.51, wps=5973, ups=0.29, wpb=20435.6, bsz=253.8, num_updates=21300, lr=0.000242251, gnorm=2.904, clip=0, loss_scale=2048, train_wall=233, wall=57920
2022-07-10 08:11:23 | INFO | train_inner | epoch 020:    110 / 1122 loss=nan, nll_loss=1.884, mask_ins=0.777, word_ins_ml=3.55, word_reposition=0.715, kpe=nan, ppl=nan, wps=7507.4, ups=0.37, wpb=20517.8, bsz=256, num_updates=21400, lr=0.000241684, gnorm=2.792, clip=0, loss_scale=2048, train_wall=235, wall=58194
2022-07-10 08:15:42 | INFO | train_inner | epoch 020:    210 / 1122 loss=5.589, nll_loss=1.905, mask_ins=0.771, word_ins_ml=3.568, word_reposition=0.721, kpe=0.529, ppl=48.14, wps=7931.3, ups=0.39, wpb=20543.8, bsz=256, num_updates=21500, lr=0.000241121, gnorm=2.73, clip=0, loss_scale=2048, train_wall=221, wall=58453
2022-07-10 08:19:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 08:20:04 | INFO | train_inner | epoch 020:    311 / 1122 loss=5.617, nll_loss=1.916, mask_ins=0.789, word_ins_ml=3.577, word_reposition=0.718, kpe=0.533, ppl=49.07, wps=7852.9, ups=0.38, wpb=20590.4, bsz=256, num_updates=21600, lr=0.000240563, gnorm=2.915, clip=0, loss_scale=1967, train_wall=224, wall=58715
2022-07-10 08:24:24 | INFO | train_inner | epoch 020:    411 / 1122 loss=nan, nll_loss=1.902, mask_ins=0.778, word_ins_ml=3.564, word_reposition=0.718, kpe=nan, ppl=nan, wps=7938.2, ups=0.39, wpb=20583.9, bsz=256, num_updates=21700, lr=0.000240008, gnorm=2.798, clip=0, loss_scale=1024, train_wall=221, wall=58974
2022-07-10 08:28:43 | INFO | train_inner | epoch 020:    511 / 1122 loss=5.566, nll_loss=1.883, mask_ins=0.771, word_ins_ml=3.548, word_reposition=0.712, kpe=0.535, ppl=47.38, wps=7898.9, ups=0.39, wpb=20497.8, bsz=256, num_updates=21800, lr=0.000239457, gnorm=2.955, clip=0, loss_scale=1024, train_wall=221, wall=59234
2022-07-10 08:30:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 08:33:05 | INFO | train_inner | epoch 020:    612 / 1122 loss=5.657, nll_loss=1.938, mask_ins=0.785, word_ins_ml=3.597, word_reposition=0.729, kpe=0.547, ppl=50.47, wps=7885.9, ups=0.38, wpb=20610.6, bsz=256, num_updates=21900, lr=0.000238909, gnorm=3.032, clip=0, loss_scale=679, train_wall=223, wall=59495
2022-07-10 08:37:24 | INFO | train_inner | epoch 020:    712 / 1122 loss=5.596, nll_loss=1.898, mask_ins=0.777, word_ins_ml=3.561, word_reposition=0.716, kpe=0.543, ppl=48.37, wps=7860.6, ups=0.39, wpb=20391.8, bsz=256, num_updates=22000, lr=0.000238366, gnorm=2.847, clip=0, loss_scale=512, train_wall=221, wall=59755
2022-07-10 08:41:44 | INFO | train_inner | epoch 020:    812 / 1122 loss=5.616, nll_loss=1.914, mask_ins=0.78, word_ins_ml=3.574, word_reposition=0.717, kpe=0.545, ppl=49.03, wps=7885, ups=0.38, wpb=20495.4, bsz=256, num_updates=22100, lr=0.000237826, gnorm=2.738, clip=0, loss_scale=512, train_wall=221, wall=60015
2022-07-10 08:46:04 | INFO | train_inner | epoch 020:    912 / 1122 loss=5.591, nll_loss=1.879, mask_ins=0.786, word_ins_ml=3.543, word_reposition=0.712, kpe=0.549, ppl=48.19, wps=7907.4, ups=0.38, wpb=20584.4, bsz=256, num_updates=22200, lr=0.000237289, gnorm=2.747, clip=0, loss_scale=512, train_wall=222, wall=60275
2022-07-10 08:50:25 | INFO | train_inner | epoch 020:   1012 / 1122 loss=5.597, nll_loss=1.9, mask_ins=0.771, word_ins_ml=3.562, word_reposition=0.714, kpe=0.549, ppl=48.39, wps=7876.5, ups=0.38, wpb=20559.8, bsz=256, num_updates=22300, lr=0.000236757, gnorm=2.8, clip=0, loss_scale=512, train_wall=223, wall=60536
2022-07-10 08:54:47 | INFO | train_inner | epoch 020:   1112 / 1122 loss=5.665, nll_loss=1.948, mask_ins=0.784, word_ins_ml=3.604, word_reposition=0.721, kpe=0.556, ppl=50.74, wps=7832.5, ups=0.38, wpb=20490.5, bsz=256, num_updates=22400, lr=0.000236228, gnorm=2.886, clip=0, loss_scale=799, train_wall=223, wall=60797
2022-07-10 08:55:25 | INFO | train | epoch 020 | loss nan | nll_loss 1.906 | mask_ins 0.779 | word_ins_ml 3.567 | word_reposition 0.718 | kpe nan | ppl nan | wps 7629.9 | ups 0.37 | wpb 20520.2 | bsz 255.8 | num_updates 22410 | lr 0.000236175 | gnorm 2.85 | clip 0 | loss_scale 1067 | train_wall 2512 | wall 60835
2022-07-10 08:56:30 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.612 | nll_loss 5.657 | mask_ins 1.511 | word_ins_ml 7.001 | word_reposition 1.385 | kpe 1.715 | ppl 3130.89 | wps 15133.3 | wpb 2367.6 | bsz 32 | num_updates 22410 | best_loss 11.557
2022-07-10 08:56:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 20 @ 22410 updates, score 11.612) (writing took 5.88968157209456 seconds)
2022-07-10 09:00:45 | INFO | train_inner | epoch 021:     90 / 1122 loss=5.536, nll_loss=1.871, mask_ins=0.773, word_ins_ml=3.536, word_reposition=0.714, kpe=0.512, ppl=46.39, wps=5716.1, ups=0.28, wpb=20465.5, bsz=253.8, num_updates=22500, lr=0.000235702, gnorm=2.93, clip=0, loss_scale=1024, train_wall=249, wall=61156
2022-07-10 09:05:03 | INFO | train_inner | epoch 021:    190 / 1122 loss=5.524, nll_loss=1.871, mask_ins=0.77, word_ins_ml=3.536, word_reposition=0.712, kpe=0.506, ppl=46.01, wps=7925.7, ups=0.39, wpb=20452.3, bsz=256, num_updates=22600, lr=0.00023518, gnorm=2.667, clip=0, loss_scale=1024, train_wall=220, wall=61414
2022-07-10 09:09:23 | INFO | train_inner | epoch 021:    290 / 1122 loss=5.514, nll_loss=1.872, mask_ins=0.757, word_ins_ml=3.537, word_reposition=0.711, kpe=0.51, ppl=45.71, wps=7937.9, ups=0.38, wpb=20648.5, bsz=256, num_updates=22700, lr=0.000234662, gnorm=2.8, clip=0, loss_scale=1024, train_wall=222, wall=61674
2022-07-10 09:13:44 | INFO | train_inner | epoch 021:    390 / 1122 loss=5.503, nll_loss=1.848, mask_ins=0.762, word_ins_ml=3.515, word_reposition=0.71, kpe=0.515, ppl=45.36, wps=7871.8, ups=0.38, wpb=20511.9, bsz=256, num_updates=22800, lr=0.000234146, gnorm=2.804, clip=0, loss_scale=1024, train_wall=223, wall=61934
2022-07-10 09:18:05 | INFO | train_inner | epoch 021:    490 / 1122 loss=nan, nll_loss=1.861, mask_ins=0.759, word_ins_ml=3.526, word_reposition=0.711, kpe=nan, ppl=nan, wps=7916.8, ups=0.38, wpb=20657, bsz=256, num_updates=22900, lr=0.000233635, gnorm=2.789, clip=0, loss_scale=1475, train_wall=222, wall=62195
2022-07-10 09:22:25 | INFO | train_inner | epoch 021:    590 / 1122 loss=5.49, nll_loss=1.831, mask_ins=0.762, word_ins_ml=3.5, word_reposition=0.708, kpe=0.52, ppl=44.94, wps=7879.6, ups=0.38, wpb=20501.8, bsz=256, num_updates=23000, lr=0.000233126, gnorm=2.844, clip=0, loss_scale=2048, train_wall=223, wall=62455
2022-07-10 09:26:46 | INFO | train_inner | epoch 021:    690 / 1122 loss=5.537, nll_loss=1.872, mask_ins=0.762, word_ins_ml=3.536, word_reposition=0.714, kpe=0.525, ppl=46.44, wps=7819.6, ups=0.38, wpb=20412.8, bsz=256, num_updates=23100, lr=0.000232621, gnorm=2.716, clip=0, loss_scale=2048, train_wall=223, wall=62716
2022-07-10 09:31:06 | INFO | train_inner | epoch 021:    790 / 1122 loss=5.524, nll_loss=1.854, mask_ins=0.761, word_ins_ml=3.52, word_reposition=0.716, kpe=0.527, ppl=46.01, wps=7896.6, ups=0.38, wpb=20574.3, bsz=256, num_updates=23200, lr=0.000232119, gnorm=2.995, clip=0, loss_scale=2048, train_wall=223, wall=62977
2022-07-10 09:31:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 09:35:28 | INFO | train_inner | epoch 021:    891 / 1122 loss=5.515, nll_loss=1.842, mask_ins=0.77, word_ins_ml=3.509, word_reposition=0.705, kpe=0.53, ppl=45.74, wps=7801.2, ups=0.38, wpb=20409.8, bsz=256, num_updates=23300, lr=0.000231621, gnorm=2.779, clip=0, loss_scale=1034, train_wall=224, wall=63239
2022-07-10 09:39:49 | INFO | train_inner | epoch 021:    991 / 1122 loss=5.526, nll_loss=1.858, mask_ins=0.761, word_ins_ml=3.524, word_reposition=0.71, kpe=0.531, ppl=46.09, wps=7890.6, ups=0.38, wpb=20580, bsz=256, num_updates=23400, lr=0.000231125, gnorm=2.852, clip=0, loss_scale=1024, train_wall=223, wall=63499
2022-07-10 09:44:09 | INFO | train_inner | epoch 021:   1091 / 1122 loss=5.554, nll_loss=1.878, mask_ins=0.765, word_ins_ml=3.541, word_reposition=0.717, kpe=0.531, ppl=46.99, wps=7860.1, ups=0.38, wpb=20490.4, bsz=256, num_updates=23500, lr=0.000230633, gnorm=2.725, clip=0, loss_scale=1024, train_wall=223, wall=63760
2022-07-10 09:45:29 | INFO | train | epoch 021 | loss nan | nll_loss 1.86 | mask_ins 0.763 | word_ins_ml 3.526 | word_reposition 0.712 | kpe nan | ppl nan | wps 7657.5 | ups 0.37 | wpb 20522.5 | bsz 255.8 | num_updates 23531 | lr 0.000230481 | gnorm 2.804 | clip 0 | loss_scale 1339 | train_wall 2508 | wall 63840
2022-07-10 09:46:35 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.713 | nll_loss 5.709 | mask_ins 1.506 | word_ins_ml 7.051 | word_reposition 1.436 | kpe 1.72 | ppl 3356.97 | wps 15107 | wpb 2367.6 | bsz 32 | num_updates 23531 | best_loss 11.557
2022-07-10 09:46:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 21 @ 23531 updates, score 11.713) (writing took 5.96527985483408 seconds)
2022-07-10 09:50:03 | INFO | train_inner | epoch 022:     69 / 1122 loss=5.486, nll_loss=1.857, mask_ins=0.751, word_ins_ml=3.522, word_reposition=0.705, kpe=0.508, ppl=44.81, wps=5751.1, ups=0.28, wpb=20344.2, bsz=253.8, num_updates=23600, lr=0.000230144, gnorm=2.869, clip=0, loss_scale=1024, train_wall=244, wall=64114
2022-07-10 09:54:22 | INFO | train_inner | epoch 022:    169 / 1122 loss=5.404, nll_loss=1.804, mask_ins=0.746, word_ins_ml=3.475, word_reposition=0.696, kpe=0.487, ppl=42.33, wps=7930.2, ups=0.39, wpb=20545.5, bsz=256, num_updates=23700, lr=0.000229658, gnorm=2.786, clip=0, loss_scale=1024, train_wall=221, wall=64373
2022-07-10 09:57:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 09:58:42 | INFO | train_inner | epoch 022:    270 / 1122 loss=5.441, nll_loss=1.828, mask_ins=0.741, word_ins_ml=3.497, word_reposition=0.708, kpe=0.495, ppl=43.44, wps=7937, ups=0.38, wpb=20626.6, bsz=256, num_updates=23800, lr=0.000229175, gnorm=2.855, clip=0, loss_scale=1683, train_wall=221, wall=64633
2022-07-10 10:02:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 10:03:03 | INFO | train_inner | epoch 022:    371 / 1122 loss=5.452, nll_loss=1.841, mask_ins=0.746, word_ins_ml=3.508, word_reposition=0.703, kpe=0.495, ppl=43.76, wps=7924.3, ups=0.38, wpb=20655.6, bsz=256, num_updates=23900, lr=0.000228695, gnorm=2.828, clip=0, loss_scale=928, train_wall=222, wall=64894
2022-07-10 10:07:21 | INFO | train_inner | epoch 022:    471 / 1122 loss=5.452, nll_loss=1.825, mask_ins=0.749, word_ins_ml=3.493, word_reposition=0.707, kpe=0.503, ppl=43.78, wps=7995.9, ups=0.39, wpb=20627.9, bsz=256, num_updates=24000, lr=0.000228218, gnorm=2.893, clip=0, loss_scale=512, train_wall=220, wall=65152
2022-07-10 10:11:39 | INFO | train_inner | epoch 022:    571 / 1122 loss=5.472, nll_loss=1.845, mask_ins=0.75, word_ins_ml=3.512, word_reposition=0.71, kpe=0.501, ppl=44.4, wps=7945.7, ups=0.39, wpb=20497.5, bsz=256, num_updates=24100, lr=0.000227744, gnorm=2.716, clip=0, loss_scale=512, train_wall=220, wall=65410
2022-07-10 10:15:58 | INFO | train_inner | epoch 022:    671 / 1122 loss=5.473, nll_loss=1.837, mask_ins=0.752, word_ins_ml=3.504, word_reposition=0.708, kpe=0.509, ppl=44.42, wps=7946.4, ups=0.39, wpb=20592.5, bsz=256, num_updates=24200, lr=0.000227273, gnorm=3.063, clip=0, loss_scale=512, train_wall=221, wall=65669
2022-07-10 10:20:18 | INFO | train_inner | epoch 022:    771 / 1122 loss=5.46, nll_loss=1.828, mask_ins=0.749, word_ins_ml=3.497, word_reposition=0.705, kpe=0.509, ppl=44.03, wps=7876.7, ups=0.38, wpb=20515.3, bsz=256, num_updates=24300, lr=0.000226805, gnorm=2.945, clip=0, loss_scale=512, train_wall=223, wall=65929
2022-07-10 10:24:40 | INFO | train_inner | epoch 022:    871 / 1122 loss=5.455, nll_loss=1.821, mask_ins=0.751, word_ins_ml=3.49, word_reposition=0.703, kpe=0.511, ppl=43.86, wps=7823.4, ups=0.38, wpb=20426.5, bsz=256, num_updates=24400, lr=0.000226339, gnorm=2.794, clip=0, loss_scale=548, train_wall=224, wall=66190
2022-07-10 10:29:00 | INFO | train_inner | epoch 022:    971 / 1122 loss=nan, nll_loss=1.833, mask_ins=0.749, word_ins_ml=3.499, word_reposition=0.707, kpe=nan, ppl=nan, wps=7926.7, ups=0.38, wpb=20623.9, bsz=256, num_updates=24500, lr=0.000225877, gnorm=2.753, clip=0, loss_scale=1024, train_wall=222, wall=66450
2022-07-10 10:33:21 | INFO | train_inner | epoch 022:   1071 / 1122 loss=5.449, nll_loss=1.817, mask_ins=0.75, word_ins_ml=3.485, word_reposition=0.697, kpe=0.516, ppl=43.67, wps=7818.9, ups=0.38, wpb=20421.5, bsz=256, num_updates=24600, lr=0.000225417, gnorm=2.799, clip=0, loss_scale=1024, train_wall=224, wall=66712
2022-07-10 10:35:32 | INFO | train | epoch 022 | loss nan | nll_loss 1.832 | mask_ins 0.749 | word_ins_ml 3.499 | word_reposition 0.705 | kpe nan | ppl nan | wps 7654.2 | ups 0.37 | wpb 20522.3 | bsz 255.8 | num_updates 24651 | lr 0.000225184 | gnorm 2.846 | clip 0 | loss_scale 850 | train_wall 2505 | wall 66843
2022-07-10 10:36:37 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 11.53 | nll_loss 5.646 | mask_ins 1.491 | word_ins_ml 6.995 | word_reposition 1.284 | kpe 1.761 | ppl 2958.01 | wps 15187 | wpb 2367.6 | bsz 32 | num_updates 24651 | best_loss 11.53
2022-07-10 10:36:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_best.pt (epoch 22 @ 24651 updates, score 11.53) (writing took 10.82992207724601 seconds)
2022-07-10 10:39:08 | INFO | train_inner | epoch 023:     49 / 1122 loss=5.477, nll_loss=1.865, mask_ins=0.748, word_ins_ml=3.528, word_reposition=0.706, kpe=0.495, ppl=44.55, wps=5827.6, ups=0.29, wpb=20241.4, bsz=253.8, num_updates=24700, lr=0.000224961, gnorm=2.789, clip=0, loss_scale=1024, train_wall=234, wall=67059
2022-07-10 10:43:42 | INFO | train_inner | epoch 023:    149 / 1122 loss=5.345, nll_loss=1.777, mask_ins=0.73, word_ins_ml=3.45, word_reposition=0.695, kpe=0.47, ppl=40.66, wps=7548.1, ups=0.37, wpb=20678.2, bsz=256, num_updates=24800, lr=0.000224507, gnorm=2.648, clip=0, loss_scale=1024, train_wall=236, wall=67333
2022-07-10 10:48:03 | INFO | train_inner | epoch 023:    249 / 1122 loss=5.397, nll_loss=1.8, mask_ins=0.742, word_ins_ml=3.471, word_reposition=0.706, kpe=0.479, ppl=42.13, wps=7881.4, ups=0.38, wpb=20529.7, bsz=256, num_updates=24900, lr=0.000224055, gnorm=2.893, clip=0, loss_scale=1024, train_wall=223, wall=67593
2022-07-10 10:48:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 10:52:25 | INFO | train_inner | epoch 023:    350 / 1122 loss=5.405, nll_loss=1.806, mask_ins=0.745, word_ins_ml=3.475, word_reposition=0.701, kpe=0.485, ppl=42.38, wps=7826.2, ups=0.38, wpb=20538.3, bsz=256, num_updates=25000, lr=0.000223607, gnorm=2.704, clip=0, loss_scale=1054, train_wall=225, wall=67856
2022-07-10 10:56:47 | INFO | train_inner | epoch 023:    450 / 1122 loss=5.374, nll_loss=1.788, mask_ins=0.735, word_ins_ml=3.459, word_reposition=0.696, kpe=0.484, ppl=41.48, wps=7839.1, ups=0.38, wpb=20501.2, bsz=256, num_updates=25100, lr=0.000223161, gnorm=2.843, clip=0, loss_scale=1024, train_wall=224, wall=68117
2022-07-10 11:01:08 | INFO | train_inner | epoch 023:    550 / 1122 loss=nan, nll_loss=1.806, mask_ins=0.746, word_ins_ml=3.476, word_reposition=0.708, kpe=nan, ppl=nan, wps=7845.4, ups=0.38, wpb=20538.3, bsz=256, num_updates=25200, lr=0.000222718, gnorm=2.877, clip=0, loss_scale=1024, train_wall=224, wall=68379
2022-07-10 11:05:26 | INFO | train_inner | epoch 023:    650 / 1122 loss=nan, nll_loss=1.808, mask_ins=0.736, word_ins_ml=3.477, word_reposition=0.704, kpe=nan, ppl=nan, wps=7976.3, ups=0.39, wpb=20564.9, bsz=256, num_updates=25300, lr=0.000222277, gnorm=2.892, clip=0, loss_scale=1024, train_wall=219, wall=68637
2022-07-10 11:09:46 | INFO | train_inner | epoch 023:    750 / 1122 loss=5.396, nll_loss=1.795, mask_ins=0.739, word_ins_ml=3.466, word_reposition=0.703, kpe=0.489, ppl=42.12, wps=7910, ups=0.39, wpb=20542.6, bsz=256, num_updates=25400, lr=0.000221839, gnorm=2.683, clip=0, loss_scale=1024, train_wall=222, wall=68897
2022-07-10 11:14:07 | INFO | train_inner | epoch 023:    850 / 1122 loss=5.416, nll_loss=1.81, mask_ins=0.74, word_ins_ml=3.478, word_reposition=0.702, kpe=0.496, ppl=42.7, wps=7868.9, ups=0.38, wpb=20523.6, bsz=256, num_updates=25500, lr=0.000221404, gnorm=2.979, clip=0, loss_scale=1853, train_wall=223, wall=69157
2022-07-10 11:18:26 | INFO | train_inner | epoch 023:    950 / 1122 loss=5.403, nll_loss=1.802, mask_ins=0.74, word_ins_ml=3.471, word_reposition=0.698, kpe=0.494, ppl=42.31, wps=7947.8, ups=0.39, wpb=20620.7, bsz=256, num_updates=25600, lr=0.000220971, gnorm=2.709, clip=0, loss_scale=2048, train_wall=222, wall=69417
2022-07-10 11:22:47 | INFO | train_inner | epoch 023:   1050 / 1122 loss=5.411, nll_loss=1.828, mask_ins=0.729, word_ins_ml=3.494, word_reposition=0.69, kpe=0.497, ppl=42.55, wps=7820.1, ups=0.38, wpb=20416.5, bsz=256, num_updates=25700, lr=0.000220541, gnorm=2.831, clip=0, loss_scale=2048, train_wall=224, wall=69678
2022-07-10 11:25:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 11:25:53 | INFO | train | epoch 023 | loss nan | nll_loss 1.804 | mask_ins 0.739 | word_ins_ml 3.473 | word_reposition 0.7 | kpe nan | ppl nan | wps 7607.6 | ups 0.37 | wpb 20519.9 | bsz 255.8 | num_updates 25771 | lr 0.000220237 | gnorm 2.798 | clip 0 | loss_scale 1332 | train_wall 2521 | wall 69864
2022-07-10 11:26:58 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 11.67 | nll_loss 5.746 | mask_ins 1.493 | word_ins_ml 7.087 | word_reposition 1.385 | kpe 1.705 | ppl 3259.22 | wps 15200.6 | wpb 2367.6 | bsz 32 | num_updates 25771 | best_loss 11.53
2022-07-10 11:27:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 23 @ 25771 updates, score 11.67) (writing took 5.9540049117058516 seconds)
2022-07-10 11:28:19 | INFO | train_inner | epoch 024:     29 / 1122 loss=5.396, nll_loss=1.793, mask_ins=0.749, word_ins_ml=3.464, word_reposition=0.696, kpe=0.487, ppl=42.09, wps=6104.8, ups=0.3, wpb=20237.2, bsz=253.8, num_updates=25800, lr=0.000220113, gnorm=2.848, clip=0, loss_scale=1561, train_wall=223, wall=70010
2022-07-10 11:33:08 | INFO | train_inner | epoch 024:    129 / 1122 loss=nan, nll_loss=1.779, mask_ins=0.73, word_ins_ml=3.45, word_reposition=0.693, kpe=nan, ppl=nan, wps=7083.8, ups=0.35, wpb=20501.2, bsz=256, num_updates=25900, lr=0.000219687, gnorm=2.736, clip=0, loss_scale=1024, train_wall=251, wall=70299
2022-07-10 11:37:30 | INFO | train_inner | epoch 024:    229 / 1122 loss=5.335, nll_loss=1.771, mask_ins=0.728, word_ins_ml=3.443, word_reposition=0.697, kpe=0.466, ppl=40.35, wps=7844.4, ups=0.38, wpb=20491.8, bsz=256, num_updates=26000, lr=0.000219265, gnorm=2.996, clip=0, loss_scale=1024, train_wall=224, wall=70560
2022-07-10 11:41:51 | INFO | train_inner | epoch 024:    329 / 1122 loss=5.327, nll_loss=1.761, mask_ins=0.729, word_ins_ml=3.435, word_reposition=0.696, kpe=0.468, ppl=40.15, wps=7846.5, ups=0.38, wpb=20492.8, bsz=256, num_updates=26100, lr=0.000218844, gnorm=3.01, clip=0, loss_scale=1024, train_wall=224, wall=70821
2022-07-10 11:46:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 11:46:13 | INFO | train_inner | epoch 024:    430 / 1122 loss=nan, nll_loss=1.786, mask_ins=0.735, word_ins_ml=3.457, word_reposition=0.703, kpe=nan, ppl=nan, wps=7835, ups=0.38, wpb=20590.1, bsz=256, num_updates=26200, lr=0.000218426, gnorm=2.937, clip=0, loss_scale=1009, train_wall=225, wall=71084
2022-07-10 11:50:36 | INFO | train_inner | epoch 024:    530 / 1122 loss=5.331, nll_loss=1.764, mask_ins=0.726, word_ins_ml=3.437, word_reposition=0.7, kpe=0.468, ppl=40.25, wps=7839.9, ups=0.38, wpb=20558.3, bsz=256, num_updates=26300, lr=0.00021801, gnorm=2.778, clip=0, loss_scale=512, train_wall=224, wall=71346
2022-07-10 11:54:56 | INFO | train_inner | epoch 024:    630 / 1122 loss=5.35, nll_loss=1.774, mask_ins=0.731, word_ins_ml=3.446, word_reposition=0.697, kpe=0.476, ppl=40.78, wps=7855.8, ups=0.38, wpb=20466.9, bsz=256, num_updates=26400, lr=0.000217597, gnorm=2.999, clip=0, loss_scale=512, train_wall=223, wall=71607
2022-07-10 11:59:16 | INFO | train_inner | epoch 024:    730 / 1122 loss=5.369, nll_loss=1.795, mask_ins=0.729, word_ins_ml=3.464, word_reposition=0.696, kpe=0.48, ppl=41.33, wps=7925, ups=0.38, wpb=20594.3, bsz=256, num_updates=26500, lr=0.000217186, gnorm=2.95, clip=0, loss_scale=512, train_wall=222, wall=71867
2022-07-10 12:03:36 | INFO | train_inner | epoch 024:    830 / 1122 loss=5.35, nll_loss=1.764, mask_ins=0.738, word_ins_ml=3.437, word_reposition=0.698, kpe=0.478, ppl=40.79, wps=7909.9, ups=0.38, wpb=20572, bsz=256, num_updates=26600, lr=0.000216777, gnorm=2.88, clip=0, loss_scale=512, train_wall=222, wall=72127
2022-07-10 12:07:56 | INFO | train_inner | epoch 024:    930 / 1122 loss=5.37, nll_loss=1.778, mask_ins=0.737, word_ins_ml=3.45, word_reposition=0.702, kpe=0.482, ppl=41.37, wps=7903.1, ups=0.38, wpb=20566.2, bsz=256, num_updates=26700, lr=0.000216371, gnorm=3.012, clip=0, loss_scale=512, train_wall=222, wall=72387
2022-07-10 12:12:19 | INFO | train_inner | epoch 024:   1030 / 1122 loss=5.369, nll_loss=1.786, mask_ins=0.733, word_ins_ml=3.456, word_reposition=0.696, kpe=0.485, ppl=41.34, wps=7820, ups=0.38, wpb=20526, bsz=256, num_updates=26800, lr=0.000215967, gnorm=2.722, clip=0, loss_scale=978, train_wall=225, wall=72650
2022-07-10 12:12:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 12:16:19 | INFO | train | epoch 024 | loss nan | nll_loss 1.777 | mask_ins 0.732 | word_ins_ml 3.448 | word_reposition 0.698 | kpe nan | ppl nan | wps 7594.8 | ups 0.37 | wpb 20520.4 | bsz 255.8 | num_updates 26891 | lr 0.000215601 | gnorm 2.9 | clip 0 | loss_scale 751 | train_wall 2532 | wall 72890
2022-07-10 12:17:24 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 11.912 | nll_loss 5.698 | mask_ins 1.538 | word_ins_ml 7.036 | word_reposition 1.49 | kpe 1.848 | ppl 3852.36 | wps 15204.6 | wpb 2367.6 | bsz 32 | num_updates 26891 | best_loss 11.53
2022-07-10 12:17:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 24 @ 26891 updates, score 11.912) (writing took 5.835745180025697 seconds)
2022-07-10 12:17:53 | INFO | train_inner | epoch 025:      9 / 1122 loss=5.367, nll_loss=1.786, mask_ins=0.729, word_ins_ml=3.455, word_reposition=0.704, kpe=0.479, ppl=41.27, wps=6116.6, ups=0.3, wpb=20454.3, bsz=253.8, num_updates=26900, lr=0.000215565, gnorm=2.97, clip=0, loss_scale=537, train_wall=225, wall=72984
2022-07-10 12:22:25 | INFO | train_inner | epoch 025:    109 / 1122 loss=5.295, nll_loss=1.753, mask_ins=0.723, word_ins_ml=3.427, word_reposition=0.697, kpe=0.447, ppl=39.26, wps=7572.3, ups=0.37, wpb=20609, bsz=256, num_updates=27000, lr=0.000215166, gnorm=2.746, clip=0, loss_scale=512, train_wall=235, wall=73256
2022-07-10 12:27:00 | INFO | train_inner | epoch 025:    209 / 1122 loss=5.301, nll_loss=1.765, mask_ins=0.722, word_ins_ml=3.438, word_reposition=0.693, kpe=0.448, ppl=39.42, wps=7463.4, ups=0.36, wpb=20514, bsz=256, num_updates=27100, lr=0.000214768, gnorm=2.782, clip=0, loss_scale=512, train_wall=237, wall=73531
2022-07-10 12:31:19 | INFO | train_inner | epoch 025:    309 / 1122 loss=5.281, nll_loss=1.738, mask_ins=0.722, word_ins_ml=3.413, word_reposition=0.696, kpe=0.45, ppl=38.87, wps=7960.1, ups=0.39, wpb=20607.6, bsz=256, num_updates=27200, lr=0.000214373, gnorm=2.788, clip=0, loss_scale=512, train_wall=221, wall=73790
2022-07-10 12:35:37 | INFO | train_inner | epoch 025:    409 / 1122 loss=5.31, nll_loss=1.767, mask_ins=0.725, word_ins_ml=3.439, word_reposition=0.696, kpe=0.451, ppl=39.68, wps=8006.7, ups=0.39, wpb=20628.3, bsz=256, num_updates=27300, lr=0.00021398, gnorm=2.744, clip=0, loss_scale=512, train_wall=220, wall=74048
2022-07-10 12:39:54 | INFO | train_inner | epoch 025:    509 / 1122 loss=5.277, nll_loss=1.736, mask_ins=0.725, word_ins_ml=3.411, word_reposition=0.687, kpe=0.454, ppl=38.78, wps=7946.9, ups=0.39, wpb=20458.7, bsz=256, num_updates=27400, lr=0.000213589, gnorm=3.039, clip=0, loss_scale=942, train_wall=219, wall=74305
2022-07-10 12:40:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 12:41:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-10 12:44:17 | INFO | train_inner | epoch 025:    611 / 1122 loss=nan, nll_loss=1.747, mask_ins=0.715, word_ins_ml=3.421, word_reposition=0.691, kpe=nan, ppl=nan, wps=7795.3, ups=0.38, wpb=20445.4, bsz=256, num_updates=27500, lr=0.000213201, gnorm=2.701, clip=0, loss_scale=414, train_wall=224, wall=74567
2022-07-10 12:48:34 | INFO | train_inner | epoch 025:    711 / 1122 loss=nan, nll_loss=1.748, mask_ins=0.715, word_ins_ml=3.422, word_reposition=0.685, kpe=nan, ppl=nan, wps=7975.3, ups=0.39, wpb=20501.8, bsz=256, num_updates=27600, lr=0.000212814, gnorm=2.872, clip=0, loss_scale=256, train_wall=219, wall=74824
2022-07-10 12:52:52 | INFO | train_inner | epoch 025:    811 / 1122 loss=5.317, nll_loss=1.761, mask_ins=0.721, word_ins_ml=3.433, word_reposition=0.701, kpe=0.462, ppl=39.85, wps=8034.6, ups=0.39, wpb=20740.7, bsz=256, num_updates=27700, lr=0.00021243, gnorm=2.708, clip=0, loss_scale=256, train_wall=220, wall=75083
2022-07-10 12:57:12 | INFO | train_inner | epoch 025:    911 / 1122 loss=5.3, nll_loss=1.754, mask_ins=0.721, word_ins_ml=3.427, word_reposition=0.687, kpe=0.465, ppl=39.41, wps=7842.3, ups=0.38, wpb=20421.5, bsz=256, num_updates=27800, lr=0.000212047, gnorm=2.704, clip=0, loss_scale=256, train_wall=222, wall=75343
2022-07-10 13:01:34 | INFO | train_inner | epoch 025:   1011 / 1122 loss=5.317, nll_loss=1.764, mask_ins=0.721, word_ins_ml=3.436, word_reposition=0.695, kpe=0.466, ppl=39.87, wps=7807.2, ups=0.38, wpb=20417.1, bsz=256, num_updates=27900, lr=0.000211667, gnorm=2.66, clip=0, loss_scale=256, train_wall=224, wall=75604
2022-07-10 13:05:54 | INFO | train_inner | epoch 025:   1111 / 1122 loss=5.292, nll_loss=1.747, mask_ins=0.72, word_ins_ml=3.42, word_reposition=0.685, kpe=0.467, ppl=39.17, wps=7861.9, ups=0.38, wpb=20474.3, bsz=256, num_updates=28000, lr=0.000211289, gnorm=2.711, clip=0, loss_scale=397, train_wall=223, wall=75865
2022-07-10 13:06:22 | INFO | train | epoch 025 | loss nan | nll_loss 1.754 | mask_ins 0.721 | word_ins_ml 3.427 | word_reposition 0.692 | kpe nan | ppl nan | wps 7654.2 | ups 0.37 | wpb 20521.6 | bsz 255.8 | num_updates 28011 | lr 0.000211247 | gnorm 2.786 | clip 0 | loss_scale 440 | train_wall 2507 | wall 75893
2022-07-10 13:07:28 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 11.703 | nll_loss 5.625 | mask_ins 1.529 | word_ins_ml 6.974 | word_reposition 1.372 | kpe 1.827 | ppl 3332.99 | wps 14933.3 | wpb 2367.6 | bsz 32 | num_updates 28011 | best_loss 11.53
2022-07-10 13:07:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_cased/checkpoint_last.pt (epoch 25 @ 28011 updates, score 11.703) (writing took 6.061474978923798 seconds)
2022-07-10 13:11:31 | INFO | train_inner | epoch 026:     89 / 1122 loss=5.299, nll_loss=1.773, mask_ins=0.725, word_ins_ml=3.444, word_reposition=0.693, kpe=0.437, ppl=39.37, wps=6068.6, ups=0.3, wpb=20429, bsz=253.8, num_updates=28100, lr=0.000210912, gnorm=2.851, clip=0, loss_scale=512, train_wall=223, wall=76201
2022-07-10 13:16:20 | INFO | train_inner | epoch 026:    189 / 1122 loss=5.222, nll_loss=1.714, mask_ins=0.71, word_ins_ml=3.391, word_reposition=0.685, kpe=0.436, ppl=37.31, wps=7094.7, ups=0.35, wpb=20507.2, bsz=256, num_updates=28200, lr=0.000210538, gnorm=2.701, clip=0, loss_scale=512, train_wall=250, wall=76491
2022-07-10 13:20:43 | INFO | train_inner | epoch 026:    289 / 1122 loss=5.211, nll_loss=1.706, mask_ins=0.709, word_ins_ml=3.384, word_reposition=0.683, kpe=0.434, ppl=37.04, wps=7770.3, ups=0.38, wpb=20407.2, bsz=256, num_updates=28300, lr=0.000210166, gnorm=2.641, clip=0, loss_scale=512, train_wall=223, wall=76753
2022-07-10 13:25:05 | INFO | train_inner | epoch 026:    389 / 1122 loss=5.246, nll_loss=1.724, mask_ins=0.713, word_ins_ml=3.4, word_reposition=0.692, kpe=0.441, ppl=37.94, wps=7844.6, ups=0.38, wpb=20618.9, bsz=256, num_updates=28400, lr=0.000209795, gnorm=2.717, clip=0, loss_scale=512, train_wall=223, wall=77016
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
