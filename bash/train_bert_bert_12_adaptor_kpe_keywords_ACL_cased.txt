nohup: ignoring input
2022-07-19 13:22:58 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:12379
2022-07-19 13:22:58 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:12379
2022-07-19 13:22:58 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:12379
2022-07-19 13:22:59 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:12379
2022-07-19 13:22:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-19 13:22:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-19 13:22:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-19 13:22:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-19 13:22:59 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-19 13:22:59 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-19 13:22:59 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-19 13:22:59 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-19 13:22:59 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-19 13:22:59 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-19 13:22:59 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-19 13:22:59 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-19 13:23:05 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:12379', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-ACL-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/数据集/preprocess/ACL-bert-cased-510/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='keyword', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', keywords_gran='word', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-19 13:23:05 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-19 13:23:05 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
start load cached examples valid ...
0it [00:00, ?it/s]2022-07-19 13:23:05 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-ACL-bert-cased-510/valid.source-target.source
start load cached examples valid ...
0it [00:00, ?it/s]2022-07-19 13:23:05 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-ACL-bert-cased-510/valid.source-target.target
2022-07-19 13:23:05 | INFO | fairseq.tasks.translation | ../data-bin-ACL-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]413it [00:00, 4123.39it/s]412it [00:00, 4118.40it/s]409it [00:00, 4085.83it/s]402it [00:00, 4010.07it/s]826it [00:00, 3755.07it/s]824it [00:00, 3748.66it/s]818it [00:00, 3704.69it/s]804it [00:00, 3644.53it/s]1216it [00:00, 3813.85it/s]1214it [00:00, 3812.98it/s]1191it [00:00, 3687.43it/s]1188it [00:00, 3726.10it/s]1599it [00:00, 3691.00it/s]1597it [00:00, 3692.91it/s]1561it [00:00, 3571.34it/s]1563it [00:00, 3547.50it/s]2011it [00:00, 3838.58it/s]2009it [00:00, 3840.32it/s]1971it [00:00, 3751.96it/s]1975it [00:00, 3742.64it/s]2395it [00:00, 3753.00it/s]2397it [00:00, 3749.20it/s]2348it [00:00, 3659.20it/s]2352it [00:00, 3649.39it/s]2822it [00:00, 3914.17it/s]2824it [00:00, 3910.40it/s]2747it [00:00, 3760.45it/s]2761it [00:00, 3784.91it/s]3215it [00:00, 3831.43it/s]3217it [00:00, 3828.96it/s]3125it [00:00, 3691.65it/s]3142it [00:00, 3708.26it/s]3631it [00:00, 3930.71it/s]3633it [00:00, 3927.37it/s]3523it [00:00, 3776.69it/s]3556it [00:00, 3836.00it/s]4026it [00:01, 3829.30it/s]4027it [00:01, 3824.48it/s]3902it [00:01, 3696.12it/s]3941it [00:01, 3702.06it/s]4436it [00:01, 3906.56it/s]4436it [00:01, 3901.29it/s]4303it [00:01, 3788.69it/s]4351it [00:01, 3817.63it/s]4698it [00:01, 3833.92it/s]4828it [00:01, 3782.24it/s]4828it [00:01, 3777.45it/s]4752it [00:01, 3872.24it/s]5239it [00:01, 3875.38it/s]5237it [00:01, 3867.74it/s]5083it [00:01, 3721.01it/s]5141it [00:01, 3706.60it/s]5464it [00:01, 3746.08it/s]5628it [00:01, 3724.27it/s]5626it [00:01, 3714.84it/s]5536it [00:01, 3775.46it/s]6007it [00:01, 3740.81it/s]6003it [00:01, 3728.08it/s]5840it [00:01, 3575.93it/s]5916it [00:01, 3554.33it/s]6383it [00:01, 3738.03it/s]6378it [00:01, 3720.79it/s]6200it [00:01, 3575.32it/s]6287it [00:01, 3597.24it/s]6758it [00:01, 2354.60it/s]6752it [00:01, 2324.44it/s]6559it [00:02, 2009.73it/s]6650it [00:02, 2065.21it/s]7125it [00:02, 2628.05it/s]7129it [00:02, 2622.76it/s]6917it [00:02, 2304.97it/s]7008it [00:02, 2351.74it/s]7445it [00:02, 2750.45it/s]7451it [00:02, 2734.39it/s]7290it [00:02, 2606.45it/s]7821it [00:02, 2998.06it/s]7347it [00:02, 2519.41it/s]7825it [00:02, 2978.44it/s]7615it [00:02, 2721.99it/s]8184it [00:02, 3161.12it/s]7722it [00:02, 2801.44it/s]8187it [00:02, 3006.09it/s]7966it [00:02, 2915.58it/s]8083it [00:02, 3001.15it/s]8528it [00:02, 3163.93it/s]8564it [00:02, 3205.24it/s]8295it [00:02, 2967.14it/s]8896it [00:02, 3303.04it/s]8421it [00:02, 3022.55it/s]8943it [00:02, 3362.25it/s]8663it [00:02, 3157.89it/s]8790it [00:02, 3199.75it/s]9242it [00:02, 3275.24it/s]9296it [00:02, 3301.54it/s]9011it [00:02, 3107.90it/s]9623it [00:02, 3425.15it/s]9131it [00:02, 3138.13it/s]9673it [00:02, 3431.83it/s]9381it [00:02, 3270.43it/s]9974it [00:02, 3344.81it/s]9505it [00:02, 3301.82it/s]10026it [00:02, 3373.16it/s]9757it [00:02, 3407.16it/s]10351it [00:03, 3465.27it/s]9867it [00:03, 3247.21it/s]10394it [00:03, 3458.89it/s]10107it [00:03, 3321.25it/s]10707it [00:03, 3385.70it/s]10221it [00:03, 3326.93it/s]10745it [00:03, 3386.65it/s]10472it [00:03, 3414.02it/s]11075it [00:03, 3467.95it/s]10595it [00:03, 3442.39it/s]11121it [00:03, 3491.18it/s]10819it [00:03, 3321.10it/s]11454it [00:03, 3560.99it/s]11489it [00:03, 3544.53it/s]10945it [00:03, 3328.88it/s]11186it [00:03, 3420.45it/s]11813it [00:03, 3460.65it/s]11318it [00:03, 3440.47it/s]11846it [00:03, 3451.56it/s]11532it [00:03, 3325.23it/s]12182it [00:03, 3526.37it/s]12227it [00:03, 3554.15it/s]11666it [00:03, 3333.59it/s]11903it [00:03, 3433.14it/s]12537it [00:03, 3427.24it/s]12036it [00:03, 3436.39it/s]12585it [00:03, 3428.58it/s]12266it [00:03, 3487.54it/s]12913it [00:03, 3522.20it/s]12957it [00:03, 3510.48it/s]12388it [00:03, 3336.21it/s]12617it [00:03, 3377.68it/s]13267it [00:03, 3403.39it/s]12763it [00:03, 3452.56it/s]13310it [00:03, 3424.41it/s]13368it [00:03, 3445.95it/s]
13368it [00:03, 3438.56it/s]
12985it [00:03, 3463.32it/s]13133it [00:03, 3523.83it/s]13333it [00:04, 3353.13it/s]13368it [00:04, 3317.02it/s]
13368it [00:04, 3304.49it/s]
2022-07-19 13:23:09 | INFO | root | success load 13368 data
2022-07-19 13:23:09 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-19 13:23:09 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-19 13:23:09 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-19 13:23:09 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-19 13:23:09 | INFO | transformer.tokenization_utils | loading file None
2022-07-19 13:23:09 | INFO | transformer.tokenization_utils | loading file None
2022-07-19 13:23:09 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
2022-07-19 13:23:10 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-19 13:23:10 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-19 13:23:10 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-19 13:23:13 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-19 13:23:13 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-19 13:23:13 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-19 13:23:13 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-19 13:23:13 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-19 13:23:16 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-19 13:23:16 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-07-19 13:23:16 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-07-19 13:23:16 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-19 13:23:16 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 142456835)
2022-07-19 13:23:16 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-07-19 13:23:16 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)

Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]353it [00:00, 3528.00it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]706it [00:00, 3395.29it/s]386it [00:00, 3859.42it/s]2022-07-19 13:23:21 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-19 13:23:21 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-19 13:23:21 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt
2022-07-19 13:23:21 | INFO | fairseq.trainer | loading train data for epoch 1
1090it [00:00, 3593.42it/s]772it [00:00, 3608.58it/s]1461it [00:00, 3638.35it/s]2022-07-19 13:23:22 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-ACL-bert-cased-510/train.source-target.source
1158it [00:00, 3715.87it/s]2022-07-19 13:23:22 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-ACL-bert-cased-510/train.source-target.target
2022-07-19 13:23:22 | INFO | fairseq.tasks.translation | ../data-bin-ACL-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]1826it [00:00, 3481.98it/s]362it [00:00, 3609.88it/s]1531it [00:00, 3547.75it/s]2217it [00:00, 3621.74it/s]1921it [00:00, 3667.26it/s]723it [00:00, 3422.89it/s]2581it [00:00, 3493.50it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]1107it [00:00, 3605.00it/s]2319it [00:00, 3547.97it/s]2941it [00:00, 3523.66it/s]388it [00:00, 3878.00it/s]1475it [00:00, 3632.71it/s]2711it [00:00, 3660.44it/s]3295it [00:00, 3466.65it/s]776it [00:00, 3597.61it/s]1839it [00:00, 3513.66it/s]3095it [00:00, 3714.79it/s]3683it [00:01, 3588.16it/s]1157it [00:00, 3689.82it/s]2218it [00:00, 3603.93it/s]3469it [00:00, 3581.64it/s]4043it [00:01, 3472.92it/s]1528it [00:00, 3488.66it/s]2580it [00:00, 3477.34it/s]3848it [00:01, 3640.98it/s]4443it [00:01, 3626.05it/s]1911it [00:00, 3603.23it/s]2954it [00:00, 3555.86it/s]4214it [00:01, 3556.01it/s]4823it [00:01, 3675.31it/s]2300it [00:00, 3695.28it/s]3311it [00:00, 3465.84it/s]4600it [00:01, 3643.18it/s]5192it [00:01, 3527.85it/s]2672it [00:00, 3543.93it/s]3689it [00:01, 3557.49it/s]4966it [00:01, 3520.19it/s]5577it [00:01, 3620.35it/s]3063it [00:00, 3653.07it/s]4046it [00:01, 3457.46it/s]5349it [00:01, 3607.58it/s]5941it [00:01, 3540.88it/s]3431it [00:00, 3521.81it/s]4425it [00:01, 3554.56it/s]5712it [00:01, 3529.40it/s]6324it [00:01, 3624.08it/s]3807it [00:01, 3589.43it/s]4807it [00:01, 3632.70it/s]6099it [00:01, 3620.97it/s]6688it [00:01, 3522.84it/s]4168it [00:01, 3472.40it/s]5172it [00:01, 3486.42it/s]6479it [00:01, 3673.09it/s]7054it [00:01, 3561.66it/s]4548it [00:01, 3567.06it/s]5558it [00:01, 3591.76it/s]6848it [00:01, 3534.66it/s]7412it [00:02, 3481.57it/s]4907it [00:01, 3447.73it/s]5919it [00:01, 3456.21it/s]7242it [00:02, 3649.31it/s]5277it [00:01, 3519.39it/s]6306it [00:01, 3572.93it/s]5650it [00:01, 3579.67it/s]6666it [00:01, 3447.56it/s]6010it [00:01, 3460.11it/s]7043it [00:01, 3537.66it/s]6392it [00:01, 3560.91it/s]7399it [00:02, 3458.29it/s]6750it [00:01, 3471.27it/s]7133it [00:02, 3573.94it/s]7762it [00:02, 1141.43it/s]8128it [00:03, 1440.42it/s]7609it [00:02, 1138.89it/s]8446it [00:03, 1693.22it/s]7992it [00:02, 1447.53it/s]8838it [00:03, 2075.83it/s]8317it [00:03, 1701.28it/s]9165it [00:03, 2304.85it/s]8695it [00:03, 2048.74it/s]9547it [00:03, 2635.62it/s]9091it [00:03, 2417.19it/s]7747it [00:02, 1072.91it/s]9945it [00:03, 2955.99it/s]9441it [00:03, 2609.24it/s]8137it [00:03, 1391.46it/s]10304it [00:03, 3034.15it/s]9830it [00:03, 2906.95it/s]8448it [00:03, 1629.39it/s]7492it [00:02, 1046.01it/s]10693it [00:03, 3256.79it/s]10187it [00:03, 3000.75it/s]8835it [00:03, 1999.08it/s]7878it [00:03, 1351.29it/s]11054it [00:03, 3230.91it/s]10584it [00:03, 3250.10it/s]9161it [00:03, 2219.68it/s]8268it [00:03, 1656.79it/s]11445it [00:03, 3415.34it/s]10946it [00:03, 3247.41it/s]9550it [00:03, 2574.40it/s]8657it [00:03, 2009.16it/s]11806it [00:04, 3368.89it/s]11343it [00:03, 3442.58it/s]9933it [00:03, 2867.05it/s]9048it [00:03, 2358.05it/s]12195it [00:04, 3514.33it/s]11708it [00:03, 3405.69it/s]10287it [00:03, 2970.05it/s]9399it [00:03, 2562.74it/s]12557it [00:04, 3411.53it/s]12108it [00:04, 3571.87it/s]10667it [00:03, 3184.01it/s]9791it [00:03, 2870.17it/s]12958it [00:04, 3574.86it/s]12477it [00:04, 3498.22it/s]11023it [00:03, 3196.33it/s]10150it [00:03, 2993.16it/s]13322it [00:04, 3534.59it/s]12872it [00:04, 3625.12it/s]11409it [00:04, 3376.05it/s]10545it [00:03, 3236.26it/s]13719it [00:04, 3656.89it/s]13286it [00:04, 3772.63it/s]11767it [00:04, 3338.61it/s]10910it [00:03, 3255.27it/s]14114it [00:04, 3739.99it/s]13669it [00:04, 3649.09it/s]12146it [00:04, 3463.87it/s]11305it [00:03, 3441.81it/s]14491it [00:04, 3662.77it/s]14064it [00:04, 3735.16it/s]12504it [00:04, 3430.72it/s]11672it [00:04, 3409.82it/s]14883it [00:04, 3736.59it/s]14441it [00:04, 3667.62it/s]12889it [00:04, 3547.08it/s]12065it [00:04, 3553.95it/s]15259it [00:04, 3625.42it/s]14833it [00:04, 3738.99it/s]13297it [00:04, 3698.68it/s]12469it [00:04, 3524.76it/s]15639it [00:05, 3675.71it/s]15209it [00:04, 3655.50it/s]13672it [00:04, 3568.93it/s]12873it [00:04, 3667.42it/s]16009it [00:05, 3578.61it/s]15601it [00:05, 3729.90it/s]14079it [00:04, 3703.97it/s]13272it [00:04, 3757.18it/s]16393it [00:05, 3653.28it/s]15976it [00:05, 3619.10it/s]14453it [00:04, 3572.70it/s]13653it [00:04, 3663.67it/s]16365it [00:05, 3694.50it/s]14853it [00:04, 3692.57it/s]14048it [00:04, 3743.02it/s]16736it [00:05, 3610.28it/s]15226it [00:05, 3557.13it/s]14426it [00:04, 3660.92it/s]15615it [00:05, 3650.17it/s]14815it [00:04, 3724.64it/s]15983it [00:05, 3497.66it/s]15190it [00:05, 3639.63it/s]16381it [00:05, 3633.58it/s]15574it [00:05, 3695.84it/s]15945it [00:05, 3591.63it/s]16330it [00:05, 3664.98it/s]16698it [00:05, 3605.17it/s]16760it [00:06, 885.51it/s] 17144it [00:06, 1156.18it/s]17099it [00:06, 942.33it/s] 17548it [00:06, 1489.77it/s]17507it [00:06, 1246.81it/s]17880it [00:06, 1748.27it/s]17835it [00:06, 1494.40it/s]18286it [00:06, 2137.07it/s]18241it [00:06, 1874.36it/s]18640it [00:06, 2388.33it/s]18589it [00:06, 2154.58it/s]19029it [00:07, 2711.09it/s]17060it [00:06, 1107.02it/s]16748it [00:06, 851.65it/s] 18990it [00:06, 2524.73it/s]19390it [00:07, 2886.08it/s]17443it [00:06, 1414.19it/s]17153it [00:06, 1131.80it/s]19351it [00:07, 2712.72it/s]19755it [00:07, 3075.86it/s]17780it [00:06, 1685.35it/s]17548it [00:06, 1445.16it/s]19747it [00:07, 3007.81it/s]20114it [00:07, 3139.47it/s]18170it [00:06, 2051.40it/s]17882it [00:06, 1693.24it/s]20112it [00:07, 3096.58it/s]20510it [00:07, 3358.02it/s]18510it [00:06, 2308.65it/s]18279it [00:07, 2063.32it/s]20509it [00:07, 3322.61it/s]20898it [00:07, 3501.39it/s]18897it [00:06, 2642.17it/s]18628it [00:07, 2311.66it/s]20900it [00:07, 3481.88it/s]21269it [00:07, 3445.36it/s]19258it [00:06, 2821.13it/s]19017it [00:07, 2643.23it/s]21275it [00:07, 3435.25it/s]21665it [00:07, 3589.04it/s]19648it [00:07, 3087.60it/s]19375it [00:07, 2814.73it/s]21675it [00:07, 3591.02it/s]20043it [00:07, 3312.36it/s]22035it [00:07, 3505.16it/s]19767it [00:07, 3084.88it/s]22049it [00:07, 3524.68it/s]22428it [00:08, 3624.98it/s]20414it [00:07, 3301.43it/s]20130it [00:07, 3153.69it/s]22452it [00:07, 3666.64it/s]22797it [00:08, 3575.54it/s]20805it [00:07, 3467.29it/s]20518it [00:07, 3345.02it/s]22827it [00:07, 3612.98it/s]23181it [00:08, 3649.34it/s]20911it [00:07, 3505.79it/s]21173it [00:07, 3416.96it/s]23217it [00:08, 3694.59it/s]23550it [00:08, 3509.02it/s]21569it [00:07, 3567.04it/s]21284it [00:07, 3452.02it/s]23591it [00:08, 3584.37it/s]23946it [00:08, 3637.29it/s]21680it [00:07, 3592.80it/s]21937it [00:07, 3492.26it/s]23988it [00:08, 3693.28it/s]24313it [00:08, 3540.81it/s]22337it [00:07, 3634.23it/s]22052it [00:08, 3507.34it/s]24361it [00:08, 3580.94it/s]24700it [00:08, 3632.82it/s]22451it [00:08, 3642.63it/s]22707it [00:07, 3541.04it/s]24760it [00:08, 3697.53it/s]25094it [00:08, 3721.36it/s]23103it [00:07, 3660.30it/s]22823it [00:08, 3564.29it/s]25139it [00:08, 3575.08it/s]25468it [00:08, 3588.07it/s]23215it [00:08, 3664.39it/s]23474it [00:08, 3512.54it/s]25536it [00:08, 3686.20it/s]25853it [00:08, 3662.17it/s]23871it [00:08, 3641.38it/s]23586it [00:08, 3543.60it/s]25934it [00:08, 3769.86it/s]26221it [00:09, 3525.17it/s]24256it [00:08, 3699.70it/s]23974it [00:08, 3639.21it/s]26313it [00:08, 3603.43it/s]26609it [00:09, 3620.85it/s]24629it [00:08, 3600.65it/s]24341it [00:08, 3550.22it/s]26702it [00:09, 3684.80it/s]25008it [00:08, 3652.55it/s]26973it [00:09, 3491.37it/s]24718it [00:08, 3607.61it/s]27073it [00:09, 3577.71it/s]27371it [00:09, 3628.65it/s]25108it [00:08, 3690.76it/s]25376it [00:08, 3557.08it/s]27476it [00:09, 3705.86it/s]27736it [00:09, 3554.34it/s]25765it [00:08, 3650.94it/s]25479it [00:09, 3561.52it/s]27849it [00:09, 3625.52it/s]28119it [00:09, 3632.43it/s]25873it [00:09, 3668.20it/s]26132it [00:08, 3541.04it/s]28238it [00:09, 3700.98it/s]26512it [00:08, 3614.58it/s]28499it [00:09, 3539.35it/s]26242it [00:09, 3489.35it/s]26875it [00:09, 3519.70it/s]26630it [00:09, 3597.48it/s]27261it [00:09, 3617.53it/s]26993it [00:09, 3498.87it/s]27659it [00:09, 3562.19it/s]27393it [00:09, 3640.21it/s]28046it [00:09, 3647.53it/s]27760it [00:09, 3545.12it/s]28433it [00:09, 3709.73it/s]28117it [00:09, 3472.17it/s]28498it [00:09, 3398.85it/s]28610it [00:10, 795.05it/s] 29001it [00:10, 1050.37it/s]29402it [00:11, 1360.31it/s]28855it [00:11, 706.52it/s] 29736it [00:11, 1611.40it/s]29253it [00:11, 952.59it/s]30126it [00:11, 1968.49it/s]29567it [00:11, 1164.67it/s]30475it [00:11, 2204.08it/s]29960it [00:11, 1501.13it/s]30860it [00:11, 2538.97it/s]30290it [00:11, 1747.31it/s]28840it [00:11, 800.09it/s] 31213it [00:11, 2729.06it/s]30674it [00:11, 2109.03it/s]29210it [00:11, 1049.02it/s]28806it [00:11, 703.45it/s] 31596it [00:11, 2992.96it/s]31053it [00:11, 2441.41it/s]29527it [00:11, 1280.50it/s]29198it [00:11, 938.40it/s]31954it [00:11, 3097.32it/s]31406it [00:11, 2634.93it/s]29914it [00:11, 1629.87it/s]29518it [00:11, 1152.23it/s]32352it [00:11, 3330.71it/s]31800it [00:12, 2943.40it/s]30247it [00:11, 1893.22it/s]29910it [00:11, 1481.77it/s]32727it [00:11, 3445.53it/s]30634it [00:11, 2261.92it/s]32160it [00:12, 3040.92it/s]30248it [00:11, 1751.28it/s]33096it [00:12, 3422.55it/s]31011it [00:11, 2578.25it/s]32552it [00:12, 3268.12it/s]30641it [00:11, 2124.70it/s]33468it [00:12, 3501.63it/s]31363it [00:11, 2747.03it/s]32915it [00:12, 3251.47it/s]31031it [00:11, 2473.70it/s]33831it [00:12, 3471.82it/s]31755it [00:11, 3033.20it/s]33309it [00:12, 3437.84it/s]31394it [00:11, 2675.18it/s]34238it [00:12, 3634.05it/s]32114it [00:12, 3110.23it/s]33673it [00:12, 3378.36it/s]31788it [00:11, 2971.27it/s]34609it [00:12, 3553.17it/s]32489it [00:12, 3278.90it/s]34063it [00:12, 3521.50it/s]32154it [00:11, 3079.32it/s]34992it [00:12, 3631.43it/s]32847it [00:12, 3257.84it/s]34448it [00:12, 3454.75it/s]32542it [00:12, 3287.58it/s]35359it [00:12, 3521.90it/s]33234it [00:12, 3426.71it/s]34819it [00:12, 3525.70it/s]32909it [00:12, 3253.57it/s]35753it [00:12, 3640.25it/s]33607it [00:12, 3371.31it/s]35214it [00:12, 3644.69it/s]33297it [00:12, 3421.30it/s]36128it [00:12, 3560.68it/s]33993it [00:12, 3506.95it/s]35584it [00:13, 3527.34it/s]33660it [00:12, 3387.64it/s]36517it [00:12, 3652.73it/s]34381it [00:12, 3611.71it/s]35982it [00:13, 3654.62it/s]34056it [00:12, 3545.44it/s]36898it [00:13, 3695.52it/s]34749it [00:12, 3499.14it/s]36351it [00:13, 3532.44it/s]34442it [00:12, 3634.00it/s]37270it [00:13, 3590.22it/s]35135it [00:12, 3601.50it/s]36733it [00:13, 3612.60it/s]34814it [00:12, 3521.32it/s]37658it [00:13, 3672.72it/s]35500it [00:13, 3463.50it/s]37097it [00:13, 3530.02it/s]35208it [00:12, 3639.53it/s]38027it [00:13, 3568.86it/s]35892it [00:13, 3592.06it/s]37474it [00:13, 3598.46it/s]35577it [00:12, 3541.83it/s]38420it [00:13, 3671.92it/s]36255it [00:13, 3484.45it/s]37836it [00:13, 3508.99it/s]35936it [00:13, 3464.27it/s]38789it [00:13, 3519.05it/s]36636it [00:13, 3576.79it/s]38228it [00:13, 3622.86it/s]36286it [00:13, 3397.05it/s]39180it [00:13, 3629.83it/s]38595it [00:13, 3635.52it/s]36997it [00:13, 3441.97it/s]36676it [00:13, 3539.61it/s]39545it [00:13, 3553.86it/s]37383it [00:13, 3559.72it/s]38960it [00:14, 3530.08it/s]37033it [00:13, 3462.14it/s]39942it [00:13, 3671.75it/s]37759it [00:13, 3617.13it/s]39348it [00:14, 3628.58it/s]37416it [00:13, 3565.92it/s]40328it [00:14, 3573.70it/s]39713it [00:14, 3542.77it/s]38123it [00:13, 3471.03it/s]37803it [00:13, 3653.09it/s]40689it [00:14, 3581.79it/s]40097it [00:14, 3627.67it/s]38498it [00:13, 3548.16it/s]38170it [00:13, 3560.25it/s]41088it [00:14, 3698.67it/s]40461it [00:14, 3537.89it/s]38855it [00:13, 3456.19it/s]38542it [00:13, 3606.27it/s]41460it [00:14, 3608.79it/s]40836it [00:14, 3598.78it/s]39243it [00:14, 3577.67it/s]38904it [00:13, 3488.90it/s]41857it [00:14, 3711.50it/s]41197it [00:14, 3527.36it/s]39603it [00:14, 3447.36it/s]39299it [00:13, 3621.48it/s]42230it [00:14, 3618.17it/s]41577it [00:14, 3604.71it/s]39988it [00:14, 3561.59it/s]39663it [00:14, 3523.85it/s]42633it [00:14, 3736.83it/s]41971it [00:14, 3701.36it/s]40347it [00:14, 3470.52it/s]40054it [00:14, 3632.94it/s]43008it [00:14, 3614.17it/s]42342it [00:14, 3573.15it/s]40729it [00:14, 3568.99it/s]40419it [00:14, 3516.07it/s]43371it [00:14, 3557.10it/s]42712it [00:15, 3607.73it/s]41105it [00:14, 3622.26it/s]40808it [00:14, 3620.65it/s]43728it [00:15, 3481.35it/s]43074it [00:15, 3500.95it/s]41469it [00:14, 3526.16it/s]41172it [00:14, 3528.83it/s]43452it [00:15, 3579.62it/s]41854it [00:14, 3617.49it/s]41565it [00:14, 3643.61it/s]42217it [00:14, 3523.06it/s]41942it [00:14, 3678.37it/s]42593it [00:15, 3590.44it/s]42311it [00:14, 3585.28it/s]42954it [00:15, 3437.91it/s]42707it [00:14, 3693.12it/s]43335it [00:15, 3542.25it/s]43078it [00:14, 3551.43it/s]43692it [00:15, 3427.27it/s]43456it [00:15, 3614.43it/s]44077it [00:16, 655.20it/s] 44472it [00:16, 890.59it/s]44777it [00:16, 1090.48it/s]45162it [00:16, 1412.61it/s]45492it [00:17, 1682.28it/s]45881it [00:17, 2055.87it/s]43812it [00:17, 548.74it/s] 46269it [00:17, 2411.50it/s]44209it [00:17, 753.67it/s]43819it [00:16, 679.14it/s] 46627it [00:17, 2617.23it/s]44598it [00:17, 989.56it/s]44210it [00:16, 912.40it/s]47007it [00:17, 2892.26it/s]44998it [00:17, 1291.03it/s]44598it [00:16, 1172.62it/s]47365it [00:17, 3014.61it/s]45389it [00:17, 1618.47it/s]44037it [00:17, 556.23it/s] 44995it [00:17, 1497.78it/s]47753it [00:17, 3239.61it/s]45743it [00:17, 1869.62it/s]44431it [00:17, 765.72it/s]45368it [00:17, 1815.40it/s]48116it [00:17, 3271.25it/s]46121it [00:17, 2202.89it/s]44736it [00:17, 951.32it/s]45716it [00:17, 2075.73it/s]48515it [00:17, 3468.16it/s]46473it [00:18, 2422.40it/s]45124it [00:17, 1255.63it/s]46099it [00:17, 2413.77it/s]48883it [00:17, 3416.38it/s]46854it [00:18, 2726.32it/s]45447it [00:17, 1505.93it/s]46453it [00:17, 2607.30it/s]49276it [00:18, 3559.02it/s]47209it [00:18, 2877.91it/s]45832it [00:17, 1870.02it/s]46826it [00:17, 2868.02it/s]49644it [00:18, 3490.40it/s]47575it [00:18, 3072.76it/s]46199it [00:17, 2196.14it/s]47180it [00:17, 2970.93it/s]50038it [00:18, 3618.05it/s]47958it [00:18, 3146.02it/s]46547it [00:17, 2410.01it/s]47572it [00:17, 3215.04it/s]50426it [00:18, 3692.83it/s]48350it [00:18, 3350.87it/s]46928it [00:18, 2722.67it/s]47958it [00:17, 3247.44it/s]50800it [00:18, 3558.46it/s]48743it [00:18, 3509.17it/s]47279it [00:18, 2852.96it/s]48339it [00:17, 3396.70it/s]51186it [00:18, 3643.69it/s]49113it [00:18, 3460.31it/s]47651it [00:18, 3071.56it/s]48729it [00:18, 3534.77it/s]51554it [00:18, 3530.29it/s]49493it [00:18, 3555.14it/s]48003it [00:18, 3119.79it/s]49098it [00:18, 3463.06it/s]51941it [00:18, 3625.79it/s]49859it [00:18, 3483.13it/s]48387it [00:18, 3314.05it/s]49485it [00:18, 3576.08it/s]52307it [00:18, 3529.26it/s]50246it [00:19, 3591.05it/s]48778it [00:18, 3478.13it/s]49851it [00:18, 3463.80it/s]52680it [00:18, 3586.29it/s]50611it [00:19, 3460.52it/s]49144it [00:18, 3378.62it/s]50241it [00:18, 3585.54it/s]53041it [00:19, 3494.41it/s]50999it [00:19, 3578.12it/s]49528it [00:18, 3507.62it/s]50605it [00:18, 3488.32it/s]53428it [00:19, 3599.76it/s]51361it [00:19, 3493.41it/s]49889it [00:18, 3429.54it/s]50995it [00:18, 3604.68it/s]53810it [00:19, 3661.75it/s]51739it [00:19, 3567.46it/s]50269it [00:19, 3533.38it/s]51359it [00:18, 3479.62it/s]54178it [00:19, 3574.44it/s]52133it [00:19, 3674.02it/s]50628it [00:19, 3400.44it/s]51744it [00:18, 3582.50it/s]54558it [00:19, 3639.47it/s]52503it [00:19, 3553.17it/s]51020it [00:19, 3544.79it/s]52135it [00:18, 3676.67it/s]54923it [00:19, 3539.89it/s]52888it [00:19, 3638.43it/s]51379it [00:19, 3449.26it/s]52505it [00:19, 3556.36it/s]55308it [00:19, 3628.65it/s]53254it [00:19, 3495.59it/s]51760it [00:19, 3551.19it/s]52877it [00:19, 3602.02it/s]55672it [00:19, 3502.24it/s]53634it [00:20, 3581.96it/s]52135it [00:19, 3606.64it/s]53239it [00:19, 3505.40it/s]56065it [00:19, 3623.91it/s]53995it [00:20, 3503.99it/s]52498it [00:19, 3492.48it/s]53623it [00:19, 3600.63it/s]56429it [00:20, 3501.18it/s]54381it [00:20, 3604.63it/s]52879it [00:19, 3583.47it/s]53985it [00:19, 3504.08it/s]56825it [00:20, 3631.94it/s]54743it [00:20, 3486.65it/s]53240it [00:19, 3471.41it/s]54372it [00:19, 3609.24it/s]57199it [00:20, 3555.59it/s]55128it [00:20, 3589.06it/s]53608it [00:19, 3529.15it/s]54735it [00:19, 3503.14it/s]57594it [00:20, 3668.19it/s]55510it [00:20, 3655.49it/s]53963it [00:20, 3442.43it/s]55119it [00:19, 3599.01it/s]57986it [00:20, 3740.60it/s]55877it [00:20, 3507.59it/s]54353it [00:20, 3571.70it/s]55497it [00:19, 3651.25it/s]58362it [00:20, 3582.62it/s]56271it [00:20, 3629.11it/s]54712it [00:20, 3424.09it/s]55864it [00:20, 3498.70it/s]58750it [00:20, 3665.53it/s]56636it [00:20, 3525.91it/s]55097it [00:20, 3542.84it/s]56253it [00:20, 3609.25it/s]59119it [00:20, 3540.01it/s]57019it [00:20, 3612.73it/s]55470it [00:20, 3596.80it/s]56616it [00:20, 3490.12it/s]59505it [00:20, 3631.15it/s]57382it [00:21, 3529.64it/s]55832it [00:20, 3485.44it/s]56987it [00:20, 3552.55it/s]59870it [00:21, 3512.95it/s]57768it [00:21, 3624.56it/s]56222it [00:20, 3602.83it/s]57344it [00:20, 3444.65it/s]60247it [00:21, 3585.28it/s]58132it [00:21, 3497.18it/s]56584it [00:20, 3445.35it/s]57726it [00:20, 3549.73it/s]60608it [00:21, 3476.06it/s]58498it [00:21, 3541.94it/s]56970it [00:20, 3562.81it/s]58083it [00:20, 3458.58it/s]60997it [00:21, 3592.87it/s]58879it [00:21, 3453.57it/s]57329it [00:21, 3482.56it/s]58461it [00:20, 3550.49it/s]61391it [00:21, 3692.03it/s]59268it [00:21, 3576.94it/s]57711it [00:21, 3578.74it/s]58846it [00:20, 3636.12it/s]61762it [00:21, 3563.25it/s]59641it [00:21, 3620.09it/s]58071it [00:21, 3473.42it/s]59211it [00:21, 3444.97it/s]62150it [00:21, 3652.21it/s]60005it [00:21, 3471.20it/s]58454it [00:21, 3575.21it/s]59592it [00:21, 3546.40it/s]60392it [00:21, 3584.47it/s]58823it [00:21, 3607.28it/s]59950it [00:21, 3435.10it/s]60753it [00:22, 3452.80it/s]59185it [00:21, 3483.33it/s]60312it [00:21, 3486.82it/s]61139it [00:22, 3567.99it/s]59553it [00:21, 3539.36it/s]60663it [00:21, 3366.17it/s]61498it [00:22, 3474.93it/s]59909it [00:21, 3372.73it/s]61028it [00:21, 3446.68it/s]61889it [00:22, 3594.56it/s]60278it [00:21, 3462.39it/s]61398it [00:21, 3313.93it/s]62251it [00:22, 3492.29it/s]60627it [00:21, 3378.99it/s]61782it [00:21, 3459.80it/s]61009it [00:22, 3503.34it/s]62153it [00:21, 3530.62it/s]61387it [00:22, 3581.33it/s]61747it [00:22, 3453.26it/s]62124it [00:22, 3543.12it/s]62517it [00:23, 515.48it/s] 62905it [00:23, 701.59it/s]63234it [00:24, 891.07it/s]63621it [00:24, 1173.20it/s]63987it [00:24, 1451.79it/s]64369it [00:24, 1792.59it/s]64761it [00:24, 2155.08it/s]65122it [00:24, 2401.61it/s]62602it [00:24, 486.30it/s] 65511it [00:24, 2723.00it/s]62995it [00:24, 671.75it/s]63316it [00:24, 853.05it/s]65875it [00:24, 2876.09it/s]63700it [00:25, 1129.62it/s]66251it [00:24, 3094.41it/s]64027it [00:25, 1378.34it/s]66612it [00:24, 3154.03it/s]64407it [00:25, 1723.65it/s]67001it [00:25, 3350.67it/s]64774it [00:25, 2053.15it/s]67364it [00:25, 3317.21it/s]65124it [00:25, 2297.61it/s]67754it [00:25, 3477.28it/s]65474it [00:25, 2554.87it/s]68137it [00:25, 3576.84it/s]65818it [00:25, 2719.86it/s]68506it [00:25, 3475.44it/s]66208it [00:25, 3011.29it/s]68895it [00:25, 3591.27it/s]62480it [00:25, 384.85it/s] 66561it [00:25, 3084.99it/s]69261it [00:25, 3488.81it/s]62848it [00:25, 526.90it/s]66948it [00:25, 3293.46it/s]69646it [00:25, 3590.94it/s]63146it [00:25, 669.19it/s]67324it [00:26, 3422.55it/s]70010it [00:25, 3499.19it/s]63512it [00:25, 897.99it/s]67688it [00:26, 3396.20it/s]62509it [00:25, 324.02it/s] 70390it [00:26, 3584.12it/s]63876it [00:25, 1167.96it/s]68073it [00:26, 3523.07it/s]62883it [00:25, 448.71it/s]70751it [00:26, 3492.93it/s]64204it [00:25, 1413.32it/s]63201it [00:25, 584.08it/s]68437it [00:26, 3429.91it/s]71130it [00:26, 3576.44it/s]64566it [00:25, 1740.58it/s]63566it [00:25, 786.76it/s]68822it [00:26, 3547.70it/s]71516it [00:26, 3657.88it/s]64899it [00:26, 1961.85it/s]63941it [00:25, 1042.37it/s]69184it [00:26, 3464.74it/s]71884it [00:26, 3543.98it/s]65256it [00:26, 2275.55it/s]64276it [00:25, 1287.08it/s]69571it [00:26, 3577.93it/s]72250it [00:26, 3575.76it/s]65615it [00:26, 2561.85it/s]64633it [00:25, 1594.59it/s]69933it [00:26, 3460.03it/s]72609it [00:26, 3475.87it/s]65954it [00:26, 2685.55it/s]64971it [00:26, 1859.81it/s]70320it [00:26, 3576.58it/s]73006it [00:26, 3617.36it/s]66310it [00:26, 2901.66it/s]65339it [00:26, 2196.92it/s]70708it [00:26, 3504.28it/s]73370it [00:26, 3504.62it/s]66647it [00:26, 2931.64it/s]65681it [00:26, 2384.95it/s]71075it [00:27, 3549.31it/s]73766it [00:26, 3633.50it/s]67009it [00:26, 3113.21it/s]66055it [00:26, 2689.20it/s]71464it [00:27, 3645.66it/s]74132it [00:27, 3530.55it/s]67346it [00:26, 3091.07it/s]66398it [00:26, 2867.80it/s]71831it [00:27, 3537.26it/s]74517it [00:27, 3620.81it/s]67708it [00:26, 3235.68it/s]66741it [00:26, 2913.15it/s]72219it [00:27, 3597.96it/s]74904it [00:27, 3691.80it/s]68070it [00:26, 3343.82it/s]67100it [00:26, 3089.93it/s]72581it [00:27, 3511.42it/s]75275it [00:27, 3557.88it/s]68415it [00:27, 3253.63it/s]67439it [00:26, 3008.18it/s]72978it [00:27, 3640.92it/s]75663it [00:27, 3649.95it/s]68778it [00:27, 3358.88it/s]67810it [00:26, 3196.18it/s]73344it [00:27, 3540.68it/s]76030it [00:27, 3496.86it/s]69120it [00:27, 3259.50it/s]68178it [00:27, 3328.63it/s]73725it [00:27, 3616.30it/s]76411it [00:27, 3584.96it/s]69487it [00:27, 3375.72it/s]68524it [00:27, 3259.30it/s]74088it [00:27, 3551.11it/s]76772it [00:27, 3486.49it/s]69855it [00:27, 3463.26it/s]68898it [00:27, 3384.03it/s]74476it [00:28, 3645.15it/s]77149it [00:27, 3567.18it/s]70205it [00:27, 3349.85it/s]69244it [00:27, 3292.69it/s]74844it [00:28, 3654.25it/s]77508it [00:28, 3489.97it/s]70565it [00:27, 3420.47it/s]69626it [00:27, 3441.91it/s]75211it [00:28, 3524.11it/s]77881it [00:28, 3558.15it/s]70910it [00:27, 3333.78it/s]69975it [00:27, 3323.85it/s]75598it [00:28, 3622.57it/s]78267it [00:28, 3646.00it/s]71283it [00:27, 3447.04it/s]70357it [00:27, 3462.14it/s]75962it [00:28, 3514.42it/s]78633it [00:28, 3534.56it/s]71630it [00:28, 3289.92it/s]70707it [00:27, 3347.11it/s]76327it [00:28, 3551.83it/s]79017it [00:28, 3621.67it/s]72012it [00:28, 3438.44it/s]71082it [00:27, 3460.16it/s]76684it [00:28, 3472.06it/s]79381it [00:28, 3514.13it/s]72368it [00:28, 3472.56it/s]71450it [00:27, 3522.06it/s]77066it [00:28, 3571.00it/s]79749it [00:28, 3561.51it/s]72718it [00:28, 3393.03it/s]71805it [00:28, 3400.69it/s]77428it [00:28, 3481.12it/s]80107it [00:28, 3473.23it/s]73087it [00:28, 3476.89it/s]72176it [00:28, 3479.90it/s]77816it [00:28, 3594.18it/s]80499it [00:28, 3600.01it/s]73437it [00:28, 3395.02it/s]78200it [00:29, 3664.22it/s]72526it [00:28, 3372.50it/s]80861it [00:28, 3503.23it/s]73827it [00:28, 3538.74it/s]72913it [00:28, 3511.99it/s]78568it [00:29, 3562.72it/s]81243it [00:29, 3592.21it/s]74183it [00:28, 3458.24it/s]78953it [00:29, 3643.29it/s]73267it [00:28, 3370.49it/s]81613it [00:29, 3621.32it/s]74554it [00:28, 3528.57it/s]73660it [00:28, 3528.07it/s]79319it [00:29, 3524.51it/s]81977it [00:29, 3512.94it/s]74908it [00:28, 3409.77it/s]74037it [00:28, 3597.55it/s]79704it [00:29, 3615.78it/s]82360it [00:29, 3602.70it/s]75287it [00:29, 3518.83it/s]74399it [00:28, 3494.22it/s]80067it [00:29, 3493.11it/s]82722it [00:29, 3509.77it/s]75669it [00:29, 3606.05it/s]74781it [00:28, 3586.82it/s]80455it [00:29, 3603.68it/s]83106it [00:29, 3604.12it/s]76031it [00:29, 3465.92it/s]80817it [00:29, 3514.70it/s]75142it [00:29, 3422.38it/s]83468it [00:29, 3466.93it/s]76408it [00:29, 3552.77it/s]81199it [00:29, 3601.54it/s]75527it [00:29, 3541.63it/s]83856it [00:29, 3582.64it/s]76765it [00:29, 3430.79it/s]81579it [00:30, 3657.07it/s]75884it [00:29, 3414.32it/s]84216it [00:29, 3481.99it/s]77140it [00:29, 3521.33it/s]81946it [00:30, 3538.79it/s]76245it [00:29, 3467.97it/s]84599it [00:29, 3579.54it/s]77494it [00:29, 3427.52it/s]82328it [00:30, 3612.52it/s]76594it [00:29, 3348.64it/s]84982it [00:30, 3649.97it/s]77871it [00:29, 3525.66it/s]82691it [00:30, 3485.37it/s]76962it [00:29, 3442.04it/s]85349it [00:30, 3481.52it/s]78246it [00:29, 3590.50it/s]83078it [00:30, 3593.31it/s]77329it [00:29, 3505.78it/s]85737it [00:30, 3594.65it/s]78607it [00:30, 3459.21it/s]83439it [00:30, 3495.00it/s]77682it [00:29, 3375.56it/s]78976it [00:30, 3522.92it/s]83823it [00:30, 3592.22it/s]78046it [00:29, 3448.83it/s]79330it [00:30, 3366.93it/s]84184it [00:30, 3489.97it/s]78393it [00:30, 3336.10it/s]79706it [00:30, 3477.02it/s]84568it [00:30, 3587.01it/s]78757it [00:30, 3422.78it/s]80056it [00:30, 3228.58it/s]84955it [00:30, 3666.47it/s]79107it [00:30, 3313.40it/s]80437it [00:30, 3384.93it/s]85323it [00:31, 3519.46it/s]79477it [00:30, 3422.69it/s]80787it [00:30, 3330.52it/s]85708it [00:31, 3613.91it/s]79848it [00:30, 3504.78it/s]81161it [00:30, 3444.13it/s]80201it [00:30, 3348.90it/s]81534it [00:30, 3525.98it/s]80588it [00:30, 3496.83it/s]81889it [00:31, 3426.08it/s]80941it [00:30, 3399.21it/s]82267it [00:31, 3525.35it/s]81323it [00:30, 3518.46it/s]82622it [00:31, 3395.90it/s]81677it [00:30, 3417.22it/s]83006it [00:31, 3520.87it/s]82060it [00:31, 3535.00it/s]83361it [00:31, 3371.99it/s]82446it [00:31, 3627.74it/s]83741it [00:31, 3492.11it/s]82811it [00:31, 3513.00it/s]84122it [00:31, 3582.39it/s]83191it [00:31, 3593.42it/s]84483it [00:31, 3440.06it/s]83552it [00:31, 3482.80it/s]84860it [00:31, 3532.23it/s]83937it [00:31, 3581.16it/s]85216it [00:31, 3402.79it/s]84297it [00:31, 3470.10it/s]85596it [00:32, 3514.90it/s]84677it [00:31, 3563.57it/s]85035it [00:31, 3442.21it/s]85403it [00:32, 3509.33it/s]85784it [00:32, 3596.00it/s]86099it [00:33, 406.50it/s] 86475it [00:33, 555.98it/s]86791it [00:33, 711.84it/s]87157it [00:33, 944.02it/s]87543it [00:33, 1238.82it/s]87883it [00:33, 1500.09it/s]88259it [00:33, 1842.58it/s]86072it [00:33, 414.27it/s] 88606it [00:33, 2106.67it/s]86453it [00:34, 568.11it/s]88984it [00:33, 2442.39it/s]86763it [00:34, 722.00it/s]89336it [00:34, 2589.00it/s]87070it [00:34, 909.52it/s]89673it [00:34, 2719.06it/s]87382it [00:34, 1135.61it/s]90002it [00:34, 2807.43it/s]87687it [00:34, 1351.94it/s]90324it [00:34, 2854.34it/s]88070it [00:34, 1727.43it/s]90666it [00:34, 3003.07it/s]88417it [00:34, 2014.16it/s]90989it [00:34, 3002.64it/s]88780it [00:34, 2337.53it/s]91347it [00:34, 3161.70it/s]89152it [00:34, 2644.73it/s]91707it [00:34, 3280.23it/s]89497it [00:34, 2768.52it/s]92045it [00:34, 3193.54it/s]89848it [00:35, 2953.23it/s]92405it [00:35, 3307.32it/s]90188it [00:35, 3003.89it/s]92742it [00:35, 3253.27it/s]90555it [00:35, 3183.19it/s]93072it [00:35, 3215.21it/s]90898it [00:35, 3083.41it/s]93446it [00:35, 3364.72it/s]91224it [00:35, 3089.70it/s]93785it [00:35, 3299.20it/s]91597it [00:35, 3267.14it/s]94158it [00:35, 3415.15it/s]91934it [00:35, 3221.53it/s]94502it [00:35, 3354.98it/s]92293it [00:35, 3323.98it/s]94839it [00:35, 3327.62it/s]92631it [00:35, 3258.02it/s]95173it [00:35, 3275.72it/s]93004it [00:36, 3391.98it/s]95547it [00:35, 3408.97it/s]93375it [00:36, 3482.96it/s]95914it [00:36, 3483.41it/s]93726it [00:36, 3372.59it/s]96264it [00:36, 3390.71it/s]94096it [00:36, 3466.35it/s]96641it [00:36, 3498.84it/s]94445it [00:36, 3373.46it/s]96992it [00:36, 3371.17it/s]94787it [00:36, 3382.38it/s]97359it [00:36, 3455.21it/s]95137it [00:36, 3296.76it/s]97706it [00:36, 3367.40it/s]95512it [00:36, 3422.75it/s]98075it [00:36, 3459.52it/s]95872it [00:36, 3473.91it/s]98441it [00:36, 3517.74it/s]96221it [00:36, 3377.48it/s]98794it [00:36, 3409.51it/s]96594it [00:37, 3476.95it/s]99169it [00:36, 3506.29it/s]96943it [00:37, 3369.89it/s]86145it [00:36, 267.74it/s] 85950it [00:36, 246.67it/s] 99521it [00:37, 3354.64it/s]97307it [00:37, 3446.59it/s]86523it [00:36, 373.68it/s]86302it [00:36, 338.58it/s]99896it [00:37, 3464.91it/s]97657it [00:37, 3357.38it/s]86816it [00:36, 479.27it/s]86665it [00:36, 465.51it/s]100245it [00:37, 3376.92it/s]98034it [00:37, 3474.89it/s]87192it [00:36, 662.02it/s]86970it [00:37, 599.98it/s]100619it [00:37, 3480.88it/s]98409it [00:37, 3553.41it/s]87561it [00:36, 885.17it/s]87336it [00:37, 812.19it/s]100995it [00:37, 3560.55it/s]98766it [00:37, 3441.29it/s]87894it [00:36, 1113.38it/s]87659it [00:37, 1021.97it/s]101353it [00:37, 3455.23it/s]99138it [00:37, 3519.47it/s]88256it [00:37, 1410.37it/s]88025it [00:37, 1320.80it/s]101723it [00:37, 3523.58it/s]99492it [00:37, 3415.17it/s]88394it [00:37, 1650.37it/s]88593it [00:37, 1682.35it/s]102077it [00:37, 3423.85it/s]99858it [00:38, 3484.92it/s]88974it [00:37, 2046.06it/s]88736it [00:37, 1899.29it/s]102451it [00:37, 3513.17it/s]100208it [00:38, 3378.74it/s]89321it [00:37, 2278.68it/s]89105it [00:37, 2235.70it/s]102804it [00:38, 3409.97it/s]100586it [00:38, 3492.18it/s]89685it [00:37, 2571.47it/s]89446it [00:37, 2427.39it/s]103178it [00:38, 3502.56it/s]100959it [00:38, 3559.03it/s]90059it [00:37, 2845.61it/s]89796it [00:37, 2671.29it/s]103533it [00:38, 3516.16it/s]101317it [00:38, 3444.65it/s]90413it [00:37, 2891.89it/s]90133it [00:38, 2774.96it/s]103886it [00:38, 3412.11it/s]101686it [00:38, 3514.98it/s]90775it [00:37, 3077.55it/s]90483it [00:38, 2959.90it/s]104255it [00:38, 3491.06it/s]102039it [00:38, 3371.85it/s]90818it [00:38, 3058.38it/s]91120it [00:37, 3029.36it/s]104606it [00:38, 3388.81it/s]102409it [00:38, 3464.43it/s]91490it [00:38, 3207.26it/s]91152it [00:38, 2939.02it/s]104978it [00:38, 3484.04it/s]102758it [00:38, 3348.73it/s]91832it [00:38, 3166.30it/s]91513it [00:38, 3118.61it/s]105328it [00:38, 3353.19it/s]103127it [00:38, 3443.41it/s]92187it [00:38, 3270.17it/s]91842it [00:38, 3044.50it/s]105698it [00:38, 3451.97it/s]103497it [00:39, 3515.19it/s]92547it [00:38, 3362.43it/s]92203it [00:38, 3199.21it/s]106058it [00:38, 3359.71it/s]103850it [00:39, 3415.61it/s]92892it [00:38, 3272.80it/s]92552it [00:38, 3280.16it/s]106433it [00:39, 3470.46it/s]104223it [00:39, 3505.59it/s]93244it [00:38, 3340.99it/s]92888it [00:38, 3203.34it/s]106808it [00:39, 3549.73it/s]104575it [00:39, 3374.11it/s]93583it [00:38, 3267.90it/s]93246it [00:38, 3304.54it/s]107165it [00:39, 3440.43it/s]104942it [00:39, 3457.51it/s]93952it [00:38, 3386.36it/s]93581it [00:39, 3218.93it/s]107526it [00:39, 3486.83it/s]105290it [00:39, 3360.34it/s]94296it [00:38, 3270.60it/s]93937it [00:39, 3315.76it/s]107876it [00:39, 3379.29it/s]105659it [00:39, 3454.18it/s]94662it [00:38, 3379.51it/s]94287it [00:39, 3367.35it/s]108247it [00:39, 3473.91it/s]106021it [00:39, 3502.27it/s]95018it [00:39, 3430.07it/s]94626it [00:39, 3231.92it/s]108596it [00:39, 3368.71it/s]106373it [00:39, 3381.18it/s]95363it [00:39, 3332.28it/s]94991it [00:39, 3349.77it/s]108972it [00:39, 3479.49it/s]106740it [00:40, 3462.22it/s]95722it [00:39, 3406.07it/s]95329it [00:39, 3255.84it/s]109336it [00:39, 3524.10it/s]107088it [00:40, 3331.84it/s]96065it [00:39, 3314.22it/s]95697it [00:39, 3376.08it/s]109690it [00:40, 3400.95it/s]107452it [00:40, 3418.97it/s]96428it [00:39, 3403.51it/s]96037it [00:39, 3278.28it/s]110058it [00:40, 3479.28it/s]107796it [00:40, 3338.76it/s]96803it [00:39, 3503.08it/s]96408it [00:39, 3399.96it/s]110408it [00:40, 3358.36it/s]108158it [00:40, 3418.42it/s]97155it [00:39, 3355.50it/s]96781it [00:40, 3495.35it/s]110771it [00:40, 3433.47it/s]108524it [00:40, 3487.23it/s]97527it [00:39, 3459.28it/s]97133it [00:40, 3385.01it/s]111116it [00:40, 3323.56it/s]108874it [00:40, 3399.95it/s]97875it [00:39, 3334.71it/s]97507it [00:40, 3486.93it/s]111491it [00:40, 3442.90it/s]109245it [00:40, 3488.54it/s]98249it [00:40, 3449.96it/s]97858it [00:40, 3387.91it/s]111861it [00:40, 3514.95it/s]109595it [00:40, 3337.04it/s]98227it [00:40, 3473.51it/s]98596it [00:40, 3321.86it/s]112214it [00:40, 3412.43it/s]109968it [00:40, 3448.02it/s]98971it [00:40, 3441.94it/s]98576it [00:40, 3368.79it/s]112584it [00:40, 3495.32it/s]110315it [00:41, 3355.47it/s]99334it [00:40, 3494.05it/s]98948it [00:40, 3468.99it/s]112935it [00:40, 3400.77it/s]110688it [00:41, 3460.88it/s]99323it [00:40, 3549.27it/s]99686it [00:40, 3376.79it/s]113311it [00:41, 3502.27it/s]111062it [00:41, 3540.16it/s]100047it [00:40, 3441.60it/s]99680it [00:40, 3422.66it/s]113663it [00:41, 3406.36it/s]111418it [00:41, 3431.12it/s]100040it [00:40, 3473.60it/s]100393it [00:40, 3342.40it/s]114039it [00:41, 3505.99it/s]111787it [00:41, 3505.21it/s]100758it [00:40, 3428.30it/s]100389it [00:41, 3360.19it/s]114418it [00:41, 3587.84it/s]112139it [00:41, 3405.74it/s]101103it [00:40, 3335.39it/s]100731it [00:41, 3283.61it/s]114778it [00:41, 3437.22it/s]112496it [00:41, 3451.44it/s]101465it [00:40, 3416.99it/s]101061it [00:41, 3218.53it/s]115155it [00:41, 3531.03it/s]112843it [00:41, 3331.03it/s]101839it [00:41, 3509.28it/s]101431it [00:41, 3355.16it/s]113216it [00:41, 3444.37it/s]101785it [00:41, 3408.37it/s]102192it [00:41, 3319.42it/s]113587it [00:42, 3519.37it/s]102527it [00:41, 3291.91it/s]102127it [00:41, 3243.13it/s]113941it [00:42, 3329.28it/s]102858it [00:41, 3188.83it/s]102454it [00:41, 3170.15it/s]114318it [00:42, 3446.98it/s]103230it [00:41, 3338.98it/s]102773it [00:41, 3119.48it/s]114666it [00:42, 3335.35it/s]103566it [00:41, 3262.87it/s]103134it [00:41, 3258.09it/s]115038it [00:42, 3443.70it/s]103932it [00:41, 3374.28it/s]103504it [00:42, 3385.64it/s]104304it [00:41, 3473.78it/s]103845it [00:42, 3263.14it/s]104653it [00:41, 3366.76it/s]104218it [00:42, 3395.28it/s]105020it [00:42, 3452.13it/s]104560it [00:42, 3271.28it/s]105367it [00:42, 3319.13it/s]104916it [00:42, 3351.49it/s]105739it [00:42, 3433.36it/s]105253it [00:42, 3285.21it/s]106085it [00:42, 3331.57it/s]105607it [00:42, 3356.90it/s]106460it [00:42, 3448.64it/s]105980it [00:42, 3463.62it/s]106831it [00:42, 3522.34it/s]106328it [00:42, 3332.48it/s]107185it [00:42, 3403.06it/s]106698it [00:42, 3435.75it/s]107558it [00:42, 3494.62it/s]107044it [00:43, 3278.16it/s]107910it [00:42, 3365.03it/s]107413it [00:43, 3393.96it/s]108288it [00:42, 3483.03it/s]107755it [00:43, 3285.70it/s]108639it [00:43, 3387.92it/s]108130it [00:43, 3416.04it/s]109014it [00:43, 3490.34it/s]108490it [00:43, 3469.08it/s]109379it [00:43, 3535.81it/s]108839it [00:43, 3381.35it/s]109734it [00:43, 3416.80it/s]109196it [00:43, 3433.73it/s]110105it [00:43, 3498.44it/s]109541it [00:43, 3350.21it/s]110457it [00:43, 3345.94it/s]109899it [00:43, 3415.22it/s]110817it [00:43, 3417.00it/s]110256it [00:44, 3323.92it/s]111161it [00:43, 3359.43it/s]110618it [00:44, 3406.89it/s]111539it [00:43, 3478.24it/s]110992it [00:44, 3502.50it/s]111916it [00:44, 3562.63it/s]111344it [00:44, 3363.70it/s]112274it [00:44, 3459.80it/s]111715it [00:44, 3461.39it/s]115510it [00:44, 350.03it/s] 112622it [00:44, 3273.83it/s]112063it [00:44, 3253.45it/s]115875it [00:44, 480.15it/s]112952it [00:44, 3216.62it/s]112427it [00:44, 3360.41it/s]116206it [00:45, 629.36it/s]113330it [00:44, 3374.32it/s]112776it [00:44, 3254.89it/s]116583it [00:45, 851.14it/s]113670it [00:44, 3307.35it/s]113141it [00:44, 3365.60it/s]116960it [00:45, 1118.55it/s]114045it [00:44, 3431.95it/s]113504it [00:44, 3439.51it/s]117297it [00:45, 1369.10it/s]114427it [00:44, 3543.06it/s]113851it [00:45, 3344.79it/s]117675it [00:45, 1708.49it/s]114783it [00:44, 3441.66it/s]114216it [00:45, 3430.47it/s]118020it [00:45, 1974.09it/s]115159it [00:44, 3531.24it/s]114561it [00:45, 3330.75it/s]118390it [00:45, 2303.13it/s]114913it [00:45, 3382.47it/s]118737it [00:45, 2491.76it/s]115270it [00:45, 3435.69it/s]119110it [00:45, 2776.19it/s]115385it [00:46, 310.47it/s] 119471it [00:45, 2982.32it/s]115751it [00:46, 430.19it/s]119823it [00:46, 3028.26it/s]116126it [00:46, 591.45it/s]120189it [00:46, 3194.77it/s]116436it [00:46, 754.04it/s]116807it [00:46, 1004.57it/s]120537it [00:46, 3184.07it/s]120899it [00:46, 3303.12it/s]117135it [00:46, 1243.21it/s]117494it [00:46, 1555.42it/s]121247it [00:46, 3260.15it/s]117876it [00:46, 1916.49it/s]121625it [00:46, 3405.07it/s]122003it [00:46, 3512.32it/s]118224it [00:46, 2168.41it/s]118597it [00:46, 2491.06it/s]122361it [00:46, 3417.40it/s]122719it [00:46, 3462.69it/s]118946it [00:47, 2646.42it/s]119305it [00:47, 2872.44it/s]123069it [00:47, 3330.25it/s]123423it [00:47, 3388.65it/s]119649it [00:47, 2909.54it/s]119999it [00:47, 3061.28it/s]123767it [00:47, 3274.79it/s]120358it [00:47, 3203.88it/s]124124it [00:47, 3358.13it/s]120702it [00:47, 3189.28it/s]124497it [00:47, 3465.44it/s]121073it [00:47, 3334.45it/s]124846it [00:47, 3366.43it/s]121419it [00:47, 3244.53it/s]125197it [00:47, 3406.91it/s]121790it [00:47, 3375.26it/s]125540it [00:47, 3323.69it/s]122135it [00:48, 3316.63it/s]125922it [00:47, 3464.49it/s]122507it [00:48, 3423.56it/s]126283it [00:47, 3506.56it/s]122888it [00:48, 3533.15it/s]126635it [00:48, 3409.50it/s]126991it [00:48, 3451.26it/s]123245it [00:48, 3375.74it/s]123599it [00:48, 3421.11it/s]127338it [00:48, 3317.85it/s]127710it [00:48, 3431.93it/s]123944it [00:48, 3323.90it/s]124319it [00:48, 3445.58it/s]128055it [00:48, 3355.88it/s]128433it [00:48, 3477.69it/s]124666it [00:48, 3343.31it/s]128803it [00:48, 3539.68it/s]125027it [00:48, 3418.23it/s]125381it [00:48, 3451.87it/s]129159it [00:48, 3374.62it/s]129528it [00:48, 3463.57it/s]125728it [00:49, 3340.94it/s]126106it [00:49, 3466.97it/s]129877it [00:49, 3379.25it/s]130240it [00:49, 3450.08it/s]126455it [00:49, 3377.45it/s]126826it [00:49, 3472.34it/s]130587it [00:49, 3354.28it/s]130960it [00:49, 3461.40it/s]127175it [00:49, 3358.20it/s]127543it [00:49, 3450.13it/s]131327it [00:49, 3349.90it/s]127899it [00:49, 3480.62it/s]131685it [00:49, 3412.43it/s]132036it [00:49, 3439.48it/s]128249it [00:49, 3301.34it/s]128609it [00:49, 3384.08it/s]132382it [00:49, 3321.92it/s]128950it [00:50, 3327.07it/s]132759it [00:49, 3449.95it/s]129331it [00:50, 3465.88it/s]133106it [00:49, 3369.90it/s]133483it [00:50, 3483.19it/s]129680it [00:50, 3361.78it/s]130063it [00:50, 3494.17it/s]133847it [00:50, 3361.48it/s]130441it [00:50, 3576.74it/s]134225it [00:50, 3479.50it/s]130801it [00:50, 3464.14it/s]134603it [00:50, 3565.30it/s]131179it [00:50, 3553.16it/s]134962it [00:50, 3453.43it/s]131536it [00:50, 3415.82it/s]135335it [00:50, 3530.95it/s]131910it [00:50, 3506.22it/s]135690it [00:50, 3437.72it/s]136069it [00:50, 3538.80it/s]132263it [00:50, 3391.14it/s]115615it [00:50, 220.73it/s] 132643it [00:51, 3506.61it/s]136425it [00:50, 3432.10it/s]115981it [00:50, 310.75it/s]136805it [00:51, 3537.26it/s]133007it [00:51, 3412.83it/s]116292it [00:50, 411.65it/s]137182it [00:51, 3603.58it/s]133382it [00:51, 3507.83it/s]115514it [00:50, 207.85it/s] 116655it [00:50, 569.22it/s]133758it [00:51, 3579.32it/s]137544it [00:51, 3441.40it/s]115890it [00:50, 292.81it/s]117034it [00:50, 779.72it/s]134118it [00:51, 3457.49it/s]137917it [00:51, 3521.86it/s]116204it [00:50, 386.80it/s]117366it [00:51, 988.77it/s]134482it [00:51, 3507.96it/s]138272it [00:51, 3422.13it/s]116582it [00:50, 539.67it/s]117742it [00:51, 1287.85it/s]134835it [00:51, 3416.32it/s]138649it [00:51, 3519.88it/s]116962it [00:50, 736.78it/s]118081it [00:51, 1547.03it/s]135214it [00:51, 3523.35it/s]139003it [00:51, 3430.36it/s]117300it [00:51, 942.03it/s]118452it [00:51, 1889.61it/s]135568it [00:51, 3431.08it/s]139380it [00:51, 3526.51it/s]117674it [00:51, 1226.11it/s]118794it [00:51, 2124.98it/s]135947it [00:52, 3528.32it/s]139735it [00:51, 3429.67it/s]118019it [00:51, 1494.23it/s]119166it [00:51, 2450.76it/s]136324it [00:52, 3597.49it/s]140114it [00:51, 3532.59it/s]118392it [00:51, 1833.85it/s]119538it [00:51, 2737.52it/s]140490it [00:52, 3597.77it/s]136685it [00:52, 3470.46it/s]118740it [00:51, 2083.71it/s]119890it [00:51, 2818.37it/s]137053it [00:52, 3529.51it/s]140851it [00:52, 3468.66it/s]119115it [00:51, 2415.47it/s]120262it [00:51, 3044.60it/s]137408it [00:52, 3424.49it/s]141219it [00:52, 3526.89it/s]119469it [00:51, 2664.99it/s]120610it [00:52, 3079.01it/s]137783it [00:52, 3516.34it/s]141574it [00:52, 3372.42it/s]119819it [00:51, 2793.88it/s]120979it [00:52, 3242.50it/s]138136it [00:52, 3423.65it/s]141954it [00:52, 3493.87it/s]120195it [00:51, 3035.97it/s]121327it [00:52, 3221.35it/s]138513it [00:52, 3523.00it/s]142306it [00:52, 3406.81it/s]120546it [00:52, 2959.54it/s]121701it [00:52, 3364.64it/s]138887it [00:52, 3424.25it/s]142683it [00:52, 3508.95it/s]120922it [00:52, 3166.64it/s]122071it [00:52, 3458.96it/s]139267it [00:52, 3529.24it/s]143062it [00:52, 3589.03it/s]121265it [00:52, 3165.40it/s]122427it [00:52, 3340.22it/s]139637it [00:53, 3577.83it/s]143423it [00:52, 3477.18it/s]121642it [00:52, 3329.89it/s]122801it [00:52, 3451.43it/s]139997it [00:53, 3469.40it/s]143802it [00:53, 3564.93it/s]122016it [00:52, 3444.65it/s]123152it [00:52, 3364.61it/s]140374it [00:53, 3554.66it/s]144160it [00:53, 3461.63it/s]122372it [00:52, 3374.18it/s]123526it [00:52, 3469.31it/s]140731it [00:53, 3452.82it/s]144543it [00:53, 3565.19it/s]122744it [00:52, 3471.58it/s]123877it [00:52, 3370.37it/s]141111it [00:53, 3550.79it/s]144901it [00:53, 3440.83it/s]123098it [00:52, 3381.82it/s]124251it [00:53, 3475.00it/s]141468it [00:53, 3445.48it/s]145280it [00:53, 3539.54it/s]123465it [00:52, 3462.69it/s]124605it [00:53, 3373.71it/s]141844it [00:53, 3534.19it/s]145636it [00:53, 3451.60it/s]123815it [00:52, 3372.18it/s]124979it [00:53, 3477.88it/s]142212it [00:53, 3576.46it/s]146018it [00:53, 3556.41it/s]124177it [00:53, 3442.93it/s]125353it [00:53, 3553.53it/s]142571it [00:53, 3461.41it/s]146392it [00:53, 3608.23it/s]124552it [00:53, 3531.55it/s]125711it [00:53, 3434.33it/s]142947it [00:54, 3545.56it/s]146754it [00:53, 3488.08it/s]124908it [00:53, 3400.26it/s]126085it [00:53, 3522.01it/s]143303it [00:54, 3424.79it/s]147128it [00:53, 3558.40it/s]125286it [00:53, 3506.89it/s]126439it [00:53, 3386.85it/s]143682it [00:54, 3527.41it/s]147486it [00:54, 3461.39it/s]125639it [00:53, 3369.64it/s]126813it [00:53, 3486.12it/s]144037it [00:54, 3430.79it/s]147862it [00:54, 3545.30it/s]126016it [00:53, 3482.78it/s]127164it [00:53, 3385.21it/s]144408it [00:54, 3510.11it/s]148218it [00:54, 3440.81it/s]126367it [00:53, 3358.45it/s]127541it [00:54, 3493.85it/s]144767it [00:54, 3425.64it/s]148598it [00:54, 3542.93it/s]126743it [00:53, 3472.34it/s]127893it [00:54, 3385.15it/s]145147it [00:54, 3532.50it/s]148967it [00:54, 3421.09it/s]127111it [00:53, 3530.76it/s]128234it [00:54, 3314.76it/s]145528it [00:54, 3613.09it/s]149346it [00:54, 3524.13it/s]127466it [00:54, 3432.26it/s]128609it [00:54, 3437.50it/s]145891it [00:54, 3498.18it/s]149728it [00:54, 3608.16it/s]127832it [00:54, 3495.88it/s]128955it [00:54, 3355.00it/s]146271it [00:54, 3584.25it/s]150091it [00:54, 3473.17it/s]128183it [00:54, 3394.95it/s]129330it [00:54, 3465.85it/s]146631it [00:55, 3465.99it/s]150470it [00:54, 3563.00it/s]128552it [00:54, 3478.81it/s]129678it [00:54, 3360.32it/s]147002it [00:55, 3533.89it/s]150829it [00:55, 3459.05it/s]128902it [00:54, 3388.59it/s]130052it [00:54, 3466.75it/s]147357it [00:55, 3423.94it/s]151205it [00:55, 3545.05it/s]129269it [00:54, 3467.67it/s]130423it [00:54, 3536.98it/s]147737it [00:55, 3531.32it/s]151562it [00:55, 3436.90it/s]129644it [00:54, 3355.26it/s]130778it [00:54, 3408.40it/s]148114it [00:55, 3598.58it/s]151939it [00:55, 3532.23it/s]130020it [00:54, 3468.18it/s]131134it [00:55, 3449.90it/s]148476it [00:55, 3479.15it/s]152317it [00:55, 3603.82it/s]130398it [00:54, 3556.64it/s]131481it [00:55, 3350.71it/s]148852it [00:55, 3557.57it/s]130756it [00:54, 3401.36it/s]131853it [00:55, 3454.86it/s]149210it [00:55, 3453.38it/s]131103it [00:55, 3418.96it/s]132200it [00:55, 3354.90it/s]149581it [00:55, 3524.51it/s]131447it [00:55, 3334.08it/s]132576it [00:55, 3470.64it/s]149935it [00:56, 3411.31it/s]131818it [00:55, 3440.85it/s]132935it [00:55, 3496.39it/s]150300it [00:56, 3479.63it/s]132164it [00:55, 3356.12it/s]133286it [00:55, 3383.92it/s]150650it [00:56, 3367.68it/s]132527it [00:55, 3433.79it/s]133657it [00:55, 3476.90it/s]151028it [00:56, 3484.00it/s]132902it [00:55, 3522.80it/s]134007it [00:55, 3374.37it/s]151406it [00:56, 3567.37it/s]133256it [00:55, 3295.23it/s]134364it [00:56, 3428.47it/s]151765it [00:56, 3443.92it/s]133634it [00:55, 3422.62it/s]134709it [00:56, 3340.26it/s]152133it [00:56, 3510.40it/s]133980it [00:55, 3319.13it/s]135082it [00:56, 3450.59it/s]134340it [00:56, 3397.70it/s]135455it [00:56, 3530.10it/s]134684it [00:56, 3322.44it/s]135810it [00:56, 3372.86it/s]135048it [00:56, 3412.85it/s]136184it [00:56, 3476.04it/s]135426it [00:56, 3517.58it/s]136534it [00:56, 3372.28it/s]135780it [00:56, 3388.79it/s]136908it [00:56, 3477.16it/s]136156it [00:56, 3493.70it/s]137258it [00:56, 3351.17it/s]136508it [00:56, 3365.91it/s]137631it [00:56, 3456.64it/s]136885it [00:56, 3479.17it/s]138005it [00:57, 3536.73it/s]137235it [00:56, 3360.55it/s]138361it [00:57, 3422.89it/s]137614it [00:56, 3480.55it/s]138722it [00:57, 3475.68it/s]137977it [00:57, 3521.70it/s]139071it [00:57, 3376.37it/s]138331it [00:57, 3414.10it/s]139445it [00:57, 3473.92it/s]138695it [00:57, 3478.79it/s]139794it [00:57, 3358.28it/s]139045it [00:57, 3387.88it/s]140165it [00:57, 3456.76it/s]139409it [00:57, 3460.25it/s]140537it [00:57, 3531.63it/s]139757it [00:57, 3362.31it/s]140892it [00:57, 3412.19it/s]140123it [00:57, 3446.52it/s]141256it [00:58, 3476.04it/s]140499it [00:57, 3535.62it/s]141605it [00:58, 3376.64it/s]140854it [00:57, 3393.62it/s]141977it [00:58, 3474.04it/s]141232it [00:58, 3503.68it/s]142326it [00:58, 3373.33it/s]141585it [00:58, 3367.76it/s]142689it [00:58, 3446.12it/s]141964it [00:58, 3479.62it/s]143064it [00:58, 3531.89it/s]142314it [00:58, 3362.90it/s]143419it [00:58, 3414.90it/s]142689it [00:58, 3472.47it/s]143792it [00:58, 3503.19it/s]143054it [00:58, 3522.00it/s]144144it [00:58, 3367.19it/s]143408it [00:58, 3412.30it/s]144517it [00:58, 3470.71it/s]143771it [00:58, 3472.78it/s]144866it [00:59, 3379.96it/s]144120it [00:58, 3381.30it/s]145242it [00:59, 3488.10it/s]144484it [00:58, 3447.98it/s]145605it [00:59, 3363.58it/s]144830it [00:59, 3378.42it/s]145983it [00:59, 3480.89it/s]152679it [00:59, 269.49it/s] 145188it [00:59, 3434.52it/s]146355it [00:59, 3549.57it/s]153061it [00:59, 377.16it/s]145567it [00:59, 3536.61it/s]146712it [00:59, 3429.32it/s]153375it [00:59, 491.84it/s]145922it [00:59, 3439.47it/s]147074it [00:59, 3483.81it/s]153755it [01:00, 677.06it/s]146288it [00:59, 3502.92it/s]147424it [00:59, 3390.34it/s]154079it [01:00, 865.11it/s]146640it [00:59, 3401.69it/s]147798it [00:59, 3489.36it/s]154459it [01:00, 1145.23it/s]147004it [00:59, 3466.38it/s]148149it [01:00, 3381.62it/s]154839it [01:00, 1463.77it/s]147352it [00:59, 3386.39it/s]148510it [01:00, 3444.57it/s]155190it [01:00, 1738.93it/s]147715it [00:59, 3454.58it/s]148881it [01:00, 3520.49it/s]155560it [01:00, 2074.05it/s]148089it [01:00, 3537.19it/s]149235it [01:00, 3407.65it/s]155911it [01:00, 2308.79it/s]148444it [01:00, 3397.86it/s]149610it [01:00, 3503.85it/s]156275it [01:00, 2595.24it/s]148821it [01:00, 3503.42it/s]149962it [01:00, 3349.05it/s]156623it [01:00, 2744.65it/s]149173it [01:00, 3372.42it/s]150336it [01:00, 3459.77it/s]152486it [01:01, 254.89it/s] 157003it [01:01, 3004.85it/s]149547it [01:00, 3475.11it/s]152868it [01:01, 359.14it/s]150685it [01:00, 3363.61it/s]157378it [01:01, 3198.57it/s]149897it [01:00, 3342.56it/s]151057it [01:00, 3441.57it/s]153236it [01:01, 489.63it/s]157737it [01:01, 3208.47it/s]150276it [01:00, 3468.19it/s]151427it [01:00, 3515.02it/s]153615it [01:01, 667.53it/s]158112it [01:01, 3356.00it/s]150631it [01:00, 3491.40it/s]153995it [01:01, 891.47it/s]151780it [01:01, 3405.63it/s]158468it [01:01, 3313.90it/s]150982it [01:00, 3391.76it/s]152155it [01:01, 3502.96it/s]154336it [01:01, 1113.89it/s]158847it [01:01, 3444.92it/s]151343it [01:00, 3453.89it/s]154712it [01:01, 1420.53it/s]159203it [01:01, 3386.57it/s]151690it [01:01, 3372.66it/s]155057it [01:01, 1685.99it/s]159580it [01:01, 3493.74it/s]152053it [01:01, 3446.43it/s]155431it [01:01, 2028.31it/s]159957it [01:01, 3383.25it/s]155778it [01:02, 2265.90it/s]160335it [01:01, 3493.57it/s]156153it [01:02, 2580.33it/s]160718it [01:02, 3587.86it/s]156516it [01:02, 2823.86it/s]161081it [01:02, 3453.63it/s]156870it [01:02, 2905.73it/s]161455it [01:02, 3534.43it/s]157244it [01:02, 3119.63it/s]161812it [01:02, 3450.25it/s]157595it [01:02, 3144.90it/s]162193it [01:02, 3553.15it/s]157968it [01:02, 3302.22it/s]162551it [01:02, 3446.38it/s]158319it [01:02, 3261.14it/s]162932it [01:02, 3549.00it/s]158696it [01:02, 3398.42it/s]163317it [01:02, 3464.88it/s]159067it [01:03, 3485.82it/s]163702it [01:02, 3573.75it/s]159424it [01:03, 3409.42it/s]164077it [01:03, 3622.52it/s]159804it [01:03, 3520.34it/s]164441it [01:03, 3509.50it/s]160161it [01:03, 3417.83it/s]164830it [01:03, 3617.00it/s]160538it [01:03, 3517.25it/s]165194it [01:03, 3525.73it/s]160893it [01:03, 3395.20it/s]165566it [01:03, 3581.02it/s]161274it [01:03, 3512.32it/s]165926it [01:03, 3498.27it/s]161636it [01:03, 3380.53it/s]166318it [01:03, 3618.36it/s]162017it [01:03, 3501.79it/s]166682it [01:03, 3522.43it/s]162398it [01:03, 3588.74it/s]167070it [01:03, 3623.86it/s]162760it [01:04, 3475.11it/s]167455it [01:03, 3689.20it/s]163142it [01:04, 3571.90it/s]167825it [01:04, 3542.05it/s]163502it [01:04, 3473.27it/s]168217it [01:04, 3640.48it/s]163882it [01:04, 3565.28it/s]168583it [01:04, 3541.40it/s]164241it [01:04, 3447.19it/s]168971it [01:04, 3636.41it/s]164624it [01:04, 3555.53it/s]169337it [01:04, 3515.95it/s]164996it [01:04, 3485.70it/s]169720it [01:04, 3603.81it/s]165373it [01:04, 3565.83it/s]170082it [01:04, 3487.03it/s]165733it [01:04, 3575.05it/s]170443it [01:04, 3521.29it/s]166092it [01:05, 3463.20it/s]170822it [01:04, 3596.66it/s]166459it [01:05, 3522.21it/s]171183it [01:05, 3487.13it/s]166813it [01:05, 3467.29it/s]171559it [01:05, 3564.57it/s]167192it [01:05, 3559.46it/s]171917it [01:05, 3479.59it/s]167549it [01:05, 3479.48it/s]172295it [01:05, 3565.11it/s]167936it [01:05, 3583.66it/s]172653it [01:05, 3493.18it/s]168324it [01:05, 3668.56it/s]173045it [01:05, 3614.75it/s]168692it [01:05, 3552.30it/s]173408it [01:05, 3495.17it/s]169069it [01:05, 3614.41it/s]173792it [01:05, 3594.01it/s]169432it [01:05, 3478.84it/s]174177it [01:05, 3667.43it/s]169814it [01:06, 3574.35it/s]174545it [01:05, 3578.00it/s]170173it [01:06, 3479.10it/s]174925it [01:06, 3639.72it/s]170556it [01:06, 3577.89it/s]175290it [01:06, 3505.23it/s]170916it [01:06, 3486.79it/s]175678it [01:06, 3611.08it/s]171296it [01:06, 3572.88it/s]176041it [01:06, 3500.66it/s]171678it [01:06, 3643.88it/s]176421it [01:06, 3586.29it/s]172044it [01:06, 3524.21it/s]176782it [01:06, 3497.14it/s]152507it [01:06, 226.00it/s] 172426it [01:06, 3607.30it/s]177162it [01:06, 3583.61it/s]152883it [01:06, 318.15it/s]172789it [01:06, 3476.99it/s]177542it [01:06, 3644.45it/s]153235it [01:06, 431.26it/s]173183it [01:07, 3607.19it/s]177908it [01:06, 3545.40it/s]153602it [01:06, 588.42it/s]173546it [01:07, 3491.15it/s]178295it [01:07, 3636.80it/s]153979it [01:06, 794.81it/s]173930it [01:07, 3588.58it/s]178660it [01:07, 3523.69it/s]154313it [01:06, 1004.16it/s]174291it [01:07, 3497.50it/s]179026it [01:07, 3562.33it/s]154688it [01:06, 1299.13it/s]174686it [01:07, 3626.99it/s]179384it [01:07, 3461.07it/s]155029it [01:07, 1551.63it/s]175069it [01:07, 3683.55it/s]179768it [01:07, 3569.61it/s]155401it [01:07, 1892.47it/s]175439it [01:07, 3539.23it/s]180127it [01:07, 3472.60it/s]155755it [01:07, 2139.85it/s]175817it [01:07, 3608.06it/s]180516it [01:07, 3590.63it/s]156125it [01:07, 2457.33it/s]152399it [01:07, 193.50it/s] 176180it [01:07, 3488.60it/s]180907it [01:07, 3681.00it/s]156484it [01:07, 2711.56it/s]152777it [01:07, 275.48it/s]176561it [01:07, 3578.08it/s]181277it [01:07, 3552.90it/s]153153it [01:07, 385.66it/s]156833it [01:07, 2819.36it/s]176921it [01:08, 3492.03it/s]181657it [01:07, 3619.80it/s]157205it [01:07, 3045.34it/s]153465it [01:07, 503.16it/s]177304it [01:08, 3588.85it/s]182021it [01:08, 3527.29it/s]153843it [01:07, 693.47it/s]157553it [01:07, 3076.08it/s]177665it [01:08, 3474.48it/s]182409it [01:08, 3626.37it/s]154173it [01:07, 889.69it/s]157911it [01:07, 3211.17it/s]178055it [01:08, 3595.89it/s]154548it [01:07, 1170.67it/s]182773it [01:08, 3533.56it/s]158275it [01:07, 3189.98it/s]178437it [01:08, 3493.97it/s]183151it [01:08, 3602.22it/s]154910it [01:07, 1471.80it/s]158647it [01:08, 3334.87it/s]178815it [01:08, 3574.58it/s]155257it [01:07, 1745.91it/s]183513it [01:08, 3509.19it/s]159023it [01:08, 3453.90it/s]179197it [01:08, 3644.01it/s]155628it [01:07, 2087.12it/s]183897it [01:08, 3599.51it/s]159378it [01:08, 3343.85it/s]179563it [01:08, 3524.22it/s]184285it [01:08, 3680.43it/s]155976it [01:08, 2316.29it/s]159751it [01:08, 3452.63it/s]179941it [01:08, 3597.25it/s]156340it [01:08, 2604.15it/s]184655it [01:08, 3574.51it/s]160102it [01:08, 3354.30it/s]180303it [01:09, 3525.38it/s]185043it [01:08, 3662.38it/s]156686it [01:08, 2738.30it/s]160476it [01:08, 3463.47it/s]180677it [01:09, 3583.90it/s]157061it [01:08, 2988.56it/s]185411it [01:09, 3524.36it/s]160826it [01:08, 3345.27it/s]181037it [01:09, 3492.42it/s]185796it [01:09, 3616.68it/s]157434it [01:08, 3049.66it/s]161202it [01:08, 3460.52it/s]181417it [01:09, 3581.12it/s]186160it [01:09, 3541.77it/s]157796it [01:08, 3198.73it/s]161569it [01:08, 3520.33it/s]186550it [01:09, 3644.02it/s]181797it [01:09, 3385.75it/s]158167it [01:08, 3338.48it/s]161924it [01:09, 3404.10it/s]182191it [01:09, 3538.99it/s]186916it [01:09, 3555.92it/s]158521it [01:08, 3275.52it/s]162283it [01:09, 3455.22it/s]182577it [01:09, 3629.29it/s]187303it [01:09, 3645.94it/s]158901it [01:08, 3420.19it/s]162631it [01:09, 3371.35it/s]182943it [01:09, 3533.76it/s]187677it [01:09, 3552.78it/s]159254it [01:09, 3320.03it/s]163004it [01:09, 3473.62it/s]183329it [01:09, 3625.93it/s]188074it [01:09, 3671.70it/s]159632it [01:09, 3447.94it/s]163353it [01:09, 3382.26it/s]188461it [01:09, 3726.93it/s]183694it [01:10, 3513.36it/s]159984it [01:09, 3355.71it/s]163719it [01:09, 3460.78it/s]184079it [01:10, 3609.47it/s]188835it [01:09, 3605.78it/s]160359it [01:09, 3465.94it/s]164092it [01:09, 3536.64it/s]189217it [01:10, 3664.90it/s]184442it [01:10, 3523.06it/s]160724it [01:09, 3513.73it/s]164447it [01:09, 3424.44it/s]184828it [01:10, 3619.10it/s]189585it [01:10, 3538.85it/s]161079it [01:09, 3407.90it/s]164827it [01:09, 3532.55it/s]185192it [01:10, 3522.58it/s]189979it [01:10, 3651.87it/s]161455it [01:09, 3507.66it/s]165182it [01:09, 3409.22it/s]185574it [01:10, 3606.89it/s]190346it [01:10, 3560.05it/s]161808it [01:09, 3411.37it/s]165559it [01:10, 3512.57it/s]185956it [01:10, 3667.50it/s]190724it [01:10, 3621.75it/s]162176it [01:09, 3486.87it/s]165912it [01:10, 3428.07it/s]186324it [01:10, 3565.69it/s]191088it [01:10, 3508.97it/s]162527it [01:09, 3396.91it/s]166281it [01:10, 3502.64it/s]186728it [01:10, 3701.46it/s]191462it [01:10, 3573.74it/s]162901it [01:10, 3495.12it/s]166667it [01:10, 3605.55it/s]187100it [01:10, 3595.40it/s]191848it [01:10, 3656.04it/s]163281it [01:10, 3581.69it/s]167029it [01:10, 3482.07it/s]187484it [01:11, 3664.43it/s]192215it [01:10, 3549.32it/s]163641it [01:10, 3431.57it/s]167406it [01:10, 3563.56it/s]187852it [01:11, 3573.14it/s]192597it [01:10, 3627.03it/s]164019it [01:10, 3529.51it/s]167764it [01:10, 3459.04it/s]188242it [01:11, 3665.33it/s]192961it [01:11, 3534.53it/s]164374it [01:10, 3436.96it/s]168144it [01:10, 3552.69it/s]188610it [01:11, 3570.09it/s]193344it [01:11, 3618.72it/s]164754it [01:10, 3540.26it/s]168501it [01:10, 3416.31it/s]188969it [01:11, 3562.98it/s]193708it [01:11, 3494.94it/s]165110it [01:10, 3458.84it/s]168879it [01:11, 3518.79it/s]194099it [01:11, 3613.80it/s]189357it [01:11, 3484.47it/s]165490it [01:10, 3556.89it/s]169233it [01:11, 3414.64it/s]189742it [01:11, 3587.31it/s]194462it [01:11, 3509.90it/s]165848it [01:10, 3469.35it/s]169614it [01:11, 3525.78it/s]190133it [01:11, 3679.85it/s]194845it [01:11, 3599.26it/s]166240it [01:11, 3597.03it/s]169988it [01:11, 3586.30it/s]190503it [01:11, 3563.23it/s]166624it [01:11, 3667.34it/s]195237it [01:11, 3555.82it/s]170348it [01:11, 3453.66it/s]190889it [01:11, 3647.81it/s]195623it [01:11, 3642.60it/s]166992it [01:11, 3540.79it/s]170727it [01:11, 3548.51it/s]196000it [01:11, 3677.03it/s]191256it [01:12, 3496.67it/s]167375it [01:11, 3624.04it/s]171084it [01:11, 3438.26it/s]191629it [01:12, 3560.90it/s]196369it [01:12, 3529.97it/s]167739it [01:11, 3475.84it/s]171448it [01:11, 3495.32it/s]196762it [01:12, 3643.84it/s]191987it [01:12, 3473.63it/s]168121it [01:11, 3573.51it/s]171799it [01:11, 3388.04it/s]192364it [01:12, 3556.68it/s]197129it [01:12, 3537.50it/s]168481it [01:11, 3438.03it/s]172185it [01:11, 3521.17it/s]197524it [01:12, 3655.10it/s]192722it [01:12, 3436.40it/s]168859it [01:11, 3534.87it/s]172555it [01:12, 3434.94it/s]193101it [01:12, 3536.62it/s]197892it [01:12, 3576.99it/s]169215it [01:11, 3408.22it/s]172928it [01:12, 3518.28it/s]193479it [01:12, 3606.85it/s]198278it [01:12, 3656.27it/s]169596it [01:11, 3521.16it/s]173310it [01:12, 3603.90it/s]193842it [01:12, 3496.73it/s]198645it [01:12, 3551.26it/s]169973it [01:12, 3592.09it/s]173672it [01:12, 3485.56it/s]194233it [01:12, 3615.30it/s]170334it [01:12, 3491.15it/s]174047it [01:12, 3561.11it/s]194597it [01:13, 3530.85it/s]170701it [01:12, 3542.26it/s]174405it [01:12, 3434.99it/s]194978it [01:13, 3610.53it/s]171057it [01:12, 3441.79it/s]174790it [01:12, 3552.91it/s]195341it [01:13, 3546.27it/s]171437it [01:12, 3543.73it/s]175148it [01:12, 3447.81it/s]195718it [01:13, 3610.17it/s]171793it [01:12, 3439.55it/s]175525it [01:12, 3538.81it/s]196080it [01:13, 3471.91it/s]172167it [01:12, 3523.40it/s]175897it [01:13, 3588.62it/s]196468it [01:13, 3587.61it/s]172554it [01:12, 3624.10it/s]176258it [01:13, 3457.26it/s]196869it [01:13, 3708.05it/s]172918it [01:12, 3511.26it/s]176632it [01:13, 3536.29it/s]197242it [01:13, 3583.84it/s]173300it [01:13, 3599.59it/s]176988it [01:13, 3418.09it/s]197629it [01:13, 3665.24it/s]173662it [01:13, 3451.69it/s]177354it [01:13, 3486.43it/s]197998it [01:13, 3582.88it/s]174042it [01:13, 3550.07it/s]177705it [01:13, 3382.67it/s]198390it [01:14, 3677.69it/s]174399it [01:13, 3465.19it/s]178088it [01:13, 3508.61it/s]174791it [01:13, 3593.30it/s]178441it [01:13, 3409.69it/s]175152it [01:13, 3441.51it/s]178812it [01:13, 3493.96it/s]175529it [01:13, 3533.78it/s]179188it [01:13, 3571.07it/s]175915it [01:13, 3451.14it/s]179547it [01:14, 3457.05it/s]176278it [01:13, 3501.04it/s]179920it [01:14, 3534.87it/s]176659it [01:13, 3587.82it/s]180275it [01:14, 3428.24it/s]177020it [01:14, 3475.91it/s]180649it [01:14, 3516.26it/s]177398it [01:14, 3562.34it/s]181002it [01:14, 3438.56it/s]181375it [01:14, 3517.22it/s]177756it [01:14, 3415.33it/s]181738it [01:14, 3549.43it/s]178142it [01:14, 3540.79it/s]182094it [01:14, 3466.57it/s]178499it [01:14, 3443.61it/s]182475it [01:14, 3566.20it/s]178882it [01:14, 3553.61it/s]179245it [01:14, 3573.35it/s]182833it [01:15, 3479.77it/s]183194it [01:15, 3516.55it/s]179604it [01:14, 3471.61it/s]179976it [01:14, 3541.15it/s]183547it [01:15, 3409.71it/s]183928it [01:15, 3522.59it/s]180332it [01:15, 3475.65it/s]184302it [01:15, 3584.02it/s]180693it [01:15, 3513.34it/s]184662it [01:15, 3482.99it/s]181046it [01:15, 3411.67it/s]185045it [01:15, 3581.37it/s]181426it [01:15, 3521.42it/s]185405it [01:15, 3454.41it/s]181795it [01:15, 3438.31it/s]185778it [01:15, 3532.92it/s]182178it [01:15, 3550.05it/s]182563it [01:15, 3636.86it/s]186133it [01:15, 3462.07it/s]186516it [01:16, 3566.82it/s]182928it [01:15, 3524.23it/s]186874it [01:16, 3522.38it/s]183308it [01:15, 3603.66it/s]187245it [01:16, 3575.44it/s]183670it [01:15, 3471.51it/s]187627it [01:16, 3646.67it/s]184052it [01:16, 3570.01it/s]187993it [01:16, 3538.33it/s]184411it [01:16, 3487.49it/s]188375it [01:16, 3614.70it/s]184795it [01:16, 3586.76it/s]188738it [01:16, 3468.05it/s]185156it [01:16, 3443.85it/s]189110it [01:16, 3538.52it/s]185532it [01:16, 3533.84it/s]189466it [01:16, 3444.54it/s]185922it [01:16, 3637.49it/s]189846it [01:17, 3545.44it/s]186288it [01:16, 3532.16it/s]190202it [01:17, 3452.78it/s]186673it [01:16, 3621.95it/s]190578it [01:17, 3540.82it/s]187037it [01:16, 3557.02it/s]190958it [01:17, 3614.70it/s]187416it [01:17, 3622.83it/s]191321it [01:17, 3466.75it/s]187780it [01:17, 3502.13it/s]191682it [01:17, 3507.61it/s]188172it [01:17, 3622.22it/s]192035it [01:17, 3438.62it/s]188536it [01:17, 3529.48it/s]192411it [01:17, 3530.99it/s]188917it [01:17, 3610.08it/s]199002it [01:18, 218.70it/s] 192766it [01:17, 3450.24it/s]189287it [01:17, 3634.81it/s]199397it [01:18, 310.72it/s]193128it [01:17, 3498.47it/s]189652it [01:17, 3506.27it/s]199730it [01:18, 413.44it/s]193499it [01:18, 3558.55it/s]190043it [01:17, 3622.22it/s]200115it [01:18, 572.71it/s]193856it [01:18, 3454.70it/s]190407it [01:17, 3516.49it/s]200445it [01:18, 740.76it/s]194243it [01:18, 3574.60it/s]190785it [01:17, 3590.04it/s]200830it [01:18, 993.71it/s]194602it [01:18, 3450.95it/s]191146it [01:18, 3467.82it/s]201185it [01:18, 1246.25it/s]194978it [01:18, 3538.29it/s]191522it [01:18, 3551.13it/s]201577it [01:18, 1588.69it/s]195334it [01:18, 3483.62it/s]201963it [01:18, 1937.60it/s]191879it [01:18, 3457.63it/s]195718it [01:18, 3585.01it/s]202326it [01:19, 2219.35it/s]192255it [01:18, 3542.89it/s]202734it [01:19, 2597.33it/s]196078it [01:18, 3398.46it/s]192644it [01:18, 3642.05it/s]196457it [01:18, 3508.29it/s]203106it [01:19, 2803.61it/s]193010it [01:18, 3507.87it/s]198760it [01:19, 226.51it/s] 196847it [01:18, 3620.99it/s]203495it [01:19, 3058.12it/s]193388it [01:18, 3583.90it/s]199152it [01:19, 318.83it/s]197212it [01:19, 3495.15it/s]203866it [01:19, 3127.79it/s]193748it [01:18, 3447.95it/s]199505it [01:19, 428.79it/s]197595it [01:19, 3589.74it/s]204255it [01:19, 3326.90it/s]194131it [01:18, 3556.64it/s]199891it [01:19, 589.58it/s]197956it [01:19, 3446.71it/s]204623it [01:19, 3313.40it/s]194489it [01:19, 3465.14it/s]200271it [01:19, 790.97it/s]198335it [01:19, 3542.68it/s]205008it [01:19, 3458.73it/s]194871it [01:19, 3564.77it/s]200619it [01:19, 1004.08it/s]205385it [01:19, 3421.79it/s]195235it [01:19, 3475.10it/s]200994it [01:20, 1290.55it/s]205773it [01:19, 3547.43it/s]195622it [01:19, 3587.56it/s]201343it [01:20, 1557.77it/s]206155it [01:20, 3616.53it/s]196002it [01:19, 3648.56it/s]201729it [01:20, 1913.96it/s]206525it [01:20, 3494.16it/s]196369it [01:19, 3516.46it/s]202083it [01:20, 2162.38it/s]206919it [01:20, 3619.08it/s]196756it [01:19, 3616.03it/s]202485it [01:20, 2537.71it/s]207286it [01:20, 3541.39it/s]197120it [01:19, 3519.06it/s]202865it [01:20, 2758.77it/s]207679it [01:20, 3651.25it/s]197511it [01:19, 3630.99it/s]203264it [01:20, 3050.58it/s]208048it [01:20, 3565.15it/s]197876it [01:19, 3543.71it/s]203647it [01:20, 3247.55it/s]208447it [01:20, 3686.93it/s]198254it [01:20, 3609.44it/s]204020it [01:20, 3261.58it/s]208818it [01:20, 3578.42it/s]198617it [01:20, 3510.26it/s]204405it [01:20, 3417.23it/s]209214it [01:20, 3685.92it/s]204773it [01:21, 3384.49it/s]209585it [01:21, 3545.02it/s]205152it [01:21, 3495.27it/s]209958it [01:21, 3595.35it/s]205515it [01:21, 3434.35it/s]210341it [01:21, 3661.02it/s]205903it [01:21, 3558.13it/s]210709it [01:21, 3511.46it/s]206267it [01:21, 3491.79it/s]211090it [01:21, 3594.30it/s]206657it [01:21, 3607.80it/s]211452it [01:21, 3474.07it/s]207044it [01:21, 3683.56it/s]211828it [01:21, 3554.63it/s]207416it [01:21, 3562.75it/s]212186it [01:21, 3445.84it/s]207805it [01:21, 3655.96it/s]212568it [01:21, 3551.12it/s]208174it [01:22, 3512.00it/s]212926it [01:21, 3557.37it/s]208545it [01:22, 3567.10it/s]213283it [01:22, 3437.24it/s]208904it [01:22, 3489.92it/s]213645it [01:22, 3488.11it/s]209297it [01:22, 3615.92it/s]213996it [01:22, 3418.38it/s]209661it [01:22, 3483.62it/s]214373it [01:22, 3519.11it/s]210038it [01:22, 3563.00it/s]214726it [01:22, 3417.17it/s]210419it [01:22, 3633.02it/s]215111it [01:22, 3540.44it/s]210784it [01:22, 3490.77it/s]215467it [01:22, 3448.33it/s]211162it [01:22, 3572.43it/s]215848it [01:22, 3551.70it/s]211522it [01:22, 3476.04it/s]216233it [01:22, 3638.19it/s]211889it [01:23, 3529.62it/s]216598it [01:23, 3523.92it/s]212244it [01:23, 3431.02it/s]216992it [01:23, 3643.65it/s]212623it [01:23, 3533.40it/s]217358it [01:23, 3505.04it/s]212978it [01:23, 3438.02it/s]217760it [01:23, 3650.47it/s]213353it [01:23, 3526.70it/s]218128it [01:23, 3548.48it/s]213725it [01:23, 3581.47it/s]218519it [01:23, 3649.59it/s]214085it [01:23, 3471.33it/s]218886it [01:23, 3557.98it/s]214458it [01:23, 3543.61it/s]219287it [01:23, 3686.14it/s]214814it [01:23, 3413.89it/s]219666it [01:23, 3573.46it/s]215193it [01:24, 3519.09it/s]220072it [01:23, 3712.25it/s]215547it [01:24, 3437.52it/s]220482it [01:24, 3822.40it/s]215930it [01:24, 3550.16it/s]220866it [01:24, 3690.22it/s]216306it [01:24, 3468.61it/s]221254it [01:24, 3743.06it/s]216696it [01:24, 3590.95it/s]221630it [01:24, 3668.94it/s]217087it [01:24, 3682.30it/s]222026it [01:24, 3750.27it/s]217457it [01:24, 3556.87it/s]222403it [01:24, 3602.86it/s]217846it [01:24, 3644.58it/s]222797it [01:24, 3698.77it/s]218213it [01:24, 3575.38it/s]223169it [01:24, 3570.55it/s]218606it [01:24, 3677.10it/s]223554it [01:24, 3648.03it/s]218975it [01:25, 3562.65it/s]223921it [01:25, 3533.37it/s]219380it [01:25, 3701.14it/s]224305it [01:25, 3619.86it/s]219752it [01:25, 3600.87it/s]224689it [01:25, 3682.96it/s]220155it [01:25, 3722.72it/s]225059it [01:25, 3519.54it/s]220529it [01:25, 3641.16it/s]225439it [01:25, 3597.82it/s]220908it [01:25, 3683.61it/s]225801it [01:25, 3477.99it/s]221298it [01:25, 3745.88it/s]226175it [01:25, 3550.23it/s]221674it [01:25, 3661.52it/s]222060it [01:25, 3718.59it/s]226532it [01:25, 3433.77it/s]226901it [01:25, 3506.20it/s]222433it [01:26, 3562.03it/s]227254it [01:25, 3404.76it/s]222821it [01:26, 3651.30it/s]198692it [01:25, 188.48it/s] 227629it [01:26, 3503.38it/s]223188it [01:26, 3535.25it/s]199065it [01:25, 264.38it/s]228002it [01:26, 3568.03it/s]223571it [01:26, 3619.36it/s]199450it [01:25, 370.88it/s]228361it [01:26, 3461.63it/s]223935it [01:26, 3521.43it/s]199769it [01:25, 485.64it/s]228725it [01:26, 3510.46it/s]224301it [01:26, 3560.73it/s]200142it [01:26, 663.95it/s]229078it [01:26, 3412.64it/s]224683it [01:26, 3636.03it/s]200476it [01:26, 854.22it/s]229467it [01:26, 3549.63it/s]225048it [01:26, 3518.71it/s]200847it [01:26, 1121.91it/s]229824it [01:26, 3465.77it/s]225426it [01:26, 3592.97it/s]201188it [01:26, 1370.41it/s]230202it [01:26, 3553.99it/s]225787it [01:26, 3490.94it/s]201568it [01:26, 1716.36it/s]230578it [01:26, 3613.36it/s]226164it [01:27, 3568.84it/s]201954it [01:26, 2076.54it/s]230941it [01:27, 3500.47it/s]226523it [01:27, 3455.47it/s]202310it [01:26, 2331.96it/s]231332it [01:27, 3617.88it/s]226892it [01:27, 3520.30it/s]202708it [01:26, 2685.76it/s]231696it [01:27, 3505.37it/s]198970it [01:26, 182.83it/s] 227246it [01:27, 3384.68it/s]203072it [01:26, 2854.40it/s]232071it [01:27, 3574.41it/s]199363it [01:26, 261.30it/s]227618it [01:27, 3478.41it/s]203447it [01:27, 3075.55it/s]199697it [01:26, 350.74it/s]232430it [01:27, 3443.12it/s]227994it [01:27, 3557.72it/s]203809it [01:27, 3095.31it/s]200071it [01:26, 485.78it/s]232808it [01:27, 3537.44it/s]228352it [01:27, 3445.12it/s]204189it [01:27, 3279.31it/s]200396it [01:27, 632.38it/s]233164it [01:27, 3451.48it/s]228725it [01:27, 3526.16it/s]204546it [01:27, 3249.80it/s]200773it [01:27, 855.67it/s]233542it [01:27, 3545.23it/s]229080it [01:27, 3425.22it/s]204927it [01:27, 3403.44it/s]201113it [01:27, 1090.37it/s]233923it [01:27, 3620.82it/s]229466it [01:28, 3549.32it/s]205314it [01:27, 3526.91it/s]201450it [01:27, 1333.64it/s]234287it [01:27, 3515.93it/s]229823it [01:28, 3466.70it/s]205679it [01:27, 3441.66it/s]201775it [01:27, 1600.39it/s]234650it [01:28, 3548.74it/s]230184it [01:28, 3505.83it/s]206057it [01:27, 3535.29it/s]202100it [01:27, 1875.97it/s]235006it [01:28, 3429.89it/s]230553it [01:28, 3557.54it/s]206417it [01:27, 3461.13it/s]202502it [01:27, 2288.79it/s]235385it [01:28, 3532.77it/s]230910it [01:28, 3455.27it/s]206800it [01:27, 3565.39it/s]202863it [01:27, 2537.57it/s]235740it [01:28, 3401.63it/s]231297it [01:28, 3574.14it/s]207161it [01:28, 3481.71it/s]203216it [01:27, 2768.36it/s]236111it [01:28, 3488.55it/s]231656it [01:28, 3466.06it/s]207544it [01:28, 3580.77it/s]203599it [01:27, 3032.94it/s]232034it [01:28, 3554.94it/s]236466it [01:28, 3374.51it/s]207905it [01:28, 3485.13it/s]203957it [01:28, 3046.38it/s]236845it [01:28, 3491.55it/s]232391it [01:28, 3458.00it/s]208288it [01:28, 3583.60it/s]204332it [01:28, 3232.50it/s]237220it [01:28, 3566.10it/s]232765it [01:28, 3536.55it/s]208674it [01:28, 3655.42it/s]204685it [01:28, 3191.07it/s]237579it [01:28, 3451.72it/s]233120it [01:29, 3425.69it/s]209042it [01:28, 3545.64it/s]205056it [01:28, 3331.55it/s]237950it [01:29, 3525.60it/s]233499it [01:29, 3530.19it/s]209423it [01:28, 3619.22it/s]205405it [01:28, 3269.81it/s]233878it [01:29, 3605.67it/s]238305it [01:29, 3417.91it/s]209787it [01:28, 3470.99it/s]205778it [01:28, 3396.90it/s]238682it [01:29, 3518.19it/s]234240it [01:29, 3496.65it/s]210159it [01:28, 3540.04it/s]206154it [01:28, 3496.07it/s]234606it [01:29, 3542.44it/s]239036it [01:29, 3407.76it/s]210515it [01:29, 3393.17it/s]206511it [01:28, 3391.91it/s]239412it [01:29, 3506.75it/s]234962it [01:29, 3439.09it/s]210885it [01:29, 3479.84it/s]206891it [01:28, 3506.02it/s]239792it [01:29, 3590.92it/s]235339it [01:29, 3533.26it/s]211264it [01:29, 3376.78it/s]207246it [01:29, 3397.24it/s]240153it [01:29, 3444.66it/s]235694it [01:29, 3409.10it/s]211637it [01:29, 3474.48it/s]207614it [01:29, 3477.48it/s]240526it [01:29, 3522.47it/s]236070it [01:29, 3508.99it/s]212013it [01:29, 3554.10it/s]207965it [01:29, 3370.54it/s]236425it [01:30, 3519.63it/s]240880it [01:29, 3413.78it/s]212371it [01:29, 3429.84it/s]208347it [01:29, 3496.38it/s]241261it [01:29, 3526.21it/s]236779it [01:30, 3415.52it/s]212735it [01:29, 3488.91it/s]208722it [01:29, 3568.38it/s]237152it [01:30, 3504.17it/s]241616it [01:30, 3415.51it/s]213086it [01:29, 3385.19it/s]209081it [01:29, 3343.42it/s]241996it [01:30, 3518.44it/s]237504it [01:30, 3402.22it/s]213457it [01:29, 3476.92it/s]209450it [01:29, 3439.51it/s]237872it [01:30, 3479.70it/s]242350it [01:30, 3414.37it/s]213807it [01:30, 3373.28it/s]242722it [01:30, 3500.80it/s]238222it [01:30, 3378.69it/s]209798it [01:29, 3226.79it/s]214184it [01:30, 3484.31it/s]243094it [01:30, 3563.11it/s]238590it [01:30, 3463.82it/s]210167it [01:29, 3353.38it/s]214555it [01:30, 3549.28it/s]238964it [01:30, 3542.09it/s]243452it [01:30, 3451.95it/s]210507it [01:29, 3237.20it/s]214912it [01:30, 3424.74it/s]243817it [01:30, 3506.39it/s]239320it [01:30, 3398.95it/s]210866it [01:30, 3334.44it/s]215276it [01:30, 3485.64it/s]239694it [01:30, 3496.27it/s]211244it [01:30, 3460.38it/s]244169it [01:30, 3404.41it/s]215626it [01:30, 3402.50it/s]244542it [01:30, 3497.35it/s]240046it [01:31, 3389.69it/s]211593it [01:30, 3314.53it/s]216001it [01:30, 3501.09it/s]240418it [01:31, 3482.73it/s]244894it [01:31, 3402.66it/s]211928it [01:30, 3315.24it/s]216353it [01:30, 3417.93it/s]245272it [01:31, 3508.71it/s]240768it [01:31, 3394.00it/s]212262it [01:30, 3179.71it/s]216730it [01:30, 3517.58it/s]245649it [01:31, 3584.57it/s]241148it [01:31, 3509.15it/s]212628it [01:30, 3312.99it/s]217117it [01:30, 3618.42it/s]246009it [01:31, 3462.98it/s]241506it [01:31, 3391.42it/s]212962it [01:30, 3234.79it/s]217480it [01:31, 3463.32it/s]246385it [01:31, 3547.05it/s]241889it [01:31, 3515.46it/s]213335it [01:30, 3375.34it/s]217876it [01:31, 3603.31it/s]242256it [01:31, 3558.19it/s]246742it [01:31, 3445.16it/s]213691it [01:30, 3428.50it/s]218239it [01:31, 3511.54it/s]247120it [01:31, 3541.37it/s]242614it [01:31, 3446.85it/s]214036it [01:31, 3353.88it/s]218623it [01:31, 3605.76it/s]242980it [01:31, 3507.69it/s]214409it [01:31, 3462.25it/s]247476it [01:31, 3380.70it/s]218986it [01:31, 3508.39it/s]247852it [01:31, 3487.18it/s]243333it [01:32, 3416.27it/s]214757it [01:31, 3353.06it/s]219384it [01:31, 3643.14it/s]248224it [01:31, 3553.24it/s]243713it [01:32, 3525.37it/s]215131it [01:31, 3463.08it/s]219750it [01:31, 3518.23it/s]244067it [01:32, 3431.03it/s]248582it [01:32, 3440.58it/s]215479it [01:31, 3381.36it/s]220147it [01:31, 3647.22it/s]244442it [01:32, 3522.70it/s]248964it [01:32, 3548.29it/s]215860it [01:31, 3504.59it/s]220514it [01:31, 3578.81it/s]244820it [01:32, 3595.65it/s]216238it [01:31, 3583.95it/s]249321it [01:32, 3354.36it/s]220908it [01:31, 3682.86it/s]245181it [01:32, 3477.85it/s]249697it [01:32, 3467.48it/s]216598it [01:31, 3418.33it/s]221296it [01:32, 3738.19it/s]245555it [01:32, 3551.19it/s]216990it [01:31, 3561.52it/s]250047it [01:32, 3421.77it/s]221671it [01:32, 3652.44it/s]245912it [01:32, 3446.49it/s]250436it [01:32, 3554.60it/s]217349it [01:31, 3455.01it/s]222044it [01:32, 3663.38it/s]246289it [01:32, 3538.51it/s]250794it [01:32, 3484.10it/s]217697it [01:32, 3404.33it/s]222412it [01:32, 3519.26it/s]246645it [01:32, 3436.18it/s]251164it [01:32, 3546.13it/s]218039it [01:32, 3364.27it/s]222799it [01:32, 3617.73it/s]247027it [01:33, 3544.40it/s]251541it [01:32, 3609.58it/s]218419it [01:32, 3487.61it/s]223163it [01:32, 3487.80it/s]247386it [01:33, 3443.40it/s]251904it [01:33, 3494.42it/s]218810it [01:32, 3610.05it/s]223538it [01:32, 3561.73it/s]247765it [01:33, 3540.76it/s]252288it [01:33, 3591.90it/s]219173it [01:32, 3510.16it/s]223896it [01:32, 3463.80it/s]248142it [01:33, 3605.64it/s]252649it [01:33, 3484.40it/s]219565it [01:32, 3628.70it/s]224278it [01:32, 3564.35it/s]253032it [01:33, 3581.70it/s]248504it [01:33, 3465.93it/s]219930it [01:32, 3534.89it/s]224639it [01:33, 3576.28it/s]248884it [01:33, 3561.60it/s]220320it [01:32, 3639.70it/s]253392it [01:33, 3483.87it/s]224998it [01:33, 3464.02it/s]253773it [01:33, 3575.85it/s]249242it [01:33, 3450.03it/s]220686it [01:32, 3569.68it/s]225376it [01:33, 3554.88it/s]249614it [01:33, 3527.13it/s]221064it [01:33, 3625.68it/s]254132it [01:33, 3447.07it/s]225733it [01:33, 3450.91it/s]249969it [01:33, 3443.79it/s]254513it [01:33, 3550.63it/s]221428it [01:33, 3544.12it/s]226100it [01:33, 3513.84it/s]250347it [01:34, 3539.30it/s]254875it [01:33, 3569.94it/s]221819it [01:33, 3650.08it/s]226453it [01:33, 3396.15it/s]250737it [01:34, 3643.46it/s]255234it [01:33, 3465.73it/s]222185it [01:33, 3504.85it/s]226809it [01:33, 3441.28it/s]255620it [01:34, 3577.04it/s]251103it [01:34, 3507.26it/s]222573it [01:33, 3611.33it/s]227180it [01:33, 3517.28it/s]251475it [01:34, 3566.13it/s]222942it [01:33, 3633.67it/s]227533it [01:33, 3396.29it/s]251834it [01:34, 3462.99it/s]223307it [01:33, 3500.56it/s]227909it [01:33, 3499.51it/s]252220it [01:34, 3575.16it/s]223682it [01:33, 3569.76it/s]228261it [01:34, 3392.58it/s]252580it [01:34, 3460.75it/s]224041it [01:33, 3475.69it/s]228632it [01:34, 3483.15it/s]252964it [01:34, 3566.81it/s]224413it [01:33, 3545.37it/s]228982it [01:34, 3356.25it/s]253323it [01:34, 3467.56it/s]224769it [01:34, 3437.22it/s]229363it [01:34, 3484.38it/s]253698it [01:34, 3547.28it/s]225135it [01:34, 3500.06it/s]229744it [01:34, 3417.87it/s]254074it [01:35, 3607.01it/s]225516it [01:34, 3589.10it/s]230118it [01:34, 3507.58it/s]254436it [01:35, 3464.73it/s]225877it [01:34, 3421.50it/s]230489it [01:34, 3565.14it/s]254812it [01:35, 3549.22it/s]226249it [01:34, 3504.96it/s]230847it [01:34, 3450.78it/s]255169it [01:35, 3450.48it/s]226602it [01:34, 3367.31it/s]231222it [01:34, 3535.29it/s]255549it [01:35, 3549.47it/s]226972it [01:34, 3459.38it/s]231577it [01:35, 3417.60it/s]227320it [01:34, 3305.27it/s]231952it [01:35, 3510.43it/s]227687it [01:34, 3406.93it/s]232305it [01:35, 3404.59it/s]228051it [01:35, 3471.82it/s]232682it [01:35, 3507.74it/s]228401it [01:35, 3368.83it/s]233057it [01:35, 3575.74it/s]228762it [01:35, 3436.24it/s]233416it [01:35, 3432.15it/s]229108it [01:35, 3347.10it/s]233797it [01:35, 3539.02it/s]229480it [01:35, 3452.46it/s]234153it [01:35, 3435.00it/s]229827it [01:35, 3378.29it/s]234522it [01:35, 3506.56it/s]230184it [01:35, 3431.98it/s]234875it [01:36, 3392.44it/s]230556it [01:35, 3514.65it/s]235252it [01:36, 3497.97it/s]230909it [01:35, 3336.74it/s]235621it [01:36, 3551.83it/s]231294it [01:35, 3481.35it/s]235978it [01:36, 3378.79it/s]231645it [01:36, 3388.78it/s]236352it [01:36, 3479.83it/s]232009it [01:36, 3460.00it/s]236703it [01:36, 3375.72it/s]232357it [01:36, 3364.99it/s]237080it [01:36, 3485.78it/s]232720it [01:36, 3440.31it/s]237431it [01:36, 3381.89it/s]233103it [01:36, 3551.25it/s]237800it [01:36, 3467.98it/s]233460it [01:36, 3384.41it/s]238149it [01:36, 3344.45it/s]233841it [01:36, 3504.82it/s]238520it [01:37, 3448.58it/s]234194it [01:36, 3415.65it/s]238895it [01:37, 3534.99it/s]234566it [01:36, 3502.37it/s]239251it [01:37, 3411.93it/s]234918it [01:37, 3387.00it/s]239624it [01:37, 3501.09it/s]235288it [01:37, 3451.74it/s]239976it [01:37, 3397.86it/s]235635it [01:37, 3350.26it/s]240338it [01:37, 3461.06it/s]235993it [01:37, 3415.20it/s]240686it [01:37, 3367.95it/s]236368it [01:37, 3510.01it/s]241060it [01:37, 3474.44it/s]236721it [01:37, 3352.46it/s]241434it [01:37, 3542.52it/s]237095it [01:37, 3461.59it/s]241790it [01:38, 3443.33it/s]237444it [01:37, 3315.44it/s]242164it [01:38, 3526.60it/s]237813it [01:37, 3419.40it/s]242518it [01:38, 3374.29it/s]238158it [01:38, 3265.97it/s]242890it [01:38, 3472.75it/s]238531it [01:38, 3394.61it/s]243240it [01:38, 3383.54it/s]238891it [01:38, 3451.97it/s]243615it [01:38, 3488.33it/s]239239it [01:38, 3294.21it/s]243993it [01:38, 3572.92it/s]239607it [01:38, 3402.58it/s]244352it [01:38, 3441.80it/s]239950it [01:38, 3323.55it/s]244717it [01:38, 3501.21it/s]240314it [01:38, 3406.59it/s]245069it [01:38, 3340.39it/s]240664it [01:38, 3325.52it/s]245437it [01:39, 3436.57it/s]241032it [01:38, 3424.36it/s]245783it [01:39, 3361.80it/s]241406it [01:38, 3515.27it/s]246158it [01:39, 3471.75it/s]241759it [01:39, 3390.91it/s]246533it [01:39, 3551.86it/s]242134it [01:39, 3492.65it/s]246890it [01:39, 3416.80it/s]242485it [01:39, 3355.35it/s]247262it [01:39, 3502.51it/s]242857it [01:39, 3459.15it/s]247614it [01:39, 3391.89it/s]243205it [01:39, 3343.22it/s]247989it [01:39, 3491.99it/s]243584it [01:39, 3469.47it/s]248340it [01:39, 3386.48it/s]255980it [01:40, 190.71it/s] 243951it [01:39, 3525.19it/s]248714it [01:40, 3484.57it/s]256367it [01:40, 270.53it/s]244306it [01:39, 3397.60it/s]249064it [01:40, 3391.23it/s]256694it [01:40, 360.53it/s]244667it [01:39, 3457.01it/s]249428it [01:40, 3460.86it/s]257084it [01:40, 506.21it/s]245015it [01:40, 3367.15it/s]249801it [01:40, 3538.72it/s]257464it [01:40, 688.86it/s]245379it [01:40, 3444.74it/s]250157it [01:40, 3438.53it/s]257807it [01:40, 885.46it/s]245725it [01:40, 3359.27it/s]250540it [01:40, 3550.40it/s]258192it [01:40, 1165.62it/s]246089it [01:40, 3439.98it/s]250897it [01:40, 3446.28it/s]258543it [01:40, 1431.21it/s]246463it [01:40, 3526.49it/s]251271it [01:40, 3529.67it/s]258930it [01:41, 1782.46it/s]246817it [01:40, 3380.03it/s]251626it [01:40, 3368.52it/s]259287it [01:41, 2054.67it/s]247188it [01:40, 3473.49it/s]251998it [01:40, 3467.03it/s]259676it [01:41, 2408.71it/s]247538it [01:40, 3318.77it/s]252375it [01:41, 3552.45it/s]260054it [01:41, 2616.52it/s]247915it [01:40, 3445.18it/s]252733it [01:41, 3434.71it/s]260438it [01:41, 2897.74it/s]248262it [01:40, 3316.21it/s]253107it [01:41, 3520.90it/s]260826it [01:41, 3139.67it/s]248640it [01:41, 3445.80it/s]253461it [01:41, 3416.89it/s]261195it [01:41, 3205.93it/s]249004it [01:41, 3500.69it/s]253824it [01:41, 3468.13it/s]261587it [01:41, 3396.02it/s]249356it [01:41, 3386.74it/s]254173it [01:41, 3369.76it/s]261956it [01:41, 3361.34it/s]249720it [01:41, 3458.14it/s]254545it [01:41, 3470.10it/s]262343it [01:42, 3500.91it/s]250068it [01:41, 3371.08it/s]254918it [01:41, 3545.54it/s]262709it [01:42, 3458.19it/s]250439it [01:41, 3467.21it/s]255274it [01:41, 3414.78it/s]263090it [01:42, 3555.59it/s]255906it [01:42, 169.59it/s] 250788it [01:41, 3383.86it/s]255651it [01:42, 3516.30it/s]263454it [01:42, 3475.56it/s]256287it [01:42, 240.33it/s]251153it [01:41, 3460.26it/s]263834it [01:42, 3559.68it/s]256667it [01:42, 336.46it/s]251521it [01:41, 3524.17it/s]264222it [01:42, 3651.76it/s]256988it [01:42, 443.20it/s]251875it [01:42, 3337.35it/s]264591it [01:42, 3540.56it/s]257366it [01:42, 611.31it/s]252258it [01:42, 3475.95it/s]264968it [01:42, 3604.52it/s]257702it [01:42, 793.29it/s]252609it [01:42, 3322.51it/s]265331it [01:42, 3520.96it/s]258081it [01:43, 1054.81it/s]252984it [01:42, 3436.42it/s]265710it [01:42, 3597.38it/s]258427it [01:43, 1310.83it/s]253331it [01:42, 3320.01it/s]266072it [01:43, 3515.15it/s]258809it [01:43, 1651.11it/s]253707it [01:42, 3442.42it/s]266457it [01:43, 3611.11it/s]259196it [01:43, 2011.26it/s]254068it [01:42, 3488.73it/s]266820it [01:43, 3535.19it/s]259559it [01:43, 2267.48it/s]254419it [01:42, 3371.91it/s]267206it [01:43, 3626.74it/s]259940it [01:43, 2588.58it/s]254781it [01:42, 3441.77it/s]267582it [01:43, 3663.22it/s]260301it [01:43, 2745.85it/s]255127it [01:42, 3308.46it/s]267950it [01:43, 3550.59it/s]260680it [01:43, 2996.98it/s]255498it [01:43, 3422.13it/s]268331it [01:43, 3625.10it/s]261038it [01:43, 3045.02it/s]268695it [01:43, 3511.94it/s]255843it [01:43, 3298.42it/s]261427it [01:44, 3265.80it/s]269071it [01:43, 3581.73it/s]261786it [01:44, 3258.87it/s]269431it [01:44, 3462.82it/s]262167it [01:44, 3408.32it/s]269806it [01:44, 3544.56it/s]262547it [01:44, 3516.02it/s]270162it [01:44, 3471.43it/s]262912it [01:44, 3446.54it/s]270539it [01:44, 3555.87it/s]263281it [01:44, 3513.85it/s]270901it [01:44, 3572.11it/s]263640it [01:44, 3409.12it/s]271260it [01:44, 3436.18it/s]264015it [01:44, 3504.06it/s]271638it [01:44, 3534.08it/s]264370it [01:44, 3429.79it/s]271993it [01:44, 3460.85it/s]264745it [01:44, 3514.74it/s]272367it [01:44, 3539.73it/s]265099it [01:45, 3411.96it/s]272723it [01:44, 3437.49it/s]265479it [01:45, 3521.73it/s]273095it [01:45, 3516.53it/s]265857it [01:45, 3594.28it/s]273458it [01:45, 3549.32it/s]266219it [01:45, 3479.71it/s]273814it [01:45, 3462.58it/s]266607it [01:45, 3592.94it/s]274195it [01:45, 3556.96it/s]266968it [01:45, 3474.77it/s]274552it [01:45, 3469.28it/s]267353it [01:45, 3581.87it/s]274932it [01:45, 3564.65it/s]267713it [01:45, 3479.37it/s]275290it [01:45, 3459.57it/s]268101it [01:45, 3591.95it/s]275668it [01:45, 3550.59it/s]268462it [01:46, 3475.19it/s]276025it [01:45, 3455.98it/s]268844it [01:46, 3572.97it/s]276404it [01:46, 3550.78it/s]269219it [01:46, 3621.64it/s]276781it [01:46, 3613.58it/s]269583it [01:46, 3489.61it/s]277144it [01:46, 3476.01it/s]269961it [01:46, 3570.34it/s]277519it [01:46, 3552.92it/s]270320it [01:46, 3430.69it/s]277876it [01:46, 3469.21it/s]270695it [01:46, 3520.57it/s]278251it [01:46, 3548.84it/s]271049it [01:46, 3425.53it/s]278608it [01:46, 3457.59it/s]271429it [01:46, 3532.12it/s]278976it [01:46, 3520.18it/s]271807it [01:46, 3602.83it/s]279350it [01:46, 3583.98it/s]272169it [01:47, 3477.51it/s]279710it [01:46, 3460.03it/s]272542it [01:47, 3547.98it/s]280082it [01:47, 3534.02it/s]272899it [01:47, 3436.84it/s]280437it [01:47, 3441.74it/s]273264it [01:47, 3495.48it/s]280803it [01:47, 3502.82it/s]273615it [01:47, 3412.66it/s]281155it [01:47, 3417.89it/s]273994it [01:47, 3521.04it/s]281532it [01:47, 3517.56it/s]274348it [01:47, 3418.00it/s]281895it [01:47, 3415.00it/s]274730it [01:47, 3532.20it/s]282268it [01:47, 3505.04it/s]275108it [01:47, 3604.10it/s]282633it [01:47, 3545.61it/s]275470it [01:48, 3485.21it/s]282989it [01:47, 3438.69it/s]275847it [01:48, 3566.42it/s]283357it [01:47, 3507.15it/s]276206it [01:48, 3420.57it/s]283709it [01:48, 3422.19it/s]276578it [01:48, 3505.00it/s]284087it [01:48, 3524.34it/s]276931it [01:48, 3405.16it/s]284441it [01:48, 3425.61it/s]277311it [01:48, 3516.66it/s]284824it [01:48, 3540.68it/s]277686it [01:48, 3582.21it/s]285196it [01:48, 3590.31it/s]278046it [01:48, 3459.34it/s]285557it [01:48, 3488.66it/s]278418it [01:48, 3532.94it/s]285937it [01:48, 3577.81it/s]278773it [01:48, 3435.19it/s]286296it [01:48, 3484.24it/s]279149it [01:49, 3528.34it/s]286665it [01:48, 3542.33it/s]279504it [01:49, 3388.31it/s]287021it [01:49, 3445.54it/s]287112it [01:49, 2632.42it/s]
279858it [01:49, 3429.87it/s]280215it [01:49, 3329.67it/s]280575it [01:49, 3405.27it/s]280954it [01:49, 3513.52it/s]281307it [01:49, 3415.29it/s]281681it [01:49, 3507.05it/s]282034it [01:49, 3405.67it/s]256005it [01:49, 156.87it/s] 282400it [01:50, 3477.51it/s]256387it [01:49, 223.42it/s]282750it [01:50, 3383.53it/s]256693it [01:49, 295.17it/s]283129it [01:50, 3498.82it/s]257075it [01:49, 418.09it/s]283503it [01:50, 3568.69it/s]257448it [01:49, 575.20it/s]283862it [01:50, 3459.76it/s]257784it [01:49, 747.23it/s]284247it [01:50, 3572.29it/s]258162it [01:50, 997.67it/s]284606it [01:50, 3465.46it/s]258504it [01:50, 1242.30it/s]284988it [01:50, 3566.38it/s]258885it [01:50, 1576.19it/s]285347it [01:50, 3434.69it/s]259233it [01:50, 1849.41it/s]285730it [01:50, 3546.00it/s]259611it [01:50, 2198.09it/s]286095it [01:51, 3439.62it/s]259981it [01:50, 2506.24it/s]286477it [01:51, 3547.46it/s]260338it [01:50, 2672.67it/s]286859it [01:51, 3625.61it/s]260721it [01:50, 2948.38it/s]287112it [01:51, 2578.50it/s]
261078it [01:50, 3023.70it/s]261463it [01:51, 3237.85it/s]261821it [01:51, 3215.93it/s]262198it [01:51, 3349.73it/s]262573it [01:51, 3309.84it/s]262955it [01:51, 3448.25it/s]263333it [01:51, 3539.62it/s]263695it [01:51, 3434.49it/s]264075it [01:51, 3537.91it/s]264434it [01:51, 3446.51it/s]264799it [01:51, 3502.93it/s]265153it [01:52, 3407.21it/s]265533it [01:52, 3517.14it/s]265913it [01:52, 3599.09it/s]266275it [01:52, 3480.08it/s]266657it [01:52, 3575.84it/s]267017it [01:52, 3442.94it/s]267402it [01:52, 3558.04it/s]267760it [01:52, 3447.81it/s]268143it [01:52, 3556.32it/s]268501it [01:53, 3437.70it/s]268873it [01:53, 3515.26it/s]269235it [01:53, 3545.37it/s]269591it [01:53, 3423.95it/s]269970it [01:53, 3527.63it/s]270325it [01:53, 3398.05it/s]270695it [01:53, 3482.26it/s]271045it [01:53, 3391.41it/s]271413it [01:53, 3472.30it/s]255843it [01:53, 3298.42it/s]255854it [01:53, 78.90it/s]  271786it [01:53, 3546.96it/s]256232it [01:53, 127.82it/s]272142it [01:54, 3431.67it/s]256610it [01:53, 195.49it/s]272513it [01:54, 3507.46it/s]256966it [01:53, 281.38it/s]272866it [01:54, 3385.42it/s]257328it [01:54, 399.66it/s]273237it [01:54, 3478.17it/s]257678it [01:54, 546.27it/s]258055it [01:54, 752.85it/s]273587it [01:54, 3320.52it/s]273963it [01:54, 3442.89it/s]258406it [01:54, 973.91it/s]274333it [01:54, 3362.41it/s]258746it [01:54, 1219.59it/s]274716it [01:54, 3493.57it/s]259133it [01:54, 1564.93it/s]275097it [01:54, 3582.46it/s]259481it [01:54, 1835.87it/s]259860it [01:54, 2188.94it/s]275458it [01:55, 3464.75it/s]275839it [01:55, 3561.68it/s]260211it [01:54, 2416.32it/s]260589it [01:54, 2720.88it/s]276197it [01:55, 3425.97it/s]276571it [01:55, 3514.45it/s]260942it [01:55, 2849.46it/s]261326it [01:55, 3099.08it/s]276925it [01:55, 3401.39it/s]261706it [01:55, 3283.16it/s]277300it [01:55, 3499.46it/s]277663it [01:55, 3535.59it/s]262069it [01:55, 3219.54it/s]262440it [01:55, 3350.79it/s]278018it [01:55, 3417.50it/s]278379it [01:55, 3471.80it/s]262794it [01:55, 3299.73it/s]263177it [01:55, 3446.03it/s]278728it [01:56, 3376.34it/s]279103it [01:56, 3481.23it/s]263532it [01:55, 3336.49it/s]279453it [01:56, 3381.83it/s]263915it [01:55, 3472.98it/s]279827it [01:56, 3482.52it/s]264269it [01:56, 3393.97it/s]280200it [01:56, 3552.75it/s]264650it [01:56, 3509.83it/s]265009it [01:56, 3531.48it/s]280557it [01:56, 3409.56it/s]280934it [01:56, 3511.54it/s]265365it [01:56, 3431.66it/s]265748it [01:56, 3544.50it/s]281287it [01:56, 3409.38it/s]281664it [01:56, 3510.96it/s]266105it [01:56, 3438.61it/s]266491it [01:56, 3558.34it/s]282017it [01:56, 3410.05it/s]282393it [01:57, 3508.02it/s]266849it [01:56, 3421.78it/s]267234it [01:56, 3542.53it/s]282746it [01:57, 3374.26it/s]283120it [01:57, 3477.16it/s]267613it [01:56, 3445.73it/s]283492it [01:57, 3545.99it/s]267986it [01:57, 3525.33it/s]283849it [01:57, 3435.69it/s]268368it [01:57, 3608.76it/s]284229it [01:57, 3539.97it/s]268731it [01:57, 3480.29it/s]284585it [01:57, 3435.83it/s]269105it [01:57, 3552.38it/s]284954it [01:57, 3506.96it/s]269462it [01:57, 3398.33it/s]285307it [01:57, 3403.07it/s]269843it [01:57, 3513.21it/s]285681it [01:58, 3499.54it/s]270197it [01:57, 3388.90it/s]286050it [01:58, 3553.28it/s]270566it [01:57, 3473.12it/s]286407it [01:58, 3435.52it/s]270923it [01:57, 3499.66it/s]286789it [01:58, 3544.94it/s]271275it [01:58, 3392.79it/s]287112it [01:58, 2424.47it/s]
2022-07-19 13:25:20 | INFO | root | success load 287112 data
2022-07-19 13:25:20 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-19 13:25:20 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-19 13:25:20 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-19 13:25:20 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-19 13:25:20 | INFO | transformer.tokenization_utils | loading file None
2022-07-19 13:25:20 | INFO | transformer.tokenization_utils | loading file None
2022-07-19 13:25:20 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
271653it [01:58, 3502.97it/s]272005it [01:58, 3410.22it/s]272369it [01:58, 3475.19it/s]272718it [01:58, 3373.04it/s]273096it [01:58, 3488.39it/s]273478it [01:58, 3583.78it/s]273838it [01:58, 3440.02it/s]274219it [01:58, 3543.52it/s]274576it [01:59, 3435.24it/s]274958it [01:59, 3543.93it/s]275315it [01:59, 3399.24it/s]275694it [01:59, 3507.64it/s]276047it [01:59, 3407.68it/s]276425it [01:59, 3513.20it/s]276786it [01:59, 3541.09it/s]277142it [01:59, 3434.98it/s]277519it [01:59, 3530.40it/s]277874it [01:59, 3388.98it/s]278248it [02:00, 3489.03it/s]278599it [02:00, 3389.36it/s]278959it [02:00, 3449.33it/s]279306it [02:00, 3420.87it/s]279650it [02:00, 3306.72it/s]280022it [02:00, 3423.82it/s]280366it [02:00, 3325.71it/s]280730it [02:00, 3413.59it/s]281073it [02:00, 3324.48it/s]281447it [02:01, 3443.47it/s]281819it [02:01, 3523.45it/s]282173it [02:01, 3344.34it/s]282552it [02:01, 3471.06it/s]282902it [02:01, 3373.94it/s]283278it [02:01, 3483.99it/s]283629it [02:01, 3381.64it/s]283996it [02:01, 3461.94it/s]284382it [02:01, 3576.24it/s]284742it [02:01, 3460.05it/s]285119it [02:02, 3547.12it/s]285476it [02:02, 3377.58it/s]285860it [02:02, 3506.43it/s]286214it [02:02, 3401.89it/s]286591it [02:02, 3504.44it/s]286944it [02:02, 3374.87it/s]287112it [02:02, 2341.04it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-19 13:33:39 | INFO | train_inner | epoch 001:    100 / 1122 loss=27.678, nll_loss=12.128, mask_ins=7.632, word_ins_ml=12.612, word_reposition=5.222, kpe=2.212, ppl=2.14759e+08, wps=4073.1, ups=0.2, wpb=20057.6, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=23.207, clip=20, loss_scale=128, train_wall=454, wall=618
2022-07-19 13:42:19 | INFO | train_inner | epoch 001:    200 / 1122 loss=22.507, nll_loss=11.012, mask_ins=5.179, word_ins_ml=11.621, word_reposition=4.295, kpe=1.412, ppl=5.95847e+06, wps=3855.4, ups=0.19, wpb=20047.3, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=16.434, clip=0, loss_scale=128, train_wall=477, wall=1138
2022-07-19 13:50:57 | INFO | train_inner | epoch 001:    300 / 1122 loss=19.117, nll_loss=10.826, mask_ins=3.047, word_ins_ml=11.45, word_reposition=3.362, kpe=1.258, ppl=568619, wps=3871, ups=0.19, wpb=20028.2, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=5.957, clip=0, loss_scale=128, train_wall=474, wall=1655
2022-07-19 13:59:44 | INFO | train_inner | epoch 001:    400 / 1122 loss=17.501, nll_loss=10.573, mask_ins=2.629, word_ins_ml=11.229, word_reposition=2.434, kpe=1.209, ppl=185512, wps=3795.2, ups=0.19, wpb=20006, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=3.983, clip=0, loss_scale=128, train_wall=485, wall=2183
2022-07-19 14:07:42 | INFO | train_inner | epoch 001:    500 / 1122 loss=16.963, nll_loss=10.396, mask_ins=2.52, word_ins_ml=11.076, word_reposition=2.211, kpe=1.156, ppl=127763, wps=4184.9, ups=0.21, wpb=20021, bsz=256, num_updates=500, lr=5.009e-05, gnorm=3.237, clip=0, loss_scale=128, train_wall=436, wall=2661
2022-07-19 14:16:32 | INFO | train_inner | epoch 001:    600 / 1122 loss=16.695, nll_loss=10.214, mask_ins=2.493, word_ins_ml=10.92, word_reposition=2.149, kpe=1.133, ppl=106130, wps=3780.9, ups=0.19, wpb=20020, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=3.025, clip=0, loss_scale=242, train_wall=487, wall=3191
2022-07-19 14:24:43 | INFO | train_inner | epoch 001:    700 / 1122 loss=16.443, nll_loss=10.011, mask_ins=2.457, word_ins_ml=10.749, word_reposition=2.128, kpe=1.109, ppl=89062.8, wps=4080.5, ups=0.2, wpb=20029.4, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=2.833, clip=0, loss_scale=256, train_wall=448, wall=3681
2022-07-19 14:31:09 | INFO | train_inner | epoch 001:    800 / 1122 loss=16.277, nll_loss=9.789, mask_ins=2.453, word_ins_ml=10.56, word_reposition=2.171, kpe=1.093, ppl=79383.7, wps=5183.5, ups=0.26, wpb=20015.4, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=2.689, clip=0, loss_scale=256, train_wall=345, wall=4068
2022-07-19 14:36:06 | INFO | train_inner | epoch 001:    900 / 1122 loss=16.049, nll_loss=9.538, mask_ins=2.425, word_ins_ml=10.345, word_reposition=2.201, kpe=1.077, ppl=67780.3, wps=6719.8, ups=0.34, wpb=19953.8, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=2.541, clip=0, loss_scale=256, train_wall=256, wall=4365
2022-07-19 14:41:00 | INFO | train_inner | epoch 001:   1000 / 1122 loss=15.854, nll_loss=9.341, mask_ins=2.41, word_ins_ml=10.176, word_reposition=2.204, kpe=1.064, ppl=59246.7, wps=6826, ups=0.34, wpb=20097, bsz=256, num_updates=1000, lr=0.00010008, gnorm=2.422, clip=0, loss_scale=256, train_wall=253, wall=4659
2022-07-19 14:45:59 | INFO | train_inner | epoch 001:   1100 / 1122 loss=15.73, nll_loss=9.215, mask_ins=2.406, word_ins_ml=10.069, word_reposition=2.202, kpe=1.053, ppl=54350.9, wps=6694.6, ups=0.34, wpb=19959.7, bsz=256, num_updates=1100, lr=0.000110078, gnorm=2.338, clip=0, loss_scale=453, train_wall=256, wall=4957
2022-07-19 14:47:03 | INFO | train | epoch 001 | loss 18.207 | nll_loss 10.255 | mask_ins 3.225 | word_ins_ml 10.964 | word_reposition 2.769 | kpe 1.249 | ppl 302519 | wps 4585.2 | ups 0.23 | wpb 20005.2 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 6.171 | clip 1.8 | loss_scale 220 | train_wall 4426 | wall 5021
2022-07-19 14:48:24 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.987 | nll_loss 9.992 | mask_ins 2.797 | word_ins_ml 10.844 | word_reposition 2.781 | kpe 1.564 | ppl 259755 | wps 12034.8 | wpb 2325.1 | bsz 32 | num_updates 1122
2022-07-19 14:48:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_best.pt (epoch 1 @ 1122 updates, score 17.987) (writing took 4.57091165240854 seconds)
2022-07-19 14:52:18 | INFO | train_inner | epoch 002:     78 / 1122 loss=15.59, nll_loss=9.092, mask_ins=2.396, word_ins_ml=9.964, word_reposition=2.186, kpe=1.045, ppl=49327.6, wps=5225.4, ups=0.26, wpb=19842.2, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=2.355, clip=0, loss_scale=512, train_wall=253, wall=5337
2022-07-19 14:57:16 | INFO | train_inner | epoch 002:    178 / 1122 loss=15.445, nll_loss=8.946, mask_ins=2.379, word_ins_ml=9.836, word_reposition=2.199, kpe=1.03, ppl=44594.6, wps=6731.2, ups=0.34, wpb=20046.6, bsz=256, num_updates=1300, lr=0.000130074, gnorm=2.18, clip=0, loss_scale=512, train_wall=256, wall=5635
2022-07-19 15:02:16 | INFO | train_inner | epoch 002:    278 / 1122 loss=15.334, nll_loss=8.822, mask_ins=2.376, word_ins_ml=9.729, word_reposition=2.206, kpe=1.024, ppl=41303.9, wps=6702, ups=0.33, wpb=20116.2, bsz=256, num_updates=1400, lr=0.000140072, gnorm=2.091, clip=0, loss_scale=512, train_wall=258, wall=5935
2022-07-19 15:07:15 | INFO | train_inner | epoch 002:    378 / 1122 loss=15.196, nll_loss=8.693, mask_ins=2.373, word_ins_ml=9.616, word_reposition=2.188, kpe=1.018, ppl=37534.5, wps=6641.7, ups=0.33, wpb=19858.7, bsz=256, num_updates=1500, lr=0.00015007, gnorm=2.072, clip=0, loss_scale=512, train_wall=258, wall=6234
2022-07-19 15:12:12 | INFO | train_inner | epoch 002:    478 / 1122 loss=15.048, nll_loss=8.541, mask_ins=2.367, word_ins_ml=9.483, word_reposition=2.182, kpe=1.016, ppl=33876.4, wps=6740.8, ups=0.34, wpb=20015.5, bsz=256, num_updates=1600, lr=0.000160068, gnorm=2.069, clip=0, loss_scale=845, train_wall=256, wall=6531
2022-07-19 15:17:10 | INFO | train_inner | epoch 002:    578 / 1122 loss=14.889, nll_loss=8.353, mask_ins=2.362, word_ins_ml=9.32, word_reposition=2.199, kpe=1.008, ppl=30342.5, wps=6722.6, ups=0.34, wpb=20035.4, bsz=256, num_updates=1700, lr=0.000170066, gnorm=2.182, clip=0, loss_scale=1024, train_wall=256, wall=6829
2022-07-19 15:22:07 | INFO | train_inner | epoch 002:    678 / 1122 loss=14.675, nll_loss=8.139, mask_ins=2.351, word_ins_ml=9.132, word_reposition=2.181, kpe=1.011, ppl=26160.2, wps=6733, ups=0.34, wpb=19977.2, bsz=256, num_updates=1800, lr=0.000180064, gnorm=2.164, clip=0, loss_scale=1024, train_wall=255, wall=7126
2022-07-19 15:27:28 | INFO | train_inner | epoch 002:    778 / 1122 loss=14.493, nll_loss=7.913, mask_ins=2.35, word_ins_ml=8.935, word_reposition=2.195, kpe=1.014, ppl=23056.1, wps=6252.7, ups=0.31, wpb=20068.6, bsz=256, num_updates=1900, lr=0.000190062, gnorm=2.259, clip=0, loss_scale=1024, train_wall=280, wall=7446
2022-07-19 15:32:32 | INFO | train_inner | epoch 002:    878 / 1122 loss=14.237, nll_loss=7.658, mask_ins=2.345, word_ins_ml=8.714, word_reposition=2.169, kpe=1.009, ppl=19315.1, wps=6559.7, ups=0.33, wpb=19917.4, bsz=256, num_updates=2000, lr=0.00020006, gnorm=2.361, clip=0, loss_scale=1024, train_wall=262, wall=7750
2022-07-19 15:37:31 | INFO | train_inner | epoch 002:    978 / 1122 loss=13.981, nll_loss=7.373, mask_ins=2.34, word_ins_ml=8.466, word_reposition=2.163, kpe=1.012, ppl=16169.5, wps=6687.3, ups=0.33, wpb=20003.7, bsz=256, num_updates=2100, lr=0.000210058, gnorm=2.427, clip=0, loss_scale=1567, train_wall=257, wall=8049
2022-07-19 15:42:27 | INFO | train_inner | epoch 002:   1078 / 1122 loss=13.792, nll_loss=7.162, mask_ins=2.356, word_ins_ml=8.284, word_reposition=2.144, kpe=1.009, ppl=14188.9, wps=6813.1, ups=0.34, wpb=20194.1, bsz=256, num_updates=2200, lr=0.000220056, gnorm=2.432, clip=0, loss_scale=2048, train_wall=254, wall=8346
2022-07-19 15:44:35 | INFO | train | epoch 002 | loss 14.727 | nll_loss 8.179 | mask_ins 2.362 | word_ins_ml 9.167 | word_reposition 2.181 | kpe 1.017 | ppl 27120.1 | wps 6501.9 | ups 0.32 | wpb 20006.1 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 2.243 | clip 0 | loss_scale 1015 | train_wall 2898 | wall 8474
2022-07-19 15:45:56 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 17.12 | nll_loss 8.88 | mask_ins 2.804 | word_ins_ml 9.868 | word_reposition 2.977 | kpe 1.471 | ppl 142452 | wps 12063.4 | wpb 2325.1 | bsz 32 | num_updates 2244 | best_loss 17.12
2022-07-19 15:46:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_best.pt (epoch 2 @ 2244 updates, score 17.12) (writing took 6.2112393993884325 seconds)
2022-07-19 15:48:45 | INFO | train_inner | epoch 003:     56 / 1122 loss=13.593, nll_loss=6.947, mask_ins=2.351, word_ins_ml=8.099, word_reposition=2.135, kpe=1.009, ppl=12354.6, wps=5250, ups=0.26, wpb=19845.6, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=2.55, clip=0, loss_scale=2048, train_wall=249, wall=8724
2022-07-19 15:53:36 | INFO | train_inner | epoch 003:    156 / 1122 loss=13.348, nll_loss=6.727, mask_ins=2.347, word_ins_ml=7.908, word_reposition=2.092, kpe=1.001, ppl=10430.4, wps=6853.8, ups=0.34, wpb=19944.8, bsz=256, num_updates=2400, lr=0.000240052, gnorm=2.451, clip=0, loss_scale=2048, train_wall=249, wall=9015
2022-07-19 15:58:34 | INFO | train_inner | epoch 003:    256 / 1122 loss=13.148, nll_loss=6.545, mask_ins=2.329, word_ins_ml=7.75, word_reposition=2.069, kpe=1.001, ppl=9077.54, wps=6728.4, ups=0.34, wpb=20072.3, bsz=256, num_updates=2500, lr=0.00025005, gnorm=2.349, clip=0, loss_scale=2048, train_wall=256, wall=9313
2022-07-19 16:03:32 | INFO | train_inner | epoch 003:    356 / 1122 loss=12.971, nll_loss=6.353, mask_ins=2.336, word_ins_ml=7.583, word_reposition=2.053, kpe=0.999, ppl=8031.66, wps=6751.7, ups=0.34, wpb=20074.5, bsz=256, num_updates=2600, lr=0.000260048, gnorm=2.36, clip=0, loss_scale=2888, train_wall=255, wall=9610
2022-07-19 16:08:28 | INFO | train_inner | epoch 003:    456 / 1122 loss=12.831, nll_loss=6.223, mask_ins=2.332, word_ins_ml=7.47, word_reposition=2.029, kpe=1, ppl=7287.66, wps=6701.2, ups=0.34, wpb=19857.3, bsz=256, num_updates=2700, lr=0.000270046, gnorm=2.415, clip=0, loss_scale=4096, train_wall=255, wall=9907
2022-07-19 16:13:26 | INFO | train_inner | epoch 003:    556 / 1122 loss=12.682, nll_loss=6.074, mask_ins=2.335, word_ins_ml=7.34, word_reposition=2.012, kpe=0.995, ppl=6569.76, wps=6728.9, ups=0.34, wpb=20028.7, bsz=256, num_updates=2800, lr=0.000280044, gnorm=2.212, clip=0, loss_scale=4096, train_wall=256, wall=10204
2022-07-19 16:18:24 | INFO | train_inner | epoch 003:    656 / 1122 loss=12.462, nll_loss=5.875, mask_ins=2.326, word_ins_ml=7.166, word_reposition=1.977, kpe=0.993, ppl=5642.76, wps=6715.6, ups=0.34, wpb=20031.3, bsz=256, num_updates=2900, lr=0.000290042, gnorm=2.292, clip=0, loss_scale=4096, train_wall=256, wall=10503
2022-07-19 16:23:22 | INFO | train_inner | epoch 003:    756 / 1122 loss=12.008, nll_loss=5.429, mask_ins=2.276, word_ins_ml=6.775, word_reposition=1.952, kpe=1.004, ppl=4117.46, wps=6731.3, ups=0.34, wpb=20065.6, bsz=256, num_updates=3000, lr=0.00030004, gnorm=2.458, clip=0, loss_scale=4096, train_wall=257, wall=10801
2022-07-19 16:28:21 | INFO | train_inner | epoch 003:    856 / 1122 loss=11.408, nll_loss=4.976, mask_ins=2.18, word_ins_ml=6.375, word_reposition=1.859, kpe=0.994, ppl=2718.21, wps=6728.7, ups=0.33, wpb=20107.1, bsz=256, num_updates=3100, lr=0.000310038, gnorm=2.455, clip=0, loss_scale=5284, train_wall=256, wall=11100
2022-07-19 16:33:35 | INFO | train_inner | epoch 003:    956 / 1122 loss=11.163, nll_loss=4.793, mask_ins=2.125, word_ins_ml=6.212, word_reposition=1.822, kpe=1.003, ppl=2292.35, wps=6384.1, ups=0.32, wpb=20040.7, bsz=256, num_updates=3200, lr=0.000320036, gnorm=2.454, clip=0, loss_scale=8192, train_wall=272, wall=11414
2022-07-19 16:38:37 | INFO | train_inner | epoch 003:   1056 / 1122 loss=11.004, nll_loss=4.663, mask_ins=2.082, word_ins_ml=6.097, word_reposition=1.827, kpe=0.998, ppl=2053.78, wps=6611.7, ups=0.33, wpb=19995.1, bsz=256, num_updates=3300, lr=0.000330034, gnorm=2.431, clip=0, loss_scale=8192, train_wall=260, wall=11716
2022-07-19 16:41:53 | INFO | train | epoch 003 | loss 12.278 | nll_loss 5.752 | mask_ins 2.258 | word_ins_ml 7.055 | word_reposition 1.965 | kpe 0.999 | ppl 4965.92 | wps 6529 | ups 0.33 | wpb 20006.6 | bsz 255.8 | num_updates 3366 | lr 0.000336633 | gnorm 2.399 | clip 0 | loss_scale 4598 | train_wall 2880 | wall 11912
2022-07-19 16:43:17 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 17.011 | nll_loss 8.247 | mask_ins 2.655 | word_ins_ml 9.4 | word_reposition 3.622 | kpe 1.335 | ppl 132082 | wps 11672.8 | wpb 2325.1 | bsz 32 | num_updates 3366 | best_loss 17.011
2022-07-19 16:43:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_best.pt (epoch 3 @ 3366 updates, score 17.011) (writing took 6.991601157002151 seconds)
2022-07-19 16:45:05 | INFO | train_inner | epoch 004:     34 / 1122 loss=10.803, nll_loss=4.542, mask_ins=2.048, word_ins_ml=5.99, word_reposition=1.77, kpe=0.995, ppl=1787.13, wps=5120.7, ups=0.26, wpb=19872.6, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=2.463, clip=0, loss_scale=8192, train_wall=256, wall=12104
2022-07-19 16:50:05 | INFO | train_inner | epoch 004:    134 / 1122 loss=10.697, nll_loss=4.462, mask_ins=2.02, word_ins_ml=5.918, word_reposition=1.772, kpe=0.986, ppl=1659.48, wps=6666.2, ups=0.33, wpb=19950.2, bsz=256, num_updates=3500, lr=0.00035003, gnorm=2.318, clip=0, loss_scale=8192, train_wall=258, wall=12403
2022-07-19 16:55:05 | INFO | train_inner | epoch 004:    234 / 1122 loss=10.589, nll_loss=4.382, mask_ins=1.998, word_ins_ml=5.846, word_reposition=1.755, kpe=0.99, ppl=1540.25, wps=6707.3, ups=0.33, wpb=20144.8, bsz=256, num_updates=3600, lr=0.000360028, gnorm=2.272, clip=0, loss_scale=9585, train_wall=257, wall=12704
2022-07-19 17:00:06 | INFO | train_inner | epoch 004:    334 / 1122 loss=10.562, nll_loss=4.377, mask_ins=1.991, word_ins_ml=5.841, word_reposition=1.737, kpe=0.993, ppl=1512.02, wps=6613.4, ups=0.33, wpb=19868.9, bsz=256, num_updates=3700, lr=0.000370026, gnorm=2.255, clip=0, loss_scale=16384, train_wall=258, wall=13004
2022-07-19 17:05:08 | INFO | train_inner | epoch 004:    434 / 1122 loss=10.446, nll_loss=4.312, mask_ins=1.957, word_ins_ml=5.782, word_reposition=1.716, kpe=0.99, ppl=1394.82, wps=6654.3, ups=0.33, wpb=20120.4, bsz=256, num_updates=3800, lr=0.000380024, gnorm=2.217, clip=0, loss_scale=16384, train_wall=259, wall=13306
2022-07-19 17:10:04 | INFO | train_inner | epoch 004:    534 / 1122 loss=10.392, nll_loss=4.233, mask_ins=1.953, word_ins_ml=5.711, word_reposition=1.733, kpe=0.996, ppl=1343.87, wps=6733.6, ups=0.34, wpb=19946.6, bsz=256, num_updates=3900, lr=0.000390022, gnorm=2.233, clip=0, loss_scale=16384, train_wall=255, wall=13603
2022-07-19 17:10:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-19 17:15:07 | INFO | train_inner | epoch 004:    635 / 1122 loss=10.389, nll_loss=4.258, mask_ins=1.952, word_ins_ml=5.733, word_reposition=1.711, kpe=0.994, ppl=1340.89, wps=6603.1, ups=0.33, wpb=20001.7, bsz=256, num_updates=4000, lr=0.00040002, gnorm=2.2, clip=0, loss_scale=9084, train_wall=261, wall=13906
2022-07-19 17:20:07 | INFO | train_inner | epoch 004:    735 / 1122 loss=10.316, nll_loss=4.184, mask_ins=1.933, word_ins_ml=5.666, word_reposition=1.72, kpe=0.996, ppl=1274.52, wps=6719.5, ups=0.33, wpb=20143.9, bsz=256, num_updates=4100, lr=0.000410018, gnorm=2.112, clip=0, loss_scale=8192, train_wall=257, wall=14205
2022-07-19 17:25:05 | INFO | train_inner | epoch 004:    835 / 1122 loss=10.29, nll_loss=4.175, mask_ins=1.926, word_ins_ml=5.657, word_reposition=1.71, kpe=0.996, ppl=1252.06, wps=6678.9, ups=0.33, wpb=19939.2, bsz=256, num_updates=4200, lr=0.000420016, gnorm=2.197, clip=0, loss_scale=8192, train_wall=257, wall=14504
2022-07-19 17:25:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-19 17:30:06 | INFO | train_inner | epoch 004:    936 / 1122 loss=10.292, nll_loss=4.193, mask_ins=1.928, word_ins_ml=5.671, word_reposition=1.703, kpe=0.99, ppl=1253.86, wps=6687.1, ups=0.33, wpb=20118.3, bsz=256, num_updates=4300, lr=0.000430014, gnorm=2.145, clip=0, loss_scale=4664, train_wall=258, wall=14805
2022-07-19 17:35:02 | INFO | train_inner | epoch 004:   1036 / 1122 loss=10.213, nll_loss=4.123, mask_ins=1.922, word_ins_ml=5.609, word_reposition=1.695, kpe=0.987, ppl=1187.23, wps=6739.3, ups=0.34, wpb=19945.3, bsz=256, num_updates=4400, lr=0.000440012, gnorm=2.049, clip=0, loss_scale=4096, train_wall=254, wall=15101
2022-07-19 17:39:42 | INFO | train | epoch 004 | loss 10.41 | nll_loss 4.265 | mask_ins 1.956 | word_ins_ml 5.739 | word_reposition 1.723 | kpe 0.992 | ppl 1360.36 | wps 6460.6 | ups 0.32 | wpb 20006 | bsz 255.8 | num_updates 4486 | lr 0.00044861 | gnorm 2.197 | clip 0 | loss_scale 9590 | train_wall 2906 | wall 15380
2022-07-19 17:41:03 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 16.169 | nll_loss 8.022 | mask_ins 2.641 | word_ins_ml 9.189 | word_reposition 2.838 | kpe 1.502 | ppl 73690.9 | wps 11998.9 | wpb 2325.1 | bsz 32 | num_updates 4486 | best_loss 16.169
2022-07-19 17:41:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_best.pt (epoch 4 @ 4486 updates, score 16.169) (writing took 6.17758516035974 seconds)
2022-07-19 17:41:51 | INFO | train_inner | epoch 005:     14 / 1122 loss=10.153, nll_loss=4.109, mask_ins=1.893, word_ins_ml=5.595, word_reposition=1.665, kpe=1, ppl=1138.75, wps=4864.1, ups=0.24, wpb=19859.5, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=2.088, clip=0, loss_scale=4096, train_wall=280, wall=15509
2022-07-19 17:46:47 | INFO | train_inner | epoch 005:    114 / 1122 loss=10.088, nll_loss=4.031, mask_ins=1.897, word_ins_ml=5.527, word_reposition=1.68, kpe=0.984, ppl=1088.59, wps=6769.5, ups=0.34, wpb=20097.9, bsz=256, num_updates=4600, lr=0.000460008, gnorm=2.034, clip=0, loss_scale=4096, train_wall=255, wall=15806
2022-07-19 17:51:48 | INFO | train_inner | epoch 005:    214 / 1122 loss=10.046, nll_loss=4.028, mask_ins=1.878, word_ins_ml=5.523, word_reposition=1.659, kpe=0.987, ppl=1057.48, wps=6661.2, ups=0.33, wpb=20008, bsz=256, num_updates=4700, lr=0.000470006, gnorm=2.066, clip=0, loss_scale=4096, train_wall=258, wall=16106
2022-07-19 17:54:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-19 17:56:47 | INFO | train_inner | epoch 005:    315 / 1122 loss=10.002, nll_loss=3.99, mask_ins=1.888, word_ins_ml=5.488, word_reposition=1.636, kpe=0.99, ppl=1025.51, wps=6735.6, ups=0.33, wpb=20130.7, bsz=256, num_updates=4800, lr=0.000480004, gnorm=2.11, clip=0, loss_scale=5069, train_wall=257, wall=16405
2022-07-19 18:01:41 | INFO | train_inner | epoch 005:    415 / 1122 loss=10.084, nll_loss=4.038, mask_ins=1.89, word_ins_ml=5.531, word_reposition=1.675, kpe=0.989, ppl=1085.56, wps=6801.9, ups=0.34, wpb=20006.1, bsz=256, num_updates=4900, lr=0.000490002, gnorm=1.992, clip=0, loss_scale=4096, train_wall=253, wall=16699
2022-07-19 18:06:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-19 18:06:40 | INFO | train_inner | epoch 005:    516 / 1122 loss=9.977, nll_loss=3.971, mask_ins=1.879, word_ins_ml=5.471, word_reposition=1.641, kpe=0.986, ppl=1007.85, wps=6651.6, ups=0.33, wpb=19901.4, bsz=256, num_updates=5000, lr=0.0005, gnorm=2.012, clip=0, loss_scale=3853, train_wall=256, wall=16999
2022-07-19 18:11:38 | INFO | train_inner | epoch 005:    616 / 1122 loss=9.955, nll_loss=3.969, mask_ins=1.863, word_ins_ml=5.467, word_reposition=1.637, kpe=0.987, ppl=992.39, wps=6697.7, ups=0.34, wpb=19966.7, bsz=256, num_updates=5100, lr=0.000495074, gnorm=1.999, clip=0, loss_scale=2048, train_wall=254, wall=17297
2022-07-19 18:16:33 | INFO | train_inner | epoch 005:    716 / 1122 loss=9.994, nll_loss=3.967, mask_ins=1.878, word_ins_ml=5.466, word_reposition=1.659, kpe=0.991, ppl=1019.51, wps=6803, ups=0.34, wpb=20025.4, bsz=256, num_updates=5200, lr=0.00049029, gnorm=2.112, clip=0, loss_scale=2048, train_wall=252, wall=17591
2022-07-19 18:21:23 | INFO | train_inner | epoch 005:    816 / 1122 loss=9.944, nll_loss=3.962, mask_ins=1.85, word_ins_ml=5.46, word_reposition=1.646, kpe=0.987, ppl=984.94, wps=6908.8, ups=0.34, wpb=20089.2, bsz=256, num_updates=5300, lr=0.000485643, gnorm=2.005, clip=0, loss_scale=2048, train_wall=250, wall=17882
2022-07-19 18:26:14 | INFO | train_inner | epoch 005:    916 / 1122 loss=9.955, nll_loss=3.952, mask_ins=1.86, word_ins_ml=5.45, word_reposition=1.647, kpe=0.998, ppl=992.71, wps=6906.7, ups=0.34, wpb=20060.4, bsz=256, num_updates=5400, lr=0.000481125, gnorm=2.308, clip=0, loss_scale=2048, train_wall=249, wall=18172
2022-07-19 18:30:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-19 18:31:12 | INFO | train_inner | epoch 005:   1017 / 1122 loss=9.879, nll_loss=3.913, mask_ins=1.846, word_ins_ml=5.415, word_reposition=1.618, kpe=1, ppl=941.91, wps=6681.9, ups=0.34, wpb=19909.9, bsz=256, num_updates=5500, lr=0.000476731, gnorm=2.179, clip=0, loss_scale=1855, train_wall=255, wall=18470
2022-07-19 18:36:06 | INFO | train_inner | epoch 005:   1117 / 1122 loss=9.83, nll_loss=3.869, mask_ins=1.845, word_ins_ml=5.376, word_reposition=1.615, kpe=0.994, ppl=910.09, wps=6806.3, ups=0.34, wpb=20047.5, bsz=256, num_updates=5600, lr=0.000472456, gnorm=1.972, clip=0, loss_scale=1024, train_wall=252, wall=18765
2022-07-19 18:36:20 | INFO | train | epoch 005 | loss 9.981 | nll_loss 3.975 | mask_ins 1.871 | word_ins_ml 5.473 | word_reposition 1.647 | kpe 0.99 | ppl 1010.73 | wps 6587.4 | ups 0.33 | wpb 20007.8 | bsz 255.8 | num_updates 5605 | lr 0.000472245 | gnorm 2.078 | clip 0 | loss_scale 2942 | train_wall 2839 | wall 18779
2022-07-19 18:37:42 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 16.431 | nll_loss 8.297 | mask_ins 2.712 | word_ins_ml 9.447 | word_reposition 2.879 | kpe 1.394 | ppl 88360 | wps 11828.7 | wpb 2325.1 | bsz 32 | num_updates 5605 | best_loss 16.169
2022-07-19 18:37:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 5 @ 5605 updates, score 16.431) (writing took 3.409344719722867 seconds)
2022-07-19 18:42:27 | INFO | train_inner | epoch 006:     95 / 1122 loss=9.833, nll_loss=3.887, mask_ins=1.848, word_ins_ml=5.391, word_reposition=1.604, kpe=0.99, ppl=911.95, wps=5196.9, ups=0.26, wpb=19805, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=2.198, clip=0, loss_scale=1024, train_wall=253, wall=19146
2022-07-19 18:47:54 | INFO | train_inner | epoch 006:    195 / 1122 loss=9.747, nll_loss=3.828, mask_ins=1.817, word_ins_ml=5.338, word_reposition=1.606, kpe=0.986, ppl=859.48, wps=6157.3, ups=0.31, wpb=20096.3, bsz=256, num_updates=5800, lr=0.000464238, gnorm=1.96, clip=0, loss_scale=1024, train_wall=285, wall=19472
2022-07-19 18:52:51 | INFO | train_inner | epoch 006:    295 / 1122 loss=9.791, nll_loss=3.848, mask_ins=1.838, word_ins_ml=5.355, word_reposition=1.613, kpe=0.985, ppl=886.12, wps=6761.7, ups=0.34, wpb=20072.6, bsz=256, num_updates=5900, lr=0.000460287, gnorm=2.036, clip=0, loss_scale=1024, train_wall=255, wall=19769
2022-07-19 18:57:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-19 18:57:51 | INFO | train_inner | epoch 006:    396 / 1122 loss=9.652, nll_loss=3.77, mask_ins=1.8, word_ins_ml=5.285, word_reposition=1.582, kpe=0.985, ppl=804.69, wps=6674.8, ups=0.33, wpb=20052.4, bsz=256, num_updates=6000, lr=0.000456435, gnorm=1.964, clip=0, loss_scale=933, train_wall=257, wall=20070
2022-07-19 19:02:49 | INFO | train_inner | epoch 006:    496 / 1122 loss=9.662, nll_loss=3.759, mask_ins=1.814, word_ins_ml=5.275, word_reposition=1.598, kpe=0.974, ppl=809.89, wps=6692.4, ups=0.34, wpb=19945.4, bsz=256, num_updates=6100, lr=0.000452679, gnorm=1.949, clip=0, loss_scale=512, train_wall=255, wall=20368
2022-07-19 19:07:44 | INFO | train_inner | epoch 006:    596 / 1122 loss=9.662, nll_loss=3.767, mask_ins=1.805, word_ins_ml=5.281, word_reposition=1.599, kpe=0.977, ppl=810.16, wps=6733.5, ups=0.34, wpb=19853.6, bsz=256, num_updates=6200, lr=0.000449013, gnorm=1.898, clip=0, loss_scale=512, train_wall=253, wall=20663
2022-07-19 19:12:41 | INFO | train_inner | epoch 006:    696 / 1122 loss=9.691, nll_loss=3.78, mask_ins=1.805, word_ins_ml=5.292, word_reposition=1.62, kpe=0.974, ppl=826.63, wps=6779.9, ups=0.34, wpb=20120.7, bsz=256, num_updates=6300, lr=0.000445435, gnorm=1.879, clip=0, loss_scale=512, train_wall=254, wall=20959
2022-07-19 19:17:31 | INFO | train_inner | epoch 006:    796 / 1122 loss=9.543, nll_loss=3.676, mask_ins=1.779, word_ins_ml=5.2, word_reposition=1.598, kpe=0.966, ppl=746.21, wps=6889.3, ups=0.34, wpb=19980.1, bsz=256, num_updates=6400, lr=0.000441942, gnorm=1.786, clip=0, loss_scale=512, train_wall=249, wall=21249
2022-07-19 19:22:20 | INFO | train_inner | epoch 006:    896 / 1122 loss=9.566, nll_loss=3.705, mask_ins=1.781, word_ins_ml=5.224, word_reposition=1.59, kpe=0.971, ppl=758.07, wps=6976.9, ups=0.35, wpb=20187.6, bsz=256, num_updates=6500, lr=0.000438529, gnorm=1.823, clip=0, loss_scale=543, train_wall=249, wall=21539
2022-07-19 19:27:10 | INFO | train_inner | epoch 006:    996 / 1122 loss=9.574, nll_loss=3.722, mask_ins=1.777, word_ins_ml=5.238, word_reposition=1.585, kpe=0.974, ppl=762.24, wps=6902.1, ups=0.35, wpb=19992.1, bsz=256, num_updates=6600, lr=0.000435194, gnorm=1.88, clip=0, loss_scale=1024, train_wall=249, wall=21828
2022-07-19 19:32:00 | INFO | train_inner | epoch 006:   1096 / 1122 loss=9.541, nll_loss=3.682, mask_ins=1.774, word_ins_ml=5.203, word_reposition=1.59, kpe=0.973, ppl=744.95, wps=6887.7, ups=0.34, wpb=19986.7, bsz=256, num_updates=6700, lr=0.000431934, gnorm=1.794, clip=0, loss_scale=1024, train_wall=249, wall=22119
2022-07-19 19:33:14 | INFO | train | epoch 006 | loss 9.656 | nll_loss 3.763 | mask_ins 1.802 | word_ins_ml 5.278 | word_reposition 1.598 | kpe 0.977 | ppl 806.75 | wps 6569 | ups 0.33 | wpb 20006.4 | bsz 255.8 | num_updates 6726 | lr 0.000431099 | gnorm 1.921 | clip 0 | loss_scale 790 | train_wall 2859 | wall 22193
2022-07-19 19:34:34 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 15.838 | nll_loss 7.834 | mask_ins 2.658 | word_ins_ml 9.022 | word_reposition 2.767 | kpe 1.39 | ppl 58569.4 | wps 12138.2 | wpb 2325.1 | bsz 32 | num_updates 6726 | best_loss 15.838
2022-07-19 19:34:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_best.pt (epoch 6 @ 6726 updates, score 15.838) (writing took 6.147857722826302 seconds)
2022-07-19 19:38:15 | INFO | train_inner | epoch 007:     74 / 1122 loss=9.549, nll_loss=3.703, mask_ins=1.778, word_ins_ml=5.222, word_reposition=1.585, kpe=0.964, ppl=749.18, wps=5270.7, ups=0.27, wpb=19774.1, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=1.863, clip=0, loss_scale=1024, train_wall=249, wall=22494
2022-07-19 19:43:04 | INFO | train_inner | epoch 007:    174 / 1122 loss=9.452, nll_loss=3.629, mask_ins=1.761, word_ins_ml=5.155, word_reposition=1.58, kpe=0.955, ppl=700.41, wps=6956.6, ups=0.35, wpb=20099.1, bsz=256, num_updates=6900, lr=0.000425628, gnorm=1.775, clip=0, loss_scale=1024, train_wall=248, wall=22783
2022-07-19 19:47:54 | INFO | train_inner | epoch 007:    274 / 1122 loss=9.433, nll_loss=3.635, mask_ins=1.761, word_ins_ml=5.16, word_reposition=1.558, kpe=0.954, ppl=691.27, wps=6955.9, ups=0.34, wpb=20166.6, bsz=256, num_updates=7000, lr=0.000422577, gnorm=1.727, clip=0, loss_scale=1024, train_wall=249, wall=23073
2022-07-19 19:53:19 | INFO | train_inner | epoch 007:    374 / 1122 loss=9.467, nll_loss=3.646, mask_ins=1.764, word_ins_ml=5.169, word_reposition=1.572, kpe=0.963, ppl=707.67, wps=6140.8, ups=0.31, wpb=19981.2, bsz=256, num_updates=7100, lr=0.000419591, gnorm=1.744, clip=0, loss_scale=1987, train_wall=284, wall=23398
2022-07-19 19:58:10 | INFO | train_inner | epoch 007:    474 / 1122 loss=9.423, nll_loss=3.597, mask_ins=1.76, word_ins_ml=5.126, word_reposition=1.581, kpe=0.955, ppl=686.29, wps=6887.4, ups=0.34, wpb=20014.7, bsz=256, num_updates=7200, lr=0.000416667, gnorm=1.802, clip=0, loss_scale=2048, train_wall=249, wall=23689
2022-07-19 20:03:00 | INFO | train_inner | epoch 007:    574 / 1122 loss=9.379, nll_loss=3.589, mask_ins=1.755, word_ins_ml=5.118, word_reposition=1.547, kpe=0.958, ppl=665.65, wps=6857.7, ups=0.34, wpb=19918.7, bsz=256, num_updates=7300, lr=0.000413803, gnorm=1.749, clip=0, loss_scale=2048, train_wall=249, wall=23979
2022-07-19 20:07:50 | INFO | train_inner | epoch 007:    674 / 1122 loss=9.36, nll_loss=3.586, mask_ins=1.755, word_ins_ml=5.115, word_reposition=1.532, kpe=0.957, ppl=657.11, wps=6915.1, ups=0.35, wpb=20014.3, bsz=256, num_updates=7400, lr=0.000410997, gnorm=1.741, clip=0, loss_scale=2048, train_wall=249, wall=24268
2022-07-19 20:12:40 | INFO | train_inner | epoch 007:    774 / 1122 loss=9.399, nll_loss=3.583, mask_ins=1.762, word_ins_ml=5.112, word_reposition=1.569, kpe=0.957, ppl=675.26, wps=6908.9, ups=0.35, wpb=20023.6, bsz=256, num_updates=7500, lr=0.000408248, gnorm=1.727, clip=0, loss_scale=2048, train_wall=249, wall=24558
2022-07-19 20:16:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-19 20:17:32 | INFO | train_inner | epoch 007:    875 / 1122 loss=9.36, nll_loss=3.587, mask_ins=1.736, word_ins_ml=5.115, word_reposition=1.556, kpe=0.953, ppl=657.19, wps=6850, ups=0.34, wpb=20053.6, bsz=256, num_updates=7600, lr=0.000405554, gnorm=1.675, clip=0, loss_scale=3163, train_wall=252, wall=24851
2022-07-19 20:17:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-19 20:22:26 | INFO | train_inner | epoch 007:    976 / 1122 loss=9.296, nll_loss=3.537, mask_ins=1.74, word_ins_ml=5.07, word_reposition=1.529, kpe=0.956, ppl=628.46, wps=6841.2, ups=0.34, wpb=20046.8, bsz=256, num_updates=7700, lr=0.000402911, gnorm=1.707, clip=0, loss_scale=1105, train_wall=251, wall=25144
2022-07-19 20:27:16 | INFO | train_inner | epoch 007:   1076 / 1122 loss=9.336, nll_loss=3.557, mask_ins=1.738, word_ins_ml=5.088, word_reposition=1.557, kpe=0.954, ppl=646.38, wps=6889.4, ups=0.34, wpb=20014.4, bsz=256, num_updates=7800, lr=0.00040032, gnorm=1.708, clip=0, loss_scale=1024, train_wall=249, wall=25435
2022-07-19 20:29:28 | INFO | train | epoch 007 | loss 9.399 | nll_loss 3.601 | mask_ins 1.755 | word_ins_ml 5.128 | word_reposition 1.559 | kpe 0.957 | ppl 675.17 | wps 6640.6 | ups 0.33 | wpb 20004.4 | bsz 255.8 | num_updates 7846 | lr 0.000399145 | gnorm 1.743 | clip 0 | loss_scale 1675 | train_wall 2828 | wall 25567
2022-07-19 20:30:49 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 15.129 | nll_loss 7.441 | mask_ins 2.595 | word_ins_ml 8.64 | word_reposition 2.557 | kpe 1.338 | ppl 35833.5 | wps 12094.7 | wpb 2325.1 | bsz 32 | num_updates 7846 | best_loss 15.129
2022-07-19 20:30:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_best.pt (epoch 7 @ 7846 updates, score 15.129) (writing took 6.206375279463828 seconds)
2022-07-19 20:33:32 | INFO | train_inner | epoch 008:     54 / 1122 loss=9.298, nll_loss=3.544, mask_ins=1.74, word_ins_ml=5.077, word_reposition=1.536, kpe=0.946, ppl=629.63, wps=5283.8, ups=0.27, wpb=19852.1, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=1.776, clip=0, loss_scale=1024, train_wall=248, wall=25810
2022-07-19 20:37:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-19 20:38:25 | INFO | train_inner | epoch 008:    155 / 1122 loss=9.179, nll_loss=3.462, mask_ins=1.722, word_ins_ml=5.003, word_reposition=1.512, kpe=0.942, ppl=579.7, wps=6831.2, ups=0.34, wpb=20045.7, bsz=256, num_updates=8000, lr=0.000395285, gnorm=1.73, clip=0, loss_scale=928, train_wall=252, wall=26104
2022-07-19 20:43:16 | INFO | train_inner | epoch 008:    255 / 1122 loss=9.273, nll_loss=3.534, mask_ins=1.719, word_ins_ml=5.067, word_reposition=1.542, kpe=0.945, ppl=618.75, wps=6897.1, ups=0.34, wpb=20020.7, bsz=256, num_updates=8100, lr=0.000392837, gnorm=1.739, clip=0, loss_scale=512, train_wall=249, wall=26394
2022-07-19 20:48:06 | INFO | train_inner | epoch 008:    355 / 1122 loss=9.272, nll_loss=3.51, mask_ins=1.73, word_ins_ml=5.045, word_reposition=1.553, kpe=0.944, ppl=618.11, wps=6892, ups=0.34, wpb=20006.2, bsz=256, num_updates=8200, lr=0.000390434, gnorm=1.726, clip=0, loss_scale=512, train_wall=249, wall=26684
2022-07-19 20:52:56 | INFO | train_inner | epoch 008:    455 / 1122 loss=9.223, nll_loss=3.507, mask_ins=1.714, word_ins_ml=5.042, word_reposition=1.527, kpe=0.94, ppl=597.56, wps=6883.4, ups=0.34, wpb=19993.5, bsz=256, num_updates=8300, lr=0.000388075, gnorm=1.704, clip=0, loss_scale=512, train_wall=250, wall=26975
2022-07-19 20:58:19 | INFO | train_inner | epoch 008:    555 / 1122 loss=9.24, nll_loss=3.488, mask_ins=1.719, word_ins_ml=5.024, word_reposition=1.553, kpe=0.944, ppl=604.86, wps=6218.5, ups=0.31, wpb=20082, bsz=256, num_updates=8400, lr=0.000385758, gnorm=1.695, clip=0, loss_scale=512, train_wall=281, wall=27298
2022-07-19 21:03:09 | INFO | train_inner | epoch 008:    655 / 1122 loss=9.185, nll_loss=3.475, mask_ins=1.711, word_ins_ml=5.013, word_reposition=1.522, kpe=0.94, ppl=582.13, wps=6932.4, ups=0.34, wpb=20122, bsz=256, num_updates=8500, lr=0.000383482, gnorm=1.654, clip=0, loss_scale=548, train_wall=249, wall=27588
2022-07-19 21:06:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-19 21:08:02 | INFO | train_inner | epoch 008:    756 / 1122 loss=9.208, nll_loss=3.45, mask_ins=1.714, word_ins_ml=4.99, word_reposition=1.558, kpe=0.945, ppl=591.44, wps=6820.3, ups=0.34, wpb=19948.1, bsz=256, num_updates=8600, lr=0.000381246, gnorm=1.739, clip=0, loss_scale=852, train_wall=251, wall=27881
2022-07-19 21:12:52 | INFO | train_inner | epoch 008:    856 / 1122 loss=9.183, nll_loss=3.495, mask_ins=1.699, word_ins_ml=5.03, word_reposition=1.515, kpe=0.939, ppl=581.24, wps=6932.6, ups=0.35, wpb=20075.8, bsz=256, num_updates=8700, lr=0.000379049, gnorm=1.59, clip=0, loss_scale=512, train_wall=249, wall=28170
2022-07-19 21:17:41 | INFO | train_inner | epoch 008:    956 / 1122 loss=9.191, nll_loss=3.469, mask_ins=1.711, word_ins_ml=5.007, word_reposition=1.527, kpe=0.946, ppl=584.28, wps=6895.2, ups=0.34, wpb=19989.5, bsz=256, num_updates=8800, lr=0.000376889, gnorm=1.879, clip=0, loss_scale=512, train_wall=249, wall=28460
2022-07-19 21:22:31 | INFO | train_inner | epoch 008:   1056 / 1122 loss=9.168, nll_loss=3.46, mask_ins=1.706, word_ins_ml=4.998, word_reposition=1.528, kpe=0.936, ppl=575.35, wps=6853.8, ups=0.35, wpb=19852.7, bsz=256, num_updates=8900, lr=0.000374766, gnorm=1.645, clip=0, loss_scale=512, train_wall=249, wall=28750
2022-07-19 21:25:42 | INFO | train | epoch 008 | loss 9.21 | nll_loss 3.484 | mask_ins 1.715 | word_ins_ml 5.021 | word_reposition 1.532 | kpe 0.942 | ppl 592.23 | wps 6642.2 | ups 0.33 | wpb 20007.1 | bsz 255.8 | num_updates 8966 | lr 0.000373384 | gnorm 1.71 | clip 0 | loss_scale 608 | train_wall 2827 | wall 28940
2022-07-19 21:27:02 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 15.364 | nll_loss 7.524 | mask_ins 2.636 | word_ins_ml 8.719 | word_reposition 2.647 | kpe 1.362 | ppl 42185.2 | wps 12149.4 | wpb 2325.1 | bsz 32 | num_updates 8966 | best_loss 15.129
2022-07-19 21:27:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 8 @ 8966 updates, score 15.364) (writing took 3.352746789343655 seconds)
2022-07-19 21:28:43 | INFO | train_inner | epoch 009:     34 / 1122 loss=9.126, nll_loss=3.444, mask_ins=1.703, word_ins_ml=4.984, word_reposition=1.504, kpe=0.935, ppl=558.66, wps=5322.8, ups=0.27, wpb=19809.2, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=1.794, clip=0, loss_scale=512, train_wall=248, wall=29122
2022-07-19 21:33:34 | INFO | train_inner | epoch 009:    134 / 1122 loss=9.084, nll_loss=3.398, mask_ins=1.696, word_ins_ml=4.944, word_reposition=1.511, kpe=0.934, ppl=542.74, wps=6930.9, ups=0.34, wpb=20177.5, bsz=256, num_updates=9100, lr=0.000370625, gnorm=2.046, clip=0, loss_scale=625, train_wall=250, wall=29413
2022-07-19 21:38:24 | INFO | train_inner | epoch 009:    234 / 1122 loss=9.091, nll_loss=3.407, mask_ins=1.699, word_ins_ml=4.951, word_reposition=1.513, kpe=0.927, ppl=545.18, wps=6891.8, ups=0.34, wpb=19977, bsz=256, num_updates=9200, lr=0.000368605, gnorm=1.581, clip=0, loss_scale=1024, train_wall=249, wall=29703
2022-07-19 21:42:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-19 21:43:17 | INFO | train_inner | epoch 009:    335 / 1122 loss=9.147, nll_loss=3.458, mask_ins=1.703, word_ins_ml=4.996, word_reposition=1.516, kpe=0.932, ppl=566.84, wps=6803.2, ups=0.34, wpb=19925.2, bsz=256, num_updates=9300, lr=0.000366618, gnorm=1.741, clip=0, loss_scale=943, train_wall=251, wall=29996
2022-07-19 21:48:07 | INFO | train_inner | epoch 009:    435 / 1122 loss=9.102, nll_loss=3.416, mask_ins=1.691, word_ins_ml=4.958, word_reposition=1.518, kpe=0.934, ppl=549.44, wps=6893.1, ups=0.35, wpb=19979.5, bsz=256, num_updates=9400, lr=0.000364662, gnorm=1.898, clip=0, loss_scale=512, train_wall=249, wall=30286
2022-07-19 21:52:57 | INFO | train_inner | epoch 009:    535 / 1122 loss=9.118, nll_loss=3.433, mask_ins=1.696, word_ins_ml=4.974, word_reposition=1.516, kpe=0.933, ppl=555.61, wps=6890.2, ups=0.34, wpb=19977.5, bsz=256, num_updates=9500, lr=0.000362738, gnorm=1.726, clip=0, loss_scale=512, train_wall=249, wall=30576
2022-07-19 21:57:47 | INFO | train_inner | epoch 009:    635 / 1122 loss=9.098, nll_loss=3.409, mask_ins=1.692, word_ins_ml=4.952, word_reposition=1.527, kpe=0.927, ppl=547.85, wps=6906.9, ups=0.34, wpb=20029.1, bsz=256, num_updates=9600, lr=0.000360844, gnorm=1.675, clip=0, loss_scale=512, train_wall=249, wall=30866
2022-07-19 22:03:11 | INFO | train_inner | epoch 009:    735 / 1122 loss=9.026, nll_loss=3.364, mask_ins=1.694, word_ins_ml=4.912, word_reposition=1.491, kpe=0.93, ppl=521.44, wps=6174.5, ups=0.31, wpb=20027.7, bsz=256, num_updates=9700, lr=0.000358979, gnorm=1.675, clip=0, loss_scale=512, train_wall=283, wall=31190
2022-07-19 22:08:01 | INFO | train_inner | epoch 009:    835 / 1122 loss=9.072, nll_loss=3.41, mask_ins=1.688, word_ins_ml=4.953, word_reposition=1.499, kpe=0.933, ppl=538.34, wps=6886.9, ups=0.34, wpb=19980.8, bsz=256, num_updates=9800, lr=0.000357143, gnorm=1.716, clip=0, loss_scale=532, train_wall=249, wall=31480
2022-07-19 22:12:51 | INFO | train_inner | epoch 009:    935 / 1122 loss=9.045, nll_loss=3.392, mask_ins=1.686, word_ins_ml=4.937, word_reposition=1.494, kpe=0.929, ppl=528.11, wps=6920.7, ups=0.35, wpb=20053.1, bsz=256, num_updates=9900, lr=0.000355335, gnorm=1.651, clip=0, loss_scale=1024, train_wall=249, wall=31770
2022-07-19 22:17:41 | INFO | train_inner | epoch 009:   1035 / 1122 loss=9.085, nll_loss=3.401, mask_ins=1.686, word_ins_ml=4.944, word_reposition=1.524, kpe=0.931, ppl=543.11, wps=6915.3, ups=0.35, wpb=20027.2, bsz=256, num_updates=10000, lr=0.000353553, gnorm=1.729, clip=0, loss_scale=1024, train_wall=249, wall=32059
2022-07-19 22:21:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-19 22:21:52 | INFO | train | epoch 009 | loss 9.086 | nll_loss 3.409 | mask_ins 1.693 | word_ins_ml 4.952 | word_reposition 1.511 | kpe 0.931 | ppl 543.57 | wps 6648.5 | ups 0.33 | wpb 20006.5 | bsz 255.8 | num_updates 10086 | lr 0.000352043 | gnorm 1.748 | clip 0 | loss_scale 737 | train_wall 2827 | wall 32311
2022-07-19 22:23:12 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 15.335 | nll_loss 7.385 | mask_ins 2.646 | word_ins_ml 8.599 | word_reposition 2.709 | kpe 1.382 | ppl 41327.1 | wps 12138 | wpb 2325.1 | bsz 32 | num_updates 10086 | best_loss 15.129
2022-07-19 22:23:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 9 @ 10086 updates, score 15.335) (writing took 3.3485228335484862 seconds)
2022-07-19 22:23:56 | INFO | train_inner | epoch 010:     14 / 1122 loss=9.069, nll_loss=3.389, mask_ins=1.692, word_ins_ml=4.933, word_reposition=1.517, kpe=0.927, ppl=537.07, wps=5322.3, ups=0.27, wpb=19990.8, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=1.739, clip=0, loss_scale=928, train_wall=251, wall=32435
2022-07-19 22:28:46 | INFO | train_inner | epoch 010:    114 / 1122 loss=8.968, nll_loss=3.321, mask_ins=1.678, word_ins_ml=4.873, word_reposition=1.498, kpe=0.919, ppl=500.83, wps=6924.6, ups=0.35, wpb=20052.5, bsz=256, num_updates=10200, lr=0.00035007, gnorm=1.637, clip=0, loss_scale=512, train_wall=249, wall=32725
2022-07-19 22:33:35 | INFO | train_inner | epoch 010:    214 / 1122 loss=8.99, nll_loss=3.355, mask_ins=1.673, word_ins_ml=4.903, word_reposition=1.493, kpe=0.922, ppl=508.53, wps=6905.6, ups=0.35, wpb=19979, bsz=256, num_updates=10300, lr=0.000348367, gnorm=1.719, clip=0, loss_scale=512, train_wall=249, wall=33014
2022-07-19 22:38:25 | INFO | train_inner | epoch 010:    314 / 1122 loss=8.99, nll_loss=3.338, mask_ins=1.683, word_ins_ml=4.888, word_reposition=1.496, kpe=0.923, ppl=508.54, wps=6932.8, ups=0.35, wpb=20051.2, bsz=256, num_updates=10400, lr=0.000346688, gnorm=1.608, clip=0, loss_scale=512, train_wall=248, wall=33303
2022-07-19 22:43:14 | INFO | train_inner | epoch 010:    414 / 1122 loss=8.969, nll_loss=3.332, mask_ins=1.676, word_ins_ml=4.882, word_reposition=1.487, kpe=0.924, ppl=501.29, wps=6934, ups=0.35, wpb=20062.5, bsz=256, num_updates=10500, lr=0.000345033, gnorm=1.855, clip=0, loss_scale=512, train_wall=249, wall=33593
2022-07-19 22:47:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-19 22:48:07 | INFO | train_inner | epoch 010:    515 / 1122 loss=8.976, nll_loss=3.349, mask_ins=1.666, word_ins_ml=4.897, word_reposition=1.491, kpe=0.921, ppl=503.43, wps=6817.6, ups=0.34, wpb=19971.3, bsz=256, num_updates=10600, lr=0.000343401, gnorm=1.602, clip=0, loss_scale=522, train_wall=252, wall=33885
2022-07-19 22:52:57 | INFO | train_inner | epoch 010:    615 / 1122 loss=9.007, nll_loss=3.349, mask_ins=1.682, word_ins_ml=4.897, word_reposition=1.508, kpe=0.92, ppl=514.54, wps=6946.7, ups=0.34, wpb=20151.8, bsz=256, num_updates=10700, lr=0.000341793, gnorm=1.78, clip=0, loss_scale=512, train_wall=249, wall=34176
2022-07-19 22:57:47 | INFO | train_inner | epoch 010:    715 / 1122 loss=8.985, nll_loss=3.335, mask_ins=1.683, word_ins_ml=4.884, word_reposition=1.498, kpe=0.92, ppl=506.76, wps=6864.8, ups=0.34, wpb=19928.9, bsz=256, num_updates=10800, lr=0.000340207, gnorm=1.61, clip=0, loss_scale=512, train_wall=249, wall=34466
2022-07-19 23:02:38 | INFO | train_inner | epoch 010:    815 / 1122 loss=8.891, nll_loss=3.313, mask_ins=1.643, word_ins_ml=4.864, word_reposition=1.469, kpe=0.915, ppl=474.59, wps=6896.8, ups=0.34, wpb=20038.5, bsz=256, num_updates=10900, lr=0.000338643, gnorm=1.551, clip=0, loss_scale=512, train_wall=249, wall=34756
2022-07-19 23:08:01 | INFO | train_inner | epoch 010:    915 / 1122 loss=8.928, nll_loss=3.297, mask_ins=1.67, word_ins_ml=4.85, word_reposition=1.493, kpe=0.915, ppl=486.95, wps=6188.6, ups=0.31, wpb=20026.5, bsz=256, num_updates=11000, lr=0.0003371, gnorm=1.664, clip=0, loss_scale=512, train_wall=282, wall=35080
2022-07-19 23:12:52 | INFO | train_inner | epoch 010:   1015 / 1122 loss=8.935, nll_loss=3.316, mask_ins=1.658, word_ins_ml=4.866, word_reposition=1.494, kpe=0.917, ppl=489.5, wps=6876.3, ups=0.34, wpb=19955.5, bsz=256, num_updates=11100, lr=0.000335578, gnorm=1.573, clip=0, loss_scale=512, train_wall=249, wall=35370
2022-07-19 23:17:43 | INFO | train_inner | epoch 010:   1115 / 1122 loss=8.905, nll_loss=3.3, mask_ins=1.651, word_ins_ml=4.853, word_reposition=1.489, kpe=0.912, ppl=479.33, wps=6868.8, ups=0.34, wpb=19976.5, bsz=256, num_updates=11200, lr=0.000334077, gnorm=1.583, clip=0, loss_scale=993, train_wall=250, wall=35661
2022-07-19 23:18:02 | INFO | train | epoch 010 | loss 8.961 | nll_loss 3.329 | mask_ins 1.67 | word_ins_ml 4.879 | word_reposition 1.493 | kpe 0.919 | ppl 498.37 | wps 6655.1 | ups 0.33 | wpb 20005.9 | bsz 255.8 | num_updates 11207 | lr 0.000333972 | gnorm 1.663 | clip 0 | loss_scale 559 | train_wall 2827 | wall 35681
2022-07-19 23:19:22 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 15.364 | nll_loss 7.352 | mask_ins 2.685 | word_ins_ml 8.57 | word_reposition 2.703 | kpe 1.406 | ppl 42164.9 | wps 12167.6 | wpb 2325.1 | bsz 32 | num_updates 11207 | best_loss 15.129
2022-07-19 23:19:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 10 @ 11207 updates, score 15.364) (writing took 3.4205196406692266 seconds)
2022-07-19 23:23:55 | INFO | train_inner | epoch 011:     93 / 1122 loss=8.918, nll_loss=3.303, mask_ins=1.659, word_ins_ml=4.855, word_reposition=1.498, kpe=0.906, ppl=483.8, wps=5329.8, ups=0.27, wpb=19829.8, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=1.682, clip=0, loss_scale=1024, train_wall=248, wall=36033
2022-07-19 23:27:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-19 23:28:47 | INFO | train_inner | epoch 011:    194 / 1122 loss=8.898, nll_loss=3.286, mask_ins=1.654, word_ins_ml=4.84, word_reposition=1.495, kpe=0.909, ppl=477.12, wps=6828.7, ups=0.34, wpb=19978.5, bsz=256, num_updates=11400, lr=0.000331133, gnorm=1.881, clip=0, loss_scale=862, train_wall=251, wall=36326
2022-07-19 23:33:37 | INFO | train_inner | epoch 011:    294 / 1122 loss=8.882, nll_loss=3.3, mask_ins=1.645, word_ins_ml=4.853, word_reposition=1.476, kpe=0.908, ppl=471.71, wps=6901.2, ups=0.34, wpb=20026.7, bsz=256, num_updates=11500, lr=0.00032969, gnorm=1.63, clip=0, loss_scale=512, train_wall=249, wall=36616
2022-07-19 23:38:28 | INFO | train_inner | epoch 011:    394 / 1122 loss=8.856, nll_loss=3.263, mask_ins=1.648, word_ins_ml=4.819, word_reposition=1.486, kpe=0.903, ppl=463.5, wps=6893.6, ups=0.34, wpb=20031.6, bsz=256, num_updates=11600, lr=0.000328266, gnorm=1.606, clip=0, loss_scale=512, train_wall=250, wall=36907
2022-07-19 23:43:18 | INFO | train_inner | epoch 011:    494 / 1122 loss=8.857, nll_loss=3.263, mask_ins=1.64, word_ins_ml=4.819, word_reposition=1.489, kpe=0.91, ppl=463.82, wps=6939, ups=0.34, wpb=20128.2, bsz=256, num_updates=11700, lr=0.00032686, gnorm=1.595, clip=0, loss_scale=512, train_wall=249, wall=37197
2022-07-19 23:48:08 | INFO | train_inner | epoch 011:    594 / 1122 loss=8.801, nll_loss=3.24, mask_ins=1.64, word_ins_ml=4.799, word_reposition=1.454, kpe=0.908, ppl=445.91, wps=6902.5, ups=0.35, wpb=20004.2, bsz=256, num_updates=11800, lr=0.000325472, gnorm=1.574, clip=0, loss_scale=512, train_wall=249, wall=37486
2022-07-19 23:52:58 | INFO | train_inner | epoch 011:    694 / 1122 loss=8.841, nll_loss=3.252, mask_ins=1.645, word_ins_ml=4.809, word_reposition=1.477, kpe=0.909, ppl=458.5, wps=6893.3, ups=0.34, wpb=19997, bsz=256, num_updates=11900, lr=0.000324102, gnorm=1.547, clip=0, loss_scale=614, train_wall=249, wall=37777
2022-07-19 23:57:48 | INFO | train_inner | epoch 011:    794 / 1122 loss=8.772, nll_loss=3.22, mask_ins=1.632, word_ins_ml=4.781, word_reposition=1.453, kpe=0.906, ppl=437.09, wps=6864.9, ups=0.35, wpb=19888.8, bsz=256, num_updates=12000, lr=0.000322749, gnorm=1.618, clip=0, loss_scale=1024, train_wall=249, wall=38066
2022-07-20 00:02:39 | INFO | train_inner | epoch 011:    894 / 1122 loss=8.818, nll_loss=3.247, mask_ins=1.635, word_ins_ml=4.804, word_reposition=1.469, kpe=0.91, ppl=451.36, wps=6911.6, ups=0.34, wpb=20113.8, bsz=256, num_updates=12100, lr=0.000321412, gnorm=1.553, clip=0, loss_scale=1024, train_wall=250, wall=38357
2022-07-20 00:07:29 | INFO | train_inner | epoch 011:    994 / 1122 loss=8.849, nll_loss=3.267, mask_ins=1.642, word_ins_ml=4.822, word_reposition=1.475, kpe=0.911, ppl=461.1, wps=6888.7, ups=0.34, wpb=20011.2, bsz=256, num_updates=12200, lr=0.000320092, gnorm=1.534, clip=0, loss_scale=1024, train_wall=250, wall=38648
2022-07-20 00:12:44 | INFO | train_inner | epoch 011:   1094 / 1122 loss=8.847, nll_loss=3.268, mask_ins=1.636, word_ins_ml=4.823, word_reposition=1.477, kpe=0.91, ppl=460.4, wps=6368.1, ups=0.32, wpb=20060.3, bsz=256, num_updates=12300, lr=0.000318788, gnorm=1.559, clip=0, loss_scale=1024, train_wall=274, wall=38963
2022-07-20 00:14:04 | INFO | train | epoch 011 | loss 8.849 | nll_loss 3.265 | mask_ins 1.643 | word_ins_ml 4.82 | word_reposition 1.477 | kpe 0.908 | ppl 461.04 | wps 6670.7 | ups 0.33 | wpb 20007.8 | bsz 255.8 | num_updates 12328 | lr 0.000318426 | gnorm 1.617 | clip 0 | loss_scale 790 | train_wall 2820 | wall 39043
2022-07-20 00:15:24 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 15.52 | nll_loss 7.464 | mask_ins 2.71 | word_ins_ml 8.672 | word_reposition 2.738 | kpe 1.401 | ppl 46990.3 | wps 12120.3 | wpb 2325.1 | bsz 32 | num_updates 12328 | best_loss 15.129
2022-07-20 00:15:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 11 @ 12328 updates, score 15.52) (writing took 3.377370254136622 seconds)
2022-07-20 00:18:57 | INFO | train_inner | epoch 012:     72 / 1122 loss=8.84, nll_loss=3.254, mask_ins=1.642, word_ins_ml=4.811, word_reposition=1.485, kpe=0.903, ppl=458.23, wps=5343, ups=0.27, wpb=19901.4, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=1.648, clip=0, loss_scale=1106, train_wall=248, wall=39335
2022-07-20 00:23:47 | INFO | train_inner | epoch 012:    172 / 1122 loss=8.774, nll_loss=3.211, mask_ins=1.633, word_ins_ml=4.772, word_reposition=1.473, kpe=0.896, ppl=437.82, wps=6897.3, ups=0.34, wpb=20005.4, bsz=256, num_updates=12500, lr=0.000316228, gnorm=1.567, clip=0, loss_scale=2048, train_wall=249, wall=39625
2022-07-20 00:28:36 | INFO | train_inner | epoch 012:    272 / 1122 loss=8.797, nll_loss=3.246, mask_ins=1.625, word_ins_ml=4.803, word_reposition=1.474, kpe=0.894, ppl=444.66, wps=6898, ups=0.35, wpb=19976.3, bsz=256, num_updates=12600, lr=0.00031497, gnorm=1.584, clip=0, loss_scale=2048, train_wall=249, wall=39915
2022-07-20 00:31:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-20 00:33:29 | INFO | train_inner | epoch 012:    373 / 1122 loss=8.753, nll_loss=3.218, mask_ins=1.617, word_ins_ml=4.778, word_reposition=1.461, kpe=0.897, ppl=431.4, wps=6844.7, ups=0.34, wpb=20021.6, bsz=256, num_updates=12700, lr=0.000313728, gnorm=1.661, clip=0, loss_scale=1693, train_wall=251, wall=40207
2022-07-20 00:38:18 | INFO | train_inner | epoch 012:    473 / 1122 loss=8.728, nll_loss=3.215, mask_ins=1.617, word_ins_ml=4.775, word_reposition=1.442, kpe=0.895, ppl=424.09, wps=6911.8, ups=0.35, wpb=19972.7, bsz=256, num_updates=12800, lr=0.0003125, gnorm=1.664, clip=0, loss_scale=1024, train_wall=248, wall=40496
2022-07-20 00:43:08 | INFO | train_inner | epoch 012:    573 / 1122 loss=8.703, nll_loss=3.164, mask_ins=1.613, word_ins_ml=4.73, word_reposition=1.466, kpe=0.894, ppl=416.67, wps=6944.1, ups=0.34, wpb=20132.9, bsz=256, num_updates=12900, lr=0.000311286, gnorm=1.56, clip=0, loss_scale=1024, train_wall=249, wall=40786
2022-07-20 00:47:57 | INFO | train_inner | epoch 012:    673 / 1122 loss=8.737, nll_loss=3.197, mask_ins=1.62, word_ins_ml=4.759, word_reposition=1.457, kpe=0.901, ppl=426.64, wps=6874.8, ups=0.35, wpb=19888.1, bsz=256, num_updates=13000, lr=0.000310087, gnorm=1.591, clip=0, loss_scale=1024, train_wall=248, wall=41076
2022-07-20 00:52:47 | INFO | train_inner | epoch 012:    773 / 1122 loss=8.734, nll_loss=3.201, mask_ins=1.624, word_ins_ml=4.762, word_reposition=1.446, kpe=0.903, ppl=425.84, wps=6927.8, ups=0.34, wpb=20088.9, bsz=256, num_updates=13100, lr=0.000308901, gnorm=1.598, clip=0, loss_scale=1024, train_wall=249, wall=41366
2022-07-20 00:57:37 | INFO | train_inner | epoch 012:    873 / 1122 loss=8.74, nll_loss=3.193, mask_ins=1.627, word_ins_ml=4.756, word_reposition=1.458, kpe=0.9, ppl=427.58, wps=6916.7, ups=0.35, wpb=20039.8, bsz=256, num_updates=13200, lr=0.000307729, gnorm=1.595, clip=0, loss_scale=1260, train_wall=249, wall=41655
2022-07-20 01:02:26 | INFO | train_inner | epoch 012:    973 / 1122 loss=8.739, nll_loss=3.197, mask_ins=1.626, word_ins_ml=4.759, word_reposition=1.456, kpe=0.899, ppl=427.41, wps=6901.7, ups=0.35, wpb=19971.2, bsz=256, num_updates=13300, lr=0.00030657, gnorm=1.548, clip=0, loss_scale=2048, train_wall=249, wall=41945
2022-07-20 01:03:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-20 01:07:19 | INFO | train_inner | epoch 012:   1074 / 1122 loss=8.77, nll_loss=3.208, mask_ins=1.621, word_ins_ml=4.768, word_reposition=1.48, kpe=0.901, ppl=436.52, wps=6839.9, ups=0.34, wpb=20041, bsz=256, num_updates=13400, lr=0.000305424, gnorm=1.546, clip=0, loss_scale=1186, train_wall=252, wall=42238
2022-07-20 01:09:37 | INFO | train | epoch 012 | loss 8.755 | nll_loss 3.209 | mask_ins 1.623 | word_ins_ml 4.77 | word_reposition 1.464 | kpe 0.898 | ppl 431.99 | wps 6722.4 | ups 0.34 | wpb 20005.5 | bsz 255.8 | num_updates 13448 | lr 0.000304878 | gnorm 1.592 | clip 0 | loss_scale 1401 | train_wall 2790 | wall 42376
2022-07-20 01:10:57 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 15.093 | nll_loss 7.249 | mask_ins 2.632 | word_ins_ml 8.47 | word_reposition 2.604 | kpe 1.387 | ppl 34953.8 | wps 12192.2 | wpb 2325.1 | bsz 32 | num_updates 13448 | best_loss 15.093
2022-07-20 01:11:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_best.pt (epoch 12 @ 13448 updates, score 15.093) (writing took 5.874617798253894 seconds)
2022-07-20 01:13:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-20 01:13:49 | INFO | train_inner | epoch 013:     53 / 1122 loss=8.77, nll_loss=3.207, mask_ins=1.627, word_ins_ml=4.768, word_reposition=1.48, kpe=0.895, ppl=436.51, wps=5086.7, ups=0.26, wpb=19827.7, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=1.618, clip=0, loss_scale=999, train_wall=263, wall=42628
2022-07-20 01:19:14 | INFO | train_inner | epoch 013:    153 / 1122 loss=8.694, nll_loss=3.163, mask_ins=1.611, word_ins_ml=4.729, word_reposition=1.468, kpe=0.886, ppl=414.27, wps=6176.7, ups=0.31, wpb=20069.2, bsz=256, num_updates=13600, lr=0.00030317, gnorm=1.572, clip=0, loss_scale=512, train_wall=283, wall=42952
2022-07-20 01:24:05 | INFO | train_inner | epoch 013:    253 / 1122 loss=8.686, nll_loss=3.174, mask_ins=1.617, word_ins_ml=4.738, word_reposition=1.446, kpe=0.885, ppl=411.89, wps=6859.3, ups=0.34, wpb=19951.7, bsz=256, num_updates=13700, lr=0.000302061, gnorm=1.553, clip=0, loss_scale=512, train_wall=249, wall=43243
2022-07-20 01:28:55 | INFO | train_inner | epoch 013:    353 / 1122 loss=8.705, nll_loss=3.199, mask_ins=1.609, word_ins_ml=4.761, word_reposition=1.446, kpe=0.889, ppl=417.19, wps=6914.6, ups=0.34, wpb=20050.3, bsz=256, num_updates=13800, lr=0.000300965, gnorm=1.639, clip=0, loss_scale=512, train_wall=249, wall=43533
2022-07-20 01:33:45 | INFO | train_inner | epoch 013:    453 / 1122 loss=8.677, nll_loss=3.147, mask_ins=1.615, word_ins_ml=4.714, word_reposition=1.462, kpe=0.886, ppl=409.16, wps=6921.5, ups=0.34, wpb=20062.7, bsz=256, num_updates=13900, lr=0.00029988, gnorm=1.607, clip=0, loss_scale=512, train_wall=249, wall=43823
2022-07-20 01:38:35 | INFO | train_inner | epoch 013:    553 / 1122 loss=8.71, nll_loss=3.2, mask_ins=1.614, word_ins_ml=4.761, word_reposition=1.446, kpe=0.889, ppl=418.87, wps=6915.5, ups=0.34, wpb=20069.5, bsz=256, num_updates=14000, lr=0.000298807, gnorm=1.653, clip=0, loss_scale=512, train_wall=250, wall=44113
2022-07-20 01:43:25 | INFO | train_inner | epoch 013:    653 / 1122 loss=8.693, nll_loss=3.18, mask_ins=1.605, word_ins_ml=4.742, word_reposition=1.455, kpe=0.891, ppl=413.97, wps=6869.6, ups=0.34, wpb=19943.8, bsz=256, num_updates=14100, lr=0.000297746, gnorm=1.636, clip=0, loss_scale=988, train_wall=249, wall=44404
2022-07-20 01:48:16 | INFO | train_inner | epoch 013:    753 / 1122 loss=8.703, nll_loss=3.198, mask_ins=1.589, word_ins_ml=4.759, word_reposition=1.463, kpe=0.893, ppl=416.83, wps=6885.5, ups=0.34, wpb=19993, bsz=256, num_updates=14200, lr=0.000296695, gnorm=1.774, clip=0, loss_scale=1024, train_wall=249, wall=44694
2022-07-20 01:53:06 | INFO | train_inner | epoch 013:    853 / 1122 loss=8.7, nll_loss=3.176, mask_ins=1.612, word_ins_ml=4.74, word_reposition=1.457, kpe=0.892, ppl=415.97, wps=6884.5, ups=0.34, wpb=20005.7, bsz=256, num_updates=14300, lr=0.000295656, gnorm=1.694, clip=0, loss_scale=1024, train_wall=249, wall=44985
2022-07-20 01:58:02 | INFO | train_inner | epoch 013:    953 / 1122 loss=8.603, nll_loss=3.108, mask_ins=1.602, word_ins_ml=4.678, word_reposition=1.433, kpe=0.89, ppl=388.95, wps=6753.2, ups=0.34, wpb=19962.9, bsz=256, num_updates=14400, lr=0.000294628, gnorm=1.615, clip=0, loss_scale=1024, train_wall=254, wall=45280
2022-07-20 02:02:56 | INFO | train_inner | epoch 013:   1053 / 1122 loss=8.673, nll_loss=3.176, mask_ins=1.598, word_ins_ml=4.739, word_reposition=1.445, kpe=0.891, ppl=408.11, wps=6827.1, ups=0.34, wpb=20102.7, bsz=256, num_updates=14500, lr=0.00029361, gnorm=1.55, clip=0, loss_scale=1024, train_wall=253, wall=45575
2022-07-20 02:06:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-20 02:06:17 | INFO | train | epoch 013 | loss 8.688 | nll_loss 3.173 | mask_ins 1.609 | word_ins_ml 4.737 | word_reposition 1.452 | kpe 0.89 | ppl 412.43 | wps 6590.6 | ups 0.33 | wpb 20003 | bsz 255.8 | num_updates 14568 | lr 0.000292924 | gnorm 1.635 | clip 0 | loss_scale 831 | train_wall 2851 | wall 45775
2022-07-20 02:07:37 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 15.328 | nll_loss 7.419 | mask_ins 2.666 | word_ins_ml 8.63 | word_reposition 2.653 | kpe 1.38 | ppl 41137.4 | wps 12089.2 | wpb 2325.1 | bsz 32 | num_updates 14568 | best_loss 15.093
2022-07-20 02:07:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 13 @ 14568 updates, score 15.328) (writing took 3.5184105318039656 seconds)
2022-07-20 02:09:14 | INFO | train_inner | epoch 014:     32 / 1122 loss=8.671, nll_loss=3.172, mask_ins=1.601, word_ins_ml=4.736, word_reposition=1.443, kpe=0.891, ppl=407.67, wps=5262, ups=0.26, wpb=19869.2, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=1.75, clip=0, loss_scale=1470, train_wall=252, wall=45952
2022-07-20 02:14:05 | INFO | train_inner | epoch 014:    132 / 1122 loss=8.614, nll_loss=3.129, mask_ins=1.603, word_ins_ml=4.698, word_reposition=1.434, kpe=0.879, ppl=391.76, wps=6869, ups=0.34, wpb=19994.5, bsz=256, num_updates=14700, lr=0.000291606, gnorm=1.619, clip=0, loss_scale=1024, train_wall=250, wall=46243
2022-07-20 02:19:20 | INFO | train_inner | epoch 014:    232 / 1122 loss=8.626, nll_loss=3.145, mask_ins=1.594, word_ins_ml=4.712, word_reposition=1.437, kpe=0.882, ppl=394.97, wps=6344.3, ups=0.32, wpb=19995.9, bsz=256, num_updates=14800, lr=0.000290619, gnorm=1.636, clip=0, loss_scale=1024, train_wall=274, wall=46559
2022-07-20 02:24:36 | INFO | train_inner | epoch 014:    332 / 1122 loss=8.653, nll_loss=3.147, mask_ins=1.603, word_ins_ml=4.713, word_reposition=1.455, kpe=0.882, ppl=402.55, wps=6341.3, ups=0.32, wpb=20042.4, bsz=256, num_updates=14900, lr=0.000289642, gnorm=1.619, clip=0, loss_scale=1024, train_wall=274, wall=46875
2022-07-20 02:29:31 | INFO | train_inner | epoch 014:    432 / 1122 loss=8.602, nll_loss=3.125, mask_ins=1.589, word_ins_ml=4.694, word_reposition=1.437, kpe=0.881, ppl=388.45, wps=6772.2, ups=0.34, wpb=19990.5, bsz=256, num_updates=15000, lr=0.000288675, gnorm=1.615, clip=0, loss_scale=1024, train_wall=253, wall=47170
2022-07-20 02:34:24 | INFO | train_inner | epoch 014:    532 / 1122 loss=8.598, nll_loss=3.119, mask_ins=1.591, word_ins_ml=4.688, word_reposition=1.44, kpe=0.879, ppl=387.54, wps=6845.4, ups=0.34, wpb=20014.2, bsz=256, num_updates=15100, lr=0.000287718, gnorm=1.535, clip=0, loss_scale=1290, train_wall=251, wall=47462
2022-07-20 02:39:16 | INFO | train_inner | epoch 014:    632 / 1122 loss=8.562, nll_loss=3.101, mask_ins=1.595, word_ins_ml=4.673, word_reposition=1.415, kpe=0.879, ppl=377.84, wps=6840.4, ups=0.34, wpb=19982.8, bsz=256, num_updates=15200, lr=0.00028677, gnorm=1.557, clip=0, loss_scale=2048, train_wall=250, wall=47754
2022-07-20 02:39:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-20 02:44:10 | INFO | train_inner | epoch 014:    733 / 1122 loss=8.586, nll_loss=3.107, mask_ins=1.582, word_ins_ml=4.677, word_reposition=1.445, kpe=0.881, ppl=384.22, wps=6826.9, ups=0.34, wpb=20112.4, bsz=256, num_updates=15300, lr=0.000285831, gnorm=1.588, clip=0, loss_scale=1115, train_wall=253, wall=48049
2022-07-20 02:49:01 | INFO | train_inner | epoch 014:    833 / 1122 loss=8.618, nll_loss=3.141, mask_ins=1.6, word_ins_ml=4.708, word_reposition=1.426, kpe=0.885, ppl=392.86, wps=6846.2, ups=0.34, wpb=19923.4, bsz=256, num_updates=15400, lr=0.000284901, gnorm=1.573, clip=0, loss_scale=1024, train_wall=250, wall=48340
2022-07-20 02:53:52 | INFO | train_inner | epoch 014:    933 / 1122 loss=8.585, nll_loss=3.103, mask_ins=1.588, word_ins_ml=4.674, word_reposition=1.439, kpe=0.884, ppl=383.97, wps=6879.2, ups=0.34, wpb=19991, bsz=256, num_updates=15500, lr=0.000283981, gnorm=1.811, clip=0, loss_scale=1024, train_wall=250, wall=48631
2022-07-20 02:58:50 | INFO | train_inner | epoch 014:   1033 / 1122 loss=8.595, nll_loss=3.114, mask_ins=1.593, word_ins_ml=4.684, word_reposition=1.436, kpe=0.882, ppl=386.68, wps=6713.3, ups=0.34, wpb=20006.7, bsz=256, num_updates=15600, lr=0.000283069, gnorm=1.67, clip=0, loss_scale=1024, train_wall=255, wall=48929
2022-07-20 03:03:08 | INFO | train | epoch 014 | loss 8.605 | nll_loss 3.124 | mask_ins 1.594 | word_ins_ml 4.693 | word_reposition 1.437 | kpe 0.881 | ppl 389.37 | wps 6573.5 | ups 0.33 | wpb 20005.7 | bsz 255.8 | num_updates 15689 | lr 0.000282265 | gnorm 1.637 | clip 0 | loss_scale 1147 | train_wall 2860 | wall 49187
2022-07-20 03:04:29 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 14.986 | nll_loss 7.224 | mask_ins 2.612 | word_ins_ml 8.44 | word_reposition 2.544 | kpe 1.389 | ppl 32444.6 | wps 12069 | wpb 2325.1 | bsz 32 | num_updates 15689 | best_loss 14.986
2022-07-20 03:04:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_best.pt (epoch 14 @ 15689 updates, score 14.986) (writing took 6.116221643052995 seconds)
2022-07-20 03:05:07 | INFO | train_inner | epoch 015:     11 / 1122 loss=8.637, nll_loss=3.14, mask_ins=1.6, word_ins_ml=4.707, word_reposition=1.449, kpe=0.881, ppl=397.99, wps=5293.9, ups=0.27, wpb=19966.2, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=1.773, clip=0, loss_scale=1024, train_wall=250, wall=49306
2022-07-20 03:09:59 | INFO | train_inner | epoch 015:    111 / 1122 loss=8.525, nll_loss=3.061, mask_ins=1.59, word_ins_ml=4.636, word_reposition=1.43, kpe=0.868, ppl=368.36, wps=6891.8, ups=0.34, wpb=20138.9, bsz=256, num_updates=15800, lr=0.000281272, gnorm=1.607, clip=0, loss_scale=1843, train_wall=251, wall=49598
2022-07-20 03:14:49 | INFO | train_inner | epoch 015:    211 / 1122 loss=8.535, nll_loss=3.1, mask_ins=1.576, word_ins_ml=4.671, word_reposition=1.418, kpe=0.87, ppl=371.05, wps=6905.8, ups=0.34, wpb=20019.9, bsz=256, num_updates=15900, lr=0.000280386, gnorm=1.729, clip=0, loss_scale=2048, train_wall=249, wall=49888
2022-07-20 03:16:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-20 03:19:42 | INFO | train_inner | epoch 015:    312 / 1122 loss=8.55, nll_loss=3.073, mask_ins=1.592, word_ins_ml=4.648, word_reposition=1.437, kpe=0.874, ppl=374.89, wps=6863, ups=0.34, wpb=20048.3, bsz=256, num_updates=16000, lr=0.000279508, gnorm=1.663, clip=0, loss_scale=1460, train_wall=251, wall=50180
2022-07-20 03:24:56 | INFO | train_inner | epoch 015:    412 / 1122 loss=8.544, nll_loss=3.094, mask_ins=1.581, word_ins_ml=4.665, word_reposition=1.422, kpe=0.876, ppl=373.35, wps=6384.2, ups=0.32, wpb=20084.3, bsz=256, num_updates=16100, lr=0.000278639, gnorm=1.803, clip=0, loss_scale=1024, train_wall=274, wall=50495
2022-07-20 03:30:09 | INFO | train_inner | epoch 015:    512 / 1122 loss=8.555, nll_loss=3.107, mask_ins=1.58, word_ins_ml=4.677, word_reposition=1.422, kpe=0.875, ppl=376.02, wps=6377.6, ups=0.32, wpb=19975.6, bsz=256, num_updates=16200, lr=0.000277778, gnorm=1.681, clip=0, loss_scale=1024, train_wall=272, wall=50808
2022-07-20 03:34:59 | INFO | train_inner | epoch 015:    612 / 1122 loss=8.56, nll_loss=3.109, mask_ins=1.585, word_ins_ml=4.679, word_reposition=1.419, kpe=0.878, ppl=377.53, wps=6905.7, ups=0.35, wpb=20001.8, bsz=256, num_updates=16300, lr=0.000276924, gnorm=1.686, clip=0, loss_scale=1024, train_wall=249, wall=51098
2022-07-20 03:39:49 | INFO | train_inner | epoch 015:    712 / 1122 loss=8.585, nll_loss=3.109, mask_ins=1.592, word_ins_ml=4.679, word_reposition=1.436, kpe=0.878, ppl=384.03, wps=6861.2, ups=0.34, wpb=19903.5, bsz=256, num_updates=16400, lr=0.000276079, gnorm=1.651, clip=0, loss_scale=1024, train_wall=249, wall=51388
2022-07-20 03:44:39 | INFO | train_inner | epoch 015:    812 / 1122 loss=8.568, nll_loss=3.098, mask_ins=1.586, word_ins_ml=4.669, word_reposition=1.441, kpe=0.873, ppl=379.63, wps=6934.5, ups=0.35, wpb=20073.3, bsz=256, num_updates=16500, lr=0.000275241, gnorm=1.659, clip=0, loss_scale=1495, train_wall=249, wall=51677
2022-07-20 03:46:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-20 03:49:31 | INFO | train_inner | epoch 015:    913 / 1122 loss=8.526, nll_loss=3.093, mask_ins=1.57, word_ins_ml=4.664, word_reposition=1.419, kpe=0.874, ppl=368.75, wps=6815.6, ups=0.34, wpb=19944.2, bsz=256, num_updates=16600, lr=0.000274411, gnorm=1.615, clip=0, loss_scale=1369, train_wall=251, wall=51970
2022-07-20 03:54:21 | INFO | train_inner | epoch 015:   1013 / 1122 loss=8.56, nll_loss=3.088, mask_ins=1.583, word_ins_ml=4.66, word_reposition=1.442, kpe=0.875, ppl=377.47, wps=6902.9, ups=0.35, wpb=19989.4, bsz=256, num_updates=16700, lr=0.000273588, gnorm=1.681, clip=0, loss_scale=1024, train_wall=249, wall=52259
2022-07-20 03:59:10 | INFO | train_inner | epoch 015:   1113 / 1122 loss=8.542, nll_loss=3.082, mask_ins=1.577, word_ins_ml=4.654, word_reposition=1.433, kpe=0.878, ppl=372.81, wps=6943.8, ups=0.35, wpb=20106.3, bsz=256, num_updates=16800, lr=0.000272772, gnorm=1.609, clip=0, loss_scale=1024, train_wall=249, wall=52549
2022-07-20 03:59:35 | INFO | train | epoch 015 | loss 8.552 | nll_loss 3.093 | mask_ins 1.583 | word_ins_ml 4.665 | word_reposition 1.429 | kpe 0.875 | ppl 375.22 | wps 6614.6 | ups 0.33 | wpb 20004.4 | bsz 255.8 | num_updates 16809 | lr 0.000272699 | gnorm 1.676 | clip 0 | loss_scale 1301 | train_wall 2840 | wall 52574
2022-07-20 04:00:55 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 15.231 | nll_loss 7.256 | mask_ins 2.673 | word_ins_ml 8.48 | word_reposition 2.66 | kpe 1.418 | ppl 38445.5 | wps 12154.5 | wpb 2325.1 | bsz 32 | num_updates 16809 | best_loss 14.986
2022-07-20 04:00:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 15 @ 16809 updates, score 15.231) (writing took 3.2946495711803436 seconds)
2022-07-20 04:05:24 | INFO | train_inner | epoch 016:     91 / 1122 loss=8.465, nll_loss=3.06, mask_ins=1.563, word_ins_ml=4.635, word_reposition=1.406, kpe=0.861, ppl=353.33, wps=5298.7, ups=0.27, wpb=19773.1, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=1.643, clip=0, loss_scale=1024, train_wall=249, wall=52922
2022-07-20 04:10:13 | INFO | train_inner | epoch 016:    191 / 1122 loss=8.48, nll_loss=3.068, mask_ins=1.56, word_ins_ml=4.642, word_reposition=1.418, kpe=0.86, ppl=357.1, wps=6916.7, ups=0.35, wpb=20042.1, bsz=256, num_updates=17000, lr=0.000271163, gnorm=1.623, clip=0, loss_scale=1024, train_wall=249, wall=53212
2022-07-20 04:15:03 | INFO | train_inner | epoch 016:    291 / 1122 loss=8.469, nll_loss=3.033, mask_ins=1.582, word_ins_ml=4.611, word_reposition=1.411, kpe=0.865, ppl=354.33, wps=6943.3, ups=0.35, wpb=20104.4, bsz=256, num_updates=17100, lr=0.000270369, gnorm=1.891, clip=0, loss_scale=1587, train_wall=248, wall=53501
2022-07-20 04:19:53 | INFO | train_inner | epoch 016:    391 / 1122 loss=8.536, nll_loss=3.087, mask_ins=1.577, word_ins_ml=4.659, word_reposition=1.434, kpe=0.866, ppl=371.09, wps=6867.3, ups=0.34, wpb=19928.3, bsz=256, num_updates=17200, lr=0.000269582, gnorm=1.688, clip=0, loss_scale=2048, train_wall=249, wall=53792
2022-07-20 04:20:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-20 04:24:46 | INFO | train_inner | epoch 016:    492 / 1122 loss=8.498, nll_loss=3.057, mask_ins=1.574, word_ins_ml=4.632, word_reposition=1.425, kpe=0.868, ppl=361.53, wps=6853.5, ups=0.34, wpb=20074, bsz=256, num_updates=17300, lr=0.000268802, gnorm=1.768, clip=0, loss_scale=1065, train_wall=251, wall=54085
2022-07-20 04:30:09 | INFO | train_inner | epoch 016:    592 / 1122 loss=8.465, nll_loss=3.026, mask_ins=1.567, word_ins_ml=4.604, word_reposition=1.423, kpe=0.871, ppl=353.4, wps=6203.9, ups=0.31, wpb=20062.1, bsz=256, num_updates=17400, lr=0.000268028, gnorm=1.689, clip=0, loss_scale=1024, train_wall=282, wall=54408
2022-07-20 04:35:00 | INFO | train_inner | epoch 016:    692 / 1122 loss=8.467, nll_loss=3.047, mask_ins=1.564, word_ins_ml=4.623, word_reposition=1.41, kpe=0.87, ppl=353.9, wps=6892.5, ups=0.34, wpb=20045.6, bsz=256, num_updates=17500, lr=0.000267261, gnorm=1.675, clip=0, loss_scale=1024, train_wall=250, wall=54699
2022-07-20 04:39:51 | INFO | train_inner | epoch 016:    792 / 1122 loss=8.446, nll_loss=3.018, mask_ins=1.569, word_ins_ml=4.597, word_reposition=1.414, kpe=0.866, ppl=348.85, wps=6879.3, ups=0.34, wpb=20016.8, bsz=256, num_updates=17600, lr=0.000266501, gnorm=1.797, clip=0, loss_scale=1024, train_wall=250, wall=54990
2022-07-20 04:44:41 | INFO | train_inner | epoch 016:    892 / 1122 loss=8.469, nll_loss=3.03, mask_ins=1.574, word_ins_ml=4.608, word_reposition=1.413, kpe=0.873, ppl=354.23, wps=6923.6, ups=0.35, wpb=20056.1, bsz=256, num_updates=17700, lr=0.000265747, gnorm=1.673, clip=0, loss_scale=1024, train_wall=249, wall=55279
2022-07-20 04:49:31 | INFO | train_inner | epoch 016:    992 / 1122 loss=8.494, nll_loss=3.066, mask_ins=1.568, word_ins_ml=4.639, word_reposition=1.419, kpe=0.869, ppl=360.66, wps=6892.1, ups=0.35, wpb=19967.8, bsz=256, num_updates=17800, lr=0.000264999, gnorm=1.697, clip=0, loss_scale=1894, train_wall=249, wall=55569
2022-07-20 04:54:21 | INFO | train_inner | epoch 016:   1092 / 1122 loss=8.498, nll_loss=3.082, mask_ins=1.571, word_ins_ml=4.653, word_reposition=1.405, kpe=0.87, ppl=361.66, wps=6872.3, ups=0.34, wpb=19934.6, bsz=256, num_updates=17900, lr=0.000264258, gnorm=1.634, clip=0, loss_scale=2048, train_wall=249, wall=55859
2022-07-20 04:55:47 | INFO | train | epoch 016 | loss 8.482 | nll_loss 3.053 | mask_ins 1.57 | word_ins_ml 4.629 | word_reposition 1.416 | kpe 0.867 | ppl 357.67 | wps 6651.9 | ups 0.33 | wpb 20004.1 | bsz 255.8 | num_updates 17930 | lr 0.000264037 | gnorm 1.708 | clip 0 | loss_scale 1365 | train_wall 2828 | wall 55945
2022-07-20 04:57:07 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 15.222 | nll_loss 7.295 | mask_ins 2.675 | word_ins_ml 8.521 | word_reposition 2.612 | kpe 1.414 | ppl 38209.6 | wps 12157.5 | wpb 2325.1 | bsz 32 | num_updates 17930 | best_loss 14.986
2022-07-20 04:57:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 16 @ 17930 updates, score 15.222) (writing took 3.6223777616396546 seconds)
2022-07-20 05:00:33 | INFO | train_inner | epoch 017:     70 / 1122 loss=8.482, nll_loss=3.054, mask_ins=1.571, word_ins_ml=4.629, word_reposition=1.418, kpe=0.864, ppl=357.61, wps=5352, ups=0.27, wpb=19937.2, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=1.704, clip=0, loss_scale=2048, train_wall=248, wall=56232
2022-07-20 05:05:24 | INFO | train_inner | epoch 017:    170 / 1122 loss=8.48, nll_loss=3.05, mask_ins=1.574, word_ins_ml=4.625, word_reposition=1.424, kpe=0.856, ppl=356.93, wps=6895.9, ups=0.34, wpb=20030.9, bsz=256, num_updates=18100, lr=0.000262794, gnorm=1.738, clip=0, loss_scale=2048, train_wall=249, wall=56522
2022-07-20 05:10:13 | INFO | train_inner | epoch 017:    270 / 1122 loss=8.436, nll_loss=3.018, mask_ins=1.564, word_ins_ml=4.597, word_reposition=1.416, kpe=0.859, ppl=346.38, wps=6886.6, ups=0.35, wpb=19950.8, bsz=256, num_updates=18200, lr=0.000262071, gnorm=1.692, clip=0, loss_scale=2048, train_wall=249, wall=56812
2022-07-20 05:11:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-20 05:13:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-20 05:15:10 | INFO | train_inner | epoch 017:    372 / 1122 loss=8.442, nll_loss=3.037, mask_ins=1.562, word_ins_ml=4.615, word_reposition=1.407, kpe=0.858, ppl=347.76, wps=6718.7, ups=0.34, wpb=19929.2, bsz=256, num_updates=18300, lr=0.000261354, gnorm=1.629, clip=0, loss_scale=1907, train_wall=255, wall=57109
2022-07-20 05:20:00 | INFO | train_inner | epoch 017:    472 / 1122 loss=8.425, nll_loss=3.028, mask_ins=1.552, word_ins_ml=4.606, word_reposition=1.41, kpe=0.857, ppl=343.67, wps=6894.1, ups=0.34, wpb=19987.4, bsz=256, num_updates=18400, lr=0.000260643, gnorm=1.633, clip=0, loss_scale=1024, train_wall=249, wall=57399
2022-07-20 05:24:50 | INFO | train_inner | epoch 017:    572 / 1122 loss=8.43, nll_loss=3.019, mask_ins=1.56, word_ins_ml=4.598, word_reposition=1.41, kpe=0.862, ppl=344.83, wps=6897.8, ups=0.34, wpb=20005, bsz=256, num_updates=18500, lr=0.000259938, gnorm=1.606, clip=0, loss_scale=1024, train_wall=249, wall=57689
2022-07-20 05:29:40 | INFO | train_inner | epoch 017:    672 / 1122 loss=8.402, nll_loss=3.001, mask_ins=1.557, word_ins_ml=4.582, word_reposition=1.405, kpe=0.858, ppl=338.33, wps=6933.4, ups=0.35, wpb=20087.9, bsz=256, num_updates=18600, lr=0.000259238, gnorm=1.691, clip=0, loss_scale=1024, train_wall=249, wall=57978
2022-07-20 05:35:14 | INFO | train_inner | epoch 017:    772 / 1122 loss=8.464, nll_loss=3.03, mask_ins=1.565, word_ins_ml=4.607, word_reposition=1.427, kpe=0.864, ppl=353.04, wps=6040.4, ups=0.3, wpb=20215.6, bsz=256, num_updates=18700, lr=0.000258544, gnorm=1.66, clip=0, loss_scale=1024, train_wall=293, wall=58313
2022-07-20 05:40:05 | INFO | train_inner | epoch 017:    872 / 1122 loss=8.432, nll_loss=3.017, mask_ins=1.556, word_ins_ml=4.596, word_reposition=1.42, kpe=0.859, ppl=345.28, wps=6870.9, ups=0.34, wpb=19962.9, bsz=256, num_updates=18800, lr=0.000257855, gnorm=1.607, clip=0, loss_scale=1188, train_wall=249, wall=58604
2022-07-20 05:44:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-20 05:44:59 | INFO | train_inner | epoch 017:    973 / 1122 loss=8.394, nll_loss=3.02, mask_ins=1.554, word_ins_ml=4.598, word_reposition=1.385, kpe=0.857, ppl=336.33, wps=6805.7, ups=0.34, wpb=19985.4, bsz=256, num_updates=18900, lr=0.000257172, gnorm=1.57, clip=0, loss_scale=1926, train_wall=252, wall=58897
2022-07-20 05:49:49 | INFO | train_inner | epoch 017:   1073 / 1122 loss=8.362, nll_loss=2.969, mask_ins=1.556, word_ins_ml=4.552, word_reposition=1.392, kpe=0.862, ppl=328.99, wps=6887, ups=0.34, wpb=19988.7, bsz=256, num_updates=19000, lr=0.000256495, gnorm=1.706, clip=0, loss_scale=1024, train_wall=249, wall=59187
2022-07-20 05:52:10 | INFO | train | epoch 017 | loss 8.428 | nll_loss 3.019 | mask_ins 1.56 | word_ins_ml 4.598 | word_reposition 1.41 | kpe 0.859 | ppl 344.34 | wps 6617.1 | ups 0.33 | wpb 20005 | bsz 255.8 | num_updates 19049 | lr 0.000256164 | gnorm 1.659 | clip 0 | loss_scale 1447 | train_wall 2838 | wall 59328
2022-07-20 05:53:29 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 15.238 | nll_loss 7.267 | mask_ins 2.673 | word_ins_ml 8.496 | word_reposition 2.603 | kpe 1.465 | ppl 38644.3 | wps 12178.5 | wpb 2325.1 | bsz 32 | num_updates 19049 | best_loss 14.986
2022-07-20 05:53:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 17 @ 19049 updates, score 15.238) (writing took 2.965516443364322 seconds)
2022-07-20 05:56:00 | INFO | train_inner | epoch 018:     51 / 1122 loss=8.416, nll_loss=3.021, mask_ins=1.556, word_ins_ml=4.599, word_reposition=1.406, kpe=0.855, ppl=341.67, wps=5323.3, ups=0.27, wpb=19766.1, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=1.685, clip=0, loss_scale=1024, train_wall=248, wall=59559
2022-07-20 06:00:50 | INFO | train_inner | epoch 018:    151 / 1122 loss=8.342, nll_loss=2.97, mask_ins=1.551, word_ins_ml=4.555, word_reposition=1.392, kpe=0.844, ppl=324.4, wps=6918.4, ups=0.34, wpb=20085, bsz=256, num_updates=19200, lr=0.000255155, gnorm=1.551, clip=0, loss_scale=1024, train_wall=250, wall=59849
2022-07-20 06:05:41 | INFO | train_inner | epoch 018:    251 / 1122 loss=8.334, nll_loss=2.945, mask_ins=1.55, word_ins_ml=4.532, word_reposition=1.402, kpe=0.849, ppl=322.62, wps=6912.6, ups=0.34, wpb=20069.7, bsz=256, num_updates=19300, lr=0.000254493, gnorm=1.581, clip=0, loss_scale=1024, train_wall=249, wall=60139
2022-07-20 06:10:31 | INFO | train_inner | epoch 018:    351 / 1122 loss=8.397, nll_loss=3.01, mask_ins=1.555, word_ins_ml=4.589, word_reposition=1.405, kpe=0.847, ppl=337.06, wps=6860.8, ups=0.34, wpb=19918.2, bsz=256, num_updates=19400, lr=0.000253837, gnorm=1.64, clip=0, loss_scale=1024, train_wall=249, wall=60430
2022-07-20 06:12:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-20 06:15:24 | INFO | train_inner | epoch 018:    452 / 1122 loss=8.359, nll_loss=2.98, mask_ins=1.555, word_ins_ml=4.563, word_reposition=1.388, kpe=0.852, ppl=328.31, wps=6843.3, ups=0.34, wpb=20063, bsz=256, num_updates=19500, lr=0.000253185, gnorm=1.656, clip=0, loss_scale=1460, train_wall=252, wall=60723
2022-07-20 06:20:14 | INFO | train_inner | epoch 018:    552 / 1122 loss=8.436, nll_loss=3.037, mask_ins=1.561, word_ins_ml=4.614, word_reposition=1.404, kpe=0.857, ppl=346.36, wps=6873.5, ups=0.35, wpb=19896, bsz=256, num_updates=19600, lr=0.000252538, gnorm=1.685, clip=0, loss_scale=1024, train_wall=249, wall=61012
2022-07-20 06:23:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-20 06:25:07 | INFO | train_inner | epoch 018:    653 / 1122 loss=8.393, nll_loss=3.006, mask_ins=1.544, word_ins_ml=4.586, word_reposition=1.414, kpe=0.849, ppl=336.07, wps=6822.4, ups=0.34, wpb=19985.7, bsz=256, num_updates=19700, lr=0.000251896, gnorm=1.646, clip=0, loss_scale=892, train_wall=252, wall=61305
2022-07-20 06:29:57 | INFO | train_inner | epoch 018:    753 / 1122 loss=8.349, nll_loss=2.976, mask_ins=1.553, word_ins_ml=4.559, word_reposition=1.39, kpe=0.847, ppl=326, wps=6926.3, ups=0.34, wpb=20128.5, bsz=256, num_updates=19800, lr=0.000251259, gnorm=1.598, clip=0, loss_scale=512, train_wall=249, wall=61596
2022-07-20 06:34:48 | INFO | train_inner | epoch 018:    853 / 1122 loss=8.371, nll_loss=2.99, mask_ins=1.549, word_ins_ml=4.571, word_reposition=1.402, kpe=0.849, ppl=331.18, wps=6940.9, ups=0.34, wpb=20148.8, bsz=256, num_updates=19900, lr=0.000250627, gnorm=1.618, clip=0, loss_scale=512, train_wall=249, wall=61886
2022-07-20 06:40:23 | INFO | train_inner | epoch 018:    953 / 1122 loss=8.356, nll_loss=2.979, mask_ins=1.546, word_ins_ml=4.562, word_reposition=1.396, kpe=0.852, ppl=327.71, wps=5951.9, ups=0.3, wpb=19957.7, bsz=256, num_updates=20000, lr=0.00025, gnorm=1.689, clip=0, loss_scale=512, train_wall=294, wall=62222
2022-07-20 06:45:14 | INFO | train_inner | epoch 018:   1053 / 1122 loss=8.406, nll_loss=3.01, mask_ins=1.554, word_ins_ml=4.589, word_reposition=1.408, kpe=0.854, ppl=339.11, wps=6878.7, ups=0.34, wpb=19989.8, bsz=256, num_updates=20100, lr=0.000249377, gnorm=1.626, clip=0, loss_scale=512, train_wall=250, wall=62512
2022-07-20 06:48:32 | INFO | train | epoch 018 | loss 8.379 | nll_loss 2.994 | mask_ins 1.553 | word_ins_ml 4.575 | word_reposition 1.401 | kpe 0.85 | ppl 332.82 | wps 6623.7 | ups 0.33 | wpb 20005.8 | bsz 255.8 | num_updates 20169 | lr 0.00024895 | gnorm 1.629 | clip 0 | loss_scale 837 | train_wall 2841 | wall 62711
2022-07-20 06:49:53 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 15.254 | nll_loss 7.283 | mask_ins 2.69 | word_ins_ml 8.51 | word_reposition 2.647 | kpe 1.407 | ppl 39064.7 | wps 12147.6 | wpb 2325.1 | bsz 32 | num_updates 20169 | best_loss 14.986
2022-07-20 06:49:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 18 @ 20169 updates, score 15.254) (writing took 3.2242740020155907 seconds)
2022-07-20 06:51:25 | INFO | train_inner | epoch 019:     31 / 1122 loss=8.374, nll_loss=2.986, mask_ins=1.552, word_ins_ml=4.568, word_reposition=1.405, kpe=0.85, ppl=331.85, wps=5324.1, ups=0.27, wpb=19794.8, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=1.665, clip=0, loss_scale=584, train_wall=248, wall=62884
2022-07-20 06:56:15 | INFO | train_inner | epoch 019:    131 / 1122 loss=8.296, nll_loss=2.969, mask_ins=1.536, word_ins_ml=4.553, word_reposition=1.375, kpe=0.833, ppl=314.32, wps=6843.5, ups=0.35, wpb=19829.5, bsz=256, num_updates=20300, lr=0.000248146, gnorm=1.647, clip=0, loss_scale=1024, train_wall=249, wall=63174
2022-07-20 07:01:05 | INFO | train_inner | epoch 019:    231 / 1122 loss=8.299, nll_loss=2.958, mask_ins=1.534, word_ins_ml=4.543, word_reposition=1.382, kpe=0.84, ppl=315.02, wps=6900.2, ups=0.35, wpb=19978.6, bsz=256, num_updates=20400, lr=0.000247537, gnorm=1.67, clip=0, loss_scale=1024, train_wall=249, wall=63463
2022-07-20 07:05:55 | INFO | train_inner | epoch 019:    331 / 1122 loss=8.324, nll_loss=2.986, mask_ins=1.552, word_ins_ml=4.568, word_reposition=1.366, kpe=0.837, ppl=320.43, wps=6880.3, ups=0.34, wpb=20006.4, bsz=256, num_updates=20500, lr=0.000246932, gnorm=1.623, clip=0, loss_scale=1024, train_wall=250, wall=63754
2022-07-20 07:06:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-20 07:10:50 | INFO | train_inner | epoch 019:    432 / 1122 loss=8.331, nll_loss=2.975, mask_ins=1.538, word_ins_ml=4.558, word_reposition=1.391, kpe=0.845, ppl=322.03, wps=6783.4, ups=0.34, wpb=19944.9, bsz=256, num_updates=20600, lr=0.000246332, gnorm=1.684, clip=0, loss_scale=563, train_wall=253, wall=64048
2022-07-20 07:15:40 | INFO | train_inner | epoch 019:    532 / 1122 loss=8.342, nll_loss=2.972, mask_ins=1.546, word_ins_ml=4.555, word_reposition=1.399, kpe=0.842, ppl=324.51, wps=6913, ups=0.34, wpb=20101.8, bsz=256, num_updates=20700, lr=0.000245737, gnorm=1.667, clip=0, loss_scale=512, train_wall=250, wall=64339
2022-07-20 07:18:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-20 07:20:34 | INFO | train_inner | epoch 019:    633 / 1122 loss=8.31, nll_loss=2.957, mask_ins=1.542, word_ins_ml=4.542, word_reposition=1.383, kpe=0.844, ppl=317.38, wps=6843.4, ups=0.34, wpb=20078, bsz=256, num_updates=20800, lr=0.000245145, gnorm=1.788, clip=0, loss_scale=398, train_wall=252, wall=64632
2022-07-20 07:25:24 | INFO | train_inner | epoch 019:    733 / 1122 loss=8.362, nll_loss=2.997, mask_ins=1.539, word_ins_ml=4.577, word_reposition=1.4, kpe=0.845, ppl=329.02, wps=6951.3, ups=0.34, wpb=20187.7, bsz=256, num_updates=20900, lr=0.000244558, gnorm=1.692, clip=0, loss_scale=256, train_wall=249, wall=64923
2022-07-20 07:30:15 | INFO | train_inner | epoch 019:    833 / 1122 loss=8.294, nll_loss=2.947, mask_ins=1.528, word_ins_ml=4.533, word_reposition=1.389, kpe=0.843, ppl=313.86, wps=6890.4, ups=0.34, wpb=20012.2, bsz=256, num_updates=21000, lr=0.000243975, gnorm=1.652, clip=0, loss_scale=256, train_wall=249, wall=65213
2022-07-20 07:35:04 | INFO | train_inner | epoch 019:    933 / 1122 loss=8.298, nll_loss=2.951, mask_ins=1.532, word_ins_ml=4.536, word_reposition=1.382, kpe=0.847, ppl=314.65, wps=6885, ups=0.35, wpb=19953.9, bsz=256, num_updates=21100, lr=0.000243396, gnorm=1.973, clip=0, loss_scale=256, train_wall=249, wall=65503
2022-07-20 07:39:55 | INFO | train_inner | epoch 019:   1033 / 1122 loss=8.342, nll_loss=2.966, mask_ins=1.542, word_ins_ml=4.55, word_reposition=1.406, kpe=0.843, ppl=324.41, wps=6929.5, ups=0.34, wpb=20145.2, bsz=256, num_updates=21200, lr=0.000242821, gnorm=1.702, clip=0, loss_scale=256, train_wall=250, wall=65794
2022-07-20 07:45:00 | INFO | train | epoch 019 | loss 8.323 | nll_loss 2.968 | mask_ins 1.54 | word_ins_ml 4.552 | word_reposition 1.389 | kpe 0.842 | ppl 320.21 | wps 6614.6 | ups 0.33 | wpb 20005.3 | bsz 255.8 | num_updates 21289 | lr 0.000242313 | gnorm 1.73 | clip 0 | loss_scale 543 | train_wall 2844 | wall 66098
2022-07-20 07:46:20 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 15.292 | nll_loss 7.265 | mask_ins 2.684 | word_ins_ml 8.507 | word_reposition 2.625 | kpe 1.476 | ppl 40132.8 | wps 12134.1 | wpb 2325.1 | bsz 32 | num_updates 21289 | best_loss 14.986
2022-07-20 07:46:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 19 @ 21289 updates, score 15.292) (writing took 3.5054511884227395 seconds)
2022-07-20 07:46:55 | INFO | train_inner | epoch 020:     11 / 1122 loss=8.37, nll_loss=2.993, mask_ins=1.547, word_ins_ml=4.574, word_reposition=1.403, kpe=0.846, ppl=330.88, wps=4744.2, ups=0.24, wpb=19935.7, bsz=253.8, num_updates=21300, lr=0.000242251, gnorm=1.945, clip=0, loss_scale=340, train_wall=296, wall=66214
2022-07-20 07:51:46 | INFO | train_inner | epoch 020:    111 / 1122 loss=8.274, nll_loss=2.93, mask_ins=1.536, word_ins_ml=4.519, word_reposition=1.388, kpe=0.831, ppl=309.5, wps=6886.9, ups=0.34, wpb=19986.4, bsz=256, num_updates=21400, lr=0.000241684, gnorm=1.638, clip=0, loss_scale=512, train_wall=249, wall=66504
2022-07-20 07:56:36 | INFO | train_inner | epoch 020:    211 / 1122 loss=8.232, nll_loss=2.929, mask_ins=1.524, word_ins_ml=4.516, word_reposition=1.36, kpe=0.831, ppl=300.6, wps=6895.5, ups=0.34, wpb=20037.8, bsz=256, num_updates=21500, lr=0.000241121, gnorm=1.735, clip=0, loss_scale=512, train_wall=249, wall=66795
2022-07-20 08:01:28 | INFO | train_inner | epoch 020:    311 / 1122 loss=8.247, nll_loss=2.917, mask_ins=1.541, word_ins_ml=4.506, word_reposition=1.37, kpe=0.829, ppl=303.78, wps=6887.3, ups=0.34, wpb=20070.8, bsz=256, num_updates=21600, lr=0.000240563, gnorm=1.729, clip=0, loss_scale=512, train_wall=250, wall=67086
2022-07-20 08:06:17 | INFO | train_inner | epoch 020:    411 / 1122 loss=8.242, nll_loss=2.922, mask_ins=1.526, word_ins_ml=4.511, word_reposition=1.37, kpe=0.834, ppl=302.66, wps=6927.9, ups=0.35, wpb=20060.7, bsz=256, num_updates=21700, lr=0.000240008, gnorm=1.681, clip=0, loss_scale=512, train_wall=248, wall=67376
2022-07-20 08:11:07 | INFO | train_inner | epoch 020:    511 / 1122 loss=8.25, nll_loss=2.918, mask_ins=1.528, word_ins_ml=4.507, word_reposition=1.376, kpe=0.839, ppl=304.35, wps=6895.9, ups=0.35, wpb=19985.7, bsz=256, num_updates=21800, lr=0.000239457, gnorm=1.731, clip=0, loss_scale=620, train_wall=249, wall=67666
2022-07-20 08:14:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-20 08:15:59 | INFO | train_inner | epoch 020:    612 / 1122 loss=8.279, nll_loss=2.953, mask_ins=1.534, word_ins_ml=4.537, word_reposition=1.375, kpe=0.833, ppl=310.69, wps=6886.3, ups=0.34, wpb=20080.7, bsz=256, num_updates=21900, lr=0.000238909, gnorm=1.729, clip=0, loss_scale=892, train_wall=250, wall=67957
2022-07-20 08:20:49 | INFO | train_inner | epoch 020:    712 / 1122 loss=8.246, nll_loss=2.934, mask_ins=1.528, word_ins_ml=4.521, word_reposition=1.359, kpe=0.838, ppl=303.69, wps=6850.6, ups=0.34, wpb=19906.2, bsz=256, num_updates=22000, lr=0.000238366, gnorm=1.613, clip=0, loss_scale=512, train_wall=249, wall=68248
2022-07-20 08:25:40 | INFO | train_inner | epoch 020:    812 / 1122 loss=8.262, nll_loss=2.918, mask_ins=1.542, word_ins_ml=4.507, word_reposition=1.378, kpe=0.835, ppl=306.94, wps=6886.1, ups=0.34, wpb=19997.1, bsz=256, num_updates=22100, lr=0.000237826, gnorm=1.661, clip=0, loss_scale=512, train_wall=249, wall=68538
2022-07-20 08:30:32 | INFO | train_inner | epoch 020:    912 / 1122 loss=8.275, nll_loss=2.933, mask_ins=1.544, word_ins_ml=4.52, word_reposition=1.376, kpe=0.835, ppl=309.68, wps=6851.8, ups=0.34, wpb=20037.5, bsz=256, num_updates=22200, lr=0.000237289, gnorm=1.669, clip=0, loss_scale=512, train_wall=251, wall=68831
2022-07-20 08:35:25 | INFO | train_inner | epoch 020:   1012 / 1122 loss=8.3, nll_loss=2.954, mask_ins=1.534, word_ins_ml=4.539, word_reposition=1.388, kpe=0.839, ppl=315.14, wps=6823.5, ups=0.34, wpb=20005.4, bsz=256, num_updates=22300, lr=0.000236757, gnorm=1.762, clip=0, loss_scale=512, train_wall=251, wall=69124
2022-07-20 08:40:16 | INFO | train_inner | epoch 020:   1112 / 1122 loss=8.32, nll_loss=2.973, mask_ins=1.537, word_ins_ml=4.556, word_reposition=1.392, kpe=0.835, ppl=319.67, wps=6892.8, ups=0.34, wpb=20031.9, bsz=256, num_updates=22400, lr=0.000236228, gnorm=1.654, clip=0, loss_scale=584, train_wall=250, wall=69414
2022-07-20 08:40:44 | INFO | train | epoch 020 | loss 8.269 | nll_loss 2.937 | mask_ins 1.535 | word_ins_ml 4.523 | word_reposition 1.376 | kpe 0.835 | ppl 308.38 | wps 6706.6 | ups 0.34 | wpb 20006.2 | bsz 255.8 | num_updates 22410 | lr 0.000236175 | gnorm 1.696 | clip 0 | loss_scale 567 | train_wall 2797 | wall 69442
2022-07-20 08:42:04 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 15.163 | nll_loss 7.239 | mask_ins 2.65 | word_ins_ml 8.473 | word_reposition 2.61 | kpe 1.429 | ppl 36676.1 | wps 12076.1 | wpb 2325.1 | bsz 32 | num_updates 22410 | best_loss 14.986
2022-07-20 08:42:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 20 @ 22410 updates, score 15.163) (writing took 3.3640556884929538 seconds)
2022-07-20 08:46:30 | INFO | train_inner | epoch 021:     90 / 1122 loss=8.247, nll_loss=2.921, mask_ins=1.533, word_ins_ml=4.51, word_reposition=1.378, kpe=0.827, ppl=303.8, wps=5313.1, ups=0.27, wpb=19904.5, bsz=253.8, num_updates=22500, lr=0.000235702, gnorm=1.737, clip=0, loss_scale=1024, train_wall=249, wall=69789
2022-07-20 08:52:11 | INFO | train_inner | epoch 021:    190 / 1122 loss=8.244, nll_loss=2.928, mask_ins=1.529, word_ins_ml=4.516, word_reposition=1.376, kpe=0.823, ppl=303.27, wps=5845, ups=0.29, wpb=19888.8, bsz=256, num_updates=22600, lr=0.00023518, gnorm=1.794, clip=0, loss_scale=1024, train_wall=298, wall=70129
2022-07-20 08:57:03 | INFO | train_inner | epoch 021:    290 / 1122 loss=8.24, nll_loss=2.92, mask_ins=1.533, word_ins_ml=4.509, word_reposition=1.375, kpe=0.824, ppl=302.36, wps=6884.7, ups=0.34, wpb=20106, bsz=256, num_updates=22700, lr=0.000234662, gnorm=1.744, clip=0, loss_scale=1024, train_wall=250, wall=70421
2022-07-20 08:57:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-20 09:01:57 | INFO | train_inner | epoch 021:    391 / 1122 loss=8.281, nll_loss=2.953, mask_ins=1.531, word_ins_ml=4.538, word_reposition=1.384, kpe=0.828, ppl=311.14, wps=6803.5, ups=0.34, wpb=20046.2, bsz=256, num_updates=22800, lr=0.000234146, gnorm=1.73, clip=0, loss_scale=603, train_wall=252, wall=70716
2022-07-20 09:06:49 | INFO | train_inner | epoch 021:    491 / 1122 loss=8.263, nll_loss=2.95, mask_ins=1.52, word_ins_ml=4.535, word_reposition=1.378, kpe=0.829, ppl=307.28, wps=6912.9, ups=0.34, wpb=20149.6, bsz=256, num_updates=22900, lr=0.000233635, gnorm=1.951, clip=0, loss_scale=512, train_wall=250, wall=71008
2022-07-20 09:11:41 | INFO | train_inner | epoch 021:    591 / 1122 loss=8.226, nll_loss=2.901, mask_ins=1.531, word_ins_ml=4.491, word_reposition=1.376, kpe=0.827, ppl=299.35, wps=6859.8, ups=0.34, wpb=20002.8, bsz=256, num_updates=23000, lr=0.000233126, gnorm=1.746, clip=0, loss_scale=512, train_wall=250, wall=71299
2022-07-20 09:16:31 | INFO | train_inner | epoch 021:    691 / 1122 loss=8.244, nll_loss=2.926, mask_ins=1.525, word_ins_ml=4.514, word_reposition=1.374, kpe=0.831, ppl=303.19, wps=6863.4, ups=0.34, wpb=19950, bsz=256, num_updates=23100, lr=0.000232621, gnorm=1.95, clip=0, loss_scale=512, train_wall=249, wall=71590
2022-07-20 09:21:20 | INFO | train_inner | epoch 021:    791 / 1122 loss=8.26, nll_loss=2.928, mask_ins=1.523, word_ins_ml=4.515, word_reposition=1.39, kpe=0.832, ppl=306.5, wps=6931.4, ups=0.35, wpb=20040.8, bsz=256, num_updates=23200, lr=0.000232119, gnorm=1.766, clip=0, loss_scale=512, train_wall=249, wall=71879
2022-07-20 09:26:12 | INFO | train_inner | epoch 021:    891 / 1122 loss=8.259, nll_loss=2.917, mask_ins=1.538, word_ins_ml=4.506, word_reposition=1.387, kpe=0.828, ppl=306.3, wps=6828.2, ups=0.34, wpb=19901.7, bsz=256, num_updates=23300, lr=0.000231621, gnorm=1.761, clip=0, loss_scale=876, train_wall=250, wall=72170
2022-07-20 09:31:02 | INFO | train_inner | epoch 021:    991 / 1122 loss=8.247, nll_loss=2.921, mask_ins=1.522, word_ins_ml=4.509, word_reposition=1.388, kpe=0.828, ppl=303.76, wps=6935.6, ups=0.34, wpb=20105.7, bsz=256, num_updates=23400, lr=0.000231125, gnorm=1.723, clip=0, loss_scale=1024, train_wall=249, wall=72460
2022-07-20 09:35:52 | INFO | train_inner | epoch 021:   1091 / 1122 loss=8.297, nll_loss=2.951, mask_ins=1.534, word_ins_ml=4.536, word_reposition=1.395, kpe=0.832, ppl=314.41, wps=6881.7, ups=0.35, wpb=19942.4, bsz=256, num_updates=23500, lr=0.000230633, gnorm=1.749, clip=0, loss_scale=1024, train_wall=249, wall=72750
2022-07-20 09:37:20 | INFO | train | epoch 021 | loss 8.254 | nll_loss 2.928 | mask_ins 1.528 | word_ins_ml 4.516 | word_reposition 1.382 | kpe 0.828 | ppl 305.29 | wps 6602.7 | ups 0.33 | wpb 20006 | bsz 255.8 | num_updates 23531 | lr 0.000230481 | gnorm 1.792 | clip 0 | loss_scale 790 | train_wall 2848 | wall 72839
2022-07-20 09:38:40 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 15.144 | nll_loss 7.125 | mask_ins 2.664 | word_ins_ml 8.364 | word_reposition 2.636 | kpe 1.481 | ppl 36212.6 | wps 12186.7 | wpb 2325.1 | bsz 32 | num_updates 23531 | best_loss 14.986
2022-07-20 09:38:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 21 @ 23531 updates, score 15.144) (writing took 3.6943892808631063 seconds)
2022-07-20 09:42:04 | INFO | train_inner | epoch 022:     69 / 1122 loss=8.242, nll_loss=2.93, mask_ins=1.525, word_ins_ml=4.518, word_reposition=1.377, kpe=0.822, ppl=302.78, wps=5330.2, ups=0.27, wpb=19848.3, bsz=253.8, num_updates=23600, lr=0.000230144, gnorm=1.845, clip=0, loss_scale=1024, train_wall=248, wall=73122
2022-07-20 09:46:54 | INFO | train_inner | epoch 022:    169 / 1122 loss=8.216, nll_loss=2.913, mask_ins=1.523, word_ins_ml=4.503, word_reposition=1.373, kpe=0.817, ppl=297.31, wps=6888.8, ups=0.34, wpb=20014.7, bsz=256, num_updates=23700, lr=0.000229658, gnorm=1.806, clip=0, loss_scale=1024, train_wall=250, wall=73413
2022-07-20 09:50:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-20 09:51:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-20 09:52:01 | INFO | train_inner | epoch 022:    271 / 1122 loss=8.17, nll_loss=2.893, mask_ins=1.508, word_ins_ml=4.484, word_reposition=1.36, kpe=0.819, ppl=288.09, wps=6573, ups=0.33, wpb=20142, bsz=256, num_updates=23800, lr=0.000229175, gnorm=1.815, clip=0, loss_scale=1310, train_wall=265, wall=73719
2022-07-20 09:57:24 | INFO | train_inner | epoch 022:    371 / 1122 loss=8.181, nll_loss=2.891, mask_ins=1.506, word_ins_ml=4.483, word_reposition=1.374, kpe=0.817, ppl=290.12, wps=6219.3, ups=0.31, wpb=20121.9, bsz=256, num_updates=23900, lr=0.000228695, gnorm=1.745, clip=0, loss_scale=512, train_wall=282, wall=74043
2022-07-20 10:02:16 | INFO | train_inner | epoch 022:    471 / 1122 loss=8.177, nll_loss=2.879, mask_ins=1.518, word_ins_ml=4.472, word_reposition=1.366, kpe=0.82, ppl=289.32, wps=6895.5, ups=0.34, wpb=20082.4, bsz=256, num_updates=24000, lr=0.000228218, gnorm=1.729, clip=0, loss_scale=512, train_wall=250, wall=74334
2022-07-20 10:02:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-20 10:07:09 | INFO | train_inner | epoch 022:    572 / 1122 loss=8.18, nll_loss=2.895, mask_ins=1.513, word_ins_ml=4.487, word_reposition=1.362, kpe=0.818, ppl=290.04, wps=6841.2, ups=0.34, wpb=20046.5, bsz=256, num_updates=24100, lr=0.000227744, gnorm=1.799, clip=0, loss_scale=281, train_wall=251, wall=74627
2022-07-20 10:11:59 | INFO | train_inner | epoch 022:    672 / 1122 loss=8.211, nll_loss=2.896, mask_ins=1.525, word_ins_ml=4.487, word_reposition=1.371, kpe=0.827, ppl=296.38, wps=6920, ups=0.34, wpb=20098.4, bsz=256, num_updates=24200, lr=0.000227273, gnorm=1.767, clip=0, loss_scale=256, train_wall=250, wall=74918
2022-07-20 10:16:49 | INFO | train_inner | epoch 022:    772 / 1122 loss=8.197, nll_loss=2.916, mask_ins=1.505, word_ins_ml=4.504, word_reposition=1.366, kpe=0.82, ppl=293.37, wps=6904.2, ups=0.35, wpb=19999.7, bsz=256, num_updates=24300, lr=0.000226805, gnorm=1.763, clip=0, loss_scale=256, train_wall=249, wall=75207
2022-07-20 10:21:39 | INFO | train_inner | epoch 022:    872 / 1122 loss=8.154, nll_loss=2.879, mask_ins=1.506, word_ins_ml=4.471, word_reposition=1.357, kpe=0.82, ppl=284.74, wps=6842.4, ups=0.34, wpb=19860.5, bsz=256, num_updates=24400, lr=0.000226339, gnorm=1.772, clip=0, loss_scale=256, train_wall=249, wall=75498
2022-07-20 10:26:29 | INFO | train_inner | epoch 022:    972 / 1122 loss=8.1, nll_loss=2.837, mask_ins=1.496, word_ins_ml=4.434, word_reposition=1.348, kpe=0.822, ppl=274.46, wps=6925.9, ups=0.34, wpb=20081.8, bsz=256, num_updates=24500, lr=0.000225877, gnorm=1.719, clip=0, loss_scale=256, train_wall=249, wall=75788
2022-07-20 10:31:19 | INFO | train_inner | epoch 022:   1072 / 1122 loss=8.243, nll_loss=2.935, mask_ins=1.521, word_ins_ml=4.521, word_reposition=1.372, kpe=0.829, ppl=302.9, wps=6866.6, ups=0.35, wpb=19878.7, bsz=256, num_updates=24600, lr=0.000225417, gnorm=1.702, clip=0, loss_scale=458, train_wall=249, wall=76077
2022-07-20 10:33:43 | INFO | train | epoch 022 | loss 8.188 | nll_loss 2.897 | mask_ins 1.514 | word_ins_ml 4.488 | word_reposition 1.365 | kpe 0.821 | ppl 291.65 | wps 6618.8 | ups 0.33 | wpb 20007 | bsz 255.8 | num_updates 24650 | lr 0.000225189 | gnorm 1.769 | clip 0 | loss_scale 545 | train_wall 2839 | wall 76221
2022-07-20 10:35:03 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 15.177 | nll_loss 7.22 | mask_ins 2.647 | word_ins_ml 8.449 | word_reposition 2.652 | kpe 1.429 | ppl 37038.6 | wps 12165.9 | wpb 2325.1 | bsz 32 | num_updates 24650 | best_loss 14.986
2022-07-20 10:35:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 22 @ 24650 updates, score 15.177) (writing took 3.843063412234187 seconds)
2022-07-20 10:37:32 | INFO | train_inner | epoch 023:     50 / 1122 loss=8.193, nll_loss=2.911, mask_ins=1.522, word_ins_ml=4.501, word_reposition=1.354, kpe=0.817, ppl=292.67, wps=5291.6, ups=0.27, wpb=19777.1, bsz=253.8, num_updates=24700, lr=0.000224961, gnorm=1.922, clip=0, loss_scale=512, train_wall=249, wall=76451
2022-07-20 10:42:24 | INFO | train_inner | epoch 023:    150 / 1122 loss=8.134, nll_loss=2.857, mask_ins=1.506, word_ins_ml=4.452, word_reposition=1.366, kpe=0.81, ppl=280.98, wps=6920.9, ups=0.34, wpb=20160.5, bsz=256, num_updates=24800, lr=0.000224507, gnorm=1.875, clip=0, loss_scale=512, train_wall=250, wall=76742
2022-07-20 10:47:16 | INFO | train_inner | epoch 023:    250 / 1122 loss=8.178, nll_loss=2.893, mask_ins=1.515, word_ins_ml=4.484, word_reposition=1.363, kpe=0.816, ppl=289.54, wps=6863.7, ups=0.34, wpb=20035.4, bsz=256, num_updates=24900, lr=0.000224055, gnorm=1.956, clip=0, loss_scale=512, train_wall=251, wall=77034
2022-07-20 10:52:06 | INFO | train_inner | epoch 023:    350 / 1122 loss=8.179, nll_loss=2.89, mask_ins=1.519, word_ins_ml=4.482, word_reposition=1.367, kpe=0.811, ppl=289.77, wps=6895.8, ups=0.34, wpb=20061.8, bsz=256, num_updates=25000, lr=0.000223607, gnorm=1.704, clip=0, loss_scale=512, train_wall=250, wall=77325
2022-07-20 10:57:09 | INFO | train_inner | epoch 023:    450 / 1122 loss=8.113, nll_loss=2.843, mask_ins=1.507, word_ins_ml=4.44, word_reposition=1.359, kpe=0.808, ppl=276.88, wps=6611.7, ups=0.33, wpb=20020.8, bsz=256, num_updates=25100, lr=0.000223161, gnorm=1.75, clip=0, loss_scale=855, train_wall=262, wall=77628
2022-07-20 11:02:33 | INFO | train_inner | epoch 023:    550 / 1122 loss=8.137, nll_loss=2.857, mask_ins=1.514, word_ins_ml=4.452, word_reposition=1.357, kpe=0.813, ppl=281.41, wps=6181.1, ups=0.31, wpb=19980.2, bsz=256, num_updates=25200, lr=0.000222718, gnorm=1.881, clip=0, loss_scale=1024, train_wall=282, wall=77951
2022-07-20 11:07:23 | INFO | train_inner | epoch 023:    650 / 1122 loss=8.153, nll_loss=2.873, mask_ins=1.508, word_ins_ml=4.466, word_reposition=1.363, kpe=0.815, ppl=284.64, wps=6897.6, ups=0.34, wpb=20025.6, bsz=256, num_updates=25300, lr=0.000222277, gnorm=1.761, clip=0, loss_scale=1024, train_wall=249, wall=78241
2022-07-20 11:12:14 | INFO | train_inner | epoch 023:    750 / 1122 loss=8.12, nll_loss=2.846, mask_ins=1.505, word_ins_ml=4.443, word_reposition=1.359, kpe=0.814, ppl=278.25, wps=6868.4, ups=0.34, wpb=19979.5, bsz=256, num_updates=25400, lr=0.000221839, gnorm=1.75, clip=0, loss_scale=1024, train_wall=250, wall=78532
2022-07-20 11:17:05 | INFO | train_inner | epoch 023:    850 / 1122 loss=8.167, nll_loss=2.886, mask_ins=1.508, word_ins_ml=4.478, word_reposition=1.366, kpe=0.815, ppl=287.35, wps=6877.5, ups=0.34, wpb=20025.5, bsz=256, num_updates=25500, lr=0.000221404, gnorm=1.783, clip=0, loss_scale=1024, train_wall=250, wall=78824
2022-07-20 11:21:56 | INFO | train_inner | epoch 023:    950 / 1122 loss=8.168, nll_loss=2.872, mask_ins=1.513, word_ins_ml=4.466, word_reposition=1.37, kpe=0.82, ppl=287.64, wps=6894.7, ups=0.34, wpb=20077, bsz=256, num_updates=25600, lr=0.000220971, gnorm=1.824, clip=0, loss_scale=1587, train_wall=249, wall=79115
2022-07-20 11:26:48 | INFO | train_inner | epoch 023:   1050 / 1122 loss=8.157, nll_loss=2.879, mask_ins=1.509, word_ins_ml=4.471, word_reposition=1.361, kpe=0.815, ppl=285.35, wps=6821.4, ups=0.34, wpb=19910.4, bsz=256, num_updates=25700, lr=0.000220541, gnorm=1.713, clip=0, loss_scale=2048, train_wall=250, wall=79407
2022-07-20 11:30:19 | INFO | train | epoch 023 | loss 8.151 | nll_loss 2.87 | mask_ins 1.51 | word_ins_ml 4.464 | word_reposition 1.363 | kpe 0.814 | ppl 284.3 | wps 6610.2 | ups 0.33 | wpb 20006 | bsz 255.8 | num_updates 25772 | lr 0.000220232 | gnorm 1.812 | clip 0 | loss_scale 1056 | train_wall 2849 | wall 79617
2022-07-20 11:31:39 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 15.476 | nll_loss 7.3 | mask_ins 2.763 | word_ins_ml 8.527 | word_reposition 2.738 | kpe 1.448 | ppl 45576.4 | wps 12078.6 | wpb 2325.1 | bsz 32 | num_updates 25772 | best_loss 14.986
2022-07-20 11:31:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 23 @ 25772 updates, score 15.476) (writing took 3.6609161384403706 seconds)
2022-07-20 11:33:04 | INFO | train_inner | epoch 024:     28 / 1122 loss=8.158, nll_loss=2.866, mask_ins=1.511, word_ins_ml=4.46, word_reposition=1.374, kpe=0.813, ppl=285.68, wps=5247.3, ups=0.27, wpb=19726, bsz=253.8, num_updates=25800, lr=0.000220113, gnorm=1.921, clip=0, loss_scale=2048, train_wall=251, wall=79783
2022-07-20 11:37:54 | INFO | train_inner | epoch 024:    128 / 1122 loss=8.111, nll_loss=2.856, mask_ins=1.496, word_ins_ml=4.452, word_reposition=1.36, kpe=0.803, ppl=276.4, wps=6873.6, ups=0.34, wpb=19945.3, bsz=256, num_updates=25900, lr=0.000219687, gnorm=1.802, clip=0, loss_scale=2048, train_wall=249, wall=80073
2022-07-20 11:38:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-20 11:42:48 | INFO | train_inner | epoch 024:    229 / 1122 loss=8.095, nll_loss=2.852, mask_ins=1.499, word_ins_ml=4.447, word_reposition=1.352, kpe=0.797, ppl=273.37, wps=6795.4, ups=0.34, wpb=19959.2, bsz=256, num_updates=26000, lr=0.000219265, gnorm=1.795, clip=0, loss_scale=1105, train_wall=252, wall=80366
2022-07-20 11:47:38 | INFO | train_inner | epoch 024:    329 / 1122 loss=8.109, nll_loss=2.86, mask_ins=1.495, word_ins_ml=4.455, word_reposition=1.356, kpe=0.803, ppl=276.05, wps=6877.5, ups=0.35, wpb=19930.2, bsz=256, num_updates=26100, lr=0.000218844, gnorm=1.808, clip=0, loss_scale=1024, train_wall=249, wall=80656
2022-07-20 11:49:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-20 11:52:31 | INFO | train_inner | epoch 024:    430 / 1122 loss=8.113, nll_loss=2.863, mask_ins=1.499, word_ins_ml=4.457, word_reposition=1.354, kpe=0.803, ppl=276.82, wps=6861.9, ups=0.34, wpb=20108.5, bsz=256, num_updates=26200, lr=0.000218426, gnorm=1.781, clip=0, loss_scale=674, train_wall=252, wall=80949
2022-07-20 11:57:20 | INFO | train_inner | epoch 024:    530 / 1122 loss=8.147, nll_loss=2.879, mask_ins=1.515, word_ins_ml=4.472, word_reposition=1.353, kpe=0.807, ppl=283.45, wps=6925.4, ups=0.35, wpb=20063, bsz=256, num_updates=26300, lr=0.00021801, gnorm=1.799, clip=0, loss_scale=512, train_wall=249, wall=81239
2022-07-20 12:02:24 | INFO | train_inner | epoch 024:    630 / 1122 loss=8.13, nll_loss=2.869, mask_ins=1.505, word_ins_ml=4.462, word_reposition=1.355, kpe=0.807, ppl=280.13, wps=6582.5, ups=0.33, wpb=19958.9, bsz=256, num_updates=26400, lr=0.000217597, gnorm=1.731, clip=0, loss_scale=512, train_wall=262, wall=81542
2022-07-20 12:07:47 | INFO | train_inner | epoch 024:    730 / 1122 loss=8.129, nll_loss=2.851, mask_ins=1.499, word_ins_ml=4.446, word_reposition=1.377, kpe=0.807, ppl=279.88, wps=6189.3, ups=0.31, wpb=20020.5, bsz=256, num_updates=26500, lr=0.000217186, gnorm=1.684, clip=0, loss_scale=512, train_wall=282, wall=81866
2022-07-20 12:12:37 | INFO | train_inner | epoch 024:    830 / 1122 loss=8.095, nll_loss=2.838, mask_ins=1.496, word_ins_ml=4.435, word_reposition=1.355, kpe=0.809, ppl=273.34, wps=6931.6, ups=0.34, wpb=20097.1, bsz=256, num_updates=26600, lr=0.000216777, gnorm=1.724, clip=0, loss_scale=512, train_wall=249, wall=82156
2022-07-20 12:17:27 | INFO | train_inner | epoch 024:    930 / 1122 loss=8.113, nll_loss=2.859, mask_ins=1.507, word_ins_ml=4.453, word_reposition=1.346, kpe=0.807, ppl=276.91, wps=6928.3, ups=0.35, wpb=20081.5, bsz=256, num_updates=26700, lr=0.000216371, gnorm=1.71, clip=0, loss_scale=804, train_wall=249, wall=82446
2022-07-20 12:22:18 | INFO | train_inner | epoch 024:   1030 / 1122 loss=8.166, nll_loss=2.866, mask_ins=1.519, word_ins_ml=4.46, word_reposition=1.374, kpe=0.814, ppl=287.23, wps=6872.7, ups=0.34, wpb=20037.7, bsz=256, num_updates=26800, lr=0.000215967, gnorm=1.733, clip=0, loss_scale=1024, train_wall=251, wall=82737
2022-07-20 12:26:47 | INFO | train | epoch 024 | loss 8.121 | nll_loss 2.859 | mask_ins 1.503 | word_ins_ml 4.454 | word_reposition 1.358 | kpe 0.806 | ppl 278.41 | wps 6612.5 | ups 0.33 | wpb 20005.6 | bsz 255.8 | num_updates 26892 | lr 0.000215597 | gnorm 1.764 | clip 0 | loss_scale 914 | train_wall 2843 | wall 83006
2022-07-20 12:28:08 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 15.293 | nll_loss 7.229 | mask_ins 2.708 | word_ins_ml 8.46 | word_reposition 2.662 | kpe 1.463 | ppl 40143.4 | wps 12049.8 | wpb 2325.1 | bsz 32 | num_updates 26892 | best_loss 14.986
2022-07-20 12:28:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 24 @ 26892 updates, score 15.293) (writing took 3.932338739745319 seconds)
2022-07-20 12:28:35 | INFO | train_inner | epoch 025:      8 / 1122 loss=8.115, nll_loss=2.853, mask_ins=1.498, word_ins_ml=4.448, word_reposition=1.358, kpe=0.81, ppl=277.16, wps=5295.5, ups=0.27, wpb=19944.6, bsz=253.8, num_updates=26900, lr=0.000215565, gnorm=1.81, clip=0, loss_scale=1024, train_wall=251, wall=83114
2022-07-20 12:33:26 | INFO | train_inner | epoch 025:    108 / 1122 loss=8.078, nll_loss=2.825, mask_ins=1.499, word_ins_ml=4.423, word_reposition=1.36, kpe=0.796, ppl=270.17, wps=6900.5, ups=0.34, wpb=20093.8, bsz=256, num_updates=27000, lr=0.000215166, gnorm=1.733, clip=0, loss_scale=1024, train_wall=250, wall=83405
2022-07-20 12:38:17 | INFO | train_inner | epoch 025:    208 / 1122 loss=8.074, nll_loss=2.839, mask_ins=1.491, word_ins_ml=4.436, word_reposition=1.355, kpe=0.792, ppl=269.56, wps=6894, ups=0.34, wpb=20062.4, bsz=256, num_updates=27100, lr=0.000214768, gnorm=1.727, clip=0, loss_scale=1024, train_wall=250, wall=83696
2022-07-20 12:43:09 | INFO | train_inner | epoch 025:    308 / 1122 loss=8.071, nll_loss=2.842, mask_ins=1.493, word_ins_ml=4.439, word_reposition=1.345, kpe=0.794, ppl=268.94, wps=6883, ups=0.34, wpb=20057, bsz=256, num_updates=27200, lr=0.000214373, gnorm=1.788, clip=0, loss_scale=1485, train_wall=250, wall=83987
2022-07-20 12:48:00 | INFO | train_inner | epoch 025:    408 / 1122 loss=8.075, nll_loss=2.839, mask_ins=1.492, word_ins_ml=4.435, word_reposition=1.349, kpe=0.799, ppl=269.68, wps=6912.8, ups=0.34, wpb=20121.6, bsz=256, num_updates=27300, lr=0.00021398, gnorm=1.787, clip=0, loss_scale=2048, train_wall=250, wall=84278
2022-07-20 12:52:50 | INFO | train_inner | epoch 025:    508 / 1122 loss=8.087, nll_loss=2.84, mask_ins=1.507, word_ins_ml=4.437, word_reposition=1.346, kpe=0.796, ppl=271.85, wps=6863.2, ups=0.34, wpb=19894.3, bsz=256, num_updates=27400, lr=0.000213589, gnorm=1.763, clip=0, loss_scale=2048, train_wall=249, wall=84568
2022-07-20 12:57:39 | INFO | train_inner | epoch 025:    608 / 1122 loss=8.07, nll_loss=2.848, mask_ins=1.498, word_ins_ml=4.443, word_reposition=1.33, kpe=0.798, ppl=268.64, wps=6882.8, ups=0.35, wpb=19933.4, bsz=256, num_updates=27500, lr=0.000213201, gnorm=1.757, clip=0, loss_scale=2048, train_wall=249, wall=84858
2022-07-20 13:02:29 | INFO | train_inner | epoch 025:    708 / 1122 loss=8.001, nll_loss=2.799, mask_ins=1.481, word_ins_ml=4.399, word_reposition=1.326, kpe=0.794, ppl=256.09, wps=6908.5, ups=0.35, wpb=20016, bsz=256, num_updates=27600, lr=0.000212814, gnorm=1.688, clip=0, loss_scale=2048, train_wall=249, wall=85148
2022-07-20 13:07:30 | INFO | train_inner | epoch 025:    808 / 1122 loss=8.114, nll_loss=2.861, mask_ins=1.5, word_ins_ml=4.455, word_reposition=1.356, kpe=0.802, ppl=277.03, wps=6697.8, ups=0.33, wpb=20185.5, bsz=256, num_updates=27700, lr=0.00021243, gnorm=1.714, clip=0, loss_scale=2724, train_wall=260, wall=85449
2022-07-20 13:12:55 | INFO | train_inner | epoch 025:    908 / 1122 loss=8.076, nll_loss=2.858, mask_ins=1.486, word_ins_ml=4.452, word_reposition=1.338, kpe=0.8, ppl=269.88, wps=6132.7, ups=0.31, wpb=19889, bsz=256, num_updates=27800, lr=0.000212047, gnorm=1.767, clip=0, loss_scale=4096, train_wall=283, wall=85773
2022-07-20 13:17:45 | INFO | train_inner | epoch 025:   1008 / 1122 loss=8.066, nll_loss=2.847, mask_ins=1.488, word_ins_ml=4.443, word_reposition=1.335, kpe=0.801, ppl=268.03, wps=6855, ups=0.34, wpb=19918.3, bsz=256, num_updates=27900, lr=0.000211667, gnorm=1.712, clip=0, loss_scale=4096, train_wall=249, wall=86064
2022-07-20 13:22:36 | INFO | train_inner | epoch 025:   1108 / 1122 loss=8.073, nll_loss=2.828, mask_ins=1.496, word_ins_ml=4.426, word_reposition=1.351, kpe=0.8, ppl=269.3, wps=6879.1, ups=0.34, wpb=19963.2, bsz=256, num_updates=28000, lr=0.000211289, gnorm=1.747, clip=0, loss_scale=4096, train_wall=249, wall=86354
2022-07-20 13:23:15 | INFO | train | epoch 025 | loss 8.072 | nll_loss 2.838 | mask_ins 1.494 | word_ins_ml 4.435 | word_reposition 1.346 | kpe 0.798 | ppl 269.19 | wps 6625.5 | ups 0.33 | wpb 20005.9 | bsz 255.8 | num_updates 28014 | lr 0.000211236 | gnorm 1.749 | clip 0 | loss_scale 2441 | train_wall 2840 | wall 86394
2022-07-20 13:24:35 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 15.252 | nll_loss 7.275 | mask_ins 2.644 | word_ins_ml 8.516 | word_reposition 2.628 | kpe 1.465 | ppl 39024.5 | wps 12160.4 | wpb 2325.1 | bsz 32 | num_updates 28014 | best_loss 14.986
2022-07-20 13:24:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keyword_ACL_cased/checkpoint_last.pt (epoch 25 @ 28014 updates, score 15.252) (writing took 3.3377652214840055 seconds)
2022-07-20 13:28:48 | INFO | train_inner | epoch 026:     86 / 1122 loss=8.081, nll_loss=2.842, mask_ins=1.497, word_ins_ml=4.438, word_reposition=1.359, kpe=0.787, ppl=270.84, wps=5358.8, ups=0.27, wpb=19934.6, bsz=253.8, num_updates=28100, lr=0.000210912, gnorm=1.816, clip=0, loss_scale=4096, train_wall=248, wall=86726
2022-07-20 13:33:37 | INFO | train_inner | epoch 026:    186 / 1122 loss=7.968, nll_loss=2.796, mask_ins=1.476, word_ins_ml=4.397, word_reposition=1.312, kpe=0.782, ppl=250.33, wps=6913.7, ups=0.35, wpb=20016.5, bsz=256, num_updates=28200, lr=0.000210538, gnorm=1.7, clip=0, loss_scale=4956, train_wall=249, wall=87016
2022-07-20 13:38:28 | INFO | train_inner | epoch 026:    286 / 1122 loss=8.012, nll_loss=2.797, mask_ins=1.492, word_ins_ml=4.398, word_reposition=1.335, kpe=0.787, ppl=258.19, wps=6836.7, ups=0.34, wpb=19921.1, bsz=256, num_updates=28300, lr=0.000210166, gnorm=1.772, clip=0, loss_scale=8192, train_wall=250, wall=87307
2022-07-20 13:41:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-20 13:43:23 | INFO | train_inner | epoch 026:    387 / 1122 loss=8.01, nll_loss=2.797, mask_ins=1.482, word_ins_ml=4.399, word_reposition=1.346, kpe=0.783, ppl=257.77, wps=6825.8, ups=0.34, wpb=20092.4, bsz=256, num_updates=28400, lr=0.000209795, gnorm=1.744, clip=0, loss_scale=6245, train_wall=253, wall=87601
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
