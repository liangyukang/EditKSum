nohup: ignoring input
2022-08-02 09:43:04 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:11959
2022-08-02 09:43:04 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:11959
2022-08-02 09:43:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-08-02 09:43:04 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:11959
2022-08-02 09:43:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-08-02 09:43:04 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:11959
2022-08-02 09:43:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-08-02 09:43:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-08-02 09:43:04 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-02 09:43:04 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-08-02 09:43:04 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-02 09:43:04 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-08-02 09:43:04 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-02 09:43:04 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-08-02 09:43:04 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-02 09:43:04 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-08-02 09:43:08 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=32, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=32, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:11959', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_cased_Ggw', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-Ggw', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510_Ggw', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=False, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-08-02 09:43:08 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-08-02 09:43:08 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-08-02 09:43:08 | INFO | fairseq.data.data_utils | loaded 189612 examples from: ../data-bin-bert-cased-Ggw/valid.source-target.source
2022-08-02 09:43:08 | INFO | fairseq.data.data_utils | loaded 189612 examples from: ../data-bin-bert-cased-Ggw/valid.source-target.target
2022-08-02 09:43:08 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-Ggw valid source-target 189612 examples
2022-08-02 09:43:08 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-02 09:43:08 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-02 09:43:08 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-08-02 09:43:12 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight']
2022-08-02 09:43:12 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-08-02 09:43:12 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-02 09:43:12 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-02 09:43:12 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
Trained parameters: len 265
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 265
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-08-02 09:43:14 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-08-02 09:43:14 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 265
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 1
Trained parameters not adapter: ['decoder.embed_mask_ins.weight']
2022-08-02 09:43:14 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-08-02 09:43:14 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-08-02 09:43:14 | INFO | fairseq_cli.train | num. model params: 380360448 (num. trained: 142061568)
2022-08-02 09:43:14 | INFO | fairseq_cli.train | num. Encoder model params: 146077440 (Encoder num. trained: 37767168)
2022-08-02 09:43:14 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)
Trained parameters: len 265
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-08-02 09:43:17 | INFO | fairseq_cli.train | training on 4 GPUs
2022-08-02 09:43:17 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 32
2022-08-02 09:43:17 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint_last.pt
2022-08-02 09:43:17 | INFO | fairseq.trainer | loading train data for epoch 1
2022-08-02 09:43:17 | INFO | fairseq.data.data_utils | loaded 3803212 examples from: ../data-bin-bert-cased-Ggw/train.source-target.source
2022-08-02 09:43:17 | INFO | fairseq.data.data_utils | loaded 3803212 examples from: ../data-bin-bert-cased-Ggw/train.source-target.target
2022-08-02 09:43:17 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-Ggw train source-target 3803212 examples
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-08-02 09:46:20 | INFO | train_inner | epoch 001:    100 / 3715 loss=24.423, nll_loss=12.963, mask_ins=7.578, word_ins_ml=13.346, word_reposition=3.499, ppl=2.24894e+07, wps=9538.1, ups=0.65, wpb=14605, bsz=1024, num_updates=100, lr=1.0098e-05, gnorm=19.43, clip=14, loss_scale=128, train_wall=153, wall=183
2022-08-02 09:48:52 | INFO | train_inner | epoch 001:    200 / 3715 loss=19.355, nll_loss=11.64, mask_ins=4.771, word_ins_ml=12.199, word_reposition=2.385, ppl=670580, wps=9598.9, ups=0.66, wpb=14603.1, bsz=1024, num_updates=200, lr=2.0096e-05, gnorm=17.683, clip=0, loss_scale=128, train_wall=150, wall=336
2022-08-02 09:51:22 | INFO | train_inner | epoch 001:    300 / 3715 loss=15.818, nll_loss=11.363, mask_ins=2.446, word_ins_ml=11.96, word_reposition=1.412, ppl=57762.9, wps=9852.1, ups=0.67, wpb=14772, bsz=1024, num_updates=300, lr=3.0094e-05, gnorm=5.815, clip=0, loss_scale=128, train_wall=148, wall=485
2022-08-02 09:53:55 | INFO | train_inner | epoch 001:    400 / 3715 loss=14.489, nll_loss=11.006, mask_ins=2.043, word_ins_ml=11.649, word_reposition=0.797, ppl=23001.6, wps=9644.3, ups=0.65, wpb=14728.8, bsz=1024, num_updates=400, lr=4.0092e-05, gnorm=3.309, clip=0, loss_scale=128, train_wall=151, wall=638
2022-08-02 09:56:27 | INFO | train_inner | epoch 001:    500 / 3715 loss=14.035, nll_loss=10.71, mask_ins=1.932, word_ins_ml=11.392, word_reposition=0.711, ppl=16781.4, wps=9625.8, ups=0.66, wpb=14607.3, bsz=1024, num_updates=500, lr=5.009e-05, gnorm=3.137, clip=0, loss_scale=128, train_wall=150, wall=790
2022-08-02 09:58:58 | INFO | train_inner | epoch 001:    600 / 3715 loss=13.703, nll_loss=10.351, mask_ins=1.901, word_ins_ml=11.076, word_reposition=0.724, ppl=13333.4, wps=9630.6, ups=0.66, wpb=14581.3, bsz=1024, num_updates=600, lr=6.0088e-05, gnorm=2.933, clip=0, loss_scale=242, train_wall=150, wall=941
2022-08-02 10:01:30 | INFO | train_inner | epoch 001:    700 / 3715 loss=13.387, nll_loss=9.973, mask_ins=1.903, word_ins_ml=10.743, word_reposition=0.74, ppl=10709.5, wps=9728.3, ups=0.66, wpb=14738.3, bsz=1024, num_updates=700, lr=7.0086e-05, gnorm=3.155, clip=0, loss_scale=256, train_wall=150, wall=1093
2022-08-02 10:04:01 | INFO | train_inner | epoch 001:    800 / 3715 loss=12.894, nll_loss=9.419, mask_ins=1.882, word_ins_ml=10.257, word_reposition=0.755, ppl=7610.06, wps=9681.1, ups=0.66, wpb=14637, bsz=1024, num_updates=800, lr=8.0084e-05, gnorm=3.171, clip=0, loss_scale=256, train_wall=149, wall=1244
2022-08-02 10:06:32 | INFO | train_inner | epoch 001:    900 / 3715 loss=12.461, nll_loss=8.929, mask_ins=1.881, word_ins_ml=9.83, word_reposition=0.75, ppl=5639.43, wps=9582.9, ups=0.66, wpb=14512.1, bsz=1024, num_updates=900, lr=9.0082e-05, gnorm=3.218, clip=0, loss_scale=256, train_wall=150, wall=1395
2022-08-02 10:09:03 | INFO | train_inner | epoch 001:   1000 / 3715 loss=12.069, nll_loss=8.5, mask_ins=1.877, word_ins_ml=9.455, word_reposition=0.738, ppl=4297.51, wps=9641.8, ups=0.66, wpb=14576.3, bsz=1024, num_updates=1000, lr=0.00010008, gnorm=3.128, clip=0, loss_scale=256, train_wall=149, wall=1547
2022-08-02 10:11:35 | INFO | train_inner | epoch 001:   1100 / 3715 loss=11.692, nll_loss=8.099, mask_ins=1.853, word_ins_ml=9.105, word_reposition=0.733, ppl=3307.62, wps=9703.5, ups=0.66, wpb=14664.5, bsz=1024, num_updates=1100, lr=0.000110078, gnorm=3.2, clip=0, loss_scale=453, train_wall=149, wall=1698
2022-08-02 10:14:06 | INFO | train_inner | epoch 001:   1200 / 3715 loss=11.375, nll_loss=7.809, mask_ins=1.797, word_ins_ml=8.853, word_reposition=0.725, ppl=2655.44, wps=9719.2, ups=0.66, wpb=14698.2, bsz=1024, num_updates=1200, lr=0.000120076, gnorm=3.255, clip=0, loss_scale=512, train_wall=149, wall=1849
2022-08-02 10:16:36 | INFO | train_inner | epoch 001:   1300 / 3715 loss=11.134, nll_loss=7.581, mask_ins=1.762, word_ins_ml=8.654, word_reposition=0.719, ppl=2247.99, wps=9801.1, ups=0.67, wpb=14674.2, bsz=1024, num_updates=1300, lr=0.000130074, gnorm=3.4, clip=1, loss_scale=512, train_wall=148, wall=1999
2022-08-02 10:19:06 | INFO | train_inner | epoch 001:   1400 / 3715 loss=10.9, nll_loss=7.375, mask_ins=1.725, word_ins_ml=8.477, word_reposition=0.698, ppl=1910.34, wps=9838.9, ups=0.67, wpb=14762.6, bsz=1024, num_updates=1400, lr=0.000140072, gnorm=2.981, clip=0, loss_scale=512, train_wall=148, wall=2149
2022-08-02 10:21:36 | INFO | train_inner | epoch 001:   1500 / 3715 loss=10.682, nll_loss=7.151, mask_ins=1.703, word_ins_ml=8.283, word_reposition=0.695, ppl=1642.64, wps=9717.3, ups=0.66, wpb=14653.9, bsz=1024, num_updates=1500, lr=0.00015007, gnorm=2.884, clip=0, loss_scale=512, train_wall=149, wall=2300
2022-08-02 10:24:06 | INFO | train_inner | epoch 001:   1600 / 3715 loss=10.505, nll_loss=6.993, mask_ins=1.676, word_ins_ml=8.146, word_reposition=0.683, ppl=1453.24, wps=9825.3, ups=0.67, wpb=14712.5, bsz=1024, num_updates=1600, lr=0.000160068, gnorm=2.87, clip=0, loss_scale=845, train_wall=148, wall=2449
2022-08-02 10:26:37 | INFO | train_inner | epoch 001:   1700 / 3715 loss=10.28, nll_loss=6.782, mask_ins=1.648, word_ins_ml=7.964, word_reposition=0.668, ppl=1243.34, wps=9786.4, ups=0.66, wpb=14722.9, bsz=1023.8, num_updates=1700, lr=0.000170066, gnorm=2.909, clip=0, loss_scale=1024, train_wall=149, wall=2600
2022-08-02 10:29:08 | INFO | train_inner | epoch 001:   1800 / 3715 loss=10.028, nll_loss=6.561, mask_ins=1.609, word_ins_ml=7.772, word_reposition=0.647, ppl=1044.15, wps=9683.8, ups=0.66, wpb=14623.7, bsz=1024, num_updates=1800, lr=0.000180064, gnorm=2.78, clip=0, loss_scale=1024, train_wall=149, wall=2751
2022-08-02 10:31:37 | INFO | train_inner | epoch 001:   1900 / 3715 loss=9.82, nll_loss=6.39, mask_ins=1.569, word_ins_ml=7.624, word_reposition=0.627, ppl=903.69, wps=9758.8, ups=0.67, wpb=14622.6, bsz=1024, num_updates=1900, lr=0.000190062, gnorm=2.631, clip=0, loss_scale=1024, train_wall=148, wall=2901
2022-08-02 10:34:08 | INFO | train_inner | epoch 001:   2000 / 3715 loss=9.624, nll_loss=6.193, mask_ins=1.549, word_ins_ml=7.454, word_reposition=0.622, ppl=789.03, wps=9761.5, ups=0.66, wpb=14718.6, bsz=1024, num_updates=2000, lr=0.00020006, gnorm=2.452, clip=0, loss_scale=1024, train_wall=149, wall=3051
2022-08-02 10:36:39 | INFO | train_inner | epoch 001:   2100 / 3715 loss=9.502, nll_loss=6.093, mask_ins=1.529, word_ins_ml=7.366, word_reposition=0.607, ppl=725.29, wps=9736.8, ups=0.66, wpb=14676.5, bsz=1024, num_updates=2100, lr=0.000210058, gnorm=2.449, clip=0, loss_scale=1567, train_wall=149, wall=3202
2022-08-02 10:39:10 | INFO | train_inner | epoch 001:   2200 / 3715 loss=9.376, nll_loss=5.981, mask_ins=1.509, word_ins_ml=7.269, word_reposition=0.598, ppl=664.53, wps=9739.2, ups=0.66, wpb=14703.8, bsz=1024, num_updates=2200, lr=0.000220056, gnorm=2.339, clip=0, loss_scale=2048, train_wall=149, wall=3353
2022-08-02 10:41:40 | INFO | train_inner | epoch 001:   2300 / 3715 loss=9.27, nll_loss=5.874, mask_ins=1.501, word_ins_ml=7.176, word_reposition=0.593, ppl=617.17, wps=9906.2, ups=0.67, wpb=14839.5, bsz=1024, num_updates=2300, lr=0.000230054, gnorm=2.15, clip=0, loss_scale=2048, train_wall=148, wall=3503
2022-08-02 10:44:08 | INFO | train_inner | epoch 001:   2400 / 3715 loss=9.156, nll_loss=5.789, mask_ins=1.48, word_ins_ml=7.102, word_reposition=0.574, ppl=570.56, wps=9844.9, ups=0.67, wpb=14630.8, bsz=1024, num_updates=2400, lr=0.000240052, gnorm=2.15, clip=0, loss_scale=2048, train_wall=147, wall=3652
2022-08-02 10:46:37 | INFO | train_inner | epoch 001:   2500 / 3715 loss=9.091, nll_loss=5.738, mask_ins=1.466, word_ins_ml=7.058, word_reposition=0.567, ppl=545.41, wps=9865.6, ups=0.67, wpb=14677.7, bsz=1024, num_updates=2500, lr=0.00025005, gnorm=2.051, clip=0, loss_scale=2048, train_wall=147, wall=3800
2022-08-02 10:49:06 | INFO | train_inner | epoch 001:   2600 / 3715 loss=9.001, nll_loss=5.658, mask_ins=1.454, word_ins_ml=6.988, word_reposition=0.56, ppl=512.46, wps=9912, ups=0.67, wpb=14766, bsz=1024, num_updates=2600, lr=0.000260048, gnorm=1.949, clip=0, loss_scale=2888, train_wall=147, wall=3949
2022-08-02 10:51:35 | INFO | train_inner | epoch 001:   2700 / 3715 loss=8.899, nll_loss=5.567, mask_ins=1.44, word_ins_ml=6.907, word_reposition=0.552, ppl=477.45, wps=9933.8, ups=0.67, wpb=14768.3, bsz=1024, num_updates=2700, lr=0.000270046, gnorm=1.882, clip=0, loss_scale=4096, train_wall=147, wall=4098
2022-08-02 10:54:03 | INFO | train_inner | epoch 001:   2800 / 3715 loss=8.831, nll_loss=5.51, mask_ins=1.426, word_ins_ml=6.858, word_reposition=0.547, ppl=455.38, wps=9876.9, ups=0.67, wpb=14687.4, bsz=1024, num_updates=2800, lr=0.000280044, gnorm=1.759, clip=0, loss_scale=4096, train_wall=147, wall=4247
2022-08-02 10:56:32 | INFO | train_inner | epoch 001:   2900 / 3715 loss=8.787, nll_loss=5.459, mask_ins=1.425, word_ins_ml=6.813, word_reposition=0.549, ppl=441.79, wps=9780, ups=0.67, wpb=14564.6, bsz=1024, num_updates=2900, lr=0.000290042, gnorm=1.835, clip=0, loss_scale=4096, train_wall=147, wall=4396
2022-08-02 10:59:03 | INFO | train_inner | epoch 001:   3000 / 3715 loss=8.735, nll_loss=5.427, mask_ins=1.417, word_ins_ml=6.784, word_reposition=0.534, ppl=426.21, wps=9791.6, ups=0.67, wpb=14712.7, bsz=1024, num_updates=3000, lr=0.00030004, gnorm=1.739, clip=0, loss_scale=4096, train_wall=148, wall=4546
2022-08-02 11:01:35 | INFO | train_inner | epoch 001:   3100 / 3715 loss=8.655, nll_loss=5.357, mask_ins=1.4, word_ins_ml=6.722, word_reposition=0.533, ppl=403.07, wps=9648.9, ups=0.66, wpb=14666.7, bsz=1024, num_updates=3100, lr=0.000310038, gnorm=1.681, clip=0, loss_scale=5284, train_wall=150, wall=4698
2022-08-02 11:04:08 | INFO | train_inner | epoch 001:   3200 / 3715 loss=8.624, nll_loss=5.328, mask_ins=1.402, word_ins_ml=6.696, word_reposition=0.527, ppl=394.57, wps=9479, ups=0.65, wpb=14545.5, bsz=1024, num_updates=3200, lr=0.000320036, gnorm=1.595, clip=0, loss_scale=8192, train_wall=152, wall=4851
2022-08-02 11:06:37 | INFO | train_inner | epoch 001:   3300 / 3715 loss=8.551, nll_loss=5.261, mask_ins=1.392, word_ins_ml=6.638, word_reposition=0.521, ppl=375.08, wps=9870.5, ups=0.67, wpb=14682.3, bsz=1024, num_updates=3300, lr=0.000330034, gnorm=1.608, clip=0, loss_scale=8192, train_wall=147, wall=5000
2022-08-02 11:09:07 | INFO | train_inner | epoch 001:   3400 / 3715 loss=8.479, nll_loss=5.202, mask_ins=1.381, word_ins_ml=6.585, word_reposition=0.512, ppl=356.7, wps=9672.9, ups=0.66, wpb=14554.9, bsz=1024, num_updates=3400, lr=0.000340032, gnorm=1.657, clip=0, loss_scale=8192, train_wall=149, wall=5151
2022-08-02 11:11:36 | INFO | train_inner | epoch 001:   3500 / 3715 loss=8.454, nll_loss=5.173, mask_ins=1.375, word_ins_ml=6.559, word_reposition=0.52, ppl=350.65, wps=9890.4, ups=0.67, wpb=14699.8, bsz=1024, num_updates=3500, lr=0.00035003, gnorm=1.532, clip=0, loss_scale=8192, train_wall=147, wall=5299
2022-08-02 11:14:04 | INFO | train_inner | epoch 001:   3600 / 3715 loss=8.404, nll_loss=5.141, mask_ins=1.363, word_ins_ml=6.531, word_reposition=0.51, ppl=338.71, wps=9886.3, ups=0.67, wpb=14646.9, bsz=1024, num_updates=3600, lr=0.000360028, gnorm=1.546, clip=0, loss_scale=9585, train_wall=146, wall=5447
2022-08-02 11:16:32 | INFO | train_inner | epoch 001:   3700 / 3715 loss=8.387, nll_loss=5.123, mask_ins=1.369, word_ins_ml=6.514, word_reposition=0.503, ppl=334.69, wps=9835.7, ups=0.67, wpb=14580.1, bsz=1024, num_updates=3700, lr=0.000370026, gnorm=1.514, clip=0, loss_scale=16384, train_wall=146, wall=5596
2022-08-02 11:16:53 | INFO | train | epoch 001 | loss 11.04 | nll_loss 7.275 | mask_ins 1.875 | word_ins_ml 8.393 | word_reposition 0.773 | ppl 2106.12 | wps 9750.6 | ups 0.67 | wpb 14662 | bsz 1023.7 | num_updates 3715 | lr 0.000371526 | gnorm 3.392 | clip 0.4 | loss_scale 2823 | train_wall 5519 | wall 5617
2022-08-02 11:18:44 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.227 | nll_loss 4.804 | mask_ins 1.343 | word_ins_ml 6.341 | word_reposition 0.543 | ppl 299.63 | wps 25106.2 | wpb 1849.4 | bsz 127.9 | num_updates 3715
2022-08-02 11:18:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint1.pt (epoch 1 @ 3715 updates, score 8.227) (writing took 4.290112845599651 seconds)
2022-08-02 11:20:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 11:20:56 | INFO | train_inner | epoch 002:     86 / 3715 loss=8.366, nll_loss=5.101, mask_ins=1.368, word_ins_ml=6.495, word_reposition=0.504, ppl=329.95, wps=5481.7, ups=0.38, wpb=14467.1, bsz=1014.7, num_updates=3800, lr=0.000380024, gnorm=1.568, clip=0, loss_scale=13221, train_wall=147, wall=5859
2022-08-02 11:23:25 | INFO | train_inner | epoch 002:    186 / 3715 loss=8.308, nll_loss=5.044, mask_ins=1.357, word_ins_ml=6.443, word_reposition=0.508, ppl=316.98, wps=9891.2, ups=0.67, wpb=14695.8, bsz=1024, num_updates=3900, lr=0.000390022, gnorm=1.51, clip=0, loss_scale=8192, train_wall=147, wall=6008
2022-08-02 11:25:54 | INFO | train_inner | epoch 002:    286 / 3715 loss=8.285, nll_loss=5.031, mask_ins=1.352, word_ins_ml=6.431, word_reposition=0.502, ppl=311.91, wps=9821.4, ups=0.67, wpb=14649.1, bsz=1024, num_updates=4000, lr=0.00040002, gnorm=1.425, clip=0, loss_scale=8192, train_wall=147, wall=6157
2022-08-02 11:28:23 | INFO | train_inner | epoch 002:    386 / 3715 loss=8.259, nll_loss=5.012, mask_ins=1.349, word_ins_ml=6.415, word_reposition=0.495, ppl=306.29, wps=9803.7, ups=0.67, wpb=14640.1, bsz=1024, num_updates=4100, lr=0.000410018, gnorm=1.389, clip=0, loss_scale=8192, train_wall=147, wall=6307
2022-08-02 11:30:52 | INFO | train_inner | epoch 002:    486 / 3715 loss=8.214, nll_loss=4.965, mask_ins=1.34, word_ins_ml=6.373, word_reposition=0.501, ppl=296.88, wps=9825.2, ups=0.67, wpb=14637.1, bsz=1024, num_updates=4200, lr=0.000420016, gnorm=1.413, clip=0, loss_scale=8192, train_wall=147, wall=6456
2022-08-02 11:33:22 | INFO | train_inner | epoch 002:    586 / 3715 loss=8.205, nll_loss=4.949, mask_ins=1.347, word_ins_ml=6.358, word_reposition=0.5, ppl=295.07, wps=9858.9, ups=0.67, wpb=14765.2, bsz=1024, num_updates=4300, lr=0.000430014, gnorm=1.464, clip=0, loss_scale=10404, train_wall=148, wall=6605
2022-08-02 11:35:52 | INFO | train_inner | epoch 002:    686 / 3715 loss=8.147, nll_loss=4.916, mask_ins=1.335, word_ins_ml=6.328, word_reposition=0.484, ppl=283.37, wps=9767, ups=0.67, wpb=14633.2, bsz=1024, num_updates=4400, lr=0.000440012, gnorm=1.356, clip=0, loss_scale=16384, train_wall=148, wall=6755
2022-08-02 11:38:21 | INFO | train_inner | epoch 002:    786 / 3715 loss=8.135, nll_loss=4.9, mask_ins=1.336, word_ins_ml=6.314, word_reposition=0.485, ppl=281.13, wps=9881, ups=0.67, wpb=14685.4, bsz=1024, num_updates=4500, lr=0.00045001, gnorm=1.434, clip=0, loss_scale=16384, train_wall=147, wall=6904
2022-08-02 11:38:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 11:40:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-02 11:40:52 | INFO | train_inner | epoch 002:    888 / 3715 loss=8.13, nll_loss=4.893, mask_ins=1.331, word_ins_ml=6.308, word_reposition=0.49, ppl=280.08, wps=9630.5, ups=0.66, wpb=14617.5, bsz=1023.8, num_updates=4600, lr=0.000460008, gnorm=1.353, clip=0, loss_scale=8031, train_wall=150, wall=7056
2022-08-02 11:43:23 | INFO | train_inner | epoch 002:    988 / 3715 loss=8.087, nll_loss=4.867, mask_ins=1.325, word_ins_ml=6.284, word_reposition=0.478, ppl=272, wps=9734.2, ups=0.66, wpb=14646.7, bsz=1024, num_updates=4700, lr=0.000470006, gnorm=1.396, clip=0, loss_scale=4096, train_wall=149, wall=7206
2022-08-02 11:45:51 | INFO | train_inner | epoch 002:   1088 / 3715 loss=8.071, nll_loss=4.843, mask_ins=1.328, word_ins_ml=6.263, word_reposition=0.481, ppl=268.99, wps=9925.3, ups=0.68, wpb=14687.8, bsz=1024, num_updates=4800, lr=0.000480004, gnorm=1.404, clip=0, loss_scale=4096, train_wall=146, wall=7354
2022-08-02 11:48:19 | INFO | train_inner | epoch 002:   1188 / 3715 loss=8.048, nll_loss=4.823, mask_ins=1.326, word_ins_ml=6.245, word_reposition=0.477, ppl=264.7, wps=9865.1, ups=0.68, wpb=14606.3, bsz=1024, num_updates=4900, lr=0.000490002, gnorm=1.358, clip=0, loss_scale=4096, train_wall=146, wall=7502
2022-08-02 11:50:49 | INFO | train_inner | epoch 002:   1288 / 3715 loss=8.017, nll_loss=4.805, mask_ins=1.314, word_ins_ml=6.229, word_reposition=0.475, ppl=259.03, wps=9833.8, ups=0.67, wpb=14723.8, bsz=1024, num_updates=5000, lr=0.0005, gnorm=1.418, clip=0, loss_scale=4096, train_wall=148, wall=7652
2022-08-02 11:53:19 | INFO | train_inner | epoch 002:   1388 / 3715 loss=7.974, nll_loss=4.766, mask_ins=1.306, word_ins_ml=6.194, word_reposition=0.474, ppl=251.35, wps=9792.7, ups=0.67, wpb=14718.8, bsz=1024, num_updates=5100, lr=0.000495074, gnorm=1.323, clip=0, loss_scale=4260, train_wall=148, wall=7802
2022-08-02 11:55:49 | INFO | train_inner | epoch 002:   1488 / 3715 loss=7.967, nll_loss=4.754, mask_ins=1.31, word_ins_ml=6.183, word_reposition=0.475, ppl=250.22, wps=9769.7, ups=0.67, wpb=14662, bsz=1024, num_updates=5200, lr=0.00049029, gnorm=1.294, clip=0, loss_scale=8192, train_wall=148, wall=7952
2022-08-02 11:58:19 | INFO | train_inner | epoch 002:   1588 / 3715 loss=7.935, nll_loss=4.734, mask_ins=1.303, word_ins_ml=6.164, word_reposition=0.467, ppl=244.64, wps=9662.6, ups=0.67, wpb=14521.1, bsz=1024, num_updates=5300, lr=0.000485643, gnorm=1.308, clip=0, loss_scale=8192, train_wall=148, wall=8102
2022-08-02 12:00:50 | INFO | train_inner | epoch 002:   1688 / 3715 loss=7.923, nll_loss=4.716, mask_ins=1.304, word_ins_ml=6.147, word_reposition=0.471, ppl=242.66, wps=9759.3, ups=0.66, wpb=14702.3, bsz=1024, num_updates=5400, lr=0.000481125, gnorm=1.354, clip=0, loss_scale=8192, train_wall=149, wall=8253
2022-08-02 12:03:20 | INFO | train_inner | epoch 002:   1788 / 3715 loss=7.876, nll_loss=4.695, mask_ins=1.286, word_ins_ml=6.129, word_reposition=0.461, ppl=234.93, wps=9797.4, ups=0.67, wpb=14718, bsz=1024, num_updates=5500, lr=0.000476731, gnorm=1.226, clip=0, loss_scale=8192, train_wall=148, wall=8403
2022-08-02 12:05:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-02 12:05:52 | INFO | train_inner | epoch 002:   1889 / 3715 loss=7.855, nll_loss=4.659, mask_ins=1.296, word_ins_ml=6.097, word_reposition=0.462, ppl=231.48, wps=9687.5, ups=0.66, wpb=14709.1, bsz=1024, num_updates=5600, lr=0.000472456, gnorm=1.31, clip=0, loss_scale=7300, train_wall=150, wall=8555
2022-08-02 12:08:22 | INFO | train_inner | epoch 002:   1989 / 3715 loss=7.834, nll_loss=4.65, mask_ins=1.291, word_ins_ml=6.088, word_reposition=0.456, ppl=228.23, wps=9788.6, ups=0.67, wpb=14663.6, bsz=1024, num_updates=5700, lr=0.000468293, gnorm=1.31, clip=0, loss_scale=4096, train_wall=148, wall=8705
2022-08-02 12:10:52 | INFO | train_inner | epoch 002:   2089 / 3715 loss=7.808, nll_loss=4.626, mask_ins=1.287, word_ins_ml=6.066, word_reposition=0.455, ppl=224.13, wps=9738.5, ups=0.66, wpb=14658.4, bsz=1024, num_updates=5800, lr=0.000464238, gnorm=1.274, clip=0, loss_scale=4096, train_wall=149, wall=8855
2022-08-02 12:13:23 | INFO | train_inner | epoch 002:   2189 / 3715 loss=7.785, nll_loss=4.602, mask_ins=1.284, word_ins_ml=6.045, word_reposition=0.456, ppl=220.58, wps=9788.6, ups=0.66, wpb=14726.9, bsz=1024, num_updates=5900, lr=0.000460287, gnorm=1.3, clip=0, loss_scale=4096, train_wall=149, wall=9006
2022-08-02 12:15:53 | INFO | train_inner | epoch 002:   2289 / 3715 loss=7.774, nll_loss=4.597, mask_ins=1.274, word_ins_ml=6.04, word_reposition=0.46, ppl=218.85, wps=9753.8, ups=0.66, wpb=14669.2, bsz=1024, num_updates=6000, lr=0.000456435, gnorm=1.257, clip=0, loss_scale=4096, train_wall=148, wall=9156
2022-08-02 12:18:23 | INFO | train_inner | epoch 002:   2389 / 3715 loss=7.746, nll_loss=4.57, mask_ins=1.276, word_ins_ml=6.015, word_reposition=0.455, ppl=214.68, wps=9778.7, ups=0.67, wpb=14669.3, bsz=1024, num_updates=6100, lr=0.000452679, gnorm=1.255, clip=0, loss_scale=4506, train_wall=148, wall=9306
2022-08-02 12:20:53 | INFO | train_inner | epoch 002:   2489 / 3715 loss=7.71, nll_loss=4.536, mask_ins=1.267, word_ins_ml=5.985, word_reposition=0.458, ppl=209.34, wps=9747, ups=0.67, wpb=14616.9, bsz=1024, num_updates=6200, lr=0.000449013, gnorm=1.214, clip=0, loss_scale=8192, train_wall=148, wall=9456
2022-08-02 12:23:23 | INFO | train_inner | epoch 002:   2589 / 3715 loss=7.671, nll_loss=4.51, mask_ins=1.262, word_ins_ml=5.962, word_reposition=0.447, ppl=203.84, wps=9742.6, ups=0.67, wpb=14599.9, bsz=1024, num_updates=6300, lr=0.000445435, gnorm=1.241, clip=0, loss_scale=8192, train_wall=148, wall=9606
2022-08-02 12:25:52 | INFO | train_inner | epoch 002:   2689 / 3715 loss=7.701, nll_loss=4.534, mask_ins=1.268, word_ins_ml=5.983, word_reposition=0.45, ppl=208.09, wps=9800.4, ups=0.67, wpb=14580.7, bsz=1024, num_updates=6400, lr=0.000441942, gnorm=1.264, clip=0, loss_scale=8192, train_wall=147, wall=9755
2022-08-02 12:28:20 | INFO | train_inner | epoch 002:   2789 / 3715 loss=7.645, nll_loss=4.492, mask_ins=1.257, word_ins_ml=5.945, word_reposition=0.444, ppl=200.23, wps=9879.4, ups=0.67, wpb=14694.9, bsz=1024, num_updates=6500, lr=0.000438529, gnorm=1.16, clip=0, loss_scale=8192, train_wall=147, wall=9904
2022-08-02 12:30:49 | INFO | train_inner | epoch 002:   2889 / 3715 loss=7.647, nll_loss=4.489, mask_ins=1.261, word_ins_ml=5.942, word_reposition=0.445, ppl=200.47, wps=9911.5, ups=0.68, wpb=14679.7, bsz=1024, num_updates=6600, lr=0.000435194, gnorm=1.168, clip=0, loss_scale=8192, train_wall=146, wall=10052
2022-08-02 12:33:18 | INFO | train_inner | epoch 002:   2989 / 3715 loss=7.636, nll_loss=4.487, mask_ins=1.254, word_ins_ml=5.94, word_reposition=0.443, ppl=198.92, wps=9731.3, ups=0.67, wpb=14567.9, bsz=1024, num_updates=6700, lr=0.000431934, gnorm=1.216, clip=0, loss_scale=16220, train_wall=148, wall=10201
2022-08-02 12:35:50 | INFO | train_inner | epoch 002:   3089 / 3715 loss=7.634, nll_loss=4.475, mask_ins=1.256, word_ins_ml=5.928, word_reposition=0.449, ppl=198.62, wps=9675.6, ups=0.66, wpb=14635.7, bsz=1024, num_updates=6800, lr=0.000428746, gnorm=1.26, clip=0, loss_scale=16384, train_wall=149, wall=10353
2022-08-02 12:36:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 12:38:23 | INFO | train_inner | epoch 002:   3190 / 3715 loss=7.58, nll_loss=4.419, mask_ins=1.252, word_ins_ml=5.879, word_reposition=0.449, ppl=191.33, wps=9636.7, ups=0.65, wpb=14825.8, bsz=1024, num_updates=6900, lr=0.000425628, gnorm=1.22, clip=0, loss_scale=11193, train_wall=152, wall=10507
2022-08-02 12:39:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-02 12:39:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-02 12:39:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-02 12:39:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-02 12:39:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-02 12:41:02 | INFO | train_inner | epoch 002:   3295 / 3715 loss=7.591, nll_loss=4.443, mask_ins=1.255, word_ins_ml=5.9, word_reposition=0.436, ppl=192.84, wps=9233.5, ups=0.63, wpb=14631.5, bsz=1024, num_updates=7000, lr=0.000422577, gnorm=1.23, clip=0, loss_scale=3206, train_wall=156, wall=10665
2022-08-02 12:43:33 | INFO | train_inner | epoch 002:   3395 / 3715 loss=7.546, nll_loss=4.414, mask_ins=1.238, word_ins_ml=5.874, word_reposition=0.434, ppl=186.9, wps=9741.2, ups=0.66, wpb=14707.9, bsz=1024, num_updates=7100, lr=0.000419591, gnorm=1.204, clip=0, loss_scale=256, train_wall=149, wall=10816
2022-08-02 12:46:02 | INFO | train_inner | epoch 002:   3495 / 3715 loss=7.547, nll_loss=4.393, mask_ins=1.246, word_ins_ml=5.855, word_reposition=0.447, ppl=187.04, wps=9907.3, ups=0.67, wpb=14727.4, bsz=1024, num_updates=7200, lr=0.000416667, gnorm=1.213, clip=0, loss_scale=256, train_wall=147, wall=10965
2022-08-02 12:48:30 | INFO | train_inner | epoch 002:   3595 / 3715 loss=7.539, nll_loss=4.4, mask_ins=1.243, word_ins_ml=5.861, word_reposition=0.434, ppl=185.92, wps=9814.6, ups=0.67, wpb=14600.9, bsz=1024, num_updates=7300, lr=0.000413803, gnorm=1.204, clip=0, loss_scale=256, train_wall=147, wall=11113
2022-08-02 12:50:59 | INFO | train_inner | epoch 002:   3695 / 3715 loss=7.508, nll_loss=4.366, mask_ins=1.24, word_ins_ml=5.83, word_reposition=0.437, ppl=182.02, wps=9915.6, ups=0.67, wpb=14786.3, bsz=1024, num_updates=7400, lr=0.000410997, gnorm=1.262, clip=0, loss_scale=256, train_wall=147, wall=11263
2022-08-02 12:51:28 | INFO | train | epoch 002 | loss 7.88 | nll_loss 4.686 | mask_ins 1.295 | word_ins_ml 6.12 | word_reposition 0.466 | ppl 235.61 | wps 9572.9 | ups 0.65 | wpb 14662.3 | bsz 1023.7 | num_updates 7420 | lr 0.000410443 | gnorm 1.31 | clip 0 | loss_scale 7160 | train_wall 5490 | wall 11291
2022-08-02 12:53:19 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.353 | nll_loss 4.075 | mask_ins 1.215 | word_ins_ml 5.678 | word_reposition 0.461 | ppl 163.52 | wps 24771.8 | wpb 1849.4 | bsz 127.9 | num_updates 7420 | best_loss 7.353
2022-08-02 12:53:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint2.pt (epoch 2 @ 7420 updates, score 7.353) (writing took 6.25530918687582 seconds)
2022-08-02 12:55:25 | INFO | train_inner | epoch 003:     80 / 3715 loss=7.493, nll_loss=4.361, mask_ins=1.237, word_ins_ml=5.826, word_reposition=0.431, ppl=180.21, wps=5501.4, ups=0.38, wpb=14638.4, bsz=1014.7, num_updates=7500, lr=0.000408248, gnorm=1.206, clip=0, loss_scale=381, train_wall=147, wall=11529
2022-08-02 12:57:56 | INFO | train_inner | epoch 003:    180 / 3715 loss=7.458, nll_loss=4.331, mask_ins=1.229, word_ins_ml=5.8, word_reposition=0.43, ppl=175.86, wps=9706.5, ups=0.67, wpb=14582.1, bsz=1024, num_updates=7600, lr=0.000405554, gnorm=1.177, clip=0, loss_scale=512, train_wall=148, wall=11679
2022-08-02 13:00:26 | INFO | train_inner | epoch 003:    280 / 3715 loss=7.472, nll_loss=4.336, mask_ins=1.23, word_ins_ml=5.804, word_reposition=0.438, ppl=177.55, wps=9817.5, ups=0.67, wpb=14722.8, bsz=1024, num_updates=7700, lr=0.000402911, gnorm=1.142, clip=0, loss_scale=512, train_wall=148, wall=11829
2022-08-02 13:02:55 | INFO | train_inner | epoch 003:    380 / 3715 loss=7.44, nll_loss=4.306, mask_ins=1.23, word_ins_ml=5.776, word_reposition=0.433, ppl=173.63, wps=9848.8, ups=0.67, wpb=14748.4, bsz=1024, num_updates=7800, lr=0.00040032, gnorm=1.16, clip=0, loss_scale=512, train_wall=148, wall=11979
2022-08-02 13:05:24 | INFO | train_inner | epoch 003:    480 / 3715 loss=7.446, nll_loss=4.314, mask_ins=1.228, word_ins_ml=5.783, word_reposition=0.434, ppl=174.34, wps=9910.7, ups=0.67, wpb=14683.9, bsz=1024, num_updates=7900, lr=0.000397779, gnorm=1.212, clip=0, loss_scale=512, train_wall=146, wall=12127
2022-08-02 13:07:51 | INFO | train_inner | epoch 003:    580 / 3715 loss=7.428, nll_loss=4.297, mask_ins=1.227, word_ins_ml=5.768, word_reposition=0.433, ppl=172.26, wps=9963.9, ups=0.68, wpb=14734, bsz=1024, num_updates=8000, lr=0.000395285, gnorm=1.214, clip=0, loss_scale=701, train_wall=146, wall=12275
2022-08-02 13:10:20 | INFO | train_inner | epoch 003:    680 / 3715 loss=7.432, nll_loss=4.306, mask_ins=1.226, word_ins_ml=5.775, word_reposition=0.431, ppl=172.71, wps=9889.3, ups=0.67, wpb=14707.5, bsz=1024, num_updates=8100, lr=0.000392837, gnorm=1.135, clip=0, loss_scale=1024, train_wall=147, wall=12423
2022-08-02 13:12:49 | INFO | train_inner | epoch 003:    780 / 3715 loss=7.389, nll_loss=4.27, mask_ins=1.22, word_ins_ml=5.744, word_reposition=0.424, ppl=167.62, wps=9831, ups=0.67, wpb=14641.9, bsz=1024, num_updates=8200, lr=0.000390434, gnorm=1.13, clip=0, loss_scale=1024, train_wall=147, wall=12572
2022-08-02 13:15:18 | INFO | train_inner | epoch 003:    880 / 3715 loss=7.385, nll_loss=4.272, mask_ins=1.215, word_ins_ml=5.745, word_reposition=0.425, ppl=167.2, wps=9854.3, ups=0.67, wpb=14704.1, bsz=1024, num_updates=8300, lr=0.000388075, gnorm=1.205, clip=0, loss_scale=1024, train_wall=147, wall=12722
2022-08-02 13:17:50 | INFO | train_inner | epoch 003:    980 / 3715 loss=7.389, nll_loss=4.271, mask_ins=1.218, word_ins_ml=5.745, word_reposition=0.427, ppl=167.62, wps=9630.9, ups=0.66, wpb=14604.5, bsz=1024, num_updates=8400, lr=0.000385758, gnorm=1.167, clip=0, loss_scale=1024, train_wall=150, wall=12873
2022-08-02 13:20:20 | INFO | train_inner | epoch 003:   1080 / 3715 loss=7.388, nll_loss=4.272, mask_ins=1.219, word_ins_ml=5.745, word_reposition=0.425, ppl=167.5, wps=9776.6, ups=0.67, wpb=14660.6, bsz=1024, num_updates=8500, lr=0.000383482, gnorm=1.196, clip=0, loss_scale=1280, train_wall=148, wall=13023
2022-08-02 13:22:49 | INFO | train_inner | epoch 003:   1180 / 3715 loss=7.362, nll_loss=4.247, mask_ins=1.215, word_ins_ml=5.722, word_reposition=0.425, ppl=164.49, wps=9774.2, ups=0.67, wpb=14593.4, bsz=1024, num_updates=8600, lr=0.000381246, gnorm=1.187, clip=0, loss_scale=2048, train_wall=147, wall=13172
2022-08-02 13:25:19 | INFO | train_inner | epoch 003:   1280 / 3715 loss=7.384, nll_loss=4.27, mask_ins=1.221, word_ins_ml=5.742, word_reposition=0.421, ppl=167.07, wps=9727.1, ups=0.67, wpb=14562.9, bsz=1024, num_updates=8700, lr=0.000379049, gnorm=1.176, clip=0, loss_scale=2048, train_wall=148, wall=13322
2022-08-02 13:27:49 | INFO | train_inner | epoch 003:   1380 / 3715 loss=7.346, nll_loss=4.232, mask_ins=1.217, word_ins_ml=5.709, word_reposition=0.42, ppl=162.68, wps=9804.9, ups=0.67, wpb=14671.5, bsz=1024, num_updates=8800, lr=0.000376889, gnorm=1.202, clip=0, loss_scale=2048, train_wall=148, wall=13472
2022-08-02 13:30:19 | INFO | train_inner | epoch 003:   1480 / 3715 loss=7.338, nll_loss=4.234, mask_ins=1.208, word_ins_ml=5.71, word_reposition=0.421, ppl=161.85, wps=9748.7, ups=0.67, wpb=14630.8, bsz=1024, num_updates=8900, lr=0.000374766, gnorm=1.172, clip=0, loss_scale=2048, train_wall=148, wall=13622
2022-08-02 13:32:50 | INFO | train_inner | epoch 003:   1580 / 3715 loss=7.332, nll_loss=4.223, mask_ins=1.213, word_ins_ml=5.701, word_reposition=0.419, ppl=161.14, wps=9634, ups=0.66, wpb=14594, bsz=1024, num_updates=9000, lr=0.000372678, gnorm=1.142, clip=0, loss_scale=2314, train_wall=150, wall=13773
2022-08-02 13:35:22 | INFO | train_inner | epoch 003:   1680 / 3715 loss=7.302, nll_loss=4.211, mask_ins=1.197, word_ins_ml=5.689, word_reposition=0.416, ppl=157.82, wps=9612.6, ups=0.66, wpb=14626.2, bsz=1024, num_updates=9100, lr=0.000370625, gnorm=1.118, clip=0, loss_scale=4096, train_wall=150, wall=13925
2022-08-02 13:37:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-02 13:37:56 | INFO | train_inner | epoch 003:   1781 / 3715 loss=7.344, nll_loss=4.231, mask_ins=1.216, word_ins_ml=5.707, word_reposition=0.421, ppl=162.46, wps=9593.5, ups=0.65, wpb=14735.6, bsz=1024, num_updates=9200, lr=0.000368605, gnorm=1.198, clip=0, loss_scale=3467, train_wall=152, wall=14079
2022-08-02 13:40:28 | INFO | train_inner | epoch 003:   1881 / 3715 loss=7.318, nll_loss=4.207, mask_ins=1.216, word_ins_ml=5.686, word_reposition=0.417, ppl=159.62, wps=9617.7, ups=0.66, wpb=14627, bsz=1024, num_updates=9300, lr=0.000366618, gnorm=1.177, clip=0, loss_scale=2048, train_wall=150, wall=14231
2022-08-02 13:43:00 | INFO | train_inner | epoch 003:   1981 / 3715 loss=7.32, nll_loss=4.214, mask_ins=1.206, word_ins_ml=5.691, word_reposition=0.424, ppl=159.83, wps=9602.6, ups=0.66, wpb=14576.9, bsz=1023.8, num_updates=9400, lr=0.000364662, gnorm=1.187, clip=0, loss_scale=2048, train_wall=150, wall=14383
2022-08-02 13:45:30 | INFO | train_inner | epoch 003:   2081 / 3715 loss=7.273, nll_loss=4.175, mask_ins=1.201, word_ins_ml=5.657, word_reposition=0.416, ppl=154.7, wps=9754.4, ups=0.67, wpb=14604.9, bsz=1024, num_updates=9500, lr=0.000362738, gnorm=1.121, clip=0, loss_scale=2048, train_wall=148, wall=14533
2022-08-02 13:47:59 | INFO | train_inner | epoch 003:   2181 / 3715 loss=7.285, nll_loss=4.182, mask_ins=1.203, word_ins_ml=5.663, word_reposition=0.42, ppl=155.99, wps=9881.8, ups=0.67, wpb=14731.3, bsz=1024, num_updates=9600, lr=0.000360844, gnorm=1.173, clip=0, loss_scale=2048, train_wall=147, wall=14682
2022-08-02 13:50:28 | INFO | train_inner | epoch 003:   2281 / 3715 loss=7.269, nll_loss=4.168, mask_ins=1.199, word_ins_ml=5.65, word_reposition=0.42, ppl=154.2, wps=9859.1, ups=0.67, wpb=14692.3, bsz=1024, num_updates=9700, lr=0.000358979, gnorm=1.144, clip=0, loss_scale=2437, train_wall=147, wall=14831
2022-08-02 13:52:57 | INFO | train_inner | epoch 003:   2381 / 3715 loss=7.238, nll_loss=4.146, mask_ins=1.195, word_ins_ml=5.63, word_reposition=0.413, ppl=150.94, wps=9843.1, ups=0.67, wpb=14668.5, bsz=1024, num_updates=9800, lr=0.000357143, gnorm=1.133, clip=0, loss_scale=4096, train_wall=147, wall=14980
2022-08-02 13:55:27 | INFO | train_inner | epoch 003:   2481 / 3715 loss=7.264, nll_loss=4.167, mask_ins=1.199, word_ins_ml=5.649, word_reposition=0.416, ppl=153.71, wps=9831.8, ups=0.67, wpb=14770.3, bsz=1024, num_updates=9900, lr=0.000355335, gnorm=1.083, clip=0, loss_scale=4096, train_wall=148, wall=15130
2022-08-02 13:57:59 | INFO | train_inner | epoch 003:   2581 / 3715 loss=7.248, nll_loss=4.158, mask_ins=1.194, word_ins_ml=5.64, word_reposition=0.414, ppl=151.99, wps=9579.9, ups=0.66, wpb=14554.8, bsz=1024, num_updates=10000, lr=0.000353553, gnorm=1.157, clip=0, loss_scale=4096, train_wall=150, wall=15282
2022-08-02 14:00:31 | INFO | train_inner | epoch 003:   2681 / 3715 loss=7.247, nll_loss=4.15, mask_ins=1.199, word_ins_ml=5.633, word_reposition=0.416, ppl=151.91, wps=9692.2, ups=0.66, wpb=14713.1, bsz=1024, num_updates=10100, lr=0.000351799, gnorm=1.154, clip=0, loss_scale=4096, train_wall=150, wall=15434
2022-08-02 14:03:02 | INFO | train_inner | epoch 003:   2781 / 3715 loss=7.251, nll_loss=4.165, mask_ins=1.195, word_ins_ml=5.646, word_reposition=0.411, ppl=152.34, wps=9648.9, ups=0.66, wpb=14609.8, bsz=1024, num_updates=10200, lr=0.00035007, gnorm=1.106, clip=0, loss_scale=4383, train_wall=150, wall=15585
2022-08-02 14:05:33 | INFO | train_inner | epoch 003:   2881 / 3715 loss=7.237, nll_loss=4.139, mask_ins=1.193, word_ins_ml=5.623, word_reposition=0.421, ppl=150.83, wps=9739.3, ups=0.66, wpb=14665.8, bsz=1024, num_updates=10300, lr=0.000348367, gnorm=1.162, clip=0, loss_scale=8192, train_wall=149, wall=15736
2022-08-02 14:08:03 | INFO | train_inner | epoch 003:   2981 / 3715 loss=7.218, nll_loss=4.127, mask_ins=1.195, word_ins_ml=5.613, word_reposition=0.411, ppl=148.89, wps=9778.4, ups=0.66, wpb=14732.7, bsz=1024, num_updates=10400, lr=0.000346688, gnorm=1.145, clip=0, loss_scale=8192, train_wall=149, wall=15886
2022-08-02 14:10:33 | INFO | train_inner | epoch 003:   3081 / 3715 loss=7.21, nll_loss=4.123, mask_ins=1.191, word_ins_ml=5.609, word_reposition=0.409, ppl=148.04, wps=9894.3, ups=0.67, wpb=14775.5, bsz=1024, num_updates=10500, lr=0.000345033, gnorm=1.136, clip=0, loss_scale=8192, train_wall=147, wall=16036
2022-08-02 14:13:02 | INFO | train_inner | epoch 003:   3181 / 3715 loss=7.224, nll_loss=4.138, mask_ins=1.19, word_ins_ml=5.622, word_reposition=0.412, ppl=149.5, wps=9773.1, ups=0.67, wpb=14632, bsz=1024, num_updates=10600, lr=0.000343401, gnorm=1.093, clip=0, loss_scale=8192, train_wall=148, wall=16186
2022-08-02 14:15:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-02 14:15:33 | INFO | train_inner | epoch 003:   3282 / 3715 loss=7.215, nll_loss=4.13, mask_ins=1.191, word_ins_ml=5.614, word_reposition=0.409, ppl=148.52, wps=9706.3, ups=0.66, wpb=14652.9, bsz=1024, num_updates=10700, lr=0.000341793, gnorm=1.117, clip=0, loss_scale=7462, train_wall=149, wall=16336
2022-08-02 14:18:04 | INFO | train_inner | epoch 003:   3382 / 3715 loss=7.201, nll_loss=4.125, mask_ins=1.183, word_ins_ml=5.61, word_reposition=0.408, ppl=147.13, wps=9666.6, ups=0.66, wpb=14568.9, bsz=1024, num_updates=10800, lr=0.000340207, gnorm=1.142, clip=0, loss_scale=4096, train_wall=149, wall=16487
2022-08-02 14:20:34 | INFO | train_inner | epoch 003:   3482 / 3715 loss=7.191, nll_loss=4.109, mask_ins=1.18, word_ins_ml=5.596, word_reposition=0.415, ppl=146.15, wps=9830.5, ups=0.67, wpb=14735.8, bsz=1024, num_updates=10900, lr=0.000338643, gnorm=1.127, clip=0, loss_scale=4096, train_wall=148, wall=16637
2022-08-02 14:23:04 | INFO | train_inner | epoch 003:   3582 / 3715 loss=7.186, nll_loss=4.097, mask_ins=1.187, word_ins_ml=5.584, word_reposition=0.415, ppl=145.63, wps=9815.1, ups=0.67, wpb=14724, bsz=1024, num_updates=11000, lr=0.0003371, gnorm=1.146, clip=0, loss_scale=4096, train_wall=148, wall=16787
2022-08-02 14:25:33 | INFO | train_inner | epoch 003:   3682 / 3715 loss=7.193, nll_loss=4.109, mask_ins=1.187, word_ins_ml=5.595, word_reposition=0.411, ppl=146.31, wps=9818.4, ups=0.67, wpb=14587, bsz=1024, num_updates=11100, lr=0.000335578, gnorm=1.118, clip=0, loss_scale=4096, train_wall=147, wall=16936
2022-08-02 14:26:21 | INFO | train | epoch 003 | loss 7.314 | nll_loss 4.209 | mask_ins 1.207 | word_ins_ml 5.687 | word_reposition 0.42 | ppl 159.16 | wps 9563.4 | ups 0.65 | wpb 14662.2 | bsz 1023.7 | num_updates 11133 | lr 0.00033508 | gnorm 1.156 | clip 0 | loss_scale 3122 | train_wall 5506 | wall 16984
2022-08-02 14:28:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.034 | nll_loss 3.834 | mask_ins 1.151 | word_ins_ml 5.447 | word_reposition 0.435 | ppl 131.02 | wps 24948.5 | wpb 1849.4 | bsz 127.9 | num_updates 11133 | best_loss 7.034
2022-08-02 14:28:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint3.pt (epoch 3 @ 11133 updates, score 7.034) (writing took 6.346753258258104 seconds)
2022-08-02 14:29:58 | INFO | train_inner | epoch 004:     67 / 3715 loss=7.138, nll_loss=4.067, mask_ins=1.175, word_ins_ml=5.558, word_reposition=0.406, ppl=140.88, wps=5471.4, ups=0.38, wpb=14514.7, bsz=1014.7, num_updates=11200, lr=0.000334077, gnorm=1.126, clip=0, loss_scale=4342, train_wall=147, wall=17201
2022-08-02 14:32:29 | INFO | train_inner | epoch 004:    167 / 3715 loss=7.129, nll_loss=4.051, mask_ins=1.177, word_ins_ml=5.544, word_reposition=0.408, ppl=140.01, wps=9794.5, ups=0.66, wpb=14777.2, bsz=1024, num_updates=11300, lr=0.000332595, gnorm=1.102, clip=0, loss_scale=8192, train_wall=149, wall=17352
2022-08-02 14:34:57 | INFO | train_inner | epoch 004:    267 / 3715 loss=7.157, nll_loss=4.081, mask_ins=1.183, word_ins_ml=5.57, word_reposition=0.404, ppl=142.7, wps=9926.4, ups=0.67, wpb=14753.4, bsz=1024, num_updates=11400, lr=0.000331133, gnorm=1.094, clip=0, loss_scale=8192, train_wall=147, wall=17500
2022-08-02 14:37:26 | INFO | train_inner | epoch 004:    367 / 3715 loss=7.129, nll_loss=4.047, mask_ins=1.178, word_ins_ml=5.54, word_reposition=0.411, ppl=139.98, wps=9837.3, ups=0.67, wpb=14645, bsz=1024, num_updates=11500, lr=0.00032969, gnorm=1.113, clip=0, loss_scale=8192, train_wall=147, wall=17649
2022-08-02 14:39:57 | INFO | train_inner | epoch 004:    467 / 3715 loss=7.104, nll_loss=4.037, mask_ins=1.173, word_ins_ml=5.531, word_reposition=0.4, ppl=137.56, wps=9714.4, ups=0.66, wpb=14630.5, bsz=1024, num_updates=11600, lr=0.000328266, gnorm=1.102, clip=0, loss_scale=8192, train_wall=149, wall=17800
2022-08-02 14:42:27 | INFO | train_inner | epoch 004:    567 / 3715 loss=7.136, nll_loss=4.063, mask_ins=1.177, word_ins_ml=5.554, word_reposition=0.404, ppl=140.63, wps=9670.2, ups=0.66, wpb=14573.7, bsz=1024, num_updates=11700, lr=0.00032686, gnorm=1.11, clip=0, loss_scale=8192, train_wall=149, wall=17951
2022-08-02 14:44:58 | INFO | train_inner | epoch 004:    667 / 3715 loss=7.113, nll_loss=4.042, mask_ins=1.173, word_ins_ml=5.535, word_reposition=0.405, ppl=138.42, wps=9789.5, ups=0.67, wpb=14708, bsz=1024, num_updates=11800, lr=0.000325472, gnorm=1.149, clip=0, loss_scale=15892, train_wall=148, wall=18101
2022-08-02 14:47:29 | INFO | train_inner | epoch 004:    767 / 3715 loss=7.118, nll_loss=4.046, mask_ins=1.177, word_ins_ml=5.539, word_reposition=0.403, ppl=138.95, wps=9708.4, ups=0.66, wpb=14656, bsz=1024, num_updates=11900, lr=0.000324102, gnorm=1.119, clip=0, loss_scale=16384, train_wall=149, wall=18252
2022-08-02 14:48:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 14:50:02 | INFO | train_inner | epoch 004:    868 / 3715 loss=7.11, nll_loss=4.034, mask_ins=1.174, word_ins_ml=5.527, word_reposition=0.41, ppl=138.14, wps=9651.2, ups=0.65, wpb=14748, bsz=1024, num_updates=12000, lr=0.000322749, gnorm=1.125, clip=0, loss_scale=10544, train_wall=151, wall=18405
2022-08-02 14:52:31 | INFO | train_inner | epoch 004:    968 / 3715 loss=7.128, nll_loss=4.06, mask_ins=1.172, word_ins_ml=5.55, word_reposition=0.405, ppl=139.85, wps=9729.5, ups=0.67, wpb=14578.6, bsz=1024, num_updates=12100, lr=0.000321412, gnorm=1.103, clip=0, loss_scale=8192, train_wall=148, wall=18555
2022-08-02 14:55:01 | INFO | train_inner | epoch 004:   1068 / 3715 loss=7.107, nll_loss=4.043, mask_ins=1.172, word_ins_ml=5.535, word_reposition=0.4, ppl=137.86, wps=9745.7, ups=0.67, wpb=14625, bsz=1023.8, num_updates=12200, lr=0.000320092, gnorm=1.109, clip=0, loss_scale=8192, train_wall=148, wall=18705
2022-08-02 14:57:32 | INFO | train_inner | epoch 004:   1168 / 3715 loss=7.106, nll_loss=4.032, mask_ins=1.172, word_ins_ml=5.526, word_reposition=0.408, ppl=137.74, wps=9732.7, ups=0.66, wpb=14698.3, bsz=1024, num_updates=12300, lr=0.000318788, gnorm=1.107, clip=0, loss_scale=8192, train_wall=149, wall=18856
2022-08-02 14:59:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-02 15:00:04 | INFO | train_inner | epoch 004:   1269 / 3715 loss=7.103, nll_loss=4.029, mask_ins=1.178, word_ins_ml=5.522, word_reposition=0.402, ppl=137.48, wps=9656.2, ups=0.66, wpb=14680.5, bsz=1024, num_updates=12400, lr=0.0003175, gnorm=1.12, clip=0, loss_scale=7543, train_wall=150, wall=19008
2022-08-02 15:02:35 | INFO | train_inner | epoch 004:   1369 / 3715 loss=7.058, nll_loss=3.999, mask_ins=1.16, word_ins_ml=5.495, word_reposition=0.403, ppl=133.26, wps=9758.3, ups=0.66, wpb=14724.5, bsz=1024, num_updates=12500, lr=0.000316228, gnorm=1.092, clip=0, loss_scale=4096, train_wall=149, wall=19159
2022-08-02 15:05:07 | INFO | train_inner | epoch 004:   1469 / 3715 loss=7.081, nll_loss=4.013, mask_ins=1.17, word_ins_ml=5.507, word_reposition=0.403, ppl=135.36, wps=9674.1, ups=0.66, wpb=14658.3, bsz=1024, num_updates=12600, lr=0.00031497, gnorm=1.09, clip=0, loss_scale=4096, train_wall=150, wall=19310
2022-08-02 15:07:38 | INFO | train_inner | epoch 004:   1569 / 3715 loss=7.063, nll_loss=4.002, mask_ins=1.164, word_ins_ml=5.498, word_reposition=0.401, ppl=133.69, wps=9790.9, ups=0.66, wpb=14773.6, bsz=1024, num_updates=12700, lr=0.000313728, gnorm=1.096, clip=0, loss_scale=4096, train_wall=149, wall=19461
2022-08-02 15:10:09 | INFO | train_inner | epoch 004:   1669 / 3715 loss=7.059, nll_loss=3.996, mask_ins=1.165, word_ins_ml=5.492, word_reposition=0.402, ppl=133.37, wps=9709.2, ups=0.66, wpb=14653.5, bsz=1024, num_updates=12800, lr=0.0003125, gnorm=1.074, clip=0, loss_scale=4096, train_wall=149, wall=19612
2022-08-02 15:12:40 | INFO | train_inner | epoch 004:   1769 / 3715 loss=7.068, nll_loss=3.999, mask_ins=1.16, word_ins_ml=5.495, word_reposition=0.412, ppl=134.15, wps=9704.1, ups=0.66, wpb=14729.8, bsz=1024, num_updates=12900, lr=0.000311286, gnorm=1.084, clip=0, loss_scale=4260, train_wall=150, wall=19764
2022-08-02 15:15:12 | INFO | train_inner | epoch 004:   1869 / 3715 loss=7.062, nll_loss=4.003, mask_ins=1.163, word_ins_ml=5.498, word_reposition=0.401, ppl=133.63, wps=9663, ups=0.66, wpb=14678.4, bsz=1024, num_updates=13000, lr=0.000310087, gnorm=1.127, clip=0, loss_scale=8192, train_wall=150, wall=19916
2022-08-02 15:17:44 | INFO | train_inner | epoch 004:   1969 / 3715 loss=7.04, nll_loss=3.98, mask_ins=1.158, word_ins_ml=5.478, word_reposition=0.404, ppl=131.6, wps=9654.5, ups=0.66, wpb=14607.9, bsz=1024, num_updates=13100, lr=0.000308901, gnorm=1.101, clip=0, loss_scale=8192, train_wall=149, wall=20067
2022-08-02 15:20:15 | INFO | train_inner | epoch 004:   2069 / 3715 loss=7.06, nll_loss=4.004, mask_ins=1.163, word_ins_ml=5.499, word_reposition=0.398, ppl=133.43, wps=9642.2, ups=0.66, wpb=14595.3, bsz=1024, num_updates=13200, lr=0.000307729, gnorm=1.087, clip=0, loss_scale=8192, train_wall=149, wall=20218
2022-08-02 15:22:46 | INFO | train_inner | epoch 004:   2169 / 3715 loss=7.07, nll_loss=4.008, mask_ins=1.167, word_ins_ml=5.502, word_reposition=0.4, ppl=134.4, wps=9713.6, ups=0.66, wpb=14649.5, bsz=1024, num_updates=13300, lr=0.00030657, gnorm=1.09, clip=0, loss_scale=8192, train_wall=149, wall=20369
2022-08-02 15:25:16 | INFO | train_inner | epoch 004:   2269 / 3715 loss=7.054, nll_loss=3.984, mask_ins=1.163, word_ins_ml=5.482, word_reposition=0.41, ppl=132.86, wps=9835.7, ups=0.66, wpb=14807.4, bsz=1024, num_updates=13400, lr=0.000305424, gnorm=1.085, clip=0, loss_scale=8192, train_wall=149, wall=20520
2022-08-02 15:27:47 | INFO | train_inner | epoch 004:   2369 / 3715 loss=7.03, nll_loss=3.965, mask_ins=1.163, word_ins_ml=5.463, word_reposition=0.404, ppl=130.67, wps=9779.1, ups=0.66, wpb=14749.1, bsz=1024, num_updates=13500, lr=0.00030429, gnorm=1.09, clip=0, loss_scale=15729, train_wall=149, wall=20670
2022-08-02 15:30:18 | INFO | train_inner | epoch 004:   2469 / 3715 loss=7.044, nll_loss=3.986, mask_ins=1.161, word_ins_ml=5.483, word_reposition=0.4, ppl=131.94, wps=9778.1, ups=0.66, wpb=14750.8, bsz=1024, num_updates=13600, lr=0.00030317, gnorm=1.063, clip=0, loss_scale=16384, train_wall=149, wall=20821
2022-08-02 15:32:49 | INFO | train_inner | epoch 004:   2569 / 3715 loss=7.013, nll_loss=3.965, mask_ins=1.157, word_ins_ml=5.465, word_reposition=0.392, ppl=129.18, wps=9667.1, ups=0.66, wpb=14584.2, bsz=1024, num_updates=13700, lr=0.000302061, gnorm=1.14, clip=0, loss_scale=16384, train_wall=149, wall=20972
2022-08-02 15:35:19 | INFO | train_inner | epoch 004:   2669 / 3715 loss=7.036, nll_loss=3.992, mask_ins=1.152, word_ins_ml=5.487, word_reposition=0.397, ppl=131.28, wps=9778.3, ups=0.67, wpb=14668.7, bsz=1024, num_updates=13800, lr=0.000300965, gnorm=1.078, clip=0, loss_scale=16384, train_wall=148, wall=21122
2022-08-02 15:37:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 15:37:51 | INFO | train_inner | epoch 004:   2770 / 3715 loss=7.003, nll_loss=3.956, mask_ins=1.153, word_ins_ml=5.456, word_reposition=0.393, ppl=128.22, wps=9599.6, ups=0.66, wpb=14562.5, bsz=1024, num_updates=13900, lr=0.00029988, gnorm=1.091, clip=0, loss_scale=15735, train_wall=150, wall=21274
2022-08-02 15:39:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-02 15:40:22 | INFO | train_inner | epoch 004:   2871 / 3715 loss=7.028, nll_loss=3.976, mask_ins=1.159, word_ins_ml=5.473, word_reposition=0.396, ppl=130.52, wps=9627.4, ups=0.66, wpb=14586.6, bsz=1024, num_updates=14000, lr=0.000298807, gnorm=1.128, clip=0, loss_scale=6691, train_wall=150, wall=21425
2022-08-02 15:42:52 | INFO | train_inner | epoch 004:   2971 / 3715 loss=7.027, nll_loss=3.978, mask_ins=1.162, word_ins_ml=5.475, word_reposition=0.39, ppl=130.41, wps=9724.2, ups=0.67, wpb=14605.6, bsz=1024, num_updates=14100, lr=0.000297746, gnorm=1.096, clip=0, loss_scale=4096, train_wall=148, wall=21576
2022-08-02 15:45:21 | INFO | train_inner | epoch 004:   3071 / 3715 loss=7.006, nll_loss=3.962, mask_ins=1.155, word_ins_ml=5.461, word_reposition=0.39, ppl=128.5, wps=9883, ups=0.67, wpb=14647.8, bsz=1024, num_updates=14200, lr=0.000296695, gnorm=1.12, clip=0, loss_scale=4096, train_wall=146, wall=21724
2022-08-02 15:47:49 | INFO | train_inner | epoch 004:   3171 / 3715 loss=7.024, nll_loss=3.971, mask_ins=1.158, word_ins_ml=5.469, word_reposition=0.397, ppl=130.17, wps=9866.4, ups=0.67, wpb=14627.9, bsz=1024, num_updates=14300, lr=0.000295656, gnorm=1.103, clip=0, loss_scale=4096, train_wall=146, wall=21872
2022-08-02 15:50:19 | INFO | train_inner | epoch 004:   3271 / 3715 loss=6.993, nll_loss=3.949, mask_ins=1.151, word_ins_ml=5.449, word_reposition=0.393, ppl=127.41, wps=9684.6, ups=0.67, wpb=14510, bsz=1024, num_updates=14400, lr=0.000294628, gnorm=1.101, clip=0, loss_scale=4096, train_wall=148, wall=22022
2022-08-02 15:52:49 | INFO | train_inner | epoch 004:   3371 / 3715 loss=7.027, nll_loss=3.982, mask_ins=1.157, word_ins_ml=5.478, word_reposition=0.392, ppl=130.42, wps=9710.7, ups=0.67, wpb=14579.7, bsz=1024, num_updates=14500, lr=0.00029361, gnorm=1.097, clip=0, loss_scale=5120, train_wall=148, wall=22172
2022-08-02 15:55:19 | INFO | train_inner | epoch 004:   3471 / 3715 loss=7.003, nll_loss=3.955, mask_ins=1.153, word_ins_ml=5.454, word_reposition=0.396, ppl=128.25, wps=9808.6, ups=0.67, wpb=14736.8, bsz=1024, num_updates=14600, lr=0.000292603, gnorm=1.082, clip=0, loss_scale=8192, train_wall=148, wall=22322
2022-08-02 15:57:49 | INFO | train_inner | epoch 004:   3571 / 3715 loss=7.004, nll_loss=3.954, mask_ins=1.153, word_ins_ml=5.453, word_reposition=0.398, ppl=128.4, wps=9833, ups=0.67, wpb=14741.1, bsz=1024, num_updates=14700, lr=0.000291606, gnorm=1.069, clip=0, loss_scale=8192, train_wall=148, wall=22472
2022-08-02 16:00:19 | INFO | train_inner | epoch 004:   3671 / 3715 loss=7, nll_loss=3.96, mask_ins=1.152, word_ins_ml=5.458, word_reposition=0.39, ppl=128, wps=9751.5, ups=0.67, wpb=14636.8, bsz=1024, num_updates=14800, lr=0.000290619, gnorm=1.104, clip=0, loss_scale=8192, train_wall=148, wall=22622
2022-08-02 16:01:24 | INFO | train | epoch 004 | loss 7.064 | nll_loss 4.003 | mask_ins 1.165 | word_ins_ml 5.499 | word_reposition 0.401 | ppl 133.78 | wps 9540.5 | ups 0.65 | wpb 14662.4 | bsz 1023.7 | num_updates 14844 | lr 0.000290188 | gnorm 1.102 | clip 0 | loss_scale 8557 | train_wall 5516 | wall 22687
2022-08-02 16:03:14 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.844 | nll_loss 3.7 | mask_ins 1.126 | word_ins_ml 5.313 | word_reposition 0.405 | ppl 114.89 | wps 24909.1 | wpb 1849.4 | bsz 127.9 | num_updates 14844 | best_loss 6.844
2022-08-02 16:03:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint4.pt (epoch 4 @ 14844 updates, score 6.844) (writing took 6.247431382536888 seconds)
2022-08-02 16:04:44 | INFO | train_inner | epoch 005:     56 / 3715 loss=6.976, nll_loss=3.928, mask_ins=1.151, word_ins_ml=5.43, word_reposition=0.395, ppl=125.93, wps=5501.4, ups=0.38, wpb=14577.8, bsz=1014.7, num_updates=14900, lr=0.000289642, gnorm=1.107, clip=0, loss_scale=8192, train_wall=147, wall=22887
2022-08-02 16:07:14 | INFO | train_inner | epoch 005:    156 / 3715 loss=6.958, nll_loss=3.913, mask_ins=1.15, word_ins_ml=5.417, word_reposition=0.391, ppl=124.33, wps=9748.5, ups=0.67, wpb=14606.2, bsz=1024, num_updates=15000, lr=0.000288675, gnorm=1.061, clip=0, loss_scale=9257, train_wall=148, wall=23037
2022-08-02 16:09:43 | INFO | train_inner | epoch 005:    256 / 3715 loss=6.932, nll_loss=3.887, mask_ins=1.14, word_ins_ml=5.393, word_reposition=0.399, ppl=122.14, wps=9806.3, ups=0.67, wpb=14656.1, bsz=1024, num_updates=15100, lr=0.000287718, gnorm=1.095, clip=0, loss_scale=16384, train_wall=148, wall=23187
2022-08-02 16:11:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 16:12:15 | INFO | train_inner | epoch 005:    357 / 3715 loss=6.959, nll_loss=3.922, mask_ins=1.147, word_ins_ml=5.424, word_reposition=0.388, ppl=124.45, wps=9714.7, ups=0.66, wpb=14681.8, bsz=1024, num_updates=15200, lr=0.00028677, gnorm=1.07, clip=0, loss_scale=15167, train_wall=149, wall=23338
2022-08-02 16:14:45 | INFO | train_inner | epoch 005:    457 / 3715 loss=6.949, nll_loss=3.909, mask_ins=1.146, word_ins_ml=5.413, word_reposition=0.39, ppl=123.58, wps=9756.8, ups=0.66, wpb=14713.8, bsz=1024, num_updates=15300, lr=0.000285831, gnorm=1.069, clip=0, loss_scale=8192, train_wall=149, wall=23489
2022-08-02 16:17:16 | INFO | train_inner | epoch 005:    557 / 3715 loss=6.943, nll_loss=3.894, mask_ins=1.148, word_ins_ml=5.399, word_reposition=0.396, ppl=123.08, wps=9694.5, ups=0.66, wpb=14641.8, bsz=1023.8, num_updates=15400, lr=0.000284901, gnorm=1.091, clip=0, loss_scale=8192, train_wall=149, wall=23640
2022-08-02 16:19:47 | INFO | train_inner | epoch 005:    657 / 3715 loss=6.938, nll_loss=3.904, mask_ins=1.142, word_ins_ml=5.408, word_reposition=0.387, ppl=122.59, wps=9783.9, ups=0.66, wpb=14765.3, bsz=1024, num_updates=15500, lr=0.000283981, gnorm=1.087, clip=0, loss_scale=8192, train_wall=149, wall=23790
2022-08-02 16:22:18 | INFO | train_inner | epoch 005:    757 / 3715 loss=6.951, nll_loss=3.909, mask_ins=1.147, word_ins_ml=5.413, word_reposition=0.391, ppl=123.74, wps=9724.7, ups=0.66, wpb=14667.8, bsz=1024, num_updates=15600, lr=0.000283069, gnorm=1.08, clip=0, loss_scale=8192, train_wall=149, wall=23941
2022-08-02 16:24:49 | INFO | train_inner | epoch 005:    857 / 3715 loss=6.916, nll_loss=3.883, mask_ins=1.138, word_ins_ml=5.389, word_reposition=0.389, ppl=120.73, wps=9883.9, ups=0.66, wpb=14871.3, bsz=1024, num_updates=15700, lr=0.000282166, gnorm=1.085, clip=0, loss_scale=8438, train_wall=149, wall=24092
2022-08-02 16:26:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 16:27:21 | INFO | train_inner | epoch 005:    958 / 3715 loss=6.939, nll_loss=3.897, mask_ins=1.151, word_ins_ml=5.402, word_reposition=0.385, ppl=122.69, wps=9684.5, ups=0.66, wpb=14768.4, bsz=1024, num_updates=15800, lr=0.000281272, gnorm=1.08, clip=0, loss_scale=12166, train_wall=151, wall=24244
2022-08-02 16:29:53 | INFO | train_inner | epoch 005:   1058 / 3715 loss=6.93, nll_loss=3.901, mask_ins=1.142, word_ins_ml=5.405, word_reposition=0.383, ppl=121.93, wps=9587.3, ups=0.66, wpb=14557.7, bsz=1024, num_updates=15900, lr=0.000280386, gnorm=1.143, clip=0, loss_scale=8192, train_wall=150, wall=24396
2022-08-02 16:32:24 | INFO | train_inner | epoch 005:   1158 / 3715 loss=6.92, nll_loss=3.88, mask_ins=1.143, word_ins_ml=5.387, word_reposition=0.39, ppl=121.09, wps=9699.8, ups=0.66, wpb=14671.9, bsz=1024, num_updates=16000, lr=0.000279508, gnorm=1.053, clip=0, loss_scale=8192, train_wall=149, wall=24547
2022-08-02 16:34:56 | INFO | train_inner | epoch 005:   1258 / 3715 loss=6.936, nll_loss=3.896, mask_ins=1.145, word_ins_ml=5.4, word_reposition=0.391, ppl=122.46, wps=9700.3, ups=0.66, wpb=14683.5, bsz=1024, num_updates=16100, lr=0.000278639, gnorm=1.053, clip=0, loss_scale=8192, train_wall=149, wall=24699
2022-08-02 16:37:27 | INFO | train_inner | epoch 005:   1358 / 3715 loss=6.93, nll_loss=3.888, mask_ins=1.145, word_ins_ml=5.393, word_reposition=0.392, ppl=121.94, wps=9649.3, ups=0.66, wpb=14637.6, bsz=1024, num_updates=16200, lr=0.000277778, gnorm=1.073, clip=0, loss_scale=8192, train_wall=150, wall=24850
2022-08-02 16:39:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 16:40:00 | INFO | train_inner | epoch 005:   1459 / 3715 loss=6.91, nll_loss=3.882, mask_ins=1.136, word_ins_ml=5.389, word_reposition=0.386, ppl=120.27, wps=9585.8, ups=0.65, wpb=14654.8, bsz=1024, num_updates=16300, lr=0.000276924, gnorm=1.079, clip=0, loss_scale=11193, train_wall=151, wall=25003
2022-08-02 16:42:32 | INFO | train_inner | epoch 005:   1559 / 3715 loss=6.928, nll_loss=3.892, mask_ins=1.141, word_ins_ml=5.397, word_reposition=0.39, ppl=121.75, wps=9653.8, ups=0.66, wpb=14640.2, bsz=1024, num_updates=16400, lr=0.000276079, gnorm=1.102, clip=0, loss_scale=8192, train_wall=150, wall=25155
2022-08-02 16:45:04 | INFO | train_inner | epoch 005:   1659 / 3715 loss=6.898, nll_loss=3.874, mask_ins=1.133, word_ins_ml=5.381, word_reposition=0.384, ppl=119.23, wps=9679.1, ups=0.66, wpb=14708.3, bsz=1024, num_updates=16500, lr=0.000275241, gnorm=1.087, clip=0, loss_scale=8192, train_wall=150, wall=25307
2022-08-02 16:47:35 | INFO | train_inner | epoch 005:   1759 / 3715 loss=6.945, nll_loss=3.908, mask_ins=1.144, word_ins_ml=5.41, word_reposition=0.391, ppl=123.2, wps=9637, ups=0.66, wpb=14570, bsz=1024, num_updates=16600, lr=0.000274411, gnorm=1.082, clip=0, loss_scale=8192, train_wall=149, wall=25458
2022-08-02 16:50:05 | INFO | train_inner | epoch 005:   1859 / 3715 loss=6.904, nll_loss=3.872, mask_ins=1.14, word_ins_ml=5.379, word_reposition=0.385, ppl=119.75, wps=9728, ups=0.67, wpb=14621.5, bsz=1024, num_updates=16700, lr=0.000273588, gnorm=1.071, clip=0, loss_scale=8192, train_wall=148, wall=25608
2022-08-02 16:52:36 | INFO | train_inner | epoch 005:   1959 / 3715 loss=6.917, nll_loss=3.877, mask_ins=1.141, word_ins_ml=5.383, word_reposition=0.393, ppl=120.86, wps=9730, ups=0.66, wpb=14700.3, bsz=1024, num_updates=16800, lr=0.000272772, gnorm=1.085, clip=0, loss_scale=8192, train_wall=149, wall=25759
2022-08-02 16:54:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 16:55:06 | INFO | train_inner | epoch 005:   2060 / 3715 loss=6.888, nll_loss=3.865, mask_ins=1.137, word_ins_ml=5.372, word_reposition=0.378, ppl=118.41, wps=9804.9, ups=0.67, wpb=14698.3, bsz=1024, num_updates=16900, lr=0.000271964, gnorm=1.047, clip=0, loss_scale=14437, train_wall=148, wall=25909
2022-08-02 16:57:35 | INFO | train_inner | epoch 005:   2160 / 3715 loss=6.907, nll_loss=3.877, mask_ins=1.141, word_ins_ml=5.383, word_reposition=0.383, ppl=120.05, wps=9845.5, ups=0.67, wpb=14678.3, bsz=1024, num_updates=17000, lr=0.000271163, gnorm=1.128, clip=0, loss_scale=8192, train_wall=147, wall=26058
2022-08-02 17:00:03 | INFO | train_inner | epoch 005:   2260 / 3715 loss=6.881, nll_loss=3.846, mask_ins=1.14, word_ins_ml=5.356, word_reposition=0.386, ppl=117.84, wps=9845.3, ups=0.68, wpb=14557.7, bsz=1024, num_updates=17100, lr=0.000270369, gnorm=1.063, clip=0, loss_scale=8192, train_wall=146, wall=26206
2022-08-02 17:02:32 | INFO | train_inner | epoch 005:   2360 / 3715 loss=6.908, nll_loss=3.876, mask_ins=1.136, word_ins_ml=5.381, word_reposition=0.391, ppl=120.06, wps=9956.3, ups=0.67, wpb=14798.1, bsz=1024, num_updates=17200, lr=0.000269582, gnorm=1.084, clip=0, loss_scale=8192, train_wall=147, wall=26355
2022-08-02 17:05:00 | INFO | train_inner | epoch 005:   2460 / 3715 loss=6.875, nll_loss=3.844, mask_ins=1.134, word_ins_ml=5.353, word_reposition=0.387, ppl=117.35, wps=9905.8, ups=0.67, wpb=14695, bsz=1024, num_updates=17300, lr=0.000268802, gnorm=1.108, clip=0, loss_scale=8192, train_wall=146, wall=26503
2022-08-02 17:07:29 | INFO | train_inner | epoch 005:   2560 / 3715 loss=6.898, nll_loss=3.869, mask_ins=1.139, word_ins_ml=5.375, word_reposition=0.385, ppl=119.28, wps=9853.4, ups=0.67, wpb=14684, bsz=1024, num_updates=17400, lr=0.000268028, gnorm=1.106, clip=0, loss_scale=8520, train_wall=147, wall=26652
2022-08-02 17:08:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 17:10:01 | INFO | train_inner | epoch 005:   2661 / 3715 loss=6.883, nll_loss=3.855, mask_ins=1.131, word_ins_ml=5.362, word_reposition=0.39, ppl=118.06, wps=9621.8, ups=0.66, wpb=14653.8, bsz=1024, num_updates=17500, lr=0.000267261, gnorm=1.094, clip=0, loss_scale=10787, train_wall=150, wall=26805
2022-08-02 17:12:33 | INFO | train_inner | epoch 005:   2761 / 3715 loss=6.906, nll_loss=3.869, mask_ins=1.142, word_ins_ml=5.376, word_reposition=0.388, ppl=119.94, wps=9658.5, ups=0.66, wpb=14643.4, bsz=1024, num_updates=17600, lr=0.000266501, gnorm=1.098, clip=0, loss_scale=8192, train_wall=150, wall=26956
2022-08-02 17:15:05 | INFO | train_inner | epoch 005:   2861 / 3715 loss=6.879, nll_loss=3.852, mask_ins=1.135, word_ins_ml=5.36, word_reposition=0.384, ppl=117.67, wps=9700, ups=0.66, wpb=14699.5, bsz=1024, num_updates=17700, lr=0.000265747, gnorm=1.052, clip=0, loss_scale=8192, train_wall=150, wall=27108
2022-08-02 17:17:36 | INFO | train_inner | epoch 005:   2961 / 3715 loss=6.874, nll_loss=3.851, mask_ins=1.13, word_ins_ml=5.359, word_reposition=0.386, ppl=117.3, wps=9713.1, ups=0.66, wpb=14656.3, bsz=1024, num_updates=17800, lr=0.000264999, gnorm=1.058, clip=0, loss_scale=8192, train_wall=149, wall=27259
2022-08-02 17:20:05 | INFO | train_inner | epoch 005:   3061 / 3715 loss=6.888, nll_loss=3.861, mask_ins=1.135, word_ins_ml=5.368, word_reposition=0.385, ppl=118.47, wps=9858, ups=0.67, wpb=14702.9, bsz=1024, num_updates=17900, lr=0.000264258, gnorm=1.077, clip=0, loss_scale=8192, train_wall=147, wall=27408
2022-08-02 17:21:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 17:22:37 | INFO | train_inner | epoch 005:   3162 / 3715 loss=6.86, nll_loss=3.834, mask_ins=1.132, word_ins_ml=5.344, word_reposition=0.383, ppl=116.15, wps=9624.7, ups=0.66, wpb=14617, bsz=1024, num_updates=18000, lr=0.000263523, gnorm=1.09, clip=0, loss_scale=10139, train_wall=150, wall=27560
2022-08-02 17:25:05 | INFO | train_inner | epoch 005:   3262 / 3715 loss=6.867, nll_loss=3.845, mask_ins=1.127, word_ins_ml=5.354, word_reposition=0.385, ppl=116.72, wps=9858.7, ups=0.68, wpb=14585.9, bsz=1024, num_updates=18100, lr=0.000262794, gnorm=1.096, clip=0, loss_scale=8192, train_wall=146, wall=27708
2022-08-02 17:27:33 | INFO | train_inner | epoch 005:   3362 / 3715 loss=6.892, nll_loss=3.869, mask_ins=1.135, word_ins_ml=5.375, word_reposition=0.383, ppl=118.78, wps=9888.3, ups=0.67, wpb=14663.2, bsz=1024, num_updates=18200, lr=0.000262071, gnorm=1.059, clip=0, loss_scale=8192, train_wall=146, wall=27856
2022-08-02 17:30:01 | INFO | train_inner | epoch 005:   3462 / 3715 loss=6.874, nll_loss=3.85, mask_ins=1.133, word_ins_ml=5.357, word_reposition=0.385, ppl=117.33, wps=9914.7, ups=0.68, wpb=14680.4, bsz=1024, num_updates=18300, lr=0.000261354, gnorm=1.081, clip=0, loss_scale=8192, train_wall=146, wall=28004
2022-08-02 17:32:29 | INFO | train_inner | epoch 005:   3562 / 3715 loss=6.87, nll_loss=3.85, mask_ins=1.132, word_ins_ml=5.357, word_reposition=0.381, ppl=116.99, wps=9860.9, ups=0.68, wpb=14608.4, bsz=1024, num_updates=18400, lr=0.000260643, gnorm=1.084, clip=0, loss_scale=8192, train_wall=146, wall=28152
2022-08-02 17:34:57 | INFO | train_inner | epoch 005:   3662 / 3715 loss=6.853, nll_loss=3.83, mask_ins=1.128, word_ins_ml=5.341, word_reposition=0.385, ppl=115.64, wps=9868.2, ups=0.68, wpb=14619.4, bsz=1024, num_updates=18500, lr=0.000259938, gnorm=1.086, clip=0, loss_scale=9994, train_wall=146, wall=28300
2022-08-02 17:36:14 | INFO | train | epoch 005 | loss 6.909 | nll_loss 3.876 | mask_ins 1.139 | word_ins_ml 5.382 | word_reposition 0.387 | ppl 120.18 | wps 9557 | ups 0.65 | wpb 14662.2 | bsz 1023.7 | num_updates 18553 | lr 0.000259566 | gnorm 1.083 | clip 0 | loss_scale 9295 | train_wall 5504 | wall 28377
2022-08-02 17:38:04 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.775 | nll_loss 3.633 | mask_ins 1.118 | word_ins_ml 5.247 | word_reposition 0.41 | ppl 109.5 | wps 24903.2 | wpb 1849.4 | bsz 127.9 | num_updates 18553 | best_loss 6.775
2022-08-02 17:38:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint5.pt (epoch 5 @ 18553 updates, score 6.775) (writing took 6.11602334305644 seconds)
2022-08-02 17:39:20 | INFO | train_inner | epoch 006:     47 / 3715 loss=6.851, nll_loss=3.833, mask_ins=1.126, word_ins_ml=5.343, word_reposition=0.382, ppl=115.45, wps=5490.8, ups=0.38, wpb=14437.3, bsz=1014.7, num_updates=18600, lr=0.000259238, gnorm=1.094, clip=0, loss_scale=16384, train_wall=145, wall=28563
2022-08-02 17:41:48 | INFO | train_inner | epoch 006:    147 / 3715 loss=6.827, nll_loss=3.798, mask_ins=1.126, word_ins_ml=5.312, word_reposition=0.389, ppl=113.57, wps=9961, ups=0.68, wpb=14703.8, bsz=1024, num_updates=18700, lr=0.000258544, gnorm=1.067, clip=0, loss_scale=16384, train_wall=146, wall=28711
2022-08-02 17:44:15 | INFO | train_inner | epoch 006:    247 / 3715 loss=6.804, nll_loss=3.78, mask_ins=1.126, word_ins_ml=5.296, word_reposition=0.382, ppl=111.72, wps=9895.6, ups=0.68, wpb=14594.5, bsz=1023.8, num_updates=18800, lr=0.000257855, gnorm=1.077, clip=0, loss_scale=16384, train_wall=146, wall=28858
2022-08-02 17:45:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 17:46:45 | INFO | train_inner | epoch 006:    348 / 3715 loss=6.827, nll_loss=3.797, mask_ins=1.128, word_ins_ml=5.311, word_reposition=0.388, ppl=113.5, wps=9816.8, ups=0.67, wpb=14681.7, bsz=1024, num_updates=18900, lr=0.000257172, gnorm=1.099, clip=0, loss_scale=13707, train_wall=148, wall=29008
2022-08-02 17:49:13 | INFO | train_inner | epoch 006:    448 / 3715 loss=6.812, nll_loss=3.801, mask_ins=1.123, word_ins_ml=5.315, word_reposition=0.374, ppl=112.33, wps=9840.6, ups=0.68, wpb=14572.7, bsz=1024, num_updates=19000, lr=0.000256495, gnorm=1.077, clip=0, loss_scale=8192, train_wall=146, wall=29156
2022-08-02 17:51:43 | INFO | train_inner | epoch 006:    548 / 3715 loss=6.849, nll_loss=3.831, mask_ins=1.123, word_ins_ml=5.341, word_reposition=0.385, ppl=115.25, wps=9697.2, ups=0.66, wpb=14590.3, bsz=1024, num_updates=19100, lr=0.000255822, gnorm=1.09, clip=0, loss_scale=8192, train_wall=149, wall=29306
2022-08-02 17:54:13 | INFO | train_inner | epoch 006:    648 / 3715 loss=6.82, nll_loss=3.8, mask_ins=1.125, word_ins_ml=5.313, word_reposition=0.383, ppl=112.99, wps=9814.8, ups=0.67, wpb=14727.6, bsz=1024, num_updates=19200, lr=0.000255155, gnorm=1.06, clip=0, loss_scale=8192, train_wall=148, wall=29457
2022-08-02 17:56:44 | INFO | train_inner | epoch 006:    748 / 3715 loss=6.803, nll_loss=3.785, mask_ins=1.119, word_ins_ml=5.3, word_reposition=0.384, ppl=111.65, wps=9852, ups=0.66, wpb=14823, bsz=1024, num_updates=19300, lr=0.000254493, gnorm=1.083, clip=0, loss_scale=8192, train_wall=149, wall=29607
2022-08-02 17:58:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 17:59:16 | INFO | train_inner | epoch 006:    849 / 3715 loss=6.838, nll_loss=3.8, mask_ins=1.132, word_ins_ml=5.313, word_reposition=0.394, ppl=114.43, wps=9687.5, ups=0.66, wpb=14715.3, bsz=1024, num_updates=19400, lr=0.000253837, gnorm=1.062, clip=0, loss_scale=8598, train_wall=150, wall=29759
2022-08-02 18:01:45 | INFO | train_inner | epoch 006:    949 / 3715 loss=6.816, nll_loss=3.8, mask_ins=1.12, word_ins_ml=5.313, word_reposition=0.383, ppl=112.71, wps=9806.5, ups=0.67, wpb=14641, bsz=1024, num_updates=19500, lr=0.000253185, gnorm=1.087, clip=0, loss_scale=8192, train_wall=147, wall=29908
2022-08-02 18:04:15 | INFO | train_inner | epoch 006:   1049 / 3715 loss=6.823, nll_loss=3.803, mask_ins=1.125, word_ins_ml=5.315, word_reposition=0.382, ppl=113.21, wps=9874.2, ups=0.67, wpb=14764.4, bsz=1024, num_updates=19600, lr=0.000252538, gnorm=1.089, clip=0, loss_scale=8192, train_wall=148, wall=30058
2022-08-02 18:06:44 | INFO | train_inner | epoch 006:   1149 / 3715 loss=6.807, nll_loss=3.783, mask_ins=1.123, word_ins_ml=5.298, word_reposition=0.386, ppl=111.95, wps=9862.9, ups=0.67, wpb=14775, bsz=1024, num_updates=19700, lr=0.000251896, gnorm=1.074, clip=0, loss_scale=8192, train_wall=148, wall=30208
2022-08-02 18:09:14 | INFO | train_inner | epoch 006:   1249 / 3715 loss=6.794, nll_loss=3.775, mask_ins=1.117, word_ins_ml=5.291, word_reposition=0.386, ppl=110.96, wps=9827.5, ups=0.67, wpb=14688.4, bsz=1024, num_updates=19800, lr=0.000251259, gnorm=1.074, clip=0, loss_scale=8192, train_wall=148, wall=30357
2022-08-02 18:11:42 | INFO | train_inner | epoch 006:   1349 / 3715 loss=6.81, nll_loss=3.796, mask_ins=1.121, word_ins_ml=5.309, word_reposition=0.38, ppl=112.19, wps=9882.5, ups=0.68, wpb=14608.3, bsz=1024, num_updates=19900, lr=0.000250627, gnorm=1.075, clip=0, loss_scale=8602, train_wall=146, wall=30505
2022-08-02 18:13:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 18:14:14 | INFO | train_inner | epoch 006:   1450 / 3715 loss=6.818, nll_loss=3.801, mask_ins=1.126, word_ins_ml=5.314, word_reposition=0.378, ppl=112.83, wps=9560.4, ups=0.65, wpb=14600.2, bsz=1024, num_updates=20000, lr=0.00025, gnorm=1.087, clip=0, loss_scale=12410, train_wall=151, wall=30658
2022-08-02 18:16:45 | INFO | train_inner | epoch 006:   1550 / 3715 loss=6.803, nll_loss=3.781, mask_ins=1.121, word_ins_ml=5.296, word_reposition=0.387, ppl=111.67, wps=9793.3, ups=0.66, wpb=14779.7, bsz=1024, num_updates=20100, lr=0.000249377, gnorm=1.098, clip=0, loss_scale=8192, train_wall=149, wall=30808
2022-08-02 18:19:16 | INFO | train_inner | epoch 006:   1650 / 3715 loss=6.8, nll_loss=3.786, mask_ins=1.122, word_ins_ml=5.3, word_reposition=0.378, ppl=111.42, wps=9721.3, ups=0.66, wpb=14659.6, bsz=1024, num_updates=20200, lr=0.000248759, gnorm=1.098, clip=0, loss_scale=8192, train_wall=149, wall=30959
2022-08-02 18:21:45 | INFO | train_inner | epoch 006:   1750 / 3715 loss=6.804, nll_loss=3.787, mask_ins=1.118, word_ins_ml=5.301, word_reposition=0.385, ppl=111.76, wps=9892.2, ups=0.67, wpb=14722, bsz=1024, num_updates=20300, lr=0.000248146, gnorm=1.072, clip=0, loss_scale=8192, train_wall=147, wall=31108
2022-08-02 18:24:14 | INFO | train_inner | epoch 006:   1850 / 3715 loss=6.792, nll_loss=3.792, mask_ins=1.116, word_ins_ml=5.305, word_reposition=0.371, ppl=110.82, wps=9775.9, ups=0.67, wpb=14555, bsz=1024, num_updates=20400, lr=0.000247537, gnorm=1.062, clip=0, loss_scale=8192, train_wall=147, wall=31257
2022-08-02 18:26:43 | INFO | train_inner | epoch 006:   1950 / 3715 loss=6.815, nll_loss=3.798, mask_ins=1.12, word_ins_ml=5.311, word_reposition=0.384, ppl=112.6, wps=9861.8, ups=0.67, wpb=14673.3, bsz=1024, num_updates=20500, lr=0.000246932, gnorm=1.08, clip=0, loss_scale=11223, train_wall=147, wall=31406
2022-08-02 18:29:11 | INFO | train_inner | epoch 006:   2050 / 3715 loss=6.805, nll_loss=3.803, mask_ins=1.123, word_ins_ml=5.315, word_reposition=0.366, ppl=111.81, wps=9877.4, ups=0.67, wpb=14684.4, bsz=1024, num_updates=20600, lr=0.000246332, gnorm=1.091, clip=0, loss_scale=16384, train_wall=147, wall=31554
2022-08-02 18:29:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 18:31:42 | INFO | train_inner | epoch 006:   2151 / 3715 loss=6.787, nll_loss=3.78, mask_ins=1.12, word_ins_ml=5.295, word_reposition=0.372, ppl=110.42, wps=9701.9, ups=0.67, wpb=14586.8, bsz=1024, num_updates=20700, lr=0.000245737, gnorm=1.069, clip=0, loss_scale=9733, train_wall=149, wall=31705
2022-08-02 18:34:10 | INFO | train_inner | epoch 006:   2251 / 3715 loss=6.806, nll_loss=3.79, mask_ins=1.119, word_ins_ml=5.303, word_reposition=0.383, ppl=111.89, wps=9937, ups=0.67, wpb=14742.1, bsz=1024, num_updates=20800, lr=0.000245145, gnorm=1.063, clip=0, loss_scale=8192, train_wall=147, wall=31853
2022-08-02 18:36:40 | INFO | train_inner | epoch 006:   2351 / 3715 loss=6.798, nll_loss=3.787, mask_ins=1.118, word_ins_ml=5.3, word_reposition=0.38, ppl=111.26, wps=9721.7, ups=0.67, wpb=14606.1, bsz=1024, num_updates=20900, lr=0.000244558, gnorm=1.097, clip=0, loss_scale=8192, train_wall=148, wall=32003
2022-08-02 18:39:08 | INFO | train_inner | epoch 006:   2451 / 3715 loss=6.79, nll_loss=3.774, mask_ins=1.117, word_ins_ml=5.289, word_reposition=0.384, ppl=110.69, wps=9957.7, ups=0.68, wpb=14727.2, bsz=1024, num_updates=21000, lr=0.000243975, gnorm=1.091, clip=0, loss_scale=8192, train_wall=146, wall=32151
2022-08-02 18:41:36 | INFO | train_inner | epoch 006:   2551 / 3715 loss=6.772, nll_loss=3.769, mask_ins=1.113, word_ins_ml=5.284, word_reposition=0.375, ppl=109.25, wps=9905.2, ups=0.68, wpb=14656.1, bsz=1024, num_updates=21100, lr=0.000243396, gnorm=1.074, clip=0, loss_scale=8192, train_wall=146, wall=32299
2022-08-02 18:44:05 | INFO | train_inner | epoch 006:   2651 / 3715 loss=6.773, nll_loss=3.765, mask_ins=1.112, word_ins_ml=5.281, word_reposition=0.38, ppl=109.38, wps=9824.7, ups=0.67, wpb=14647.2, bsz=1024, num_updates=21200, lr=0.000242821, gnorm=1.062, clip=0, loss_scale=13926, train_wall=147, wall=32448
2022-08-02 18:44:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 18:46:36 | INFO | train_inner | epoch 006:   2752 / 3715 loss=6.769, nll_loss=3.761, mask_ins=1.116, word_ins_ml=5.278, word_reposition=0.375, ppl=109.06, wps=9670.5, ups=0.66, wpb=14566, bsz=1024, num_updates=21300, lr=0.000242251, gnorm=1.082, clip=0, loss_scale=10787, train_wall=149, wall=32599
2022-08-02 18:49:04 | INFO | train_inner | epoch 006:   2852 / 3715 loss=6.796, nll_loss=3.784, mask_ins=1.113, word_ins_ml=5.298, word_reposition=0.385, ppl=111.13, wps=9889.9, ups=0.67, wpb=14671.1, bsz=1024, num_updates=21400, lr=0.000241684, gnorm=1.055, clip=0, loss_scale=8192, train_wall=147, wall=32747
2022-08-02 18:51:32 | INFO | train_inner | epoch 006:   2952 / 3715 loss=6.785, nll_loss=3.782, mask_ins=1.116, word_ins_ml=5.296, word_reposition=0.374, ppl=110.29, wps=9913.6, ups=0.68, wpb=14635, bsz=1024, num_updates=21500, lr=0.000241121, gnorm=1.091, clip=0, loss_scale=8192, train_wall=146, wall=32895
2022-08-02 18:54:00 | INFO | train_inner | epoch 006:   3052 / 3715 loss=6.782, nll_loss=3.777, mask_ins=1.112, word_ins_ml=5.291, word_reposition=0.379, ppl=110.08, wps=9944.8, ups=0.68, wpb=14709.3, bsz=1024, num_updates=21600, lr=0.000240563, gnorm=1.118, clip=0, loss_scale=8192, train_wall=146, wall=33043
2022-08-02 18:56:27 | INFO | train_inner | epoch 006:   3152 / 3715 loss=6.77, nll_loss=3.752, mask_ins=1.119, word_ins_ml=5.27, word_reposition=0.381, ppl=109.1, wps=9903.9, ups=0.68, wpb=14634.8, bsz=1024, num_updates=21700, lr=0.000240008, gnorm=1.114, clip=0, loss_scale=8192, train_wall=146, wall=33191
2022-08-02 18:58:56 | INFO | train_inner | epoch 006:   3252 / 3715 loss=6.791, nll_loss=3.787, mask_ins=1.112, word_ins_ml=5.3, word_reposition=0.379, ppl=110.74, wps=9846.8, ups=0.67, wpb=14594.9, bsz=1024, num_updates=21800, lr=0.000239457, gnorm=1.076, clip=0, loss_scale=12861, train_wall=146, wall=33339
2022-08-02 19:00:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 19:01:25 | INFO | train_inner | epoch 006:   3353 / 3715 loss=6.752, nll_loss=3.75, mask_ins=1.111, word_ins_ml=5.267, word_reposition=0.374, ppl=107.78, wps=9779.8, ups=0.67, wpb=14621.1, bsz=1024, num_updates=21900, lr=0.000238909, gnorm=1.066, clip=0, loss_scale=14113, train_wall=148, wall=33488
2022-08-02 19:03:54 | INFO | train_inner | epoch 006:   3453 / 3715 loss=6.789, nll_loss=3.77, mask_ins=1.121, word_ins_ml=5.285, word_reposition=0.382, ppl=110.59, wps=9880.7, ups=0.67, wpb=14675, bsz=1024, num_updates=22000, lr=0.000238366, gnorm=1.07, clip=0, loss_scale=8192, train_wall=147, wall=33637
2022-08-02 19:06:22 | INFO | train_inner | epoch 006:   3553 / 3715 loss=6.756, nll_loss=3.747, mask_ins=1.113, word_ins_ml=5.265, word_reposition=0.378, ppl=108.1, wps=9927.6, ups=0.67, wpb=14724.7, bsz=1024, num_updates=22100, lr=0.000237826, gnorm=1.084, clip=0, loss_scale=8192, train_wall=146, wall=33785
2022-08-02 19:08:52 | INFO | train_inner | epoch 006:   3653 / 3715 loss=6.74, nll_loss=3.749, mask_ins=1.111, word_ins_ml=5.267, word_reposition=0.363, ppl=106.92, wps=9757.7, ups=0.67, wpb=14656.3, bsz=1024, num_updates=22200, lr=0.000237289, gnorm=1.087, clip=0, loss_scale=8192, train_wall=148, wall=33935
2022-08-02 19:10:25 | INFO | train | epoch 006 | loss 6.798 | nll_loss 3.784 | mask_ins 1.119 | word_ins_ml 5.299 | word_reposition 0.38 | ppl 111.26 | wps 9623.5 | ups 0.66 | wpb 14662.5 | bsz 1023.7 | num_updates 22262 | lr 0.000236959 | gnorm 1.081 | clip 0 | loss_scale 9879 | train_wall 5466 | wall 34028
2022-08-02 19:12:15 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.68 | nll_loss 3.565 | mask_ins 1.095 | word_ins_ml 5.185 | word_reposition 0.399 | ppl 102.54 | wps 24962.7 | wpb 1849.4 | bsz 127.9 | num_updates 22262 | best_loss 6.68
2022-08-02 19:12:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint6.pt (epoch 6 @ 22262 updates, score 6.68) (writing took 6.204615311697125 seconds)
2022-08-02 19:13:19 | INFO | train_inner | epoch 007:     38 / 3715 loss=6.772, nll_loss=3.775, mask_ins=1.109, word_ins_ml=5.289, word_reposition=0.374, ppl=109.26, wps=5429.7, ups=0.37, wpb=14489.3, bsz=1014.7, num_updates=22300, lr=0.000236757, gnorm=1.103, clip=0, loss_scale=8192, train_wall=149, wall=34202
2022-08-02 19:15:49 | INFO | train_inner | epoch 007:    138 / 3715 loss=6.717, nll_loss=3.72, mask_ins=1.108, word_ins_ml=5.241, word_reposition=0.368, ppl=105.21, wps=9724.9, ups=0.67, wpb=14617.9, bsz=1024, num_updates=22400, lr=0.000236228, gnorm=1.08, clip=0, loss_scale=9503, train_wall=148, wall=34353
2022-08-02 19:18:17 | INFO | train_inner | epoch 007:    238 / 3715 loss=6.748, nll_loss=3.743, mask_ins=1.111, word_ins_ml=5.261, word_reposition=0.376, ppl=107.49, wps=9845.6, ups=0.68, wpb=14584, bsz=1024, num_updates=22500, lr=0.000235702, gnorm=1.069, clip=0, loss_scale=16384, train_wall=146, wall=34501
2022-08-02 19:18:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 19:20:47 | INFO | train_inner | epoch 007:    339 / 3715 loss=6.721, nll_loss=3.717, mask_ins=1.109, word_ins_ml=5.238, word_reposition=0.374, ppl=105.5, wps=9879.3, ups=0.67, wpb=14756.1, bsz=1024, num_updates=22600, lr=0.00023518, gnorm=1.058, clip=0, loss_scale=9003, train_wall=148, wall=34650
2022-08-02 19:23:15 | INFO | train_inner | epoch 007:    439 / 3715 loss=6.758, nll_loss=3.742, mask_ins=1.115, word_ins_ml=5.26, word_reposition=0.382, ppl=108.21, wps=9905.4, ups=0.68, wpb=14630.3, bsz=1024, num_updates=22700, lr=0.000234662, gnorm=1.083, clip=0, loss_scale=8192, train_wall=146, wall=34798
2022-08-02 19:25:42 | INFO | train_inner | epoch 007:    539 / 3715 loss=6.724, nll_loss=3.722, mask_ins=1.107, word_ins_ml=5.242, word_reposition=0.375, ppl=105.74, wps=9923, ups=0.68, wpb=14650.8, bsz=1024, num_updates=22800, lr=0.000234146, gnorm=1.072, clip=0, loss_scale=8192, train_wall=146, wall=34945
2022-08-02 19:28:10 | INFO | train_inner | epoch 007:    639 / 3715 loss=6.727, nll_loss=3.727, mask_ins=1.105, word_ins_ml=5.247, word_reposition=0.375, ppl=105.9, wps=9945, ups=0.68, wpb=14697.7, bsz=1024, num_updates=22900, lr=0.000233635, gnorm=1.086, clip=0, loss_scale=8192, train_wall=146, wall=35093
2022-08-02 19:30:40 | INFO | train_inner | epoch 007:    739 / 3715 loss=6.715, nll_loss=3.714, mask_ins=1.106, word_ins_ml=5.235, word_reposition=0.374, ppl=105.08, wps=9767.7, ups=0.67, wpb=14624.1, bsz=1024, num_updates=23000, lr=0.000233126, gnorm=1.09, clip=0, loss_scale=8192, train_wall=148, wall=35243
2022-08-02 19:33:11 | INFO | train_inner | epoch 007:    839 / 3715 loss=6.741, nll_loss=3.737, mask_ins=1.108, word_ins_ml=5.256, word_reposition=0.378, ppl=106.95, wps=9688.4, ups=0.66, wpb=14690, bsz=1024, num_updates=23100, lr=0.000232621, gnorm=1.118, clip=0, loss_scale=14664, train_wall=150, wall=35394
2022-08-02 19:34:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 19:35:43 | INFO | train_inner | epoch 007:    940 / 3715 loss=6.72, nll_loss=3.714, mask_ins=1.107, word_ins_ml=5.235, word_reposition=0.378, ppl=105.42, wps=9661.8, ups=0.66, wpb=14648.6, bsz=1024, num_updates=23200, lr=0.000232119, gnorm=1.101, clip=0, loss_scale=11031, train_wall=150, wall=35546
2022-08-02 19:38:11 | INFO | train_inner | epoch 007:   1040 / 3715 loss=6.738, nll_loss=3.726, mask_ins=1.112, word_ins_ml=5.246, word_reposition=0.381, ppl=106.77, wps=9916.7, ups=0.68, wpb=14670, bsz=1024, num_updates=23300, lr=0.000231621, gnorm=1.082, clip=0, loss_scale=8192, train_wall=146, wall=35694
2022-08-02 19:38:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-02 19:40:40 | INFO | train_inner | epoch 007:   1141 / 3715 loss=6.738, nll_loss=3.735, mask_ins=1.108, word_ins_ml=5.253, word_reposition=0.376, ppl=106.71, wps=9924.3, ups=0.67, wpb=14818.6, bsz=1024, num_updates=23400, lr=0.000231125, gnorm=1.066, clip=0, loss_scale=4137, train_wall=147, wall=35843
2022-08-02 19:43:08 | INFO | train_inner | epoch 007:   1241 / 3715 loss=6.73, nll_loss=3.727, mask_ins=1.11, word_ins_ml=5.246, word_reposition=0.374, ppl=106.18, wps=9925.5, ups=0.68, wpb=14677.2, bsz=1024, num_updates=23500, lr=0.000230633, gnorm=1.061, clip=0, loss_scale=4096, train_wall=146, wall=35991
2022-08-02 19:45:36 | INFO | train_inner | epoch 007:   1341 / 3715 loss=6.71, nll_loss=3.716, mask_ins=1.1, word_ins_ml=5.236, word_reposition=0.374, ppl=104.68, wps=9956, ups=0.68, wpb=14706.6, bsz=1024, num_updates=23600, lr=0.000230144, gnorm=1.094, clip=0, loss_scale=4096, train_wall=146, wall=36139
2022-08-02 19:48:03 | INFO | train_inner | epoch 007:   1441 / 3715 loss=6.736, nll_loss=3.729, mask_ins=1.107, word_ins_ml=5.249, word_reposition=0.38, ppl=106.6, wps=10007.8, ups=0.68, wpb=14751.5, bsz=1024, num_updates=23700, lr=0.000229658, gnorm=1.078, clip=0, loss_scale=4096, train_wall=146, wall=36286
2022-08-02 19:50:31 | INFO | train_inner | epoch 007:   1541 / 3715 loss=6.699, nll_loss=3.708, mask_ins=1.102, word_ins_ml=5.229, word_reposition=0.368, ppl=103.93, wps=9911.4, ups=0.68, wpb=14630.8, bsz=1024, num_updates=23800, lr=0.000229175, gnorm=1.077, clip=0, loss_scale=4096, train_wall=146, wall=36434
2022-08-02 19:52:59 | INFO | train_inner | epoch 007:   1641 / 3715 loss=6.71, nll_loss=3.709, mask_ins=1.105, word_ins_ml=5.23, word_reposition=0.376, ppl=104.72, wps=9911.6, ups=0.68, wpb=14641.6, bsz=1024, num_updates=23900, lr=0.000228695, gnorm=1.083, clip=0, loss_scale=7700, train_wall=146, wall=36582
2022-08-02 19:55:26 | INFO | train_inner | epoch 007:   1741 / 3715 loss=6.703, nll_loss=3.715, mask_ins=1.098, word_ins_ml=5.236, word_reposition=0.369, ppl=104.2, wps=9915.7, ups=0.68, wpb=14655.4, bsz=1024, num_updates=24000, lr=0.000228218, gnorm=1.064, clip=0, loss_scale=8192, train_wall=146, wall=36729
2022-08-02 19:57:54 | INFO | train_inner | epoch 007:   1841 / 3715 loss=6.73, nll_loss=3.731, mask_ins=1.106, word_ins_ml=5.25, word_reposition=0.374, ppl=106.15, wps=9867.9, ups=0.68, wpb=14575.6, bsz=1024, num_updates=24100, lr=0.000227744, gnorm=1.096, clip=0, loss_scale=8192, train_wall=146, wall=36877
2022-08-02 20:00:22 | INFO | train_inner | epoch 007:   1941 / 3715 loss=6.74, nll_loss=3.738, mask_ins=1.111, word_ins_ml=5.256, word_reposition=0.373, ppl=106.88, wps=9894, ups=0.68, wpb=14591.1, bsz=1024, num_updates=24200, lr=0.000227273, gnorm=1.067, clip=0, loss_scale=8192, train_wall=146, wall=37025
2022-08-02 20:02:49 | INFO | train_inner | epoch 007:   2041 / 3715 loss=6.715, nll_loss=3.709, mask_ins=1.106, word_ins_ml=5.23, word_reposition=0.379, ppl=105.03, wps=9962.6, ups=0.68, wpb=14717.9, bsz=1024, num_updates=24300, lr=0.000226805, gnorm=1.07, clip=0, loss_scale=8192, train_wall=146, wall=37172
2022-08-02 20:04:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 20:05:18 | INFO | train_inner | epoch 007:   2142 / 3715 loss=6.707, nll_loss=3.711, mask_ins=1.1, word_ins_ml=5.232, word_reposition=0.375, ppl=104.46, wps=9745.1, ups=0.67, wpb=14528.4, bsz=1023.8, num_updates=24400, lr=0.000226339, gnorm=1.067, clip=0, loss_scale=12491, train_wall=147, wall=37321
2022-08-02 20:07:46 | INFO | train_inner | epoch 007:   2242 / 3715 loss=6.711, nll_loss=3.717, mask_ins=1.099, word_ins_ml=5.237, word_reposition=0.375, ppl=104.74, wps=9966.8, ups=0.68, wpb=14711.1, bsz=1024, num_updates=24500, lr=0.000225877, gnorm=1.079, clip=0, loss_scale=8192, train_wall=146, wall=37469
2022-08-02 20:10:13 | INFO | train_inner | epoch 007:   2342 / 3715 loss=6.705, nll_loss=3.708, mask_ins=1.099, word_ins_ml=5.229, word_reposition=0.377, ppl=104.32, wps=9982.4, ups=0.68, wpb=14716.8, bsz=1024, num_updates=24600, lr=0.000225417, gnorm=1.063, clip=0, loss_scale=8192, train_wall=146, wall=37617
2022-08-02 20:12:41 | INFO | train_inner | epoch 007:   2442 / 3715 loss=6.721, nll_loss=3.733, mask_ins=1.1, word_ins_ml=5.251, word_reposition=0.37, ppl=105.52, wps=9945.9, ups=0.68, wpb=14707.4, bsz=1024, num_updates=24700, lr=0.000224961, gnorm=1.07, clip=0, loss_scale=8192, train_wall=146, wall=37764
2022-08-02 20:15:09 | INFO | train_inner | epoch 007:   2542 / 3715 loss=6.692, nll_loss=3.694, mask_ins=1.106, word_ins_ml=5.216, word_reposition=0.371, ppl=103.43, wps=9946.6, ups=0.68, wpb=14686.3, bsz=1024, num_updates=24800, lr=0.000224507, gnorm=1.066, clip=0, loss_scale=8192, train_wall=146, wall=37912
2022-08-02 20:17:37 | INFO | train_inner | epoch 007:   2642 / 3715 loss=6.718, nll_loss=3.731, mask_ins=1.095, word_ins_ml=5.249, word_reposition=0.374, ppl=105.29, wps=9831.6, ups=0.67, wpb=14572.4, bsz=1024, num_updates=24900, lr=0.000224055, gnorm=1.082, clip=0, loss_scale=9175, train_wall=146, wall=38060
2022-08-02 20:18:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 20:20:06 | INFO | train_inner | epoch 007:   2743 / 3715 loss=6.697, nll_loss=3.705, mask_ins=1.101, word_ins_ml=5.226, word_reposition=0.369, ppl=103.72, wps=9845.5, ups=0.67, wpb=14692.1, bsz=1024, num_updates=25000, lr=0.000223607, gnorm=1.108, clip=0, loss_scale=11112, train_wall=147, wall=38210
2022-08-02 20:22:34 | INFO | train_inner | epoch 007:   2843 / 3715 loss=6.676, nll_loss=3.677, mask_ins=1.103, word_ins_ml=5.201, word_reposition=0.372, ppl=102.24, wps=10016.9, ups=0.68, wpb=14775, bsz=1024, num_updates=25100, lr=0.000223161, gnorm=1.087, clip=0, loss_scale=8192, train_wall=146, wall=38357
2022-08-02 20:25:02 | INFO | train_inner | epoch 007:   2943 / 3715 loss=6.743, nll_loss=3.744, mask_ins=1.108, word_ins_ml=5.261, word_reposition=0.375, ppl=107.12, wps=9870, ups=0.68, wpb=14621.6, bsz=1024, num_updates=25200, lr=0.000222718, gnorm=1.087, clip=0, loss_scale=8192, train_wall=146, wall=38505
2022-08-02 20:27:30 | INFO | train_inner | epoch 007:   3043 / 3715 loss=6.693, nll_loss=3.696, mask_ins=1.103, word_ins_ml=5.218, word_reposition=0.372, ppl=103.45, wps=9922.4, ups=0.68, wpb=14675.1, bsz=1024, num_updates=25300, lr=0.000222277, gnorm=1.086, clip=0, loss_scale=8192, train_wall=146, wall=38653
2022-08-02 20:29:58 | INFO | train_inner | epoch 007:   3143 / 3715 loss=6.687, nll_loss=3.698, mask_ins=1.097, word_ins_ml=5.219, word_reposition=0.371, ppl=103.03, wps=9862.1, ups=0.68, wpb=14585.2, bsz=1024, num_updates=25400, lr=0.000221839, gnorm=1.061, clip=0, loss_scale=8192, train_wall=146, wall=38801
2022-08-02 20:31:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 20:32:28 | INFO | train_inner | epoch 007:   3244 / 3715 loss=6.664, nll_loss=3.672, mask_ins=1.097, word_ins_ml=5.196, word_reposition=0.371, ppl=101.42, wps=9796.1, ups=0.67, wpb=14707.2, bsz=1024, num_updates=25500, lr=0.000221404, gnorm=1.066, clip=0, loss_scale=9976, train_wall=148, wall=38951
2022-08-02 20:34:56 | INFO | train_inner | epoch 007:   3344 / 3715 loss=6.665, nll_loss=3.678, mask_ins=1.093, word_ins_ml=5.202, word_reposition=0.37, ppl=101.5, wps=9877.9, ups=0.68, wpb=14625.5, bsz=1024, num_updates=25600, lr=0.000220971, gnorm=1.075, clip=0, loss_scale=8192, train_wall=146, wall=39099
2022-08-02 20:37:24 | INFO | train_inner | epoch 007:   3444 / 3715 loss=6.664, nll_loss=3.677, mask_ins=1.097, word_ins_ml=5.202, word_reposition=0.366, ppl=101.44, wps=9976.5, ups=0.68, wpb=14724.5, bsz=1024, num_updates=25700, lr=0.000220541, gnorm=1.06, clip=0, loss_scale=8192, train_wall=146, wall=39247
2022-08-02 20:39:52 | INFO | train_inner | epoch 007:   3544 / 3715 loss=6.685, nll_loss=3.696, mask_ins=1.095, word_ins_ml=5.218, word_reposition=0.373, ppl=102.88, wps=9890.5, ups=0.68, wpb=14646.3, bsz=1024, num_updates=25800, lr=0.000220113, gnorm=1.091, clip=0, loss_scale=8192, train_wall=146, wall=39395
2022-08-02 20:41:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-02 20:42:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-02 20:42:23 | INFO | train_inner | epoch 007:   3646 / 3715 loss=6.709, nll_loss=3.719, mask_ins=1.101, word_ins_ml=5.238, word_reposition=0.37, ppl=104.63, wps=9719.6, ups=0.66, wpb=14702.3, bsz=1024, num_updates=25900, lr=0.000219687, gnorm=1.073, clip=0, loss_scale=6465, train_wall=149, wall=39546
2022-08-02 20:44:04 | INFO | train | epoch 007 | loss 6.713 | nll_loss 3.715 | mask_ins 1.104 | word_ins_ml 5.236 | word_reposition 0.374 | ppl 104.91 | wps 9673.7 | ups 0.66 | wpb 14661.8 | bsz 1023.7 | num_updates 25969 | lr 0.000219395 | gnorm 1.079 | clip 0 | loss_scale 8266 | train_wall 5434 | wall 39647
2022-08-02 20:45:54 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.603 | nll_loss 3.517 | mask_ins 1.086 | word_ins_ml 5.134 | word_reposition 0.384 | ppl 97.2 | wps 24954.7 | wpb 1849.4 | bsz 127.9 | num_updates 25969 | best_loss 6.603
2022-08-02 20:46:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint7.pt (epoch 7 @ 25969 updates, score 6.603) (writing took 6.020240031182766 seconds)
2022-08-02 20:46:46 | INFO | train_inner | epoch 008:     31 / 3715 loss=6.69, nll_loss=3.701, mask_ins=1.103, word_ins_ml=5.222, word_reposition=0.365, ppl=103.28, wps=5507.9, ups=0.38, wpb=14471.7, bsz=1014.7, num_updates=26000, lr=0.000219265, gnorm=1.096, clip=0, loss_scale=2048, train_wall=145, wall=39809
2022-08-02 20:49:14 | INFO | train_inner | epoch 008:    131 / 3715 loss=6.656, nll_loss=3.665, mask_ins=1.095, word_ins_ml=5.191, word_reposition=0.37, ppl=100.84, wps=9933, ups=0.67, wpb=14740.9, bsz=1024, num_updates=26100, lr=0.000218844, gnorm=1.086, clip=0, loss_scale=2048, train_wall=147, wall=39957
2022-08-02 20:51:42 | INFO | train_inner | epoch 008:    231 / 3715 loss=6.634, nll_loss=3.649, mask_ins=1.089, word_ins_ml=5.176, word_reposition=0.369, ppl=99.32, wps=9934.2, ups=0.67, wpb=14743, bsz=1024, num_updates=26200, lr=0.000218426, gnorm=1.072, clip=0, loss_scale=2048, train_wall=147, wall=40106
2022-08-02 20:54:10 | INFO | train_inner | epoch 008:    331 / 3715 loss=6.645, nll_loss=3.652, mask_ins=1.094, word_ins_ml=5.179, word_reposition=0.372, ppl=100.09, wps=9914.9, ups=0.68, wpb=14648.3, bsz=1024, num_updates=26300, lr=0.00021801, gnorm=1.069, clip=0, loss_scale=2048, train_wall=146, wall=40253
2022-08-02 20:56:38 | INFO | train_inner | epoch 008:    431 / 3715 loss=6.668, nll_loss=3.679, mask_ins=1.094, word_ins_ml=5.203, word_reposition=0.371, ppl=101.65, wps=9943.3, ups=0.68, wpb=14659.7, bsz=1024, num_updates=26400, lr=0.000217597, gnorm=1.107, clip=0, loss_scale=2048, train_wall=146, wall=40401
2022-08-02 20:59:06 | INFO | train_inner | epoch 008:    531 / 3715 loss=6.659, nll_loss=3.666, mask_ins=1.094, word_ins_ml=5.191, word_reposition=0.375, ppl=101.08, wps=9902.5, ups=0.68, wpb=14648.6, bsz=1024, num_updates=26500, lr=0.000217186, gnorm=1.061, clip=0, loss_scale=3932, train_wall=146, wall=40549
2022-08-02 21:01:33 | INFO | train_inner | epoch 008:    631 / 3715 loss=6.657, nll_loss=3.663, mask_ins=1.099, word_ins_ml=5.189, word_reposition=0.368, ppl=100.89, wps=9863.6, ups=0.68, wpb=14554.4, bsz=1024, num_updates=26600, lr=0.000216777, gnorm=1.074, clip=0, loss_scale=4096, train_wall=146, wall=40696
2022-08-02 21:04:01 | INFO | train_inner | epoch 008:    731 / 3715 loss=6.656, nll_loss=3.659, mask_ins=1.097, word_ins_ml=5.186, word_reposition=0.374, ppl=100.83, wps=9905.6, ups=0.68, wpb=14637.9, bsz=1024, num_updates=26700, lr=0.000216371, gnorm=1.078, clip=0, loss_scale=4096, train_wall=146, wall=40844
2022-08-02 21:06:28 | INFO | train_inner | epoch 008:    831 / 3715 loss=6.651, nll_loss=3.659, mask_ins=1.094, word_ins_ml=5.185, word_reposition=0.371, ppl=100.5, wps=9925.3, ups=0.68, wpb=14646.8, bsz=1024, num_updates=26800, lr=0.000215967, gnorm=1.074, clip=0, loss_scale=4096, train_wall=146, wall=40992
2022-08-02 21:08:56 | INFO | train_inner | epoch 008:    931 / 3715 loss=6.669, nll_loss=3.674, mask_ins=1.095, word_ins_ml=5.198, word_reposition=0.376, ppl=101.74, wps=9928.5, ups=0.68, wpb=14645.2, bsz=1024, num_updates=26900, lr=0.000215565, gnorm=1.085, clip=0, loss_scale=4096, train_wall=146, wall=41139
2022-08-02 21:11:24 | INFO | train_inner | epoch 008:   1031 / 3715 loss=6.648, nll_loss=3.662, mask_ins=1.094, word_ins_ml=5.187, word_reposition=0.367, ppl=100.29, wps=9900.9, ups=0.68, wpb=14634.9, bsz=1024, num_updates=27000, lr=0.000215166, gnorm=1.084, clip=0, loss_scale=7373, train_wall=146, wall=41287
2022-08-02 21:13:52 | INFO | train_inner | epoch 008:   1131 / 3715 loss=6.661, nll_loss=3.672, mask_ins=1.093, word_ins_ml=5.197, word_reposition=0.372, ppl=101.23, wps=9877.1, ups=0.68, wpb=14590.3, bsz=1024, num_updates=27100, lr=0.000214768, gnorm=1.072, clip=0, loss_scale=8192, train_wall=146, wall=41435
2022-08-02 21:16:19 | INFO | train_inner | epoch 008:   1231 / 3715 loss=6.658, nll_loss=3.665, mask_ins=1.093, word_ins_ml=5.191, word_reposition=0.374, ppl=100.98, wps=9959.2, ups=0.68, wpb=14693, bsz=1024, num_updates=27200, lr=0.000214373, gnorm=1.07, clip=0, loss_scale=8192, train_wall=146, wall=41582
2022-08-02 21:18:47 | INFO | train_inner | epoch 008:   1331 / 3715 loss=6.66, nll_loss=3.675, mask_ins=1.096, word_ins_ml=5.199, word_reposition=0.365, ppl=101.1, wps=9909.4, ups=0.68, wpb=14621.7, bsz=1024, num_updates=27300, lr=0.00021398, gnorm=1.071, clip=0, loss_scale=8192, train_wall=146, wall=41730
2022-08-02 21:21:14 | INFO | train_inner | epoch 008:   1431 / 3715 loss=6.626, nll_loss=3.64, mask_ins=1.09, word_ins_ml=5.167, word_reposition=0.368, ppl=98.79, wps=9964, ups=0.68, wpb=14695.4, bsz=1024, num_updates=27400, lr=0.000213589, gnorm=1.071, clip=0, loss_scale=8192, train_wall=146, wall=41877
2022-08-02 21:22:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 21:23:43 | INFO | train_inner | epoch 008:   1532 / 3715 loss=6.639, nll_loss=3.649, mask_ins=1.094, word_ins_ml=5.176, word_reposition=0.37, ppl=99.68, wps=9854.4, ups=0.67, wpb=14706.8, bsz=1024, num_updates=27500, lr=0.000213201, gnorm=1.085, clip=0, loss_scale=11112, train_wall=147, wall=42026
2022-08-02 21:26:11 | INFO | train_inner | epoch 008:   1632 / 3715 loss=6.664, nll_loss=3.673, mask_ins=1.094, word_ins_ml=5.196, word_reposition=0.373, ppl=101.39, wps=9981, ups=0.68, wpb=14738, bsz=1024, num_updates=27600, lr=0.000212814, gnorm=1.067, clip=0, loss_scale=8192, train_wall=146, wall=42174
2022-08-02 21:28:39 | INFO | train_inner | epoch 008:   1732 / 3715 loss=6.635, nll_loss=3.649, mask_ins=1.092, word_ins_ml=5.176, word_reposition=0.367, ppl=99.37, wps=9983.2, ups=0.68, wpb=14742, bsz=1024, num_updates=27700, lr=0.00021243, gnorm=1.066, clip=0, loss_scale=8192, train_wall=146, wall=42322
2022-08-02 21:31:06 | INFO | train_inner | epoch 008:   1832 / 3715 loss=6.671, nll_loss=3.681, mask_ins=1.101, word_ins_ml=5.204, word_reposition=0.366, ppl=101.88, wps=9892.1, ups=0.68, wpb=14583.8, bsz=1024, num_updates=27800, lr=0.000212047, gnorm=1.091, clip=0, loss_scale=8192, train_wall=146, wall=42469
2022-08-02 21:33:34 | INFO | train_inner | epoch 008:   1932 / 3715 loss=6.642, nll_loss=3.655, mask_ins=1.094, word_ins_ml=5.181, word_reposition=0.367, ppl=99.88, wps=9930.8, ups=0.68, wpb=14658.4, bsz=1024, num_updates=27900, lr=0.000211667, gnorm=1.079, clip=0, loss_scale=8192, train_wall=146, wall=42617
2022-08-02 21:36:02 | INFO | train_inner | epoch 008:   2032 / 3715 loss=6.652, nll_loss=3.666, mask_ins=1.098, word_ins_ml=5.191, word_reposition=0.364, ppl=100.59, wps=9880.6, ups=0.68, wpb=14610.5, bsz=1024, num_updates=28000, lr=0.000211289, gnorm=1.093, clip=0, loss_scale=9912, train_wall=146, wall=42765
2022-08-02 21:36:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 21:38:31 | INFO | train_inner | epoch 008:   2133 / 3715 loss=6.638, nll_loss=3.657, mask_ins=1.09, word_ins_ml=5.183, word_reposition=0.365, ppl=99.63, wps=9821.3, ups=0.67, wpb=14658.1, bsz=1024, num_updates=28100, lr=0.000210912, gnorm=1.07, clip=0, loss_scale=10463, train_wall=147, wall=42914
2022-08-02 21:40:58 | INFO | train_inner | epoch 008:   2233 / 3715 loss=6.631, nll_loss=3.645, mask_ins=1.092, word_ins_ml=5.172, word_reposition=0.368, ppl=99.14, wps=9897.6, ups=0.68, wpb=14610, bsz=1024, num_updates=28200, lr=0.000210538, gnorm=1.091, clip=0, loss_scale=8192, train_wall=146, wall=43062
2022-08-02 21:43:27 | INFO | train_inner | epoch 008:   2333 / 3715 loss=6.65, nll_loss=3.656, mask_ins=1.097, word_ins_ml=5.182, word_reposition=0.371, ppl=100.42, wps=9924.1, ups=0.68, wpb=14697.6, bsz=1024, num_updates=28300, lr=0.000210166, gnorm=1.074, clip=0, loss_scale=8192, train_wall=146, wall=43210
2022-08-02 21:45:56 | INFO | train_inner | epoch 008:   2433 / 3715 loss=6.615, nll_loss=3.637, mask_ins=1.09, word_ins_ml=5.165, word_reposition=0.36, ppl=98, wps=9798.3, ups=0.67, wpb=14635.1, bsz=1024, num_updates=28400, lr=0.000209795, gnorm=1.104, clip=0, loss_scale=8192, train_wall=148, wall=43359
2022-08-02 21:48:27 | INFO | train_inner | epoch 008:   2533 / 3715 loss=6.609, nll_loss=3.628, mask_ins=1.084, word_ins_ml=5.157, word_reposition=0.368, ppl=97.62, wps=9744.3, ups=0.66, wpb=14681.6, bsz=1024, num_updates=28500, lr=0.000209427, gnorm=1.063, clip=0, loss_scale=8192, train_wall=149, wall=43510
2022-08-02 21:50:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 21:50:56 | INFO | train_inner | epoch 008:   2634 / 3715 loss=6.637, nll_loss=3.654, mask_ins=1.093, word_ins_ml=5.18, word_reposition=0.364, ppl=99.5, wps=9857.6, ups=0.67, wpb=14710.7, bsz=1024, num_updates=28600, lr=0.000209061, gnorm=1.069, clip=0, loss_scale=11842, train_wall=147, wall=43659
2022-08-02 21:52:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-02 21:53:25 | INFO | train_inner | epoch 008:   2735 / 3715 loss=6.64, nll_loss=3.649, mask_ins=1.097, word_ins_ml=5.176, word_reposition=0.367, ppl=99.74, wps=9862.7, ups=0.67, wpb=14701.2, bsz=1024, num_updates=28700, lr=0.000208696, gnorm=1.069, clip=0, loss_scale=6286, train_wall=147, wall=43808
2022-08-02 21:55:53 | INFO | train_inner | epoch 008:   2835 / 3715 loss=6.633, nll_loss=3.64, mask_ins=1.09, word_ins_ml=5.167, word_reposition=0.376, ppl=99.26, wps=10008.8, ups=0.68, wpb=14794.8, bsz=1024, num_updates=28800, lr=0.000208333, gnorm=1.085, clip=0, loss_scale=4096, train_wall=146, wall=43956
2022-08-02 21:58:21 | INFO | train_inner | epoch 008:   2935 / 3715 loss=6.652, nll_loss=3.666, mask_ins=1.095, word_ins_ml=5.19, word_reposition=0.367, ppl=100.56, wps=9892.8, ups=0.68, wpb=14635.1, bsz=1024, num_updates=28900, lr=0.000207973, gnorm=1.099, clip=0, loss_scale=4096, train_wall=146, wall=44104
2022-08-02 22:00:49 | INFO | train_inner | epoch 008:   3035 / 3715 loss=6.649, nll_loss=3.657, mask_ins=1.096, word_ins_ml=5.182, word_reposition=0.371, ppl=100.36, wps=9993.2, ups=0.68, wpb=14784.8, bsz=1024, num_updates=29000, lr=0.000207614, gnorm=1.065, clip=0, loss_scale=4096, train_wall=146, wall=44252
2022-08-02 22:03:16 | INFO | train_inner | epoch 008:   3135 / 3715 loss=6.646, nll_loss=3.659, mask_ins=1.094, word_ins_ml=5.184, word_reposition=0.369, ppl=100.14, wps=9910.8, ups=0.68, wpb=14640.6, bsz=1024, num_updates=29100, lr=0.000207257, gnorm=1.093, clip=0, loss_scale=4096, train_wall=146, wall=44399
2022-08-02 22:05:44 | INFO | train_inner | epoch 008:   3235 / 3715 loss=6.601, nll_loss=3.624, mask_ins=1.086, word_ins_ml=5.153, word_reposition=0.362, ppl=97.06, wps=9911.9, ups=0.68, wpb=14655.5, bsz=1024, num_updates=29200, lr=0.000206901, gnorm=1.096, clip=0, loss_scale=5530, train_wall=146, wall=44547
2022-08-02 22:08:12 | INFO | train_inner | epoch 008:   3335 / 3715 loss=6.633, nll_loss=3.655, mask_ins=1.09, word_ins_ml=5.18, word_reposition=0.362, ppl=99.26, wps=9893.7, ups=0.68, wpb=14597.2, bsz=1024, num_updates=29300, lr=0.000206548, gnorm=1.063, clip=0, loss_scale=8192, train_wall=146, wall=44695
2022-08-02 22:10:40 | INFO | train_inner | epoch 008:   3435 / 3715 loss=6.646, nll_loss=3.666, mask_ins=1.091, word_ins_ml=5.19, word_reposition=0.365, ppl=100.13, wps=9898.4, ups=0.68, wpb=14631.7, bsz=1024, num_updates=29400, lr=0.000206197, gnorm=1.078, clip=0, loss_scale=8192, train_wall=146, wall=44843
2022-08-02 22:13:08 | INFO | train_inner | epoch 008:   3535 / 3715 loss=6.635, nll_loss=3.648, mask_ins=1.087, word_ins_ml=5.174, word_reposition=0.374, ppl=99.41, wps=9950.7, ups=0.68, wpb=14728.9, bsz=1024, num_updates=29500, lr=0.000205847, gnorm=1.066, clip=0, loss_scale=8192, train_wall=146, wall=44991
2022-08-02 22:15:35 | INFO | train_inner | epoch 008:   3635 / 3715 loss=6.607, nll_loss=3.622, mask_ins=1.088, word_ins_ml=5.151, word_reposition=0.367, ppl=97.44, wps=9863.7, ups=0.68, wpb=14592.2, bsz=1023.8, num_updates=29600, lr=0.000205499, gnorm=1.066, clip=0, loss_scale=8192, train_wall=146, wall=45139
2022-08-02 22:17:34 | INFO | train | epoch 008 | loss 6.643 | nll_loss 3.656 | mask_ins 1.093 | word_ins_ml 5.182 | word_reposition 0.369 | ppl 99.97 | wps 9699.7 | ups 0.66 | wpb 14662.5 | bsz 1023.7 | num_updates 29680 | lr 0.000205222 | gnorm 1.078 | clip 0 | loss_scale 6630 | train_wall 5425 | wall 45257
2022-08-02 22:19:23 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.558 | nll_loss 3.473 | mask_ins 1.077 | word_ins_ml 5.096 | word_reposition 0.384 | ppl 94.2 | wps 24945 | wpb 1849.4 | bsz 127.9 | num_updates 29680 | best_loss 6.558
2022-08-02 22:19:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint8.pt (epoch 8 @ 29680 updates, score 6.558) (writing took 6.128961840644479 seconds)
2022-08-02 22:19:59 | INFO | train_inner | epoch 009:     20 / 3715 loss=6.621, nll_loss=3.642, mask_ins=1.087, word_ins_ml=5.169, word_reposition=0.366, ppl=98.46, wps=5555.3, ups=0.38, wpb=14653.2, bsz=1014.7, num_updates=29700, lr=0.000205152, gnorm=1.091, clip=0, loss_scale=10076, train_wall=146, wall=45402
2022-08-02 22:20:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 22:21:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-02 22:22:30 | INFO | train_inner | epoch 009:    122 / 3715 loss=6.601, nll_loss=3.622, mask_ins=1.084, word_ins_ml=5.152, word_reposition=0.365, ppl=97.1, wps=9654.4, ups=0.66, wpb=14551.3, bsz=1024, num_updates=29800, lr=0.000204808, gnorm=1.07, clip=0, loss_scale=8272, train_wall=149, wall=45553
2022-08-02 22:24:58 | INFO | train_inner | epoch 009:    222 / 3715 loss=6.588, nll_loss=3.604, mask_ins=1.085, word_ins_ml=5.135, word_reposition=0.368, ppl=96.23, wps=9796.5, ups=0.67, wpb=14550.1, bsz=1023.8, num_updates=29900, lr=0.000204465, gnorm=1.076, clip=0, loss_scale=4096, train_wall=147, wall=45702
2022-08-02 22:27:27 | INFO | train_inner | epoch 009:    322 / 3715 loss=6.588, nll_loss=3.609, mask_ins=1.084, word_ins_ml=5.14, word_reposition=0.364, ppl=96.19, wps=9841.6, ups=0.67, wpb=14647.5, bsz=1024, num_updates=30000, lr=0.000204124, gnorm=1.072, clip=0, loss_scale=4096, train_wall=147, wall=45850
2022-08-02 22:29:56 | INFO | train_inner | epoch 009:    422 / 3715 loss=6.59, nll_loss=3.606, mask_ins=1.087, word_ins_ml=5.137, word_reposition=0.366, ppl=96.36, wps=9888.8, ups=0.67, wpb=14713.4, bsz=1024, num_updates=30100, lr=0.000203785, gnorm=1.089, clip=0, loss_scale=4096, train_wall=147, wall=45999
2022-08-02 22:32:26 | INFO | train_inner | epoch 009:    522 / 3715 loss=6.588, nll_loss=3.609, mask_ins=1.08, word_ins_ml=5.14, word_reposition=0.368, ppl=96.18, wps=9866, ups=0.67, wpb=14800.3, bsz=1024, num_updates=30200, lr=0.000203447, gnorm=1.06, clip=0, loss_scale=4096, train_wall=148, wall=46149
2022-08-02 22:34:55 | INFO | train_inner | epoch 009:    622 / 3715 loss=6.602, nll_loss=3.609, mask_ins=1.09, word_ins_ml=5.141, word_reposition=0.371, ppl=97.11, wps=9954.7, ups=0.67, wpb=14782.1, bsz=1024, num_updates=30300, lr=0.000203111, gnorm=1.097, clip=0, loss_scale=5734, train_wall=147, wall=46298
2022-08-02 22:37:23 | INFO | train_inner | epoch 009:    722 / 3715 loss=6.592, nll_loss=3.623, mask_ins=1.082, word_ins_ml=5.152, word_reposition=0.358, ppl=96.49, wps=9749.8, ups=0.67, wpb=14483.7, bsz=1024, num_updates=30400, lr=0.000202777, gnorm=1.081, clip=0, loss_scale=8192, train_wall=147, wall=46446
2022-08-02 22:39:51 | INFO | train_inner | epoch 009:    822 / 3715 loss=6.611, nll_loss=3.632, mask_ins=1.084, word_ins_ml=5.16, word_reposition=0.366, ppl=97.72, wps=9862.1, ups=0.67, wpb=14621.6, bsz=1024, num_updates=30500, lr=0.000202444, gnorm=1.074, clip=0, loss_scale=8192, train_wall=146, wall=46595
2022-08-02 22:42:20 | INFO | train_inner | epoch 009:    922 / 3715 loss=6.578, nll_loss=3.597, mask_ins=1.086, word_ins_ml=5.129, word_reposition=0.363, ppl=95.54, wps=9880.1, ups=0.67, wpb=14640.7, bsz=1024, num_updates=30600, lr=0.000202113, gnorm=1.062, clip=0, loss_scale=8192, train_wall=146, wall=46743
2022-08-02 22:44:48 | INFO | train_inner | epoch 009:   1022 / 3715 loss=6.598, nll_loss=3.614, mask_ins=1.085, word_ins_ml=5.144, word_reposition=0.368, ppl=96.88, wps=9923.6, ups=0.67, wpb=14765, bsz=1024, num_updates=30700, lr=0.000201784, gnorm=1.073, clip=0, loss_scale=8192, train_wall=147, wall=46892
2022-08-02 22:47:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 22:47:18 | INFO | train_inner | epoch 009:   1123 / 3715 loss=6.586, nll_loss=3.611, mask_ins=1.081, word_ins_ml=5.141, word_reposition=0.364, ppl=96.09, wps=9842.6, ups=0.67, wpb=14740, bsz=1024, num_updates=30800, lr=0.000201456, gnorm=1.076, clip=0, loss_scale=10058, train_wall=148, wall=47041
2022-08-02 22:49:46 | INFO | train_inner | epoch 009:   1223 / 3715 loss=6.588, nll_loss=3.609, mask_ins=1.083, word_ins_ml=5.139, word_reposition=0.365, ppl=96.17, wps=9860.5, ups=0.67, wpb=14613.9, bsz=1024, num_updates=30900, lr=0.000201129, gnorm=1.073, clip=0, loss_scale=8192, train_wall=146, wall=47190
2022-08-02 22:52:14 | INFO | train_inner | epoch 009:   1323 / 3715 loss=6.59, nll_loss=3.608, mask_ins=1.086, word_ins_ml=5.139, word_reposition=0.365, ppl=96.33, wps=9971.4, ups=0.68, wpb=14712, bsz=1024, num_updates=31000, lr=0.000200805, gnorm=1.074, clip=0, loss_scale=8192, train_wall=146, wall=47337
2022-08-02 22:54:42 | INFO | train_inner | epoch 009:   1423 / 3715 loss=6.587, nll_loss=3.614, mask_ins=1.082, word_ins_ml=5.144, word_reposition=0.361, ppl=96.16, wps=9884.4, ups=0.68, wpb=14588.5, bsz=1024, num_updates=31100, lr=0.000200482, gnorm=1.078, clip=0, loss_scale=8192, train_wall=146, wall=47485
2022-08-02 22:57:09 | INFO | train_inner | epoch 009:   1523 / 3715 loss=6.606, nll_loss=3.624, mask_ins=1.088, word_ins_ml=5.153, word_reposition=0.365, ppl=97.39, wps=9922, ups=0.68, wpb=14651.7, bsz=1024, num_updates=31200, lr=0.00020016, gnorm=1.085, clip=0, loss_scale=8192, train_wall=146, wall=47632
2022-08-02 22:59:37 | INFO | train_inner | epoch 009:   1623 / 3715 loss=6.595, nll_loss=3.627, mask_ins=1.081, word_ins_ml=5.155, word_reposition=0.359, ppl=96.66, wps=9875, ups=0.68, wpb=14574.9, bsz=1024, num_updates=31300, lr=0.00019984, gnorm=1.063, clip=0, loss_scale=8192, train_wall=146, wall=47780
2022-08-02 22:59:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 23:02:06 | INFO | train_inner | epoch 009:   1724 / 3715 loss=6.609, nll_loss=3.633, mask_ins=1.084, word_ins_ml=5.161, word_reposition=0.365, ppl=97.63, wps=9873.9, ups=0.67, wpb=14753.4, bsz=1024, num_updates=31400, lr=0.000199522, gnorm=1.077, clip=0, loss_scale=8841, train_wall=148, wall=47929
2022-08-02 23:04:34 | INFO | train_inner | epoch 009:   1824 / 3715 loss=6.567, nll_loss=3.596, mask_ins=1.077, word_ins_ml=5.128, word_reposition=0.362, ppl=94.81, wps=9932.4, ups=0.67, wpb=14730, bsz=1024, num_updates=31500, lr=0.000199205, gnorm=1.061, clip=0, loss_scale=8192, train_wall=146, wall=48078
2022-08-02 23:07:03 | INFO | train_inner | epoch 009:   1924 / 3715 loss=6.578, nll_loss=3.602, mask_ins=1.082, word_ins_ml=5.133, word_reposition=0.363, ppl=95.53, wps=9863.3, ups=0.67, wpb=14638.4, bsz=1024, num_updates=31600, lr=0.000198889, gnorm=1.088, clip=0, loss_scale=8192, train_wall=147, wall=48226
2022-08-02 23:09:31 | INFO | train_inner | epoch 009:   2024 / 3715 loss=6.595, nll_loss=3.614, mask_ins=1.086, word_ins_ml=5.144, word_reposition=0.364, ppl=96.65, wps=9838, ups=0.67, wpb=14616.4, bsz=1024, num_updates=31700, lr=0.000198575, gnorm=1.085, clip=0, loss_scale=8192, train_wall=147, wall=48375
2022-08-02 23:11:59 | INFO | train_inner | epoch 009:   2124 / 3715 loss=6.586, nll_loss=3.6, mask_ins=1.089, word_ins_ml=5.13, word_reposition=0.366, ppl=96.07, wps=9984.4, ups=0.68, wpb=14753.2, bsz=1024, num_updates=31800, lr=0.000198263, gnorm=1.075, clip=0, loss_scale=8192, train_wall=146, wall=48522
2022-08-02 23:14:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 23:14:29 | INFO | train_inner | epoch 009:   2225 / 3715 loss=6.599, nll_loss=3.62, mask_ins=1.083, word_ins_ml=5.149, word_reposition=0.367, ppl=96.91, wps=9840.2, ups=0.67, wpb=14724.7, bsz=1024, num_updates=31900, lr=0.000197952, gnorm=1.062, clip=0, loss_scale=14032, train_wall=148, wall=48672
2022-08-02 23:16:57 | INFO | train_inner | epoch 009:   2325 / 3715 loss=6.597, nll_loss=3.629, mask_ins=1.077, word_ins_ml=5.158, word_reposition=0.362, ppl=96.78, wps=9846.2, ups=0.67, wpb=14631.9, bsz=1024, num_updates=32000, lr=0.000197642, gnorm=1.068, clip=0, loss_scale=8192, train_wall=147, wall=48821
2022-08-02 23:19:26 | INFO | train_inner | epoch 009:   2425 / 3715 loss=6.591, nll_loss=3.618, mask_ins=1.084, word_ins_ml=5.147, word_reposition=0.361, ppl=96.43, wps=9872.8, ups=0.67, wpb=14637, bsz=1024, num_updates=32100, lr=0.000197334, gnorm=1.084, clip=0, loss_scale=8192, train_wall=146, wall=48969
2022-08-02 23:21:54 | INFO | train_inner | epoch 009:   2525 / 3715 loss=6.584, nll_loss=3.605, mask_ins=1.083, word_ins_ml=5.136, word_reposition=0.366, ppl=95.93, wps=9895.6, ups=0.67, wpb=14704.7, bsz=1024, num_updates=32200, lr=0.000197028, gnorm=1.074, clip=0, loss_scale=8192, train_wall=147, wall=49118
2022-08-02 23:24:22 | INFO | train_inner | epoch 009:   2625 / 3715 loss=6.58, nll_loss=3.602, mask_ins=1.083, word_ins_ml=5.133, word_reposition=0.364, ppl=95.65, wps=9942, ups=0.68, wpb=14643.1, bsz=1024, num_updates=32300, lr=0.000196722, gnorm=1.072, clip=0, loss_scale=8192, train_wall=145, wall=49265
2022-08-02 23:26:49 | INFO | train_inner | epoch 009:   2725 / 3715 loss=6.572, nll_loss=3.601, mask_ins=1.076, word_ins_ml=5.132, word_reposition=0.364, ppl=95.15, wps=9993.4, ups=0.68, wpb=14726.3, bsz=1024, num_updates=32400, lr=0.000196419, gnorm=1.084, clip=0, loss_scale=8192, train_wall=146, wall=49412
2022-08-02 23:28:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 23:29:18 | INFO | train_inner | epoch 009:   2826 / 3715 loss=6.578, nll_loss=3.603, mask_ins=1.078, word_ins_ml=5.134, word_reposition=0.366, ppl=95.52, wps=9886.3, ups=0.67, wpb=14697.9, bsz=1024, num_updates=32500, lr=0.000196116, gnorm=1.099, clip=0, loss_scale=12653, train_wall=147, wall=49561
2022-08-02 23:31:45 | INFO | train_inner | epoch 009:   2926 / 3715 loss=6.593, nll_loss=3.617, mask_ins=1.08, word_ins_ml=5.146, word_reposition=0.366, ppl=96.52, wps=9979.7, ups=0.68, wpb=14695.4, bsz=1024, num_updates=32600, lr=0.000195815, gnorm=1.077, clip=0, loss_scale=8192, train_wall=145, wall=49708
2022-08-02 23:34:13 | INFO | train_inner | epoch 009:   3026 / 3715 loss=6.565, nll_loss=3.595, mask_ins=1.079, word_ins_ml=5.127, word_reposition=0.359, ppl=94.65, wps=9932.6, ups=0.68, wpb=14684.1, bsz=1024, num_updates=32700, lr=0.000195515, gnorm=1.073, clip=0, loss_scale=8192, train_wall=146, wall=49856
2022-08-02 23:36:42 | INFO | train_inner | epoch 009:   3126 / 3715 loss=6.581, nll_loss=3.603, mask_ins=1.086, word_ins_ml=5.134, word_reposition=0.36, ppl=95.71, wps=9776.8, ups=0.67, wpb=14606.9, bsz=1024, num_updates=32800, lr=0.000195217, gnorm=1.094, clip=0, loss_scale=8192, train_wall=148, wall=50005
2022-08-02 23:39:11 | INFO | train_inner | epoch 009:   3226 / 3715 loss=6.591, nll_loss=3.618, mask_ins=1.083, word_ins_ml=5.148, word_reposition=0.361, ppl=96.42, wps=9821.7, ups=0.67, wpb=14626.2, bsz=1024, num_updates=32900, lr=0.00019492, gnorm=1.068, clip=0, loss_scale=8192, train_wall=147, wall=50154
2022-08-02 23:41:40 | INFO | train_inner | epoch 009:   3326 / 3715 loss=6.587, nll_loss=3.613, mask_ins=1.081, word_ins_ml=5.142, word_reposition=0.364, ppl=96.17, wps=9895.4, ups=0.67, wpb=14745, bsz=1024, num_updates=33000, lr=0.000194625, gnorm=1.096, clip=0, loss_scale=10322, train_wall=147, wall=50303
2022-08-02 23:43:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 23:44:13 | INFO | train_inner | epoch 009:   3427 / 3715 loss=6.579, nll_loss=3.604, mask_ins=1.084, word_ins_ml=5.135, word_reposition=0.36, ppl=95.61, wps=9552, ups=0.66, wpb=14576, bsz=1024, num_updates=33100, lr=0.000194331, gnorm=1.084, clip=0, loss_scale=12410, train_wall=151, wall=50456
2022-08-02 23:46:43 | INFO | train_inner | epoch 009:   3527 / 3715 loss=6.573, nll_loss=3.596, mask_ins=1.079, word_ins_ml=5.127, word_reposition=0.367, ppl=95.23, wps=9779.9, ups=0.66, wpb=14738.8, bsz=1024, num_updates=33200, lr=0.000194038, gnorm=1.055, clip=0, loss_scale=8192, train_wall=149, wall=50607
2022-08-02 23:49:14 | INFO | train_inner | epoch 009:   3627 / 3715 loss=6.582, nll_loss=3.604, mask_ins=1.082, word_ins_ml=5.134, word_reposition=0.366, ppl=95.83, wps=9706.3, ups=0.66, wpb=14636.5, bsz=1024, num_updates=33300, lr=0.000193746, gnorm=1.08, clip=0, loss_scale=8192, train_wall=149, wall=50757
2022-08-02 23:51:26 | INFO | train | epoch 009 | loss 6.588 | nll_loss 3.611 | mask_ins 1.083 | word_ins_ml 5.141 | word_reposition 0.364 | ppl 96.2 | wps 9652 | ups 0.66 | wpb 14661.8 | bsz 1023.7 | num_updates 33388 | lr 0.000193491 | gnorm 1.077 | clip 0 | loss_scale 8252 | train_wall 5447 | wall 50889
2022-08-02 23:53:16 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.516 | nll_loss 3.45 | mask_ins 1.063 | word_ins_ml 5.068 | word_reposition 0.385 | ppl 91.5 | wps 24946.8 | wpb 1849.4 | bsz 127.9 | num_updates 33388 | best_loss 6.516
2022-08-02 23:53:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint9.pt (epoch 9 @ 33388 updates, score 6.516) (writing took 6.54477758705616 seconds)
2022-08-02 23:53:41 | INFO | train_inner | epoch 010:     12 / 3715 loss=6.56, nll_loss=3.589, mask_ins=1.076, word_ins_ml=5.121, word_reposition=0.362, ppl=94.33, wps=5423, ups=0.38, wpb=14454.5, bsz=1014.7, num_updates=33400, lr=0.000193456, gnorm=1.083, clip=0, loss_scale=8192, train_wall=148, wall=51024
2022-08-02 23:56:12 | INFO | train_inner | epoch 010:    112 / 3715 loss=6.55, nll_loss=3.572, mask_ins=1.078, word_ins_ml=5.107, word_reposition=0.366, ppl=93.71, wps=9703.1, ups=0.66, wpb=14638.8, bsz=1024, num_updates=33500, lr=0.000193167, gnorm=1.064, clip=0, loss_scale=8192, train_wall=149, wall=51175
2022-08-02 23:58:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-02 23:58:44 | INFO | train_inner | epoch 010:    213 / 3715 loss=6.566, nll_loss=3.583, mask_ins=1.08, word_ins_ml=5.116, word_reposition=0.37, ppl=94.72, wps=9612.9, ups=0.66, wpb=14614.1, bsz=1024, num_updates=33600, lr=0.000192879, gnorm=1.084, clip=0, loss_scale=9328, train_wall=150, wall=51327
2022-08-03 00:01:14 | INFO | train_inner | epoch 010:    313 / 3715 loss=6.55, nll_loss=3.578, mask_ins=1.077, word_ins_ml=5.112, word_reposition=0.361, ppl=93.68, wps=9756.1, ups=0.66, wpb=14690.9, bsz=1023.8, num_updates=33700, lr=0.000192593, gnorm=1.064, clip=0, loss_scale=8192, train_wall=149, wall=51477
2022-08-03 00:03:45 | INFO | train_inner | epoch 010:    413 / 3715 loss=6.547, nll_loss=3.577, mask_ins=1.075, word_ins_ml=5.111, word_reposition=0.361, ppl=93.51, wps=9684.5, ups=0.67, wpb=14561.9, bsz=1024, num_updates=33800, lr=0.000192308, gnorm=1.074, clip=0, loss_scale=8192, train_wall=148, wall=51628
2022-08-03 00:06:16 | INFO | train_inner | epoch 010:    513 / 3715 loss=6.549, nll_loss=3.577, mask_ins=1.077, word_ins_ml=5.111, word_reposition=0.36, ppl=93.61, wps=9601.5, ups=0.66, wpb=14559.7, bsz=1024, num_updates=33900, lr=0.000192024, gnorm=1.073, clip=0, loss_scale=8192, train_wall=150, wall=51779
2022-08-03 00:07:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 00:08:49 | INFO | train_inner | epoch 010:    614 / 3715 loss=6.54, nll_loss=3.566, mask_ins=1.075, word_ins_ml=5.1, word_reposition=0.366, ppl=93.08, wps=9679.7, ups=0.65, wpb=14779.6, bsz=1024, num_updates=34000, lr=0.000191741, gnorm=1.081, clip=0, loss_scale=6083, train_wall=151, wall=51932
2022-08-03 00:11:20 | INFO | train_inner | epoch 010:    714 / 3715 loss=6.53, nll_loss=3.566, mask_ins=1.073, word_ins_ml=5.1, word_reposition=0.357, ppl=92.4, wps=9772.6, ups=0.66, wpb=14801.9, bsz=1024, num_updates=34100, lr=0.00019146, gnorm=1.075, clip=0, loss_scale=4096, train_wall=150, wall=52084
2022-08-03 00:13:51 | INFO | train_inner | epoch 010:    814 / 3715 loss=6.556, nll_loss=3.584, mask_ins=1.079, word_ins_ml=5.117, word_reposition=0.359, ppl=94.07, wps=9730.8, ups=0.66, wpb=14692.5, bsz=1024, num_updates=34200, lr=0.00019118, gnorm=1.084, clip=0, loss_scale=4096, train_wall=149, wall=52235
2022-08-03 00:16:22 | INFO | train_inner | epoch 010:    914 / 3715 loss=6.542, nll_loss=3.578, mask_ins=1.074, word_ins_ml=5.111, word_reposition=0.356, ppl=93.16, wps=9776.1, ups=0.66, wpb=14764.2, bsz=1024, num_updates=34300, lr=0.000190901, gnorm=1.056, clip=0, loss_scale=4096, train_wall=149, wall=52386
2022-08-03 00:18:53 | INFO | train_inner | epoch 010:   1014 / 3715 loss=6.544, nll_loss=3.577, mask_ins=1.077, word_ins_ml=5.11, word_reposition=0.357, ppl=93.31, wps=9701.1, ups=0.66, wpb=14656.4, bsz=1024, num_updates=34400, lr=0.000190623, gnorm=1.108, clip=0, loss_scale=4096, train_wall=149, wall=52537
2022-08-03 00:21:24 | INFO | train_inner | epoch 010:   1114 / 3715 loss=6.518, nll_loss=3.552, mask_ins=1.072, word_ins_ml=5.087, word_reposition=0.358, ppl=91.65, wps=9769.6, ups=0.66, wpb=14753.1, bsz=1024, num_updates=34500, lr=0.000190347, gnorm=1.058, clip=0, loss_scale=5734, train_wall=149, wall=52688
2022-08-03 00:23:56 | INFO | train_inner | epoch 010:   1214 / 3715 loss=6.534, nll_loss=3.573, mask_ins=1.074, word_ins_ml=5.107, word_reposition=0.353, ppl=92.67, wps=9630.4, ups=0.66, wpb=14585.2, bsz=1024, num_updates=34600, lr=0.000190071, gnorm=1.091, clip=0, loss_scale=8192, train_wall=150, wall=52839
2022-08-03 00:26:27 | INFO | train_inner | epoch 010:   1314 / 3715 loss=6.524, nll_loss=3.57, mask_ins=1.067, word_ins_ml=5.104, word_reposition=0.354, ppl=92.05, wps=9699.6, ups=0.66, wpb=14649.6, bsz=1024, num_updates=34700, lr=0.000189797, gnorm=1.059, clip=0, loss_scale=8192, train_wall=149, wall=52990
2022-08-03 00:28:58 | INFO | train_inner | epoch 010:   1414 / 3715 loss=6.545, nll_loss=3.573, mask_ins=1.075, word_ins_ml=5.107, word_reposition=0.363, ppl=93.4, wps=9681.3, ups=0.66, wpb=14580.8, bsz=1024, num_updates=34800, lr=0.000189525, gnorm=1.087, clip=0, loss_scale=8192, train_wall=149, wall=53141
2022-08-03 00:31:29 | INFO | train_inner | epoch 010:   1514 / 3715 loss=6.545, nll_loss=3.578, mask_ins=1.075, word_ins_ml=5.111, word_reposition=0.358, ppl=93.36, wps=9645.2, ups=0.66, wpb=14587.4, bsz=1024, num_updates=34900, lr=0.000189253, gnorm=1.083, clip=0, loss_scale=8192, train_wall=149, wall=53292
2022-08-03 00:34:00 | INFO | train_inner | epoch 010:   1614 / 3715 loss=6.545, nll_loss=3.579, mask_ins=1.068, word_ins_ml=5.112, word_reposition=0.365, ppl=93.37, wps=9730, ups=0.66, wpb=14693.3, bsz=1024, num_updates=35000, lr=0.000188982, gnorm=1.067, clip=0, loss_scale=10486, train_wall=149, wall=53443
2022-08-03 00:34:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 00:36:33 | INFO | train_inner | epoch 010:   1715 / 3715 loss=6.534, nll_loss=3.561, mask_ins=1.078, word_ins_ml=5.096, word_reposition=0.36, ppl=92.68, wps=9594.5, ups=0.65, wpb=14677, bsz=1024, num_updates=35100, lr=0.000188713, gnorm=1.08, clip=0, loss_scale=8922, train_wall=151, wall=53596
2022-08-03 00:39:04 | INFO | train_inner | epoch 010:   1815 / 3715 loss=6.536, nll_loss=3.562, mask_ins=1.078, word_ins_ml=5.097, word_reposition=0.361, ppl=92.8, wps=9706.9, ups=0.66, wpb=14658.3, bsz=1024, num_updates=35200, lr=0.000188445, gnorm=1.063, clip=0, loss_scale=8192, train_wall=149, wall=53747
2022-08-03 00:41:35 | INFO | train_inner | epoch 010:   1915 / 3715 loss=6.541, nll_loss=3.568, mask_ins=1.076, word_ins_ml=5.102, word_reposition=0.363, ppl=93.12, wps=9718, ups=0.66, wpb=14653.6, bsz=1024, num_updates=35300, lr=0.000188177, gnorm=1.095, clip=0, loss_scale=8192, train_wall=149, wall=53898
2022-08-03 00:44:05 | INFO | train_inner | epoch 010:   2015 / 3715 loss=6.514, nll_loss=3.547, mask_ins=1.072, word_ins_ml=5.083, word_reposition=0.358, ppl=91.38, wps=9817.5, ups=0.67, wpb=14743, bsz=1024, num_updates=35400, lr=0.000187912, gnorm=1.078, clip=0, loss_scale=8192, train_wall=148, wall=54048
2022-08-03 00:46:33 | INFO | train_inner | epoch 010:   2115 / 3715 loss=6.532, nll_loss=3.566, mask_ins=1.069, word_ins_ml=5.1, word_reposition=0.363, ppl=92.56, wps=9955.7, ups=0.67, wpb=14776.8, bsz=1024, num_updates=35500, lr=0.000187647, gnorm=1.078, clip=0, loss_scale=8192, train_wall=147, wall=54196
2022-08-03 00:49:02 | INFO | train_inner | epoch 010:   2215 / 3715 loss=6.527, nll_loss=3.569, mask_ins=1.069, word_ins_ml=5.104, word_reposition=0.355, ppl=92.24, wps=9795.2, ups=0.67, wpb=14531.9, bsz=1024, num_updates=35600, lr=0.000187383, gnorm=1.083, clip=0, loss_scale=14746, train_wall=147, wall=54345
2022-08-03 00:49:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 00:51:31 | INFO | train_inner | epoch 010:   2316 / 3715 loss=6.543, nll_loss=3.578, mask_ins=1.071, word_ins_ml=5.111, word_reposition=0.361, ppl=93.27, wps=9779.2, ups=0.67, wpb=14597.6, bsz=1024, num_updates=35700, lr=0.00018712, gnorm=1.08, clip=0, loss_scale=10220, train_wall=147, wall=54494
2022-08-03 00:53:59 | INFO | train_inner | epoch 010:   2416 / 3715 loss=6.528, nll_loss=3.565, mask_ins=1.072, word_ins_ml=5.099, word_reposition=0.357, ppl=92.31, wps=9795.9, ups=0.68, wpb=14489.1, bsz=1024, num_updates=35800, lr=0.000186859, gnorm=1.073, clip=0, loss_scale=8192, train_wall=146, wall=54642
2022-08-03 00:56:26 | INFO | train_inner | epoch 010:   2516 / 3715 loss=6.545, nll_loss=3.579, mask_ins=1.077, word_ins_ml=5.113, word_reposition=0.355, ppl=93.39, wps=9948.6, ups=0.68, wpb=14692.7, bsz=1024, num_updates=35900, lr=0.000186598, gnorm=1.074, clip=0, loss_scale=8192, train_wall=146, wall=54790
2022-08-03 00:58:54 | INFO | train_inner | epoch 010:   2616 / 3715 loss=6.521, nll_loss=3.557, mask_ins=1.072, word_ins_ml=5.092, word_reposition=0.357, ppl=91.84, wps=9887.5, ups=0.68, wpb=14606.5, bsz=1024, num_updates=36000, lr=0.000186339, gnorm=1.068, clip=0, loss_scale=8192, train_wall=146, wall=54937
2022-08-03 01:01:22 | INFO | train_inner | epoch 010:   2716 / 3715 loss=6.534, nll_loss=3.568, mask_ins=1.07, word_ins_ml=5.102, word_reposition=0.363, ppl=92.69, wps=10013.2, ups=0.68, wpb=14794.3, bsz=1024, num_updates=36100, lr=0.000186081, gnorm=1.075, clip=0, loss_scale=8192, train_wall=146, wall=55085
2022-08-03 01:02:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 01:03:52 | INFO | train_inner | epoch 010:   2817 / 3715 loss=6.532, nll_loss=3.566, mask_ins=1.07, word_ins_ml=5.1, word_reposition=0.362, ppl=92.56, wps=9813.7, ups=0.67, wpb=14753.4, bsz=1024, num_updates=36200, lr=0.000185824, gnorm=1.064, clip=0, loss_scale=8760, train_wall=148, wall=55235
2022-08-03 01:06:21 | INFO | train_inner | epoch 010:   2917 / 3715 loss=6.572, nll_loss=3.594, mask_ins=1.076, word_ins_ml=5.125, word_reposition=0.371, ppl=95.12, wps=9960.1, ups=0.67, wpb=14804.2, bsz=1024, num_updates=36300, lr=0.000185567, gnorm=1.072, clip=0, loss_scale=8192, train_wall=147, wall=55384
2022-08-03 01:08:50 | INFO | train_inner | epoch 010:   3017 / 3715 loss=6.541, nll_loss=3.569, mask_ins=1.073, word_ins_ml=5.102, word_reposition=0.366, ppl=93.1, wps=9882.2, ups=0.67, wpb=14747.8, bsz=1024, num_updates=36400, lr=0.000185312, gnorm=1.077, clip=0, loss_scale=8192, train_wall=147, wall=55533
2022-08-03 01:11:19 | INFO | train_inner | epoch 010:   3117 / 3715 loss=6.542, nll_loss=3.572, mask_ins=1.074, word_ins_ml=5.105, word_reposition=0.363, ppl=93.17, wps=9876.4, ups=0.67, wpb=14750.9, bsz=1024, num_updates=36500, lr=0.000185058, gnorm=1.071, clip=0, loss_scale=8192, train_wall=147, wall=55683
2022-08-03 01:13:50 | INFO | train_inner | epoch 010:   3217 / 3715 loss=6.562, nll_loss=3.595, mask_ins=1.078, word_ins_ml=5.126, word_reposition=0.358, ppl=94.5, wps=9737.3, ups=0.67, wpb=14634.8, bsz=1024, num_updates=36600, lr=0.000184805, gnorm=1.077, clip=0, loss_scale=8192, train_wall=148, wall=55833
2022-08-03 01:16:20 | INFO | train_inner | epoch 010:   3317 / 3715 loss=6.529, nll_loss=3.565, mask_ins=1.069, word_ins_ml=5.099, word_reposition=0.361, ppl=92.35, wps=9746.6, ups=0.66, wpb=14679.8, bsz=1024, num_updates=36700, lr=0.000184553, gnorm=1.077, clip=0, loss_scale=11960, train_wall=149, wall=55984
2022-08-03 01:17:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 01:18:52 | INFO | train_inner | epoch 010:   3418 / 3715 loss=6.515, nll_loss=3.543, mask_ins=1.07, word_ins_ml=5.08, word_reposition=0.366, ppl=91.45, wps=9732.1, ups=0.66, wpb=14762.8, bsz=1024, num_updates=36800, lr=0.000184302, gnorm=1.086, clip=0, loss_scale=11761, train_wall=150, wall=56135
2022-08-03 01:21:23 | INFO | train_inner | epoch 010:   3518 / 3715 loss=6.512, nll_loss=3.555, mask_ins=1.067, word_ins_ml=5.09, word_reposition=0.354, ppl=91.25, wps=9648, ups=0.66, wpb=14560.4, bsz=1024, num_updates=36900, lr=0.000184053, gnorm=1.075, clip=0, loss_scale=8192, train_wall=149, wall=56286
2022-08-03 01:23:53 | INFO | train_inner | epoch 010:   3618 / 3715 loss=6.508, nll_loss=3.544, mask_ins=1.067, word_ins_ml=5.08, word_reposition=0.36, ppl=91, wps=9674.2, ups=0.67, wpb=14503.1, bsz=1024, num_updates=37000, lr=0.000183804, gnorm=1.087, clip=0, loss_scale=8192, train_wall=148, wall=56436
2022-08-03 01:26:18 | INFO | train | epoch 010 | loss 6.537 | nll_loss 3.569 | mask_ins 1.073 | word_ins_ml 5.103 | word_reposition 0.361 | ppl 92.88 | wps 9553.8 | ups 0.65 | wpb 14661.6 | bsz 1023.7 | num_updates 37097 | lr 0.000183563 | gnorm 1.077 | clip 0 | loss_scale 8185 | train_wall 5506 | wall 56581
2022-08-03 01:28:08 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.478 | nll_loss 3.427 | mask_ins 1.055 | word_ins_ml 5.05 | word_reposition 0.373 | ppl 89.12 | wps 24968.2 | wpb 1849.4 | bsz 127.9 | num_updates 37097 | best_loss 6.478
2022-08-03 01:28:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint10.pt (epoch 10 @ 37097 updates, score 6.478) (writing took 6.308863628655672 seconds)
2022-08-03 01:28:19 | INFO | train_inner | epoch 011:      3 / 3715 loss=6.533, nll_loss=3.566, mask_ins=1.073, word_ins_ml=5.1, word_reposition=0.36, ppl=92.59, wps=5429.9, ups=0.38, wpb=14434.5, bsz=1014.7, num_updates=37100, lr=0.000183556, gnorm=1.097, clip=0, loss_scale=8192, train_wall=148, wall=56702
2022-08-03 01:30:50 | INFO | train_inner | epoch 011:    103 / 3715 loss=6.516, nll_loss=3.547, mask_ins=1.073, word_ins_ml=5.084, word_reposition=0.359, ppl=91.51, wps=9696.7, ups=0.66, wpb=14664.5, bsz=1024, num_updates=37200, lr=0.000183309, gnorm=1.074, clip=0, loss_scale=8192, train_wall=149, wall=56853
2022-08-03 01:33:21 | INFO | train_inner | epoch 011:    203 / 3715 loss=6.48, nll_loss=3.51, mask_ins=1.062, word_ins_ml=5.05, word_reposition=0.368, ppl=89.28, wps=9721.7, ups=0.66, wpb=14712.5, bsz=1024, num_updates=37300, lr=0.000183063, gnorm=1.07, clip=0, loss_scale=11878, train_wall=149, wall=57004
2022-08-03 01:33:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 01:35:54 | INFO | train_inner | epoch 011:    304 / 3715 loss=6.491, nll_loss=3.524, mask_ins=1.065, word_ins_ml=5.063, word_reposition=0.362, ppl=89.95, wps=9583.9, ups=0.65, wpb=14659.7, bsz=1024, num_updates=37400, lr=0.000182818, gnorm=1.078, clip=0, loss_scale=9084, train_wall=151, wall=57157
2022-08-03 01:38:25 | INFO | train_inner | epoch 011:    404 / 3715 loss=6.516, nll_loss=3.545, mask_ins=1.07, word_ins_ml=5.082, word_reposition=0.364, ppl=91.5, wps=9703.6, ups=0.66, wpb=14632.1, bsz=1024, num_updates=37500, lr=0.000182574, gnorm=1.089, clip=0, loss_scale=8192, train_wall=149, wall=57308
2022-08-03 01:40:56 | INFO | train_inner | epoch 011:    504 / 3715 loss=6.486, nll_loss=3.536, mask_ins=1.064, word_ins_ml=5.073, word_reposition=0.349, ppl=89.64, wps=9643.2, ups=0.66, wpb=14536.8, bsz=1024, num_updates=37600, lr=0.000182331, gnorm=1.06, clip=0, loss_scale=8192, train_wall=149, wall=57459
2022-08-03 01:43:26 | INFO | train_inner | epoch 011:    604 / 3715 loss=6.506, nll_loss=3.542, mask_ins=1.07, word_ins_ml=5.078, word_reposition=0.358, ppl=90.89, wps=9758.7, ups=0.67, wpb=14672.7, bsz=1024, num_updates=37700, lr=0.000182089, gnorm=1.088, clip=0, loss_scale=8192, train_wall=148, wall=57609
2022-08-03 01:45:54 | INFO | train_inner | epoch 011:    704 / 3715 loss=6.506, nll_loss=3.538, mask_ins=1.071, word_ins_ml=5.075, word_reposition=0.36, ppl=90.89, wps=9982.7, ups=0.68, wpb=14729.1, bsz=1024, num_updates=37800, lr=0.000181848, gnorm=1.073, clip=0, loss_scale=8192, train_wall=146, wall=57757
2022-08-03 01:47:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 01:48:23 | INFO | train_inner | epoch 011:    805 / 3715 loss=6.491, nll_loss=3.531, mask_ins=1.065, word_ins_ml=5.069, word_reposition=0.358, ppl=89.96, wps=9878.6, ups=0.67, wpb=14732.4, bsz=1024, num_updates=37900, lr=0.000181608, gnorm=1.062, clip=0, loss_scale=11842, train_wall=147, wall=57906
2022-08-03 01:50:50 | INFO | train_inner | epoch 011:    905 / 3715 loss=6.471, nll_loss=3.507, mask_ins=1.067, word_ins_ml=5.047, word_reposition=0.357, ppl=88.69, wps=9991.3, ups=0.68, wpb=14712.7, bsz=1024, num_updates=38000, lr=0.000181369, gnorm=1.065, clip=0, loss_scale=8192, train_wall=145, wall=58053
2022-08-03 01:53:18 | INFO | train_inner | epoch 011:   1005 / 3715 loss=6.501, nll_loss=3.531, mask_ins=1.07, word_ins_ml=5.07, word_reposition=0.361, ppl=90.58, wps=9966.6, ups=0.68, wpb=14690.9, bsz=1024, num_updates=38100, lr=0.000181131, gnorm=1.073, clip=0, loss_scale=8192, train_wall=146, wall=58201
2022-08-03 01:55:45 | INFO | train_inner | epoch 011:   1105 / 3715 loss=6.492, nll_loss=3.524, mask_ins=1.072, word_ins_ml=5.063, word_reposition=0.358, ppl=90.01, wps=9903.5, ups=0.68, wpb=14639.2, bsz=1024, num_updates=38200, lr=0.000180894, gnorm=1.082, clip=0, loss_scale=8192, train_wall=146, wall=58348
2022-08-03 01:58:13 | INFO | train_inner | epoch 011:   1205 / 3715 loss=6.517, nll_loss=3.555, mask_ins=1.07, word_ins_ml=5.091, word_reposition=0.356, ppl=91.56, wps=9922.2, ups=0.68, wpb=14690, bsz=1024, num_updates=38300, lr=0.000180657, gnorm=1.051, clip=0, loss_scale=8192, train_wall=146, wall=58497
2022-08-03 02:00:41 | INFO | train_inner | epoch 011:   1305 / 3715 loss=6.487, nll_loss=3.53, mask_ins=1.066, word_ins_ml=5.068, word_reposition=0.354, ppl=89.71, wps=9836.4, ups=0.68, wpb=14517.5, bsz=1024, num_updates=38400, lr=0.000180422, gnorm=1.075, clip=0, loss_scale=9994, train_wall=146, wall=58644
2022-08-03 02:02:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 02:03:11 | INFO | train_inner | epoch 011:   1406 / 3715 loss=6.499, nll_loss=3.539, mask_ins=1.067, word_ins_ml=5.075, word_reposition=0.356, ppl=90.43, wps=9884.5, ups=0.67, wpb=14795.2, bsz=1024, num_updates=38500, lr=0.000180187, gnorm=1.062, clip=0, loss_scale=13707, train_wall=148, wall=58794
2022-08-03 02:04:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 02:05:40 | INFO | train_inner | epoch 011:   1507 / 3715 loss=6.489, nll_loss=3.529, mask_ins=1.067, word_ins_ml=5.067, word_reposition=0.355, ppl=89.85, wps=9849.3, ups=0.67, wpb=14708.1, bsz=1024, num_updates=38600, lr=0.000179954, gnorm=1.066, clip=0, loss_scale=6975, train_wall=147, wall=58943
2022-08-03 02:08:10 | INFO | train_inner | epoch 011:   1607 / 3715 loss=6.492, nll_loss=3.529, mask_ins=1.065, word_ins_ml=5.067, word_reposition=0.361, ppl=90.02, wps=9835.9, ups=0.67, wpb=14758.7, bsz=1024, num_updates=38700, lr=0.000179721, gnorm=1.08, clip=0, loss_scale=4096, train_wall=148, wall=59093
2022-08-03 02:10:40 | INFO | train_inner | epoch 011:   1707 / 3715 loss=6.508, nll_loss=3.549, mask_ins=1.067, word_ins_ml=5.085, word_reposition=0.356, ppl=91.03, wps=9812.9, ups=0.67, wpb=14680.5, bsz=1024, num_updates=38800, lr=0.00017949, gnorm=1.072, clip=0, loss_scale=4096, train_wall=148, wall=59243
2022-08-03 02:13:09 | INFO | train_inner | epoch 011:   1807 / 3715 loss=6.477, nll_loss=3.522, mask_ins=1.067, word_ins_ml=5.06, word_reposition=0.349, ppl=89.07, wps=9830.2, ups=0.67, wpb=14633.6, bsz=1024, num_updates=38900, lr=0.000179259, gnorm=1.085, clip=0, loss_scale=4096, train_wall=147, wall=59392
2022-08-03 02:15:39 | INFO | train_inner | epoch 011:   1907 / 3715 loss=6.495, nll_loss=3.538, mask_ins=1.067, word_ins_ml=5.075, word_reposition=0.353, ppl=90.21, wps=9731.8, ups=0.67, wpb=14609.6, bsz=1024, num_updates=39000, lr=0.000179029, gnorm=1.085, clip=0, loss_scale=4096, train_wall=148, wall=59542
2022-08-03 02:18:08 | INFO | train_inner | epoch 011:   2007 / 3715 loss=6.486, nll_loss=3.53, mask_ins=1.064, word_ins_ml=5.067, word_reposition=0.355, ppl=89.65, wps=9783.3, ups=0.67, wpb=14647.9, bsz=1023.8, num_updates=39100, lr=0.0001788, gnorm=1.073, clip=0, loss_scale=4833, train_wall=148, wall=59692
2022-08-03 02:20:39 | INFO | train_inner | epoch 011:   2107 / 3715 loss=6.479, nll_loss=3.523, mask_ins=1.06, word_ins_ml=5.061, word_reposition=0.358, ppl=89.2, wps=9693.7, ups=0.67, wpb=14570.5, bsz=1024, num_updates=39200, lr=0.000178571, gnorm=1.079, clip=0, loss_scale=8192, train_wall=148, wall=59842
2022-08-03 02:23:09 | INFO | train_inner | epoch 011:   2207 / 3715 loss=6.477, nll_loss=3.525, mask_ins=1.06, word_ins_ml=5.063, word_reposition=0.354, ppl=89.1, wps=9749.3, ups=0.67, wpb=14654.6, bsz=1024, num_updates=39300, lr=0.000178344, gnorm=1.082, clip=0, loss_scale=8192, train_wall=148, wall=59992
2022-08-03 02:25:39 | INFO | train_inner | epoch 011:   2307 / 3715 loss=6.494, nll_loss=3.534, mask_ins=1.068, word_ins_ml=5.071, word_reposition=0.355, ppl=90.16, wps=9754.2, ups=0.67, wpb=14644.9, bsz=1024, num_updates=39400, lr=0.000178118, gnorm=1.061, clip=0, loss_scale=8192, train_wall=148, wall=60142
2022-08-03 02:28:09 | INFO | train_inner | epoch 011:   2407 / 3715 loss=6.498, nll_loss=3.541, mask_ins=1.068, word_ins_ml=5.077, word_reposition=0.353, ppl=90.41, wps=9730.6, ups=0.67, wpb=14592.7, bsz=1024, num_updates=39500, lr=0.000177892, gnorm=1.087, clip=0, loss_scale=8192, train_wall=148, wall=60292
2022-08-03 02:30:39 | INFO | train_inner | epoch 011:   2507 / 3715 loss=6.474, nll_loss=3.514, mask_ins=1.066, word_ins_ml=5.053, word_reposition=0.355, ppl=88.91, wps=9844.1, ups=0.67, wpb=14777.1, bsz=1024, num_updates=39600, lr=0.000177667, gnorm=1.066, clip=0, loss_scale=8684, train_wall=148, wall=60442
2022-08-03 02:33:10 | INFO | train_inner | epoch 011:   2607 / 3715 loss=6.493, nll_loss=3.528, mask_ins=1.068, word_ins_ml=5.065, word_reposition=0.36, ppl=90.05, wps=9745.3, ups=0.66, wpb=14666, bsz=1024, num_updates=39700, lr=0.000177443, gnorm=1.072, clip=0, loss_scale=16384, train_wall=149, wall=60593
2022-08-03 02:34:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 02:35:42 | INFO | train_inner | epoch 011:   2708 / 3715 loss=6.49, nll_loss=3.535, mask_ins=1.065, word_ins_ml=5.072, word_reposition=0.353, ppl=89.87, wps=9646.6, ups=0.66, wpb=14688.2, bsz=1024, num_updates=39800, lr=0.00017722, gnorm=1.074, clip=0, loss_scale=11274, train_wall=150, wall=60745
2022-08-03 02:36:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 02:38:12 | INFO | train_inner | epoch 011:   2809 / 3715 loss=6.505, nll_loss=3.547, mask_ins=1.068, word_ins_ml=5.082, word_reposition=0.355, ppl=90.84, wps=9802.8, ups=0.67, wpb=14723.4, bsz=1024, num_updates=39900, lr=0.000176998, gnorm=1.072, clip=0, loss_scale=5556, train_wall=148, wall=60895
2022-08-03 02:40:43 | INFO | train_inner | epoch 011:   2909 / 3715 loss=6.493, nll_loss=3.526, mask_ins=1.064, word_ins_ml=5.064, word_reposition=0.365, ppl=90.04, wps=9675.3, ups=0.66, wpb=14629.4, bsz=1024, num_updates=40000, lr=0.000176777, gnorm=1.06, clip=0, loss_scale=4096, train_wall=149, wall=61047
2022-08-03 02:43:15 | INFO | train_inner | epoch 011:   3009 / 3715 loss=6.493, nll_loss=3.536, mask_ins=1.067, word_ins_ml=5.072, word_reposition=0.353, ppl=90.07, wps=9672.4, ups=0.66, wpb=14640.5, bsz=1024, num_updates=40100, lr=0.000176556, gnorm=1.088, clip=0, loss_scale=4096, train_wall=149, wall=61198
2022-08-03 02:45:46 | INFO | train_inner | epoch 011:   3109 / 3715 loss=6.506, nll_loss=3.541, mask_ins=1.068, word_ins_ml=5.077, word_reposition=0.361, ppl=90.88, wps=9691.5, ups=0.66, wpb=14633.8, bsz=1024, num_updates=40200, lr=0.000176336, gnorm=1.091, clip=0, loss_scale=4096, train_wall=149, wall=61349
2022-08-03 02:48:16 | INFO | train_inner | epoch 011:   3209 / 3715 loss=6.489, nll_loss=3.533, mask_ins=1.065, word_ins_ml=5.069, word_reposition=0.354, ppl=89.8, wps=9719.6, ups=0.66, wpb=14645.1, bsz=1024, num_updates=40300, lr=0.000176117, gnorm=1.06, clip=0, loss_scale=4096, train_wall=149, wall=61500
2022-08-03 02:50:46 | INFO | train_inner | epoch 011:   3309 / 3715 loss=6.474, nll_loss=3.519, mask_ins=1.064, word_ins_ml=5.058, word_reposition=0.351, ppl=88.88, wps=9855.2, ups=0.67, wpb=14706.8, bsz=1024, num_updates=40400, lr=0.000175899, gnorm=1.068, clip=0, loss_scale=6267, train_wall=147, wall=61649
2022-08-03 02:53:16 | INFO | train_inner | epoch 011:   3409 / 3715 loss=6.481, nll_loss=3.53, mask_ins=1.059, word_ins_ml=5.067, word_reposition=0.354, ppl=89.33, wps=9678.4, ups=0.66, wpb=14591.6, bsz=1024, num_updates=40500, lr=0.000175682, gnorm=1.08, clip=0, loss_scale=8192, train_wall=149, wall=61800
2022-08-03 02:55:46 | INFO | train_inner | epoch 011:   3509 / 3715 loss=6.498, nll_loss=3.532, mask_ins=1.068, word_ins_ml=5.069, word_reposition=0.362, ppl=90.41, wps=9777, ups=0.67, wpb=14639, bsz=1024, num_updates=40600, lr=0.000175466, gnorm=1.083, clip=0, loss_scale=8192, train_wall=148, wall=61949
2022-08-03 02:58:17 | INFO | train_inner | epoch 011:   3609 / 3715 loss=6.489, nll_loss=3.537, mask_ins=1.063, word_ins_ml=5.074, word_reposition=0.352, ppl=89.8, wps=9752.4, ups=0.66, wpb=14682.4, bsz=1024, num_updates=40700, lr=0.00017525, gnorm=1.055, clip=0, loss_scale=8192, train_wall=149, wall=62100
2022-08-03 03:00:48 | INFO | train_inner | epoch 011:   3709 / 3715 loss=6.505, nll_loss=3.536, mask_ins=1.069, word_ins_ml=5.073, word_reposition=0.363, ppl=90.82, wps=9738.1, ups=0.66, wpb=14700.3, bsz=1024, num_updates=40800, lr=0.000175035, gnorm=1.072, clip=0, loss_scale=8192, train_wall=149, wall=62251
2022-08-03 03:00:56 | INFO | train | epoch 011 | loss 6.493 | nll_loss 3.532 | mask_ins 1.066 | word_ins_ml 5.07 | word_reposition 0.357 | ppl 90.08 | wps 9578.5 | ups 0.65 | wpb 14662.1 | bsz 1023.7 | num_updates 40806 | lr 0.000175022 | gnorm 1.074 | clip 0 | loss_scale 7802 | train_wall 5492 | wall 62259
2022-08-03 03:02:46 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.454 | nll_loss 3.399 | mask_ins 1.049 | word_ins_ml 5.022 | word_reposition 0.382 | ppl 87.65 | wps 24799.7 | wpb 1849.4 | bsz 127.9 | num_updates 40806 | best_loss 6.454
2022-08-03 03:02:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint11.pt (epoch 11 @ 40806 updates, score 6.454) (writing took 6.194392403587699 seconds)
2022-08-03 03:05:13 | INFO | train_inner | epoch 012:     94 / 3715 loss=6.467, nll_loss=3.506, mask_ins=1.063, word_ins_ml=5.046, word_reposition=0.358, ppl=88.48, wps=5458.5, ups=0.38, wpb=14484.9, bsz=1014.7, num_updates=40900, lr=0.000174821, gnorm=1.089, clip=0, loss_scale=11551, train_wall=147, wall=62516
2022-08-03 03:05:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 03:07:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 03:07:45 | INFO | train_inner | epoch 012:    196 / 3715 loss=6.455, nll_loss=3.495, mask_ins=1.061, word_ins_ml=5.036, word_reposition=0.357, ppl=87.72, wps=9684, ups=0.66, wpb=14739.7, bsz=1024, num_updates=41000, lr=0.000174608, gnorm=1.079, clip=0, loss_scale=8031, train_wall=150, wall=62668
2022-08-03 03:10:15 | INFO | train_inner | epoch 012:    296 / 3715 loss=6.468, nll_loss=3.507, mask_ins=1.066, word_ins_ml=5.047, word_reposition=0.356, ppl=88.55, wps=9811.6, ups=0.67, wpb=14667.5, bsz=1024, num_updates=41100, lr=0.000174395, gnorm=1.079, clip=0, loss_scale=4096, train_wall=148, wall=62818
2022-08-03 03:12:45 | INFO | train_inner | epoch 012:    396 / 3715 loss=6.456, nll_loss=3.506, mask_ins=1.058, word_ins_ml=5.047, word_reposition=0.351, ppl=87.8, wps=9817.1, ups=0.67, wpb=14719.3, bsz=1024, num_updates=41200, lr=0.000174183, gnorm=1.083, clip=0, loss_scale=4096, train_wall=148, wall=62968
2022-08-03 03:15:16 | INFO | train_inner | epoch 012:    496 / 3715 loss=6.468, nll_loss=3.509, mask_ins=1.058, word_ins_ml=5.049, word_reposition=0.362, ppl=88.51, wps=9776, ups=0.66, wpb=14779.4, bsz=1024, num_updates=41300, lr=0.000173972, gnorm=1.073, clip=0, loss_scale=4096, train_wall=149, wall=63119
2022-08-03 03:17:46 | INFO | train_inner | epoch 012:    596 / 3715 loss=6.452, nll_loss=3.495, mask_ins=1.061, word_ins_ml=5.036, word_reposition=0.354, ppl=87.52, wps=9717.1, ups=0.66, wpb=14637.6, bsz=1024, num_updates=41400, lr=0.000173762, gnorm=1.086, clip=0, loss_scale=4096, train_wall=149, wall=63270
2022-08-03 03:20:19 | INFO | train_inner | epoch 012:    696 / 3715 loss=6.453, nll_loss=3.504, mask_ins=1.058, word_ins_ml=5.045, word_reposition=0.35, ppl=87.59, wps=9669.8, ups=0.66, wpb=14722.3, bsz=1024, num_updates=41500, lr=0.000173553, gnorm=1.087, clip=0, loss_scale=4096, train_wall=150, wall=63422
2022-08-03 03:22:51 | INFO | train_inner | epoch 012:    796 / 3715 loss=6.464, nll_loss=3.5, mask_ins=1.063, word_ins_ml=5.04, word_reposition=0.361, ppl=88.28, wps=9773.7, ups=0.66, wpb=14836.7, bsz=1024, num_updates=41600, lr=0.000173344, gnorm=1.062, clip=0, loss_scale=8028, train_wall=150, wall=63574
2022-08-03 03:25:23 | INFO | train_inner | epoch 012:    896 / 3715 loss=6.443, nll_loss=3.493, mask_ins=1.057, word_ins_ml=5.034, word_reposition=0.352, ppl=86.99, wps=9608, ups=0.66, wpb=14610.4, bsz=1024, num_updates=41700, lr=0.000173136, gnorm=1.074, clip=0, loss_scale=8192, train_wall=150, wall=63726
2022-08-03 03:27:54 | INFO | train_inner | epoch 012:    996 / 3715 loss=6.461, nll_loss=3.507, mask_ins=1.057, word_ins_ml=5.046, word_reposition=0.357, ppl=88.11, wps=9743.4, ups=0.66, wpb=14726.5, bsz=1024, num_updates=41800, lr=0.000172929, gnorm=1.079, clip=0, loss_scale=8192, train_wall=149, wall=63877
2022-08-03 03:30:26 | INFO | train_inner | epoch 012:   1096 / 3715 loss=6.44, nll_loss=3.488, mask_ins=1.056, word_ins_ml=5.03, word_reposition=0.354, ppl=86.83, wps=9651.9, ups=0.66, wpb=14684.5, bsz=1024, num_updates=41900, lr=0.000172722, gnorm=1.063, clip=0, loss_scale=8192, train_wall=150, wall=64029
2022-08-03 03:32:56 | INFO | train_inner | epoch 012:   1196 / 3715 loss=6.472, nll_loss=3.517, mask_ins=1.056, word_ins_ml=5.056, word_reposition=0.36, ppl=88.8, wps=9763, ups=0.67, wpb=14650.3, bsz=1024, num_updates=42000, lr=0.000172516, gnorm=1.078, clip=0, loss_scale=8192, train_wall=148, wall=64179
2022-08-03 03:35:29 | INFO | train_inner | epoch 012:   1296 / 3715 loss=6.473, nll_loss=3.516, mask_ins=1.065, word_ins_ml=5.055, word_reposition=0.353, ppl=88.84, wps=9611, ups=0.65, wpb=14693.3, bsz=1024, num_updates=42100, lr=0.000172311, gnorm=1.063, clip=0, loss_scale=15073, train_wall=151, wall=64332
2022-08-03 03:38:00 | INFO | train_inner | epoch 012:   1396 / 3715 loss=6.472, nll_loss=3.513, mask_ins=1.065, word_ins_ml=5.052, word_reposition=0.355, ppl=88.75, wps=9688.8, ups=0.66, wpb=14640.7, bsz=1024, num_updates=42200, lr=0.000172107, gnorm=1.085, clip=0, loss_scale=16384, train_wall=149, wall=64483
2022-08-03 03:38:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 03:40:33 | INFO | train_inner | epoch 012:   1497 / 3715 loss=6.448, nll_loss=3.491, mask_ins=1.065, word_ins_ml=5.033, word_reposition=0.351, ppl=87.34, wps=9582.8, ups=0.65, wpb=14687.5, bsz=1024, num_updates=42300, lr=0.000171904, gnorm=1.081, clip=0, loss_scale=10706, train_wall=151, wall=64636
2022-08-03 03:43:05 | INFO | train_inner | epoch 012:   1597 / 3715 loss=6.462, nll_loss=3.51, mask_ins=1.061, word_ins_ml=5.05, word_reposition=0.351, ppl=88.17, wps=9651.3, ups=0.66, wpb=14648.2, bsz=1024, num_updates=42400, lr=0.000171701, gnorm=1.081, clip=0, loss_scale=8192, train_wall=150, wall=64788
2022-08-03 03:45:37 | INFO | train_inner | epoch 012:   1697 / 3715 loss=6.457, nll_loss=3.505, mask_ins=1.059, word_ins_ml=5.044, word_reposition=0.354, ppl=87.85, wps=9705.7, ups=0.66, wpb=14729.1, bsz=1024, num_updates=42500, lr=0.000171499, gnorm=1.062, clip=0, loss_scale=8192, train_wall=150, wall=64940
2022-08-03 03:48:08 | INFO | train_inner | epoch 012:   1797 / 3715 loss=6.452, nll_loss=3.499, mask_ins=1.059, word_ins_ml=5.04, word_reposition=0.352, ppl=87.53, wps=9648.9, ups=0.66, wpb=14565.5, bsz=1024, num_updates=42600, lr=0.000171297, gnorm=1.093, clip=0, loss_scale=8192, train_wall=149, wall=65091
2022-08-03 03:50:39 | INFO | train_inner | epoch 012:   1897 / 3715 loss=6.446, nll_loss=3.481, mask_ins=1.062, word_ins_ml=5.023, word_reposition=0.361, ppl=87.2, wps=9789.4, ups=0.66, wpb=14855.9, bsz=1024, num_updates=42700, lr=0.000171096, gnorm=1.098, clip=0, loss_scale=8192, train_wall=150, wall=65243
2022-08-03 03:53:10 | INFO | train_inner | epoch 012:   1997 / 3715 loss=6.458, nll_loss=3.498, mask_ins=1.059, word_ins_ml=5.039, word_reposition=0.36, ppl=87.91, wps=9776.4, ups=0.66, wpb=14757.4, bsz=1024, num_updates=42800, lr=0.000170896, gnorm=1.071, clip=0, loss_scale=12943, train_wall=149, wall=65394
2022-08-03 03:55:41 | INFO | train_inner | epoch 012:   2097 / 3715 loss=6.467, nll_loss=3.512, mask_ins=1.064, word_ins_ml=5.052, word_reposition=0.351, ppl=88.48, wps=9737.7, ups=0.66, wpb=14651.8, bsz=1024, num_updates=42900, lr=0.000170697, gnorm=1.081, clip=0, loss_scale=16384, train_wall=149, wall=65544
2022-08-03 03:58:12 | INFO | train_inner | epoch 012:   2197 / 3715 loss=6.458, nll_loss=3.496, mask_ins=1.063, word_ins_ml=5.037, word_reposition=0.358, ppl=87.91, wps=9745.7, ups=0.66, wpb=14725.8, bsz=1024, num_updates=43000, lr=0.000170499, gnorm=1.073, clip=0, loss_scale=16384, train_wall=149, wall=65695
2022-08-03 03:59:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 04:00:44 | INFO | train_inner | epoch 012:   2298 / 3715 loss=6.464, nll_loss=3.519, mask_ins=1.059, word_ins_ml=5.058, word_reposition=0.347, ppl=88.27, wps=9560.7, ups=0.66, wpb=14563.1, bsz=1024, num_updates=43100, lr=0.000170301, gnorm=1.086, clip=0, loss_scale=12410, train_wall=150, wall=65847
2022-08-03 04:03:15 | INFO | train_inner | epoch 012:   2398 / 3715 loss=6.46, nll_loss=3.506, mask_ins=1.059, word_ins_ml=5.046, word_reposition=0.355, ppl=88.05, wps=9690.2, ups=0.66, wpb=14574.9, bsz=1024, num_updates=43200, lr=0.000170103, gnorm=1.087, clip=0, loss_scale=8192, train_wall=149, wall=65998
2022-08-03 04:05:43 | INFO | train_inner | epoch 012:   2498 / 3715 loss=6.476, nll_loss=3.517, mask_ins=1.066, word_ins_ml=5.056, word_reposition=0.354, ppl=89.01, wps=9844.6, ups=0.67, wpb=14593.5, bsz=1024, num_updates=43300, lr=0.000169907, gnorm=1.098, clip=0, loss_scale=8192, train_wall=146, wall=66146
2022-08-03 04:08:12 | INFO | train_inner | epoch 012:   2598 / 3715 loss=6.464, nll_loss=3.506, mask_ins=1.061, word_ins_ml=5.045, word_reposition=0.358, ppl=88.25, wps=9861.6, ups=0.67, wpb=14669.2, bsz=1024, num_updates=43400, lr=0.000169711, gnorm=1.071, clip=0, loss_scale=8192, train_wall=147, wall=66295
2022-08-03 04:10:40 | INFO | train_inner | epoch 012:   2698 / 3715 loss=6.448, nll_loss=3.496, mask_ins=1.056, word_ins_ml=5.038, word_reposition=0.354, ppl=87.31, wps=9846.3, ups=0.67, wpb=14651.6, bsz=1024, num_updates=43500, lr=0.000169516, gnorm=1.084, clip=0, loss_scale=8192, train_wall=147, wall=66444
2022-08-03 04:12:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 04:13:11 | INFO | train_inner | epoch 012:   2799 / 3715 loss=6.471, nll_loss=3.519, mask_ins=1.06, word_ins_ml=5.057, word_reposition=0.354, ppl=88.72, wps=9664.4, ups=0.66, wpb=14541.4, bsz=1024, num_updates=43600, lr=0.000169321, gnorm=1.091, clip=0, loss_scale=8516, train_wall=149, wall=66594
2022-08-03 04:14:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 04:15:41 | INFO | train_inner | epoch 012:   2900 / 3715 loss=6.448, nll_loss=3.5, mask_ins=1.058, word_ins_ml=5.04, word_reposition=0.35, ppl=87.32, wps=9729.2, ups=0.67, wpb=14591.4, bsz=1024, num_updates=43700, lr=0.000169128, gnorm=1.076, clip=0, loss_scale=6448, train_wall=148, wall=66744
2022-08-03 04:18:10 | INFO | train_inner | epoch 012:   3000 / 3715 loss=6.45, nll_loss=3.5, mask_ins=1.062, word_ins_ml=5.04, word_reposition=0.349, ppl=87.45, wps=9778.1, ups=0.67, wpb=14618.5, bsz=1024, num_updates=43800, lr=0.000168934, gnorm=1.08, clip=0, loss_scale=4096, train_wall=148, wall=66894
2022-08-03 04:20:39 | INFO | train_inner | epoch 012:   3100 / 3715 loss=6.467, nll_loss=3.513, mask_ins=1.059, word_ins_ml=5.052, word_reposition=0.355, ppl=88.43, wps=9810.2, ups=0.67, wpb=14605.3, bsz=1024, num_updates=43900, lr=0.000168742, gnorm=1.084, clip=0, loss_scale=4096, train_wall=147, wall=67042
2022-08-03 04:23:10 | INFO | train_inner | epoch 012:   3200 / 3715 loss=6.46, nll_loss=3.511, mask_ins=1.061, word_ins_ml=5.05, word_reposition=0.349, ppl=88.04, wps=9722.7, ups=0.67, wpb=14612.7, bsz=1024, num_updates=44000, lr=0.00016855, gnorm=1.081, clip=0, loss_scale=4096, train_wall=148, wall=67193
2022-08-03 04:25:41 | INFO | train_inner | epoch 012:   3300 / 3715 loss=6.43, nll_loss=3.48, mask_ins=1.054, word_ins_ml=5.022, word_reposition=0.354, ppl=86.23, wps=9736.4, ups=0.66, wpb=14726, bsz=1024, num_updates=44100, lr=0.000168359, gnorm=1.08, clip=0, loss_scale=4096, train_wall=149, wall=67344
2022-08-03 04:28:13 | INFO | train_inner | epoch 012:   3400 / 3715 loss=6.438, nll_loss=3.494, mask_ins=1.051, word_ins_ml=5.035, word_reposition=0.351, ppl=86.71, wps=9685.8, ups=0.66, wpb=14700.1, bsz=1024, num_updates=44200, lr=0.000168168, gnorm=1.098, clip=0, loss_scale=5366, train_wall=150, wall=67496
2022-08-03 04:30:43 | INFO | train_inner | epoch 012:   3500 / 3715 loss=6.451, nll_loss=3.5, mask_ins=1.06, word_ins_ml=5.04, word_reposition=0.35, ppl=87.48, wps=9670.2, ups=0.66, wpb=14557.9, bsz=1024, num_updates=44300, lr=0.000167978, gnorm=1.072, clip=0, loss_scale=8192, train_wall=149, wall=67646
2022-08-03 04:33:13 | INFO | train_inner | epoch 012:   3600 / 3715 loss=6.463, nll_loss=3.507, mask_ins=1.063, word_ins_ml=5.047, word_reposition=0.354, ppl=88.23, wps=9793.4, ups=0.67, wpb=14664.5, bsz=1024, num_updates=44400, lr=0.000167789, gnorm=1.09, clip=0, loss_scale=8192, train_wall=148, wall=67796
2022-08-03 04:35:42 | INFO | train_inner | epoch 012:   3700 / 3715 loss=6.431, nll_loss=3.488, mask_ins=1.056, word_ins_ml=5.029, word_reposition=0.347, ppl=86.29, wps=9827.3, ups=0.67, wpb=14627.6, bsz=1023.8, num_updates=44500, lr=0.0001676, gnorm=1.075, clip=0, loss_scale=8192, train_wall=147, wall=67945
2022-08-03 04:36:03 | INFO | train | epoch 012 | loss 6.457 | nll_loss 3.503 | mask_ins 1.06 | word_ins_ml 5.043 | word_reposition 0.354 | ppl 87.86 | wps 9528.5 | ups 0.65 | wpb 14662.2 | bsz 1023.7 | num_updates 44515 | lr 0.000167572 | gnorm 1.081 | clip 0 | loss_scale 8324 | train_wall 5522 | wall 67966
2022-08-03 04:37:53 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.42 | nll_loss 3.376 | mask_ins 1.048 | word_ins_ml 5 | word_reposition 0.373 | ppl 85.62 | wps 24945.8 | wpb 1849.4 | bsz 127.9 | num_updates 44515 | best_loss 6.42
2022-08-03 04:38:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint12.pt (epoch 12 @ 44515 updates, score 6.42) (writing took 11.605512168258429 seconds)
2022-08-03 04:40:13 | INFO | train_inner | epoch 013:     85 / 3715 loss=6.417, nll_loss=3.456, mask_ins=1.057, word_ins_ml=5.002, word_reposition=0.358, ppl=85.42, wps=5381.3, ups=0.37, wpb=14590.3, bsz=1014.7, num_updates=44600, lr=0.000167412, gnorm=1.128, clip=0, loss_scale=8192, train_wall=148, wall=68216
2022-08-03 04:42:45 | INFO | train_inner | epoch 013:    185 / 3715 loss=6.438, nll_loss=3.482, mask_ins=1.058, word_ins_ml=5.024, word_reposition=0.355, ppl=86.68, wps=9588.9, ups=0.66, wpb=14574.4, bsz=1024, num_updates=44700, lr=0.000167225, gnorm=1.077, clip=0, loss_scale=9748, train_wall=150, wall=68368
2022-08-03 04:45:15 | INFO | train_inner | epoch 013:    285 / 3715 loss=6.432, nll_loss=3.481, mask_ins=1.057, word_ins_ml=5.023, word_reposition=0.352, ppl=86.32, wps=9814.3, ups=0.67, wpb=14754.1, bsz=1024, num_updates=44800, lr=0.000167038, gnorm=1.076, clip=0, loss_scale=16384, train_wall=148, wall=68518
2022-08-03 04:47:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 04:47:46 | INFO | train_inner | epoch 013:    386 / 3715 loss=6.42, nll_loss=3.469, mask_ins=1.055, word_ins_ml=5.013, word_reposition=0.352, ppl=85.64, wps=9703.2, ups=0.66, wpb=14602.5, bsz=1024, num_updates=44900, lr=0.000166852, gnorm=1.082, clip=0, loss_scale=13951, train_wall=149, wall=68669
2022-08-03 04:50:15 | INFO | train_inner | epoch 013:    486 / 3715 loss=6.422, nll_loss=3.471, mask_ins=1.056, word_ins_ml=5.015, word_reposition=0.351, ppl=85.74, wps=9797.8, ups=0.67, wpb=14618.9, bsz=1024, num_updates=45000, lr=0.000166667, gnorm=1.089, clip=0, loss_scale=8192, train_wall=147, wall=68818
2022-08-03 04:52:46 | INFO | train_inner | epoch 013:    586 / 3715 loss=6.453, nll_loss=3.498, mask_ins=1.061, word_ins_ml=5.039, word_reposition=0.353, ppl=87.61, wps=9669.2, ups=0.66, wpb=14624.3, bsz=1024, num_updates=45100, lr=0.000166482, gnorm=1.082, clip=0, loss_scale=8192, train_wall=149, wall=68969
2022-08-03 04:55:18 | INFO | train_inner | epoch 013:    686 / 3715 loss=6.41, nll_loss=3.455, mask_ins=1.052, word_ins_ml=5.001, word_reposition=0.357, ppl=85.02, wps=9658.7, ups=0.66, wpb=14700.4, bsz=1024, num_updates=45200, lr=0.000166298, gnorm=1.078, clip=0, loss_scale=8192, train_wall=150, wall=69122
2022-08-03 04:57:50 | INFO | train_inner | epoch 013:    786 / 3715 loss=6.43, nll_loss=3.476, mask_ins=1.056, word_ins_ml=5.019, word_reposition=0.355, ppl=86.24, wps=9646.6, ups=0.66, wpb=14641.8, bsz=1024, num_updates=45300, lr=0.000166114, gnorm=1.08, clip=0, loss_scale=8192, train_wall=150, wall=69273
2022-08-03 05:00:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 05:00:23 | INFO | train_inner | epoch 013:    887 / 3715 loss=6.406, nll_loss=3.456, mask_ins=1.048, word_ins_ml=5.002, word_reposition=0.356, ppl=84.82, wps=9604.1, ups=0.65, wpb=14698.5, bsz=1024, num_updates=45400, lr=0.000165931, gnorm=1.095, clip=0, loss_scale=9165, train_wall=151, wall=69426
2022-08-03 05:02:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 05:02:55 | INFO | train_inner | epoch 013:    988 / 3715 loss=6.416, nll_loss=3.464, mask_ins=1.054, word_ins_ml=5.008, word_reposition=0.354, ppl=85.37, wps=9698, ups=0.66, wpb=14710.9, bsz=1024, num_updates=45500, lr=0.000165748, gnorm=1.078, clip=0, loss_scale=7381, train_wall=150, wall=69578
2022-08-03 05:05:26 | INFO | train_inner | epoch 013:   1088 / 3715 loss=6.43, nll_loss=3.477, mask_ins=1.057, word_ins_ml=5.019, word_reposition=0.354, ppl=86.24, wps=9728.9, ups=0.66, wpb=14720.7, bsz=1024, num_updates=45600, lr=0.000165567, gnorm=1.079, clip=0, loss_scale=4096, train_wall=149, wall=69729
2022-08-03 05:07:56 | INFO | train_inner | epoch 013:   1188 / 3715 loss=6.438, nll_loss=3.48, mask_ins=1.058, word_ins_ml=5.023, word_reposition=0.357, ppl=86.7, wps=9810.4, ups=0.67, wpb=14696.6, bsz=1024, num_updates=45700, lr=0.000165385, gnorm=1.085, clip=0, loss_scale=4096, train_wall=148, wall=69879
2022-08-03 05:10:24 | INFO | train_inner | epoch 013:   1288 / 3715 loss=6.42, nll_loss=3.466, mask_ins=1.054, word_ins_ml=5.01, word_reposition=0.356, ppl=85.6, wps=9918.7, ups=0.67, wpb=14704.5, bsz=1024, num_updates=45800, lr=0.000165205, gnorm=1.094, clip=0, loss_scale=4096, train_wall=146, wall=70027
2022-08-03 05:12:55 | INFO | train_inner | epoch 013:   1388 / 3715 loss=6.415, nll_loss=3.46, mask_ins=1.054, word_ins_ml=5.004, word_reposition=0.357, ppl=85.35, wps=9916.8, ups=0.66, wpb=14921.3, bsz=1024, num_updates=45900, lr=0.000165025, gnorm=1.084, clip=0, loss_scale=4096, train_wall=149, wall=70178
2022-08-03 05:15:24 | INFO | train_inner | epoch 013:   1488 / 3715 loss=6.428, nll_loss=3.473, mask_ins=1.057, word_ins_ml=5.016, word_reposition=0.354, ppl=86.08, wps=9820.5, ups=0.67, wpb=14675.9, bsz=1024, num_updates=46000, lr=0.000164845, gnorm=1.077, clip=0, loss_scale=4424, train_wall=148, wall=70327
2022-08-03 05:17:53 | INFO | train_inner | epoch 013:   1588 / 3715 loss=6.41, nll_loss=3.466, mask_ins=1.051, word_ins_ml=5.01, word_reposition=0.349, ppl=85.03, wps=9851, ups=0.67, wpb=14676.4, bsz=1024, num_updates=46100, lr=0.000164666, gnorm=1.072, clip=0, loss_scale=8192, train_wall=147, wall=70476
2022-08-03 05:20:24 | INFO | train_inner | epoch 013:   1688 / 3715 loss=6.441, nll_loss=3.495, mask_ins=1.056, word_ins_ml=5.036, word_reposition=0.349, ppl=86.87, wps=9641.2, ups=0.66, wpb=14582.7, bsz=1024, num_updates=46200, lr=0.000164488, gnorm=1.086, clip=0, loss_scale=8192, train_wall=149, wall=70628
2022-08-03 05:22:55 | INFO | train_inner | epoch 013:   1788 / 3715 loss=6.45, nll_loss=3.494, mask_ins=1.056, word_ins_ml=5.035, word_reposition=0.359, ppl=87.42, wps=9715.5, ups=0.66, wpb=14674.1, bsz=1024, num_updates=46300, lr=0.00016431, gnorm=1.088, clip=0, loss_scale=8192, train_wall=149, wall=70779
2022-08-03 05:25:23 | INFO | train_inner | epoch 013:   1888 / 3715 loss=6.399, nll_loss=3.462, mask_ins=1.045, word_ins_ml=5.007, word_reposition=0.348, ppl=84.38, wps=9922.9, ups=0.68, wpb=14677.2, bsz=1024, num_updates=46400, lr=0.000164133, gnorm=1.058, clip=0, loss_scale=8192, train_wall=146, wall=70927
2022-08-03 05:27:53 | INFO | train_inner | epoch 013:   1988 / 3715 loss=6.422, nll_loss=3.476, mask_ins=1.054, word_ins_ml=5.018, word_reposition=0.35, ppl=85.73, wps=9826.4, ups=0.67, wpb=14672.9, bsz=1024, num_updates=46500, lr=0.000163956, gnorm=1.086, clip=0, loss_scale=8192, train_wall=147, wall=71076
2022-08-03 05:28:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 05:30:25 | INFO | train_inner | epoch 013:   2089 / 3715 loss=6.451, nll_loss=3.495, mask_ins=1.06, word_ins_ml=5.036, word_reposition=0.355, ppl=87.47, wps=9585.5, ups=0.66, wpb=14611.7, bsz=1024, num_updates=46600, lr=0.00016378, gnorm=1.096, clip=0, loss_scale=9733, train_wall=151, wall=71228
2022-08-03 05:32:56 | INFO | train_inner | epoch 013:   2189 / 3715 loss=6.416, nll_loss=3.477, mask_ins=1.047, word_ins_ml=5.019, word_reposition=0.35, ppl=85.36, wps=9763.9, ups=0.66, wpb=14709.9, bsz=1024, num_updates=46700, lr=0.000163605, gnorm=1.083, clip=0, loss_scale=8192, train_wall=149, wall=71379
2022-08-03 05:35:24 | INFO | train_inner | epoch 013:   2289 / 3715 loss=6.427, nll_loss=3.474, mask_ins=1.058, word_ins_ml=5.017, word_reposition=0.352, ppl=86.03, wps=9867, ups=0.67, wpb=14667.4, bsz=1024, num_updates=46800, lr=0.00016343, gnorm=1.082, clip=0, loss_scale=8192, train_wall=147, wall=71528
2022-08-03 05:37:53 | INFO | train_inner | epoch 013:   2389 / 3715 loss=6.431, nll_loss=3.484, mask_ins=1.056, word_ins_ml=5.026, word_reposition=0.35, ppl=86.3, wps=9881.5, ups=0.68, wpb=14636.5, bsz=1024, num_updates=46900, lr=0.000163256, gnorm=1.093, clip=0, loss_scale=8192, train_wall=146, wall=71676
2022-08-03 05:40:20 | INFO | train_inner | epoch 013:   2489 / 3715 loss=6.425, nll_loss=3.475, mask_ins=1.056, word_ins_ml=5.019, word_reposition=0.351, ppl=85.93, wps=9871.8, ups=0.68, wpb=14554.6, bsz=1024, num_updates=47000, lr=0.000163082, gnorm=1.089, clip=0, loss_scale=8192, train_wall=146, wall=71823
2022-08-03 05:42:47 | INFO | train_inner | epoch 013:   2589 / 3715 loss=6.409, nll_loss=3.456, mask_ins=1.054, word_ins_ml=5.001, word_reposition=0.354, ppl=84.97, wps=10028.1, ups=0.68, wpb=14765.1, bsz=1024, num_updates=47100, lr=0.000162909, gnorm=1.077, clip=0, loss_scale=13599, train_wall=145, wall=71970
2022-08-03 05:45:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 05:45:16 | INFO | train_inner | epoch 013:   2690 / 3715 loss=6.422, nll_loss=3.478, mask_ins=1.05, word_ins_ml=5.021, word_reposition=0.35, ppl=85.73, wps=9880.1, ups=0.67, wpb=14697.9, bsz=1024, num_updates=47200, lr=0.000162736, gnorm=1.077, clip=0, loss_scale=15492, train_wall=147, wall=72119
2022-08-03 05:47:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 05:47:45 | INFO | train_inner | epoch 013:   2791 / 3715 loss=6.433, nll_loss=3.481, mask_ins=1.056, word_ins_ml=5.023, word_reposition=0.354, ppl=86.4, wps=9798.9, ups=0.67, wpb=14590.4, bsz=1024, num_updates=47300, lr=0.000162564, gnorm=1.091, clip=0, loss_scale=8070, train_wall=147, wall=72268
2022-08-03 05:50:13 | INFO | train_inner | epoch 013:   2891 / 3715 loss=6.423, nll_loss=3.479, mask_ins=1.051, word_ins_ml=5.021, word_reposition=0.351, ppl=85.8, wps=9857.7, ups=0.68, wpb=14593.1, bsz=1024, num_updates=47400, lr=0.000162392, gnorm=1.077, clip=0, loss_scale=4096, train_wall=146, wall=72416
2022-08-03 05:52:41 | INFO | train_inner | epoch 013:   2991 / 3715 loss=6.396, nll_loss=3.447, mask_ins=1.049, word_ins_ml=4.992, word_reposition=0.354, ppl=84.21, wps=9843.9, ups=0.67, wpb=14584.4, bsz=1024, num_updates=47500, lr=0.000162221, gnorm=1.089, clip=0, loss_scale=4096, train_wall=146, wall=72564
2022-08-03 05:55:10 | INFO | train_inner | epoch 013:   3091 / 3715 loss=6.434, nll_loss=3.482, mask_ins=1.056, word_ins_ml=5.024, word_reposition=0.354, ppl=86.48, wps=9803.5, ups=0.67, wpb=14623.9, bsz=1024, num_updates=47600, lr=0.000162051, gnorm=1.082, clip=0, loss_scale=4096, train_wall=147, wall=72713
2022-08-03 05:56:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-03 05:57:40 | INFO | train_inner | epoch 013:   3192 / 3715 loss=6.397, nll_loss=3.463, mask_ins=1.045, word_ins_ml=5.007, word_reposition=0.346, ppl=84.3, wps=9809, ups=0.67, wpb=14694.5, bsz=1024, num_updates=47700, lr=0.000161881, gnorm=1.068, clip=0, loss_scale=2960, train_wall=148, wall=72863
2022-08-03 06:00:08 | INFO | train_inner | epoch 013:   3292 / 3715 loss=6.432, nll_loss=3.478, mask_ins=1.056, word_ins_ml=5.02, word_reposition=0.357, ppl=86.36, wps=9896.4, ups=0.67, wpb=14661.9, bsz=1023.8, num_updates=47800, lr=0.000161712, gnorm=1.078, clip=0, loss_scale=2048, train_wall=146, wall=73011
2022-08-03 06:02:36 | INFO | train_inner | epoch 013:   3392 / 3715 loss=6.409, nll_loss=3.464, mask_ins=1.049, word_ins_ml=5.007, word_reposition=0.352, ppl=84.96, wps=9832.4, ups=0.68, wpb=14555.3, bsz=1024, num_updates=47900, lr=0.000161543, gnorm=1.094, clip=0, loss_scale=2048, train_wall=146, wall=73159
2022-08-03 06:05:04 | INFO | train_inner | epoch 013:   3492 / 3715 loss=6.425, nll_loss=3.48, mask_ins=1.048, word_ins_ml=5.022, word_reposition=0.355, ppl=85.93, wps=9874.6, ups=0.67, wpb=14634.3, bsz=1024, num_updates=48000, lr=0.000161374, gnorm=1.086, clip=0, loss_scale=2048, train_wall=146, wall=73308
2022-08-03 06:07:34 | INFO | train_inner | epoch 013:   3592 / 3715 loss=6.434, nll_loss=3.477, mask_ins=1.057, word_ins_ml=5.019, word_reposition=0.358, ppl=86.49, wps=9840.2, ups=0.67, wpb=14718.3, bsz=1024, num_updates=48100, lr=0.000161206, gnorm=1.087, clip=0, loss_scale=2048, train_wall=148, wall=73457
2022-08-03 06:10:04 | INFO | train_inner | epoch 013:   3692 / 3715 loss=6.417, nll_loss=3.472, mask_ins=1.048, word_ins_ml=5.016, word_reposition=0.353, ppl=85.44, wps=9747.8, ups=0.67, wpb=14647.5, bsz=1024, num_updates=48200, lr=0.000161039, gnorm=1.081, clip=0, loss_scale=2949, train_wall=148, wall=73607
2022-08-03 06:10:38 | INFO | train | epoch 013 | loss 6.424 | nll_loss 3.473 | mask_ins 1.054 | word_ins_ml 5.016 | word_reposition 0.353 | ppl 85.84 | wps 9579.9 | ups 0.65 | wpb 14662.2 | bsz 1023.7 | num_updates 48223 | lr 0.000161001 | gnorm 1.084 | clip 0 | loss_scale 7154 | train_wall 5485 | wall 73641
2022-08-03 06:12:28 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.4 | nll_loss 3.362 | mask_ins 1.035 | word_ins_ml 4.983 | word_reposition 0.382 | ppl 84.43 | wps 24930.9 | wpb 1849.4 | bsz 127.9 | num_updates 48223 | best_loss 6.4
2022-08-03 06:12:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint13.pt (epoch 13 @ 48223 updates, score 6.4) (writing took 6.606108218431473 seconds)
2022-08-03 06:14:31 | INFO | train_inner | epoch 014:     77 / 3715 loss=6.414, nll_loss=3.463, mask_ins=1.049, word_ins_ml=5.007, word_reposition=0.358, ppl=85.29, wps=5455.7, ups=0.37, wpb=14575.3, bsz=1014.7, num_updates=48300, lr=0.000160872, gnorm=1.117, clip=0, loss_scale=4096, train_wall=149, wall=73875
2022-08-03 06:17:01 | INFO | train_inner | epoch 014:    177 / 3715 loss=6.426, nll_loss=3.471, mask_ins=1.053, word_ins_ml=5.014, word_reposition=0.359, ppl=85.98, wps=9804.8, ups=0.67, wpb=14712.4, bsz=1024, num_updates=48400, lr=0.000160706, gnorm=1.094, clip=0, loss_scale=4096, train_wall=148, wall=74025
2022-08-03 06:19:31 | INFO | train_inner | epoch 014:    277 / 3715 loss=6.404, nll_loss=3.453, mask_ins=1.049, word_ins_ml=4.999, word_reposition=0.356, ppl=84.66, wps=9757.2, ups=0.67, wpb=14635, bsz=1024, num_updates=48500, lr=0.00016054, gnorm=1.108, clip=0, loss_scale=4096, train_wall=148, wall=74175
2022-08-03 06:22:01 | INFO | train_inner | epoch 014:    377 / 3715 loss=6.396, nll_loss=3.454, mask_ins=1.044, word_ins_ml=4.999, word_reposition=0.353, ppl=84.24, wps=9750.1, ups=0.67, wpb=14619.4, bsz=1024, num_updates=48600, lr=0.000160375, gnorm=1.076, clip=0, loss_scale=4096, train_wall=148, wall=74325
2022-08-03 06:24:30 | INFO | train_inner | epoch 014:    477 / 3715 loss=6.392, nll_loss=3.448, mask_ins=1.047, word_ins_ml=4.994, word_reposition=0.351, ppl=83.99, wps=9830.5, ups=0.67, wpb=14624.2, bsz=1024, num_updates=48700, lr=0.00016021, gnorm=1.091, clip=0, loss_scale=5407, train_wall=147, wall=74473
2022-08-03 06:26:59 | INFO | train_inner | epoch 014:    577 / 3715 loss=6.376, nll_loss=3.431, mask_ins=1.048, word_ins_ml=4.978, word_reposition=0.351, ppl=83.08, wps=9810.2, ups=0.67, wpb=14630.7, bsz=1024, num_updates=48800, lr=0.000160046, gnorm=1.089, clip=0, loss_scale=8192, train_wall=147, wall=74623
2022-08-03 06:27:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 06:29:33 | INFO | train_inner | epoch 014:    678 / 3715 loss=6.401, nll_loss=3.451, mask_ins=1.054, word_ins_ml=4.997, word_reposition=0.351, ppl=84.52, wps=9502.4, ups=0.65, wpb=14581.6, bsz=1024, num_updates=48900, lr=0.000159882, gnorm=1.101, clip=0, loss_scale=4542, train_wall=152, wall=74776
2022-08-03 06:32:03 | INFO | train_inner | epoch 014:    778 / 3715 loss=6.377, nll_loss=3.442, mask_ins=1.045, word_ins_ml=4.989, word_reposition=0.343, ppl=83.1, wps=9716.5, ups=0.67, wpb=14595.8, bsz=1024, num_updates=49000, lr=0.000159719, gnorm=1.079, clip=0, loss_scale=4096, train_wall=148, wall=74926
2022-08-03 06:34:33 | INFO | train_inner | epoch 014:    878 / 3715 loss=6.389, nll_loss=3.443, mask_ins=1.047, word_ins_ml=4.989, word_reposition=0.353, ppl=83.79, wps=9791.6, ups=0.67, wpb=14684.7, bsz=1024, num_updates=49100, lr=0.000159556, gnorm=1.097, clip=0, loss_scale=4096, train_wall=148, wall=75076
2022-08-03 06:37:02 | INFO | train_inner | epoch 014:    978 / 3715 loss=6.384, nll_loss=3.441, mask_ins=1.046, word_ins_ml=4.987, word_reposition=0.351, ppl=83.5, wps=9841.8, ups=0.67, wpb=14713.9, bsz=1024, num_updates=49200, lr=0.000159394, gnorm=1.084, clip=0, loss_scale=4096, train_wall=148, wall=75226
2022-08-03 06:39:32 | INFO | train_inner | epoch 014:   1078 / 3715 loss=6.397, nll_loss=3.449, mask_ins=1.051, word_ins_ml=4.994, word_reposition=0.352, ppl=84.28, wps=9794.1, ups=0.67, wpb=14642.1, bsz=1024, num_updates=49300, lr=0.000159232, gnorm=1.101, clip=0, loss_scale=4096, train_wall=148, wall=75375
2022-08-03 06:42:02 | INFO | train_inner | epoch 014:   1178 / 3715 loss=6.396, nll_loss=3.45, mask_ins=1.05, word_ins_ml=4.996, word_reposition=0.35, ppl=84.2, wps=9858.7, ups=0.67, wpb=14774.7, bsz=1024, num_updates=49400, lr=0.000159071, gnorm=1.097, clip=0, loss_scale=7291, train_wall=148, wall=75525
2022-08-03 06:44:33 | INFO | train_inner | epoch 014:   1278 / 3715 loss=6.381, nll_loss=3.43, mask_ins=1.052, word_ins_ml=4.977, word_reposition=0.352, ppl=83.34, wps=9736.8, ups=0.66, wpb=14730.8, bsz=1024, num_updates=49500, lr=0.00015891, gnorm=1.088, clip=0, loss_scale=8192, train_wall=149, wall=75676
2022-08-03 06:47:02 | INFO | train_inner | epoch 014:   1378 / 3715 loss=6.391, nll_loss=3.452, mask_ins=1.047, word_ins_ml=4.998, word_reposition=0.347, ppl=83.95, wps=9738.7, ups=0.67, wpb=14540.5, bsz=1024, num_updates=49600, lr=0.00015875, gnorm=1.085, clip=0, loss_scale=8192, train_wall=147, wall=75826
2022-08-03 06:47:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 06:49:34 | INFO | train_inner | epoch 014:   1479 / 3715 loss=6.399, nll_loss=3.449, mask_ins=1.049, word_ins_ml=4.995, word_reposition=0.356, ppl=84.41, wps=9727.8, ups=0.66, wpb=14766, bsz=1024, num_updates=49700, lr=0.00015859, gnorm=1.096, clip=0, loss_scale=4785, train_wall=150, wall=75977
2022-08-03 06:52:05 | INFO | train_inner | epoch 014:   1579 / 3715 loss=6.391, nll_loss=3.441, mask_ins=1.051, word_ins_ml=4.987, word_reposition=0.353, ppl=83.93, wps=9702.1, ups=0.66, wpb=14633.8, bsz=1024, num_updates=49800, lr=0.000158431, gnorm=1.097, clip=0, loss_scale=4096, train_wall=149, wall=76128
2022-08-03 06:54:37 | INFO | train_inner | epoch 014:   1679 / 3715 loss=6.401, nll_loss=3.457, mask_ins=1.052, word_ins_ml=5.001, word_reposition=0.348, ppl=84.53, wps=9603.8, ups=0.66, wpb=14623.1, bsz=1024, num_updates=49900, lr=0.000158272, gnorm=1.09, clip=0, loss_scale=4096, train_wall=150, wall=76281
2022-08-03 06:57:08 | INFO | train_inner | epoch 014:   1779 / 3715 loss=6.403, nll_loss=3.454, mask_ins=1.047, word_ins_ml=4.999, word_reposition=0.357, ppl=84.61, wps=9801.9, ups=0.66, wpb=14788, bsz=1024, num_updates=50000, lr=0.000158114, gnorm=1.079, clip=0, loss_scale=4096, train_wall=149, wall=76431
2022-08-03 06:59:38 | INFO | train_inner | epoch 014:   1879 / 3715 loss=6.379, nll_loss=3.447, mask_ins=1.043, word_ins_ml=4.993, word_reposition=0.343, ppl=83.24, wps=9822.9, ups=0.67, wpb=14710.5, bsz=1024, num_updates=50100, lr=0.000157956, gnorm=1.087, clip=0, loss_scale=4096, train_wall=148, wall=76581
2022-08-03 07:01:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 07:02:10 | INFO | train_inner | epoch 014:   1980 / 3715 loss=6.396, nll_loss=3.448, mask_ins=1.051, word_ins_ml=4.994, word_reposition=0.351, ppl=84.2, wps=9617.6, ups=0.66, wpb=14623.7, bsz=1024, num_updates=50200, lr=0.000157799, gnorm=1.088, clip=0, loss_scale=5313, train_wall=150, wall=76733
2022-08-03 07:04:40 | INFO | train_inner | epoch 014:   2080 / 3715 loss=6.407, nll_loss=3.459, mask_ins=1.05, word_ins_ml=5.004, word_reposition=0.353, ppl=84.87, wps=9774.7, ups=0.67, wpb=14649.8, bsz=1024, num_updates=50300, lr=0.000157642, gnorm=1.088, clip=0, loss_scale=4096, train_wall=148, wall=76883
2022-08-03 07:07:10 | INFO | train_inner | epoch 014:   2180 / 3715 loss=6.391, nll_loss=3.441, mask_ins=1.053, word_ins_ml=4.987, word_reposition=0.351, ppl=83.92, wps=9825.5, ups=0.67, wpb=14752, bsz=1024, num_updates=50400, lr=0.000157485, gnorm=1.082, clip=0, loss_scale=4096, train_wall=148, wall=77033
2022-08-03 07:09:41 | INFO | train_inner | epoch 014:   2280 / 3715 loss=6.372, nll_loss=3.433, mask_ins=1.042, word_ins_ml=4.98, word_reposition=0.35, ppl=82.81, wps=9678.8, ups=0.66, wpb=14608.9, bsz=1024, num_updates=50500, lr=0.000157329, gnorm=1.096, clip=0, loss_scale=4096, train_wall=149, wall=77184
2022-08-03 07:12:12 | INFO | train_inner | epoch 014:   2380 / 3715 loss=6.386, nll_loss=3.45, mask_ins=1.044, word_ins_ml=4.995, word_reposition=0.347, ppl=83.61, wps=9734.8, ups=0.66, wpb=14674.9, bsz=1024, num_updates=50600, lr=0.000157174, gnorm=1.083, clip=0, loss_scale=4096, train_wall=149, wall=77335
2022-08-03 07:14:42 | INFO | train_inner | epoch 014:   2480 / 3715 loss=6.391, nll_loss=3.445, mask_ins=1.049, word_ins_ml=4.99, word_reposition=0.352, ppl=83.95, wps=9802, ups=0.67, wpb=14692.5, bsz=1024, num_updates=50700, lr=0.000157019, gnorm=1.077, clip=0, loss_scale=5366, train_wall=148, wall=77485
2022-08-03 07:17:10 | INFO | train_inner | epoch 014:   2580 / 3715 loss=6.39, nll_loss=3.455, mask_ins=1.048, word_ins_ml=4.999, word_reposition=0.343, ppl=83.89, wps=9763.8, ups=0.67, wpb=14502, bsz=1024, num_updates=50800, lr=0.000156864, gnorm=1.089, clip=0, loss_scale=8192, train_wall=147, wall=77633
2022-08-03 07:18:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 07:19:40 | INFO | train_inner | epoch 014:   2681 / 3715 loss=6.38, nll_loss=3.436, mask_ins=1.049, word_ins_ml=4.984, word_reposition=0.347, ppl=83.28, wps=9722.1, ups=0.67, wpb=14569.1, bsz=1024, num_updates=50900, lr=0.00015671, gnorm=1.102, clip=0, loss_scale=5434, train_wall=148, wall=77783
2022-08-03 07:22:10 | INFO | train_inner | epoch 014:   2781 / 3715 loss=6.413, nll_loss=3.468, mask_ins=1.052, word_ins_ml=5.01, word_reposition=0.351, ppl=85.24, wps=9753.6, ups=0.67, wpb=14636.2, bsz=1024, num_updates=51000, lr=0.000156556, gnorm=1.091, clip=0, loss_scale=4096, train_wall=148, wall=77933
2022-08-03 07:24:42 | INFO | train_inner | epoch 014:   2881 / 3715 loss=6.399, nll_loss=3.457, mask_ins=1.046, word_ins_ml=5.002, word_reposition=0.352, ppl=84.41, wps=9674.9, ups=0.66, wpb=14658.4, bsz=1024, num_updates=51100, lr=0.000156403, gnorm=1.097, clip=0, loss_scale=4096, train_wall=150, wall=78085
2022-08-03 07:27:12 | INFO | train_inner | epoch 014:   2981 / 3715 loss=6.397, nll_loss=3.46, mask_ins=1.043, word_ins_ml=5.004, word_reposition=0.349, ppl=84.25, wps=9766.5, ups=0.67, wpb=14682.3, bsz=1023.8, num_updates=51200, lr=0.00015625, gnorm=1.072, clip=0, loss_scale=4096, train_wall=148, wall=78235
2022-08-03 07:29:42 | INFO | train_inner | epoch 014:   3081 / 3715 loss=6.405, nll_loss=3.455, mask_ins=1.054, word_ins_ml=5, word_reposition=0.351, ppl=84.75, wps=9766.5, ups=0.66, wpb=14687.2, bsz=1024, num_updates=51300, lr=0.000156098, gnorm=1.094, clip=0, loss_scale=4096, train_wall=149, wall=78385
2022-08-03 07:32:13 | INFO | train_inner | epoch 014:   3181 / 3715 loss=6.381, nll_loss=3.444, mask_ins=1.041, word_ins_ml=4.99, word_reposition=0.349, ppl=83.31, wps=9732.8, ups=0.67, wpb=14622.3, bsz=1024, num_updates=51400, lr=0.000155946, gnorm=1.076, clip=0, loss_scale=6390, train_wall=148, wall=78536
2022-08-03 07:34:43 | INFO | train_inner | epoch 014:   3281 / 3715 loss=6.41, nll_loss=3.462, mask_ins=1.055, word_ins_ml=5.005, word_reposition=0.349, ppl=85.02, wps=9740.1, ups=0.66, wpb=14671.8, bsz=1024, num_updates=51500, lr=0.000155794, gnorm=1.086, clip=0, loss_scale=8192, train_wall=149, wall=78686
2022-08-03 07:37:14 | INFO | train_inner | epoch 014:   3381 / 3715 loss=6.359, nll_loss=3.428, mask_ins=1.04, word_ins_ml=4.975, word_reposition=0.344, ppl=82.08, wps=9716, ups=0.66, wpb=14639.1, bsz=1024, num_updates=51600, lr=0.000155643, gnorm=1.084, clip=0, loss_scale=8192, train_wall=149, wall=78837
2022-08-03 07:39:44 | INFO | train_inner | epoch 014:   3481 / 3715 loss=6.383, nll_loss=3.431, mask_ins=1.048, word_ins_ml=4.978, word_reposition=0.356, ppl=83.44, wps=9786, ups=0.66, wpb=14735.9, bsz=1024, num_updates=51700, lr=0.000155493, gnorm=1.085, clip=0, loss_scale=8192, train_wall=149, wall=78988
2022-08-03 07:42:16 | INFO | train_inner | epoch 014:   3581 / 3715 loss=6.382, nll_loss=3.435, mask_ins=1.051, word_ins_ml=4.981, word_reposition=0.349, ppl=83.39, wps=9706.3, ups=0.66, wpb=14696.4, bsz=1024, num_updates=51800, lr=0.000155342, gnorm=1.095, clip=0, loss_scale=8192, train_wall=150, wall=79139
2022-08-03 07:44:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 07:44:49 | INFO | train_inner | epoch 014:   3682 / 3715 loss=6.385, nll_loss=3.45, mask_ins=1.044, word_ins_ml=4.995, word_reposition=0.346, ppl=83.6, wps=9649.8, ups=0.65, wpb=14775.7, bsz=1024, num_updates=51900, lr=0.000155193, gnorm=1.08, clip=0, loss_scale=9652, train_wall=151, wall=79292
2022-08-03 07:45:38 | INFO | train | epoch 014 | loss 6.393 | nll_loss 3.448 | mask_ins 1.048 | word_ins_ml 4.994 | word_reposition 0.351 | ppl 84.01 | wps 9543.2 | ups 0.65 | wpb 14661.5 | bsz 1023.7 | num_updates 51933 | lr 0.000155143 | gnorm 1.09 | clip 0 | loss_scale 5483 | train_wall 5514 | wall 79341
2022-08-03 07:47:28 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.391 | nll_loss 3.36 | mask_ins 1.037 | word_ins_ml 4.986 | word_reposition 0.368 | ppl 83.93 | wps 24909.1 | wpb 1849.4 | bsz 127.9 | num_updates 51933 | best_loss 6.391
2022-08-03 07:47:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint14.pt (epoch 14 @ 51933 updates, score 6.391) (writing took 6.475076261907816 seconds)
2022-08-03 07:49:16 | INFO | train_inner | epoch 015:     67 / 3715 loss=6.377, nll_loss=3.434, mask_ins=1.048, word_ins_ml=4.981, word_reposition=0.348, ppl=83.12, wps=5454.9, ups=0.37, wpb=14589.9, bsz=1014.7, num_updates=52000, lr=0.000155043, gnorm=1.116, clip=0, loss_scale=8192, train_wall=149, wall=79560
2022-08-03 07:51:47 | INFO | train_inner | epoch 015:    167 / 3715 loss=6.358, nll_loss=3.418, mask_ins=1.043, word_ins_ml=4.967, word_reposition=0.348, ppl=82.02, wps=9791.7, ups=0.66, wpb=14776.3, bsz=1024, num_updates=52100, lr=0.000154895, gnorm=1.088, clip=0, loss_scale=8192, train_wall=149, wall=79710
2022-08-03 07:54:18 | INFO | train_inner | epoch 015:    267 / 3715 loss=6.381, nll_loss=3.445, mask_ins=1.048, word_ins_ml=4.991, word_reposition=0.343, ppl=83.36, wps=9686.9, ups=0.66, wpb=14623.3, bsz=1024, num_updates=52200, lr=0.000154746, gnorm=1.086, clip=0, loss_scale=8192, train_wall=149, wall=79861
2022-08-03 07:56:51 | INFO | train_inner | epoch 015:    367 / 3715 loss=6.384, nll_loss=3.445, mask_ins=1.044, word_ins_ml=4.991, word_reposition=0.349, ppl=83.52, wps=9632, ups=0.66, wpb=14676.3, bsz=1024, num_updates=52300, lr=0.000154598, gnorm=1.094, clip=0, loss_scale=8192, train_wall=150, wall=80014
2022-08-03 07:59:24 | INFO | train_inner | epoch 015:    467 / 3715 loss=6.347, nll_loss=3.414, mask_ins=1.041, word_ins_ml=4.963, word_reposition=0.343, ppl=81.42, wps=9619.4, ups=0.65, wpb=14758.5, bsz=1024, num_updates=52400, lr=0.000154451, gnorm=1.094, clip=0, loss_scale=9421, train_wall=152, wall=80167
2022-08-03 08:01:55 | INFO | train_inner | epoch 015:    567 / 3715 loss=6.38, nll_loss=3.433, mask_ins=1.049, word_ins_ml=4.98, word_reposition=0.352, ppl=83.31, wps=9684.9, ups=0.66, wpb=14582.4, bsz=1024, num_updates=52500, lr=0.000154303, gnorm=1.101, clip=0, loss_scale=16384, train_wall=149, wall=80318
2022-08-03 08:04:26 | INFO | train_inner | epoch 015:    667 / 3715 loss=6.359, nll_loss=3.424, mask_ins=1.045, word_ins_ml=4.972, word_reposition=0.342, ppl=82.09, wps=9667, ups=0.66, wpb=14650.1, bsz=1024, num_updates=52600, lr=0.000154157, gnorm=1.083, clip=0, loss_scale=16384, train_wall=150, wall=80469
2022-08-03 08:04:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 08:06:57 | INFO | train_inner | epoch 015:    768 / 3715 loss=6.35, nll_loss=3.412, mask_ins=1.044, word_ins_ml=4.961, word_reposition=0.346, ppl=81.58, wps=9847.9, ups=0.66, wpb=14818.7, bsz=1024, num_updates=52700, lr=0.00015401, gnorm=1.092, clip=0, loss_scale=8679, train_wall=149, wall=80620
2022-08-03 08:09:26 | INFO | train_inner | epoch 015:    868 / 3715 loss=6.376, nll_loss=3.432, mask_ins=1.048, word_ins_ml=4.98, word_reposition=0.348, ppl=83.03, wps=9780.3, ups=0.67, wpb=14615.3, bsz=1024, num_updates=52800, lr=0.000153864, gnorm=1.112, clip=0, loss_scale=8192, train_wall=148, wall=80769
2022-08-03 08:11:56 | INFO | train_inner | epoch 015:    968 / 3715 loss=6.362, nll_loss=3.419, mask_ins=1.044, word_ins_ml=4.967, word_reposition=0.351, ppl=82.24, wps=9801.9, ups=0.67, wpb=14710.4, bsz=1024, num_updates=52900, lr=0.000153719, gnorm=1.102, clip=0, loss_scale=8192, train_wall=148, wall=80919
2022-08-03 08:14:25 | INFO | train_inner | epoch 015:   1068 / 3715 loss=6.386, nll_loss=3.454, mask_ins=1.045, word_ins_ml=4.998, word_reposition=0.343, ppl=83.64, wps=9751.5, ups=0.67, wpb=14502.8, bsz=1024, num_updates=53000, lr=0.000153574, gnorm=1.11, clip=0, loss_scale=8192, train_wall=147, wall=81068
2022-08-03 08:16:54 | INFO | train_inner | epoch 015:   1168 / 3715 loss=6.353, nll_loss=3.414, mask_ins=1.043, word_ins_ml=4.963, word_reposition=0.347, ppl=81.74, wps=9901.9, ups=0.67, wpb=14722.7, bsz=1023.8, num_updates=53100, lr=0.000153429, gnorm=1.091, clip=0, loss_scale=8192, train_wall=147, wall=81217
2022-08-03 08:17:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 08:19:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 08:19:27 | INFO | train_inner | epoch 015:   1270 / 3715 loss=6.359, nll_loss=3.417, mask_ins=1.04, word_ins_ml=4.967, word_reposition=0.352, ppl=82.06, wps=9607.3, ups=0.65, wpb=14718, bsz=1024, num_updates=53200, lr=0.000153285, gnorm=1.093, clip=0, loss_scale=8393, train_wall=151, wall=81370
2022-08-03 08:21:57 | INFO | train_inner | epoch 015:   1370 / 3715 loss=6.365, nll_loss=3.416, mask_ins=1.05, word_ins_ml=4.965, word_reposition=0.35, ppl=82.41, wps=9835.3, ups=0.67, wpb=14754.8, bsz=1024, num_updates=53300, lr=0.000153141, gnorm=1.089, clip=0, loss_scale=4096, train_wall=148, wall=81520
2022-08-03 08:24:26 | INFO | train_inner | epoch 015:   1470 / 3715 loss=6.385, nll_loss=3.448, mask_ins=1.044, word_ins_ml=4.994, word_reposition=0.347, ppl=83.56, wps=9798.8, ups=0.67, wpb=14600.2, bsz=1024, num_updates=53400, lr=0.000152998, gnorm=1.101, clip=0, loss_scale=4096, train_wall=147, wall=81669
2022-08-03 08:26:55 | INFO | train_inner | epoch 015:   1570 / 3715 loss=6.368, nll_loss=3.425, mask_ins=1.045, word_ins_ml=4.973, word_reposition=0.35, ppl=82.59, wps=9809.9, ups=0.67, wpb=14614.7, bsz=1024, num_updates=53500, lr=0.000152854, gnorm=1.111, clip=0, loss_scale=4096, train_wall=147, wall=81818
2022-08-03 08:29:24 | INFO | train_inner | epoch 015:   1670 / 3715 loss=6.368, nll_loss=3.436, mask_ins=1.043, word_ins_ml=4.982, word_reposition=0.342, ppl=82.57, wps=9852.7, ups=0.67, wpb=14718.4, bsz=1024, num_updates=53600, lr=0.000152712, gnorm=1.111, clip=0, loss_scale=4096, train_wall=148, wall=81967
2022-08-03 08:31:53 | INFO | train_inner | epoch 015:   1770 / 3715 loss=6.376, nll_loss=3.435, mask_ins=1.045, word_ins_ml=4.982, word_reposition=0.349, ppl=83.03, wps=9857.7, ups=0.67, wpb=14645.5, bsz=1024, num_updates=53700, lr=0.00015257, gnorm=1.103, clip=0, loss_scale=4301, train_wall=147, wall=82116
2022-08-03 08:34:21 | INFO | train_inner | epoch 015:   1870 / 3715 loss=6.361, nll_loss=3.42, mask_ins=1.044, word_ins_ml=4.969, word_reposition=0.348, ppl=82.21, wps=9844.2, ups=0.67, wpb=14613.5, bsz=1024, num_updates=53800, lr=0.000152428, gnorm=1.104, clip=0, loss_scale=8192, train_wall=147, wall=82264
2022-08-03 08:36:49 | INFO | train_inner | epoch 015:   1970 / 3715 loss=6.362, nll_loss=3.425, mask_ins=1.047, word_ins_ml=4.973, word_reposition=0.341, ppl=82.24, wps=9865.9, ups=0.68, wpb=14613.5, bsz=1024, num_updates=53900, lr=0.000152286, gnorm=1.102, clip=0, loss_scale=8192, train_wall=146, wall=82412
2022-08-03 08:39:20 | INFO | train_inner | epoch 015:   2070 / 3715 loss=6.368, nll_loss=3.434, mask_ins=1.04, word_ins_ml=4.981, word_reposition=0.347, ppl=82.57, wps=9741.5, ups=0.66, wpb=14670.5, bsz=1024, num_updates=54000, lr=0.000152145, gnorm=1.101, clip=0, loss_scale=8192, train_wall=149, wall=82563
2022-08-03 08:41:51 | INFO | train_inner | epoch 015:   2170 / 3715 loss=6.387, nll_loss=3.439, mask_ins=1.049, word_ins_ml=4.985, word_reposition=0.352, ppl=83.7, wps=9683.3, ups=0.66, wpb=14626.2, bsz=1024, num_updates=54100, lr=0.000152004, gnorm=1.108, clip=0, loss_scale=8192, train_wall=149, wall=82714
2022-08-03 08:44:23 | INFO | train_inner | epoch 015:   2270 / 3715 loss=6.335, nll_loss=3.401, mask_ins=1.039, word_ins_ml=4.951, word_reposition=0.346, ppl=80.75, wps=9652.3, ups=0.66, wpb=14629.2, bsz=1024, num_updates=54200, lr=0.000151864, gnorm=1.092, clip=0, loss_scale=8192, train_wall=150, wall=82866
2022-08-03 08:44:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 08:46:53 | INFO | train_inner | epoch 015:   2371 / 3715 loss=6.361, nll_loss=3.421, mask_ins=1.042, word_ins_ml=4.969, word_reposition=0.349, ppl=82.17, wps=9780, ups=0.66, wpb=14760, bsz=1024, num_updates=54300, lr=0.000151724, gnorm=1.082, clip=0, loss_scale=8922, train_wall=149, wall=83017
2022-08-03 08:49:26 | INFO | train_inner | epoch 015:   2471 / 3715 loss=6.369, nll_loss=3.423, mask_ins=1.044, word_ins_ml=4.971, word_reposition=0.354, ppl=82.64, wps=9705, ups=0.66, wpb=14763.5, bsz=1024, num_updates=54400, lr=0.000151585, gnorm=1.096, clip=0, loss_scale=8192, train_wall=150, wall=83169
2022-08-03 08:51:57 | INFO | train_inner | epoch 015:   2571 / 3715 loss=6.367, nll_loss=3.429, mask_ins=1.038, word_ins_ml=4.976, word_reposition=0.353, ppl=82.55, wps=9653.4, ups=0.66, wpb=14623.5, bsz=1024, num_updates=54500, lr=0.000151446, gnorm=1.09, clip=0, loss_scale=8192, train_wall=150, wall=83320
2022-08-03 08:54:26 | INFO | train_inner | epoch 015:   2671 / 3715 loss=6.344, nll_loss=3.411, mask_ins=1.042, word_ins_ml=4.96, word_reposition=0.342, ppl=81.21, wps=9755.9, ups=0.67, wpb=14523.1, bsz=1024, num_updates=54600, lr=0.000151307, gnorm=1.088, clip=0, loss_scale=8192, train_wall=147, wall=83469
2022-08-03 08:56:55 | INFO | train_inner | epoch 015:   2771 / 3715 loss=6.385, nll_loss=3.436, mask_ins=1.047, word_ins_ml=4.983, word_reposition=0.355, ppl=83.55, wps=9787.3, ups=0.67, wpb=14600.1, bsz=1024, num_updates=54700, lr=0.000151169, gnorm=1.093, clip=0, loss_scale=8192, train_wall=147, wall=83618
2022-08-03 08:59:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 08:59:25 | INFO | train_inner | epoch 015:   2872 / 3715 loss=6.355, nll_loss=3.419, mask_ins=1.042, word_ins_ml=4.968, word_reposition=0.345, ppl=81.88, wps=9825.2, ups=0.67, wpb=14765.1, bsz=1024, num_updates=54800, lr=0.000151031, gnorm=1.106, clip=0, loss_scale=12815, train_wall=148, wall=83769
2022-08-03 09:01:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 09:01:57 | INFO | train_inner | epoch 015:   2973 / 3715 loss=6.359, nll_loss=3.415, mask_ins=1.045, word_ins_ml=4.964, word_reposition=0.35, ppl=82.1, wps=9727.3, ups=0.66, wpb=14729.9, bsz=1024, num_updates=54900, lr=0.000150893, gnorm=1.106, clip=0, loss_scale=7340, train_wall=150, wall=83920
2022-08-03 09:04:29 | INFO | train_inner | epoch 015:   3073 / 3715 loss=6.354, nll_loss=3.425, mask_ins=1.037, word_ins_ml=4.973, word_reposition=0.343, ppl=81.8, wps=9637.4, ups=0.66, wpb=14622.1, bsz=1024, num_updates=55000, lr=0.000150756, gnorm=1.09, clip=0, loss_scale=4096, train_wall=150, wall=84072
2022-08-03 09:07:00 | INFO | train_inner | epoch 015:   3173 / 3715 loss=6.361, nll_loss=3.43, mask_ins=1.042, word_ins_ml=4.977, word_reposition=0.343, ppl=82.21, wps=9620.3, ups=0.66, wpb=14602.8, bsz=1024, num_updates=55100, lr=0.000150619, gnorm=1.093, clip=0, loss_scale=4096, train_wall=150, wall=84223
2022-08-03 09:09:32 | INFO | train_inner | epoch 015:   3273 / 3715 loss=6.346, nll_loss=3.417, mask_ins=1.037, word_ins_ml=4.966, word_reposition=0.344, ppl=81.35, wps=9669.2, ups=0.66, wpb=14661.4, bsz=1024, num_updates=55200, lr=0.000150482, gnorm=1.088, clip=0, loss_scale=4096, train_wall=150, wall=84375
2022-08-03 09:12:04 | INFO | train_inner | epoch 015:   3373 / 3715 loss=6.319, nll_loss=3.39, mask_ins=1.034, word_ins_ml=4.941, word_reposition=0.344, ppl=79.81, wps=9637.6, ups=0.66, wpb=14610.4, bsz=1024, num_updates=55300, lr=0.000150346, gnorm=1.103, clip=0, loss_scale=4096, train_wall=150, wall=84527
2022-08-03 09:14:34 | INFO | train_inner | epoch 015:   3473 / 3715 loss=6.356, nll_loss=3.417, mask_ins=1.042, word_ins_ml=4.965, word_reposition=0.349, ppl=81.88, wps=9725.3, ups=0.66, wpb=14663.6, bsz=1024, num_updates=55400, lr=0.00015021, gnorm=1.096, clip=0, loss_scale=4465, train_wall=149, wall=84678
2022-08-03 09:17:06 | INFO | train_inner | epoch 015:   3573 / 3715 loss=6.346, nll_loss=3.407, mask_ins=1.039, word_ins_ml=4.957, word_reposition=0.35, ppl=81.34, wps=9699.1, ups=0.66, wpb=14666, bsz=1024, num_updates=55500, lr=0.000150075, gnorm=1.093, clip=0, loss_scale=8192, train_wall=149, wall=84829
2022-08-03 09:19:37 | INFO | train_inner | epoch 015:   3673 / 3715 loss=6.384, nll_loss=3.439, mask_ins=1.044, word_ins_ml=4.985, word_reposition=0.355, ppl=83.51, wps=9692.7, ups=0.66, wpb=14641.2, bsz=1024, num_updates=55600, lr=0.00014994, gnorm=1.094, clip=0, loss_scale=8192, train_wall=149, wall=84980
2022-08-03 09:20:39 | INFO | train | epoch 015 | loss 6.363 | nll_loss 3.425 | mask_ins 1.043 | word_ins_ml 4.973 | word_reposition 0.347 | ppl 82.33 | wps 9539.2 | ups 0.65 | wpb 14662.5 | bsz 1023.7 | num_updates 55642 | lr 0.000149883 | gnorm 1.097 | clip 0 | loss_scale 7720 | train_wall 5516 | wall 85042
2022-08-03 09:22:28 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.394 | nll_loss 3.355 | mask_ins 1.037 | word_ins_ml 4.982 | word_reposition 0.375 | ppl 84.11 | wps 25019.1 | wpb 1849.4 | bsz 127.9 | num_updates 55642 | best_loss 6.391
2022-08-03 09:22:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint15.pt (epoch 15 @ 55642 updates, score 6.394) (writing took 4.258880855515599 seconds)
2022-08-03 09:24:00 | INFO | train_inner | epoch 016:     58 / 3715 loss=6.348, nll_loss=3.418, mask_ins=1.039, word_ins_ml=4.966, word_reposition=0.343, ppl=81.45, wps=5585.4, ups=0.38, wpb=14719.4, bsz=1014.7, num_updates=55700, lr=0.000149805, gnorm=1.106, clip=0, loss_scale=8192, train_wall=148, wall=85243
2022-08-03 09:26:31 | INFO | train_inner | epoch 016:    158 / 3715 loss=6.355, nll_loss=3.414, mask_ins=1.043, word_ins_ml=4.963, word_reposition=0.348, ppl=81.84, wps=9770.6, ups=0.66, wpb=14720.7, bsz=1024, num_updates=55800, lr=0.000149671, gnorm=1.092, clip=0, loss_scale=8192, train_wall=149, wall=85394
2022-08-03 09:29:01 | INFO | train_inner | epoch 016:    258 / 3715 loss=6.315, nll_loss=3.378, mask_ins=1.039, word_ins_ml=4.931, word_reposition=0.346, ppl=79.63, wps=9728.2, ups=0.66, wpb=14653.4, bsz=1024, num_updates=55900, lr=0.000149537, gnorm=1.092, clip=0, loss_scale=8192, train_wall=149, wall=85545
2022-08-03 09:31:31 | INFO | train_inner | epoch 016:    358 / 3715 loss=6.345, nll_loss=3.409, mask_ins=1.038, word_ins_ml=4.959, word_reposition=0.349, ppl=81.3, wps=9873.4, ups=0.67, wpb=14721.7, bsz=1024, num_updates=56000, lr=0.000149404, gnorm=1.097, clip=0, loss_scale=16138, train_wall=147, wall=85694
2022-08-03 09:33:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 09:34:00 | INFO | train_inner | epoch 016:    459 / 3715 loss=6.338, nll_loss=3.402, mask_ins=1.045, word_ins_ml=4.952, word_reposition=0.341, ppl=80.9, wps=9841.4, ups=0.67, wpb=14678.3, bsz=1024, num_updates=56100, lr=0.00014927, gnorm=1.105, clip=0, loss_scale=13383, train_wall=147, wall=85843
2022-08-03 09:36:27 | INFO | train_inner | epoch 016:    559 / 3715 loss=6.348, nll_loss=3.412, mask_ins=1.036, word_ins_ml=4.961, word_reposition=0.35, ppl=81.47, wps=9811.9, ups=0.68, wpb=14469.9, bsz=1024, num_updates=56200, lr=0.000149137, gnorm=1.108, clip=0, loss_scale=8192, train_wall=146, wall=85990
2022-08-03 09:38:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 09:38:57 | INFO | train_inner | epoch 016:    660 / 3715 loss=6.363, nll_loss=3.422, mask_ins=1.045, word_ins_ml=4.97, word_reposition=0.348, ppl=82.3, wps=9753.1, ups=0.67, wpb=14626.1, bsz=1024, num_updates=56300, lr=0.000149005, gnorm=1.097, clip=0, loss_scale=6894, train_wall=148, wall=86140
2022-08-03 09:41:28 | INFO | train_inner | epoch 016:    760 / 3715 loss=6.331, nll_loss=3.403, mask_ins=1.033, word_ins_ml=4.953, word_reposition=0.346, ppl=80.53, wps=9846.2, ups=0.66, wpb=14824.8, bsz=1024, num_updates=56400, lr=0.000148873, gnorm=1.094, clip=0, loss_scale=4096, train_wall=149, wall=86291
2022-08-03 09:43:57 | INFO | train_inner | epoch 016:    860 / 3715 loss=6.337, nll_loss=3.403, mask_ins=1.038, word_ins_ml=4.953, word_reposition=0.346, ppl=80.85, wps=9765.5, ups=0.67, wpb=14619.4, bsz=1024, num_updates=56500, lr=0.000148741, gnorm=1.097, clip=0, loss_scale=4096, train_wall=148, wall=86441
2022-08-03 09:46:26 | INFO | train_inner | epoch 016:    960 / 3715 loss=6.337, nll_loss=3.4, mask_ins=1.04, word_ins_ml=4.951, word_reposition=0.346, ppl=80.83, wps=9812.1, ups=0.67, wpb=14622.6, bsz=1024, num_updates=56600, lr=0.00014861, gnorm=1.085, clip=0, loss_scale=4096, train_wall=147, wall=86590
2022-08-03 09:48:56 | INFO | train_inner | epoch 016:   1060 / 3715 loss=6.36, nll_loss=3.426, mask_ins=1.041, word_ins_ml=4.973, word_reposition=0.346, ppl=82.13, wps=9734.9, ups=0.67, wpb=14596.8, bsz=1024, num_updates=56700, lr=0.000148478, gnorm=1.099, clip=0, loss_scale=4096, train_wall=148, wall=86740
2022-08-03 09:51:27 | INFO | train_inner | epoch 016:   1160 / 3715 loss=6.356, nll_loss=3.427, mask_ins=1.037, word_ins_ml=4.974, word_reposition=0.346, ppl=81.93, wps=9778.5, ups=0.66, wpb=14744.5, bsz=1024, num_updates=56800, lr=0.000148348, gnorm=1.094, clip=0, loss_scale=4915, train_wall=149, wall=86890
2022-08-03 09:53:58 | INFO | train_inner | epoch 016:   1260 / 3715 loss=6.343, nll_loss=3.409, mask_ins=1.041, word_ins_ml=4.959, word_reposition=0.343, ppl=81.16, wps=9732.9, ups=0.66, wpb=14711.1, bsz=1024, num_updates=56900, lr=0.000148217, gnorm=1.096, clip=0, loss_scale=8192, train_wall=149, wall=87041
2022-08-03 09:56:26 | INFO | train_inner | epoch 016:   1360 / 3715 loss=6.325, nll_loss=3.394, mask_ins=1.039, word_ins_ml=4.945, word_reposition=0.341, ppl=80.17, wps=9943.5, ups=0.68, wpb=14690.3, bsz=1024, num_updates=57000, lr=0.000148087, gnorm=1.092, clip=0, loss_scale=8192, train_wall=146, wall=87189
2022-08-03 09:58:54 | INFO | train_inner | epoch 016:   1460 / 3715 loss=6.35, nll_loss=3.414, mask_ins=1.041, word_ins_ml=4.962, word_reposition=0.348, ppl=81.59, wps=9962.4, ups=0.68, wpb=14719.8, bsz=1024, num_updates=57100, lr=0.000147957, gnorm=1.096, clip=0, loss_scale=8192, train_wall=146, wall=87337
2022-08-03 10:01:22 | INFO | train_inner | epoch 016:   1560 / 3715 loss=6.308, nll_loss=3.373, mask_ins=1.04, word_ins_ml=4.927, word_reposition=0.341, ppl=79.22, wps=9919.2, ups=0.68, wpb=14670.7, bsz=1024, num_updates=57200, lr=0.000147828, gnorm=1.102, clip=0, loss_scale=8192, train_wall=146, wall=87485
2022-08-03 10:03:50 | INFO | train_inner | epoch 016:   1660 / 3715 loss=6.336, nll_loss=3.403, mask_ins=1.035, word_ins_ml=4.953, word_reposition=0.348, ppl=80.78, wps=9904.6, ups=0.68, wpb=14658.6, bsz=1024, num_updates=57300, lr=0.000147699, gnorm=1.1, clip=0, loss_scale=8847, train_wall=146, wall=87633
2022-08-03 10:06:18 | INFO | train_inner | epoch 016:   1760 / 3715 loss=6.318, nll_loss=3.39, mask_ins=1.038, word_ins_ml=4.941, word_reposition=0.338, ppl=79.76, wps=9898.7, ups=0.68, wpb=14649.9, bsz=1024, num_updates=57400, lr=0.00014757, gnorm=1.104, clip=0, loss_scale=16384, train_wall=146, wall=87781
2022-08-03 10:07:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 10:08:47 | INFO | train_inner | epoch 016:   1861 / 3715 loss=6.338, nll_loss=3.403, mask_ins=1.039, word_ins_ml=4.953, word_reposition=0.347, ppl=80.91, wps=9787.1, ups=0.67, wpb=14621.2, bsz=1024, num_updates=57500, lr=0.000147442, gnorm=1.102, clip=0, loss_scale=13383, train_wall=148, wall=87930
2022-08-03 10:11:15 | INFO | train_inner | epoch 016:   1961 / 3715 loss=6.332, nll_loss=3.394, mask_ins=1.038, word_ins_ml=4.945, word_reposition=0.35, ppl=80.58, wps=9956.1, ups=0.68, wpb=14697.6, bsz=1024, num_updates=57600, lr=0.000147314, gnorm=1.094, clip=0, loss_scale=8192, train_wall=146, wall=88078
2022-08-03 10:13:43 | INFO | train_inner | epoch 016:   2061 / 3715 loss=6.34, nll_loss=3.406, mask_ins=1.042, word_ins_ml=4.956, word_reposition=0.343, ppl=81.03, wps=9901, ups=0.68, wpb=14657.7, bsz=1024, num_updates=57700, lr=0.000147186, gnorm=1.1, clip=0, loss_scale=8192, train_wall=146, wall=88226
2022-08-03 10:16:11 | INFO | train_inner | epoch 016:   2161 / 3715 loss=6.337, nll_loss=3.401, mask_ins=1.042, word_ins_ml=4.95, word_reposition=0.344, ppl=80.83, wps=9998.7, ups=0.68, wpb=14808.8, bsz=1024, num_updates=57800, lr=0.000147059, gnorm=1.097, clip=0, loss_scale=8192, train_wall=146, wall=88374
2022-08-03 10:18:39 | INFO | train_inner | epoch 016:   2261 / 3715 loss=6.363, nll_loss=3.421, mask_ins=1.047, word_ins_ml=4.969, word_reposition=0.347, ppl=82.33, wps=9878.4, ups=0.68, wpb=14611, bsz=1024, num_updates=57900, lr=0.000146932, gnorm=1.109, clip=0, loss_scale=8192, train_wall=146, wall=88522
2022-08-03 10:20:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 10:21:10 | INFO | train_inner | epoch 016:   2362 / 3715 loss=6.354, nll_loss=3.418, mask_ins=1.039, word_ins_ml=4.967, word_reposition=0.348, ppl=81.81, wps=9675.1, ups=0.66, wpb=14627.7, bsz=1024, num_updates=58000, lr=0.000146805, gnorm=1.104, clip=0, loss_scale=8598, train_wall=149, wall=88673
2022-08-03 10:23:38 | INFO | train_inner | epoch 016:   2462 / 3715 loss=6.332, nll_loss=3.397, mask_ins=1.037, word_ins_ml=4.948, word_reposition=0.348, ppl=80.58, wps=9950.7, ups=0.68, wpb=14710, bsz=1024, num_updates=58100, lr=0.000146679, gnorm=1.098, clip=0, loss_scale=8192, train_wall=146, wall=88821
2022-08-03 10:26:05 | INFO | train_inner | epoch 016:   2562 / 3715 loss=6.344, nll_loss=3.41, mask_ins=1.04, word_ins_ml=4.959, word_reposition=0.344, ppl=81.24, wps=9988.4, ups=0.68, wpb=14736.8, bsz=1024, num_updates=58200, lr=0.000146553, gnorm=1.104, clip=0, loss_scale=8192, train_wall=146, wall=88968
2022-08-03 10:28:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-03 10:28:35 | INFO | train_inner | epoch 016:   2663 / 3715 loss=6.332, nll_loss=3.404, mask_ins=1.033, word_ins_ml=4.954, word_reposition=0.345, ppl=80.57, wps=9827.6, ups=0.67, wpb=14679.5, bsz=1024, num_updates=58300, lr=0.000146427, gnorm=1.102, clip=0, loss_scale=7827, train_wall=148, wall=89118
2022-08-03 10:31:02 | INFO | train_inner | epoch 016:   2763 / 3715 loss=6.347, nll_loss=3.414, mask_ins=1.038, word_ins_ml=4.962, word_reposition=0.347, ppl=81.41, wps=9924.2, ups=0.68, wpb=14647.2, bsz=1024, num_updates=58400, lr=0.000146301, gnorm=1.106, clip=0, loss_scale=4096, train_wall=146, wall=89265
2022-08-03 10:33:31 | INFO | train_inner | epoch 016:   2863 / 3715 loss=6.34, nll_loss=3.405, mask_ins=1.041, word_ins_ml=4.955, word_reposition=0.344, ppl=81.02, wps=9807.5, ups=0.67, wpb=14617.2, bsz=1023.8, num_updates=58500, lr=0.000146176, gnorm=1.102, clip=0, loss_scale=4096, train_wall=147, wall=89415
2022-08-03 10:36:00 | INFO | train_inner | epoch 016:   2963 / 3715 loss=6.344, nll_loss=3.412, mask_ins=1.039, word_ins_ml=4.96, word_reposition=0.345, ppl=81.23, wps=9908.7, ups=0.67, wpb=14767, bsz=1024, num_updates=58600, lr=0.000146052, gnorm=1.096, clip=0, loss_scale=4096, train_wall=147, wall=89564
2022-08-03 10:38:30 | INFO | train_inner | epoch 016:   3063 / 3715 loss=6.35, nll_loss=3.418, mask_ins=1.037, word_ins_ml=4.966, word_reposition=0.347, ppl=81.56, wps=9711, ups=0.67, wpb=14517.5, bsz=1024, num_updates=58700, lr=0.000145927, gnorm=1.115, clip=0, loss_scale=4096, train_wall=148, wall=89713
2022-08-03 10:41:01 | INFO | train_inner | epoch 016:   3163 / 3715 loss=6.331, nll_loss=3.402, mask_ins=1.035, word_ins_ml=4.952, word_reposition=0.344, ppl=80.51, wps=9626, ups=0.66, wpb=14547.3, bsz=1024, num_updates=58800, lr=0.000145803, gnorm=1.09, clip=0, loss_scale=4096, train_wall=149, wall=89864
2022-08-03 10:43:33 | INFO | train_inner | epoch 016:   3263 / 3715 loss=6.329, nll_loss=3.396, mask_ins=1.037, word_ins_ml=4.946, word_reposition=0.345, ppl=80.39, wps=9635.2, ups=0.66, wpb=14615.4, bsz=1024, num_updates=58900, lr=0.000145679, gnorm=1.105, clip=0, loss_scale=8069, train_wall=150, wall=90016
2022-08-03 10:46:04 | INFO | train_inner | epoch 016:   3363 / 3715 loss=6.325, nll_loss=3.393, mask_ins=1.036, word_ins_ml=4.944, word_reposition=0.344, ppl=80.14, wps=9707.4, ups=0.66, wpb=14684.5, bsz=1024, num_updates=59000, lr=0.000145556, gnorm=1.114, clip=0, loss_scale=8192, train_wall=149, wall=90167
2022-08-03 10:48:35 | INFO | train_inner | epoch 016:   3463 / 3715 loss=6.341, nll_loss=3.408, mask_ins=1.038, word_ins_ml=4.957, word_reposition=0.347, ppl=81.09, wps=9741.1, ups=0.66, wpb=14709.4, bsz=1024, num_updates=59100, lr=0.000145432, gnorm=1.1, clip=0, loss_scale=8192, train_wall=149, wall=90318
2022-08-03 10:51:06 | INFO | train_inner | epoch 016:   3563 / 3715 loss=6.336, nll_loss=3.409, mask_ins=1.037, word_ins_ml=4.957, word_reposition=0.342, ppl=80.79, wps=9692.4, ups=0.66, wpb=14625.1, bsz=1024, num_updates=59200, lr=0.00014531, gnorm=1.097, clip=0, loss_scale=8192, train_wall=149, wall=90469
2022-08-03 10:53:38 | INFO | train_inner | epoch 016:   3663 / 3715 loss=6.327, nll_loss=3.396, mask_ins=1.034, word_ins_ml=4.946, word_reposition=0.347, ppl=80.27, wps=9591.3, ups=0.66, wpb=14591.8, bsz=1024, num_updates=59300, lr=0.000145187, gnorm=1.107, clip=0, loss_scale=8192, train_wall=150, wall=90621
2022-08-03 10:54:56 | INFO | train | epoch 016 | loss 6.339 | nll_loss 3.405 | mask_ins 1.039 | word_ins_ml 4.955 | word_reposition 0.345 | ppl 80.96 | wps 9614.7 | ups 0.66 | wpb 14661.8 | bsz 1023.7 | num_updates 59352 | lr 0.000145123 | gnorm 1.1 | clip 0 | loss_scale 7890 | train_wall 5475 | wall 90699
2022-08-03 10:56:47 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.359 | nll_loss 3.328 | mask_ins 1.031 | word_ins_ml 4.951 | word_reposition 0.377 | ppl 82.06 | wps 24834.7 | wpb 1849.4 | bsz 127.9 | num_updates 59352 | best_loss 6.359
2022-08-03 10:56:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_cased_Ggw/checkpoint16.pt (epoch 16 @ 59352 updates, score 6.359) (writing took 6.426911087706685 seconds)
2022-08-03 10:57:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 10:58:07 | INFO | train_inner | epoch 017:     49 / 3715 loss=6.332, nll_loss=3.397, mask_ins=1.037, word_ins_ml=4.948, word_reposition=0.347, ppl=80.59, wps=5423.3, ups=0.37, wpb=14615.4, bsz=1014.7, num_updates=59400, lr=0.000145065, gnorm=1.127, clip=0, loss_scale=14113, train_wall=151, wall=90891
2022-08-03 11:00:39 | INFO | train_inner | epoch 017:    149 / 3715 loss=6.316, nll_loss=3.388, mask_ins=1.033, word_ins_ml=4.939, word_reposition=0.344, ppl=79.68, wps=9713.4, ups=0.66, wpb=14749.6, bsz=1024, num_updates=59500, lr=0.000144943, gnorm=1.098, clip=0, loss_scale=8192, train_wall=150, wall=91042
2022-08-03 11:03:10 | INFO | train_inner | epoch 017:    249 / 3715 loss=6.309, nll_loss=3.38, mask_ins=1.033, word_ins_ml=4.932, word_reposition=0.344, ppl=79.3, wps=9716.3, ups=0.66, wpb=14659.6, bsz=1024, num_updates=59600, lr=0.000144821, gnorm=1.105, clip=0, loss_scale=8192, train_wall=149, wall=91193
2022-08-03 11:05:41 | INFO | train_inner | epoch 017:    349 / 3715 loss=6.303, nll_loss=3.368, mask_ins=1.034, word_ins_ml=4.922, word_reposition=0.347, ppl=78.95, wps=9707.5, ups=0.66, wpb=14628, bsz=1024, num_updates=59700, lr=0.0001447, gnorm=1.095, clip=0, loss_scale=8192, train_wall=149, wall=91344
2022-08-03 11:08:12 | INFO | train_inner | epoch 017:    449 / 3715 loss=6.307, nll_loss=3.375, mask_ins=1.034, word_ins_ml=4.928, word_reposition=0.345, ppl=79.16, wps=9712.1, ups=0.66, wpb=14636.6, bsz=1024, num_updates=59800, lr=0.000144579, gnorm=1.107, clip=0, loss_scale=8192, train_wall=149, wall=91495
2022-08-03 11:10:43 | INFO | train_inner | epoch 017:    549 / 3715 loss=6.302, nll_loss=3.374, mask_ins=1.036, word_ins_ml=4.927, word_reposition=0.339, ppl=78.88, wps=9707.9, ups=0.66, wpb=14689.5, bsz=1024, num_updates=59900, lr=0.000144458, gnorm=1.109, clip=0, loss_scale=8274, train_wall=149, wall=91646
2022-08-03 11:12:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-03 11:13:13 | INFO | train_inner | epoch 017:    650 / 3715 loss=6.292, nll_loss=3.366, mask_ins=1.028, word_ins_ml=4.921, word_reposition=0.343, ppl=78.34, wps=9704.5, ups=0.67, wpb=14581.7, bsz=1024, num_updates=60000, lr=0.000144338, gnorm=1.104, clip=0, loss_scale=12734, train_wall=148, wall=91796
2022-08-03 11:15:42 | INFO | train_inner | epoch 017:    750 / 3715 loss=6.321, nll_loss=3.387, mask_ins=1.038, word_ins_ml=4.939, word_reposition=0.345, ppl=79.97, wps=9808.1, ups=0.67, wpb=14612.8, bsz=1024, num_updates=60100, lr=0.000144217, gnorm=1.096, clip=0, loss_scale=8192, train_wall=147, wall=91945
2022-08-03 11:18:11 | INFO | train_inner | epoch 017:    850 / 3715 loss=6.325, nll_loss=3.39, mask_ins=1.037, word_ins_ml=4.942, word_reposition=0.347, ppl=80.17, wps=9676.7, ups=0.67, wpb=14415.5, bsz=1024, num_updates=60200, lr=0.000144098, gnorm=1.118, clip=0, loss_scale=8192, train_wall=147, wall=92094
2022-08-03 11:20:41 | INFO | train_inner | epoch 017:    950 / 3715 loss=6.307, nll_loss=3.372, mask_ins=1.039, word_ins_ml=4.926, word_reposition=0.342, ppl=79.16, wps=9788.5, ups=0.67, wpb=14715, bsz=1024, num_updates=60300, lr=0.000143978, gnorm=1.115, clip=0, loss_scale=8192, train_wall=148, wall=92245
2022-08-03 11:23:10 | INFO | train_inner | epoch 017:   1050 / 3715 loss=6.314, nll_loss=3.377, mask_ins=1.035, word_ins_ml=4.93, word_reposition=0.348, ppl=79.55, wps=9908.5, ups=0.67, wpb=14739.8, bsz=1024, num_updates=60400, lr=0.000143859, gnorm=1.107, clip=0, loss_scale=8192, train_wall=147, wall=92393
2022-08-03 11:25:38 | INFO | train_inner | epoch 017:   1150 / 3715 loss=6.32, nll_loss=3.389, mask_ins=1.037, word_ins_ml=4.94, word_reposition=0.342, ppl=79.89, wps=9909, ups=0.68, wpb=14648, bsz=1024, num_updates=60500, lr=0.00014374, gnorm=1.106, clip=0, loss_scale=10895, train_wall=146, wall=92541
2022-08-03 11:28:06 | INFO | train_inner | epoch 017:   1250 / 3715 loss=6.321, nll_loss=3.387, mask_ins=1.038, word_ins_ml=4.939, word_reposition=0.344, ppl=79.93, wps=9897.7, ups=0.68, wpb=14611.7, bsz=1024, num_updates=60600, lr=0.000143621, gnorm=1.105, clip=0, loss_scale=16384, train_wall=146, wall=92689
2022-08-03 11:30:35 | INFO | train_inner | epoch 017:   1350 / 3715 loss=6.31, nll_loss=3.372, mask_ins=1.038, word_ins_ml=4.926, word_reposition=0.347, ppl=79.34, wps=9834.8, ups=0.67, wpb=14732.4, bsz=1024, num_updates=60700, lr=0.000143503, gnorm=1.105, clip=0, loss_scale=16384, train_wall=148, wall=92839
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
train.sh: line 39: --no-epoch-checkpoints: command not found
