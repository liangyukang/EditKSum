nohup: ignoring input
2022-07-04 15:22:08 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:12134
2022-07-04 15:22:08 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:12134
2022-07-04 15:22:09 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:12134
2022-07-04 15:22:09 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-04 15:22:09 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:12134
2022-07-04 15:22:09 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-04 15:22:09 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-04 15:22:09 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-04 15:22:09 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-04 15:22:09 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-04 15:22:09 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-04 15:22:09 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-04 15:22:09 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-04 15:22:09 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-04 15:22:09 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-04 15:22:09 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-04 15:22:13 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:12134', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_transformer_transformer', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='/data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_uncased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-uncased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-uncased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-uncased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-uncased')
2022-07-04 15:22:13 | INFO | fairseq.tasks.translation | [source] dictionary: 30522 types
2022-07-04 15:22:13 | INFO | fairseq.tasks.translation | [target] dictionary: 30522 types
start load cached examples valid ...
0it [00:00, ?it/s]2022-07-04 15:22:13 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510/valid.source-target.source
2022-07-04 15:22:13 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510/valid.source-target.target
2022-07-04 15:22:13 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]399it [00:00, 3984.58it/s]380it [00:00, 3796.73it/s]399it [00:00, 3962.77it/s]399it [00:00, 3982.59it/s]760it [00:00, 3525.02it/s]798it [00:00, 3573.74it/s]796it [00:00, 3612.65it/s]798it [00:00, 3584.08it/s]1131it [00:00, 3602.49it/s]1159it [00:00, 3568.90it/s]1171it [00:00, 3670.18it/s]1171it [00:00, 3644.13it/s]1493it [00:00, 3485.26it/s]1518it [00:00, 3426.99it/s]1538it [00:00, 3441.09it/s]1540it [00:00, 3307.15it/s]1875it [00:00, 3598.32it/s]1916it [00:00, 3614.84it/s]1934it [00:00, 3615.66it/s]1936it [00:00, 3520.35it/s]2280it [00:00, 3529.60it/s]2236it [00:00, 3415.29it/s]2299it [00:00, 3449.33it/s]2294it [00:00, 3380.79it/s]2667it [00:00, 3634.66it/s]2625it [00:00, 3561.18it/s]2683it [00:00, 3567.57it/s]2700it [00:00, 3585.79it/s]3028it [00:00, 3702.35it/s]3074it [00:00, 3576.77it/s]3063it [00:00, 3544.48it/s]3074it [00:00, 3524.11it/s]3401it [00:00, 3570.48it/s]3471it [00:00, 3692.01it/s]3459it [00:00, 3667.76it/s]3469it [00:00, 3647.64it/s]3794it [00:01, 3675.82it/s]3847it [00:01, 3710.88it/s]3853it [00:01, 3747.16it/s]3860it [00:01, 3725.17it/s]4164it [00:01, 3595.73it/s]4220it [00:01, 3597.52it/s]4230it [00:01, 3638.90it/s]4235it [00:01, 3547.52it/s]4535it [00:01, 3627.54it/s]4608it [00:01, 3678.57it/s]4613it [00:01, 3694.71it/s]4604it [00:01, 3587.71it/s]4899it [00:01, 3531.50it/s]4978it [00:01, 3546.51it/s]4984it [00:01, 3583.44it/s]4965it [00:01, 3494.84it/s]5266it [00:01, 3569.62it/s]5345it [00:01, 3579.92it/s]5367it [00:01, 3653.82it/s]5327it [00:01, 3528.97it/s]5624it [00:01, 3463.45it/s]5705it [00:01, 3446.12it/s]5734it [00:01, 3497.62it/s]5682it [00:01, 3419.50it/s]5983it [00:01, 3497.89it/s]6065it [00:01, 3487.75it/s]6039it [00:01, 3460.25it/s]6086it [00:01, 3440.48it/s]6343it [00:01, 3525.45it/s]6416it [00:01, 3479.05it/s]6396it [00:01, 3490.47it/s]6432it [00:01, 3320.33it/s]6697it [00:02, 2058.15it/s]6765it [00:02, 1996.21it/s]6746it [00:02, 1985.84it/s]6766it [00:02, 1975.23it/s]7036it [00:02, 2319.27it/s]7106it [00:02, 2267.92it/s]7081it [00:02, 2247.62it/s]7118it [00:02, 2273.76it/s]7344it [00:02, 2467.42it/s]7400it [00:02, 2325.96it/s]7419it [00:02, 2432.11it/s]7374it [00:02, 2372.59it/s]7700it [00:02, 2726.13it/s]7743it [00:02, 2577.59it/s]7748it [00:02, 2628.46it/s]7706it [00:02, 2593.98it/s]8054it [00:02, 2930.65it/s]8083it [00:02, 2778.96it/s]8103it [00:02, 2859.25it/s]8059it [00:02, 2826.37it/s]8381it [00:02, 2926.22it/s]8395it [00:02, 2811.95it/s]8422it [00:02, 2904.95it/s]8376it [00:02, 2839.16it/s]8736it [00:02, 3092.91it/s]8738it [00:02, 2976.68it/s]8777it [00:02, 3079.76it/s]8731it [00:02, 3028.47it/s]9064it [00:02, 3073.59it/s]9055it [00:02, 2957.41it/s]9103it [00:02, 3002.88it/s]9053it [00:02, 3011.69it/s]9403it [00:02, 3160.82it/s]9405it [00:02, 3105.84it/s]9456it [00:02, 3148.73it/s]9388it [00:02, 3104.84it/s]9760it [00:03, 3277.00it/s]9745it [00:03, 3187.69it/s]9814it [00:03, 3268.46it/s]9744it [00:03, 3234.01it/s]10095it [00:03, 3199.54it/s]10072it [00:03, 3116.90it/s]10076it [00:03, 3166.35it/s]10149it [00:03, 3128.01it/s]10435it [00:03, 3256.04it/s]10413it [00:03, 3198.92it/s]10429it [00:03, 3269.27it/s]10504it [00:03, 3244.40it/s]10765it [00:03, 3158.55it/s]10738it [00:03, 3112.56it/s]10761it [00:03, 3147.10it/s]10834it [00:03, 3174.97it/s]11102it [00:03, 3218.42it/s]11088it [00:03, 3222.58it/s]11112it [00:03, 3248.55it/s]11186it [00:03, 3272.20it/s]11456it [00:03, 3311.13it/s]11429it [00:03, 3276.48it/s]11446it [00:03, 3272.72it/s]11519it [00:03, 3287.05it/s]11790it [00:03, 3183.66it/s]11759it [00:03, 3171.03it/s]11850it [00:03, 3202.38it/s]11776it [00:03, 3177.89it/s]12146it [00:03, 3290.39it/s]12113it [00:03, 3276.70it/s]12207it [00:03, 3308.22it/s]12132it [00:03, 3285.26it/s]12478it [00:03, 3206.48it/s]12443it [00:03, 3149.86it/s]12540it [00:03, 3221.52it/s]12463it [00:03, 3200.35it/s]12819it [00:03, 3264.65it/s]12794it [00:04, 3251.39it/s]12875it [00:04, 3256.04it/s]12817it [00:04, 3297.86it/s]13171it [00:04, 3338.40it/s]13144it [00:04, 3322.40it/s]13169it [00:04, 3361.66it/s]13208it [00:04, 3177.63it/s]13368it [00:04, 3206.82it/s]
13368it [00:04, 3189.32it/s]
2022-07-04 15:22:18 | INFO | root | success load 13368 data
2022-07-04 15:22:18 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-uncased' is a path or url to a directory containing tokenizer files.
2022-07-04 15:22:18 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-uncased/added_tokens.json. We won't load it.
2022-07-04 15:22:18 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-uncased/special_tokens_map.json. We won't load it.
2022-07-04 15:22:18 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-uncased/vocab.txt
2022-07-04 15:22:18 | INFO | transformer.tokenization_utils | loading file None
2022-07-04 15:22:18 | INFO | transformer.tokenization_utils | loading file None
2022-07-04 15:22:18 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-uncased/tokenizer_config.json
13368it [00:04, 3177.56it/s]
13368it [00:04, 3165.33it/s]
2022-07-04 15:22:18 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-uncased/config.json
2022-07-04 15:22:18 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-07-04 15:22:18 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-uncased/pytorch_model.bin
2022-07-04 15:22:22 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-04 15:22:22 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-04 15:22:22 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-uncased/config.json
2022-07-04 15:22:22 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-07-04 15:22:22 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-uncased-decoder/pytorch_model.bin
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-04 15:22:24 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-04 15:22:24 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-07-04 15:22:24 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=30522, bias=False)
  )
)
2022-07-04 15:22:24 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-04 15:22:24 | INFO | fairseq_cli.train | num. model params: 384271619 (num. trained: 142456835)
2022-07-04 15:22:24 | INFO | fairseq_cli.train | num. Encoder model params: 147644675 (Encoder num. trained: 38162435)
2022-07-04 15:22:24 | INFO | fairseq_cli.train | num. Decoder model params: 236626944 (Decoder num. trained: 104294400)

Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]2022-07-04 15:22:24 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-04 15:22:24 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-04 15:22:24 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_transformer_transformer/checkpoint_last.pt
2022-07-04 15:22:24 | INFO | fairseq.trainer | loading train data for epoch 1

Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]374it [00:00, 3732.84it/s]2022-07-04 15:22:24 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510/train.source-target.source
373it [00:00, 3726.77it/s]2022-07-04 15:22:24 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510/train.source-target.target
2022-07-04 15:22:24 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]748it [00:00, 3521.50it/s]367it [00:00, 3667.76it/s]746it [00:00, 3509.66it/s]1128it [00:00, 3643.32it/s]1117it [00:00, 3597.52it/s]734it [00:00, 3498.88it/s]1494it [00:00, 3478.98it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]1112it [00:00, 3619.88it/s]1478it [00:00, 3380.04it/s]1885it [00:00, 3627.59it/s]360it [00:00, 3598.42it/s]1857it [00:00, 3519.17it/s]1475it [00:00, 3431.84it/s]2250it [00:00, 3505.28it/s]720it [00:00, 3423.48it/s]1862it [00:00, 3582.12it/s]2211it [00:00, 3405.78it/s]2619it [00:00, 3561.20it/s]1097it [00:00, 3576.07it/s]2585it [00:00, 3508.48it/s]2222it [00:00, 3456.02it/s]3005it [00:00, 3652.08it/s]1456it [00:00, 3397.60it/s]2950it [00:00, 3550.01it/s]2599it [00:00, 3551.44it/s]3372it [00:00, 3527.45it/s]1835it [00:00, 3529.95it/s]2980it [00:00, 3629.76it/s]3307it [00:00, 3431.02it/s]3749it [00:01, 3597.86it/s]2190it [00:00, 3402.40it/s]3677it [00:01, 3509.97it/s]3345it [00:00, 3496.17it/s]4111it [00:01, 3463.43it/s]2569it [00:00, 3520.34it/s]3717it [00:01, 3561.98it/s]4030it [00:01, 3392.73it/s]4493it [00:01, 3565.38it/s]2950it [00:00, 3599.92it/s]4408it [00:01, 3504.31it/s]4075it [00:01, 3439.39it/s]4852it [00:01, 3467.32it/s]3312it [00:00, 3432.57it/s]4458it [00:01, 3551.50it/s]4760it [00:01, 3369.14it/s]5231it [00:01, 3560.35it/s]3690it [00:01, 3534.38it/s]5126it [00:01, 3451.96it/s]4815it [00:01, 3425.35it/s]5589it [00:01, 3465.64it/s]4046it [00:01, 3426.48it/s]5500it [00:01, 3535.00it/s]5171it [00:01, 3460.96it/s]5955it [00:01, 3519.82it/s]4417it [00:01, 3506.85it/s]5856it [00:01, 3416.56it/s]5539it [00:01, 3374.01it/s]6334it [00:01, 3596.68it/s]4770it [00:01, 3394.75it/s]6221it [00:01, 3482.77it/s]5902it [00:01, 3444.75it/s]6695it [00:01, 3500.65it/s]5137it [00:01, 3471.86it/s]6263it [00:01, 3490.35it/s]6571it [00:01, 3388.13it/s]7074it [00:01, 3582.93it/s]5517it [00:01, 3566.25it/s]6940it [00:02, 3467.69it/s]6614it [00:01, 3395.50it/s]5876it [00:01, 3439.10it/s]6971it [00:02, 3443.76it/s]6240it [00:01, 3489.96it/s]6591it [00:01, 3372.43it/s]6955it [00:02, 3447.51it/s]7434it [00:02, 1344.64it/s]7289it [00:02, 3376.72it/s]7817it [00:02, 1680.96it/s]7628it [00:02, 1326.56it/s]8142it [00:02, 1934.84it/s]7990it [00:02, 1644.87it/s]8526it [00:02, 2292.86it/s]8311it [00:02, 1903.49it/s]8909it [00:03, 2618.26it/s]7317it [00:02, 1017.42it/s]8689it [00:03, 2261.88it/s]9256it [00:03, 2748.45it/s]7671it [00:03, 1293.18it/s]9010it [00:03, 2459.94it/s]9637it [00:03, 3006.17it/s]8045it [00:03, 1623.56it/s]9373it [00:03, 2733.20it/s]7302it [00:02, 1048.52it/s]9987it [00:03, 3078.05it/s]8356it [00:03, 1851.40it/s]9753it [00:03, 2999.57it/s]7678it [00:02, 1351.46it/s]10369it [00:03, 3272.65it/s]8730it [00:03, 2203.91it/s]10100it [00:03, 3061.75it/s]8059it [00:03, 1689.65it/s]10724it [00:03, 3240.77it/s]9057it [00:03, 2369.43it/s]10484it [00:03, 3271.06it/s]8377it [00:03, 1932.55it/s]11105it [00:03, 3395.93it/s]9434it [00:03, 2687.13it/s]10837it [00:03, 3247.23it/s]8748it [00:03, 2270.08it/s]11488it [00:03, 3517.57it/s]9808it [00:03, 2801.68it/s]11219it [00:03, 3404.32it/s]9082it [00:03, 2471.21it/s]11851it [00:03, 3432.60it/s]10183it [00:03, 3038.30it/s]11574it [00:03, 3308.79it/s]9465it [00:03, 2785.92it/s]12237it [00:04, 3551.94it/s]10563it [00:03, 3237.46it/s]11956it [00:03, 3451.52it/s]9810it [00:03, 2879.31it/s]12599it [00:04, 3453.05it/s]10916it [00:03, 3201.93it/s]12329it [00:04, 3403.07it/s]10196it [00:03, 3131.48it/s]12987it [00:04, 3572.64it/s]11292it [00:04, 3353.08it/s]12719it [00:04, 3542.51it/s]10581it [00:03, 3322.87it/s]13349it [00:04, 3522.46it/s]11644it [00:04, 3264.28it/s]13112it [00:04, 3653.43it/s]10942it [00:03, 3270.44it/s]13741it [00:04, 3636.28it/s]12004it [00:04, 3355.67it/s]13482it [00:04, 3562.17it/s]11331it [00:04, 3441.15it/s]14108it [00:04, 3555.04it/s]12349it [00:04, 3321.05it/s]13856it [00:04, 3612.96it/s]11691it [00:04, 3371.39it/s]14484it [00:04, 3613.84it/s]12715it [00:04, 3416.99it/s]14220it [00:04, 3522.46it/s]12078it [00:04, 3509.62it/s]14850it [00:04, 3533.72it/s]13103it [00:04, 3550.24it/s]14610it [00:04, 3630.38it/s]12438it [00:04, 3421.52it/s]15240it [00:04, 3639.37it/s]13462it [00:04, 3421.53it/s]14975it [00:04, 3541.28it/s]12829it [00:04, 3559.28it/s]15619it [00:04, 3683.35it/s]13853it [00:04, 3559.14it/s]15359it [00:04, 3625.69it/s]13191it [00:04, 3504.82it/s]15989it [00:05, 3547.16it/s]14212it [00:04, 3411.93it/s]15723it [00:05, 3499.70it/s]13577it [00:04, 3606.51it/s]16375it [00:05, 3634.91it/s]14593it [00:05, 3523.74it/s]16081it [00:05, 3522.52it/s]13973it [00:04, 3709.10it/s]14949it [00:05, 3447.78it/s]16473it [00:05, 3636.03it/s]14347it [00:04, 3600.72it/s]15325it [00:05, 3535.57it/s]14737it [00:04, 3685.42it/s]15681it [00:05, 3539.09it/s]15108it [00:05, 3564.34it/s]16037it [00:05, 3435.30it/s]15489it [00:05, 3633.68it/s]16405it [00:05, 3503.89it/s]15855it [00:05, 3521.50it/s]16215it [00:05, 3541.89it/s]16571it [00:05, 3450.03it/s]16740it [00:06, 1111.18it/s]17135it [00:06, 1431.15it/s]17447it [00:06, 1664.85it/s]16838it [00:06, 1055.21it/s]17836it [00:06, 2032.34it/s]17226it [00:06, 1360.95it/s]18221it [00:06, 2378.71it/s]17535it [00:06, 1592.49it/s]18569it [00:06, 2590.27it/s]17924it [00:06, 1961.16it/s]18951it [00:06, 2874.80it/s]18279it [00:06, 2225.50it/s]19306it [00:06, 2971.32it/s]18665it [00:06, 2564.41it/s]16757it [00:06, 896.57it/s] 19687it [00:06, 3185.89it/s]19044it [00:06, 2844.61it/s]16918it [00:06, 1027.26it/s]17148it [00:06, 1186.55it/s]20043it [00:06, 3221.26it/s]19401it [00:06, 2943.40it/s]17305it [00:06, 1336.22it/s]17457it [00:06, 1416.57it/s]20424it [00:07, 3380.36it/s]19763it [00:06, 3114.61it/s]17631it [00:06, 1594.47it/s]17845it [00:06, 1778.89it/s]20799it [00:07, 3346.49it/s]20114it [00:07, 3144.93it/s]18021it [00:06, 1965.92it/s]18231it [00:07, 2139.53it/s]21181it [00:07, 3478.06it/s]20490it [00:07, 3310.61it/s]18351it [00:06, 2198.07it/s]18575it [00:07, 2362.80it/s]21565it [00:07, 3578.60it/s]20842it [00:07, 3275.30it/s]18741it [00:06, 2553.40it/s]18956it [00:07, 2678.95it/s]21931it [00:07, 3490.33it/s]21219it [00:07, 3412.90it/s]19119it [00:07, 2734.34it/s]19306it [00:07, 2813.09it/s]22316it [00:07, 3591.99it/s]21600it [00:07, 3525.69it/s]19493it [00:07, 2976.14it/s]19685it [00:07, 3056.82it/s]22680it [00:07, 3509.24it/s]21962it [00:07, 3393.48it/s]19878it [00:07, 3199.69it/s]20038it [00:07, 3125.76it/s]23060it [00:07, 3590.72it/s]22341it [00:07, 3505.12it/s]20238it [00:07, 3219.12it/s]20401it [00:07, 3260.26it/s]23422it [00:07, 3483.99it/s]22697it [00:07, 3435.06it/s]20617it [00:07, 3372.14it/s]20783it [00:07, 3413.87it/s]23801it [00:08, 3571.55it/s]23076it [00:07, 3536.42it/s]20976it [00:07, 3324.02it/s]21143it [00:07, 3345.13it/s]24161it [00:08, 3445.81it/s]23433it [00:08, 3418.25it/s]21350it [00:07, 3437.39it/s]21522it [00:07, 3468.44it/s]24544it [00:08, 3554.63it/s]23809it [00:08, 3513.14it/s]21706it [00:07, 3385.15it/s]21879it [00:08, 3385.23it/s]24922it [00:08, 3618.39it/s]22084it [00:07, 3494.96it/s]24163it [00:08, 3325.33it/s]22258it [00:08, 3499.15it/s]25286it [00:08, 3501.67it/s]22461it [00:07, 3573.40it/s]24542it [00:08, 3455.13it/s]22614it [00:08, 3461.94it/s]25671it [00:08, 3600.18it/s]24917it [00:08, 3537.18it/s]22823it [00:08, 3482.55it/s]22982it [00:08, 3524.68it/s]26033it [00:08, 3451.91it/s]23202it [00:08, 3568.38it/s]25274it [00:08, 3430.31it/s]23338it [00:08, 3388.47it/s]26411it [00:08, 3543.86it/s]25654it [00:08, 3535.16it/s]23562it [00:08, 3409.89it/s]23702it [00:08, 3459.74it/s]26768it [00:08, 3441.05it/s]23943it [00:08, 3520.98it/s]26010it [00:08, 3377.69it/s]24084it [00:08, 3563.29it/s]27158it [00:08, 3570.07it/s]26387it [00:08, 3486.29it/s]24298it [00:08, 3433.27it/s]24443it [00:08, 3444.15it/s]27519it [00:09, 3481.19it/s]24673it [00:08, 3523.67it/s]26739it [00:09, 3381.76it/s]24809it [00:08, 3505.73it/s]27889it [00:09, 3542.30it/s]27118it [00:09, 3496.94it/s]25028it [00:08, 3353.34it/s]25162it [00:09, 3350.38it/s]28269it [00:09, 3617.03it/s]27502it [00:09, 3593.45it/s]25407it [00:08, 3474.22it/s]25529it [00:09, 3439.05it/s]27864it [00:09, 3483.28it/s]25789it [00:08, 3571.57it/s]25876it [00:09, 3352.74it/s]28224it [00:09, 3514.71it/s]26149it [00:09, 3397.58it/s]26252it [00:09, 3466.90it/s]26522it [00:09, 3490.48it/s]26614it [00:09, 3510.78it/s]26874it [00:09, 3371.78it/s]26967it [00:09, 3386.62it/s]27239it [00:09, 3447.23it/s]27343it [00:09, 3492.00it/s]27586it [00:09, 3377.04it/s]27694it [00:09, 3384.23it/s]27962it [00:09, 3486.38it/s]28078it [00:09, 3507.12it/s]28335it [00:09, 3556.35it/s]28632it [00:10, 847.45it/s] 29002it [00:10, 1102.09it/s]29330it [00:10, 1348.40it/s]28577it [00:10, 810.13it/s] 29703it [00:10, 1679.26it/s]28949it [00:10, 1064.23it/s]30083it [00:10, 2030.72it/s]29269it [00:10, 1299.77it/s]30425it [00:10, 2269.21it/s]29640it [00:10, 1628.79it/s]30802it [00:11, 2587.89it/s]30013it [00:11, 1971.40it/s]31150it [00:11, 2749.59it/s]30349it [00:11, 2211.68it/s]31528it [00:11, 3002.72it/s]30707it [00:11, 2499.66it/s]31880it [00:11, 3071.39it/s]31046it [00:11, 2659.38it/s]32266it [00:11, 3281.72it/s]28692it [00:11, 749.39it/s] 31420it [00:11, 2922.93it/s]28431it [00:11, 690.92it/s] 32629it [00:11, 3235.82it/s]29053it [00:11, 981.16it/s]28791it [00:11, 910.03it/s]31789it [00:11, 3008.54it/s]33002it [00:11, 3369.96it/s]29358it [00:11, 1195.56it/s]29169it [00:11, 1188.85it/s]32138it [00:11, 3135.13it/s]33378it [00:11, 3477.34it/s]29731it [00:11, 1522.55it/s]29483it [00:11, 1426.43it/s]32508it [00:11, 3287.67it/s]30104it [00:11, 1865.74it/s]33738it [00:11, 3413.72it/s]29813it [00:11, 1703.93it/s]32858it [00:11, 3240.16it/s]34118it [00:12, 3521.37it/s]30438it [00:11, 2090.43it/s]30132it [00:11, 1961.63it/s]33230it [00:12, 3374.11it/s]30808it [00:11, 2416.25it/s]34477it [00:12, 3455.11it/s]30511it [00:11, 2325.85it/s]33579it [00:12, 3307.62it/s]34852it [00:12, 3538.21it/s]31146it [00:11, 2582.35it/s]30882it [00:12, 2632.81it/s]33963it [00:12, 3456.01it/s]31508it [00:11, 2830.66it/s]35210it [00:12, 3448.24it/s]31229it [00:12, 2783.37it/s]34315it [00:12, 3366.99it/s]35585it [00:12, 3533.10it/s]31848it [00:11, 2907.29it/s]31578it [00:12, 2958.65it/s]34687it [00:12, 3467.32it/s]35956it [00:12, 3582.70it/s]32227it [00:12, 3136.78it/s]31920it [00:12, 3033.70it/s]35055it [00:12, 3528.30it/s]32589it [00:12, 3253.54it/s]36317it [00:12, 3469.09it/s]32303it [00:12, 3247.45it/s]35411it [00:12, 3419.56it/s]36690it [00:12, 3542.30it/s]32938it [00:12, 3205.33it/s]32653it [00:12, 3233.19it/s]35769it [00:12, 3464.33it/s]33310it [00:12, 3348.37it/s]37046it [00:12, 3447.13it/s]32994it [00:12, 3249.82it/s]36118it [00:12, 3374.47it/s]37422it [00:12, 3535.77it/s]33658it [00:12, 3279.32it/s]33372it [00:12, 3398.98it/s]36486it [00:12, 3462.03it/s]34026it [00:12, 3390.49it/s]37777it [00:13, 3440.25it/s]33722it [00:12, 3355.10it/s]36834it [00:13, 3378.52it/s]38160it [00:13, 3551.01it/s]34372it [00:12, 3320.21it/s]34105it [00:13, 3490.62it/s]37209it [00:13, 3484.08it/s]34739it [00:12, 3417.74it/s]38517it [00:13, 3423.40it/s]34460it [00:13, 3428.26it/s]37579it [00:13, 3544.84it/s]35093it [00:12, 3451.62it/s]38900it [00:13, 3539.12it/s]34819it [00:13, 3473.86it/s]37935it [00:13, 3392.25it/s]39276it [00:13, 3602.98it/s]35442it [00:13, 3353.54it/s]35170it [00:13, 3398.40it/s]38315it [00:13, 3506.47it/s]35815it [00:13, 3460.82it/s]39638it [00:13, 3489.91it/s]35550it [00:13, 3513.17it/s]38668it [00:13, 3359.29it/s]40018it [00:13, 3577.86it/s]36164it [00:13, 3351.78it/s]35904it [00:13, 3509.35it/s]39048it [00:13, 3483.25it/s]36517it [00:13, 3400.90it/s]40378it [00:13, 3485.68it/s]36257it [00:13, 3413.90it/s]39399it [00:13, 3410.23it/s]40750it [00:13, 3551.61it/s]36859it [00:13, 3308.66it/s]36629it [00:13, 3500.37it/s]39773it [00:13, 3504.24it/s]37231it [00:13, 3425.81it/s]41107it [00:14, 3461.33it/s]36981it [00:13, 3410.56it/s]40133it [00:14, 3529.40it/s]37586it [00:13, 3460.80it/s]41495it [00:14, 3580.40it/s]37344it [00:13, 3473.67it/s]40488it [00:14, 3428.56it/s]37934it [00:13, 3345.19it/s]41870it [00:14, 3478.60it/s]37693it [00:14, 3355.74it/s]40863it [00:14, 3521.19it/s]38309it [00:13, 3461.65it/s]42256it [00:14, 3584.61it/s]38076it [00:14, 3491.04it/s]41217it [00:14, 3430.54it/s]42636it [00:14, 3644.28it/s]38657it [00:13, 3266.83it/s]38438it [00:14, 3517.88it/s]41590it [00:14, 3513.53it/s]43002it [00:14, 3516.76it/s]39036it [00:14, 3413.59it/s]38791it [00:14, 3400.69it/s]41943it [00:14, 3422.55it/s]43378it [00:14, 3585.21it/s]39381it [00:14, 3338.33it/s]39167it [00:14, 3502.20it/s]42303it [00:14, 3472.95it/s]39754it [00:14, 3448.55it/s]39519it [00:14, 3403.87it/s]42683it [00:14, 3565.95it/s]40101it [00:14, 3430.94it/s]39885it [00:14, 3475.93it/s]43041it [00:14, 3448.03it/s]40446it [00:14, 3336.74it/s]40234it [00:14, 3378.39it/s]43413it [00:14, 3526.06it/s]40816it [00:14, 3441.04it/s]40607it [00:14, 3477.91it/s]41162it [00:14, 3347.59it/s]40974it [00:15, 3533.52it/s]41527it [00:14, 3432.12it/s]41329it [00:15, 3425.09it/s]41872it [00:14, 3335.98it/s]41711it [00:15, 3536.48it/s]42253it [00:15, 3471.07it/s]42066it [00:15, 3376.02it/s]42635it [00:15, 3570.35it/s]42440it [00:15, 3477.02it/s]42994it [00:15, 3392.64it/s]42790it [00:15, 3423.16it/s]43365it [00:15, 3481.66it/s]43154it [00:15, 3483.93it/s]43516it [00:15, 3521.96it/s]43738it [00:15, 788.94it/s] 44123it [00:16, 1044.97it/s]44459it [00:16, 1289.72it/s]44845it [00:16, 1629.09it/s]45218it [00:16, 1961.92it/s]45562it [00:16, 2214.45it/s]45939it [00:16, 2536.77it/s]43767it [00:16, 642.98it/s] 46290it [00:16, 2706.89it/s]44140it [00:16, 860.91it/s]46668it [00:16, 2966.75it/s]44459it [00:16, 1073.96it/s]47021it [00:16, 3044.24it/s]44839it [00:16, 1388.95it/s]47407it [00:17, 3258.35it/s]45218it [00:16, 1728.59it/s]47788it [00:17, 3407.88it/s]45558it [00:17, 1992.04it/s]48152it [00:17, 3374.74it/s]45929it [00:17, 2321.96it/s]48532it [00:17, 3493.00it/s]46274it [00:17, 2523.84it/s]48894it [00:17, 3422.76it/s]46645it [00:17, 2799.57it/s]49277it [00:17, 3538.17it/s]46993it [00:17, 2909.26it/s]49638it [00:17, 3476.95it/s]47378it [00:17, 3150.96it/s]50015it [00:17, 3559.49it/s]43716it [00:17, 555.05it/s] 47743it [00:17, 3284.66it/s]43870it [00:17, 565.54it/s] 50375it [00:17, 3459.86it/s]44095it [00:17, 754.33it/s]48100it [00:17, 3258.32it/s]44247it [00:17, 767.75it/s]50756it [00:17, 3558.72it/s]44459it [00:17, 976.47it/s]48478it [00:17, 3402.22it/s]44570it [00:17, 971.32it/s]51141it [00:18, 3642.44it/s]44835it [00:17, 1261.67it/s]44934it [00:17, 1253.18it/s]48834it [00:18, 3355.36it/s]51508it [00:18, 3522.00it/s]45211it [00:17, 1580.77it/s]49213it [00:18, 3477.32it/s]45299it [00:18, 1542.43it/s]51886it [00:18, 3594.60it/s]45549it [00:17, 1835.43it/s]45678it [00:18, 1891.86it/s]49569it [00:18, 3396.64it/s]52248it [00:18, 3468.45it/s]45905it [00:17, 2143.65it/s]46017it [00:18, 2146.72it/s]49945it [00:18, 3498.89it/s]52624it [00:18, 3550.97it/s]46243it [00:18, 2350.14it/s]50318it [00:18, 3564.20it/s]46351it [00:18, 2366.11it/s]52981it [00:18, 3457.34it/s]46613it [00:18, 2648.62it/s]46727it [00:18, 2678.66it/s]50678it [00:18, 3451.73it/s]53358it [00:18, 3546.94it/s]46980it [00:18, 2779.86it/s]47071it [00:18, 2819.52it/s]51049it [00:18, 3525.52it/s]53715it [00:18, 3439.85it/s]47350it [00:18, 3007.10it/s]47456it [00:18, 3080.40it/s]51404it [00:18, 3423.87it/s]54101it [00:18, 3558.77it/s]47723it [00:18, 3194.01it/s]51780it [00:18, 3519.41it/s]47819it [00:18, 3124.82it/s]54481it [00:18, 3627.40it/s]48076it [00:18, 3179.88it/s]48166it [00:18, 3217.10it/s]52134it [00:18, 3421.68it/s]54846it [00:19, 3506.78it/s]48449it [00:18, 3327.08it/s]48543it [00:18, 3369.33it/s]52509it [00:19, 3506.25it/s]55220it [00:19, 3572.47it/s]48800it [00:18, 3256.47it/s]48897it [00:19, 3340.38it/s]52862it [00:19, 3410.74it/s]55579it [00:19, 3434.23it/s]49175it [00:18, 3393.18it/s]49279it [00:19, 3474.24it/s]53237it [00:19, 3507.77it/s]55954it [00:19, 3521.78it/s]49525it [00:18, 3316.64it/s]53609it [00:19, 3567.05it/s]49636it [00:19, 3293.87it/s]56308it [00:19, 3446.38it/s]49903it [00:19, 3445.67it/s]50011it [00:19, 3421.25it/s]53967it [00:19, 3460.39it/s]56685it [00:19, 3538.64it/s]50265it [00:19, 3493.41it/s]54336it [00:19, 3526.22it/s]50360it [00:19, 3361.15it/s]57059it [00:19, 3459.11it/s]50619it [00:19, 3367.92it/s]50739it [00:19, 3481.76it/s]54690it [00:19, 3416.47it/s]57440it [00:19, 3557.31it/s]50999it [00:19, 3489.92it/s]51121it [00:19, 3577.64it/s]55062it [00:19, 3502.00it/s]57819it [00:19, 3624.58it/s]51352it [00:19, 3372.41it/s]51482it [00:19, 3429.00it/s]55414it [00:19, 3380.78it/s]58183it [00:20, 3499.68it/s]51712it [00:19, 3436.15it/s]51861it [00:19, 3531.84it/s]55786it [00:20, 3477.61it/s]58555it [00:20, 3562.45it/s]52058it [00:19, 3335.37it/s]56164it [00:20, 3563.26it/s]52217it [00:20, 3444.18it/s]58913it [00:20, 3433.13it/s]52432it [00:19, 3450.80it/s]56522it [00:20, 3446.34it/s]52587it [00:20, 3446.23it/s]59288it [00:20, 3521.43it/s]52801it [00:19, 3517.45it/s]56897it [00:20, 3532.69it/s]52934it [00:20, 3380.11it/s]59642it [00:20, 3411.19it/s]53155it [00:20, 3357.95it/s]57252it [00:20, 3440.43it/s]53312it [00:20, 3493.62it/s]60016it [00:20, 3502.82it/s]53526it [00:20, 3456.08it/s]53688it [00:20, 3568.51it/s]57619it [00:20, 3497.65it/s]60389it [00:20, 3568.13it/s]53874it [00:20, 3346.76it/s]54047it [00:20, 3481.22it/s]57970it [00:20, 3405.55it/s]60748it [00:20, 3458.19it/s]54254it [00:20, 3475.57it/s]54415it [00:20, 3536.14it/s]58336it [00:20, 3478.78it/s]61123it [00:20, 3539.58it/s]54604it [00:20, 3333.47it/s]58709it [00:20, 3551.90it/s]54770it [00:20, 3438.55it/s]61479it [00:21, 3436.58it/s]54969it [00:20, 3421.88it/s]55125it [00:20, 3470.57it/s]59066it [00:20, 3419.34it/s]61856it [00:21, 3530.60it/s]55324it [00:20, 3458.34it/s]59437it [00:21, 3502.60it/s]55473it [00:20, 3311.60it/s]55672it [00:20, 3316.81it/s]55838it [00:21, 3407.59it/s]59789it [00:21, 3383.21it/s]56034it [00:20, 3400.83it/s]60158it [00:21, 3468.84it/s]56209it [00:21, 3493.03it/s]56376it [00:20, 3314.74it/s]56560it [00:21, 3411.64it/s]60507it [00:21, 3376.41it/s]56748it [00:21, 3429.73it/s]56924it [00:21, 3476.03it/s]60872it [00:21, 3453.52it/s]57093it [00:21, 3342.88it/s]61252it [00:21, 3552.80it/s]57273it [00:21, 3418.66it/s]57458it [00:21, 3429.07it/s]57637it [00:21, 3480.54it/s]61609it [00:21, 3432.35it/s]57829it [00:21, 3508.60it/s]57986it [00:21, 3410.35it/s]61984it [00:21, 3521.84it/s]58182it [00:21, 3374.03it/s]58342it [00:21, 3451.95it/s]58548it [00:21, 3454.35it/s]58720it [00:21, 3546.34it/s]58895it [00:21, 3285.45it/s]59076it [00:22, 3401.55it/s]59266it [00:21, 3403.01it/s]59447it [00:22, 3487.82it/s]59609it [00:21, 3292.74it/s]59798it [00:22, 3368.71it/s]59982it [00:22, 3415.60it/s]60174it [00:22, 3479.93it/s]60339it [00:22, 3458.19it/s]60524it [00:22, 3322.45it/s]60687it [00:22, 3352.60it/s]60901it [00:22, 3447.59it/s]62211it [00:22, 629.43it/s] 61056it [00:22, 3447.79it/s]61259it [00:22, 3368.94it/s]62591it [00:22, 847.85it/s]61403it [00:22, 3341.88it/s]61622it [00:22, 3441.62it/s]62968it [00:22, 1109.55it/s]61761it [00:22, 3409.60it/s]62003it [00:22, 3547.00it/s]63291it [00:23, 1349.95it/s]62104it [00:22, 3325.73it/s]63647it [00:23, 1658.08it/s]63977it [00:23, 1924.54it/s]64351it [00:23, 2271.19it/s]64693it [00:23, 2482.80it/s]65070it [00:23, 2780.38it/s]65442it [00:23, 3014.07it/s]62338it [00:23, 583.56it/s] 65797it [00:23, 3072.45it/s]62713it [00:23, 787.49it/s]66169it [00:23, 3244.24it/s]63019it [00:23, 978.90it/s]66523it [00:24, 3231.21it/s]63396it [00:23, 1278.63it/s]66890it [00:24, 3351.91it/s]63765it [00:24, 1599.45it/s]67241it [00:24, 3313.78it/s]64100it [00:24, 1858.87it/s]64471it [00:24, 2199.56it/s]67616it [00:24, 3434.90it/s]67992it [00:24, 3527.91it/s]64812it [00:24, 2417.26it/s]65182it [00:24, 2706.86it/s]68351it [00:24, 3433.82it/s]68728it [00:24, 3530.20it/s]65529it [00:24, 2806.83it/s]65903it [00:24, 3042.64it/s]69085it [00:24, 3435.39it/s]66272it [00:24, 3214.35it/s]69459it [00:24, 3520.65it/s]66625it [00:24, 3200.08it/s]69814it [00:24, 3434.87it/s]66999it [00:24, 3344.83it/s]70197it [00:25, 3546.61it/s]70562it [00:25, 3574.98it/s]67351it [00:25, 3302.17it/s]67727it [00:25, 3429.80it/s]70921it [00:25, 3443.66it/s]71295it [00:25, 3528.28it/s]68079it [00:25, 3343.17it/s]62360it [00:25, 459.68it/s] 68454it [00:25, 3458.39it/s]71650it [00:25, 3429.66it/s]62733it [00:25, 627.13it/s]68818it [00:25, 3509.57it/s]72027it [00:25, 3526.49it/s]62438it [00:25, 427.51it/s] 63052it [00:25, 801.72it/s]62807it [00:25, 591.07it/s]72382it [00:25, 3428.94it/s]69173it [00:25, 3389.74it/s]63417it [00:25, 1054.37it/s]63114it [00:25, 756.69it/s]72758it [00:25, 3521.37it/s]69544it [00:25, 3479.59it/s]63790it [00:25, 1355.21it/s]63485it [00:25, 1014.06it/s]73112it [00:25, 3461.03it/s]69895it [00:25, 3380.69it/s]64125it [00:25, 1612.98it/s]63842it [00:25, 1297.05it/s]73492it [00:26, 3558.58it/s]70273it [00:25, 3494.93it/s]64497it [00:25, 1956.92it/s]64167it [00:25, 1551.21it/s]73859it [00:26, 3588.52it/s]70625it [00:26, 3392.62it/s]64838it [00:25, 2206.27it/s]64539it [00:25, 1902.14it/s]74219it [00:26, 3496.15it/s]70990it [00:26, 3466.17it/s]65197it [00:26, 2497.59it/s]64874it [00:25, 2140.18it/s]74595it [00:26, 3572.29it/s]71359it [00:26, 3531.00it/s]65539it [00:26, 2660.30it/s]65226it [00:25, 2428.10it/s]74954it [00:26, 3456.43it/s]71714it [00:26, 3409.99it/s]65895it [00:26, 2880.38it/s]65561it [00:26, 2577.48it/s]75326it [00:26, 3530.55it/s]72086it [00:26, 3497.62it/s]66268it [00:26, 3098.74it/s]65933it [00:26, 2853.89it/s]75681it [00:26, 3421.24it/s]72438it [00:26, 3346.28it/s]66619it [00:26, 3098.70it/s]66298it [00:26, 3057.99it/s]76052it [00:26, 3504.06it/s]72810it [00:26, 3449.49it/s]66998it [00:26, 3278.15it/s]66645it [00:26, 3042.71it/s]76422it [00:26, 3559.05it/s]73158it [00:26, 3391.08it/s]67348it [00:26, 3267.69it/s]67014it [00:26, 3216.72it/s]76779it [00:26, 3446.50it/s]73533it [00:26, 3493.63it/s]67710it [00:26, 3365.23it/s]67358it [00:26, 3185.03it/s]77133it [00:27, 3470.82it/s]73917it [00:26, 3592.12it/s]68058it [00:26, 3315.40it/s]67729it [00:26, 3330.93it/s]77482it [00:27, 3397.83it/s]74278it [00:27, 3466.23it/s]68430it [00:27, 3429.96it/s]77853it [00:27, 3487.08it/s]68074it [00:26, 3226.88it/s]74649it [00:27, 3534.77it/s]68807it [00:27, 3527.45it/s]68445it [00:26, 3362.31it/s]78203it [00:27, 3397.04it/s]75004it [00:27, 3403.26it/s]69165it [00:27, 3430.03it/s]68815it [00:26, 3457.86it/s]78581it [00:27, 3505.46it/s]75371it [00:27, 3479.08it/s]69520it [00:27, 3463.72it/s]78954it [00:27, 3568.96it/s]69167it [00:27, 3346.71it/s]75721it [00:27, 3374.10it/s]69870it [00:27, 3395.05it/s]69520it [00:27, 3397.46it/s]79312it [00:27, 3451.03it/s]76077it [00:27, 3425.20it/s]70251it [00:27, 3513.30it/s]79681it [00:27, 3519.28it/s]69864it [00:27, 3315.86it/s]76445it [00:27, 3497.11it/s]70605it [00:27, 3424.65it/s]70238it [00:27, 3435.52it/s]80035it [00:27, 3432.10it/s]76796it [00:27, 3376.00it/s]70975it [00:27, 3503.37it/s]80411it [00:27, 3489.16it/s]70584it [00:27, 3331.65it/s]77160it [00:27, 3450.99it/s]71327it [00:27, 3434.55it/s]70934it [00:27, 3377.29it/s]80761it [00:28, 3406.97it/s]77507it [00:28, 3360.48it/s]71672it [00:27, 3369.91it/s]71302it [00:27, 3464.53it/s]81132it [00:28, 3492.48it/s]77878it [00:28, 3460.15it/s]72044it [00:28, 3469.76it/s]71650it [00:27, 3346.68it/s]81489it [00:28, 3379.97it/s]78226it [00:28, 3349.84it/s]72392it [00:28, 3386.21it/s]72022it [00:27, 3454.03it/s]81862it [00:28, 3479.68it/s]78601it [00:28, 3464.07it/s]72769it [00:28, 3495.27it/s]82237it [00:28, 3555.48it/s]72370it [00:28, 3301.58it/s]78970it [00:28, 3352.68it/s]73120it [00:28, 3435.10it/s]82594it [00:28, 3436.26it/s]72740it [00:28, 3410.64it/s]79325it [00:28, 3407.95it/s]73501it [00:28, 3542.56it/s]82963it [00:28, 3508.86it/s]73090it [00:28, 3343.04it/s]79695it [00:28, 3489.56it/s]73883it [00:28, 3623.44it/s]73461it [00:28, 3446.92it/s]83316it [00:28, 3404.54it/s]80046it [00:28, 3379.43it/s]74247it [00:28, 3423.54it/s]73826it [00:28, 3503.65it/s]83687it [00:28, 3489.57it/s]80421it [00:28, 3483.12it/s]74624it [00:28, 3520.12it/s]74178it [00:28, 3393.26it/s]84038it [00:29, 3357.50it/s]80771it [00:28, 3378.48it/s]74979it [00:28, 3400.60it/s]74550it [00:28, 3484.71it/s]84404it [00:29, 3442.21it/s]81138it [00:29, 3462.01it/s]75349it [00:29, 3485.48it/s]84775it [00:29, 3518.51it/s]74900it [00:28, 3358.57it/s]81490it [00:29, 3340.72it/s]75700it [00:29, 3390.98it/s]75254it [00:28, 3409.08it/s]85129it [00:29, 3405.58it/s]81860it [00:29, 3442.89it/s]76070it [00:29, 3477.90it/s]85500it [00:29, 3491.02it/s]75609it [00:29, 3300.49it/s]82229it [00:29, 3513.71it/s]76420it [00:29, 3410.96it/s]75976it [00:29, 3403.74it/s]82582it [00:29, 3345.18it/s]76763it [00:29, 3344.97it/s]76338it [00:29, 3464.58it/s]82953it [00:29, 3448.74it/s]77131it [00:29, 3440.71it/s]76686it [00:29, 3311.03it/s]83301it [00:29, 3351.88it/s]77477it [00:29, 3372.18it/s]77049it [00:29, 3399.42it/s]83666it [00:29, 3435.01it/s]77848it [00:29, 3466.73it/s]77391it [00:29, 3306.91it/s]84012it [00:29, 3351.58it/s]78196it [00:29, 3390.62it/s]77760it [00:29, 3413.95it/s]84382it [00:30, 3449.33it/s]78570it [00:29, 3490.25it/s]78111it [00:29, 3441.54it/s]84750it [00:30, 3515.73it/s]78924it [00:30, 3504.32it/s]78457it [00:29, 3334.00it/s]85103it [00:30, 3395.10it/s]79276it [00:30, 3401.77it/s]78824it [00:29, 3429.12it/s]85472it [00:30, 3478.90it/s]79646it [00:30, 3486.51it/s]79169it [00:30, 3316.05it/s]79996it [00:30, 3408.63it/s]79531it [00:30, 3401.41it/s]80370it [00:30, 3502.66it/s]79873it [00:30, 3312.60it/s]80722it [00:30, 3344.32it/s]80235it [00:30, 3398.91it/s]81094it [00:30, 3449.53it/s]80607it [00:30, 3491.99it/s]81464it [00:30, 3520.42it/s]80958it [00:30, 3362.98it/s]81818it [00:30, 3421.42it/s]81325it [00:30, 3450.71it/s]82192it [00:30, 3512.59it/s]81672it [00:30, 3327.58it/s]82545it [00:31, 3372.48it/s]82041it [00:30, 3431.02it/s]82921it [00:31, 3480.63it/s]82386it [00:31, 3315.05it/s]83271it [00:31, 3388.58it/s]82753it [00:31, 3414.19it/s]83641it [00:31, 3477.19it/s]83106it [00:31, 3445.80it/s]84000it [00:31, 3508.45it/s]83452it [00:31, 3317.39it/s]84353it [00:31, 3372.91it/s]85851it [00:31, 450.85it/s] 83808it [00:31, 3386.82it/s]84709it [00:31, 3418.64it/s]86233it [00:31, 622.21it/s]84149it [00:31, 3263.13it/s]85053it [00:31, 3280.28it/s]86598it [00:32, 818.66it/s]84508it [00:31, 3355.43it/s]85407it [00:31, 3354.04it/s]86968it [00:32, 1070.36it/s]84849it [00:31, 3234.57it/s]85745it [00:32, 3265.33it/s]87346it [00:32, 1370.77it/s]85214it [00:31, 3350.43it/s]87683it [00:32, 1633.03it/s]85583it [00:31, 3447.28it/s]88061it [00:32, 1981.46it/s]88407it [00:32, 2230.49it/s]88783it [00:32, 2550.55it/s]89133it [00:32, 2718.53it/s]89507it [00:32, 2967.58it/s]85822it [00:32, 432.47it/s] 89869it [00:33, 3134.42it/s]86199it [00:32, 596.37it/s]90224it [00:33, 3147.71it/s]86572it [00:33, 801.77it/s]90583it [00:33, 3266.26it/s]86885it [00:33, 999.36it/s]90931it [00:33, 3211.93it/s]87259it [00:33, 1297.29it/s]91288it [00:33, 3309.08it/s]87589it [00:33, 1556.55it/s]91638it [00:33, 3239.86it/s]87962it [00:33, 1904.71it/s]91996it [00:33, 3334.98it/s]88302it [00:33, 2148.07it/s]92356it [00:33, 3408.73it/s]88682it [00:33, 2489.47it/s]92702it [00:33, 3305.86it/s]89051it [00:33, 2763.20it/s]93052it [00:33, 3359.26it/s]89404it [00:33, 2836.21it/s]93391it [00:34, 3272.59it/s]89777it [00:34, 3061.54it/s]93753it [00:34, 3370.94it/s]90126it [00:34, 3082.19it/s]94112it [00:34, 3432.30it/s]90485it [00:34, 3217.08it/s]94457it [00:34, 3328.99it/s]90829it [00:34, 3161.95it/s]94817it [00:34, 3404.88it/s]91188it [00:34, 3278.16it/s]95159it [00:34, 3301.62it/s]91545it [00:34, 3360.30it/s]95520it [00:34, 3389.06it/s]91890it [00:34, 3249.04it/s]95861it [00:34, 3289.68it/s]92249it [00:34, 3344.83it/s]86073it [00:34, 393.10it/s] 96213it [00:34, 3354.22it/s]92589it [00:34, 3242.72it/s]86435it [00:34, 544.18it/s]96574it [00:35, 3427.60it/s]92948it [00:34, 3339.17it/s]86749it [00:34, 706.88it/s]96918it [00:35, 3317.21it/s]93291it [00:35, 3364.17it/s]87119it [00:35, 952.88it/s]97280it [00:35, 3402.78it/s]93630it [00:35, 3259.95it/s]87438it [00:35, 1183.73it/s]97622it [00:35, 3298.24it/s]93988it [00:35, 3349.32it/s]85930it [00:34, 365.74it/s] 87816it [00:35, 1521.36it/s]97984it [00:35, 3390.44it/s]94325it [00:35, 3244.01it/s]86306it [00:35, 509.20it/s]88189it [00:35, 1867.20it/s]98343it [00:35, 3448.02it/s]94683it [00:35, 3339.10it/s]86608it [00:35, 652.11it/s]88532it [00:35, 2116.53it/s]98689it [00:35, 3333.28it/s]95019it [00:35, 3233.95it/s]86973it [00:35, 877.58it/s]88907it [00:35, 2450.15it/s]99050it [00:35, 3411.81it/s]95377it [00:35, 3331.52it/s]87344it [00:35, 1152.39it/s]89254it [00:35, 2629.00it/s]99393it [00:35, 3293.19it/s]95735it [00:35, 3403.34it/s]87674it [00:35, 1401.99it/s]89623it [00:35, 2883.63it/s]99753it [00:35, 3378.55it/s]96077it [00:35, 3279.77it/s]88045it [00:35, 1741.32it/s]89971it [00:35, 2948.15it/s]100093it [00:36, 3282.96it/s]96436it [00:36, 3366.88it/s]88383it [00:35, 1993.74it/s]90339it [00:35, 3138.16it/s]100455it [00:36, 3377.87it/s]96775it [00:36, 3141.05it/s]88755it [00:35, 2332.27it/s]90696it [00:36, 3254.36it/s]100813it [00:36, 3436.14it/s]97133it [00:36, 3261.29it/s]89112it [00:35, 2602.94it/s]91046it [00:36, 3202.58it/s]101158it [00:36, 3329.31it/s]97493it [00:36, 3356.60it/s]89459it [00:35, 2725.57it/s]91395it [00:36, 3281.58it/s]101518it [00:36, 3405.62it/s]97832it [00:36, 3254.00it/s]89827it [00:36, 2960.66it/s]91736it [00:36, 3212.06it/s]101860it [00:36, 3308.36it/s]98191it [00:36, 3348.51it/s]90172it [00:36, 2986.76it/s]92094it [00:36, 3315.60it/s]102223it [00:36, 3398.94it/s]98529it [00:36, 3241.29it/s]90527it [00:36, 3134.48it/s]92443it [00:36, 3364.39it/s]102565it [00:36, 3298.97it/s]98888it [00:36, 3338.52it/s]90867it [00:36, 3084.36it/s]92785it [00:36, 3273.02it/s]102916it [00:36, 3359.27it/s]99224it [00:36, 3237.44it/s]91220it [00:36, 3200.60it/s]93146it [00:36, 3368.15it/s]103276it [00:37, 3427.96it/s]99583it [00:36, 3335.77it/s]91573it [00:36, 3292.62it/s]93486it [00:36, 3276.67it/s]103620it [00:37, 3319.81it/s]99932it [00:37, 3378.97it/s]91913it [00:36, 3193.36it/s]93837it [00:37, 3341.42it/s]103982it [00:37, 3405.58it/s]100272it [00:37, 3256.09it/s]92256it [00:36, 3258.01it/s]94174it [00:37, 3248.87it/s]104324it [00:37, 3301.81it/s]100632it [00:37, 3352.24it/s]92588it [00:36, 3155.61it/s]94538it [00:37, 3360.00it/s]104686it [00:37, 3391.13it/s]100969it [00:37, 3239.74it/s]92945it [00:37, 3270.71it/s]94898it [00:37, 3428.68it/s]105042it [00:37, 3439.67it/s]101327it [00:37, 3336.78it/s]93301it [00:37, 3353.23it/s]95243it [00:37, 3289.96it/s]105388it [00:37, 3321.05it/s]101667it [00:37, 3353.88it/s]93640it [00:37, 3230.94it/s]95603it [00:37, 3377.05it/s]105747it [00:37, 3396.33it/s]102004it [00:37, 3247.94it/s]93993it [00:37, 3314.86it/s]95943it [00:37, 3276.15it/s]106088it [00:37, 3271.89it/s]102364it [00:37, 3347.35it/s]94327it [00:37, 3199.34it/s]96294it [00:37, 3342.56it/s]106449it [00:37, 3367.73it/s]102701it [00:37, 3239.55it/s]94682it [00:37, 3298.96it/s]96655it [00:37, 3419.85it/s]106788it [00:38, 3282.26it/s]103050it [00:38, 3310.93it/s]96999it [00:37, 3311.22it/s]95014it [00:37, 3158.62it/s]107149it [00:38, 3374.37it/s]103399it [00:38, 3221.04it/s]97360it [00:38, 3397.04it/s]95368it [00:37, 3266.40it/s]107509it [00:38, 3438.56it/s]103755it [00:38, 3315.99it/s]95722it [00:37, 3342.82it/s]97702it [00:38, 3271.16it/s]107855it [00:38, 3332.11it/s]104117it [00:38, 3402.40it/s]98064it [00:38, 3370.49it/s]96059it [00:37, 3220.09it/s]108218it [00:38, 3415.70it/s]104459it [00:38, 3271.72it/s]96415it [00:38, 3315.19it/s]98403it [00:38, 3271.90it/s]108561it [00:38, 3321.67it/s]104819it [00:38, 3363.79it/s]98765it [00:38, 3370.25it/s]96749it [00:38, 3208.32it/s]108926it [00:38, 3414.97it/s]105158it [00:38, 3241.33it/s]99115it [00:38, 3407.12it/s]97102it [00:38, 3300.13it/s]109277it [00:38, 3442.21it/s]105513it [00:38, 3328.59it/s]97459it [00:38, 3376.57it/s]99457it [00:38, 3305.09it/s]109623it [00:38, 3322.92it/s]105869it [00:38, 3395.29it/s]99817it [00:38, 3388.35it/s]97799it [00:38, 3225.63it/s]109981it [00:39, 3394.89it/s]106211it [00:38, 3240.24it/s]98152it [00:38, 3309.97it/s]100158it [00:38, 3258.60it/s]110322it [00:39, 3306.29it/s]106570it [00:39, 3338.06it/s]100518it [00:39, 3355.75it/s]98485it [00:38, 3204.96it/s]110680it [00:39, 3384.88it/s]106906it [00:39, 3234.43it/s]98842it [00:38, 3308.47it/s]100878it [00:39, 3267.95it/s]111020it [00:39, 3295.71it/s]107266it [00:39, 3337.86it/s]99198it [00:38, 3380.75it/s]101240it [00:39, 3365.80it/s]111382it [00:39, 3387.35it/s]107602it [00:39, 3231.18it/s]101590it [00:39, 3403.64it/s]99538it [00:39, 3251.37it/s]111742it [00:39, 3448.58it/s]107961it [00:39, 3332.93it/s]99893it [00:39, 3336.69it/s]101932it [00:39, 3304.86it/s]112088it [00:39, 3338.84it/s]108322it [00:39, 3412.98it/s]102294it [00:39, 3393.80it/s]100229it [00:39, 3211.60it/s]112451it [00:39, 3421.12it/s]108665it [00:39, 3295.07it/s]100573it [00:39, 3274.19it/s]102635it [00:39, 3292.18it/s]112795it [00:39, 3285.96it/s]109028it [00:39, 3390.02it/s]102988it [00:39, 3358.87it/s]100903it [00:39, 3171.20it/s]113157it [00:39, 3379.08it/s]109369it [00:39, 3272.95it/s]103348it [00:39, 3427.47it/s]101261it [00:39, 3286.43it/s]113497it [00:40, 3278.29it/s]109713it [00:40, 3320.51it/s]101616it [00:39, 3360.03it/s]103692it [00:39, 3319.30it/s]113860it [00:40, 3376.82it/s]110071it [00:40, 3393.95it/s]104046it [00:40, 3366.66it/s]101954it [00:39, 3235.19it/s]114226it [00:40, 3458.81it/s]110412it [00:40, 3276.65it/s]102310it [00:39, 3327.65it/s]104384it [00:40, 3271.95it/s]114574it [00:40, 3343.95it/s]110772it [00:40, 3368.41it/s]104745it [00:40, 3368.26it/s]102645it [00:39, 3211.74it/s]114936it [00:40, 3421.21it/s]111111it [00:40, 3263.56it/s]105084it [00:40, 3268.52it/s]103002it [00:40, 3313.27it/s]111472it [00:40, 3360.26it/s]105428it [00:40, 3317.47it/s]103346it [00:40, 3348.90it/s]111810it [00:40, 3248.96it/s]105788it [00:40, 3397.98it/s]103683it [00:40, 3222.42it/s]112173it [00:40, 3355.59it/s]106129it [00:40, 3293.82it/s]104041it [00:40, 3323.25it/s]112535it [00:40, 3430.68it/s]106489it [00:40, 3379.92it/s]104376it [00:40, 3207.13it/s]112880it [00:40, 3288.12it/s]106829it [00:40, 3255.46it/s]104730it [00:40, 3296.84it/s]113241it [00:41, 3379.23it/s]107190it [00:41, 3355.25it/s]105080it [00:40, 3184.04it/s]113581it [00:41, 3273.36it/s]107552it [00:41, 3429.47it/s]105431it [00:40, 3274.81it/s]113941it [00:41, 3365.82it/s]107897it [00:41, 3292.46it/s]105786it [00:40, 3352.21it/s]114303it [00:41, 3437.80it/s]108261it [00:41, 3389.65it/s]106123it [00:41, 3206.23it/s]114649it [00:41, 3314.18it/s]108602it [00:41, 3312.21it/s]106477it [00:41, 3300.86it/s]115009it [00:41, 3395.11it/s]108967it [00:41, 3407.31it/s]106810it [00:41, 3199.46it/s]109310it [00:41, 3278.81it/s]107166it [00:41, 3301.69it/s]109672it [00:41, 3375.20it/s]107521it [00:41, 3371.31it/s]110035it [00:41, 3448.92it/s]107860it [00:41, 3248.18it/s]110382it [00:41, 3341.19it/s]108217it [00:41, 3340.24it/s]110732it [00:42, 3384.96it/s]108553it [00:41, 3235.27it/s]111072it [00:42, 3295.73it/s]108901it [00:41, 3303.84it/s]111438it [00:42, 3399.53it/s]109259it [00:41, 3383.30it/s]111798it [00:42, 3269.78it/s]109599it [00:42, 3258.44it/s]112163it [00:42, 3375.82it/s]109946it [00:42, 3318.31it/s]112529it [00:42, 3456.79it/s]110280it [00:42, 3207.71it/s]112877it [00:42, 3345.56it/s]110638it [00:42, 3313.79it/s]113226it [00:42, 3385.78it/s]110971it [00:42, 3203.22it/s]113567it [00:42, 3290.84it/s]111330it [00:42, 3313.22it/s]113932it [00:43, 3391.73it/s]111674it [00:42, 3348.32it/s]114299it [00:43, 3471.65it/s]112011it [00:42, 3231.17it/s]114648it [00:43, 3359.28it/s]112369it [00:42, 3329.37it/s]114997it [00:43, 3396.37it/s]112704it [00:43, 3222.95it/s]115280it [00:43, 348.76it/s] 113060it [00:43, 3317.26it/s]115633it [00:43, 477.93it/s]113416it [00:43, 3385.55it/s]115998it [00:43, 651.64it/s]113756it [00:43, 3257.89it/s]116304it [00:43, 826.45it/s]114112it [00:43, 3344.32it/s]116670it [00:44, 1092.65it/s]114449it [00:43, 3201.60it/s]116992it [00:44, 1338.20it/s]114804it [00:43, 3300.10it/s]117356it [00:44, 1669.76it/s]115160it [00:43, 3195.85it/s]117721it [00:44, 2006.93it/s]118063it [00:44, 2243.36it/s]118429it [00:44, 2547.43it/s]118772it [00:44, 2667.95it/s]119136it [00:44, 2906.09it/s]119477it [00:44, 2953.94it/s]119839it [00:44, 3129.70it/s]120201it [00:45, 3263.32it/s]115351it [00:45, 321.60it/s] 120548it [00:45, 3221.78it/s]115712it [00:45, 445.78it/s]120910it [00:45, 3333.33it/s]116068it [00:45, 599.89it/s]121254it [00:45, 3264.10it/s]116431it [00:45, 804.60it/s]121616it [00:45, 3364.49it/s]116794it [00:45, 1053.15it/s]121959it [00:45, 3243.44it/s]117119it [00:45, 1289.31it/s]122323it [00:45, 3354.33it/s]117478it [00:45, 1603.75it/s]122689it [00:45, 3441.54it/s]117809it [00:45, 1858.77it/s]123037it [00:45, 3341.57it/s]118158it [00:45, 2163.68it/s]123400it [00:45, 3422.83it/s]118518it [00:45, 2465.69it/s]123745it [00:46, 3321.41it/s]118858it [00:46, 2618.09it/s]124107it [00:46, 3406.88it/s]119218it [00:46, 2851.52it/s]124468it [00:46, 3311.79it/s]119556it [00:46, 2904.76it/s]124831it [00:46, 3400.60it/s]119916it [00:46, 3088.05it/s]125177it [00:46, 3416.90it/s]120269it [00:46, 3074.95it/s]125521it [00:46, 3312.10it/s]120632it [00:46, 3223.93it/s]125885it [00:46, 3404.32it/s]120992it [00:46, 3328.09it/s]126227it [00:46, 3309.18it/s]121337it [00:46, 3218.16it/s]126588it [00:46, 3394.72it/s]121697it [00:46, 3325.28it/s]126950it [00:47, 3457.63it/s]122037it [00:47, 3230.54it/s]127297it [00:47, 3350.88it/s]115338it [00:46, 298.94it/s] 122401it [00:47, 3343.88it/s]127659it [00:47, 3428.25it/s]115701it [00:47, 416.58it/s]122763it [00:47, 3421.83it/s]116012it [00:47, 545.96it/s]128004it [00:47, 3318.05it/s]123109it [00:47, 3296.84it/s]116318it [00:47, 707.40it/s]128363it [00:47, 3395.91it/s]123468it [00:47, 3379.17it/s]116682it [00:47, 953.69it/s]128704it [00:47, 3266.78it/s]123809it [00:47, 3266.09it/s]116997it [00:47, 1187.16it/s]129069it [00:47, 3373.83it/s]124169it [00:47, 3360.40it/s]117359it [00:47, 1509.63it/s]129432it [00:47, 3446.28it/s]124508it [00:47, 3224.21it/s]117722it [00:47, 1846.64it/s]129779it [00:47, 3335.44it/s]124870it [00:47, 3334.73it/s]118061it [00:47, 2099.81it/s]130139it [00:47, 3409.79it/s]125229it [00:47, 3405.80it/s]115482it [00:47, 283.54it/s] 118423it [00:47, 2414.01it/s]130482it [00:48, 3313.55it/s]125572it [00:48, 3285.48it/s]115841it [00:47, 396.91it/s]118763it [00:47, 2586.06it/s]130841it [00:48, 3392.02it/s]125931it [00:48, 3371.15it/s]116119it [00:47, 508.17it/s]119124it [00:48, 2832.37it/s]131188it [00:48, 3294.69it/s]116480it [00:47, 702.09it/s]126270it [00:48, 3260.89it/s]119465it [00:48, 2903.82it/s]131548it [00:48, 3380.97it/s]116839it [00:47, 938.15it/s]126628it [00:48, 3350.96it/s]119796it [00:48, 3005.99it/s]131893it [00:48, 3400.46it/s]117154it [00:48, 1163.21it/s]126986it [00:48, 3415.63it/s]120158it [00:48, 3173.07it/s]132235it [00:48, 3308.61it/s]117510it [00:48, 1473.28it/s]127329it [00:48, 3296.16it/s]120498it [00:48, 3158.05it/s]132595it [00:48, 3390.47it/s]117834it [00:48, 1725.30it/s]127675it [00:48, 3340.98it/s]120861it [00:48, 3290.57it/s]132936it [00:48, 3296.60it/s]118189it [00:48, 2053.64it/s]128011it [00:48, 3238.90it/s]121202it [00:48, 3234.35it/s]133295it [00:48, 3379.26it/s]118547it [00:48, 2366.03it/s]128366it [00:48, 3326.23it/s]121564it [00:48, 3342.59it/s]133656it [00:49, 3446.13it/s]118884it [00:48, 2499.57it/s]128700it [00:49, 3234.36it/s]121925it [00:48, 3417.48it/s]134002it [00:49, 3334.30it/s]119240it [00:48, 2750.24it/s]129061it [00:49, 3340.82it/s]122272it [00:49, 3322.49it/s]134358it [00:49, 3397.11it/s]129420it [00:49, 3410.98it/s]119573it [00:48, 2806.38it/s]122638it [00:49, 3419.03it/s]134699it [00:49, 3296.25it/s]119929it [00:48, 3000.29it/s]129763it [00:49, 3289.26it/s]122983it [00:49, 3325.09it/s]135044it [00:49, 3339.80it/s]130118it [00:49, 3363.96it/s]120269it [00:49, 2990.10it/s]123319it [00:49, 3194.50it/s]135388it [00:49, 3254.93it/s]120626it [00:49, 3146.91it/s]130456it [00:49, 3241.61it/s]123641it [00:49, 3130.88it/s]135747it [00:49, 3349.42it/s]120981it [00:49, 3258.16it/s]130800it [00:49, 3297.46it/s]123996it [00:49, 3248.25it/s]136103it [00:49, 3410.54it/s]131153it [00:49, 3363.19it/s]121320it [00:49, 3168.34it/s]124350it [00:49, 3331.04it/s]136446it [00:49, 3288.59it/s]121668it [00:49, 3255.24it/s]131491it [00:49, 3239.86it/s]124685it [00:49, 3225.41it/s]136804it [00:49, 3370.34it/s]131847it [00:49, 3330.99it/s]122001it [00:49, 3162.35it/s]125042it [00:49, 3323.59it/s]137143it [00:50, 3275.58it/s]122360it [00:49, 3281.00it/s]132182it [00:50, 3225.77it/s]125376it [00:49, 3232.53it/s]137504it [00:50, 3371.35it/s]122720it [00:49, 3367.12it/s]132542it [00:50, 3331.76it/s]125732it [00:50, 3324.62it/s]137866it [00:50, 3441.66it/s]123061it [00:49, 3252.07it/s]132877it [00:50, 3230.59it/s]126090it [00:50, 3398.76it/s]138212it [00:50, 3291.19it/s]123417it [00:49, 3337.82it/s]133232it [00:50, 3321.15it/s]126432it [00:50, 3281.05it/s]138572it [00:50, 3378.42it/s]133592it [00:50, 3399.66it/s]123754it [00:50, 3218.28it/s]126789it [00:50, 3363.93it/s]138912it [00:50, 3283.59it/s]124109it [00:50, 3310.60it/s]133934it [00:50, 3279.16it/s]127127it [00:50, 3226.90it/s]139277it [00:50, 3387.80it/s]124453it [00:50, 3346.18it/s]134278it [00:50, 3324.71it/s]127484it [00:50, 3323.14it/s]139618it [00:50, 3297.61it/s]124790it [00:50, 3222.28it/s]134612it [00:50, 3226.09it/s]127829it [00:50, 3233.91it/s]139983it [00:50, 3397.71it/s]125145it [00:50, 3313.98it/s]134968it [00:50, 3317.22it/s]128185it [00:50, 3324.46it/s]140347it [00:51, 3467.22it/s]135327it [00:51, 3395.71it/s]125479it [00:50, 3198.48it/s]128543it [00:50, 3397.57it/s]140695it [00:51, 3349.34it/s]125837it [00:50, 3305.59it/s]135668it [00:51, 3283.05it/s]128885it [00:51, 3286.53it/s]141057it [00:51, 3427.04it/s]136030it [00:51, 3378.12it/s]126170it [00:50, 3198.42it/s]129245it [00:51, 3375.33it/s]141402it [00:51, 3319.57it/s]126525it [00:50, 3297.19it/s]136370it [00:51, 3254.13it/s]129585it [00:51, 3268.68it/s]141746it [00:51, 3351.77it/s]126880it [00:51, 3369.80it/s]136727it [00:51, 3343.49it/s]129914it [00:51, 3269.12it/s]142105it [00:51, 3419.54it/s]137068it [00:51, 3237.59it/s]127219it [00:51, 3215.30it/s]130272it [00:51, 3358.48it/s]142448it [00:51, 3308.77it/s]137412it [00:51, 3294.55it/s]127575it [00:51, 3312.92it/s]130609it [00:51, 3264.05it/s]142806it [00:51, 3385.35it/s]137768it [00:51, 3365.96it/s]127909it [00:51, 3198.05it/s]130967it [00:51, 3354.55it/s]143146it [00:51, 3286.79it/s]138106it [00:51, 3260.87it/s]128261it [00:51, 3289.27it/s]131304it [00:51, 3251.25it/s]143507it [00:51, 3378.03it/s]138462it [00:51, 3345.98it/s]128618it [00:51, 3368.36it/s]131661it [00:51, 3342.15it/s]143847it [00:52, 3286.42it/s]138798it [00:52, 3234.93it/s]128957it [00:51, 3245.92it/s]132019it [00:51, 3410.08it/s]144207it [00:52, 3375.95it/s]139156it [00:52, 3333.47it/s]129315it [00:51, 3339.68it/s]132362it [00:52, 3293.39it/s]144570it [00:52, 3448.83it/s]139518it [00:52, 3406.83it/s]129651it [00:51, 3222.21it/s]132720it [00:52, 3373.64it/s]144916it [00:52, 3302.94it/s]139860it [00:52, 3296.88it/s]129999it [00:51, 3294.41it/s]133059it [00:52, 3264.93it/s]145280it [00:52, 3398.08it/s]140218it [00:52, 3374.38it/s]130350it [00:52, 3191.20it/s]133400it [00:52, 3304.68it/s]145622it [00:52, 3308.44it/s]140557it [00:52, 3225.27it/s]130705it [00:52, 3290.12it/s]133732it [00:52, 3216.79it/s]145987it [00:52, 3406.41it/s]140918it [00:52, 3324.89it/s]131063it [00:52, 3371.16it/s]134091it [00:52, 3322.60it/s]146330it [00:52, 3295.94it/s]141268it [00:52, 3239.84it/s]131402it [00:52, 3235.41it/s]134447it [00:52, 3389.78it/s]146689it [00:52, 3379.66it/s]141626it [00:52, 3335.05it/s]131756it [00:52, 3319.97it/s]134788it [00:52, 3276.30it/s]147051it [00:53, 3449.18it/s]141984it [00:53, 3403.05it/s]132090it [00:52, 3205.70it/s]135148it [00:52, 3367.73it/s]147398it [00:53, 3334.95it/s]142326it [00:53, 3272.61it/s]132447it [00:52, 3309.12it/s]135487it [00:53, 3261.86it/s]147760it [00:53, 3416.19it/s]142681it [00:53, 3350.84it/s]132792it [00:52, 3348.94it/s]135826it [00:53, 3298.10it/s]148104it [00:53, 3312.64it/s]143018it [00:53, 3235.80it/s]133129it [00:52, 3221.47it/s]136183it [00:53, 3375.15it/s]148450it [00:53, 3354.61it/s]143374it [00:53, 3327.71it/s]133484it [00:53, 3312.92it/s]136522it [00:53, 3255.37it/s]148810it [00:53, 3424.99it/s]143733it [00:53, 3402.06it/s]133817it [00:53, 3202.03it/s]136877it [00:53, 3339.33it/s]149154it [00:53, 3322.32it/s]134173it [00:53, 3303.94it/s]144075it [00:53, 3232.37it/s]137213it [00:53, 3234.88it/s]149517it [00:53, 3409.35it/s]134528it [00:53, 3374.67it/s]144432it [00:53, 3327.55it/s]137571it [00:53, 3331.56it/s]149860it [00:53, 3312.66it/s]144768it [00:53, 3241.83it/s]134867it [00:53, 3248.47it/s]137909it [00:53, 3229.94it/s]150221it [00:53, 3396.13it/s]145127it [00:53, 3340.69it/s]135222it [00:53, 3332.96it/s]138254it [00:53, 3292.26it/s]150562it [00:54, 3293.02it/s]145468it [00:54, 3258.57it/s]135557it [00:53, 3188.28it/s]138609it [00:53, 3361.03it/s]150925it [00:54, 3388.72it/s]145828it [00:54, 3353.93it/s]135911it [00:53, 3286.79it/s]138947it [00:54, 3261.37it/s]151281it [00:54, 3436.81it/s]146188it [00:54, 3425.09it/s]136242it [00:53, 3178.22it/s]139307it [00:54, 3357.42it/s]151626it [00:54, 3289.87it/s]146532it [00:54, 3291.47it/s]136596it [00:54, 3280.01it/s]139645it [00:54, 3259.26it/s]151985it [00:54, 3373.66it/s]146895it [00:54, 3386.03it/s]136951it [00:54, 3357.44it/s]140007it [00:54, 3362.28it/s]147236it [00:54, 3267.59it/s]140364it [00:54, 3421.36it/s]137289it [00:54, 3231.23it/s]147597it [00:54, 3362.76it/s]137646it [00:54, 3325.69it/s]140708it [00:54, 3304.91it/s]147938it [00:54, 3375.62it/s]141062it [00:54, 3370.85it/s]137981it [00:54, 3210.70it/s]148277it [00:54, 3262.18it/s]138329it [00:54, 3285.36it/s]141401it [00:54, 3240.44it/s]148633it [00:55, 3345.50it/s]138680it [00:54, 3348.70it/s]141755it [00:54, 3324.40it/s]148969it [00:55, 3247.09it/s]139017it [00:54, 3237.04it/s]142109it [00:55, 3226.96it/s]149328it [00:55, 3343.14it/s]139377it [00:54, 3339.48it/s]142466it [00:55, 3322.39it/s]149668it [00:55, 3241.48it/s]139713it [00:54, 3223.53it/s]142821it [00:55, 3385.66it/s]150028it [00:55, 3342.73it/s]140072it [00:55, 3327.75it/s]143162it [00:55, 3279.72it/s]150387it [00:55, 3414.33it/s]140427it [00:55, 3390.77it/s]143519it [00:55, 3362.87it/s]150730it [00:55, 3306.89it/s]140768it [00:55, 3250.77it/s]143857it [00:55, 3231.93it/s]151090it [00:55, 3390.69it/s]141111it [00:55, 3301.02it/s]144211it [00:55, 3318.48it/s]151431it [00:55, 3236.45it/s]141443it [00:55, 3190.03it/s]144574it [00:55, 3408.25it/s]151789it [00:55, 3332.53it/s]141795it [00:55, 3282.20it/s]144917it [00:55, 3298.00it/s]152148it [00:56, 3404.31it/s]142125it [00:55, 3172.06it/s]145278it [00:55, 3385.85it/s]142479it [00:55, 3274.88it/s]145619it [00:56, 3290.48it/s]142831it [00:55, 3344.00it/s]145980it [00:56, 3379.64it/s]143167it [00:56, 3220.15it/s]146320it [00:56, 3229.19it/s]143523it [00:56, 3316.13it/s]146677it [00:56, 3325.77it/s]147037it [00:56, 3404.06it/s]143857it [00:56, 3180.74it/s]144210it [00:56, 3275.26it/s]147380it [00:56, 3304.48it/s]144569it [00:56, 3364.88it/s]147740it [00:56, 3387.15it/s]144908it [00:56, 3246.85it/s]148081it [00:56, 3282.70it/s]145263it [00:56, 3332.48it/s]148438it [00:56, 3364.56it/s]148794it [00:57, 3420.32it/s]145598it [00:56, 3229.66it/s]145954it [00:56, 3322.86it/s]149138it [00:57, 3271.75it/s]146305it [00:56, 3376.66it/s]149498it [00:57, 3364.59it/s]146644it [00:57, 3210.25it/s]149837it [00:57, 3270.71it/s]147001it [00:57, 3310.94it/s]150192it [00:57, 3349.03it/s]147335it [00:57, 3200.48it/s]150529it [00:57, 3249.48it/s]147692it [00:57, 3305.48it/s]150889it [00:57, 3348.90it/s]151246it [00:57, 3412.24it/s]148025it [00:57, 3199.12it/s]148380it [00:57, 3298.83it/s]151589it [00:57, 3255.77it/s]148734it [00:57, 3366.66it/s]151947it [00:57, 3347.14it/s]149073it [00:57, 3216.95it/s]149430it [00:57, 3315.04it/s]152325it [00:58, 275.18it/s] 149764it [00:58, 3211.46it/s]152685it [00:58, 383.64it/s]150116it [00:58, 3299.00it/s]153048it [00:58, 528.19it/s]150468it [00:58, 3362.85it/s]153356it [00:58, 680.70it/s]150806it [00:58, 3232.00it/s]153724it [00:58, 916.69it/s]151160it [00:58, 3319.53it/s]154047it [00:59, 1145.76it/s]151494it [00:58, 3199.23it/s]154413it [00:59, 1461.51it/s]151840it [00:58, 3268.04it/s]154778it [00:59, 1750.71it/s]152190it [00:58, 3175.08it/s]155147it [00:59, 2089.19it/s]155514it [00:59, 2404.44it/s]155862it [00:59, 2580.95it/s]156227it [00:59, 2833.72it/s]156573it [00:59, 2914.07it/s]156941it [00:59, 3112.02it/s]157297it [00:59, 3095.69it/s]157664it [01:00, 3250.25it/s]158029it [01:00, 3360.51it/s]158379it [01:00, 3288.18it/s]158747it [01:00, 3398.59it/s]152491it [01:00, 259.03it/s] 159095it [01:00, 3323.81it/s]152851it [01:00, 361.49it/s]159463it [01:00, 3425.24it/s]153153it [01:00, 473.05it/s]159817it [01:00, 3337.77it/s]153515it [01:00, 651.00it/s]160174it [01:00, 3402.32it/s]153876it [01:00, 872.21it/s]160538it [01:00, 3470.36it/s]154200it [01:00, 1094.63it/s]160888it [01:01, 3360.90it/s]154560it [01:00, 1396.67it/s]161255it [01:01, 3449.48it/s]154891it [01:01, 1657.14it/s]161602it [01:01, 3346.42it/s]155255it [01:01, 1997.07it/s]161973it [01:01, 3450.62it/s]155617it [01:01, 2316.50it/s]162337it [01:01, 3363.57it/s]155961it [01:01, 2470.35it/s]162697it [01:01, 3430.25it/s]156321it [01:01, 2732.28it/s]163067it [01:01, 3505.99it/s]156659it [01:01, 2827.05it/s]163419it [01:01, 3395.53it/s]157019it [01:01, 3025.47it/s]163791it [01:01, 3486.95it/s]157358it [01:01, 3049.03it/s]164142it [01:01, 3394.24it/s]157723it [01:01, 3211.90it/s]164511it [01:02, 3479.10it/s]158086it [01:01, 3327.86it/s]164861it [01:02, 3405.21it/s]158433it [01:02, 3245.99it/s]152284it [01:02, 270.19it/s] 165219it [01:02, 3454.27it/s]158796it [01:02, 3351.97it/s]152643it [01:02, 377.21it/s]165592it [01:02, 3532.48it/s]159140it [01:02, 3263.61it/s]153004it [01:02, 519.49it/s]165947it [01:02, 3433.98it/s]159488it [01:02, 3322.48it/s]153305it [01:02, 666.78it/s]166323it [01:02, 3527.37it/s]159825it [01:02, 3237.00it/s]153653it [01:02, 885.82it/s]166677it [01:02, 3427.28it/s]160186it [01:02, 3342.64it/s]153968it [01:02, 1108.30it/s]167052it [01:02, 3518.17it/s]160550it [01:02, 3426.30it/s]154329it [01:02, 1420.33it/s]167405it [01:02, 3412.08it/s]160895it [01:02, 3310.70it/s]154692it [01:02, 1754.25it/s]167782it [01:02, 3513.04it/s]161261it [01:02, 3410.88it/s]155029it [01:02, 2007.07it/s]168157it [01:03, 3580.21it/s]161605it [01:03, 3288.54it/s]155391it [01:02, 2328.67it/s]168517it [01:03, 3335.93it/s]161972it [01:03, 3397.04it/s]155730it [01:03, 2506.94it/s]168890it [01:03, 3445.42it/s]162336it [01:03, 3465.36it/s]156090it [01:03, 2764.03it/s]169238it [01:03, 3365.35it/s]156440it [01:03, 2947.97it/s]162685it [01:03, 3350.13it/s]169611it [01:03, 3469.07it/s]163044it [01:03, 3418.70it/s]156782it [01:03, 2983.48it/s]169961it [01:03, 3371.29it/s]157144it [01:03, 3154.50it/s]163388it [01:03, 3269.27it/s]170334it [01:03, 3473.63it/s]163755it [01:03, 3382.71it/s]157485it [01:03, 3132.85it/s]170706it [01:03, 3543.50it/s]157844it [01:03, 3258.03it/s]164096it [01:03, 3287.06it/s]171062it [01:03, 3430.77it/s]164464it [01:03, 3398.69it/s]158183it [01:03, 3194.67it/s]152510it [01:03, 223.55it/s] 171433it [01:04, 3509.60it/s]164835it [01:03, 3488.69it/s]158544it [01:03, 3311.43it/s]152866it [01:03, 315.45it/s]171786it [01:04, 3389.92it/s]158897it [01:03, 3374.10it/s]165186it [01:04, 3378.10it/s]153144it [01:03, 408.74it/s]172160it [01:04, 3488.16it/s]165556it [01:04, 3469.95it/s]159240it [01:04, 3228.57it/s]153502it [01:03, 571.42it/s]172511it [01:04, 3359.79it/s]159604it [01:04, 3345.08it/s]165905it [01:04, 3366.99it/s]153862it [01:03, 778.15it/s]172890it [01:04, 3481.14it/s]166275it [01:04, 3462.25it/s]159943it [01:04, 3261.29it/s]154177it [01:04, 982.51it/s]173258it [01:04, 3387.65it/s]160307it [01:04, 3367.72it/s]166623it [01:04, 3355.59it/s]154533it [01:04, 1271.00it/s]173628it [01:04, 3475.81it/s]166971it [01:04, 3391.13it/s]160658it [01:04, 3274.21it/s]154856it [01:04, 1520.89it/s]173997it [01:04, 3536.30it/s]167341it [01:04, 3479.56it/s]161018it [01:04, 3364.76it/s]155216it [01:04, 1858.80it/s]174353it [01:04, 3421.18it/s]167691it [01:04, 3371.30it/s]161372it [01:04, 3413.47it/s]155572it [01:04, 2178.80it/s]174732it [01:05, 3526.12it/s]168060it [01:04, 3462.58it/s]161716it [01:04, 3303.71it/s]155908it [01:04, 2349.54it/s]175087it [01:05, 3386.71it/s]168408it [01:05, 3361.61it/s]162083it [01:04, 3407.45it/s]156265it [01:04, 2625.62it/s]175454it [01:05, 3467.06it/s]168778it [01:05, 3450.69it/s]162426it [01:05, 3303.97it/s]156598it [01:04, 2720.90it/s]175803it [01:05, 3372.17it/s]169125it [01:05, 3350.39it/s]162792it [01:05, 3403.61it/s]156953it [01:04, 2931.13it/s]176167it [01:05, 3447.56it/s]169490it [01:05, 3434.19it/s]163158it [01:05, 3475.56it/s]157300it [01:05, 2951.45it/s]176535it [01:05, 3514.56it/s]169856it [01:05, 3497.84it/s]163507it [01:05, 3359.36it/s]157658it [01:05, 3118.22it/s]176888it [01:05, 3413.47it/s]170207it [01:05, 3385.74it/s]163865it [01:05, 3420.01it/s]158013it [01:05, 3236.72it/s]177252it [01:05, 3476.68it/s]170575it [01:05, 3467.81it/s]164209it [01:05, 3317.33it/s]158353it [01:05, 3155.86it/s]177601it [01:05, 3330.09it/s]164574it [01:05, 3412.64it/s]170924it [01:05, 3334.72it/s]158700it [01:05, 3235.49it/s]177979it [01:05, 3456.99it/s]171293it [01:05, 3433.56it/s]164917it [01:05, 3336.06it/s]159033it [01:05, 3166.04it/s]178327it [01:06, 3382.60it/s]165282it [01:05, 3424.12it/s]171639it [01:06, 3321.59it/s]159395it [01:05, 3293.42it/s]178700it [01:06, 3482.03it/s]165654it [01:05, 3509.69it/s]172006it [01:06, 3418.72it/s]159753it [01:05, 3375.29it/s]179068it [01:06, 3536.38it/s]172369it [01:06, 3479.70it/s]166007it [01:06, 3398.01it/s]160095it [01:05, 3259.67it/s]179423it [01:06, 3409.05it/s]166374it [01:06, 3475.99it/s]172719it [01:06, 3363.12it/s]160451it [01:05, 3345.13it/s]179801it [01:06, 3513.56it/s]173093it [01:06, 3470.77it/s]166723it [01:06, 3349.54it/s]160789it [01:06, 3242.86it/s]180154it [01:06, 3416.04it/s]167089it [01:06, 3436.37it/s]173442it [01:06, 3343.71it/s]161148it [01:06, 3341.43it/s]180528it [01:06, 3507.87it/s]173808it [01:06, 3433.39it/s]167435it [01:06, 3338.77it/s]161493it [01:06, 3370.12it/s]180881it [01:06, 3379.44it/s]167803it [01:06, 3436.53it/s]174154it [01:06, 3318.82it/s]161832it [01:06, 3256.91it/s]181248it [01:06, 3460.92it/s]168174it [01:06, 3515.80it/s]174528it [01:06, 3436.70it/s]162191it [01:06, 3351.14it/s]181617it [01:07, 3527.12it/s]174888it [01:06, 3483.83it/s]168527it [01:06, 3398.93it/s]162528it [01:06, 3244.80it/s]181972it [01:07, 3411.84it/s]168892it [01:06, 3469.87it/s]175238it [01:07, 3376.13it/s]162887it [01:06, 3342.55it/s]182348it [01:07, 3510.53it/s]175600it [01:07, 3446.20it/s]169241it [01:07, 3325.39it/s]163223it [01:06, 3239.48it/s]182701it [01:07, 3426.78it/s]169608it [01:07, 3422.11it/s]175946it [01:07, 3339.54it/s]163586it [01:06, 3349.59it/s]183070it [01:07, 3501.73it/s]176310it [01:07, 3423.92it/s]169953it [01:07, 3317.44it/s]163947it [01:07, 3425.05it/s]183422it [01:07, 3372.67it/s]170322it [01:07, 3423.38it/s]176654it [01:07, 3291.09it/s]164291it [01:07, 3278.94it/s]183797it [01:07, 3480.26it/s]170690it [01:07, 3496.88it/s]177018it [01:07, 3390.04it/s]164650it [01:07, 3364.95it/s]184173it [01:07, 3559.70it/s]177386it [01:07, 3473.91it/s]171042it [01:07, 3379.91it/s]164989it [01:07, 3292.13it/s]184531it [01:07, 3448.16it/s]171407it [01:07, 3455.00it/s]177735it [01:07, 3342.43it/s]165351it [01:07, 3385.80it/s]184912it [01:07, 3550.25it/s]178104it [01:07, 3441.94it/s]171754it [01:07, 3300.69it/s]165700it [01:07, 3284.64it/s]185269it [01:08, 3444.24it/s]172125it [01:07, 3414.90it/s]178450it [01:08, 3331.41it/s]166069it [01:07, 3399.17it/s]185638it [01:08, 3512.83it/s]178801it [01:08, 3380.90it/s]172469it [01:08, 3319.31it/s]166433it [01:07, 3468.82it/s]185991it [01:08, 3386.79it/s]172838it [01:08, 3417.78it/s]179141it [01:08, 3284.20it/s]166782it [01:07, 3355.20it/s]186370it [01:08, 3500.98it/s]173208it [01:08, 3496.70it/s]179501it [01:08, 3372.40it/s]167133it [01:07, 3397.64it/s]186722it [01:08, 3451.01it/s]179875it [01:08, 3477.66it/s]173560it [01:08, 3368.72it/s]167475it [01:08, 3288.68it/s]187101it [01:08, 3546.40it/s]180225it [01:08, 3375.07it/s]173924it [01:08, 3444.86it/s]167841it [01:08, 3394.52it/s]187475it [01:08, 3600.91it/s]180592it [01:08, 3457.65it/s]174271it [01:08, 3312.21it/s]168211it [01:08, 3483.00it/s]187837it [01:08, 3492.47it/s]180940it [01:08, 3365.15it/s]174648it [01:08, 3440.88it/s]168561it [01:08, 3354.12it/s]188218it [01:08, 3581.96it/s]181306it [01:08, 3448.63it/s]174995it [01:08, 3330.53it/s]168920it [01:08, 3421.20it/s]188578it [01:09, 3486.25it/s]181653it [01:08, 3355.08it/s]175359it [01:08, 3418.34it/s]169264it [01:08, 3306.61it/s]188937it [01:09, 3514.37it/s]181990it [01:09, 3292.21it/s]175727it [01:08, 3493.31it/s]169626it [01:08, 3393.70it/s]189290it [01:09, 3407.54it/s]182359it [01:09, 3405.79it/s]176078it [01:09, 3368.49it/s]169967it [01:08, 3246.02it/s]189656it [01:09, 3478.25it/s]182701it [01:09, 3340.61it/s]176439it [01:09, 3435.66it/s]170336it [01:08, 3369.63it/s]190037it [01:09, 3573.32it/s]183065it [01:09, 3425.49it/s]176785it [01:09, 3344.84it/s]170703it [01:09, 3454.49it/s]190396it [01:09, 3477.06it/s]183409it [01:09, 3334.75it/s]177132it [01:09, 3380.71it/s]171051it [01:09, 3322.87it/s]190769it [01:09, 3550.02it/s]183778it [01:09, 3436.11it/s]177472it [01:09, 3288.41it/s]171415it [01:09, 3411.18it/s]191126it [01:09, 3438.49it/s]184151it [01:09, 3520.65it/s]177840it [01:09, 3398.66it/s]171759it [01:09, 3277.27it/s]191485it [01:09, 3482.04it/s]184505it [01:09, 3396.96it/s]178215it [01:09, 3500.23it/s]172126it [01:09, 3388.67it/s]191835it [01:09, 3400.73it/s]184879it [01:09, 3494.33it/s]178567it [01:09, 3374.67it/s]172467it [01:09, 3279.07it/s]192216it [01:10, 3518.18it/s]185230it [01:10, 3357.51it/s]178936it [01:09, 3464.14it/s]172822it [01:09, 3355.49it/s]192578it [01:10, 3433.18it/s]185590it [01:10, 3426.49it/s]179284it [01:10, 3345.08it/s]173190it [01:09, 3440.76it/s]192963it [01:10, 3551.48it/s]185935it [01:10, 3343.03it/s]179645it [01:10, 3418.31it/s]173536it [01:09, 3309.44it/s]193340it [01:10, 3614.94it/s]186312it [01:10, 3465.37it/s]179989it [01:10, 3317.63it/s]173895it [01:09, 3389.30it/s]193703it [01:10, 3481.33it/s]186698it [01:10, 3404.67it/s]180371it [01:10, 3459.87it/s]174236it [01:10, 3288.49it/s]194072it [01:10, 3540.06it/s]187078it [01:10, 3515.29it/s]180734it [01:10, 3508.94it/s]174607it [01:10, 3407.63it/s]194428it [01:10, 3439.94it/s]187455it [01:10, 3586.98it/s]181087it [01:10, 3413.89it/s]174950it [01:10, 3277.05it/s]194807it [01:10, 3538.30it/s]181448it [01:10, 3465.93it/s]187816it [01:10, 3435.35it/s]175321it [01:10, 3397.73it/s]195163it [01:10, 3486.98it/s]188190it [01:10, 3520.68it/s]181796it [01:10, 3363.16it/s]175673it [01:10, 3432.33it/s]195534it [01:10, 3550.62it/s]182154it [01:10, 3423.93it/s]188544it [01:10, 3416.41it/s]195908it [01:11, 3603.50it/s]176018it [01:10, 3302.21it/s]188913it [01:11, 3492.10it/s]182498it [01:10, 3321.08it/s]176376it [01:10, 3380.79it/s]196270it [01:11, 3471.72it/s]182874it [01:11, 3445.92it/s]189264it [01:11, 3378.67it/s]196656it [01:11, 3583.51it/s]176716it [01:10, 3277.42it/s]183242it [01:11, 3511.36it/s]189634it [01:11, 3468.03it/s]177074it [01:10, 3363.35it/s]197016it [01:11, 3452.62it/s]190007it [01:11, 3504.34it/s]183595it [01:11, 3393.20it/s]177438it [01:11, 3443.44it/s]197398it [01:11, 3556.82it/s]183965it [01:11, 3479.18it/s]190359it [01:11, 3402.69it/s]177784it [01:11, 3306.35it/s]197756it [01:11, 3448.19it/s]190728it [01:11, 3484.65it/s]184315it [01:11, 3386.35it/s]178158it [01:11, 3428.00it/s]198142it [01:11, 3563.30it/s]184689it [01:11, 3487.52it/s]191078it [01:11, 3366.13it/s]198501it [01:11, 3462.95it/s]178503it [01:11, 3268.80it/s]191450it [01:11, 3467.26it/s]185040it [01:11, 3365.57it/s]178868it [01:11, 3375.01it/s]185402it [01:11, 3437.58it/s]191799it [01:11, 3380.52it/s]179208it [01:11, 3266.47it/s]185773it [01:11, 3515.58it/s]192176it [01:12, 3490.67it/s]179572it [01:11, 3370.83it/s]192553it [01:12, 3570.18it/s]186126it [01:12, 3426.38it/s]179932it [01:11, 3436.30it/s]186492it [01:12, 3491.85it/s]192912it [01:12, 3426.84it/s]180278it [01:11, 3341.25it/s]186843it [01:12, 3421.47it/s]193284it [01:12, 3508.39it/s]180638it [01:11, 3414.47it/s]187212it [01:12, 3497.83it/s]193637it [01:12, 3373.34it/s]180981it [01:12, 3319.99it/s]187563it [01:12, 3359.52it/s]194025it [01:12, 3516.31it/s]181332it [01:12, 3373.89it/s]187943it [01:12, 3483.03it/s]194379it [01:12, 3416.75it/s]181671it [01:12, 3262.11it/s]188313it [01:12, 3544.00it/s]194749it [01:12, 3496.77it/s]182039it [01:12, 3379.79it/s]188669it [01:12, 3445.22it/s]195101it [01:12, 3437.22it/s]182403it [01:12, 3453.43it/s]189038it [01:12, 3513.51it/s]195472it [01:12, 3513.84it/s]182750it [01:12, 3348.69it/s]189391it [01:12, 3410.06it/s]195831it [01:13, 3535.16it/s]183113it [01:12, 3428.64it/s]189761it [01:13, 3490.88it/s]196186it [01:13, 3408.70it/s]183458it [01:12, 3322.96it/s]190112it [01:13, 3366.95it/s]196566it [01:13, 3520.73it/s]183827it [01:12, 3426.25it/s]190482it [01:13, 3460.54it/s]196920it [01:13, 3437.04it/s]184180it [01:13, 3280.35it/s]190854it [01:13, 3534.41it/s]197300it [01:13, 3540.01it/s]184552it [01:13, 3403.71it/s]191209it [01:13, 3419.25it/s]197656it [01:13, 3415.86it/s]184925it [01:13, 3497.34it/s]191579it [01:13, 3499.41it/s]198039it [01:13, 3532.19it/s]185277it [01:13, 3363.83it/s]191931it [01:13, 3418.98it/s]198413it [01:13, 3591.13it/s]185641it [01:13, 3440.62it/s]192299it [01:13, 3493.81it/s]185987it [01:13, 3338.14it/s]192650it [01:13, 3390.62it/s]186363it [01:13, 3456.79it/s]193028it [01:13, 3501.15it/s]186711it [01:13, 3330.59it/s]193400it [01:14, 3562.44it/s]187087it [01:13, 3450.41it/s]193758it [01:14, 3454.09it/s]187460it [01:13, 3530.62it/s]194136it [01:14, 3546.64it/s]187815it [01:14, 3398.33it/s]194492it [01:14, 3441.29it/s]188189it [01:14, 3495.86it/s]194867it [01:14, 3527.71it/s]188541it [01:14, 3390.15it/s]195222it [01:14, 3460.27it/s]188911it [01:14, 3476.57it/s]195586it [01:14, 3511.09it/s]189261it [01:14, 3357.22it/s]195939it [01:14, 3386.75it/s]189627it [01:14, 3442.82it/s]196312it [01:14, 3485.01it/s]189999it [01:14, 3521.58it/s]196694it [01:15, 3581.82it/s]190353it [01:14, 3389.55it/s]197054it [01:15, 3470.07it/s]190720it [01:14, 3469.35it/s]197431it [01:15, 3555.79it/s]191069it [01:15, 3347.87it/s]197788it [01:15, 3450.12it/s]191441it [01:15, 3453.36it/s]198159it [01:15, 3524.17it/s]191789it [01:15, 3346.14it/s]198513it [01:15, 3428.99it/s]192165it [01:15, 3463.16it/s]192542it [01:15, 3549.46it/s]192899it [01:15, 3435.63it/s]193275it [01:15, 3527.99it/s]193630it [01:15, 3357.67it/s]194015it [01:15, 3495.37it/s]194368it [01:16, 3363.75it/s]194738it [01:16, 3458.56it/s]195100it [01:16, 3381.23it/s]198849it [01:16, 232.36it/s] 195460it [01:16, 3442.28it/s]199227it [01:16, 327.12it/s]195816it [01:16, 3473.52it/s]199556it [01:16, 435.97it/s]196165it [01:16, 3336.85it/s]199926it [01:17, 598.77it/s]196534it [01:16, 3435.18it/s]200246it [01:17, 769.92it/s]196880it [01:16, 3332.09it/s]200616it [01:17, 1023.51it/s]197251it [01:16, 3439.88it/s]200987it [01:17, 1318.92it/s]197620it [01:16, 3324.79it/s]201331it [01:17, 1581.76it/s]201711it [01:17, 1937.00it/s]197955it [01:17, 2698.63it/s]202058it [01:17, 2196.34it/s]198317it [01:17, 2923.49it/s]202448it [01:17, 2548.96it/s]202803it [01:17, 2735.87it/s]203190it [01:18, 3011.54it/s]203550it [01:18, 3133.52it/s]203906it [01:18, 3120.76it/s]204274it [01:18, 3270.16it/s]204624it [01:18, 3233.79it/s]204997it [01:18, 3369.73it/s]205347it [01:18, 3301.69it/s]205717it [01:18, 3413.73it/s]206088it [01:18, 3345.34it/s]206467it [01:18, 3470.12it/s]206830it [01:19, 3513.64it/s]207185it [01:19, 3392.47it/s]198774it [01:19, 219.73it/s] 207557it [01:19, 3483.43it/s]199153it [01:19, 308.76it/s]207908it [01:19, 3409.68it/s]199471it [01:19, 407.51it/s]208297it [01:19, 3546.99it/s]199843it [01:19, 562.63it/s]208654it [01:19, 3455.38it/s]200195it [01:19, 748.09it/s]209037it [01:19, 3562.32it/s]200526it [01:19, 953.42it/s]209405it [01:19, 3596.50it/s]200894it [01:19, 1238.41it/s]209766it [01:19, 3457.96it/s]201232it [01:19, 1499.83it/s]210136it [01:20, 3525.31it/s]201607it [01:19, 1847.66it/s]210491it [01:20, 3417.74it/s]201950it [01:20, 2104.75it/s]210859it [01:20, 3490.49it/s]202331it [01:20, 2450.90it/s]202704it [01:20, 2737.40it/s]211210it [01:20, 3393.72it/s]211585it [01:20, 3493.65it/s]203060it [01:20, 2878.30it/s]211953it [01:20, 3546.25it/s]203429it [01:20, 3083.50it/s]212309it [01:20, 3442.50it/s]203783it [01:20, 3106.25it/s]212681it [01:20, 3522.41it/s]204157it [01:20, 3275.01it/s]213035it [01:20, 3426.15it/s]204509it [01:20, 3245.66it/s]213409it [01:20, 3516.20it/s]204885it [01:20, 3386.22it/s]205246it [01:20, 3449.43it/s]213762it [01:21, 3411.21it/s]214137it [01:21, 3502.85it/s]205601it [01:21, 3358.91it/s]198858it [01:21, 207.18it/s] 205979it [01:21, 3476.17it/s]214489it [01:21, 3375.05it/s]199231it [01:21, 292.45it/s]214862it [01:21, 3474.22it/s]206333it [01:21, 3393.56it/s]199561it [01:21, 392.96it/s]215240it [01:21, 3560.42it/s]206708it [01:21, 3495.05it/s]199927it [01:21, 541.80it/s]215598it [01:21, 3457.22it/s]207061it [01:21, 3396.05it/s]200244it [01:21, 701.67it/s]215967it [01:21, 3523.75it/s]207433it [01:21, 3488.25it/s]200603it [01:21, 933.24it/s]216321it [01:21, 3438.43it/s]207785it [01:21, 3363.55it/s]200973it [01:21, 1217.63it/s]216700it [01:21, 3539.23it/s]208170it [01:21, 3502.25it/s]201314it [01:21, 1482.35it/s]217056it [01:22, 3471.30it/s]208549it [01:21, 3582.87it/s]201688it [01:21, 1827.92it/s]217422it [01:22, 3524.27it/s]208910it [01:22, 3470.28it/s]202033it [01:22, 2092.59it/s]217816it [01:22, 3643.96it/s]209291it [01:22, 3565.99it/s]202426it [01:22, 2462.77it/s]218182it [01:22, 3555.15it/s]209650it [01:22, 3429.52it/s]202781it [01:22, 2652.91it/s]218562it [01:22, 3624.82it/s]210020it [01:22, 3504.24it/s]203152it [01:22, 2905.14it/s]218926it [01:22, 3537.55it/s]210373it [01:22, 3356.95it/s]203525it [01:22, 3113.70it/s]219321it [01:22, 3656.02it/s]210745it [01:22, 3457.41it/s]203885it [01:22, 3136.54it/s]219688it [01:22, 3552.46it/s]211112it [01:22, 3517.13it/s]204257it [01:22, 3292.91it/s]220073it [01:22, 3636.19it/s]211466it [01:22, 3396.40it/s]204612it [01:22, 3270.84it/s]220438it [01:22, 3566.00it/s]211840it [01:22, 3492.85it/s]204986it [01:22, 3399.39it/s]220827it [01:23, 3658.15it/s]212192it [01:23, 3376.60it/s]205340it [01:22, 3337.22it/s]221207it [01:23, 3545.38it/s]212558it [01:23, 3456.57it/s]205707it [01:23, 3431.08it/s]221603it [01:23, 3663.01it/s]212906it [01:23, 3323.60it/s]206074it [01:23, 3498.88it/s]221986it [01:23, 3710.80it/s]213278it [01:23, 3435.53it/s]206430it [01:23, 3421.70it/s]222359it [01:23, 3559.81it/s]213648it [01:23, 3335.48it/s]206802it [01:23, 3505.73it/s]198628it [01:23, 186.48it/s] 222743it [01:23, 3638.32it/s]214025it [01:23, 3455.96it/s]207156it [01:23, 3420.39it/s]199004it [01:23, 267.80it/s]223109it [01:23, 3480.05it/s]214394it [01:23, 3520.93it/s]207529it [01:23, 3509.59it/s]199368it [01:23, 372.37it/s]223483it [01:23, 3551.23it/s]214748it [01:23, 3389.12it/s]207883it [01:23, 3427.95it/s]199744it [01:23, 517.56it/s]223841it [01:23, 3449.13it/s]215105it [01:23, 3438.36it/s]208264it [01:23, 3538.05it/s]200064it [01:23, 672.32it/s]224220it [01:24, 3544.74it/s]215451it [01:23, 3348.76it/s]208620it [01:23, 3417.16it/s]200378it [01:23, 857.76it/s]224577it [01:24, 3440.42it/s]215822it [01:24, 3450.70it/s]209003it [01:23, 3535.02it/s]200744it [01:23, 1131.93it/s]224953it [01:24, 3531.55it/s]216169it [01:24, 3365.43it/s]209381it [01:24, 3603.54it/s]201071it [01:23, 1379.35it/s]225328it [01:24, 3594.45it/s]216548it [01:24, 3485.55it/s]209743it [01:24, 3468.64it/s]201436it [01:23, 1714.32it/s]225689it [01:24, 3444.58it/s]216932it [01:24, 3586.49it/s]210114it [01:24, 3537.51it/s]201811it [01:24, 2068.49it/s]226058it [01:24, 3513.35it/s]217292it [01:24, 3449.39it/s]210470it [01:24, 3430.55it/s]202157it [01:24, 2301.93it/s]226412it [01:24, 3405.39it/s]217664it [01:24, 3525.67it/s]210842it [01:24, 3512.34it/s]202542it [01:24, 2639.80it/s]226784it [01:24, 3492.91it/s]218019it [01:24, 3448.83it/s]211195it [01:24, 3378.43it/s]202893it [01:24, 2789.62it/s]227135it [01:24, 3382.58it/s]218397it [01:24, 3543.40it/s]211555it [01:24, 3438.96it/s]203236it [01:24, 2878.76it/s]227475it [01:24, 3374.10it/s]218753it [01:24, 3347.74it/s]211901it [01:24, 3036.63it/s]203570it [01:24, 2792.00it/s]227814it [01:25, 3306.23it/s]219091it [01:25, 3321.48it/s]203882it [01:24, 2610.40it/s]219428it [01:25, 3333.01it/s]228146it [01:25, 2957.77it/s]212214it [01:25, 2520.09it/s]204167it [01:24, 2624.41it/s]228472it [01:25, 3037.53it/s]219763it [01:25, 3121.79it/s]212539it [01:25, 2692.64it/s]204447it [01:24, 2624.22it/s]228782it [01:25, 2913.24it/s]220079it [01:25, 3016.21it/s]212826it [01:25, 2679.66it/s]204771it [01:25, 2787.85it/s]229079it [01:25, 2928.41it/s]213135it [01:25, 2785.13it/s]220384it [01:25, 2954.73it/s]205061it [01:25, 2736.65it/s]213457it [01:25, 2903.10it/s]220722it [01:25, 3071.24it/s]229376it [01:25, 2736.57it/s]221057it [01:25, 3148.15it/s]213756it [01:25, 2819.03it/s]205342it [01:25, 2505.31it/s]229654it [01:25, 2727.09it/s]205666it [01:25, 2698.38it/s]229979it [01:25, 2871.43it/s]214044it [01:25, 2818.19it/s]221374it [01:25, 3045.82it/s]230292it [01:25, 2943.03it/s]221681it [01:25, 3051.69it/s]205982it [01:25, 2808.18it/s]214330it [01:25, 2755.62it/s]222012it [01:25, 3124.11it/s]206270it [01:25, 2771.39it/s]230589it [01:26, 2853.60it/s]214609it [01:25, 2718.94it/s]206597it [01:25, 2909.84it/s]230918it [01:26, 2975.63it/s]214930it [01:25, 2858.06it/s]222326it [01:26, 2961.42it/s]206923it [01:25, 3008.10it/s]231230it [01:26, 3015.52it/s]215258it [01:26, 2972.89it/s]222628it [01:26, 2976.29it/s]207228it [01:25, 2918.15it/s]231534it [01:26, 2916.92it/s]215558it [01:26, 2895.62it/s]222928it [01:26, 2892.95it/s]207551it [01:26, 3006.16it/s]231857it [01:26, 3006.13it/s]215876it [01:26, 2976.83it/s]223253it [01:26, 2992.37it/s]223576it [01:26, 3060.77it/s]207854it [01:26, 2913.66it/s]216176it [01:26, 2896.50it/s]232160it [01:26, 2904.19it/s]208189it [01:26, 3030.12it/s]216505it [01:26, 3008.67it/s]232485it [01:26, 3003.00it/s]223884it [01:26, 2950.72it/s]208522it [01:26, 3116.84it/s]216836it [01:26, 3094.12it/s]232799it [01:26, 3040.18it/s]224214it [01:26, 3048.26it/s]224527it [01:26, 3069.57it/s]208836it [01:26, 3004.26it/s]233105it [01:26, 2958.61it/s]217147it [01:26, 3005.73it/s]233436it [01:27, 3058.13it/s]209169it [01:26, 3090.50it/s]217449it [01:26, 2929.15it/s]224836it [01:26, 2950.42it/s]233762it [01:27, 3116.15it/s]217792it [01:26, 3072.12it/s]225160it [01:27, 3032.17it/s]209480it [01:26, 2851.80it/s]234075it [01:27, 2990.84it/s]218101it [01:27, 2954.34it/s]225465it [01:27, 2920.65it/s]209770it [01:26, 2779.34it/s]234400it [01:27, 3065.20it/s]218415it [01:27, 3004.68it/s]225785it [01:27, 2998.09it/s]210090it [01:26, 2894.24it/s]226103it [01:27, 3050.17it/s]218717it [01:27, 2967.97it/s]234708it [01:27, 2943.14it/s]210383it [01:27, 2687.20it/s]219052it [01:27, 3076.07it/s]235024it [01:27, 3002.80it/s]226410it [01:27, 2872.96it/s]210706it [01:27, 2834.04it/s]219393it [01:27, 3171.50it/s]235348it [01:27, 3063.77it/s]226726it [01:27, 2951.62it/s]211026it [01:27, 2936.06it/s]235656it [01:27, 3031.56it/s]219712it [01:27, 3064.85it/s]227087it [01:27, 3140.49it/s]211324it [01:27, 2922.94it/s]236023it [01:27, 3216.48it/s]220095it [01:27, 3284.41it/s]227404it [01:27, 3092.39it/s]211673it [01:27, 3083.93it/s]236346it [01:27, 3184.58it/s]220435it [01:27, 3318.04it/s]227770it [01:27, 3255.20it/s]211984it [01:27, 3048.96it/s]236716it [01:28, 3334.13it/s]220808it [01:27, 3437.43it/s]228098it [01:27, 3193.44it/s]212354it [01:27, 3236.44it/s]237090it [01:28, 3452.60it/s]221197it [01:27, 3571.24it/s]228467it [01:28, 3337.15it/s]212724it [01:27, 3371.61it/s]221556it [01:28, 3513.06it/s]237437it [01:28, 3366.35it/s]228803it [01:28, 3248.03it/s]213063it [01:27, 3267.05it/s]221944it [01:28, 3619.11it/s]237797it [01:28, 3432.76it/s]229166it [01:28, 3358.41it/s]213429it [01:27, 3379.60it/s]238142it [01:28, 3349.54it/s]222307it [01:28, 3500.39it/s]229545it [01:28, 3483.05it/s]213769it [01:28, 3275.61it/s]238507it [01:28, 3435.96it/s]222683it [01:28, 3573.87it/s]229895it [01:28, 3376.28it/s]214139it [01:28, 3390.11it/s]238852it [01:28, 3347.93it/s]223042it [01:28, 3467.68it/s]230262it [01:28, 3460.61it/s]214489it [01:28, 3280.19it/s]239222it [01:28, 3447.47it/s]223406it [01:28, 3515.97it/s]230610it [01:28, 3347.27it/s]214853it [01:28, 3382.42it/s]239591it [01:28, 3518.29it/s]223759it [01:28, 3423.06it/s]230987it [01:28, 3466.92it/s]215223it [01:28, 3473.96it/s]239944it [01:28, 3413.16it/s]224144it [01:28, 3544.11it/s]231336it [01:28, 3346.30it/s]215572it [01:28, 3361.42it/s]240304it [01:29, 3465.03it/s]224517it [01:28, 3595.51it/s]231708it [01:29, 3443.97it/s]215941it [01:28, 3453.38it/s]240652it [01:29, 3370.61it/s]224878it [01:29, 3473.16it/s]232077it [01:29, 3514.88it/s]216288it [01:28, 3346.18it/s]241022it [01:29, 3464.57it/s]225254it [01:29, 3555.62it/s]232430it [01:29, 3392.87it/s]216659it [01:28, 3441.97it/s]241370it [01:29, 3383.52it/s]225611it [01:29, 3443.34it/s]232800it [01:29, 3478.38it/s]217009it [01:28, 3365.18it/s]241746it [01:29, 3491.91it/s]225974it [01:29, 3496.19it/s]233150it [01:29, 3381.40it/s]217363it [01:29, 3414.95it/s]242121it [01:29, 3565.14it/s]226325it [01:29, 3373.91it/s]233517it [01:29, 3463.05it/s]217751it [01:29, 3550.07it/s]242479it [01:29, 3455.76it/s]226695it [01:29, 3465.29it/s]233865it [01:29, 3358.39it/s]218108it [01:29, 3426.61it/s]242838it [01:29, 3488.33it/s]227062it [01:29, 3523.18it/s]234237it [01:29, 3459.73it/s]218479it [01:29, 3507.52it/s]243188it [01:29, 3396.72it/s]227416it [01:29, 3401.12it/s]234602it [01:29, 3514.07it/s]218832it [01:29, 3204.77it/s]243560it [01:30, 3488.62it/s]227783it [01:29, 3475.79it/s]234955it [01:29, 3386.88it/s]219159it [01:29, 3187.67it/s]243910it [01:30, 3405.00it/s]228132it [01:29, 3366.92it/s]235326it [01:30, 3478.79it/s]219529it [01:29, 3179.65it/s]244280it [01:30, 3489.70it/s]228499it [01:30, 3452.71it/s]235676it [01:30, 3358.63it/s]219910it [01:29, 3352.51it/s]244651it [01:30, 3552.49it/s]228846it [01:30, 3328.96it/s]236038it [01:30, 3432.24it/s]220285it [01:29, 3462.63it/s]245008it [01:30, 3437.16it/s]229207it [01:30, 3406.61it/s]236383it [01:30, 3313.13it/s]245379it [01:30, 3515.18it/s]229591it [01:30, 3529.67it/s]220635it [01:30, 3101.54it/s]236750it [01:30, 3414.80it/s]245732it [01:30, 3379.59it/s]220954it [01:30, 3093.22it/s]229946it [01:30, 3420.53it/s]237118it [01:30, 3489.39it/s]246102it [01:30, 3469.93it/s]230310it [01:30, 3481.58it/s]221270it [01:30, 2878.21it/s]237469it [01:30, 3362.52it/s]246451it [01:30, 3362.50it/s]230660it [01:30, 3367.62it/s]221598it [01:30, 2981.74it/s]237831it [01:30, 3434.04it/s]246823it [01:30, 3463.52it/s]231039it [01:30, 3488.18it/s]238176it [01:30, 3337.03it/s]221902it [01:30, 2832.13it/s]247192it [01:31, 3527.74it/s]231390it [01:30, 3363.40it/s]238531it [01:31, 3396.18it/s]222190it [01:30, 2745.55it/s]247547it [01:31, 3392.38it/s]231759it [01:31, 3455.49it/s]238872it [01:31, 3293.97it/s]222551it [01:30, 2979.28it/s]247917it [01:31, 3480.17it/s]232126it [01:31, 3517.49it/s]239235it [01:31, 3387.90it/s]222889it [01:30, 2994.10it/s]248267it [01:31, 3237.01it/s]232480it [01:31, 3372.52it/s]239576it [01:31, 3237.88it/s]223260it [01:30, 3193.58it/s]248628it [01:31, 3339.22it/s]232848it [01:31, 3459.78it/s]223625it [01:31, 3322.18it/s]239902it [01:31, 3184.13it/s]233196it [01:31, 3385.13it/s]248966it [01:31, 3181.92it/s]240268it [01:31, 3317.47it/s]223961it [01:31, 3232.58it/s]233570it [01:31, 3484.72it/s]249337it [01:31, 3327.18it/s]224336it [01:31, 3380.55it/s]240602it [01:31, 3215.55it/s]249703it [01:31, 3420.45it/s]233920it [01:31, 3392.57it/s]240971it [01:31, 3349.76it/s]224677it [01:31, 3273.72it/s]234295it [01:31, 3494.25it/s]250049it [01:31, 3333.72it/s]241347it [01:31, 3466.94it/s]225048it [01:31, 3396.42it/s]234646it [01:31, 3496.39it/s]250426it [01:32, 3458.20it/s]241696it [01:31, 3360.43it/s]225409it [01:31, 3295.64it/s]234997it [01:31, 3397.37it/s]250775it [01:32, 3335.49it/s]242068it [01:32, 3453.90it/s]225781it [01:31, 3414.96it/s]235366it [01:32, 3479.46it/s]251147it [01:32, 3444.04it/s]242415it [01:32, 3361.61it/s]226146it [01:31, 3481.67it/s]235716it [01:32, 3369.66it/s]251494it [01:32, 3336.23it/s]242780it [01:32, 3443.39it/s]226497it [01:31, 3354.41it/s]236085it [01:32, 3459.91it/s]251864it [01:32, 3438.49it/s]243126it [01:32, 3317.13it/s]226862it [01:32, 3437.45it/s]252237it [01:32, 3522.12it/s]236433it [01:32, 3357.53it/s]243498it [01:32, 3431.07it/s]227208it [01:32, 3314.53it/s]236803it [01:32, 3455.54it/s]252591it [01:32, 3387.24it/s]243871it [01:32, 3515.38it/s]227573it [01:32, 3409.28it/s]252963it [01:32, 3481.56it/s]237168it [01:32, 3357.88it/s]244225it [01:32, 3389.13it/s]227929it [01:32, 3290.09it/s]237519it [01:32, 3399.96it/s]253313it [01:32, 3338.41it/s]244588it [01:32, 3453.44it/s]228284it [01:32, 3361.27it/s]237885it [01:32, 3472.83it/s]253683it [01:33, 3441.13it/s]244935it [01:32, 3352.44it/s]228654it [01:32, 3456.56it/s]238234it [01:32, 3366.08it/s]254030it [01:33, 3336.97it/s]245298it [01:33, 3431.42it/s]229002it [01:32, 3317.98it/s]238600it [01:33, 3449.58it/s]254396it [01:33, 3428.41it/s]245643it [01:33, 3308.06it/s]229378it [01:32, 3442.45it/s]238947it [01:33, 3350.62it/s]254766it [01:33, 3504.60it/s]246011it [01:33, 3412.43it/s]229725it [01:32, 3335.25it/s]239314it [01:33, 3440.69it/s]255118it [01:33, 3381.33it/s]246378it [01:33, 3485.05it/s]230091it [01:32, 3425.38it/s]239683it [01:33, 3511.96it/s]255485it [01:33, 3462.41it/s]246728it [01:33, 3364.64it/s]230449it [01:33, 3293.91it/s]240036it [01:33, 3384.02it/s]247095it [01:33, 3452.15it/s]230818it [01:33, 3404.59it/s]240399it [01:33, 3454.59it/s]247442it [01:33, 3338.04it/s]231194it [01:33, 3504.71it/s]240746it [01:33, 3316.11it/s]247797it [01:33, 3396.37it/s]231547it [01:33, 3369.13it/s]241116it [01:33, 3423.53it/s]248139it [01:33, 3293.22it/s]231918it [01:33, 3464.25it/s]241461it [01:33, 3338.94it/s]248507it [01:34, 3403.35it/s]232267it [01:33, 3164.45it/s]241835it [01:33, 3453.10it/s]248873it [01:34, 3477.76it/s]232632it [01:33, 3296.41it/s]242204it [01:34, 3520.44it/s]249223it [01:34, 3374.07it/s]232969it [01:33, 3220.78it/s]242558it [01:34, 3405.16it/s]249591it [01:34, 3460.45it/s]233344it [01:33, 3368.79it/s]242926it [01:34, 3482.57it/s]249939it [01:34, 3362.44it/s]233701it [01:34, 3426.23it/s]243276it [01:34, 3390.96it/s]250302it [01:34, 3438.05it/s]234047it [01:34, 3322.58it/s]243632it [01:34, 3437.91it/s]250648it [01:34, 3360.94it/s]234416it [01:34, 3427.18it/s]243977it [01:34, 3358.71it/s]251023it [01:34, 3471.33it/s]234761it [01:34, 3305.68it/s]244344it [01:34, 3447.29it/s]251389it [01:34, 3524.26it/s]235132it [01:34, 3419.64it/s]244713it [01:34, 3515.66it/s]251743it [01:34, 3401.18it/s]235489it [01:34, 3295.91it/s]245066it [01:34, 3410.95it/s]252117it [01:35, 3498.13it/s]235855it [01:34, 3398.51it/s]245432it [01:35, 3482.20it/s]252469it [01:35, 3380.35it/s]236223it [01:34, 3478.96it/s]245782it [01:35, 3385.13it/s]252828it [01:35, 3439.16it/s]236573it [01:34, 3341.89it/s]246150it [01:35, 3468.45it/s]253174it [01:35, 3337.01it/s]236937it [01:35, 3425.19it/s]246498it [01:35, 3366.08it/s]253546it [01:35, 3445.88it/s]246853it [01:35, 3418.62it/s]237282it [01:35, 3262.97it/s]253912it [01:35, 3507.48it/s]247222it [01:35, 3496.44it/s]237648it [01:35, 3372.41it/s]254264it [01:35, 3376.74it/s]247573it [01:35, 3392.75it/s]238009it [01:35, 3269.94it/s]254632it [01:35, 3463.01it/s]247942it [01:35, 3477.58it/s]238373it [01:35, 3371.67it/s]254980it [01:35, 3320.13it/s]248291it [01:35, 3372.17it/s]238739it [01:35, 3452.98it/s]255346it [01:35, 3414.66it/s]248656it [01:35, 3450.97it/s]239087it [01:35, 3321.93it/s]255690it [01:36, 3324.14it/s]249003it [01:36, 3372.68it/s]239439it [01:35, 3377.95it/s]249360it [01:36, 3427.42it/s]239779it [01:35, 3279.50it/s]249730it [01:36, 3505.47it/s]240145it [01:35, 3387.23it/s]250082it [01:36, 3415.29it/s]240493it [01:36, 3413.52it/s]250459it [01:36, 3515.70it/s]240836it [01:36, 3255.91it/s]250812it [01:36, 3422.30it/s]241204it [01:36, 3375.87it/s]251182it [01:36, 3500.53it/s]241544it [01:36, 3282.34it/s]251534it [01:36, 3396.63it/s]241918it [01:36, 3411.20it/s]251902it [01:36, 3476.54it/s]242261it [01:36, 3300.40it/s]252265it [01:36, 3520.01it/s]242629it [01:36, 3400.41it/s]252618it [01:37, 3408.34it/s]242996it [01:36, 3476.09it/s]252990it [01:37, 3495.70it/s]243346it [01:36, 3355.33it/s]253341it [01:37, 3402.26it/s]243716it [01:37, 3453.42it/s]253714it [01:37, 3495.53it/s]244063it [01:37, 3173.23it/s]254065it [01:37, 3402.08it/s]244427it [01:37, 3300.26it/s]254435it [01:37, 3486.87it/s]244762it [01:37, 3220.25it/s]254807it [01:37, 3552.44it/s]245118it [01:37, 3315.49it/s]255164it [01:37, 3431.82it/s]245486it [01:37, 3419.18it/s]255519it [01:37, 3465.75it/s]245831it [01:37, 3300.37it/s]246199it [01:37, 3402.39it/s]246542it [01:37, 3299.64it/s]246908it [01:38, 3402.08it/s]247251it [01:38, 3295.17it/s]247609it [01:38, 3375.27it/s]247964it [01:38, 3425.37it/s]248308it [01:38, 3307.90it/s]248677it [01:38, 3415.54it/s]249021it [01:38, 3311.89it/s]249391it [01:38, 3420.66it/s]249758it [01:38, 3492.63it/s]250109it [01:38, 3370.32it/s]250481it [01:39, 3468.34it/s]250830it [01:39, 3329.69it/s]251180it [01:39, 3377.42it/s]251520it [01:39, 3265.56it/s]251888it [01:39, 3383.41it/s]252262it [01:39, 3485.39it/s]252613it [01:39, 3348.12it/s]252984it [01:39, 3451.45it/s]253332it [01:39, 3334.54it/s]253698it [01:40, 3426.92it/s]254043it [01:40, 3313.76it/s]255833it [01:40, 161.83it/s] 254411it [01:40, 3416.02it/s]256206it [01:40, 229.62it/s]254774it [01:40, 3475.70it/s]256556it [01:40, 315.27it/s]255124it [01:40, 3170.26it/s]256933it [01:40, 440.55it/s]255490it [01:40, 3302.74it/s]257305it [01:41, 602.14it/s]257639it [01:41, 778.87it/s]258017it [01:41, 1035.59it/s]258360it [01:41, 1279.78it/s]258735it [01:41, 1608.39it/s]259080it [01:41, 1879.21it/s]259454it [01:41, 2221.13it/s]259831it [01:41, 2542.29it/s]260188it [01:41, 2712.22it/s]260560it [01:42, 2955.42it/s]260915it [01:42, 2996.14it/s]261296it [01:42, 3207.99it/s]261649it [01:42, 3211.95it/s]262024it [01:42, 3358.86it/s]262399it [01:42, 3467.59it/s]262759it [01:42, 3389.84it/s]263136it [01:42, 3491.34it/s]263493it [01:42, 3396.36it/s]263862it [01:42, 3477.77it/s]264214it [01:43, 3402.63it/s]264592it [01:43, 3508.94it/s]264956it [01:43, 3405.39it/s]265335it [01:43, 3513.45it/s]256025it [01:43, 156.19it/s] 265714it [01:43, 3592.88it/s]256400it [01:43, 223.77it/s]256708it [01:43, 298.49it/s]266076it [01:43, 3463.04it/s]257070it [01:43, 418.13it/s]266441it [01:43, 3516.31it/s]257396it [01:43, 555.59it/s]266795it [01:43, 3421.60it/s]257766it [01:43, 760.48it/s]267173it [01:43, 3522.83it/s]258143it [01:43, 1016.62it/s]267527it [01:44, 3425.86it/s]258485it [01:44, 1265.69it/s]267903it [01:44, 3521.79it/s]258862it [01:44, 1598.90it/s]268283it [01:44, 3600.40it/s]259209it [01:44, 1858.23it/s]268645it [01:44, 3477.45it/s]259576it [01:44, 2188.69it/s]269006it [01:44, 3513.05it/s]259921it [01:44, 2399.98it/s]269359it [01:44, 3398.73it/s]260295it [01:44, 2700.81it/s]269731it [01:44, 3488.36it/s]255867it [01:44, 175.87it/s] 260666it [01:44, 2945.29it/s]256240it [01:44, 249.39it/s]270082it [01:44, 3394.20it/s]261020it [01:44, 3008.82it/s]256556it [01:44, 332.90it/s]270459it [01:44, 3500.79it/s]261393it [01:44, 3197.82it/s]256931it [01:44, 467.47it/s]270833it [01:44, 3568.19it/s]257299it [01:44, 638.57it/s]261745it [01:44, 3162.00it/s]271191it [01:45, 3421.47it/s]257632it [01:44, 825.13it/s]262118it [01:45, 3315.44it/s]271551it [01:45, 3472.30it/s]258007it [01:45, 1092.29it/s]262467it [01:45, 3258.87it/s]271900it [01:45, 3375.83it/s]258349it [01:45, 1341.64it/s]262841it [01:45, 3391.60it/s]272270it [01:45, 3466.51it/s]258723it [01:45, 1676.88it/s]263210it [01:45, 3476.60it/s]272619it [01:45, 3348.96it/s]259076it [01:45, 1952.27it/s]263565it [01:45, 3360.07it/s]272990it [01:45, 3450.61it/s]259446it [01:45, 2282.81it/s]263941it [01:45, 3473.76it/s]273356it [01:45, 3342.54it/s]259821it [01:45, 2595.26it/s]264293it [01:45, 3352.71it/s]273730it [01:45, 3452.30it/s]260176it [01:45, 2750.67it/s]264666it [01:45, 3459.84it/s]274105it [01:45, 3536.80it/s]260521it [01:45, 2908.87it/s]265016it [01:45, 3357.53it/s]274461it [01:46, 3376.23it/s]260864it [01:45, 2982.84it/s]265389it [01:46, 3461.84it/s]274828it [01:46, 3457.91it/s]261241it [01:46, 3191.38it/s]265764it [01:46, 3545.16it/s]275176it [01:46, 3355.98it/s]261596it [01:46, 3193.48it/s]266121it [01:46, 3422.67it/s]275551it [01:46, 3466.35it/s]261974it [01:46, 3354.11it/s]266484it [01:46, 3482.07it/s]275900it [01:46, 3358.33it/s]262346it [01:46, 3457.56it/s]266834it [01:46, 3385.53it/s]276271it [01:46, 3457.52it/s]262704it [01:46, 3379.34it/s]267207it [01:46, 3482.06it/s]276642it [01:46, 3528.74it/s]263077it [01:46, 3477.30it/s]267557it [01:46, 3384.98it/s]276997it [01:46, 3368.66it/s]263432it [01:46, 3348.85it/s]267936it [01:46, 3498.99it/s]277370it [01:46, 3471.44it/s]263808it [01:46, 3463.99it/s]268313it [01:46, 3575.12it/s]277720it [01:46, 3353.18it/s]264159it [01:46, 3387.13it/s]268672it [01:46, 3444.07it/s]278092it [01:47, 3456.98it/s]264536it [01:46, 3491.89it/s]269032it [01:47, 3487.91it/s]278440it [01:47, 3350.37it/s]264911it [01:47, 3564.41it/s]269383it [01:47, 3382.05it/s]278812it [01:47, 3455.70it/s]265270it [01:47, 3421.50it/s]269759it [01:47, 3489.08it/s]279181it [01:47, 3523.44it/s]265650it [01:47, 3529.69it/s]270110it [01:47, 3403.82it/s]279535it [01:47, 3369.52it/s]266006it [01:47, 3426.53it/s]270484it [01:47, 3498.29it/s]279904it [01:47, 3458.57it/s]266384it [01:47, 3526.02it/s]270837it [01:47, 3381.37it/s]280252it [01:47, 3342.60it/s]266739it [01:47, 3432.18it/s]271205it [01:47, 3466.30it/s]280623it [01:47, 3444.95it/s]267118it [01:47, 3534.23it/s]271563it [01:47, 3499.17it/s]280970it [01:47, 3347.88it/s]267476it [01:47, 3392.75it/s]271915it [01:47, 3392.23it/s]281336it [01:48, 3435.93it/s]267855it [01:47, 3503.69it/s]272285it [01:48, 3478.89it/s]281703it [01:48, 3503.09it/s]268235it [01:48, 3588.72it/s]272635it [01:48, 3364.60it/s]282055it [01:48, 3349.10it/s]268596it [01:48, 3470.87it/s]273007it [01:48, 3457.89it/s]282424it [01:48, 3445.61it/s]268970it [01:48, 3547.75it/s]273357it [01:48, 3346.45it/s]282771it [01:48, 3335.99it/s]269327it [01:48, 3442.27it/s]273718it [01:48, 3420.63it/s]255826it [01:48, 151.49it/s] 283138it [01:48, 3429.70it/s]269687it [01:48, 3486.64it/s]274093it [01:48, 3514.83it/s]256195it [01:48, 215.44it/s]283483it [01:48, 3318.73it/s]270037it [01:48, 3411.52it/s]274446it [01:48, 3394.22it/s]256550it [01:48, 299.34it/s]283855it [01:48, 3432.58it/s]270416it [01:48, 3512.28it/s]274816it [01:48, 3481.74it/s]256850it [01:48, 390.81it/s]284221it [01:48, 3498.39it/s]270790it [01:48, 3577.66it/s]275166it [01:48, 3374.91it/s]257204it [01:48, 538.74it/s]284573it [01:48, 3380.44it/s]271149it [01:48, 3462.90it/s]275538it [01:48, 3472.35it/s]257511it [01:48, 695.18it/s]284934it [01:49, 3444.02it/s]271522it [01:48, 3539.00it/s]275887it [01:49, 3360.68it/s]257878it [01:48, 936.78it/s]285280it [01:49, 3336.69it/s]271878it [01:49, 3395.98it/s]276246it [01:49, 3424.05it/s]258237it [01:48, 1194.25it/s]285652it [01:49, 3444.91it/s]272247it [01:49, 3479.86it/s]276616it [01:49, 3503.36it/s]258594it [01:48, 1498.74it/s]285999it [01:49, 3347.38it/s]272597it [01:49, 3391.05it/s]276968it [01:49, 3387.83it/s]258954it [01:49, 1823.80it/s]286373it [01:49, 3459.10it/s]272971it [01:49, 3489.01it/s]277341it [01:49, 3484.03it/s]259292it [01:49, 2053.74it/s]286743it [01:49, 3528.31it/s]273341it [01:49, 3549.64it/s]277691it [01:49, 3372.60it/s]259648it [01:49, 2356.29it/s]287098it [01:49, 3425.32it/s]287112it [01:49, 2616.77it/s]
273698it [01:49, 3452.02it/s]278060it [01:49, 3462.30it/s]259983it [01:49, 2375.24it/s]274056it [01:49, 3481.56it/s]278408it [01:49, 3318.28it/s]260347it [01:49, 2661.75it/s]274406it [01:49, 3391.08it/s]278782it [01:49, 3435.64it/s]260719it [01:49, 2920.41it/s]274779it [01:49, 3487.02it/s]279153it [01:50, 3513.64it/s]261058it [01:49, 2948.36it/s]275129it [01:50, 3397.20it/s]279507it [01:50, 3396.64it/s]261422it [01:49, 3129.68it/s]275505it [01:50, 3500.91it/s]279873it [01:50, 3470.60it/s]261761it [01:49, 3100.60it/s]275876it [01:50, 3406.65it/s]280222it [01:50, 3348.51it/s]262119it [01:49, 3231.97it/s]276246it [01:50, 3489.70it/s]280591it [01:50, 3443.69it/s]262456it [01:50, 3160.22it/s]276601it [01:50, 3505.46it/s]280938it [01:50, 3316.11it/s]262828it [01:50, 3315.71it/s]276953it [01:50, 3409.41it/s]281309it [01:50, 3427.62it/s]263188it [01:50, 3396.75it/s]277326it [01:50, 3500.81it/s]281676it [01:50, 3495.36it/s]263534it [01:50, 3272.22it/s]277678it [01:50, 3401.12it/s]282028it [01:50, 3376.72it/s]263867it [01:50, 3286.59it/s]278047it [01:50, 3484.22it/s]282392it [01:50, 3451.45it/s]264200it [01:50, 3230.11it/s]278397it [01:50, 3377.92it/s]282739it [01:51, 3335.38it/s]264561it [01:50, 3336.97it/s]278758it [01:51, 3443.70it/s]283107it [01:51, 3433.37it/s]264933it [01:50, 3447.88it/s]279131it [01:51, 3525.84it/s]283452it [01:51, 3283.89it/s]265280it [01:50, 3314.53it/s]279485it [01:51, 3416.24it/s]283822it [01:51, 3399.87it/s]265645it [01:51, 3410.04it/s]279854it [01:51, 3493.02it/s]284190it [01:51, 3479.65it/s]265988it [01:51, 3287.35it/s]280205it [01:51, 3379.23it/s]284540it [01:51, 3365.66it/s]266352it [01:51, 3386.37it/s]280559it [01:51, 3424.50it/s]284899it [01:51, 3428.73it/s]266693it [01:51, 3269.43it/s]280904it [01:51, 3430.70it/s]285244it [01:51, 3224.83it/s]267065it [01:51, 3396.58it/s]281248it [01:51, 3279.94it/s]285596it [01:51, 3306.71it/s]267428it [01:51, 3463.25it/s]281613it [01:51, 3385.22it/s]285957it [01:52, 3237.80it/s]267776it [01:51, 3289.46it/s]281954it [01:52, 3302.15it/s]286328it [01:52, 3368.01it/s]268140it [01:51, 3388.15it/s]282311it [01:52, 3378.53it/s]286702it [01:52, 3472.07it/s]268482it [01:51, 3285.35it/s]282651it [01:52, 3296.87it/s]287052it [01:52, 3373.98it/s]287112it [01:52, 2554.70it/s]
268844it [01:52, 3380.36it/s]283016it [01:52, 3394.79it/s]269185it [01:52, 3257.96it/s]283385it [01:52, 3480.10it/s]269558it [01:52, 3391.95it/s]283735it [01:52, 3387.85it/s]269924it [01:52, 3467.29it/s]284093it [01:52, 3442.99it/s]270273it [01:52, 3329.77it/s]284439it [01:52, 3361.72it/s]270628it [01:52, 3392.22it/s]284810it [01:52, 3461.55it/s]270970it [01:52, 3273.41it/s]285158it [01:52, 3368.16it/s]271300it [01:52, 3279.50it/s]285530it [01:53, 3467.89it/s]271670it [01:52, 3400.15it/s]285886it [01:53, 3490.36it/s]272012it [01:52, 3304.50it/s]286236it [01:53, 3395.76it/s]272367it [01:53, 3373.87it/s]286614it [01:53, 3507.10it/s]272706it [01:53, 3245.87it/s]286966it [01:53, 3422.52it/s]287112it [01:53, 2529.63it/s]
2022-07-04 15:24:18 | INFO | root | success load 287112 data
2022-07-04 15:24:18 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-uncased' is a path or url to a directory containing tokenizer files.
2022-07-04 15:24:18 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-uncased/added_tokens.json. We won't load it.
2022-07-04 15:24:18 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-uncased/special_tokens_map.json. We won't load it.
2022-07-04 15:24:18 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-uncased/vocab.txt
2022-07-04 15:24:18 | INFO | transformer.tokenization_utils | loading file None
2022-07-04 15:24:18 | INFO | transformer.tokenization_utils | loading file None
2022-07-04 15:24:18 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-uncased/tokenizer_config.json
273064it [01:53, 3339.91it/s]273400it [01:53, 3221.95it/s]273776it [01:53, 3373.89it/s]274132it [01:53, 3425.40it/s]274477it [01:53, 3296.23it/s]274826it [01:53, 3350.35it/s]275163it [01:53, 3204.77it/s]275525it [01:54, 3321.29it/s]275878it [01:54, 3161.09it/s]276228it [01:54, 3246.27it/s]276584it [01:54, 3335.08it/s]276920it [01:54, 3232.91it/s]277278it [01:54, 3325.50it/s]277613it [01:54, 3222.55it/s]277973it [01:54, 3328.50it/s]278336it [01:54, 3415.62it/s]278680it [01:54, 3088.19it/s]279051it [01:55, 3257.44it/s]279384it [01:55, 3166.44it/s]279740it [01:55, 3276.00it/s]280078it [01:55, 3174.84it/s]280432it [01:55, 3275.31it/s]280798it [01:55, 3385.13it/s]281140it [01:55, 3249.86it/s]281497it [01:55, 3339.13it/s]281834it [01:55, 3215.83it/s]282167it [01:56, 3246.07it/s]282532it [01:56, 3360.74it/s]282870it [01:56, 3265.40it/s]283224it [01:56, 3344.06it/s]283560it [01:56, 3217.16it/s]283917it [01:56, 3315.58it/s]284276it [01:56, 3394.53it/s]284617it [01:56, 3257.99it/s]284984it [01:56, 3373.50it/s]285324it [01:57, 3256.73it/s]285662it [01:57, 3290.37it/s]285993it [01:57, 3181.21it/s]286353it [01:57, 3299.30it/s]286713it [01:57, 3384.74it/s]287053it [01:57, 3275.69it/s]287112it [01:57, 2442.45it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-04 15:29:17 | INFO | train_inner | epoch 001:    100 / 1122 loss=nan, nll_loss=12.001, mask_ins=7.584, word_ins_ml=12.491, word_reposition=5.754, kpe=nan, ppl=nan, wps=6880.3, ups=0.34, wpb=19974.3, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=27.716, clip=27, loss_scale=128, train_wall=258, wall=412
2022-07-04 15:34:05 | INFO | train_inner | epoch 001:    200 / 1122 loss=22.947, nll_loss=10.972, mask_ins=4.73, word_ins_ml=11.585, word_reposition=5.013, kpe=1.619, ppl=8.08821e+06, wps=6931.8, ups=0.35, wpb=19964.4, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=19.594, clip=0, loss_scale=128, train_wall=250, wall=700
2022-07-04 15:38:51 | INFO | train_inner | epoch 001:    300 / 1122 loss=nan, nll_loss=11.199, mask_ins=2.436, word_ins_ml=11.779, word_reposition=3.533, kpe=nan, ppl=nan, wps=6948.5, ups=0.35, wpb=19922, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=7.028, clip=0, loss_scale=128, train_wall=249, wall=987
2022-07-04 15:43:38 | INFO | train_inner | epoch 001:    400 / 1122 loss=16.921, nll_loss=11.025, mask_ins=2.009, word_ins_ml=11.622, word_reposition=2.048, kpe=1.242, ppl=124085, wps=6957.9, ups=0.35, wpb=19941.1, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=4.287, clip=0, loss_scale=128, train_wall=249, wall=1274
2022-07-04 15:48:25 | INFO | train_inner | epoch 001:    500 / 1122 loss=16.025, nll_loss=10.747, mask_ins=1.867, word_ins_ml=11.378, word_reposition=1.604, kpe=1.176, ppl=66674.4, wps=6966.5, ups=0.35, wpb=19970, bsz=256, num_updates=500, lr=5.009e-05, gnorm=3.353, clip=0, loss_scale=128, train_wall=249, wall=1560
2022-07-04 15:53:24 | INFO | train_inner | epoch 001:    600 / 1122 loss=15.717, nll_loss=10.608, mask_ins=1.849, word_ins_ml=11.259, word_reposition=1.475, kpe=1.134, ppl=53859.5, wps=6644.1, ups=0.33, wpb=19904.3, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=3.012, clip=0, loss_scale=242, train_wall=262, wall=1860
2022-07-04 15:58:12 | INFO | train_inner | epoch 001:    700 / 1122 loss=15.547, nll_loss=10.524, mask_ins=1.835, word_ins_ml=11.187, word_reposition=1.416, kpe=1.109, ppl=47870.5, wps=6924.9, ups=0.35, wpb=19941.3, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=2.849, clip=0, loss_scale=256, train_wall=250, wall=2148
2022-07-04 16:02:59 | INFO | train_inner | epoch 001:    800 / 1122 loss=15.396, nll_loss=10.436, mask_ins=1.84, word_ins_ml=11.113, word_reposition=1.356, kpe=1.088, ppl=43126.6, wps=6939.1, ups=0.35, wpb=19916.1, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=2.602, clip=0, loss_scale=256, train_wall=249, wall=2435
2022-07-04 16:07:47 | INFO | train_inner | epoch 001:    900 / 1122 loss=15.305, nll_loss=10.37, mask_ins=1.845, word_ins_ml=11.058, word_reposition=1.331, kpe=1.072, ppl=40489.8, wps=6900.5, ups=0.35, wpb=19826.8, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=2.359, clip=0, loss_scale=256, train_wall=249, wall=2722
2022-07-04 16:12:33 | INFO | train_inner | epoch 001:   1000 / 1122 loss=15.196, nll_loss=10.296, mask_ins=1.833, word_ins_ml=10.996, word_reposition=1.31, kpe=1.057, ppl=37527.7, wps=6967.8, ups=0.35, wpb=19960.1, bsz=256, num_updates=1000, lr=0.00010008, gnorm=2.367, clip=0, loss_scale=256, train_wall=249, wall=3009
2022-07-04 16:17:20 | INFO | train_inner | epoch 001:   1100 / 1122 loss=14.973, nll_loss=10.089, mask_ins=1.784, word_ins_ml=10.815, word_reposition=1.334, kpe=1.04, ppl=32162.4, wps=6925, ups=0.35, wpb=19883.5, bsz=256, num_updates=1100, lr=0.000110078, gnorm=2.288, clip=0, loss_scale=453, train_wall=249, wall=3296
2022-07-04 16:18:22 | INFO | train | epoch 001 | loss nan | nll_loss 10.737 | mask_ins 2.674 | word_ins_ml 11.377 | word_reposition 2.36 | kpe nan | ppl nan | wps 6904.5 | ups 0.35 | wpb 19912.5 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 6.955 | clip 2.4 | loss_scale 220 | train_wall 2816 | wall 3358
2022-07-04 16:19:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 15.695 | nll_loss 10.265 | mask_ins 1.874 | word_ins_ml 11.035 | word_reposition 1.292 | kpe 1.494 | ppl 53050.4 | wps 11978.2 | wpb 2279.4 | bsz 32 | num_updates 1122
2022-07-04 16:19:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_best.pt (epoch 1 @ 1122 updates, score 15.695) (writing took 4.089650556445122 seconds)
2022-07-04 16:23:29 | INFO | train_inner | epoch 002:     78 / 1122 loss=14.82, nll_loss=9.914, mask_ins=1.741, word_ins_ml=10.664, word_reposition=1.382, kpe=1.032, ppl=28916.1, wps=5347.2, ups=0.27, wpb=19740.9, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=2.358, clip=0, loss_scale=512, train_wall=248, wall=3665
2022-07-04 16:28:17 | INFO | train_inner | epoch 002:    178 / 1122 loss=14.562, nll_loss=9.621, mask_ins=1.709, word_ins_ml=10.406, word_reposition=1.423, kpe=1.023, ppl=24186.6, wps=6940.1, ups=0.35, wpb=19979.9, bsz=256, num_updates=1300, lr=0.000130074, gnorm=2.22, clip=0, loss_scale=512, train_wall=250, wall=3953
2022-07-04 16:33:05 | INFO | train_inner | epoch 002:    278 / 1122 loss=14.343, nll_loss=9.371, mask_ins=1.704, word_ins_ml=10.187, word_reposition=1.44, kpe=1.012, ppl=20775.5, wps=6954.6, ups=0.35, wpb=19990.6, bsz=256, num_updates=1400, lr=0.000140072, gnorm=2.067, clip=0, loss_scale=512, train_wall=250, wall=4240
2022-07-04 16:37:52 | INFO | train_inner | epoch 002:    378 / 1122 loss=14.169, nll_loss=9.184, mask_ins=1.695, word_ins_ml=10.022, word_reposition=1.438, kpe=1.014, ppl=18417, wps=6876.4, ups=0.35, wpb=19731.5, bsz=256, num_updates=1500, lr=0.00015007, gnorm=2.057, clip=0, loss_scale=512, train_wall=250, wall=4527
2022-07-04 16:42:39 | INFO | train_inner | epoch 002:    478 / 1122 loss=14, nll_loss=9, mask_ins=1.689, word_ins_ml=9.861, word_reposition=1.446, kpe=1.004, ppl=16388.7, wps=6939, ups=0.35, wpb=19935.8, bsz=256, num_updates=1600, lr=0.000160068, gnorm=1.912, clip=0, loss_scale=845, train_wall=250, wall=4815
2022-07-04 16:47:26 | INFO | train_inner | epoch 002:    578 / 1122 loss=13.831, nll_loss=8.832, mask_ins=1.672, word_ins_ml=9.714, word_reposition=1.441, kpe=1.003, ppl=14573.9, wps=6950, ups=0.35, wpb=19940.7, bsz=256, num_updates=1700, lr=0.000170066, gnorm=1.957, clip=0, loss_scale=1024, train_wall=249, wall=5102
2022-07-04 16:52:13 | INFO | train_inner | epoch 002:    678 / 1122 loss=nan, nll_loss=8.672, mask_ins=1.675, word_ins_ml=9.575, word_reposition=1.439, kpe=nan, ppl=nan, wps=6934.1, ups=0.35, wpb=19899.1, bsz=256, num_updates=1800, lr=0.000180064, gnorm=1.802, clip=0, loss_scale=1024, train_wall=249, wall=5389
2022-07-04 16:57:37 | INFO | train_inner | epoch 002:    778 / 1122 loss=nan, nll_loss=8.497, mask_ins=1.653, word_ins_ml=9.422, word_reposition=1.436, kpe=nan, ppl=nan, wps=6170.4, ups=0.31, wpb=19987.5, bsz=256, num_updates=1900, lr=0.000190062, gnorm=1.812, clip=0, loss_scale=1024, train_wall=286, wall=5712
2022-07-04 17:02:25 | INFO | train_inner | epoch 002:    878 / 1122 loss=13.361, nll_loss=8.346, mask_ins=1.662, word_ins_ml=9.291, word_reposition=1.424, kpe=0.985, ppl=10524, wps=6882.9, ups=0.35, wpb=19828.7, bsz=256, num_updates=2000, lr=0.00020006, gnorm=1.874, clip=0, loss_scale=1024, train_wall=250, wall=6001
2022-07-04 17:07:11 | INFO | train_inner | epoch 002:    978 / 1122 loss=13.201, nll_loss=8.172, mask_ins=1.654, word_ins_ml=9.139, word_reposition=1.424, kpe=0.984, ppl=9415.37, wps=6954.1, ups=0.35, wpb=19915.7, bsz=256, num_updates=2100, lr=0.000210058, gnorm=1.776, clip=0, loss_scale=1567, train_wall=249, wall=6287
2022-07-04 17:11:59 | INFO | train_inner | epoch 002:   1078 / 1122 loss=13.039, nll_loss=7.943, mask_ins=1.665, word_ins_ml=8.938, word_reposition=1.45, kpe=0.986, ppl=8416.26, wps=6993.2, ups=0.35, wpb=20105.2, bsz=256, num_updates=2200, lr=0.000220056, gnorm=1.887, clip=0, loss_scale=2048, train_wall=249, wall=6574
2022-07-04 17:14:04 | INFO | train | epoch 002 | loss nan | nll_loss 8.802 | mask_ins 1.681 | word_ins_ml 9.689 | word_reposition 1.433 | kpe nan | ppl nan | wps 6685.4 | ups 0.34 | wpb 19913.7 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 1.962 | clip 0 | loss_scale 1015 | train_wall 2834 | wall 6700
2022-07-04 17:15:24 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.441 | nll_loss 7.855 | mask_ins 1.817 | word_ins_ml 8.917 | word_reposition 1.39 | kpe 1.317 | ppl 11118.8 | wps 11942.6 | wpb 2279.4 | bsz 32 | num_updates 2244 | best_loss 13.441
2022-07-04 17:15:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_best.pt (epoch 2 @ 2244 updates, score 13.441) (writing took 6.064403837546706 seconds)
2022-07-04 17:18:11 | INFO | train_inner | epoch 003:     56 / 1122 loss=nan, nll_loss=7.641, mask_ins=1.66, word_ins_ml=8.675, word_reposition=1.436, kpe=nan, ppl=nan, wps=5308.8, ups=0.27, wpb=19763.5, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=1.978, clip=0, loss_scale=2048, train_wall=249, wall=6947
2022-07-04 17:22:58 | INFO | train_inner | epoch 003:    156 / 1122 loss=12.374, nll_loss=7.243, mask_ins=1.645, word_ins_ml=8.327, word_reposition=1.421, kpe=0.981, ppl=5308.56, wps=6913.1, ups=0.35, wpb=19849.7, bsz=256, num_updates=2400, lr=0.000240052, gnorm=2.146, clip=0, loss_scale=2048, train_wall=249, wall=7234
2022-07-04 17:27:45 | INFO | train_inner | epoch 003:    256 / 1122 loss=12.046, nll_loss=6.888, mask_ins=1.633, word_ins_ml=8.02, word_reposition=1.404, kpe=0.99, ppl=4228.43, wps=6970.1, ups=0.35, wpb=20010.3, bsz=256, num_updates=2500, lr=0.00025005, gnorm=2.319, clip=0, loss_scale=2048, train_wall=250, wall=7521
2022-07-04 17:32:32 | INFO | train_inner | epoch 003:    356 / 1122 loss=11.66, nll_loss=6.476, mask_ins=1.631, word_ins_ml=7.663, word_reposition=1.376, kpe=0.99, ppl=3235.61, wps=6959.7, ups=0.35, wpb=19973.9, bsz=256, num_updates=2600, lr=0.000260048, gnorm=2.389, clip=0, loss_scale=2888, train_wall=249, wall=7808
2022-07-04 17:37:19 | INFO | train_inner | epoch 003:    456 / 1122 loss=11.398, nll_loss=6.182, mask_ins=1.622, word_ins_ml=7.41, word_reposition=1.367, kpe=0.998, ppl=2698.16, wps=6895.6, ups=0.35, wpb=19759, bsz=256, num_updates=2700, lr=0.000270046, gnorm=2.376, clip=0, loss_scale=4096, train_wall=249, wall=8095
2022-07-04 17:42:05 | INFO | train_inner | epoch 003:    556 / 1122 loss=11.106, nll_loss=5.905, mask_ins=1.618, word_ins_ml=7.173, word_reposition=1.322, kpe=0.993, ppl=2203.52, wps=6936.5, ups=0.35, wpb=19874.3, bsz=256, num_updates=2800, lr=0.000280044, gnorm=2.399, clip=0, loss_scale=4096, train_wall=249, wall=8381
2022-07-04 17:46:52 | INFO | train_inner | epoch 003:    656 / 1122 loss=10.943, nll_loss=5.72, mask_ins=1.62, word_ins_ml=7.016, word_reposition=1.313, kpe=0.993, ppl=1968.58, wps=6971.4, ups=0.35, wpb=19975.5, bsz=256, num_updates=2900, lr=0.000290042, gnorm=2.409, clip=0, loss_scale=4096, train_wall=249, wall=8668
2022-07-04 17:51:39 | INFO | train_inner | epoch 003:    756 / 1122 loss=10.732, nll_loss=5.486, mask_ins=1.623, word_ins_ml=6.815, word_reposition=1.297, kpe=0.997, ppl=1700.55, wps=6970.5, ups=0.35, wpb=20009.2, bsz=256, num_updates=3000, lr=0.00030004, gnorm=2.448, clip=0, loss_scale=4096, train_wall=250, wall=8955
2022-07-04 17:56:26 | INFO | train_inner | epoch 003:    856 / 1122 loss=nan, nll_loss=5.318, mask_ins=1.609, word_ins_ml=6.672, word_reposition=1.259, kpe=nan, ppl=nan, wps=6962.7, ups=0.35, wpb=19964, bsz=256, num_updates=3100, lr=0.000310038, gnorm=2.478, clip=0, loss_scale=5284, train_wall=249, wall=9241
2022-07-04 18:01:49 | INFO | train_inner | epoch 003:    956 / 1122 loss=10.267, nll_loss=5.032, mask_ins=1.601, word_ins_ml=6.424, word_reposition=1.248, kpe=0.994, ppl=1232.04, wps=6164.8, ups=0.31, wpb=19947.5, bsz=256, num_updates=3200, lr=0.000320036, gnorm=2.561, clip=0, loss_scale=8192, train_wall=286, wall=9565
2022-07-04 18:06:37 | INFO | train_inner | epoch 003:   1056 / 1122 loss=9.814, nll_loss=4.628, mask_ins=1.555, word_ins_ml=6.071, word_reposition=1.19, kpe=0.999, ppl=900.3, wps=6917.5, ups=0.35, wpb=19931.7, bsz=256, num_updates=3300, lr=0.000330034, gnorm=2.725, clip=0, loss_scale=8192, train_wall=250, wall=9853
2022-07-04 18:09:45 | INFO | train | epoch 003 | loss nan | nll_loss 5.875 | mask_ins 1.61 | word_ins_ml 7.148 | word_reposition 1.315 | kpe nan | ppl nan | wps 6687.6 | ups 0.34 | wpb 19914.1 | bsz 255.8 | num_updates 3366 | lr 0.000336633 | gnorm 2.433 | clip 0 | loss_scale 4598 | train_wall 2831 | wall 10041
2022-07-04 18:11:05 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 13.152 | nll_loss 7.189 | mask_ins 1.699 | word_ins_ml 8.494 | word_reposition 1.565 | kpe 1.393 | ppl 9099.34 | wps 11943.3 | wpb 2279.4 | bsz 32 | num_updates 3366 | best_loss 13.152
2022-07-04 18:11:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_best.pt (epoch 3 @ 3366 updates, score 13.152) (writing took 6.124364855699241 seconds)
2022-07-04 18:12:49 | INFO | train_inner | epoch 004:     34 / 1122 loss=9.302, nll_loss=4.189, mask_ins=1.476, word_ins_ml=5.687, word_reposition=1.139, kpe=1.001, ppl=631.39, wps=5332, ups=0.27, wpb=19787.8, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=2.985, clip=0, loss_scale=8192, train_wall=247, wall=10224
2022-07-04 18:17:36 | INFO | train_inner | epoch 004:    134 / 1122 loss=8.973, nll_loss=3.928, mask_ins=1.43, word_ins_ml=5.457, word_reposition=1.09, kpe=0.995, ppl=502.48, wps=6915.7, ups=0.35, wpb=19911.3, bsz=256, num_updates=3500, lr=0.00035003, gnorm=2.809, clip=0, loss_scale=8192, train_wall=250, wall=10512
2022-07-04 18:22:24 | INFO | train_inner | epoch 004:    234 / 1122 loss=8.785, nll_loss=3.789, mask_ins=1.381, word_ins_ml=5.335, word_reposition=1.075, kpe=0.995, ppl=441.21, wps=6980.1, ups=0.35, wpb=20058.3, bsz=256, num_updates=3600, lr=0.000360028, gnorm=2.682, clip=0, loss_scale=9585, train_wall=249, wall=10800
2022-07-04 18:22:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-04 18:27:13 | INFO | train_inner | epoch 004:    335 / 1122 loss=8.62, nll_loss=3.672, mask_ins=1.345, word_ins_ml=5.231, word_reposition=1.044, kpe=0.999, ppl=393.46, wps=6848.7, ups=0.35, wpb=19806.5, bsz=256, num_updates=3700, lr=0.000370026, gnorm=2.686, clip=0, loss_scale=8516, train_wall=251, wall=11089
2022-07-04 18:32:01 | INFO | train_inner | epoch 004:    435 / 1122 loss=8.516, nll_loss=3.611, mask_ins=1.31, word_ins_ml=5.177, word_reposition=1.03, kpe=0.999, ppl=366.04, wps=6947.2, ups=0.35, wpb=20001, bsz=256, num_updates=3800, lr=0.000380024, gnorm=2.665, clip=0, loss_scale=8192, train_wall=250, wall=11377
2022-07-04 18:36:48 | INFO | train_inner | epoch 004:    535 / 1122 loss=8.412, nll_loss=3.538, mask_ins=1.276, word_ins_ml=5.112, word_reposition=1.021, kpe=1.004, ppl=340.64, wps=6921.8, ups=0.35, wpb=19876.8, bsz=256, num_updates=3900, lr=0.000390022, gnorm=2.625, clip=0, loss_scale=8192, train_wall=249, wall=11664
2022-07-04 18:41:35 | INFO | train_inner | epoch 004:    635 / 1122 loss=nan, nll_loss=3.536, mask_ins=1.271, word_ins_ml=5.11, word_reposition=1.017, kpe=nan, ppl=nan, wps=6943.2, ups=0.35, wpb=19890.8, bsz=256, num_updates=4000, lr=0.00040002, gnorm=2.629, clip=0, loss_scale=8192, train_wall=248, wall=11950
2022-07-04 18:46:22 | INFO | train_inner | epoch 004:    735 / 1122 loss=8.252, nll_loss=3.434, mask_ins=1.235, word_ins_ml=5.018, word_reposition=1.001, kpe=0.998, ppl=304.78, wps=6998.2, ups=0.35, wpb=20081.6, bsz=256, num_updates=4100, lr=0.000410018, gnorm=2.422, clip=0, loss_scale=8192, train_wall=250, wall=12237
2022-07-04 18:47:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-04 18:47:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-04 18:51:14 | INFO | train_inner | epoch 004:    837 / 1122 loss=nan, nll_loss=3.372, mask_ins=1.208, word_ins_ml=4.963, word_reposition=0.979, kpe=nan, ppl=nan, wps=6802, ups=0.34, wpb=19858.6, bsz=256, num_updates=4200, lr=0.000420016, gnorm=2.55, clip=0, loss_scale=5582, train_wall=254, wall=12529
2022-07-04 18:56:01 | INFO | train_inner | epoch 004:    937 / 1122 loss=8.185, nll_loss=3.394, mask_ins=1.21, word_ins_ml=4.981, word_reposition=0.989, kpe=1.005, ppl=291.03, wps=6959.2, ups=0.35, wpb=19990.9, bsz=256, num_updates=4300, lr=0.000430014, gnorm=2.413, clip=0, loss_scale=4096, train_wall=249, wall=12817
2022-07-04 19:00:48 | INFO | train_inner | epoch 004:   1037 / 1122 loss=8.059, nll_loss=3.306, mask_ins=1.183, word_ins_ml=4.902, word_reposition=0.973, kpe=1.001, ppl=266.69, wps=6907.1, ups=0.35, wpb=19822.9, bsz=256, num_updates=4400, lr=0.000440012, gnorm=2.35, clip=0, loss_scale=4096, train_wall=249, wall=13104
2022-07-04 19:05:26 | INFO | train | epoch 004 | loss nan | nll_loss 3.556 | mask_ins 1.282 | word_ins_ml 5.126 | word_reposition 1.022 | kpe nan | ppl nan | wps 6670.4 | ups 0.33 | wpb 19913.7 | bsz 255.8 | num_updates 4485 | lr 0.00044851 | gnorm 2.603 | clip 0 | loss_scale 7068 | train_wall 2830 | wall 13382
2022-07-04 19:06:46 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 13.177 | nll_loss 6.617 | mask_ins 2.036 | word_ins_ml 7.996 | word_reposition 1.59 | kpe 1.555 | ppl 9261.74 | wps 11884.4 | wpb 2279.4 | bsz 32 | num_updates 4485 | best_loss 13.152
2022-07-04 19:06:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_last.pt (epoch 4 @ 4485 updates, score 13.177) (writing took 3.4845805503427982 seconds)
2022-07-04 19:07:33 | INFO | train_inner | epoch 005:     15 / 1122 loss=8.088, nll_loss=3.317, mask_ins=1.186, word_ins_ml=4.911, word_reposition=0.978, kpe=1.013, ppl=272.13, wps=4869.3, ups=0.25, wpb=19726, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=2.683, clip=0, loss_scale=4096, train_wall=284, wall=13509
2022-07-04 19:12:20 | INFO | train_inner | epoch 005:    115 / 1122 loss=7.984, nll_loss=3.242, mask_ins=1.174, word_ins_ml=4.845, word_reposition=0.969, kpe=0.996, ppl=253.13, wps=6963.6, ups=0.35, wpb=20012.6, bsz=256, num_updates=4600, lr=0.000460008, gnorm=2.249, clip=0, loss_scale=4096, train_wall=250, wall=13796
2022-07-04 19:17:07 | INFO | train_inner | epoch 005:    215 / 1122 loss=7.93, nll_loss=3.21, mask_ins=1.158, word_ins_ml=4.814, word_reposition=0.963, kpe=0.995, ppl=243.91, wps=6946.8, ups=0.35, wpb=19922.4, bsz=256, num_updates=4700, lr=0.000470006, gnorm=2.295, clip=0, loss_scale=6431, train_wall=249, wall=14083
2022-07-04 19:21:54 | INFO | train_inner | epoch 005:    315 / 1122 loss=7.942, nll_loss=3.227, mask_ins=1.164, word_ins_ml=4.828, word_reposition=0.956, kpe=0.995, ppl=245.97, wps=6968.5, ups=0.35, wpb=19989.4, bsz=256, num_updates=4800, lr=0.000480004, gnorm=2.282, clip=0, loss_scale=8192, train_wall=249, wall=14370
2022-07-04 19:23:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-04 19:24:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-04 19:26:47 | INFO | train_inner | epoch 005:    417 / 1122 loss=7.891, nll_loss=3.178, mask_ins=1.159, word_ins_ml=4.784, word_reposition=0.957, kpe=0.991, ppl=237.33, wps=6800.7, ups=0.34, wpb=19893.6, bsz=256, num_updates=4900, lr=0.000490002, gnorm=2.226, clip=0, loss_scale=3996, train_wall=254, wall=14662
2022-07-04 19:31:34 | INFO | train_inner | epoch 005:    517 / 1122 loss=nan, nll_loss=3.184, mask_ins=1.162, word_ins_ml=4.789, word_reposition=0.952, kpe=nan, ppl=nan, wps=6874.2, ups=0.35, wpb=19774.3, bsz=256, num_updates=5000, lr=0.0005, gnorm=2.368, clip=0, loss_scale=2048, train_wall=249, wall=14950
2022-07-04 19:36:21 | INFO | train_inner | epoch 005:    617 / 1122 loss=7.822, nll_loss=3.149, mask_ins=1.128, word_ins_ml=4.756, word_reposition=0.939, kpe=0.998, ppl=226.21, wps=6935, ups=0.35, wpb=19878.6, bsz=256, num_updates=5100, lr=0.000495074, gnorm=2.359, clip=0, loss_scale=2048, train_wall=249, wall=15237
2022-07-04 19:41:08 | INFO | train_inner | epoch 005:    717 / 1122 loss=7.854, nll_loss=3.161, mask_ins=1.145, word_ins_ml=4.766, word_reposition=0.942, kpe=1.002, ppl=231.36, wps=6925, ups=0.35, wpb=19904.7, bsz=256, num_updates=5200, lr=0.00049029, gnorm=2.299, clip=0, loss_scale=2048, train_wall=250, wall=15524
2022-07-04 19:45:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-04 19:45:58 | INFO | train_inner | epoch 005:    818 / 1122 loss=7.827, nll_loss=3.142, mask_ins=1.123, word_ins_ml=4.748, word_reposition=0.945, kpe=1.011, ppl=227, wps=6919.8, ups=0.35, wpb=20023.4, bsz=256, num_updates=5300, lr=0.000485643, gnorm=2.962, clip=0, loss_scale=1967, train_wall=251, wall=15813
2022-07-04 19:50:44 | INFO | train_inner | epoch 005:    918 / 1122 loss=7.905, nll_loss=3.168, mask_ins=1.133, word_ins_ml=4.772, word_reposition=0.951, kpe=1.049, ppl=239.61, wps=6965.8, ups=0.35, wpb=19947, bsz=256, num_updates=5400, lr=0.000481125, gnorm=3.11, clip=0, loss_scale=1024, train_wall=249, wall=16100
2022-07-04 19:55:30 | INFO | train_inner | epoch 005:   1018 / 1122 loss=nan, nll_loss=3.146, mask_ins=1.11, word_ins_ml=4.751, word_reposition=0.936, kpe=nan, ppl=nan, wps=6933, ups=0.35, wpb=19826.1, bsz=256, num_updates=5500, lr=0.000476731, gnorm=2.503, clip=0, loss_scale=1024, train_wall=249, wall=16386
2022-07-04 20:00:16 | INFO | train_inner | epoch 005:   1118 / 1122 loss=7.713, nll_loss=3.053, mask_ins=1.111, word_ins_ml=4.667, word_reposition=0.926, kpe=1.009, ppl=209.77, wps=6992.3, ups=0.35, wpb=20030.2, bsz=256, num_updates=5600, lr=0.000472456, gnorm=2.248, clip=0, loss_scale=1024, train_wall=249, wall=16672
2022-07-04 20:00:27 | INFO | train | epoch 005 | loss nan | nll_loss 3.173 | mask_ins 1.144 | word_ins_ml 4.778 | word_reposition 0.949 | kpe nan | ppl nan | wps 6750.7 | ups 0.34 | wpb 19914.2 | bsz 255.8 | num_updates 5604 | lr 0.000472287 | gnorm 2.46 | clip 0 | loss_scale 3088 | train_wall 2794 | wall 16683
2022-07-04 20:01:46 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.671 | nll_loss 6.086 | mask_ins 1.538 | word_ins_ml 7.496 | word_reposition 1.304 | kpe 1.333 | ppl 3260.14 | wps 11987.2 | wpb 2279.4 | bsz 32 | num_updates 5604 | best_loss 11.671
2022-07-04 20:01:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_best.pt (epoch 5 @ 5604 updates, score 11.671) (writing took 6.082138595171273 seconds)
2022-07-04 20:06:28 | INFO | train_inner | epoch 006:     96 / 1122 loss=7.732, nll_loss=3.076, mask_ins=1.108, word_ins_ml=4.687, word_reposition=0.925, kpe=1.012, ppl=212.62, wps=5314.4, ups=0.27, wpb=19734.8, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=2.417, clip=0, loss_scale=1024, train_wall=248, wall=17043
2022-07-04 20:12:04 | INFO | train_inner | epoch 006:    196 / 1122 loss=7.645, nll_loss=3.02, mask_ins=1.091, word_ins_ml=4.636, word_reposition=0.926, kpe=0.993, ppl=200.2, wps=5937.4, ups=0.3, wpb=19981.3, bsz=256, num_updates=5800, lr=0.000464238, gnorm=2.04, clip=0, loss_scale=1024, train_wall=299, wall=17380
2022-07-04 20:16:51 | INFO | train_inner | epoch 006:    296 / 1122 loss=7.559, nll_loss=2.941, mask_ins=1.092, word_ins_ml=4.565, word_reposition=0.912, kpe=0.99, ppl=188.63, wps=6951.8, ups=0.35, wpb=19948.8, bsz=256, num_updates=5900, lr=0.000460287, gnorm=2.016, clip=0, loss_scale=2007, train_wall=249, wall=17667
2022-07-04 20:21:38 | INFO | train_inner | epoch 006:    396 / 1122 loss=7.536, nll_loss=2.945, mask_ins=1.082, word_ins_ml=4.567, word_reposition=0.899, kpe=0.988, ppl=185.59, wps=6964.1, ups=0.35, wpb=19948.1, bsz=256, num_updates=6000, lr=0.000456435, gnorm=2.093, clip=0, loss_scale=2048, train_wall=249, wall=17953
2022-07-04 20:22:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-04 20:26:27 | INFO | train_inner | epoch 006:    497 / 1122 loss=nan, nll_loss=2.939, mask_ins=1.074, word_ins_ml=4.561, word_reposition=0.908, kpe=nan, ppl=nan, wps=6896.5, ups=0.35, wpb=19913.6, bsz=256, num_updates=6100, lr=0.000452679, gnorm=2.181, clip=0, loss_scale=1257, train_wall=251, wall=18242
2022-07-04 20:31:14 | INFO | train_inner | epoch 006:    597 / 1122 loss=7.544, nll_loss=2.955, mask_ins=1.076, word_ins_ml=4.574, word_reposition=0.9, kpe=0.994, ppl=186.62, wps=6887.5, ups=0.35, wpb=19778.1, bsz=256, num_updates=6200, lr=0.000449013, gnorm=2.383, clip=0, loss_scale=1024, train_wall=249, wall=18529
2022-07-04 20:36:00 | INFO | train_inner | epoch 006:    697 / 1122 loss=7.57, nll_loss=2.977, mask_ins=1.069, word_ins_ml=4.594, word_reposition=0.906, kpe=1.002, ppl=190.05, wps=6992.7, ups=0.35, wpb=20041.9, bsz=256, num_updates=6300, lr=0.000445435, gnorm=2.656, clip=0, loss_scale=1024, train_wall=249, wall=18816
2022-07-04 20:40:47 | INFO | train_inner | epoch 006:    797 / 1122 loss=nan, nll_loss=2.874, mask_ins=1.058, word_ins_ml=4.501, word_reposition=0.899, kpe=nan, ppl=nan, wps=6933, ups=0.35, wpb=19881.4, bsz=256, num_updates=6400, lr=0.000441942, gnorm=2.022, clip=0, loss_scale=1024, train_wall=249, wall=19103
2022-07-04 20:45:34 | INFO | train_inner | epoch 006:    897 / 1122 loss=7.421, nll_loss=2.853, mask_ins=1.048, word_ins_ml=4.482, word_reposition=0.904, kpe=0.987, ppl=171.36, wps=6980.4, ups=0.35, wpb=20038.7, bsz=256, num_updates=6500, lr=0.000438529, gnorm=1.921, clip=0, loss_scale=1024, train_wall=250, wall=19390
2022-07-04 20:48:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-04 20:50:24 | INFO | train_inner | epoch 006:    998 / 1122 loss=7.429, nll_loss=2.868, mask_ins=1.042, word_ins_ml=4.494, word_reposition=0.898, kpe=0.995, ppl=172.28, wps=6880.7, ups=0.35, wpb=19928.3, bsz=256, num_updates=6600, lr=0.000435194, gnorm=2.191, clip=0, loss_scale=1369, train_wall=252, wall=19679
2022-07-04 20:55:10 | INFO | train_inner | epoch 006:   1098 / 1122 loss=7.361, nll_loss=2.828, mask_ins=1.026, word_ins_ml=4.46, word_reposition=0.891, kpe=0.985, ppl=164.34, wps=6937.3, ups=0.35, wpb=19886, bsz=256, num_updates=6700, lr=0.000431934, gnorm=2.07, clip=0, loss_scale=1024, train_wall=249, wall=19966
2022-07-04 20:56:18 | INFO | train | epoch 006 | loss nan | nll_loss 2.932 | mask_ins 1.069 | word_ins_ml 4.554 | word_reposition 0.906 | kpe nan | ppl nan | wps 6654.9 | ups 0.33 | wpb 19912.6 | bsz 255.8 | num_updates 6724 | lr 0.000431163 | gnorm 2.171 | clip 0 | loss_scale 1255 | train_wall 2842 | wall 20034
2022-07-04 20:57:37 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.848 | nll_loss 6.075 | mask_ins 1.578 | word_ins_ml 7.487 | word_reposition 1.371 | kpe 1.412 | ppl 3685.79 | wps 12044 | wpb 2279.4 | bsz 32 | num_updates 6724 | best_loss 11.671
2022-07-04 20:57:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_last.pt (epoch 6 @ 6724 updates, score 11.848) (writing took 3.418311635032296 seconds)
2022-07-04 21:01:18 | INFO | train_inner | epoch 007:     76 / 1122 loss=7.37, nll_loss=2.837, mask_ins=1.045, word_ins_ml=4.466, word_reposition=0.882, kpe=0.977, ppl=165.47, wps=5350.5, ups=0.27, wpb=19668.9, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=1.898, clip=0, loss_scale=1024, train_wall=248, wall=20334
2022-07-04 21:06:05 | INFO | train_inner | epoch 007:    176 / 1122 loss=7.294, nll_loss=2.779, mask_ins=1.032, word_ins_ml=4.413, word_reposition=0.882, kpe=0.967, ppl=156.89, wps=6951.1, ups=0.35, wpb=19959.6, bsz=256, num_updates=6900, lr=0.000425628, gnorm=1.735, clip=0, loss_scale=1024, train_wall=250, wall=20621
2022-07-04 21:10:52 | INFO | train_inner | epoch 007:    276 / 1122 loss=7.269, nll_loss=2.776, mask_ins=1.014, word_ins_ml=4.41, word_reposition=0.883, kpe=0.962, ppl=154.28, wps=6977.6, ups=0.35, wpb=20026.3, bsz=256, num_updates=7000, lr=0.000422577, gnorm=1.751, clip=0, loss_scale=1024, train_wall=249, wall=20908
2022-07-04 21:16:27 | INFO | train_inner | epoch 007:    376 / 1122 loss=7.222, nll_loss=2.742, mask_ins=1.003, word_ins_ml=4.379, word_reposition=0.874, kpe=0.966, ppl=149.27, wps=5950.8, ups=0.3, wpb=19912, bsz=256, num_updates=7100, lr=0.000419591, gnorm=1.787, clip=0, loss_scale=1239, train_wall=296, wall=21243
2022-07-04 21:21:14 | INFO | train_inner | epoch 007:    476 / 1122 loss=nan, nll_loss=2.763, mask_ins=1.009, word_ins_ml=4.397, word_reposition=0.877, kpe=nan, ppl=nan, wps=6935.9, ups=0.35, wpb=19907, bsz=256, num_updates=7200, lr=0.000416667, gnorm=1.776, clip=0, loss_scale=2048, train_wall=249, wall=21530
2022-07-04 21:26:01 | INFO | train_inner | epoch 007:    576 / 1122 loss=7.225, nll_loss=2.759, mask_ins=0.998, word_ins_ml=4.393, word_reposition=0.87, kpe=0.964, ppl=149.57, wps=6905.7, ups=0.35, wpb=19812.9, bsz=256, num_updates=7300, lr=0.000413803, gnorm=1.781, clip=0, loss_scale=2048, train_wall=249, wall=21816
2022-07-04 21:30:48 | INFO | train_inner | epoch 007:    676 / 1122 loss=nan, nll_loss=2.685, mask_ins=0.994, word_ins_ml=4.326, word_reposition=0.871, kpe=nan, ppl=nan, wps=6959.9, ups=0.35, wpb=19960.2, bsz=256, num_updates=7400, lr=0.000410997, gnorm=1.656, clip=0, loss_scale=2048, train_wall=249, wall=22103
2022-07-04 21:35:35 | INFO | train_inner | epoch 007:    776 / 1122 loss=7.163, nll_loss=2.7, mask_ins=0.99, word_ins_ml=4.339, word_reposition=0.871, kpe=0.963, ppl=143.33, wps=6945.1, ups=0.35, wpb=19950.3, bsz=256, num_updates=7500, lr=0.000408248, gnorm=1.736, clip=0, loss_scale=2048, train_wall=249, wall=22391
2022-07-04 21:39:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-04 21:40:25 | INFO | train_inner | epoch 007:    877 / 1122 loss=7.179, nll_loss=2.733, mask_ins=0.984, word_ins_ml=4.368, word_reposition=0.866, kpe=0.961, ppl=144.87, wps=6878.9, ups=0.34, wpb=19947.5, bsz=256, num_updates=7600, lr=0.000405554, gnorm=1.721, clip=0, loss_scale=1916, train_wall=252, wall=22680
2022-07-04 21:45:11 | INFO | train_inner | epoch 007:    977 / 1122 loss=7.135, nll_loss=2.678, mask_ins=0.989, word_ins_ml=4.318, word_reposition=0.871, kpe=0.957, ppl=140.56, wps=6999.8, ups=0.35, wpb=20049.4, bsz=256, num_updates=7700, lr=0.000402911, gnorm=1.688, clip=0, loss_scale=1024, train_wall=249, wall=22967
2022-07-04 21:49:58 | INFO | train_inner | epoch 007:   1077 / 1122 loss=7.144, nll_loss=2.708, mask_ins=0.976, word_ins_ml=4.344, word_reposition=0.862, kpe=0.962, ppl=141.39, wps=6955.2, ups=0.35, wpb=19924.6, bsz=256, num_updates=7800, lr=0.00040032, gnorm=1.745, clip=0, loss_scale=1024, train_wall=249, wall=23253
2022-07-04 21:52:06 | INFO | train | epoch 007 | loss nan | nll_loss 2.736 | mask_ins 1.001 | word_ins_ml 4.372 | word_reposition 0.873 | kpe nan | ppl nan | wps 6667.3 | ups 0.33 | wpb 19913.1 | bsz 255.8 | num_updates 7845 | lr 0.000399171 | gnorm 1.75 | clip 0 | loss_scale 1489 | train_wall 2842 | wall 23382
2022-07-04 21:53:26 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.469 | nll_loss 5.882 | mask_ins 1.526 | word_ins_ml 7.304 | word_reposition 1.322 | kpe 1.318 | ppl 2835.07 | wps 11984.9 | wpb 2279.4 | bsz 32 | num_updates 7845 | best_loss 11.469
2022-07-04 21:53:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_best.pt (epoch 7 @ 7845 updates, score 11.469) (writing took 6.046435886062682 seconds)
2022-07-04 21:56:10 | INFO | train_inner | epoch 008:     55 / 1122 loss=7.09, nll_loss=2.653, mask_ins=0.975, word_ins_ml=4.295, word_reposition=0.866, kpe=0.954, ppl=136.19, wps=5302.5, ups=0.27, wpb=19719.6, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=1.836, clip=0, loss_scale=1024, train_wall=249, wall=23625
2022-07-04 22:00:57 | INFO | train_inner | epoch 008:    155 / 1122 loss=7.083, nll_loss=2.661, mask_ins=0.978, word_ins_ml=4.302, word_reposition=0.855, kpe=0.948, ppl=135.61, wps=6930.5, ups=0.35, wpb=19909.7, bsz=256, num_updates=8000, lr=0.000395285, gnorm=1.67, clip=0, loss_scale=1024, train_wall=250, wall=23913
2022-07-04 22:05:44 | INFO | train_inner | epoch 008:    255 / 1122 loss=7.064, nll_loss=2.643, mask_ins=0.972, word_ins_ml=4.286, word_reposition=0.857, kpe=0.95, ppl=133.81, wps=6926.4, ups=0.35, wpb=19889, bsz=256, num_updates=8100, lr=0.000392837, gnorm=1.653, clip=0, loss_scale=1034, train_wall=249, wall=24200
2022-07-04 22:10:30 | INFO | train_inner | epoch 008:    355 / 1122 loss=7.017, nll_loss=2.621, mask_ins=0.958, word_ins_ml=4.265, word_reposition=0.846, kpe=0.948, ppl=129.5, wps=6971.6, ups=0.35, wpb=19962.4, bsz=256, num_updates=8200, lr=0.000390434, gnorm=1.693, clip=0, loss_scale=2048, train_wall=249, wall=24486
2022-07-04 22:15:17 | INFO | train_inner | epoch 008:    455 / 1122 loss=7.038, nll_loss=2.644, mask_ins=0.952, word_ins_ml=4.285, word_reposition=0.857, kpe=0.945, ppl=131.4, wps=6949.9, ups=0.35, wpb=19927.4, bsz=256, num_updates=8300, lr=0.000388075, gnorm=1.627, clip=0, loss_scale=2048, train_wall=249, wall=24773
2022-07-04 22:19:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-04 22:20:54 | INFO | train_inner | epoch 008:    556 / 1122 loss=nan, nll_loss=2.622, mask_ins=0.965, word_ins_ml=4.265, word_reposition=0.861, kpe=nan, ppl=nan, wps=5949.8, ups=0.3, wpb=20019.2, bsz=256, num_updates=8400, lr=0.000385758, gnorm=1.635, clip=0, loss_scale=1683, train_wall=298, wall=25109
2022-07-04 22:25:41 | INFO | train_inner | epoch 008:    656 / 1122 loss=7.042, nll_loss=2.635, mask_ins=0.959, word_ins_ml=4.276, word_reposition=0.859, kpe=0.948, ppl=131.77, wps=6955.1, ups=0.35, wpb=19963.9, bsz=256, num_updates=8500, lr=0.000383482, gnorm=1.639, clip=0, loss_scale=1024, train_wall=249, wall=25396
2022-07-04 22:30:27 | INFO | train_inner | epoch 008:    756 / 1122 loss=6.981, nll_loss=2.584, mask_ins=0.957, word_ins_ml=4.23, word_reposition=0.851, kpe=0.943, ppl=126.29, wps=6931.7, ups=0.35, wpb=19878.6, bsz=256, num_updates=8600, lr=0.000381246, gnorm=1.601, clip=0, loss_scale=1024, train_wall=249, wall=25683
2022-07-04 22:35:14 | INFO | train_inner | epoch 008:    856 / 1122 loss=nan, nll_loss=2.599, mask_ins=0.954, word_ins_ml=4.243, word_reposition=0.853, kpe=nan, ppl=nan, wps=6975.1, ups=0.35, wpb=19978.3, bsz=256, num_updates=8700, lr=0.000379049, gnorm=1.637, clip=0, loss_scale=1024, train_wall=249, wall=25970
2022-07-04 22:40:00 | INFO | train_inner | epoch 008:    956 / 1122 loss=6.966, nll_loss=2.568, mask_ins=0.957, word_ins_ml=4.216, word_reposition=0.844, kpe=0.949, ppl=125.01, wps=6946.8, ups=0.35, wpb=19908.9, bsz=256, num_updates=8800, lr=0.000376889, gnorm=1.66, clip=0, loss_scale=1024, train_wall=249, wall=26256
2022-07-04 22:44:47 | INFO | train_inner | epoch 008:   1056 / 1122 loss=6.951, nll_loss=2.57, mask_ins=0.95, word_ins_ml=4.216, word_reposition=0.84, kpe=0.945, ppl=123.73, wps=6897.7, ups=0.35, wpb=19766.4, bsz=256, num_updates=8900, lr=0.000374766, gnorm=1.663, clip=0, loss_scale=1270, train_wall=249, wall=26543
2022-07-04 22:47:56 | INFO | train | epoch 008 | loss nan | nll_loss 2.615 | mask_ins 0.961 | word_ins_ml 4.258 | word_reposition 0.853 | kpe nan | ppl nan | wps 6664.8 | ups 0.33 | wpb 19913.1 | bsz 255.8 | num_updates 8966 | lr 0.000373384 | gnorm 1.663 | clip 0 | loss_scale 1349 | train_wall 2839 | wall 26731
2022-07-04 22:49:15 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 11.772 | nll_loss 5.998 | mask_ins 1.614 | word_ins_ml 7.416 | word_reposition 1.337 | kpe 1.405 | ppl 3496.38 | wps 12006.1 | wpb 2279.4 | bsz 32 | num_updates 8966 | best_loss 11.469
2022-07-04 22:49:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_last.pt (epoch 8 @ 8966 updates, score 11.772) (writing took 3.365880914963782 seconds)
2022-07-04 22:50:55 | INFO | train_inner | epoch 009:     34 / 1122 loss=6.981, nll_loss=2.594, mask_ins=0.957, word_ins_ml=4.238, word_reposition=0.844, kpe=0.942, ppl=126.32, wps=5360.9, ups=0.27, wpb=19739.5, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=1.743, clip=0, loss_scale=2048, train_wall=248, wall=26911
2022-07-04 22:51:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-04 22:55:45 | INFO | train_inner | epoch 009:    135 / 1122 loss=6.901, nll_loss=2.533, mask_ins=0.944, word_ins_ml=4.183, word_reposition=0.841, kpe=0.933, ppl=119.5, wps=6912.8, ups=0.34, wpb=20056.2, bsz=256, num_updates=9100, lr=0.000370625, gnorm=1.626, clip=0, loss_scale=1166, train_wall=252, wall=27201
2022-07-04 23:00:32 | INFO | train_inner | epoch 009:    235 / 1122 loss=6.947, nll_loss=2.573, mask_ins=0.951, word_ins_ml=4.218, word_reposition=0.843, kpe=0.935, ppl=123.4, wps=6926.1, ups=0.35, wpb=19860.6, bsz=256, num_updates=9200, lr=0.000368605, gnorm=1.635, clip=0, loss_scale=1024, train_wall=249, wall=27488
2022-07-04 23:05:19 | INFO | train_inner | epoch 009:    335 / 1122 loss=6.885, nll_loss=2.521, mask_ins=0.944, word_ins_ml=4.172, word_reposition=0.834, kpe=0.936, ppl=118.23, wps=6912.3, ups=0.35, wpb=19830, bsz=256, num_updates=9300, lr=0.000366618, gnorm=1.6, clip=0, loss_scale=1024, train_wall=249, wall=27775
2022-07-04 23:10:05 | INFO | train_inner | epoch 009:    435 / 1122 loss=nan, nll_loss=2.528, mask_ins=0.935, word_ins_ml=4.178, word_reposition=0.834, kpe=nan, ppl=nan, wps=6940.6, ups=0.35, wpb=19868.2, bsz=256, num_updates=9400, lr=0.000364662, gnorm=1.619, clip=0, loss_scale=1024, train_wall=249, wall=28061
2022-07-04 23:14:52 | INFO | train_inner | epoch 009:    535 / 1122 loss=6.858, nll_loss=2.506, mask_ins=0.933, word_ins_ml=4.159, word_reposition=0.833, kpe=0.934, ppl=116.02, wps=6933.9, ups=0.35, wpb=19911.1, bsz=256, num_updates=9500, lr=0.000362738, gnorm=1.639, clip=0, loss_scale=1024, train_wall=249, wall=28348
2022-07-04 23:19:40 | INFO | train_inner | epoch 009:    635 / 1122 loss=6.846, nll_loss=2.495, mask_ins=0.934, word_ins_ml=4.149, word_reposition=0.83, kpe=0.933, ppl=115.04, wps=6945.9, ups=0.35, wpb=19961.5, bsz=256, num_updates=9600, lr=0.000360844, gnorm=1.636, clip=0, loss_scale=1792, train_wall=249, wall=28636
2022-07-04 23:25:14 | INFO | train_inner | epoch 009:    735 / 1122 loss=6.81, nll_loss=2.466, mask_ins=0.927, word_ins_ml=4.122, word_reposition=0.832, kpe=0.929, ppl=112.24, wps=5945.5, ups=0.3, wpb=19888, bsz=256, num_updates=9700, lr=0.000358979, gnorm=1.59, clip=0, loss_scale=2048, train_wall=297, wall=28970
2022-07-04 23:30:01 | INFO | train_inner | epoch 009:    835 / 1122 loss=6.857, nll_loss=2.518, mask_ins=0.925, word_ins_ml=4.168, word_reposition=0.835, kpe=0.928, ppl=115.93, wps=6939.4, ups=0.35, wpb=19899.3, bsz=256, num_updates=9800, lr=0.000357143, gnorm=1.549, clip=0, loss_scale=2048, train_wall=249, wall=29257
2022-07-04 23:34:47 | INFO | train_inner | epoch 009:    935 / 1122 loss=6.832, nll_loss=2.493, mask_ins=0.924, word_ins_ml=4.146, word_reposition=0.828, kpe=0.934, ppl=113.93, wps=6982.2, ups=0.35, wpb=19986.3, bsz=256, num_updates=9900, lr=0.000355335, gnorm=1.557, clip=0, loss_scale=2048, train_wall=248, wall=29543
2022-07-04 23:39:34 | INFO | train_inner | epoch 009:   1035 / 1122 loss=nan, nll_loss=2.528, mask_ins=0.933, word_ins_ml=4.177, word_reposition=0.836, kpe=nan, ppl=nan, wps=6959.9, ups=0.35, wpb=19954, bsz=256, num_updates=10000, lr=0.000353553, gnorm=1.595, clip=0, loss_scale=2048, train_wall=249, wall=29830
2022-07-04 23:43:42 | INFO | train | epoch 009 | loss nan | nll_loss 2.517 | mask_ins 0.935 | word_ins_ml 4.168 | word_reposition 0.835 | kpe nan | ppl nan | wps 6669.8 | ups 0.33 | wpb 19913.3 | bsz 255.8 | num_updates 10087 | lr 0.000352025 | gnorm 1.606 | clip 0 | loss_scale 1672 | train_wall 2839 | wall 30078
2022-07-04 23:45:02 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 11.6 | nll_loss 5.939 | mask_ins 1.497 | word_ins_ml 7.359 | word_reposition 1.37 | kpe 1.374 | ppl 3104.97 | wps 12013.6 | wpb 2279.4 | bsz 32 | num_updates 10087 | best_loss 11.469
2022-07-04 23:45:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_last.pt (epoch 9 @ 10087 updates, score 11.6) (writing took 3.606094225309789 seconds)
2022-07-04 23:45:43 | INFO | train_inner | epoch 010:     13 / 1122 loss=6.825, nll_loss=2.483, mask_ins=0.93, word_ins_ml=4.136, word_reposition=0.829, kpe=0.931, ppl=113.4, wps=5390.1, ups=0.27, wpb=19870.5, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=1.617, clip=0, loss_scale=3338, train_wall=248, wall=30198
2022-07-04 23:47:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-04 23:50:32 | INFO | train_inner | epoch 010:    114 / 1122 loss=6.75, nll_loss=2.443, mask_ins=0.908, word_ins_ml=4.101, word_reposition=0.822, kpe=0.919, ppl=107.64, wps=6896.9, ups=0.35, wpb=19947.3, bsz=256, num_updates=10200, lr=0.00035007, gnorm=1.564, clip=0, loss_scale=2981, train_wall=251, wall=30488
2022-07-04 23:54:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-04 23:55:22 | INFO | train_inner | epoch 010:    215 / 1122 loss=6.795, nll_loss=2.477, mask_ins=0.919, word_ins_ml=4.13, word_reposition=0.823, kpe=0.924, ppl=111.05, wps=6868, ups=0.35, wpb=19887.2, bsz=256, num_updates=10300, lr=0.000348367, gnorm=1.603, clip=0, loss_scale=1754, train_wall=252, wall=30777
2022-07-05 00:00:07 | INFO | train_inner | epoch 010:    315 / 1122 loss=6.8, nll_loss=2.468, mask_ins=0.923, word_ins_ml=4.122, word_reposition=0.829, kpe=0.925, ppl=111.4, wps=6994.6, ups=0.35, wpb=19981.5, bsz=256, num_updates=10400, lr=0.000346688, gnorm=1.521, clip=0, loss_scale=1024, train_wall=248, wall=31063
2022-07-05 00:04:54 | INFO | train_inner | epoch 010:    415 / 1122 loss=6.815, nll_loss=2.488, mask_ins=0.919, word_ins_ml=4.14, word_reposition=0.834, kpe=0.922, ppl=112.57, wps=6955.8, ups=0.35, wpb=19975, bsz=256, num_updates=10500, lr=0.000345033, gnorm=1.574, clip=0, loss_scale=1024, train_wall=249, wall=31350
2022-07-05 00:09:39 | INFO | train_inner | epoch 010:    515 / 1122 loss=6.801, nll_loss=2.467, mask_ins=0.924, word_ins_ml=4.121, word_reposition=0.834, kpe=0.922, ppl=111.49, wps=6980, ups=0.35, wpb=19880.2, bsz=256, num_updates=10600, lr=0.000343401, gnorm=1.515, clip=0, loss_scale=1024, train_wall=248, wall=31635
2022-07-05 00:14:24 | INFO | train_inner | epoch 010:    615 / 1122 loss=nan, nll_loss=2.436, mask_ins=0.915, word_ins_ml=4.093, word_reposition=0.826, kpe=nan, ppl=nan, wps=7066.7, ups=0.35, wpb=20107.7, bsz=256, num_updates=10700, lr=0.000341793, gnorm=1.55, clip=0, loss_scale=1024, train_wall=248, wall=31919
2022-07-05 00:19:08 | INFO | train_inner | epoch 010:    715 / 1122 loss=6.743, nll_loss=2.434, mask_ins=0.91, word_ins_ml=4.091, word_reposition=0.818, kpe=0.924, ppl=107.12, wps=6981.2, ups=0.35, wpb=19868.2, bsz=256, num_updates=10800, lr=0.000340207, gnorm=1.531, clip=0, loss_scale=1198, train_wall=248, wall=32204
2022-07-05 00:24:06 | INFO | train_inner | epoch 010:    815 / 1122 loss=nan, nll_loss=2.425, mask_ins=0.913, word_ins_ml=4.082, word_reposition=0.823, kpe=nan, ppl=nan, wps=6707.3, ups=0.34, wpb=19949.5, bsz=256, num_updates=10900, lr=0.000338643, gnorm=1.546, clip=0, loss_scale=2048, train_wall=261, wall=32502
2022-07-05 00:26:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-05 00:29:28 | INFO | train_inner | epoch 010:    916 / 1122 loss=6.715, nll_loss=2.412, mask_ins=0.905, word_ins_ml=4.07, word_reposition=0.819, kpe=0.921, ppl=105.07, wps=6193.6, ups=0.31, wpb=19953.9, bsz=256, num_updates=11000, lr=0.0003371, gnorm=1.55, clip=0, loss_scale=1348, train_wall=285, wall=32824
2022-07-05 00:34:12 | INFO | train_inner | epoch 010:   1016 / 1122 loss=6.743, nll_loss=2.441, mask_ins=0.908, word_ins_ml=4.097, word_reposition=0.816, kpe=0.922, ppl=107.09, wps=6990.6, ups=0.35, wpb=19864.5, bsz=256, num_updates=11100, lr=0.000335578, gnorm=1.572, clip=0, loss_scale=1024, train_wall=248, wall=33108
2022-07-05 00:38:57 | INFO | train_inner | epoch 010:   1116 / 1122 loss=6.707, nll_loss=2.399, mask_ins=0.909, word_ins_ml=4.059, word_reposition=0.818, kpe=0.921, ppl=104.44, wps=6943.2, ups=0.35, wpb=19779.6, bsz=256, num_updates=11200, lr=0.000334077, gnorm=1.546, clip=0, loss_scale=1024, train_wall=248, wall=33393
2022-07-05 00:39:13 | INFO | train | epoch 010 | loss nan | nll_loss 2.444 | mask_ins 0.914 | word_ins_ml 4.1 | word_reposition 0.824 | kpe nan | ppl nan | wps 6690.6 | ups 0.34 | wpb 19914 | bsz 255.8 | num_updates 11206 | lr 0.000333987 | gnorm 1.558 | clip 0 | loss_scale 1437 | train_wall 2833 | wall 33409
2022-07-05 00:40:32 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 11.679 | nll_loss 5.956 | mask_ins 1.528 | word_ins_ml 7.374 | word_reposition 1.393 | kpe 1.384 | ppl 3279.74 | wps 12102.4 | wpb 2279.4 | bsz 32 | num_updates 11206 | best_loss 11.469
2022-07-05 00:40:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_last.pt (epoch 10 @ 11206 updates, score 11.679) (writing took 3.434295886196196 seconds)
2022-07-05 00:45:02 | INFO | train_inner | epoch 011:     94 / 1122 loss=6.7, nll_loss=2.41, mask_ins=0.9, word_ins_ml=4.068, word_reposition=0.818, kpe=0.913, ppl=103.94, wps=5410.4, ups=0.27, wpb=19765.1, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=1.598, clip=0, loss_scale=1024, train_wall=247, wall=33758
2022-07-05 00:49:47 | INFO | train_inner | epoch 011:    194 / 1122 loss=nan, nll_loss=2.399, mask_ins=0.903, word_ins_ml=4.059, word_reposition=0.815, kpe=nan, ppl=nan, wps=6985.2, ups=0.35, wpb=19850.5, bsz=256, num_updates=11400, lr=0.000331133, gnorm=1.564, clip=0, loss_scale=1024, train_wall=248, wall=34042
2022-07-05 00:54:30 | INFO | train_inner | epoch 011:    294 / 1122 loss=6.679, nll_loss=2.397, mask_ins=0.902, word_ins_ml=4.056, word_reposition=0.808, kpe=0.912, ppl=102.45, wps=7025.4, ups=0.35, wpb=19932.3, bsz=256, num_updates=11500, lr=0.00032969, gnorm=1.581, clip=0, loss_scale=1608, train_wall=248, wall=34326
2022-07-05 00:55:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-05 00:59:18 | INFO | train_inner | epoch 011:    395 / 1122 loss=6.649, nll_loss=2.37, mask_ins=0.893, word_ins_ml=4.033, word_reposition=0.808, kpe=0.916, ppl=100.37, wps=6941.4, ups=0.35, wpb=19956.8, bsz=256, num_updates=11600, lr=0.000328266, gnorm=1.597, clip=0, loss_scale=1288, train_wall=251, wall=34613
2022-07-05 01:04:04 | INFO | train_inner | epoch 011:    495 / 1122 loss=6.711, nll_loss=2.426, mask_ins=0.894, word_ins_ml=4.083, word_reposition=0.822, kpe=0.912, ppl=104.73, wps=7006.8, ups=0.35, wpb=20017.6, bsz=256, num_updates=11700, lr=0.00032686, gnorm=1.516, clip=0, loss_scale=1024, train_wall=249, wall=34899
2022-07-05 01:08:48 | INFO | train_inner | epoch 011:    595 / 1122 loss=6.685, nll_loss=2.404, mask_ins=0.893, word_ins_ml=4.062, word_reposition=0.815, kpe=0.915, ppl=102.87, wps=7015.8, ups=0.35, wpb=19963.6, bsz=256, num_updates=11800, lr=0.000325472, gnorm=1.543, clip=0, loss_scale=1024, train_wall=248, wall=35184
2022-07-05 01:13:34 | INFO | train_inner | epoch 011:    695 / 1122 loss=6.655, nll_loss=2.384, mask_ins=0.888, word_ins_ml=4.044, word_reposition=0.809, kpe=0.914, ppl=100.78, wps=6981.2, ups=0.35, wpb=19939.2, bsz=256, num_updates=11900, lr=0.000324102, gnorm=1.512, clip=0, loss_scale=1024, train_wall=249, wall=35469
2022-07-05 01:18:18 | INFO | train_inner | epoch 011:    795 / 1122 loss=6.649, nll_loss=2.371, mask_ins=0.893, word_ins_ml=4.033, word_reposition=0.813, kpe=0.91, ppl=100.37, wps=6968.7, ups=0.35, wpb=19798.5, bsz=256, num_updates=12000, lr=0.000322749, gnorm=1.517, clip=0, loss_scale=1024, train_wall=248, wall=35753
2022-07-05 01:23:03 | INFO | train_inner | epoch 011:    895 / 1122 loss=6.664, nll_loss=2.406, mask_ins=0.878, word_ins_ml=4.064, word_reposition=0.811, kpe=0.912, ppl=101.42, wps=7014.4, ups=0.35, wpb=19989.8, bsz=256, num_updates=12100, lr=0.000321412, gnorm=1.506, clip=0, loss_scale=1669, train_wall=248, wall=36038
2022-07-05 01:28:00 | INFO | train_inner | epoch 011:    995 / 1122 loss=6.674, nll_loss=2.392, mask_ins=0.896, word_ins_ml=4.051, word_reposition=0.811, kpe=0.915, ppl=102.08, wps=6690.8, ups=0.34, wpb=19860.5, bsz=256, num_updates=12200, lr=0.000320092, gnorm=1.51, clip=0, loss_scale=2048, train_wall=260, wall=36335
2022-07-05 01:29:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-05 01:33:22 | INFO | train_inner | epoch 011:   1096 / 1122 loss=nan, nll_loss=2.392, mask_ins=0.888, word_ins_ml=4.05, word_reposition=0.809, kpe=nan, ppl=nan, wps=6188.1, ups=0.31, wpb=19954.8, bsz=256, num_updates=12300, lr=0.000318788, gnorm=1.498, clip=0, loss_scale=1206, train_wall=285, wall=36658
2022-07-05 01:34:35 | INFO | train | epoch 011 | loss nan | nll_loss 2.395 | mask_ins 0.893 | word_ins_ml 4.055 | word_reposition 0.812 | kpe nan | ppl nan | wps 6713.1 | ups 0.34 | wpb 19913 | bsz 255.8 | num_updates 12326 | lr 0.000318452 | gnorm 1.54 | clip 0 | loss_scale 1265 | train_wall 2830 | wall 36731
2022-07-05 01:35:54 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 11.336 | nll_loss 5.721 | mask_ins 1.501 | word_ins_ml 7.152 | word_reposition 1.326 | kpe 1.358 | ppl 2585.17 | wps 12107.4 | wpb 2279.4 | bsz 32 | num_updates 12326 | best_loss 11.336
2022-07-05 01:36:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_best.pt (epoch 11 @ 12326 updates, score 11.336) (writing took 6.34720448218286 seconds)
2022-07-05 01:39:31 | INFO | train_inner | epoch 012:     74 / 1122 loss=6.651, nll_loss=2.389, mask_ins=0.889, word_ins_ml=4.048, word_reposition=0.81, kpe=0.905, ppl=100.53, wps=5374.9, ups=0.27, wpb=19822.4, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=1.6, clip=0, loss_scale=1024, train_wall=247, wall=37027
2022-07-05 01:40:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-05 01:44:19 | INFO | train_inner | epoch 012:    175 / 1122 loss=6.588, nll_loss=2.332, mask_ins=0.883, word_ins_ml=3.997, word_reposition=0.806, kpe=0.902, ppl=96.21, wps=6910.5, ups=0.35, wpb=19885.1, bsz=256, num_updates=12500, lr=0.000316228, gnorm=1.52, clip=0, loss_scale=608, train_wall=251, wall=37314
2022-07-05 01:49:03 | INFO | train_inner | epoch 012:    275 / 1122 loss=6.613, nll_loss=2.353, mask_ins=0.886, word_ins_ml=4.016, word_reposition=0.807, kpe=0.904, ppl=97.86, wps=7004.8, ups=0.35, wpb=19913.5, bsz=256, num_updates=12600, lr=0.00031497, gnorm=1.488, clip=0, loss_scale=512, train_wall=248, wall=37599
2022-07-05 01:53:47 | INFO | train_inner | epoch 012:    375 / 1122 loss=6.619, nll_loss=2.356, mask_ins=0.89, word_ins_ml=4.018, word_reposition=0.807, kpe=0.904, ppl=98.27, wps=7011, ups=0.35, wpb=19904, bsz=256, num_updates=12700, lr=0.000313728, gnorm=1.525, clip=0, loss_scale=512, train_wall=248, wall=37883
2022-07-05 01:58:31 | INFO | train_inner | epoch 012:    475 / 1122 loss=6.603, nll_loss=2.352, mask_ins=0.878, word_ins_ml=4.014, word_reposition=0.806, kpe=0.904, ppl=97.19, wps=7001.1, ups=0.35, wpb=19879.8, bsz=256, num_updates=12800, lr=0.0003125, gnorm=1.486, clip=0, loss_scale=512, train_wall=247, wall=38166
2022-07-05 02:03:16 | INFO | train_inner | epoch 012:    575 / 1122 loss=6.572, nll_loss=2.324, mask_ins=0.875, word_ins_ml=3.989, word_reposition=0.802, kpe=0.906, ppl=95.15, wps=7018.6, ups=0.35, wpb=20038.9, bsz=256, num_updates=12900, lr=0.000311286, gnorm=1.472, clip=0, loss_scale=512, train_wall=249, wall=38452
2022-07-05 02:08:00 | INFO | train_inner | epoch 012:    675 / 1122 loss=6.576, nll_loss=2.332, mask_ins=0.883, word_ins_ml=3.996, word_reposition=0.798, kpe=0.899, ppl=95.42, wps=6977.1, ups=0.35, wpb=19803.2, bsz=256, num_updates=13000, lr=0.000310087, gnorm=1.501, clip=0, loss_scale=870, train_wall=248, wall=38736
2022-07-05 02:12:45 | INFO | train_inner | epoch 012:    775 / 1122 loss=6.6, nll_loss=2.353, mask_ins=0.874, word_ins_ml=4.014, word_reposition=0.809, kpe=0.903, ppl=97, wps=7024.9, ups=0.35, wpb=20014.9, bsz=256, num_updates=13100, lr=0.000308901, gnorm=1.498, clip=0, loss_scale=1024, train_wall=248, wall=39021
2022-07-05 02:17:30 | INFO | train_inner | epoch 012:    875 / 1122 loss=6.564, nll_loss=2.326, mask_ins=0.873, word_ins_ml=3.991, word_reposition=0.799, kpe=0.9, ppl=94.61, wps=7015.2, ups=0.35, wpb=19961.4, bsz=256, num_updates=13200, lr=0.000307729, gnorm=1.518, clip=0, loss_scale=1024, train_wall=248, wall=39305
2022-07-05 02:22:14 | INFO | train_inner | epoch 012:    975 / 1122 loss=nan, nll_loss=2.3, mask_ins=0.877, word_ins_ml=3.967, word_reposition=0.812, kpe=nan, ppl=nan, wps=6995, ups=0.35, wpb=19899.3, bsz=256, num_updates=13300, lr=0.00030657, gnorm=1.466, clip=0, loss_scale=1024, train_wall=248, wall=39590
2022-07-05 02:26:59 | INFO | train_inner | epoch 012:   1075 / 1122 loss=nan, nll_loss=2.339, mask_ins=0.88, word_ins_ml=4.002, word_reposition=0.798, kpe=nan, ppl=nan, wps=6991.4, ups=0.35, wpb=19897.4, bsz=256, num_updates=13400, lr=0.000305424, gnorm=1.513, clip=0, loss_scale=1024, train_wall=248, wall=39874
2022-07-05 02:29:12 | INFO | train | epoch 012 | loss nan | nll_loss 2.341 | mask_ins 0.88 | word_ins_ml 4.004 | word_reposition 0.805 | kpe nan | ppl nan | wps 6813.1 | ups 0.34 | wpb 19913.5 | bsz 255.8 | num_updates 13447 | lr 0.000304889 | gnorm 1.507 | clip 0 | loss_scale 795 | train_wall 2782 | wall 40008
2022-07-05 02:30:31 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.456 | nll_loss 5.714 | mask_ins 1.5 | word_ins_ml 7.151 | word_reposition 1.404 | kpe 1.4 | ppl 2809.02 | wps 12089.4 | wpb 2279.4 | bsz 32 | num_updates 13447 | best_loss 11.336
2022-07-05 02:30:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_last.pt (epoch 12 @ 13447 updates, score 11.456) (writing took 3.3614059863612056 seconds)
2022-07-05 02:31:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-05 02:33:42 | INFO | train_inner | epoch 013:     54 / 1122 loss=6.577, nll_loss=2.318, mask_ins=0.889, word_ins_ml=3.983, word_reposition=0.804, kpe=0.901, ppl=95.49, wps=4899.1, ups=0.25, wpb=19765.8, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=1.532, clip=0, loss_scale=1166, train_wall=285, wall=40278
2022-07-05 02:38:39 | INFO | train_inner | epoch 013:    154 / 1122 loss=6.567, nll_loss=2.33, mask_ins=0.874, word_ins_ml=3.994, word_reposition=0.804, kpe=0.894, ppl=94.79, wps=6745.3, ups=0.34, wpb=19998, bsz=256, num_updates=13600, lr=0.00030317, gnorm=1.533, clip=0, loss_scale=1024, train_wall=260, wall=40574
2022-07-05 02:43:24 | INFO | train_inner | epoch 013:    254 / 1122 loss=6.53, nll_loss=2.301, mask_ins=0.872, word_ins_ml=3.968, word_reposition=0.797, kpe=0.893, ppl=92.39, wps=6964.1, ups=0.35, wpb=19869.2, bsz=256, num_updates=13700, lr=0.000302061, gnorm=1.49, clip=0, loss_scale=1024, train_wall=249, wall=40860
2022-07-05 02:48:09 | INFO | train_inner | epoch 013:    354 / 1122 loss=nan, nll_loss=2.323, mask_ins=0.863, word_ins_ml=3.988, word_reposition=0.802, kpe=nan, ppl=nan, wps=6995.7, ups=0.35, wpb=19940.4, bsz=256, num_updates=13800, lr=0.000300965, gnorm=1.485, clip=0, loss_scale=1024, train_wall=249, wall=41145
2022-07-05 02:52:54 | INFO | train_inner | epoch 013:    454 / 1122 loss=6.545, nll_loss=2.316, mask_ins=0.872, word_ins_ml=3.981, word_reposition=0.801, kpe=0.891, ppl=93.38, wps=6984.9, ups=0.35, wpb=19903.3, bsz=256, num_updates=13900, lr=0.00029988, gnorm=1.491, clip=0, loss_scale=1024, train_wall=248, wall=41430
2022-07-05 02:57:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-05 02:57:42 | INFO | train_inner | epoch 013:    555 / 1122 loss=6.53, nll_loss=2.305, mask_ins=0.861, word_ins_ml=3.971, word_reposition=0.804, kpe=0.894, ppl=92.44, wps=6919.1, ups=0.35, wpb=19929.4, bsz=256, num_updates=14000, lr=0.000298807, gnorm=1.521, clip=0, loss_scale=1267, train_wall=252, wall=41718
2022-07-05 03:02:27 | INFO | train_inner | epoch 013:    655 / 1122 loss=nan, nll_loss=2.332, mask_ins=0.875, word_ins_ml=3.994, word_reposition=0.806, kpe=nan, ppl=nan, wps=6988.3, ups=0.35, wpb=19884.6, bsz=256, num_updates=14100, lr=0.000297746, gnorm=1.479, clip=0, loss_scale=1024, train_wall=248, wall=42002
2022-07-05 03:02:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-05 03:07:15 | INFO | train_inner | epoch 013:    756 / 1122 loss=6.515, nll_loss=2.313, mask_ins=0.854, word_ins_ml=3.978, word_reposition=0.791, kpe=0.892, ppl=91.48, wps=6907.6, ups=0.35, wpb=19915.2, bsz=256, num_updates=14200, lr=0.000296695, gnorm=1.474, clip=0, loss_scale=517, train_wall=251, wall=42291
2022-07-05 03:12:00 | INFO | train_inner | epoch 013:    856 / 1122 loss=6.601, nll_loss=2.363, mask_ins=0.875, word_ins_ml=4.021, word_reposition=0.806, kpe=0.898, ppl=97.06, wps=7001.6, ups=0.35, wpb=19932.9, bsz=256, num_updates=14300, lr=0.000295656, gnorm=1.491, clip=0, loss_scale=512, train_wall=248, wall=42575
2022-07-05 03:16:45 | INFO | train_inner | epoch 013:    956 / 1122 loss=6.521, nll_loss=2.289, mask_ins=0.869, word_ins_ml=3.957, word_reposition=0.8, kpe=0.895, ppl=91.81, wps=6971, ups=0.35, wpb=19878.5, bsz=256, num_updates=14400, lr=0.000294628, gnorm=1.458, clip=0, loss_scale=512, train_wall=249, wall=42860
2022-07-05 03:21:29 | INFO | train_inner | epoch 013:   1056 / 1122 loss=6.549, nll_loss=2.318, mask_ins=0.877, word_ins_ml=3.981, word_reposition=0.793, kpe=0.898, ppl=93.64, wps=7040.6, ups=0.35, wpb=20012.3, bsz=256, num_updates=14500, lr=0.00029361, gnorm=1.471, clip=0, loss_scale=512, train_wall=248, wall=43145
2022-07-05 03:24:37 | INFO | train | epoch 013 | loss nan | nll_loss 2.318 | mask_ins 0.87 | word_ins_ml 3.982 | word_reposition 0.801 | kpe nan | ppl nan | wps 6701.8 | ups 0.34 | wpb 19911.9 | bsz 255.8 | num_updates 14566 | lr 0.000292944 | gnorm 1.496 | clip 0 | loss_scale 841 | train_wall 2833 | wall 43332
2022-07-05 03:25:55 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 11.401 | nll_loss 5.72 | mask_ins 1.515 | word_ins_ml 7.157 | word_reposition 1.375 | kpe 1.355 | ppl 2704.61 | wps 12122 | wpb 2279.4 | bsz 32 | num_updates 14566 | best_loss 11.336
2022-07-05 03:25:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_last.pt (epoch 13 @ 14566 updates, score 11.401) (writing took 3.3043174585327506 seconds)
2022-07-05 03:27:35 | INFO | train_inner | epoch 014:     34 / 1122 loss=6.516, nll_loss=2.311, mask_ins=0.853, word_ins_ml=3.975, word_reposition=0.797, kpe=0.891, ppl=91.51, wps=5419.4, ups=0.27, wpb=19830.6, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=1.583, clip=0, loss_scale=512, train_wall=248, wall=43511
2022-07-05 03:32:20 | INFO | train_inner | epoch 014:    134 / 1122 loss=6.487, nll_loss=2.284, mask_ins=0.862, word_ins_ml=3.952, word_reposition=0.79, kpe=0.884, ppl=89.71, wps=6983.9, ups=0.35, wpb=19880.5, bsz=256, num_updates=14700, lr=0.000291606, gnorm=1.456, clip=0, loss_scale=963, train_wall=248, wall=43795
2022-07-05 03:37:39 | INFO | train_inner | epoch 014:    234 / 1122 loss=6.504, nll_loss=2.286, mask_ins=0.869, word_ins_ml=3.953, word_reposition=0.796, kpe=0.886, ppl=90.79, wps=6235.5, ups=0.31, wpb=19936.2, bsz=256, num_updates=14800, lr=0.000290619, gnorm=1.482, clip=0, loss_scale=1024, train_wall=283, wall=44115
2022-07-05 03:42:39 | INFO | train_inner | epoch 014:    334 / 1122 loss=6.479, nll_loss=2.28, mask_ins=0.862, word_ins_ml=3.948, word_reposition=0.785, kpe=0.884, ppl=89.23, wps=6655, ups=0.33, wpb=19928.8, bsz=256, num_updates=14900, lr=0.000289642, gnorm=1.468, clip=0, loss_scale=1024, train_wall=262, wall=44414
2022-07-05 03:47:30 | INFO | train_inner | epoch 014:    434 / 1122 loss=6.507, nll_loss=2.297, mask_ins=0.864, word_ins_ml=3.963, word_reposition=0.791, kpe=0.89, ppl=90.96, wps=6848.9, ups=0.34, wpb=19923.3, bsz=256, num_updates=15000, lr=0.000288675, gnorm=1.533, clip=0, loss_scale=1024, train_wall=254, wall=44705
2022-07-05 03:52:16 | INFO | train_inner | epoch 014:    534 / 1122 loss=6.463, nll_loss=2.25, mask_ins=0.86, word_ins_ml=3.921, word_reposition=0.79, kpe=0.893, ppl=88.23, wps=6956.7, ups=0.35, wpb=19896.6, bsz=256, num_updates=15100, lr=0.000287718, gnorm=1.486, clip=0, loss_scale=1024, train_wall=249, wall=44991
2022-07-05 03:57:03 | INFO | train_inner | epoch 014:    634 / 1122 loss=6.468, nll_loss=2.266, mask_ins=0.855, word_ins_ml=3.935, word_reposition=0.791, kpe=0.886, ppl=88.5, wps=6921.6, ups=0.35, wpb=19902, bsz=256, num_updates=15200, lr=0.00028677, gnorm=1.452, clip=0, loss_scale=1802, train_wall=251, wall=45279
2022-07-05 04:01:51 | INFO | train_inner | epoch 014:    734 / 1122 loss=6.488, nll_loss=2.293, mask_ins=0.854, word_ins_ml=3.959, word_reposition=0.791, kpe=0.885, ppl=89.75, wps=6969.5, ups=0.35, wpb=20024.5, bsz=256, num_updates=15300, lr=0.000285831, gnorm=1.45, clip=0, loss_scale=2048, train_wall=251, wall=45566
2022-07-05 04:02:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-05 04:06:38 | INFO | train_inner | epoch 014:    835 / 1122 loss=6.5, nll_loss=2.297, mask_ins=0.857, word_ins_ml=3.962, word_reposition=0.793, kpe=0.888, ppl=90.49, wps=6905.6, ups=0.35, wpb=19838.5, bsz=256, num_updates=15400, lr=0.000284901, gnorm=1.452, clip=0, loss_scale=1105, train_wall=250, wall=45854
2022-07-05 04:11:22 | INFO | train_inner | epoch 014:    935 / 1122 loss=6.469, nll_loss=2.259, mask_ins=0.86, word_ins_ml=3.929, word_reposition=0.791, kpe=0.89, ppl=88.59, wps=6995.2, ups=0.35, wpb=19884.4, bsz=256, num_updates=15500, lr=0.000283981, gnorm=1.483, clip=0, loss_scale=1024, train_wall=248, wall=46138
2022-07-05 04:12:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-05 04:16:09 | INFO | train_inner | epoch 014:   1036 / 1122 loss=6.459, nll_loss=2.262, mask_ins=0.849, word_ins_ml=3.931, word_reposition=0.797, kpe=0.882, ppl=87.99, wps=6937.5, ups=0.35, wpb=19921, bsz=256, num_updates=15600, lr=0.000283069, gnorm=1.459, clip=0, loss_scale=618, train_wall=250, wall=46425
2022-07-05 04:20:13 | INFO | train | epoch 014 | loss nan | nll_loss 2.279 | mask_ins 0.858 | word_ins_ml 3.946 | word_reposition 0.791 | kpe nan | ppl nan | wps 6684.2 | ups 0.34 | wpb 19911.8 | bsz 255.8 | num_updates 15686 | lr 0.000282292 | gnorm 1.476 | clip 0 | loss_scale 1095 | train_wall 2843 | wall 46669
2022-07-05 04:21:32 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.372 | nll_loss 5.697 | mask_ins 1.494 | word_ins_ml 7.139 | word_reposition 1.319 | kpe 1.42 | ppl 2650.35 | wps 12092.6 | wpb 2279.4 | bsz 32 | num_updates 15686 | best_loss 11.336
2022-07-05 04:21:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer/checkpoint_last.pt (epoch 14 @ 15686 updates, score 11.372) (writing took 3.4727252423763275 seconds)
2022-07-05 04:22:15 | INFO | train_inner | epoch 015:     14 / 1122 loss=nan, nll_loss=2.262, mask_ins=0.852, word_ins_ml=3.931, word_reposition=0.789, kpe=nan, ppl=nan, wps=5424.8, ups=0.27, wpb=19843.3, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=1.503, clip=0, loss_scale=512, train_wall=248, wall=46791
2022-07-05 04:27:00 | INFO | train_inner | epoch 015:    114 / 1122 loss=6.458, nll_loss=2.259, mask_ins=0.863, word_ins_ml=3.928, word_reposition=0.791, kpe=0.876, ppl=87.88, wps=7011.1, ups=0.35, wpb=19999.7, bsz=256, num_updates=15800, lr=0.000281272, gnorm=1.432, clip=0, loss_scale=512, train_wall=248, wall=47076
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
