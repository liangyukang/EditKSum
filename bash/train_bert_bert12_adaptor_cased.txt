nohup: ignoring input
2022-07-11 09:13:26 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:13439
2022-07-11 09:13:26 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:13439
2022-07-11 09:13:26 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:13439
2022-07-11 09:13:26 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-11 09:13:26 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:13439
2022-07-11 09:13:26 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-11 09:13:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-11 09:13:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-11 09:13:27 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-11 09:13:27 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-11 09:13:27 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-11 09:13:27 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-11 09:13:27 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-11 09:13:27 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-11 09:13:27 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-11 09:13:27 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-11 09:13:30 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:13439', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='/data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=False, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-11 09:13:30 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-11 09:13:30 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-07-11 09:13:30 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.source
2022-07-11 09:13:30 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.target
2022-07-11 09:13:30 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 valid source-target 13368 examples
2022-07-11 09:13:31 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-11 09:13:31 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-11 09:13:31 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-11 09:13:34 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight']
2022-07-11 09:13:34 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-11 09:13:34 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-11 09:13:34 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-11 09:13:34 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
2022-07-11 09:13:36 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-11 09:13:36 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 265
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 1
Trained parameters not adapter: ['decoder.embed_mask_ins.weight']
2022-07-11 09:13:36 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-07-11 09:13:36 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-11 09:13:36 | INFO | fairseq_cli.train | num. model params: 380360448 (num. trained: 142061568)
2022-07-11 09:13:36 | INFO | fairseq_cli.train | num. Encoder model params: 146077440 (Encoder num. trained: 37767168)
2022-07-11 09:13:36 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)
Trained parameters: len 265
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 265
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 265
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-11 09:13:40 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-11 09:13:40 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-11 09:13:40 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_bert12_cased/checkpoint_last.pt
2022-07-11 09:13:40 | INFO | fairseq.trainer | loading train data for epoch 1
2022-07-11 09:13:40 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.source
2022-07-11 09:13:40 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.target
2022-07-11 09:13:40 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 train source-target 287112 examples
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-11 09:16:56 | INFO | train_inner | epoch 001:    100 / 1122 loss=26.195, nll_loss=12.319, mask_ins=7.607, word_ins_ml=12.784, word_reposition=5.804, ppl=7.68243e+07, wps=10753.1, ups=0.52, wpb=20527, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=24.38, clip=18, loss_scale=128, train_wall=193, wall=196
2022-07-11 09:20:09 | INFO | train_inner | epoch 001:    200 / 1122 loss=21.02, nll_loss=11.301, mask_ins=4.671, word_ins_ml=11.882, word_reposition=4.467, ppl=2.12673e+06, wps=10696.4, ups=0.52, wpb=20583.2, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=19.045, clip=0, loss_scale=128, train_wall=192, wall=389
2022-07-11 09:23:22 | INFO | train_inner | epoch 001:    300 / 1122 loss=16.34, nll_loss=11.319, mask_ins=2.283, word_ins_ml=11.88, word_reposition=2.177, ppl=82963.2, wps=10637.3, ups=0.52, wpb=20561.3, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=5.886, clip=0, loss_scale=128, train_wall=192, wall=582
2022-07-11 09:26:35 | INFO | train_inner | epoch 001:    400 / 1122 loss=15.067, nll_loss=10.998, mask_ins=1.926, word_ins_ml=11.596, word_reposition=1.545, ppl=34335.8, wps=10680.2, ups=0.52, wpb=20576.5, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=3.267, clip=0, loss_scale=128, train_wall=192, wall=775
2022-07-11 09:29:45 | INFO | train_inner | epoch 001:    500 / 1122 loss=14.714, nll_loss=10.803, mask_ins=1.856, word_ins_ml=11.43, word_reposition=1.429, ppl=26873.2, wps=10765.6, ups=0.52, wpb=20523.5, bsz=256, num_updates=500, lr=5.009e-05, gnorm=3.034, clip=0, loss_scale=128, train_wall=190, wall=965
2022-07-11 09:32:56 | INFO | train_inner | epoch 001:    600 / 1122 loss=14.447, nll_loss=10.506, mask_ins=1.839, word_ins_ml=11.177, word_reposition=1.432, ppl=22341, wps=10726.9, ups=0.52, wpb=20491.4, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=2.876, clip=0, loss_scale=242, train_wall=190, wall=1156
2022-07-11 09:36:09 | INFO | train_inner | epoch 001:    700 / 1122 loss=14.192, nll_loss=10.172, mask_ins=1.845, word_ins_ml=10.893, word_reposition=1.454, ppl=18718.5, wps=10677.4, ups=0.52, wpb=20542.5, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=2.884, clip=0, loss_scale=256, train_wall=192, wall=1349
2022-07-11 09:39:22 | INFO | train_inner | epoch 001:    800 / 1122 loss=13.904, nll_loss=9.859, mask_ins=1.816, word_ins_ml=10.625, word_reposition=1.463, ppl=15331.1, wps=10630.5, ups=0.52, wpb=20579, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=2.716, clip=0, loss_scale=256, train_wall=193, wall=1542
2022-07-11 09:42:34 | INFO | train_inner | epoch 001:    900 / 1122 loss=13.661, nll_loss=9.621, mask_ins=1.756, word_ins_ml=10.423, word_reposition=1.481, ppl=12952.2, wps=10649.1, ups=0.52, wpb=20464, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=2.563, clip=0, loss_scale=256, train_wall=191, wall=1735
2022-07-11 09:45:46 | INFO | train_inner | epoch 001:   1000 / 1122 loss=13.42, nll_loss=9.424, mask_ins=1.709, word_ins_ml=10.254, word_reposition=1.457, ppl=10963.3, wps=10762.4, ups=0.52, wpb=20597.8, bsz=256, num_updates=1000, lr=0.00010008, gnorm=2.232, clip=0, loss_scale=256, train_wall=191, wall=1926
2022-07-11 09:48:56 | INFO | train_inner | epoch 001:   1100 / 1122 loss=13.292, nll_loss=9.257, mask_ins=1.715, word_ins_ml=10.11, word_reposition=1.467, ppl=10027.3, wps=10756.7, ups=0.53, wpb=20473.7, bsz=256, num_updates=1100, lr=0.000110078, gnorm=2.284, clip=0, loss_scale=453, train_wall=190, wall=2116
2022-07-11 09:49:37 | INFO | train | epoch 001 | loss 15.969 | nll_loss 10.482 | mask_ins 2.621 | word_ins_ml 11.165 | word_reposition 2.183 | ppl 64134.4 | wps 10698.3 | ups 0.52 | wpb 20520.3 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 6.386 | clip 1.6 | loss_scale 220 | train_wall 2146 | wall 2158
2022-07-11 09:50:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.442 | nll_loss 9.221 | mask_ins 1.773 | word_ins_ml 10.135 | word_reposition 1.533 | ppl 11125 | wps 25172.8 | wpb 2367.6 | bsz 32 | num_updates 1122
2022-07-11 09:50:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 1 @ 1122 updates, score 13.442) (writing took 4.415074348449707 seconds)
2022-07-11 09:52:53 | INFO | train_inner | epoch 002:     78 / 1122 loss=13.133, nll_loss=9.119, mask_ins=1.699, word_ins_ml=9.991, word_reposition=1.442, ppl=8981.83, wps=8577.7, ups=0.42, wpb=20333.3, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=2.084, clip=0, loss_scale=512, train_wall=192, wall=2353
2022-07-11 09:56:07 | INFO | train_inner | epoch 002:    178 / 1122 loss=12.98, nll_loss=8.973, mask_ins=1.677, word_ins_ml=9.864, word_reposition=1.438, ppl=8079.06, wps=10634.8, ups=0.52, wpb=20587.3, bsz=256, num_updates=1300, lr=0.000130074, gnorm=1.996, clip=0, loss_scale=512, train_wall=193, wall=2547
2022-07-11 09:59:20 | INFO | train_inner | epoch 002:    278 / 1122 loss=12.855, nll_loss=8.846, mask_ins=1.674, word_ins_ml=9.755, word_reposition=1.426, ppl=7410.29, wps=10653.7, ups=0.52, wpb=20599.8, bsz=256, num_updates=1400, lr=0.000140072, gnorm=1.977, clip=0, loss_scale=512, train_wall=192, wall=2740
2022-07-11 10:02:30 | INFO | train_inner | epoch 002:    378 / 1122 loss=12.72, nll_loss=8.721, mask_ins=1.669, word_ins_ml=9.645, word_reposition=1.406, ppl=6749.12, wps=10725.2, ups=0.53, wpb=20347.3, bsz=256, num_updates=1500, lr=0.00015007, gnorm=1.847, clip=0, loss_scale=512, train_wall=189, wall=2930
2022-07-11 10:05:40 | INFO | train_inner | epoch 002:    478 / 1122 loss=12.589, nll_loss=8.587, mask_ins=1.66, word_ins_ml=9.529, word_reposition=1.4, ppl=6162.94, wps=10801.4, ups=0.53, wpb=20567.7, bsz=256, num_updates=1600, lr=0.000160068, gnorm=1.824, clip=0, loss_scale=845, train_wall=190, wall=3120
2022-07-11 10:08:49 | INFO | train_inner | epoch 002:    578 / 1122 loss=12.444, nll_loss=8.433, mask_ins=1.649, word_ins_ml=9.394, word_reposition=1.401, ppl=5571.61, wps=10857.2, ups=0.53, wpb=20536.9, bsz=256, num_updates=1700, lr=0.000170066, gnorm=1.771, clip=0, loss_scale=1024, train_wall=188, wall=3310
2022-07-11 10:11:59 | INFO | train_inner | epoch 002:    678 / 1122 loss=12.3, nll_loss=8.287, mask_ins=1.646, word_ins_ml=9.266, word_reposition=1.388, ppl=5042.44, wps=10816.8, ups=0.53, wpb=20477.4, bsz=256, num_updates=1800, lr=0.000180064, gnorm=1.678, clip=0, loss_scale=1024, train_wall=189, wall=3499
2022-07-11 10:15:08 | INFO | train_inner | epoch 002:    778 / 1122 loss=12.13, nll_loss=8.117, mask_ins=1.63, word_ins_ml=9.117, word_reposition=1.383, ppl=4483.17, wps=10867.7, ups=0.53, wpb=20576, bsz=256, num_updates=1900, lr=0.000190062, gnorm=1.748, clip=0, loss_scale=1024, train_wall=189, wall=3688
2022-07-11 10:18:17 | INFO | train_inner | epoch 002:    878 / 1122 loss=11.946, nll_loss=7.888, mask_ins=1.639, word_ins_ml=8.916, word_reposition=1.391, ppl=3944.62, wps=10825.4, ups=0.53, wpb=20447.7, bsz=256, num_updates=2000, lr=0.00020006, gnorm=1.829, clip=0, loss_scale=1024, train_wall=188, wall=3877
2022-07-11 10:21:26 | INFO | train_inner | epoch 002:    978 / 1122 loss=11.579, nll_loss=7.53, mask_ins=1.617, word_ins_ml=8.602, word_reposition=1.36, ppl=3058.96, wps=10823.2, ups=0.53, wpb=20513.5, bsz=256, num_updates=2100, lr=0.000210058, gnorm=1.937, clip=0, loss_scale=1567, train_wall=189, wall=4067
2022-07-11 10:24:36 | INFO | train_inner | epoch 002:   1078 / 1122 loss=11.173, nll_loss=7.068, mask_ins=1.613, word_ins_ml=8.201, word_reposition=1.36, ppl=2309.27, wps=10910.8, ups=0.53, wpb=20708.1, bsz=256, num_updates=2200, lr=0.000220056, gnorm=2.186, clip=0, loss_scale=2048, train_wall=189, wall=4257
2022-07-11 10:25:59 | INFO | train | epoch 002 | loss 12.278 | nll_loss 8.246 | mask_ins 1.651 | word_ins_ml 9.23 | word_reposition 1.397 | ppl 4965.19 | wps 10552.2 | ups 0.51 | wpb 20521 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 1.917 | clip 0 | loss_scale 1015 | train_wall 2129 | wall 4340
2022-07-11 10:26:38 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.797 | nll_loss 7.509 | mask_ins 1.698 | word_ins_ml 8.702 | word_reposition 1.397 | ppl 3558.57 | wps 25409.9 | wpb 2367.6 | bsz 32 | num_updates 2244 | best_loss 11.797
2022-07-11 10:26:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 2 @ 2244 updates, score 11.797) (writing took 5.872323292307556 seconds)
2022-07-11 10:28:31 | INFO | train_inner | epoch 003:     56 / 1122 loss=10.817, nll_loss=6.647, mask_ins=1.634, word_ins_ml=7.837, word_reposition=1.345, ppl=1803.42, wps=8686.4, ups=0.43, wpb=20387.7, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=2.445, clip=0, loss_scale=2048, train_wall=189, wall=4491
2022-07-11 10:31:41 | INFO | train_inner | epoch 003:    156 / 1122 loss=10.422, nll_loss=6.267, mask_ins=1.619, word_ins_ml=7.508, word_reposition=1.295, ppl=1371.69, wps=10761.1, ups=0.53, wpb=20466.9, bsz=256, num_updates=2400, lr=0.000240052, gnorm=2.346, clip=0, loss_scale=2048, train_wall=189, wall=4681
2022-07-11 10:34:52 | INFO | train_inner | epoch 003:    256 / 1122 loss=10.155, nll_loss=5.988, mask_ins=1.601, word_ins_ml=7.269, word_reposition=1.285, ppl=1140.14, wps=10811.4, ups=0.53, wpb=20590.4, bsz=256, num_updates=2500, lr=0.00025005, gnorm=2.318, clip=0, loss_scale=2048, train_wall=190, wall=4872
2022-07-11 10:38:02 | INFO | train_inner | epoch 003:    356 / 1122 loss=9.86, nll_loss=5.697, mask_ins=1.593, word_ins_ml=7.017, word_reposition=1.249, ppl=929.11, wps=10783.2, ups=0.52, wpb=20552.9, bsz=256, num_updates=2600, lr=0.000260048, gnorm=2.328, clip=0, loss_scale=2888, train_wall=190, wall=5062
2022-07-11 10:41:12 | INFO | train_inner | epoch 003:    456 / 1122 loss=9.665, nll_loss=5.514, mask_ins=1.59, word_ins_ml=6.859, word_reposition=1.217, ppl=812.01, wps=10762, ups=0.53, wpb=20384, bsz=256, num_updates=2700, lr=0.000270046, gnorm=2.275, clip=0, loss_scale=4096, train_wall=189, wall=5252
2022-07-11 10:44:21 | INFO | train_inner | epoch 003:    556 / 1122 loss=9.478, nll_loss=5.331, mask_ins=1.577, word_ins_ml=6.7, word_reposition=1.202, ppl=713.28, wps=10811.5, ups=0.53, wpb=20480.9, bsz=256, num_updates=2800, lr=0.000280044, gnorm=2.21, clip=0, loss_scale=4096, train_wall=189, wall=5441
2022-07-11 10:47:31 | INFO | train_inner | epoch 003:    656 / 1122 loss=9.424, nll_loss=5.257, mask_ins=1.587, word_ins_ml=6.635, word_reposition=1.201, ppl=686.99, wps=10851.6, ups=0.53, wpb=20612.3, bsz=256, num_updates=2900, lr=0.000290042, gnorm=2.146, clip=0, loss_scale=4096, train_wall=189, wall=5631
2022-07-11 10:50:41 | INFO | train_inner | epoch 003:    756 / 1122 loss=9.247, nll_loss=5.087, mask_ins=1.589, word_ins_ml=6.487, word_reposition=1.171, ppl=607.51, wps=10840.5, ups=0.53, wpb=20597.8, bsz=256, num_updates=3000, lr=0.00030004, gnorm=2.192, clip=0, loss_scale=4096, train_wall=189, wall=5821
2022-07-11 10:53:50 | INFO | train_inner | epoch 003:    856 / 1122 loss=9.104, nll_loss=4.974, mask_ins=1.563, word_ins_ml=6.387, word_reposition=1.154, ppl=550.1, wps=10913.4, ups=0.53, wpb=20609.8, bsz=256, num_updates=3100, lr=0.000310038, gnorm=2.084, clip=0, loss_scale=5284, train_wall=188, wall=6010
2022-07-11 10:56:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 10:57:01 | INFO | train_inner | epoch 003:    957 / 1122 loss=8.649, nll_loss=4.583, mask_ins=1.494, word_ins_ml=6.043, word_reposition=1.112, ppl=401.55, wps=10775.1, ups=0.52, wpb=20569.8, bsz=256, num_updates=3200, lr=0.000320036, gnorm=2.271, clip=0, loss_scale=7219, train_wall=190, wall=6201
2022-07-11 11:00:10 | INFO | train_inner | epoch 003:   1057 / 1122 loss=8.258, nll_loss=4.262, mask_ins=1.432, word_ins_ml=5.758, word_reposition=1.068, ppl=306.11, wps=10821, ups=0.53, wpb=20512, bsz=256, num_updates=3300, lr=0.000330034, gnorm=2.299, clip=0, loss_scale=4096, train_wall=189, wall=6391
2022-07-11 11:02:12 | INFO | train | epoch 003 | loss 9.412 | nll_loss 5.291 | mask_ins 1.558 | word_ins_ml 6.661 | word_reposition 1.193 | ppl 681.15 | wps 10586.4 | ups 0.52 | wpb 20520.5 | bsz 255.8 | num_updates 3365 | lr 0.000336533 | gnorm 2.263 | clip 0 | loss_scale 3908 | train_wall 2119 | wall 6513
2022-07-11 11:02:52 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.184 | nll_loss 6.838 | mask_ins 1.597 | word_ins_ml 8.187 | word_reposition 1.4 | ppl 2327.22 | wps 25165.2 | wpb 2367.6 | bsz 32 | num_updates 3365 | best_loss 11.184
2022-07-11 11:02:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 3 @ 3365 updates, score 11.184) (writing took 5.833440446294844 seconds)
2022-07-11 11:04:03 | INFO | train_inner | epoch 004:     35 / 1122 loss=7.999, nll_loss=4.059, mask_ins=1.385, word_ins_ml=5.577, word_reposition=1.036, ppl=255.76, wps=8722.6, ups=0.43, wpb=20332.9, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=2.402, clip=0, loss_scale=4096, train_wall=187, wall=6624
2022-07-11 11:07:14 | INFO | train_inner | epoch 004:    135 / 1122 loss=7.806, nll_loss=3.902, mask_ins=1.352, word_ins_ml=5.439, word_reposition=1.016, ppl=223.85, wps=10782.8, ups=0.53, wpb=20505.9, bsz=256, num_updates=3500, lr=0.00035003, gnorm=2.259, clip=0, loss_scale=4096, train_wall=189, wall=6814
2022-07-11 11:10:23 | INFO | train_inner | epoch 004:    235 / 1122 loss=7.624, nll_loss=3.771, mask_ins=1.307, word_ins_ml=5.321, word_reposition=0.997, ppl=197.29, wps=10900.2, ups=0.53, wpb=20607.3, bsz=256, num_updates=3600, lr=0.000360028, gnorm=2.186, clip=0, loss_scale=4096, train_wall=188, wall=7003
2022-07-11 11:13:31 | INFO | train_inner | epoch 004:    335 / 1122 loss=7.587, nll_loss=3.744, mask_ins=1.305, word_ins_ml=5.295, word_reposition=0.987, ppl=192.28, wps=10832.4, ups=0.53, wpb=20432.2, bsz=256, num_updates=3700, lr=0.000370026, gnorm=2.186, clip=0, loss_scale=4588, train_wall=188, wall=7192
2022-07-11 11:16:41 | INFO | train_inner | epoch 004:    435 / 1122 loss=7.435, nll_loss=3.638, mask_ins=1.262, word_ins_ml=5.2, word_reposition=0.973, ppl=172.99, wps=10901.8, ups=0.53, wpb=20656.5, bsz=256, num_updates=3800, lr=0.000380024, gnorm=2.085, clip=0, loss_scale=8192, train_wall=189, wall=7381
2022-07-11 11:19:50 | INFO | train_inner | epoch 004:    535 / 1122 loss=7.443, nll_loss=3.641, mask_ins=1.264, word_ins_ml=5.2, word_reposition=0.979, ppl=173.95, wps=10835.5, ups=0.53, wpb=20481.9, bsz=256, num_updates=3900, lr=0.000390022, gnorm=2.096, clip=0, loss_scale=8192, train_wall=188, wall=7570
2022-07-11 11:22:59 | INFO | train_inner | epoch 004:    635 / 1122 loss=7.417, nll_loss=3.629, mask_ins=1.26, word_ins_ml=5.189, word_reposition=0.969, ppl=170.93, wps=10818.5, ups=0.53, wpb=20519.3, bsz=256, num_updates=4000, lr=0.00040002, gnorm=2.064, clip=0, loss_scale=8192, train_wall=189, wall=7760
2022-07-11 11:26:09 | INFO | train_inner | epoch 004:    735 / 1122 loss=7.34, nll_loss=3.602, mask_ins=1.223, word_ins_ml=5.164, word_reposition=0.953, ppl=161.99, wps=10857.9, ups=0.53, wpb=20609.6, bsz=256, num_updates=4100, lr=0.000410018, gnorm=1.979, clip=0, loss_scale=8192, train_wall=189, wall=7950
2022-07-11 11:27:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 11:29:20 | INFO | train_inner | epoch 004:    836 / 1122 loss=7.235, nll_loss=3.513, mask_ins=1.212, word_ins_ml=5.083, word_reposition=0.94, ppl=150.69, wps=10723.7, ups=0.52, wpb=20442.9, bsz=256, num_updates=4200, lr=0.000420016, gnorm=1.963, clip=0, loss_scale=5475, train_wall=190, wall=8140
2022-07-11 11:32:30 | INFO | train_inner | epoch 004:    936 / 1122 loss=7.26, nll_loss=3.529, mask_ins=1.217, word_ins_ml=5.096, word_reposition=0.947, ppl=153.26, wps=10874.6, ups=0.53, wpb=20641.5, bsz=256, num_updates=4300, lr=0.000430014, gnorm=1.976, clip=0, loss_scale=4096, train_wall=189, wall=8330
2022-07-11 11:35:39 | INFO | train_inner | epoch 004:   1036 / 1122 loss=7.207, nll_loss=3.486, mask_ins=1.213, word_ins_ml=5.057, word_reposition=0.938, ppl=147.77, wps=10816.9, ups=0.53, wpb=20455.6, bsz=256, num_updates=4400, lr=0.000440012, gnorm=1.916, clip=0, loss_scale=4096, train_wall=188, wall=8519
2022-07-11 11:38:21 | INFO | train | epoch 004 | loss 7.433 | nll_loss 3.644 | mask_ins 1.26 | word_ins_ml 5.202 | word_reposition 0.97 | ppl 172.77 | wps 10606.5 | ups 0.52 | wpb 20519.2 | bsz 255.8 | num_updates 4486 | lr 0.00044861 | gnorm 2.075 | clip 0 | loss_scale 5724 | train_wall 2114 | wall 8681
2022-07-11 11:39:00 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.637 | nll_loss 6.176 | mask_ins 1.779 | word_ins_ml 7.568 | word_reposition 1.29 | ppl 1592.85 | wps 25410.7 | wpb 2367.6 | bsz 32 | num_updates 4486 | best_loss 10.637
2022-07-11 11:39:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 4 @ 4486 updates, score 10.637) (writing took 6.135019179433584 seconds)
2022-07-11 11:39:33 | INFO | train_inner | epoch 005:     14 / 1122 loss=7.192, nll_loss=3.469, mask_ins=1.205, word_ins_ml=5.041, word_reposition=0.947, ppl=146.24, wps=8712.9, ups=0.43, wpb=20383.7, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=2.058, clip=0, loss_scale=4096, train_wall=188, wall=8753
2022-07-11 11:42:42 | INFO | train_inner | epoch 005:    114 / 1122 loss=7.119, nll_loss=3.417, mask_ins=1.189, word_ins_ml=4.994, word_reposition=0.936, ppl=138.97, wps=10891.9, ups=0.53, wpb=20625.6, bsz=256, num_updates=4600, lr=0.000460008, gnorm=1.843, clip=0, loss_scale=4096, train_wall=189, wall=8942
2022-07-11 11:44:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 11:45:54 | INFO | train_inner | epoch 005:    215 / 1122 loss=7.041, nll_loss=3.356, mask_ins=1.181, word_ins_ml=4.939, word_reposition=0.92, ppl=131.65, wps=10708.8, ups=0.52, wpb=20532.1, bsz=256, num_updates=4700, lr=0.000470006, gnorm=1.907, clip=0, loss_scale=2879, train_wall=191, wall=9134
2022-07-11 11:49:04 | INFO | train_inner | epoch 005:    315 / 1122 loss=7.036, nll_loss=3.357, mask_ins=1.178, word_ins_ml=4.939, word_reposition=0.919, ppl=131.25, wps=10791.9, ups=0.52, wpb=20565.4, bsz=256, num_updates=4800, lr=0.000480004, gnorm=1.824, clip=0, loss_scale=2048, train_wall=190, wall=9325
2022-07-11 11:52:17 | INFO | train_inner | epoch 005:    415 / 1122 loss=7.072, nll_loss=3.399, mask_ins=1.169, word_ins_ml=4.975, word_reposition=0.928, ppl=134.55, wps=10671.8, ups=0.52, wpb=20494.5, bsz=256, num_updates=4900, lr=0.000490002, gnorm=1.818, clip=0, loss_scale=2048, train_wall=191, wall=9517
2022-07-11 11:55:28 | INFO | train_inner | epoch 005:    515 / 1122 loss=6.991, nll_loss=3.318, mask_ins=1.169, word_ins_ml=4.904, word_reposition=0.918, ppl=127.18, wps=10626.2, ups=0.52, wpb=20366.2, bsz=256, num_updates=5000, lr=0.0005, gnorm=1.818, clip=0, loss_scale=2048, train_wall=191, wall=9708
2022-07-11 11:58:39 | INFO | train_inner | epoch 005:    615 / 1122 loss=7.017, nll_loss=3.347, mask_ins=1.166, word_ins_ml=4.928, word_reposition=0.923, ppl=129.5, wps=10732.8, ups=0.52, wpb=20503.9, bsz=256, num_updates=5100, lr=0.000495074, gnorm=1.76, clip=0, loss_scale=2048, train_wall=190, wall=9899
2022-07-11 12:01:50 | INFO | train_inner | epoch 005:    715 / 1122 loss=6.978, nll_loss=3.326, mask_ins=1.154, word_ins_ml=4.909, word_reposition=0.915, ppl=126.04, wps=10779.3, ups=0.53, wpb=20525, bsz=256, num_updates=5200, lr=0.00049029, gnorm=1.761, clip=0, loss_scale=3031, train_wall=190, wall=10090
2022-07-11 12:05:00 | INFO | train_inner | epoch 005:    815 / 1122 loss=6.96, nll_loss=3.313, mask_ins=1.148, word_ins_ml=4.895, word_reposition=0.917, ppl=124.53, wps=10859.6, ups=0.53, wpb=20661.6, bsz=256, num_updates=5300, lr=0.000485643, gnorm=1.767, clip=0, loss_scale=4096, train_wall=189, wall=10280
2022-07-11 12:08:10 | INFO | train_inner | epoch 005:    915 / 1122 loss=6.879, nll_loss=3.236, mask_ins=1.144, word_ins_ml=4.826, word_reposition=0.909, ppl=117.7, wps=10815.6, ups=0.53, wpb=20542.5, bsz=256, num_updates=5400, lr=0.000481125, gnorm=1.762, clip=0, loss_scale=4096, train_wall=189, wall=10470
2022-07-11 12:11:20 | INFO | train_inner | epoch 005:   1015 / 1122 loss=6.857, nll_loss=3.242, mask_ins=1.131, word_ins_ml=4.83, word_reposition=0.896, ppl=115.93, wps=10782.1, ups=0.53, wpb=20470.4, bsz=256, num_updates=5500, lr=0.000476731, gnorm=1.731, clip=0, loss_scale=4096, train_wall=189, wall=10660
2022-07-11 12:14:30 | INFO | train_inner | epoch 005:   1115 / 1122 loss=6.841, nll_loss=3.209, mask_ins=1.135, word_ins_ml=4.801, word_reposition=0.906, ppl=114.68, wps=10840.7, ups=0.53, wpb=20587.8, bsz=256, num_updates=5600, lr=0.000472456, gnorm=1.699, clip=0, loss_scale=4096, train_wall=189, wall=10850
2022-07-11 12:14:42 | INFO | train | epoch 005 | loss 6.985 | nll_loss 3.323 | mask_ins 1.161 | word_ins_ml 4.906 | word_reposition 0.918 | ppl 126.67 | wps 10546 | ups 0.51 | wpb 20520.6 | bsz 255.8 | num_updates 5607 | lr 0.000472161 | gnorm 1.803 | clip 0 | loss_scale 3161 | train_wall 2126 | wall 10862
2022-07-11 12:15:21 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.957 | nll_loss 6.35 | mask_ins 1.736 | word_ins_ml 7.747 | word_reposition 1.474 | ppl 1987.82 | wps 25337.4 | wpb 2367.6 | bsz 32 | num_updates 5607 | best_loss 10.637
2022-07-11 12:15:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 5 @ 5607 updates, score 10.957) (writing took 3.2422669269144535 seconds)
2022-07-11 12:18:23 | INFO | train_inner | epoch 006:     93 / 1122 loss=6.808, nll_loss=3.175, mask_ins=1.141, word_ins_ml=4.77, word_reposition=0.897, ppl=112.05, wps=8721.1, ups=0.43, wpb=20338.5, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=1.767, clip=0, loss_scale=5571, train_wall=190, wall=11083
2022-07-11 12:21:34 | INFO | train_inner | epoch 006:    193 / 1122 loss=6.758, nll_loss=3.155, mask_ins=1.121, word_ins_ml=4.752, word_reposition=0.886, ppl=108.22, wps=10729.6, ups=0.52, wpb=20563.4, bsz=256, num_updates=5800, lr=0.000464238, gnorm=1.655, clip=0, loss_scale=8192, train_wall=191, wall=11275
2022-07-11 12:24:45 | INFO | train_inner | epoch 006:    293 / 1122 loss=6.75, nll_loss=3.13, mask_ins=1.124, word_ins_ml=4.728, word_reposition=0.898, ppl=107.66, wps=10803.4, ups=0.52, wpb=20628.1, bsz=256, num_updates=5900, lr=0.000460287, gnorm=1.62, clip=0, loss_scale=8192, train_wall=190, wall=11466
2022-07-11 12:28:02 | INFO | train_inner | epoch 006:    393 / 1122 loss=6.694, nll_loss=3.118, mask_ins=1.103, word_ins_ml=4.717, word_reposition=0.874, ppl=103.55, wps=10439, ups=0.51, wpb=20560, bsz=256, num_updates=6000, lr=0.000456435, gnorm=1.625, clip=0, loss_scale=8192, train_wall=196, wall=11663
2022-07-11 12:29:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 12:31:18 | INFO | train_inner | epoch 006:    494 / 1122 loss=6.672, nll_loss=3.076, mask_ins=1.115, word_ins_ml=4.68, word_reposition=0.877, ppl=101.94, wps=10484.5, ups=0.51, wpb=20525.2, bsz=256, num_updates=6100, lr=0.000452679, gnorm=1.615, clip=0, loss_scale=6002, train_wall=195, wall=11858
2022-07-11 12:34:30 | INFO | train_inner | epoch 006:    594 / 1122 loss=6.674, nll_loss=3.096, mask_ins=1.103, word_ins_ml=4.696, word_reposition=0.876, ppl=102.12, wps=10591.2, ups=0.52, wpb=20349.8, bsz=256, num_updates=6200, lr=0.000449013, gnorm=1.545, clip=0, loss_scale=4096, train_wall=191, wall=12051
2022-07-11 12:37:40 | INFO | train_inner | epoch 006:    694 / 1122 loss=6.649, nll_loss=3.077, mask_ins=1.097, word_ins_ml=4.679, word_reposition=0.873, ppl=100.34, wps=10895.4, ups=0.53, wpb=20646.6, bsz=256, num_updates=6300, lr=0.000445435, gnorm=1.543, clip=0, loss_scale=4096, train_wall=189, wall=12240
2022-07-11 12:40:48 | INFO | train_inner | epoch 006:    794 / 1122 loss=6.633, nll_loss=3.059, mask_ins=1.09, word_ins_ml=4.662, word_reposition=0.881, ppl=99.24, wps=10868.2, ups=0.53, wpb=20511.3, bsz=256, num_updates=6400, lr=0.000441942, gnorm=1.593, clip=0, loss_scale=4096, train_wall=188, wall=12429
2022-07-11 12:43:59 | INFO | train_inner | epoch 006:    894 / 1122 loss=6.639, nll_loss=3.064, mask_ins=1.097, word_ins_ml=4.665, word_reposition=0.878, ppl=99.68, wps=10857, ups=0.53, wpb=20643.9, bsz=256, num_updates=6500, lr=0.000438529, gnorm=1.542, clip=0, loss_scale=4096, train_wall=189, wall=12619
2022-07-11 12:47:08 | INFO | train_inner | epoch 006:    994 / 1122 loss=6.593, nll_loss=3.058, mask_ins=1.068, word_ins_ml=4.659, word_reposition=0.865, ppl=96.51, wps=10789.4, ups=0.53, wpb=20479.4, bsz=256, num_updates=6600, lr=0.000435194, gnorm=1.508, clip=0, loss_scale=5816, train_wall=189, wall=12809
2022-07-11 12:50:17 | INFO | train_inner | epoch 006:   1094 / 1122 loss=6.535, nll_loss=2.999, mask_ins=1.079, word_ins_ml=4.606, word_reposition=0.85, ppl=92.76, wps=10854.6, ups=0.53, wpb=20508.1, bsz=256, num_updates=6700, lr=0.000431934, gnorm=1.491, clip=0, loss_scale=8192, train_wall=188, wall=12998
2022-07-11 12:51:10 | INFO | train | epoch 006 | loss 6.669 | nll_loss 3.089 | mask_ins 1.102 | word_ins_ml 4.689 | word_reposition 0.877 | ppl 101.72 | wps 10516.5 | ups 0.51 | wpb 20521.4 | bsz 255.8 | num_updates 6728 | lr 0.000431034 | gnorm 1.586 | clip 0 | loss_scale 6115 | train_wall 2135 | wall 13050
2022-07-11 12:51:49 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.441 | nll_loss 6.106 | mask_ins 1.58 | word_ins_ml 7.509 | word_reposition 1.352 | ppl 1390.44 | wps 25398 | wpb 2367.6 | bsz 32 | num_updates 6728 | best_loss 10.441
2022-07-11 12:51:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 6 @ 6728 updates, score 10.441) (writing took 6.127840610221028 seconds)
2022-07-11 12:54:11 | INFO | train_inner | epoch 007:     72 / 1122 loss=6.557, nll_loss=3.004, mask_ins=1.083, word_ins_ml=4.61, word_reposition=0.863, ppl=94.14, wps=8676.8, ups=0.43, wpb=20311.4, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=1.565, clip=0, loss_scale=8192, train_wall=188, wall=13232
2022-07-11 12:57:21 | INFO | train_inner | epoch 007:    172 / 1122 loss=6.506, nll_loss=2.957, mask_ins=1.079, word_ins_ml=4.568, word_reposition=0.859, ppl=90.87, wps=10878, ups=0.53, wpb=20598.7, bsz=256, num_updates=6900, lr=0.000425628, gnorm=1.514, clip=0, loss_scale=8192, train_wall=189, wall=13421
2022-07-11 13:00:31 | INFO | train_inner | epoch 007:    272 / 1122 loss=6.546, nll_loss=3.013, mask_ins=1.061, word_ins_ml=4.617, word_reposition=0.867, ppl=93.47, wps=10839.2, ups=0.53, wpb=20643.1, bsz=256, num_updates=7000, lr=0.000422577, gnorm=1.499, clip=0, loss_scale=8192, train_wall=190, wall=13612
2022-07-11 13:03:41 | INFO | train_inner | epoch 007:    372 / 1122 loss=6.464, nll_loss=2.947, mask_ins=1.059, word_ins_ml=4.558, word_reposition=0.847, ppl=88.26, wps=10809.9, ups=0.53, wpb=20519.3, bsz=256, num_updates=7100, lr=0.000419591, gnorm=1.494, clip=0, loss_scale=10650, train_wall=189, wall=13801
2022-07-11 13:06:51 | INFO | train_inner | epoch 007:    472 / 1122 loss=6.47, nll_loss=2.939, mask_ins=1.068, word_ins_ml=4.551, word_reposition=0.851, ppl=88.63, wps=10815.1, ups=0.53, wpb=20516.7, bsz=256, num_updates=7200, lr=0.000416667, gnorm=1.47, clip=0, loss_scale=16384, train_wall=189, wall=13991
2022-07-11 13:07:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-11 13:10:02 | INFO | train_inner | epoch 007:    573 / 1122 loss=6.463, nll_loss=2.94, mask_ins=1.061, word_ins_ml=4.55, word_reposition=0.851, ppl=88.19, wps=10662, ups=0.52, wpb=20430.8, bsz=256, num_updates=7300, lr=0.000413803, gnorm=1.462, clip=0, loss_scale=10139, train_wall=191, wall=14183
2022-07-11 13:11:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 13:13:14 | INFO | train_inner | epoch 007:    674 / 1122 loss=6.455, nll_loss=2.928, mask_ins=1.058, word_ins_ml=4.54, word_reposition=0.857, ppl=87.75, wps=10746.4, ups=0.52, wpb=20550.3, bsz=256, num_updates=7400, lr=0.000410997, gnorm=1.444, clip=0, loss_scale=6205, train_wall=190, wall=14374
2022-07-11 13:16:23 | INFO | train_inner | epoch 007:    774 / 1122 loss=6.421, nll_loss=2.903, mask_ins=1.058, word_ins_ml=4.517, word_reposition=0.847, ppl=85.7, wps=10831.1, ups=0.53, wpb=20536.4, bsz=256, num_updates=7500, lr=0.000408248, gnorm=1.499, clip=0, loss_scale=4096, train_wall=189, wall=14564
2022-07-11 13:19:33 | INFO | train_inner | epoch 007:    874 / 1122 loss=6.448, nll_loss=2.931, mask_ins=1.052, word_ins_ml=4.541, word_reposition=0.854, ppl=87.28, wps=10849.1, ups=0.53, wpb=20561.4, bsz=256, num_updates=7600, lr=0.000405554, gnorm=1.464, clip=0, loss_scale=4096, train_wall=189, wall=14753
2022-07-11 13:22:42 | INFO | train_inner | epoch 007:    974 / 1122 loss=6.384, nll_loss=2.871, mask_ins=1.047, word_ins_ml=4.488, word_reposition=0.849, ppl=83.49, wps=10871.6, ups=0.53, wpb=20551.7, bsz=256, num_updates=7700, lr=0.000402911, gnorm=1.463, clip=0, loss_scale=4096, train_wall=188, wall=14942
2022-07-11 13:25:51 | INFO | train_inner | epoch 007:   1074 / 1122 loss=6.401, nll_loss=2.898, mask_ins=1.043, word_ins_ml=4.511, word_reposition=0.847, ppl=84.49, wps=10892.5, ups=0.53, wpb=20549.6, bsz=256, num_updates=7800, lr=0.00040032, gnorm=1.454, clip=0, loss_scale=4096, train_wall=188, wall=15131
2022-07-11 13:27:20 | INFO | train | epoch 007 | loss 6.46 | nll_loss 2.935 | mask_ins 1.061 | word_ins_ml 4.546 | word_reposition 0.853 | ppl 88.04 | wps 10588.4 | ups 0.52 | wpb 20519.8 | bsz 255.8 | num_updates 7848 | lr 0.000399094 | gnorm 1.483 | clip 0 | loss_scale 7502 | train_wall 2116 | wall 15220
2022-07-11 13:27:59 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.98 | nll_loss 5.804 | mask_ins 1.499 | word_ins_ml 7.224 | word_reposition 1.257 | ppl 1009.82 | wps 25424 | wpb 2367.6 | bsz 32 | num_updates 7848 | best_loss 9.98
2022-07-11 13:28:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 7 @ 7848 updates, score 9.98) (writing took 6.039284270256758 seconds)
2022-07-11 13:29:44 | INFO | train_inner | epoch 008:     52 / 1122 loss=6.392, nll_loss=2.871, mask_ins=1.058, word_ins_ml=4.487, word_reposition=0.847, ppl=83.96, wps=8722.8, ups=0.43, wpb=20368.9, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=1.483, clip=0, loss_scale=5612, train_wall=188, wall=15364
2022-07-11 13:32:53 | INFO | train_inner | epoch 008:    152 / 1122 loss=6.379, nll_loss=2.877, mask_ins=1.043, word_ins_ml=4.493, word_reposition=0.843, ppl=83.23, wps=10856.3, ups=0.53, wpb=20539.6, bsz=256, num_updates=8000, lr=0.000395285, gnorm=1.429, clip=0, loss_scale=8192, train_wall=188, wall=15553
2022-07-11 13:34:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 13:36:04 | INFO | train_inner | epoch 008:    253 / 1122 loss=6.344, nll_loss=2.863, mask_ins=1.031, word_ins_ml=4.479, word_reposition=0.834, ppl=81.24, wps=10730, ups=0.52, wpb=20487.8, bsz=256, num_updates=8100, lr=0.000392837, gnorm=1.461, clip=0, loss_scale=5759, train_wall=190, wall=15744
2022-07-11 13:39:13 | INFO | train_inner | epoch 008:    353 / 1122 loss=6.341, nll_loss=2.852, mask_ins=1.039, word_ins_ml=4.469, word_reposition=0.833, ppl=81.04, wps=10914.5, ups=0.53, wpb=20561.6, bsz=256, num_updates=8200, lr=0.000390434, gnorm=1.396, clip=0, loss_scale=4096, train_wall=188, wall=15933
2022-07-11 13:42:22 | INFO | train_inner | epoch 008:    453 / 1122 loss=6.323, nll_loss=2.832, mask_ins=1.03, word_ins_ml=4.451, word_reposition=0.842, ppl=80.05, wps=10851.3, ups=0.53, wpb=20531.3, bsz=256, num_updates=8300, lr=0.000388075, gnorm=1.423, clip=0, loss_scale=4096, train_wall=188, wall=16122
2022-07-11 13:45:31 | INFO | train_inner | epoch 008:    553 / 1122 loss=6.314, nll_loss=2.822, mask_ins=1.03, word_ins_ml=4.442, word_reposition=0.842, ppl=79.57, wps=10866.8, ups=0.53, wpb=20569.8, bsz=256, num_updates=8400, lr=0.000385758, gnorm=1.402, clip=0, loss_scale=4096, train_wall=189, wall=16311
2022-07-11 13:48:40 | INFO | train_inner | epoch 008:    653 / 1122 loss=6.296, nll_loss=2.83, mask_ins=1.006, word_ins_ml=4.448, word_reposition=0.841, ppl=78.55, wps=10897.4, ups=0.53, wpb=20627.1, bsz=256, num_updates=8500, lr=0.000383482, gnorm=1.389, clip=0, loss_scale=4096, train_wall=188, wall=16501
2022-07-11 13:51:50 | INFO | train_inner | epoch 008:    753 / 1122 loss=6.274, nll_loss=2.806, mask_ins=1.011, word_ins_ml=4.427, word_reposition=0.836, ppl=77.37, wps=10780.6, ups=0.53, wpb=20475.9, bsz=256, num_updates=8600, lr=0.000381246, gnorm=1.41, clip=0, loss_scale=6062, train_wall=189, wall=16691
2022-07-11 13:55:00 | INFO | train_inner | epoch 008:    853 / 1122 loss=6.272, nll_loss=2.811, mask_ins=1.006, word_ins_ml=4.431, word_reposition=0.836, ppl=77.28, wps=10841.6, ups=0.53, wpb=20552.7, bsz=256, num_updates=8700, lr=0.000379049, gnorm=1.396, clip=0, loss_scale=8192, train_wall=189, wall=16880
2022-07-11 13:58:10 | INFO | train_inner | epoch 008:    953 / 1122 loss=6.206, nll_loss=2.758, mask_ins=0.995, word_ins_ml=4.383, word_reposition=0.828, ppl=73.83, wps=10797.3, ups=0.52, wpb=20582, bsz=256, num_updates=8800, lr=0.000376889, gnorm=1.372, clip=0, loss_scale=8192, train_wall=190, wall=17071
2022-07-11 13:59:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 14:01:21 | INFO | train_inner | epoch 008:   1054 / 1122 loss=6.233, nll_loss=2.79, mask_ins=0.994, word_ins_ml=4.411, word_reposition=0.829, ppl=75.24, wps=10667.3, ups=0.52, wpb=20373.1, bsz=256, num_updates=8900, lr=0.000374766, gnorm=1.409, clip=0, loss_scale=5353, train_wall=190, wall=17262
2022-07-11 14:03:30 | INFO | train | epoch 008 | loss 6.301 | nll_loss 2.825 | mask_ins 1.019 | word_ins_ml 4.444 | word_reposition 0.837 | ppl 78.82 | wps 10595.2 | ups 0.52 | wpb 20521.8 | bsz 255.8 | num_updates 8968 | lr 0.000373342 | gnorm 1.412 | clip 0 | loss_scale 5764 | train_wall 2115 | wall 17390
2022-07-11 14:04:08 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.155 | nll_loss 5.944 | mask_ins 1.493 | word_ins_ml 7.366 | word_reposition 1.296 | ppl 1139.83 | wps 25439.4 | wpb 2367.6 | bsz 32 | num_updates 8968 | best_loss 9.98
2022-07-11 14:04:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 8 @ 8968 updates, score 10.155) (writing took 3.174942970275879 seconds)
2022-07-11 14:05:12 | INFO | train_inner | epoch 009:     32 / 1122 loss=6.242, nll_loss=2.792, mask_ins=0.998, word_ins_ml=4.413, word_reposition=0.83, ppl=75.68, wps=8798.1, ups=0.43, wpb=20299.1, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=1.428, clip=0, loss_scale=4096, train_wall=188, wall=17492
2022-07-11 14:08:22 | INFO | train_inner | epoch 009:    132 / 1122 loss=6.228, nll_loss=2.796, mask_ins=0.985, word_ins_ml=4.416, word_reposition=0.826, ppl=74.94, wps=10898.2, ups=0.53, wpb=20651.8, bsz=256, num_updates=9100, lr=0.000370625, gnorm=1.364, clip=0, loss_scale=4096, train_wall=189, wall=17682
2022-07-11 14:11:30 | INFO | train_inner | epoch 009:    232 / 1122 loss=6.191, nll_loss=2.759, mask_ins=0.981, word_ins_ml=4.383, word_reposition=0.828, ppl=73.08, wps=10856.3, ups=0.53, wpb=20468.8, bsz=256, num_updates=9200, lr=0.000368605, gnorm=1.417, clip=0, loss_scale=4096, train_wall=188, wall=17870
2022-07-11 14:14:39 | INFO | train_inner | epoch 009:    332 / 1122 loss=6.146, nll_loss=2.724, mask_ins=0.977, word_ins_ml=4.352, word_reposition=0.817, ppl=70.81, wps=10835, ups=0.53, wpb=20458.5, bsz=256, num_updates=9300, lr=0.000366618, gnorm=1.386, clip=0, loss_scale=4096, train_wall=188, wall=18059
2022-07-11 14:17:48 | INFO | train_inner | epoch 009:    432 / 1122 loss=6.173, nll_loss=2.764, mask_ins=0.961, word_ins_ml=4.386, word_reposition=0.825, ppl=72.13, wps=10853.3, ups=0.53, wpb=20481.1, bsz=256, num_updates=9400, lr=0.000364662, gnorm=1.381, clip=0, loss_scale=6472, train_wall=188, wall=18248
2022-07-11 14:20:58 | INFO | train_inner | epoch 009:    532 / 1122 loss=6.159, nll_loss=2.75, mask_ins=0.969, word_ins_ml=4.374, word_reposition=0.816, ppl=71.45, wps=10819.1, ups=0.53, wpb=20547.4, bsz=256, num_updates=9500, lr=0.000362738, gnorm=1.377, clip=0, loss_scale=8192, train_wall=189, wall=18438
2022-07-11 14:24:08 | INFO | train_inner | epoch 009:    632 / 1122 loss=6.145, nll_loss=2.743, mask_ins=0.96, word_ins_ml=4.367, word_reposition=0.818, ppl=70.79, wps=10797.5, ups=0.53, wpb=20545, bsz=256, num_updates=9600, lr=0.000360844, gnorm=1.389, clip=0, loss_scale=8192, train_wall=189, wall=18628
2022-07-11 14:24:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 14:27:20 | INFO | train_inner | epoch 009:    733 / 1122 loss=6.133, nll_loss=2.739, mask_ins=0.959, word_ins_ml=4.363, word_reposition=0.811, ppl=70.2, wps=10737.9, ups=0.52, wpb=20573.4, bsz=256, num_updates=9700, lr=0.000358979, gnorm=1.337, clip=0, loss_scale=4583, train_wall=191, wall=18820
2022-07-11 14:30:29 | INFO | train_inner | epoch 009:    833 / 1122 loss=6.153, nll_loss=2.761, mask_ins=0.958, word_ins_ml=4.383, word_reposition=0.812, ppl=71.16, wps=10820.2, ups=0.53, wpb=20506.8, bsz=256, num_updates=9800, lr=0.000357143, gnorm=1.396, clip=0, loss_scale=4096, train_wall=189, wall=19009
2022-07-11 14:33:39 | INFO | train_inner | epoch 009:    933 / 1122 loss=6.118, nll_loss=2.73, mask_ins=0.95, word_ins_ml=4.355, word_reposition=0.813, ppl=69.47, wps=10833.2, ups=0.53, wpb=20556.2, bsz=256, num_updates=9900, lr=0.000355335, gnorm=1.402, clip=0, loss_scale=4096, train_wall=189, wall=19199
2022-07-11 14:36:48 | INFO | train_inner | epoch 009:   1033 / 1122 loss=6.121, nll_loss=2.723, mask_ins=0.956, word_ins_ml=4.349, word_reposition=0.816, ppl=69.59, wps=10814.5, ups=0.53, wpb=20508.2, bsz=256, num_updates=10000, lr=0.000353553, gnorm=1.384, clip=0, loss_scale=4096, train_wall=189, wall=19389
2022-07-11 14:39:37 | INFO | train | epoch 009 | loss 6.154 | nll_loss 2.745 | mask_ins 0.965 | word_ins_ml 4.37 | word_reposition 0.819 | ppl 71.23 | wps 10614.2 | ups 0.52 | wpb 20521.4 | bsz 255.8 | num_updates 10089 | lr 0.000351991 | gnorm 1.389 | clip 0 | loss_scale 5082 | train_wall 2116 | wall 19557
2022-07-11 14:40:16 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.982 | nll_loss 5.8 | mask_ins 1.457 | word_ins_ml 7.228 | word_reposition 1.297 | ppl 1011.49 | wps 25420.5 | wpb 2367.6 | bsz 32 | num_updates 10089 | best_loss 9.98
2022-07-11 14:40:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 9 @ 10089 updates, score 9.982) (writing took 3.205883380956948 seconds)
2022-07-11 14:40:40 | INFO | train_inner | epoch 010:     11 / 1122 loss=6.129, nll_loss=2.716, mask_ins=0.959, word_ins_ml=4.342, word_reposition=0.827, ppl=69.97, wps=8849.2, ups=0.43, wpb=20490.2, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=1.46, clip=0, loss_scale=4096, train_wall=189, wall=19620
2022-07-11 14:43:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 14:43:52 | INFO | train_inner | epoch 010:    112 / 1122 loss=6.045, nll_loss=2.659, mask_ins=0.94, word_ins_ml=4.292, word_reposition=0.813, ppl=66.03, wps=10730.2, ups=0.52, wpb=20567.5, bsz=256, num_updates=10200, lr=0.00035007, gnorm=1.355, clip=0, loss_scale=6570, train_wall=191, wall=19812
2022-07-11 14:47:01 | INFO | train_inner | epoch 010:    212 / 1122 loss=6.075, nll_loss=2.696, mask_ins=0.938, word_ins_ml=4.325, word_reposition=0.812, ppl=67.42, wps=10823.2, ups=0.53, wpb=20540.9, bsz=256, num_updates=10300, lr=0.000348367, gnorm=1.386, clip=0, loss_scale=4096, train_wall=189, wall=20002
2022-07-11 14:50:12 | INFO | train_inner | epoch 010:    312 / 1122 loss=6.084, nll_loss=2.684, mask_ins=0.947, word_ins_ml=4.313, word_reposition=0.824, ppl=67.84, wps=10821.6, ups=0.52, wpb=20613.3, bsz=256, num_updates=10400, lr=0.000346688, gnorm=1.395, clip=0, loss_scale=4096, train_wall=190, wall=20192
2022-07-11 14:53:22 | INFO | train_inner | epoch 010:    412 / 1122 loss=6.057, nll_loss=2.68, mask_ins=0.939, word_ins_ml=4.31, word_reposition=0.808, ppl=66.57, wps=10798.1, ups=0.53, wpb=20535.3, bsz=256, num_updates=10500, lr=0.000345033, gnorm=1.37, clip=0, loss_scale=4096, train_wall=189, wall=20382
2022-07-11 14:56:31 | INFO | train_inner | epoch 010:    512 / 1122 loss=6.063, nll_loss=2.689, mask_ins=0.938, word_ins_ml=4.317, word_reposition=0.808, ppl=66.85, wps=10854.5, ups=0.53, wpb=20528.8, bsz=256, num_updates=10600, lr=0.000343401, gnorm=1.371, clip=0, loss_scale=4096, train_wall=188, wall=20572
2022-07-11 14:59:41 | INFO | train_inner | epoch 010:    612 / 1122 loss=6.055, nll_loss=2.67, mask_ins=0.945, word_ins_ml=4.3, word_reposition=0.81, ppl=66.49, wps=10925, ups=0.53, wpb=20692.8, bsz=256, num_updates=10700, lr=0.000341793, gnorm=1.363, clip=0, loss_scale=4301, train_wall=189, wall=20761
2022-07-11 15:02:49 | INFO | train_inner | epoch 010:    712 / 1122 loss=6.04, nll_loss=2.664, mask_ins=0.943, word_ins_ml=4.295, word_reposition=0.802, ppl=65.81, wps=10815.7, ups=0.53, wpb=20411.2, bsz=256, num_updates=10800, lr=0.000340207, gnorm=1.378, clip=0, loss_scale=8192, train_wall=188, wall=20950
2022-07-11 15:05:59 | INFO | train_inner | epoch 010:    812 / 1122 loss=6.05, nll_loss=2.685, mask_ins=0.933, word_ins_ml=4.313, word_reposition=0.803, ppl=66.24, wps=10839.6, ups=0.53, wpb=20532.8, bsz=256, num_updates=10900, lr=0.000338643, gnorm=1.355, clip=0, loss_scale=8192, train_wall=188, wall=21139
2022-07-11 15:09:08 | INFO | train_inner | epoch 010:    912 / 1122 loss=6.027, nll_loss=2.657, mask_ins=0.937, word_ins_ml=4.288, word_reposition=0.802, ppl=65.21, wps=10850.9, ups=0.53, wpb=20531.3, bsz=256, num_updates=11000, lr=0.0003371, gnorm=1.405, clip=0, loss_scale=8192, train_wall=188, wall=21328
2022-07-11 15:12:18 | INFO | train_inner | epoch 010:   1012 / 1122 loss=6.026, nll_loss=2.667, mask_ins=0.931, word_ins_ml=4.296, word_reposition=0.799, ppl=65.17, wps=10797.6, ups=0.53, wpb=20459, bsz=256, num_updates=11100, lr=0.000335578, gnorm=1.39, clip=0, loss_scale=8192, train_wall=189, wall=21518
2022-07-11 15:15:27 | INFO | train_inner | epoch 010:   1112 / 1122 loss=5.958, nll_loss=2.621, mask_ins=0.911, word_ins_ml=4.256, word_reposition=0.791, ppl=62.15, wps=10824.4, ups=0.53, wpb=20477.5, bsz=256, num_updates=11200, lr=0.000334077, gnorm=1.329, clip=0, loss_scale=8192, train_wall=188, wall=21707
2022-07-11 15:15:45 | INFO | train | epoch 010 | loss 6.044 | nll_loss 2.67 | mask_ins 0.937 | word_ins_ml 4.301 | word_reposition 0.807 | ppl 65.99 | wps 10610.3 | ups 0.52 | wpb 20521 | bsz 255.8 | num_updates 11210 | lr 0.000333928 | gnorm 1.378 | clip 0 | loss_scale 6221 | train_wall 2116 | wall 21725
2022-07-11 15:16:24 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.773 | nll_loss 5.676 | mask_ins 1.443 | word_ins_ml 7.11 | word_reposition 1.22 | ppl 874.9 | wps 25397.1 | wpb 2367.6 | bsz 32 | num_updates 11210 | best_loss 9.773
2022-07-11 15:16:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 10 @ 11210 updates, score 9.773) (writing took 6.332457168027759 seconds)
2022-07-11 15:17:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-11 15:19:23 | INFO | train_inner | epoch 011:     91 / 1122 loss=5.968, nll_loss=2.611, mask_ins=0.923, word_ins_ml=4.247, word_reposition=0.798, ppl=62.6, wps=8634.8, ups=0.42, wpb=20384.6, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=1.416, clip=0, loss_scale=10220, train_wall=190, wall=21943
2022-07-11 15:21:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 15:22:34 | INFO | train_inner | epoch 011:    192 / 1122 loss=5.982, nll_loss=2.627, mask_ins=0.925, word_ins_ml=4.26, word_reposition=0.797, ppl=63.22, wps=10715.6, ups=0.52, wpb=20489.3, bsz=256, num_updates=11400, lr=0.000331133, gnorm=1.357, clip=0, loss_scale=7259, train_wall=190, wall=22134
2022-07-11 15:25:43 | INFO | train_inner | epoch 011:    292 / 1122 loss=5.969, nll_loss=2.627, mask_ins=0.917, word_ins_ml=4.26, word_reposition=0.792, ppl=62.63, wps=10837, ups=0.53, wpb=20533.6, bsz=256, num_updates=11500, lr=0.00032969, gnorm=1.33, clip=0, loss_scale=4096, train_wall=189, wall=22324
2022-07-11 15:28:53 | INFO | train_inner | epoch 011:    392 / 1122 loss=5.967, nll_loss=2.611, mask_ins=0.92, word_ins_ml=4.246, word_reposition=0.801, ppl=62.54, wps=10840.3, ups=0.53, wpb=20547.9, bsz=256, num_updates=11600, lr=0.000328266, gnorm=1.393, clip=0, loss_scale=4096, train_wall=189, wall=22513
2022-07-11 15:32:02 | INFO | train_inner | epoch 011:    492 / 1122 loss=6.01, nll_loss=2.663, mask_ins=0.912, word_ins_ml=4.292, word_reposition=0.806, ppl=64.45, wps=10881.7, ups=0.53, wpb=20616.4, bsz=256, num_updates=11700, lr=0.00032686, gnorm=1.414, clip=0, loss_scale=4096, train_wall=189, wall=22703
2022-07-11 15:32:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 15:35:14 | INFO | train_inner | epoch 011:    593 / 1122 loss=5.984, nll_loss=2.638, mask_ins=0.914, word_ins_ml=4.27, word_reposition=0.801, ppl=63.29, wps=10744.6, ups=0.52, wpb=20571.1, bsz=256, num_updates=11800, lr=0.000325472, gnorm=1.432, clip=0, loss_scale=2251, train_wall=191, wall=22894
2022-07-11 15:38:24 | INFO | train_inner | epoch 011:    693 / 1122 loss=5.944, nll_loss=2.593, mask_ins=0.914, word_ins_ml=4.23, word_reposition=0.801, ppl=61.58, wps=10789.3, ups=0.53, wpb=20535.8, bsz=256, num_updates=11900, lr=0.000324102, gnorm=1.382, clip=0, loss_scale=2048, train_wall=189, wall=23085
2022-07-11 15:41:34 | INFO | train_inner | epoch 011:    793 / 1122 loss=5.964, nll_loss=2.642, mask_ins=0.908, word_ins_ml=4.273, word_reposition=0.783, ppl=62.43, wps=10705.4, ups=0.53, wpb=20347.2, bsz=256, num_updates=12000, lr=0.000322749, gnorm=1.389, clip=0, loss_scale=2048, train_wall=189, wall=23275
2022-07-11 15:44:44 | INFO | train_inner | epoch 011:    893 / 1122 loss=5.931, nll_loss=2.608, mask_ins=0.897, word_ins_ml=4.242, word_reposition=0.792, ppl=61.03, wps=10895.3, ups=0.53, wpb=20615.3, bsz=256, num_updates=12100, lr=0.000321412, gnorm=1.382, clip=0, loss_scale=2048, train_wall=188, wall=23464
2022-07-11 15:47:53 | INFO | train_inner | epoch 011:    993 / 1122 loss=5.956, nll_loss=2.611, mask_ins=0.914, word_ins_ml=4.245, word_reposition=0.797, ppl=62.07, wps=10824.5, ups=0.53, wpb=20493.1, bsz=256, num_updates=12200, lr=0.000320092, gnorm=1.422, clip=0, loss_scale=2048, train_wall=189, wall=23653
2022-07-11 15:51:02 | INFO | train_inner | epoch 011:   1093 / 1122 loss=5.932, nll_loss=2.597, mask_ins=0.906, word_ins_ml=4.232, word_reposition=0.793, ppl=61.04, wps=10864.2, ups=0.53, wpb=20575.7, bsz=256, num_updates=12300, lr=0.000318788, gnorm=1.469, clip=0, loss_scale=3666, train_wall=189, wall=23843
2022-07-11 15:51:56 | INFO | train | epoch 011 | loss 5.963 | nll_loss 2.62 | mask_ins 0.914 | word_ins_ml 4.253 | word_reposition 0.796 | ppl 62.4 | wps 10575 | ups 0.52 | wpb 20521.1 | bsz 255.8 | num_updates 12329 | lr 0.000318413 | gnorm 1.396 | clip 0 | loss_scale 3939 | train_wall 2116 | wall 23897
2022-07-11 15:52:35 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.057 | nll_loss 5.826 | mask_ins 1.445 | word_ins_ml 7.263 | word_reposition 1.35 | ppl 1065.33 | wps 25422.1 | wpb 2367.6 | bsz 32 | num_updates 12329 | best_loss 9.773
2022-07-11 15:52:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 11 @ 12329 updates, score 10.057) (writing took 3.4507948476821184 seconds)
2022-07-11 15:54:53 | INFO | train_inner | epoch 012:     71 / 1122 loss=5.918, nll_loss=2.579, mask_ins=0.91, word_ins_ml=4.216, word_reposition=0.792, ppl=60.48, wps=8842.5, ups=0.43, wpb=20434.5, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=1.398, clip=0, loss_scale=4096, train_wall=188, wall=24074
2022-07-11 15:58:02 | INFO | train_inner | epoch 012:    171 / 1122 loss=5.855, nll_loss=2.529, mask_ins=0.899, word_ins_ml=4.172, word_reposition=0.784, ppl=57.87, wps=10854.7, ups=0.53, wpb=20505.6, bsz=256, num_updates=12500, lr=0.000316228, gnorm=1.4, clip=0, loss_scale=4096, train_wall=188, wall=24263
2022-07-11 16:01:11 | INFO | train_inner | epoch 012:    271 / 1122 loss=5.925, nll_loss=2.594, mask_ins=0.903, word_ins_ml=4.23, word_reposition=0.793, ppl=60.76, wps=10848.1, ups=0.53, wpb=20503.2, bsz=256, num_updates=12600, lr=0.00031497, gnorm=1.428, clip=0, loss_scale=4096, train_wall=188, wall=24452
2022-07-11 16:04:19 | INFO | train_inner | epoch 012:    371 / 1122 loss=5.889, nll_loss=2.56, mask_ins=0.897, word_ins_ml=4.199, word_reposition=0.793, ppl=59.25, wps=10948.1, ups=0.53, wpb=20590.8, bsz=256, num_updates=12700, lr=0.000313728, gnorm=1.383, clip=0, loss_scale=4096, train_wall=187, wall=24640
2022-07-11 16:07:27 | INFO | train_inner | epoch 012:    471 / 1122 loss=5.879, nll_loss=2.562, mask_ins=0.891, word_ins_ml=4.2, word_reposition=0.788, ppl=58.84, wps=10905.1, ups=0.53, wpb=20504.1, bsz=256, num_updates=12800, lr=0.0003125, gnorm=1.321, clip=0, loss_scale=6840, train_wall=187, wall=24828
2022-07-11 16:08:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 16:10:38 | INFO | train_inner | epoch 012:    572 / 1122 loss=5.873, nll_loss=2.545, mask_ins=0.893, word_ins_ml=4.185, word_reposition=0.794, ppl=58.59, wps=10791.9, ups=0.52, wpb=20586.6, bsz=256, num_updates=12900, lr=0.000311286, gnorm=1.37, clip=0, loss_scale=5434, train_wall=190, wall=25018
2022-07-11 16:13:46 | INFO | train_inner | epoch 012:    672 / 1122 loss=5.88, nll_loss=2.571, mask_ins=0.89, word_ins_ml=4.208, word_reposition=0.783, ppl=58.89, wps=10849.8, ups=0.53, wpb=20390.2, bsz=256, num_updates=13000, lr=0.000310087, gnorm=1.369, clip=0, loss_scale=4096, train_wall=187, wall=25206
2022-07-11 16:16:55 | INFO | train_inner | epoch 012:    772 / 1122 loss=5.832, nll_loss=2.537, mask_ins=0.877, word_ins_ml=4.178, word_reposition=0.776, ppl=56.95, wps=10913, ups=0.53, wpb=20614.2, bsz=256, num_updates=13100, lr=0.000308901, gnorm=1.311, clip=0, loss_scale=4096, train_wall=188, wall=25395
2022-07-11 16:20:03 | INFO | train_inner | epoch 012:    872 / 1122 loss=5.865, nll_loss=2.557, mask_ins=0.887, word_ins_ml=4.195, word_reposition=0.782, ppl=58.27, wps=10916.8, ups=0.53, wpb=20549.9, bsz=256, num_updates=13200, lr=0.000307729, gnorm=1.315, clip=0, loss_scale=4096, train_wall=187, wall=25583
2022-07-11 16:23:12 | INFO | train_inner | epoch 012:    972 / 1122 loss=5.87, nll_loss=2.565, mask_ins=0.885, word_ins_ml=4.202, word_reposition=0.782, ppl=58.49, wps=10880.8, ups=0.53, wpb=20502.8, bsz=256, num_updates=13300, lr=0.00030657, gnorm=1.331, clip=0, loss_scale=4096, train_wall=188, wall=25772
2022-07-11 16:25:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 16:26:22 | INFO | train_inner | epoch 012:   1073 / 1122 loss=5.856, nll_loss=2.558, mask_ins=0.882, word_ins_ml=4.196, word_reposition=0.778, ppl=57.92, wps=10756, ups=0.52, wpb=20499.5, bsz=256, num_updates=13400, lr=0.000305424, gnorm=1.329, clip=0, loss_scale=5799, train_wall=190, wall=25962
2022-07-11 16:27:54 | INFO | train | epoch 012 | loss 5.875 | nll_loss 2.56 | mask_ins 0.891 | word_ins_ml 4.198 | word_reposition 0.786 | ppl 58.68 | wps 10650.8 | ups 0.52 | wpb 20519.1 | bsz 255.8 | num_updates 13449 | lr 0.000304867 | gnorm 1.361 | clip 0 | loss_scale 4614 | train_wall 2106 | wall 26054
2022-07-11 16:28:33 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.026 | nll_loss 5.802 | mask_ins 1.486 | word_ins_ml 7.239 | word_reposition 1.301 | ppl 1042.3 | wps 25400 | wpb 2367.6 | bsz 32 | num_updates 13449 | best_loss 9.773
2022-07-11 16:28:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 12 @ 13449 updates, score 10.026) (writing took 3.304601957090199 seconds)
2022-07-11 16:30:13 | INFO | train_inner | epoch 013:     51 / 1122 loss=5.861, nll_loss=2.553, mask_ins=0.884, word_ins_ml=4.191, word_reposition=0.786, ppl=58.13, wps=8848.2, ups=0.43, wpb=20385.5, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=1.411, clip=0, loss_scale=4096, train_wall=187, wall=26193
2022-07-11 16:33:21 | INFO | train_inner | epoch 013:    151 / 1122 loss=5.855, nll_loss=2.547, mask_ins=0.882, word_ins_ml=4.186, word_reposition=0.787, ppl=57.88, wps=10905, ups=0.53, wpb=20584.8, bsz=256, num_updates=13600, lr=0.00030317, gnorm=1.31, clip=0, loss_scale=4096, train_wall=188, wall=26382
2022-07-11 16:36:30 | INFO | train_inner | epoch 013:    251 / 1122 loss=5.852, nll_loss=2.552, mask_ins=0.88, word_ins_ml=4.19, word_reposition=0.782, ppl=57.77, wps=10833, ups=0.53, wpb=20461.1, bsz=256, num_updates=13700, lr=0.000302061, gnorm=1.299, clip=0, loss_scale=4096, train_wall=188, wall=26571
2022-07-11 16:39:39 | INFO | train_inner | epoch 013:    351 / 1122 loss=5.864, nll_loss=2.576, mask_ins=0.873, word_ins_ml=4.211, word_reposition=0.78, ppl=58.26, wps=10869.9, ups=0.53, wpb=20567.3, bsz=256, num_updates=13800, lr=0.000300965, gnorm=1.314, clip=0, loss_scale=4096, train_wall=188, wall=26760
2022-07-11 16:42:49 | INFO | train_inner | epoch 013:    451 / 1122 loss=5.87, nll_loss=2.568, mask_ins=0.878, word_ins_ml=4.204, word_reposition=0.788, ppl=58.5, wps=10838.7, ups=0.53, wpb=20571.4, bsz=256, num_updates=13900, lr=0.00029988, gnorm=1.339, clip=0, loss_scale=4219, train_wall=189, wall=26950
2022-07-11 16:46:00 | INFO | train_inner | epoch 013:    551 / 1122 loss=5.787, nll_loss=2.5, mask_ins=0.867, word_ins_ml=4.144, word_reposition=0.776, ppl=55.2, wps=10760.6, ups=0.52, wpb=20521.2, bsz=256, num_updates=14000, lr=0.000298807, gnorm=1.326, clip=0, loss_scale=8192, train_wall=190, wall=27140
2022-07-11 16:49:09 | INFO | train_inner | epoch 013:    651 / 1122 loss=5.837, nll_loss=2.539, mask_ins=0.88, word_ins_ml=4.178, word_reposition=0.778, ppl=57.15, wps=10810.4, ups=0.53, wpb=20469.1, bsz=256, num_updates=14100, lr=0.000297746, gnorm=1.362, clip=0, loss_scale=8192, train_wall=189, wall=27330
2022-07-11 16:52:19 | INFO | train_inner | epoch 013:    751 / 1122 loss=5.882, nll_loss=2.589, mask_ins=0.878, word_ins_ml=4.222, word_reposition=0.783, ppl=58.98, wps=10846.4, ups=0.53, wpb=20523.8, bsz=256, num_updates=14200, lr=0.000296695, gnorm=1.33, clip=0, loss_scale=8192, train_wall=188, wall=27519
2022-07-11 16:53:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 16:55:30 | INFO | train_inner | epoch 013:    852 / 1122 loss=5.832, nll_loss=2.548, mask_ins=0.87, word_ins_ml=4.186, word_reposition=0.776, ppl=56.97, wps=10689.2, ups=0.52, wpb=20512.7, bsz=256, num_updates=14300, lr=0.000295656, gnorm=1.345, clip=0, loss_scale=5110, train_wall=191, wall=27711
2022-07-11 16:58:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 16:58:42 | INFO | train_inner | epoch 013:    953 / 1122 loss=5.809, nll_loss=2.526, mask_ins=0.868, word_ins_ml=4.166, word_reposition=0.775, ppl=56.06, wps=10669, ups=0.52, wpb=20472.5, bsz=256, num_updates=14400, lr=0.000294628, gnorm=1.395, clip=0, loss_scale=3873, train_wall=191, wall=27903
2022-07-11 17:01:52 | INFO | train_inner | epoch 013:   1053 / 1122 loss=5.769, nll_loss=2.495, mask_ins=0.862, word_ins_ml=4.139, word_reposition=0.768, ppl=54.53, wps=10884.2, ups=0.53, wpb=20658.7, bsz=256, num_updates=14500, lr=0.00029361, gnorm=1.399, clip=0, loss_scale=2048, train_wall=189, wall=28092
2022-07-11 17:04:03 | INFO | train | epoch 013 | loss 5.835 | nll_loss 2.542 | mask_ins 0.874 | word_ins_ml 4.181 | word_reposition 0.78 | ppl 57.06 | wps 10597.9 | ups 0.52 | wpb 20519.6 | bsz 255.8 | num_updates 14569 | lr 0.000292914 | gnorm 1.35 | clip 0 | loss_scale 4965 | train_wall 2117 | wall 28223
2022-07-11 17:04:42 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.761 | nll_loss 5.646 | mask_ins 1.45 | word_ins_ml 7.089 | word_reposition 1.222 | ppl 867.57 | wps 25449.1 | wpb 2367.6 | bsz 32 | num_updates 14569 | best_loss 9.761
2022-07-11 17:04:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 13 @ 14569 updates, score 9.761) (writing took 6.175590152852237 seconds)
2022-07-11 17:05:46 | INFO | train_inner | epoch 014:     31 / 1122 loss=5.789, nll_loss=2.503, mask_ins=0.867, word_ins_ml=4.146, word_reposition=0.776, ppl=55.28, wps=8694.2, ups=0.43, wpb=20372.8, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=1.442, clip=0, loss_scale=2048, train_wall=188, wall=28327
2022-07-11 17:08:56 | INFO | train_inner | epoch 014:    131 / 1122 loss=5.841, nll_loss=2.555, mask_ins=0.869, word_ins_ml=4.192, word_reposition=0.78, ppl=57.31, wps=10855.7, ups=0.53, wpb=20530.4, bsz=256, num_updates=14700, lr=0.000291606, gnorm=1.381, clip=0, loss_scale=2048, train_wall=188, wall=28516
2022-07-11 17:12:04 | INFO | train_inner | epoch 014:    231 / 1122 loss=5.764, nll_loss=2.494, mask_ins=0.859, word_ins_ml=4.137, word_reposition=0.768, ppl=54.33, wps=10879.4, ups=0.53, wpb=20534.5, bsz=256, num_updates=14800, lr=0.000290619, gnorm=1.35, clip=0, loss_scale=2048, train_wall=188, wall=28705
2022-07-11 17:15:13 | INFO | train_inner | epoch 014:    331 / 1122 loss=5.743, nll_loss=2.463, mask_ins=0.858, word_ins_ml=4.11, word_reposition=0.775, ppl=53.56, wps=10876.3, ups=0.53, wpb=20565.6, bsz=256, num_updates=14900, lr=0.000289642, gnorm=1.313, clip=0, loss_scale=2048, train_wall=188, wall=28894
2022-07-11 17:18:23 | INFO | train_inner | epoch 014:    431 / 1122 loss=5.75, nll_loss=2.477, mask_ins=0.851, word_ins_ml=4.123, word_reposition=0.776, ppl=53.81, wps=10824.7, ups=0.53, wpb=20498, bsz=256, num_updates=15000, lr=0.000288675, gnorm=1.321, clip=0, loss_scale=4076, train_wall=189, wall=29083
2022-07-11 17:21:33 | INFO | train_inner | epoch 014:    531 / 1122 loss=5.729, nll_loss=2.456, mask_ins=0.86, word_ins_ml=4.103, word_reposition=0.766, ppl=53.04, wps=10791.4, ups=0.53, wpb=20526, bsz=256, num_updates=15100, lr=0.000287718, gnorm=1.329, clip=0, loss_scale=4096, train_wall=189, wall=29273
2022-07-11 17:24:42 | INFO | train_inner | epoch 014:    631 / 1122 loss=5.737, nll_loss=2.463, mask_ins=0.859, word_ins_ml=4.109, word_reposition=0.769, ppl=53.35, wps=10822.5, ups=0.53, wpb=20479.5, bsz=256, num_updates=15200, lr=0.00028677, gnorm=1.297, clip=0, loss_scale=4096, train_wall=188, wall=29462
2022-07-11 17:27:52 | INFO | train_inner | epoch 014:    731 / 1122 loss=5.719, nll_loss=2.448, mask_ins=0.85, word_ins_ml=4.096, word_reposition=0.773, ppl=52.69, wps=10878.3, ups=0.53, wpb=20680.8, bsz=256, num_updates=15300, lr=0.000285831, gnorm=1.3, clip=0, loss_scale=4096, train_wall=189, wall=29653
2022-07-11 17:31:04 | INFO | train_inner | epoch 014:    831 / 1122 loss=5.772, nll_loss=2.512, mask_ins=0.857, word_ins_ml=4.152, word_reposition=0.763, ppl=54.65, wps=10656.1, ups=0.52, wpb=20403.3, bsz=256, num_updates=15400, lr=0.000284901, gnorm=1.321, clip=0, loss_scale=4096, train_wall=191, wall=29844
2022-07-11 17:32:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 17:34:38 | INFO | train_inner | epoch 014:    932 / 1122 loss=5.766, nll_loss=2.477, mask_ins=0.868, word_ins_ml=4.122, word_reposition=0.776, ppl=54.43, wps=9574.6, ups=0.47, wpb=20489.8, bsz=256, num_updates=15500, lr=0.000283981, gnorm=1.328, clip=0, loss_scale=4542, train_wall=213, wall=30058
2022-07-11 17:37:48 | INFO | train_inner | epoch 014:   1032 / 1122 loss=5.77, nll_loss=2.493, mask_ins=0.855, word_ins_ml=4.135, word_reposition=0.779, ppl=54.57, wps=10821.4, ups=0.53, wpb=20542.7, bsz=256, num_updates=15600, lr=0.000283069, gnorm=1.279, clip=0, loss_scale=4096, train_wall=189, wall=30248
2022-07-11 17:40:38 | INFO | train | epoch 014 | loss 5.759 | nll_loss 2.484 | mask_ins 0.858 | word_ins_ml 4.128 | word_reposition 0.773 | ppl 54.14 | wps 10475.5 | ups 0.51 | wpb 20519.4 | bsz 255.8 | num_updates 15690 | lr 0.000282256 | gnorm 1.326 | clip 0 | loss_scale 3530 | train_wall 2142 | wall 30419
2022-07-11 17:41:18 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.701 | nll_loss 5.572 | mask_ins 1.442 | word_ins_ml 7.019 | word_reposition 1.239 | ppl 832.34 | wps 25375.5 | wpb 2367.6 | bsz 32 | num_updates 15690 | best_loss 9.701
2022-07-11 17:41:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 14 @ 15690 updates, score 9.701) (writing took 5.9835527604445815 seconds)
2022-07-11 17:41:42 | INFO | train_inner | epoch 015:     10 / 1122 loss=5.752, nll_loss=2.483, mask_ins=0.855, word_ins_ml=4.126, word_reposition=0.77, ppl=53.9, wps=8712.3, ups=0.43, wpb=20450.5, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=1.346, clip=0, loss_scale=4096, train_wall=189, wall=30483
2022-07-11 17:44:52 | INFO | train_inner | epoch 015:    110 / 1122 loss=5.785, nll_loss=2.502, mask_ins=0.861, word_ins_ml=4.144, word_reposition=0.78, ppl=55.12, wps=10888.5, ups=0.53, wpb=20673.3, bsz=256, num_updates=15800, lr=0.000281272, gnorm=1.295, clip=0, loss_scale=4096, train_wall=189, wall=30673
2022-07-11 17:48:02 | INFO | train_inner | epoch 015:    210 / 1122 loss=5.686, nll_loss=2.43, mask_ins=0.844, word_ins_ml=4.079, word_reposition=0.763, ppl=51.49, wps=10834.4, ups=0.53, wpb=20539.3, bsz=256, num_updates=15900, lr=0.000280386, gnorm=1.292, clip=0, loss_scale=4096, train_wall=189, wall=30862
2022-07-11 17:51:12 | INFO | train_inner | epoch 015:    310 / 1122 loss=5.697, nll_loss=2.436, mask_ins=0.846, word_ins_ml=4.085, word_reposition=0.766, ppl=51.88, wps=10799.9, ups=0.53, wpb=20562.4, bsz=256, num_updates=16000, lr=0.000279508, gnorm=1.313, clip=0, loss_scale=6758, train_wall=189, wall=31052
2022-07-11 17:54:21 | INFO | train_inner | epoch 015:    410 / 1122 loss=5.738, nll_loss=2.458, mask_ins=0.859, word_ins_ml=4.104, word_reposition=0.776, ppl=53.38, wps=10864.5, ups=0.53, wpb=20543, bsz=256, num_updates=16100, lr=0.000278639, gnorm=1.303, clip=0, loss_scale=8192, train_wall=188, wall=31242
2022-07-11 17:56:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 17:57:32 | INFO | train_inner | epoch 015:    511 / 1122 loss=5.749, nll_loss=2.489, mask_ins=0.848, word_ins_ml=4.132, word_reposition=0.769, ppl=53.77, wps=10738.4, ups=0.52, wpb=20482.3, bsz=256, num_updates=16200, lr=0.000277778, gnorm=1.305, clip=0, loss_scale=6935, train_wall=190, wall=31432
2022-07-11 18:00:42 | INFO | train_inner | epoch 015:    611 / 1122 loss=5.729, nll_loss=2.469, mask_ins=0.846, word_ins_ml=4.113, word_reposition=0.77, ppl=53.05, wps=10816.2, ups=0.53, wpb=20533.4, bsz=256, num_updates=16300, lr=0.000276924, gnorm=1.292, clip=0, loss_scale=4096, train_wall=189, wall=31622
2022-07-11 18:03:52 | INFO | train_inner | epoch 015:    711 / 1122 loss=5.767, nll_loss=2.498, mask_ins=0.859, word_ins_ml=4.139, word_reposition=0.769, ppl=54.45, wps=10775.5, ups=0.53, wpb=20459.5, bsz=256, num_updates=16400, lr=0.000276079, gnorm=1.284, clip=0, loss_scale=4096, train_wall=189, wall=31812
2022-07-11 18:07:02 | INFO | train_inner | epoch 015:    811 / 1122 loss=5.717, nll_loss=2.466, mask_ins=0.846, word_ins_ml=4.11, word_reposition=0.761, ppl=52.62, wps=10817.2, ups=0.53, wpb=20535.3, bsz=256, num_updates=16500, lr=0.000275241, gnorm=1.292, clip=0, loss_scale=4096, train_wall=189, wall=32002
2022-07-11 18:10:12 | INFO | train_inner | epoch 015:    911 / 1122 loss=5.712, nll_loss=2.459, mask_ins=0.85, word_ins_ml=4.104, word_reposition=0.758, ppl=52.43, wps=10794.2, ups=0.53, wpb=20501.7, bsz=256, num_updates=16600, lr=0.000274411, gnorm=1.285, clip=0, loss_scale=4096, train_wall=189, wall=32192
2022-07-11 18:13:22 | INFO | train_inner | epoch 015:   1011 / 1122 loss=5.709, nll_loss=2.454, mask_ins=0.843, word_ins_ml=4.1, word_reposition=0.767, ppl=52.31, wps=10776.4, ups=0.53, wpb=20509.5, bsz=256, num_updates=16700, lr=0.000273588, gnorm=1.298, clip=0, loss_scale=4874, train_wall=189, wall=32382
2022-07-11 18:14:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 18:16:33 | INFO | train_inner | epoch 015:   1112 / 1122 loss=5.675, nll_loss=2.416, mask_ins=0.841, word_ins_ml=4.066, word_reposition=0.768, ppl=51.08, wps=10760.2, ups=0.52, wpb=20617.9, bsz=256, num_updates=16800, lr=0.000272772, gnorm=1.322, clip=0, loss_scale=6002, train_wall=191, wall=32574
2022-07-11 18:16:52 | INFO | train | epoch 015 | loss 5.723 | nll_loss 2.461 | mask_ins 0.849 | word_ins_ml 4.107 | word_reposition 0.767 | ppl 52.84 | wps 10576.6 | ups 0.52 | wpb 20521.8 | bsz 255.8 | num_updates 16810 | lr 0.000272691 | gnorm 1.304 | clip 0 | loss_scale 5195 | train_wall 2119 | wall 32592
2022-07-11 18:17:31 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.796 | nll_loss 5.652 | mask_ins 1.467 | word_ins_ml 7.091 | word_reposition 1.238 | ppl 888.95 | wps 25383.4 | wpb 2367.6 | bsz 32 | num_updates 16810 | best_loss 9.701
2022-07-11 18:17:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 15 @ 16810 updates, score 9.796) (writing took 3.069145099259913 seconds)
2022-07-11 18:20:24 | INFO | train_inner | epoch 016:     90 / 1122 loss=5.709, nll_loss=2.458, mask_ins=0.842, word_ins_ml=4.103, word_reposition=0.764, ppl=52.3, wps=8778.8, ups=0.43, wpb=20272.3, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=1.39, clip=0, loss_scale=4096, train_wall=188, wall=32805
2022-07-11 18:23:35 | INFO | train_inner | epoch 016:    190 / 1122 loss=5.737, nll_loss=2.498, mask_ins=0.835, word_ins_ml=4.139, word_reposition=0.763, ppl=53.35, wps=10766.1, ups=0.53, wpb=20493.9, bsz=256, num_updates=17000, lr=0.000271163, gnorm=1.305, clip=0, loss_scale=4096, train_wall=189, wall=32995
2022-07-11 18:26:46 | INFO | train_inner | epoch 016:    290 / 1122 loss=5.66, nll_loss=2.403, mask_ins=0.843, word_ins_ml=4.054, word_reposition=0.763, ppl=50.57, wps=10805.2, ups=0.52, wpb=20656.5, bsz=256, num_updates=17100, lr=0.000270369, gnorm=1.287, clip=0, loss_scale=4096, train_wall=190, wall=33186
2022-07-11 18:29:57 | INFO | train_inner | epoch 016:    390 / 1122 loss=5.716, nll_loss=2.464, mask_ins=0.839, word_ins_ml=4.108, word_reposition=0.769, ppl=52.57, wps=10707.1, ups=0.52, wpb=20436.9, bsz=256, num_updates=17200, lr=0.000269582, gnorm=1.314, clip=0, loss_scale=4096, train_wall=190, wall=33377
2022-07-11 18:33:08 | INFO | train_inner | epoch 016:    490 / 1122 loss=5.717, nll_loss=2.471, mask_ins=0.842, word_ins_ml=4.114, word_reposition=0.761, ppl=52.61, wps=10815.4, ups=0.52, wpb=20636.2, bsz=256, num_updates=17300, lr=0.000268802, gnorm=1.301, clip=0, loss_scale=5816, train_wall=190, wall=33568
2022-07-11 18:36:17 | INFO | train_inner | epoch 016:    590 / 1122 loss=5.654, nll_loss=2.415, mask_ins=0.832, word_ins_ml=4.065, word_reposition=0.757, ppl=50.36, wps=10848.3, ups=0.53, wpb=20538.8, bsz=256, num_updates=17400, lr=0.000268028, gnorm=1.29, clip=0, loss_scale=8192, train_wall=188, wall=33757
2022-07-11 18:39:26 | INFO | train_inner | epoch 016:    690 / 1122 loss=5.657, nll_loss=2.422, mask_ins=0.83, word_ins_ml=4.071, word_reposition=0.756, ppl=50.45, wps=10868.8, ups=0.53, wpb=20600.1, bsz=256, num_updates=17500, lr=0.000267261, gnorm=1.305, clip=0, loss_scale=8192, train_wall=189, wall=33947
2022-07-11 18:40:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 18:42:39 | INFO | train_inner | epoch 016:    791 / 1122 loss=5.666, nll_loss=2.415, mask_ins=0.839, word_ins_ml=4.064, word_reposition=0.762, ppl=50.76, wps=10641.8, ups=0.52, wpb=20525.9, bsz=256, num_updates=17600, lr=0.000266501, gnorm=1.262, clip=0, loss_scale=6002, train_wall=192, wall=34140
2022-07-11 18:45:50 | INFO | train_inner | epoch 016:    891 / 1122 loss=5.634, nll_loss=2.395, mask_ins=0.832, word_ins_ml=4.047, word_reposition=0.755, ppl=49.65, wps=10773.7, ups=0.53, wpb=20512.4, bsz=256, num_updates=17700, lr=0.000265747, gnorm=1.253, clip=0, loss_scale=4096, train_wall=190, wall=34330
2022-07-11 18:49:00 | INFO | train_inner | epoch 016:    991 / 1122 loss=5.675, nll_loss=2.424, mask_ins=0.839, word_ins_ml=4.072, word_reposition=0.764, ppl=51.09, wps=10837.5, ups=0.53, wpb=20561.8, bsz=256, num_updates=17800, lr=0.000264999, gnorm=1.296, clip=0, loss_scale=4096, train_wall=189, wall=34520
2022-07-11 18:52:08 | INFO | train_inner | epoch 016:   1091 / 1122 loss=5.672, nll_loss=2.436, mask_ins=0.829, word_ins_ml=4.082, word_reposition=0.761, ppl=50.97, wps=10853.1, ups=0.53, wpb=20490.8, bsz=256, num_updates=17900, lr=0.000264258, gnorm=1.296, clip=0, loss_scale=4096, train_wall=188, wall=34709
2022-07-11 18:53:06 | INFO | train | epoch 016 | loss 5.683 | nll_loss 2.438 | mask_ins 0.837 | word_ins_ml 4.085 | word_reposition 0.762 | ppl 51.39 | wps 10578.1 | ups 0.52 | wpb 20521.3 | bsz 255.8 | num_updates 17931 | lr 0.00026403 | gnorm 1.3 | clip 0 | loss_scale 5151 | train_wall 2123 | wall 34767
2022-07-11 18:53:45 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.725 | nll_loss 5.593 | mask_ins 1.444 | word_ins_ml 7.037 | word_reposition 1.244 | ppl 846.09 | wps 25441.3 | wpb 2367.6 | bsz 32 | num_updates 17931 | best_loss 9.701
2022-07-11 18:53:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 16 @ 17931 updates, score 9.725) (writing took 3.160126129165292 seconds)
2022-07-11 18:55:59 | INFO | train_inner | epoch 017:     69 / 1122 loss=5.693, nll_loss=2.442, mask_ins=0.837, word_ins_ml=4.089, word_reposition=0.768, ppl=51.75, wps=8871, ups=0.43, wpb=20440.5, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=1.347, clip=0, loss_scale=4096, train_wall=188, wall=34939
2022-07-11 18:58:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 18:59:10 | INFO | train_inner | epoch 017:    170 / 1122 loss=5.688, nll_loss=2.447, mask_ins=0.835, word_ins_ml=4.092, word_reposition=0.762, ppl=51.57, wps=10732.1, ups=0.52, wpb=20514.7, bsz=256, num_updates=18100, lr=0.000262794, gnorm=1.281, clip=0, loss_scale=4867, train_wall=190, wall=35130
2022-07-11 19:02:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 19:02:21 | INFO | train_inner | epoch 017:    271 / 1122 loss=5.689, nll_loss=2.435, mask_ins=0.84, word_ins_ml=4.082, word_reposition=0.767, ppl=51.6, wps=10744, ups=0.52, wpb=20489.5, bsz=256, num_updates=18200, lr=0.000262071, gnorm=1.292, clip=0, loss_scale=4035, train_wall=190, wall=35321
2022-07-11 19:05:30 | INFO | train_inner | epoch 017:    371 / 1122 loss=5.673, nll_loss=2.429, mask_ins=0.84, word_ins_ml=4.077, word_reposition=0.756, ppl=51.02, wps=10777.2, ups=0.53, wpb=20443.3, bsz=256, num_updates=18300, lr=0.000261354, gnorm=1.311, clip=0, loss_scale=2048, train_wall=189, wall=35511
2022-07-11 19:08:40 | INFO | train_inner | epoch 017:    471 / 1122 loss=5.673, nll_loss=2.443, mask_ins=0.828, word_ins_ml=4.088, word_reposition=0.756, ppl=51.01, wps=10797, ups=0.53, wpb=20464.9, bsz=256, num_updates=18400, lr=0.000260643, gnorm=1.284, clip=0, loss_scale=2048, train_wall=189, wall=35700
2022-07-11 19:11:49 | INFO | train_inner | epoch 017:    571 / 1122 loss=5.622, nll_loss=2.388, mask_ins=0.83, word_ins_ml=4.04, word_reposition=0.752, ppl=49.23, wps=10842.5, ups=0.53, wpb=20484.6, bsz=256, num_updates=18500, lr=0.000259938, gnorm=1.335, clip=0, loss_scale=2048, train_wall=188, wall=35889
2022-07-11 19:14:58 | INFO | train_inner | epoch 017:    671 / 1122 loss=5.613, nll_loss=2.383, mask_ins=0.822, word_ins_ml=4.035, word_reposition=0.756, ppl=48.94, wps=10906, ups=0.53, wpb=20634.3, bsz=256, num_updates=18600, lr=0.000259238, gnorm=1.282, clip=0, loss_scale=2048, train_wall=188, wall=36078
2022-07-11 19:18:07 | INFO | train_inner | epoch 017:    771 / 1122 loss=5.618, nll_loss=2.379, mask_ins=0.828, word_ins_ml=4.032, word_reposition=0.758, ppl=49.1, wps=10942.2, ups=0.53, wpb=20713.9, bsz=256, num_updates=18700, lr=0.000258544, gnorm=1.292, clip=0, loss_scale=2048, train_wall=189, wall=36267
2022-07-11 19:21:17 | INFO | train_inner | epoch 017:    871 / 1122 loss=5.61, nll_loss=2.372, mask_ins=0.827, word_ins_ml=4.026, word_reposition=0.757, ppl=48.85, wps=10788.8, ups=0.53, wpb=20511.2, bsz=256, num_updates=18800, lr=0.000257855, gnorm=1.294, clip=0, loss_scale=3912, train_wall=189, wall=36458
2022-07-11 19:24:28 | INFO | train_inner | epoch 017:    971 / 1122 loss=5.663, nll_loss=2.428, mask_ins=0.83, word_ins_ml=4.075, word_reposition=0.758, ppl=50.66, wps=10739.4, ups=0.52, wpb=20489.2, bsz=256, num_updates=18900, lr=0.000257172, gnorm=1.316, clip=0, loss_scale=4096, train_wall=190, wall=36648
2022-07-11 19:27:39 | INFO | train_inner | epoch 017:   1071 / 1122 loss=5.614, nll_loss=2.389, mask_ins=0.82, word_ins_ml=4.04, word_reposition=0.753, ppl=48.96, wps=10777.8, ups=0.52, wpb=20539.6, bsz=256, num_updates=19000, lr=0.000256495, gnorm=1.314, clip=0, loss_scale=4096, train_wall=190, wall=36839
2022-07-11 19:29:15 | INFO | train | epoch 017 | loss 5.646 | nll_loss 2.409 | mask_ins 0.83 | word_ins_ml 4.058 | word_reposition 0.758 | ppl 50.07 | wps 10597.4 | ups 0.52 | wpb 20519.8 | bsz 255.8 | num_updates 19051 | lr 0.000256151 | gnorm 1.302 | clip 0 | loss_scale 3231 | train_wall 2118 | wall 36935
2022-07-11 19:29:54 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.74 | nll_loss 5.583 | mask_ins 1.443 | word_ins_ml 7.032 | word_reposition 1.266 | ppl 855.03 | wps 25380.3 | wpb 2367.6 | bsz 32 | num_updates 19051 | best_loss 9.701
2022-07-11 19:29:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 17 @ 19051 updates, score 9.74) (writing took 3.3586043221876025 seconds)
2022-07-11 19:31:31 | INFO | train_inner | epoch 018:     49 / 1122 loss=5.629, nll_loss=2.4, mask_ins=0.826, word_ins_ml=4.05, word_reposition=0.753, ppl=49.49, wps=8755.9, ups=0.43, wpb=20294.4, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=1.308, clip=0, loss_scale=4096, train_wall=189, wall=37071
2022-07-11 19:34:41 | INFO | train_inner | epoch 018:    149 / 1122 loss=5.622, nll_loss=2.392, mask_ins=0.821, word_ins_ml=4.043, word_reposition=0.758, ppl=49.26, wps=10814.6, ups=0.53, wpb=20564.6, bsz=256, num_updates=19200, lr=0.000255155, gnorm=1.283, clip=0, loss_scale=4096, train_wall=189, wall=37261
2022-07-11 19:37:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 19:37:52 | INFO | train_inner | epoch 018:    250 / 1122 loss=5.602, nll_loss=2.368, mask_ins=0.828, word_ins_ml=4.022, word_reposition=0.752, ppl=48.56, wps=10772.2, ups=0.52, wpb=20582.2, bsz=256, num_updates=19300, lr=0.000254493, gnorm=1.292, clip=0, loss_scale=6975, train_wall=190, wall=37452
2022-07-11 19:41:02 | INFO | train_inner | epoch 018:    350 / 1122 loss=5.606, nll_loss=2.382, mask_ins=0.823, word_ins_ml=4.033, word_reposition=0.751, ppl=48.72, wps=10785.1, ups=0.53, wpb=20470.2, bsz=256, num_updates=19400, lr=0.000253837, gnorm=1.264, clip=0, loss_scale=4096, train_wall=189, wall=37642
2022-07-11 19:44:12 | INFO | train_inner | epoch 018:    450 / 1122 loss=5.569, nll_loss=2.355, mask_ins=0.814, word_ins_ml=4.01, word_reposition=0.746, ppl=47.48, wps=10796.6, ups=0.53, wpb=20563.1, bsz=256, num_updates=19500, lr=0.000253185, gnorm=1.278, clip=0, loss_scale=4096, train_wall=190, wall=37832
2022-07-11 19:47:21 | INFO | train_inner | epoch 018:    550 / 1122 loss=5.598, nll_loss=2.38, mask_ins=0.818, word_ins_ml=4.032, word_reposition=0.747, ppl=48.44, wps=10811.1, ups=0.53, wpb=20465.2, bsz=256, num_updates=19600, lr=0.000252538, gnorm=1.346, clip=0, loss_scale=4096, train_wall=188, wall=38022
2022-07-11 19:50:31 | INFO | train_inner | epoch 018:    650 / 1122 loss=5.619, nll_loss=2.391, mask_ins=0.823, word_ins_ml=4.041, word_reposition=0.754, ppl=49.13, wps=10777.1, ups=0.53, wpb=20447.7, bsz=256, num_updates=19700, lr=0.000251896, gnorm=1.317, clip=0, loss_scale=4096, train_wall=189, wall=38211
2022-07-11 19:53:41 | INFO | train_inner | epoch 018:    750 / 1122 loss=5.647, nll_loss=2.402, mask_ins=0.834, word_ins_ml=4.052, word_reposition=0.762, ppl=50.13, wps=10876.8, ups=0.53, wpb=20607.3, bsz=256, num_updates=19800, lr=0.000251259, gnorm=1.267, clip=0, loss_scale=4096, train_wall=189, wall=38401
2022-07-11 19:56:51 | INFO | train_inner | epoch 018:    850 / 1122 loss=5.626, nll_loss=2.389, mask_ins=0.83, word_ins_ml=4.04, word_reposition=0.757, ppl=49.39, wps=10896.2, ups=0.53, wpb=20712.2, bsz=256, num_updates=19900, lr=0.000250627, gnorm=1.26, clip=0, loss_scale=8069, train_wall=189, wall=38591
2022-07-11 19:59:59 | INFO | train_inner | epoch 018:    950 / 1122 loss=5.615, nll_loss=2.39, mask_ins=0.821, word_ins_ml=4.041, word_reposition=0.754, ppl=49.02, wps=10829.3, ups=0.53, wpb=20446.4, bsz=256, num_updates=20000, lr=0.00025, gnorm=1.305, clip=0, loss_scale=8192, train_wall=188, wall=38780
2022-07-11 20:03:09 | INFO | train_inner | epoch 018:   1050 / 1122 loss=5.616, nll_loss=2.385, mask_ins=0.824, word_ins_ml=4.035, word_reposition=0.757, ppl=49.06, wps=10839.4, ups=0.53, wpb=20521.1, bsz=256, num_updates=20100, lr=0.000249377, gnorm=1.249, clip=0, loss_scale=8192, train_wall=189, wall=38969
2022-07-11 20:04:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 20:05:24 | INFO | train | epoch 018 | loss 5.615 | nll_loss 2.386 | mask_ins 0.824 | word_ins_ml 4.037 | word_reposition 0.754 | ppl 49.01 | wps 10598.6 | ups 0.52 | wpb 20521.3 | bsz 255.8 | num_updates 20171 | lr 0.000248938 | gnorm 1.289 | clip 0 | loss_scale 5542 | train_wall 2117 | wall 39104
2022-07-11 20:06:03 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 9.694 | nll_loss 5.537 | mask_ins 1.43 | word_ins_ml 6.991 | word_reposition 1.274 | ppl 828.54 | wps 25428.5 | wpb 2367.6 | bsz 32 | num_updates 20171 | best_loss 9.694
2022-07-11 20:06:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 18 @ 20171 updates, score 9.694) (writing took 6.264390813186765 seconds)
2022-07-11 20:07:04 | INFO | train_inner | epoch 019:     29 / 1122 loss=5.623, nll_loss=2.391, mask_ins=0.83, word_ins_ml=4.041, word_reposition=0.751, ppl=49.27, wps=8642.8, ups=0.43, wpb=20325.9, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=1.347, clip=0, loss_scale=5232, train_wall=189, wall=39204
2022-07-11 20:10:13 | INFO | train_inner | epoch 019:    129 / 1122 loss=5.583, nll_loss=2.365, mask_ins=0.818, word_ins_ml=4.019, word_reposition=0.747, ppl=47.95, wps=10743.5, ups=0.53, wpb=20343.6, bsz=256, num_updates=20300, lr=0.000248146, gnorm=1.269, clip=0, loss_scale=4096, train_wall=189, wall=39393
2022-07-11 20:13:24 | INFO | train_inner | epoch 019:    229 / 1122 loss=5.62, nll_loss=2.395, mask_ins=0.821, word_ins_ml=4.044, word_reposition=0.754, ppl=49.18, wps=10783.7, ups=0.53, wpb=20517.7, bsz=256, num_updates=20400, lr=0.000247537, gnorm=1.306, clip=0, loss_scale=4096, train_wall=189, wall=39584
2022-07-11 20:16:34 | INFO | train_inner | epoch 019:    329 / 1122 loss=5.621, nll_loss=2.398, mask_ins=0.821, word_ins_ml=4.048, word_reposition=0.752, ppl=49.22, wps=10785.9, ups=0.53, wpb=20530.9, bsz=256, num_updates=20500, lr=0.000246932, gnorm=1.283, clip=0, loss_scale=4096, train_wall=189, wall=39774
2022-07-11 20:19:45 | INFO | train_inner | epoch 019:    429 / 1122 loss=5.599, nll_loss=2.381, mask_ins=0.818, word_ins_ml=4.032, word_reposition=0.749, ppl=48.47, wps=10686.4, ups=0.52, wpb=20385.9, bsz=256, num_updates=20600, lr=0.000246332, gnorm=1.32, clip=0, loss_scale=4096, train_wall=190, wall=39965
2022-07-11 20:22:55 | INFO | train_inner | epoch 019:    529 / 1122 loss=5.617, nll_loss=2.391, mask_ins=0.825, word_ins_ml=4.041, word_reposition=0.751, ppl=49.06, wps=10870.4, ups=0.53, wpb=20686.4, bsz=256, num_updates=20700, lr=0.000245737, gnorm=1.27, clip=0, loss_scale=6595, train_wall=189, wall=40155
2022-07-11 20:26:05 | INFO | train_inner | epoch 019:    629 / 1122 loss=5.585, nll_loss=2.379, mask_ins=0.813, word_ins_ml=4.03, word_reposition=0.743, ppl=48.02, wps=10858, ups=0.53, wpb=20616.3, bsz=256, num_updates=20800, lr=0.000245145, gnorm=1.261, clip=0, loss_scale=8192, train_wall=189, wall=40345
2022-07-11 20:29:15 | INFO | train_inner | epoch 019:    729 / 1122 loss=5.575, nll_loss=2.354, mask_ins=0.814, word_ins_ml=4.008, word_reposition=0.754, ppl=47.68, wps=10868.3, ups=0.53, wpb=20697.1, bsz=256, num_updates=20900, lr=0.000244558, gnorm=1.262, clip=0, loss_scale=8192, train_wall=190, wall=40535
2022-07-11 20:30:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 20:32:27 | INFO | train_inner | epoch 019:    830 / 1122 loss=5.587, nll_loss=2.371, mask_ins=0.813, word_ins_ml=4.023, word_reposition=0.751, ppl=48.08, wps=10711.6, ups=0.52, wpb=20535.9, bsz=256, num_updates=21000, lr=0.000243975, gnorm=1.313, clip=0, loss_scale=5394, train_wall=191, wall=40727
2022-07-11 20:35:36 | INFO | train_inner | epoch 019:    930 / 1122 loss=5.57, nll_loss=2.36, mask_ins=0.814, word_ins_ml=4.013, word_reposition=0.743, ppl=47.51, wps=10845.7, ups=0.53, wpb=20457, bsz=256, num_updates=21100, lr=0.000243396, gnorm=1.273, clip=0, loss_scale=4096, train_wall=188, wall=40916
2022-07-11 20:38:45 | INFO | train_inner | epoch 019:   1030 / 1122 loss=5.567, nll_loss=2.353, mask_ins=0.811, word_ins_ml=4.007, word_reposition=0.749, ppl=47.41, wps=10850.2, ups=0.53, wpb=20600.8, bsz=256, num_updates=21200, lr=0.000242821, gnorm=1.278, clip=0, loss_scale=4096, train_wall=189, wall=41106
2022-07-11 20:41:40 | INFO | train | epoch 019 | loss 5.592 | nll_loss 2.372 | mask_ins 0.818 | word_ins_ml 4.024 | word_reposition 0.75 | ppl 48.22 | wps 10566.9 | ups 0.51 | wpb 20519.9 | bsz 255.8 | num_updates 21292 | lr 0.000242296 | gnorm 1.286 | clip 0 | loss_scale 5166 | train_wall 2122 | wall 41281
2022-07-11 20:42:19 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.676 | nll_loss 5.536 | mask_ins 1.429 | word_ins_ml 6.991 | word_reposition 1.257 | ppl 818.21 | wps 25440.5 | wpb 2367.6 | bsz 32 | num_updates 21292 | best_loss 9.676
2022-07-11 20:42:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 19 @ 21292 updates, score 9.676) (writing took 6.076524514704943 seconds)
2022-07-11 20:42:41 | INFO | train_inner | epoch 020:      8 / 1122 loss=5.576, nll_loss=2.35, mask_ins=0.82, word_ins_ml=4.004, word_reposition=0.752, ppl=47.69, wps=8686.3, ups=0.43, wpb=20435.2, bsz=253.8, num_updates=21300, lr=0.000242251, gnorm=1.303, clip=0, loss_scale=4096, train_wall=189, wall=41341
2022-07-11 20:45:50 | INFO | train_inner | epoch 020:    108 / 1122 loss=5.56, nll_loss=2.348, mask_ins=0.81, word_ins_ml=4.002, word_reposition=0.748, ppl=47.17, wps=10832.1, ups=0.53, wpb=20505.7, bsz=256, num_updates=21400, lr=0.000241684, gnorm=1.292, clip=0, loss_scale=4096, train_wall=188, wall=41530
2022-07-11 20:48:59 | INFO | train_inner | epoch 020:    208 / 1122 loss=5.573, nll_loss=2.353, mask_ins=0.815, word_ins_ml=4.006, word_reposition=0.753, ppl=47.62, wps=10868.9, ups=0.53, wpb=20549.4, bsz=256, num_updates=21500, lr=0.000241121, gnorm=1.284, clip=0, loss_scale=6431, train_wall=188, wall=41719
2022-07-11 20:51:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 20:52:11 | INFO | train_inner | epoch 020:    309 / 1122 loss=5.609, nll_loss=2.383, mask_ins=0.821, word_ins_ml=4.033, word_reposition=0.754, ppl=48.81, wps=10736.4, ups=0.52, wpb=20595.9, bsz=256, num_updates=21600, lr=0.000240563, gnorm=1.272, clip=0, loss_scale=7786, train_wall=191, wall=41911
2022-07-11 20:55:21 | INFO | train_inner | epoch 020:    409 / 1122 loss=5.559, nll_loss=2.354, mask_ins=0.808, word_ins_ml=4.008, word_reposition=0.744, ppl=47.15, wps=10833.6, ups=0.53, wpb=20581.9, bsz=256, num_updates=21700, lr=0.000240008, gnorm=1.289, clip=0, loss_scale=4096, train_wall=189, wall=42101
2022-07-11 20:58:30 | INFO | train_inner | epoch 020:    509 / 1122 loss=5.525, nll_loss=2.313, mask_ins=0.808, word_ins_ml=3.971, word_reposition=0.746, ppl=46.05, wps=10821.5, ups=0.53, wpb=20508.8, bsz=256, num_updates=21800, lr=0.000239457, gnorm=1.245, clip=0, loss_scale=4096, train_wall=189, wall=42291
2022-07-11 21:01:40 | INFO | train_inner | epoch 020:    609 / 1122 loss=5.559, nll_loss=2.343, mask_ins=0.817, word_ins_ml=3.998, word_reposition=0.744, ppl=47.16, wps=10875.4, ups=0.53, wpb=20610.9, bsz=256, num_updates=21900, lr=0.000238909, gnorm=1.295, clip=0, loss_scale=4096, train_wall=189, wall=42480
2022-07-11 21:04:48 | INFO | train_inner | epoch 020:    709 / 1122 loss=5.53, nll_loss=2.319, mask_ins=0.811, word_ins_ml=3.976, word_reposition=0.743, ppl=46.21, wps=10865.6, ups=0.53, wpb=20421.4, bsz=256, num_updates=22000, lr=0.000238366, gnorm=1.256, clip=0, loss_scale=4096, train_wall=187, wall=42668
2022-07-11 21:07:56 | INFO | train_inner | epoch 020:    809 / 1122 loss=5.539, nll_loss=2.321, mask_ins=0.815, word_ins_ml=3.978, word_reposition=0.746, ppl=46.5, wps=10861.3, ups=0.53, wpb=20478.6, bsz=256, num_updates=22100, lr=0.000237826, gnorm=1.276, clip=0, loss_scale=4096, train_wall=188, wall=42857
2022-07-11 21:11:06 | INFO | train_inner | epoch 020:    909 / 1122 loss=5.52, nll_loss=2.298, mask_ins=0.815, word_ins_ml=3.958, word_reposition=0.747, ppl=45.88, wps=10841.1, ups=0.53, wpb=20594.6, bsz=256, num_updates=22200, lr=0.000237289, gnorm=1.252, clip=0, loss_scale=8110, train_wall=189, wall=43047
2022-07-11 21:14:16 | INFO | train_inner | epoch 020:   1009 / 1122 loss=5.557, nll_loss=2.349, mask_ins=0.806, word_ins_ml=4.002, word_reposition=0.749, ppl=47.07, wps=10830.4, ups=0.53, wpb=20537.6, bsz=256, num_updates=22300, lr=0.000236757, gnorm=1.292, clip=0, loss_scale=8192, train_wall=189, wall=43236
2022-07-11 21:17:26 | INFO | train_inner | epoch 020:   1109 / 1122 loss=5.569, nll_loss=2.358, mask_ins=0.816, word_ins_ml=4.011, word_reposition=0.742, ppl=47.48, wps=10816.7, ups=0.53, wpb=20508.6, bsz=256, num_updates=22400, lr=0.000236228, gnorm=1.321, clip=0, loss_scale=8192, train_wall=189, wall=43426
2022-07-11 21:17:50 | INFO | train | epoch 020 | loss 5.555 | nll_loss 2.34 | mask_ins 0.813 | word_ins_ml 3.995 | word_reposition 0.747 | ppl 47.01 | wps 10604.3 | ups 0.52 | wpb 20521.2 | bsz 255.8 | num_updates 22413 | lr 0.000236159 | gnorm 1.283 | clip 0 | loss_scale 5772 | train_wall 2115 | wall 43450
2022-07-11 21:18:29 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.679 | nll_loss 5.498 | mask_ins 1.436 | word_ins_ml 6.954 | word_reposition 1.29 | ppl 819.76 | wps 25397.3 | wpb 2367.6 | bsz 32 | num_updates 22413 | best_loss 9.676
2022-07-11 21:18:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 20 @ 22413 updates, score 9.679) (writing took 3.318061794154346 seconds)
2022-07-11 21:21:17 | INFO | train_inner | epoch 021:     87 / 1122 loss=5.538, nll_loss=2.327, mask_ins=0.811, word_ins_ml=3.983, word_reposition=0.744, ppl=46.45, wps=8832.7, ups=0.43, wpb=20436.7, bsz=253.8, num_updates=22500, lr=0.000235702, gnorm=1.298, clip=0, loss_scale=8192, train_wall=188, wall=43657
2022-07-11 21:24:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 21:24:29 | INFO | train_inner | epoch 021:    188 / 1122 loss=5.552, nll_loss=2.347, mask_ins=0.812, word_ins_ml=4, word_reposition=0.739, ppl=46.9, wps=10664.8, ups=0.52, wpb=20474.8, bsz=256, num_updates=22600, lr=0.00023518, gnorm=1.288, clip=0, loss_scale=7989, train_wall=191, wall=43849
2022-07-11 21:27:39 | INFO | train_inner | epoch 021:    288 / 1122 loss=5.546, nll_loss=2.341, mask_ins=0.805, word_ins_ml=3.995, word_reposition=0.746, ppl=46.73, wps=10868.1, ups=0.53, wpb=20648.6, bsz=256, num_updates=22700, lr=0.000234662, gnorm=1.275, clip=0, loss_scale=4096, train_wall=189, wall=44039
2022-07-11 21:30:50 | INFO | train_inner | epoch 021:    388 / 1122 loss=5.513, nll_loss=2.302, mask_ins=0.812, word_ins_ml=3.961, word_reposition=0.74, ppl=45.67, wps=10773.2, ups=0.52, wpb=20523.1, bsz=256, num_updates=22800, lr=0.000234146, gnorm=1.265, clip=0, loss_scale=4096, train_wall=190, wall=44230
2022-07-11 21:34:00 | INFO | train_inner | epoch 021:    488 / 1122 loss=5.522, nll_loss=2.316, mask_ins=0.803, word_ins_ml=3.972, word_reposition=0.746, ppl=45.94, wps=10818.9, ups=0.52, wpb=20631.2, bsz=256, num_updates=22900, lr=0.000233635, gnorm=1.251, clip=0, loss_scale=4096, train_wall=190, wall=44420
2022-07-11 21:37:11 | INFO | train_inner | epoch 021:    588 / 1122 loss=5.486, nll_loss=2.277, mask_ins=0.803, word_ins_ml=3.939, word_reposition=0.744, ppl=44.8, wps=10783.2, ups=0.53, wpb=20534.3, bsz=256, num_updates=23000, lr=0.000233126, gnorm=1.26, clip=0, loss_scale=4096, train_wall=190, wall=44611
2022-07-11 21:40:20 | INFO | train_inner | epoch 021:    688 / 1122 loss=5.567, nll_loss=2.359, mask_ins=0.812, word_ins_ml=4.011, word_reposition=0.744, ppl=47.4, wps=10752.2, ups=0.53, wpb=20406.2, bsz=256, num_updates=23100, lr=0.000232621, gnorm=1.255, clip=0, loss_scale=4096, train_wall=189, wall=44801
2022-07-11 21:43:30 | INFO | train_inner | epoch 021:    788 / 1122 loss=5.462, nll_loss=2.268, mask_ins=0.796, word_ins_ml=3.931, word_reposition=0.735, ppl=44.08, wps=10854.6, ups=0.53, wpb=20566, bsz=256, num_updates=23200, lr=0.000232119, gnorm=1.243, clip=0, loss_scale=7905, train_wall=189, wall=44990
2022-07-11 21:46:39 | INFO | train_inner | epoch 021:    888 / 1122 loss=5.494, nll_loss=2.287, mask_ins=0.806, word_ins_ml=3.947, word_reposition=0.741, ppl=45.07, wps=10787.6, ups=0.53, wpb=20397.1, bsz=256, num_updates=23300, lr=0.000231621, gnorm=1.273, clip=0, loss_scale=8192, train_wall=188, wall=45179
2022-07-11 21:46:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 21:49:51 | INFO | train_inner | epoch 021:    989 / 1122 loss=5.527, nll_loss=2.327, mask_ins=0.803, word_ins_ml=3.983, word_reposition=0.741, ppl=46.1, wps=10717.7, ups=0.52, wpb=20595.3, bsz=256, num_updates=23400, lr=0.000231125, gnorm=1.282, clip=0, loss_scale=4218, train_wall=191, wall=45371
2022-07-11 21:53:01 | INFO | train_inner | epoch 021:   1089 / 1122 loss=5.563, nll_loss=2.365, mask_ins=0.8, word_ins_ml=4.016, word_reposition=0.747, ppl=47.27, wps=10806.9, ups=0.53, wpb=20464.2, bsz=256, num_updates=23500, lr=0.000230633, gnorm=1.294, clip=0, loss_scale=4096, train_wall=189, wall=45561
2022-07-11 21:54:02 | INFO | train | epoch 021 | loss 5.525 | nll_loss 2.32 | mask_ins 0.805 | word_ins_ml 3.977 | word_reposition 0.743 | ppl 46.03 | wps 10579.9 | ups 0.52 | wpb 20522.2 | bsz 255.8 | num_updates 23533 | lr 0.000230471 | gnorm 1.272 | clip 0 | loss_scale 5480 | train_wall 2121 | wall 45623
2022-07-11 21:54:41 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.644 | nll_loss 5.55 | mask_ins 1.423 | word_ins_ml 7.007 | word_reposition 1.215 | ppl 800.26 | wps 25405.9 | wpb 2367.6 | bsz 32 | num_updates 23533 | best_loss 9.644
2022-07-11 21:54:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 21 @ 23533 updates, score 9.644) (writing took 6.357089954428375 seconds)
2022-07-11 21:56:55 | INFO | train_inner | epoch 022:     67 / 1122 loss=5.535, nll_loss=2.339, mask_ins=0.799, word_ins_ml=3.993, word_reposition=0.743, ppl=46.36, wps=8690.5, ups=0.43, wpb=20341.5, bsz=253.8, num_updates=23600, lr=0.000230144, gnorm=1.324, clip=0, loss_scale=4096, train_wall=188, wall=45795
2022-07-11 22:00:04 | INFO | train_inner | epoch 022:    167 / 1122 loss=5.487, nll_loss=2.29, mask_ins=0.798, word_ins_ml=3.95, word_reposition=0.74, ppl=44.85, wps=10842, ups=0.53, wpb=20552.1, bsz=256, num_updates=23700, lr=0.000229658, gnorm=1.246, clip=0, loss_scale=4096, train_wall=189, wall=45984
2022-07-11 22:03:14 | INFO | train_inner | epoch 022:    267 / 1122 loss=5.498, nll_loss=2.299, mask_ins=0.797, word_ins_ml=3.957, word_reposition=0.744, ppl=45.21, wps=10885.9, ups=0.53, wpb=20630.5, bsz=256, num_updates=23800, lr=0.000229175, gnorm=1.278, clip=0, loss_scale=4096, train_wall=189, wall=46174
2022-07-11 22:06:23 | INFO | train_inner | epoch 022:    367 / 1122 loss=5.458, nll_loss=2.278, mask_ins=0.789, word_ins_ml=3.938, word_reposition=0.73, ppl=43.96, wps=10870.3, ups=0.53, wpb=20632.7, bsz=256, num_updates=23900, lr=0.000228695, gnorm=1.291, clip=0, loss_scale=7619, train_wall=189, wall=46364
2022-07-11 22:07:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 22:09:35 | INFO | train_inner | epoch 022:    468 / 1122 loss=5.514, nll_loss=2.321, mask_ins=0.793, word_ins_ml=3.977, word_reposition=0.744, ppl=45.71, wps=10795.4, ups=0.52, wpb=20648.8, bsz=256, num_updates=24000, lr=0.000228218, gnorm=1.26, clip=0, loss_scale=6083, train_wall=190, wall=46555
2022-07-11 22:12:45 | INFO | train_inner | epoch 022:    568 / 1122 loss=5.511, nll_loss=2.315, mask_ins=0.8, word_ins_ml=3.972, word_reposition=0.739, ppl=45.61, wps=10779.7, ups=0.53, wpb=20500.7, bsz=256, num_updates=24100, lr=0.000227744, gnorm=1.269, clip=0, loss_scale=4096, train_wall=189, wall=46745
2022-07-11 22:15:55 | INFO | train_inner | epoch 022:    668 / 1122 loss=5.553, nll_loss=2.339, mask_ins=0.81, word_ins_ml=3.993, word_reposition=0.751, ppl=46.96, wps=10844.7, ups=0.53, wpb=20585.8, bsz=256, num_updates=24200, lr=0.000227273, gnorm=1.279, clip=0, loss_scale=4096, train_wall=189, wall=46935
2022-07-11 22:19:04 | INFO | train_inner | epoch 022:    768 / 1122 loss=5.495, nll_loss=2.303, mask_ins=0.795, word_ins_ml=3.961, word_reposition=0.738, ppl=45.08, wps=10846.1, ups=0.53, wpb=20529.9, bsz=256, num_updates=24300, lr=0.000226805, gnorm=1.323, clip=0, loss_scale=4096, train_wall=188, wall=47124
2022-07-11 22:22:13 | INFO | train_inner | epoch 022:    868 / 1122 loss=5.5, nll_loss=2.302, mask_ins=0.799, word_ins_ml=3.96, word_reposition=0.741, ppl=45.24, wps=10789.4, ups=0.53, wpb=20420.5, bsz=256, num_updates=24400, lr=0.000226339, gnorm=1.253, clip=0, loss_scale=4096, train_wall=188, wall=47314
2022-07-11 22:25:23 | INFO | train_inner | epoch 022:    968 / 1122 loss=5.479, nll_loss=2.292, mask_ins=0.792, word_ins_ml=3.951, word_reposition=0.736, ppl=44.6, wps=10868.2, ups=0.53, wpb=20608.1, bsz=256, num_updates=24500, lr=0.000225877, gnorm=1.275, clip=0, loss_scale=5734, train_wall=189, wall=47503
2022-07-11 22:28:34 | INFO | train_inner | epoch 022:   1068 / 1122 loss=5.52, nll_loss=2.332, mask_ins=0.793, word_ins_ml=3.986, word_reposition=0.741, ppl=45.88, wps=10702.8, ups=0.52, wpb=20420.6, bsz=256, num_updates=24600, lr=0.000225417, gnorm=1.279, clip=0, loss_scale=8192, train_wall=190, wall=47694
2022-07-11 22:30:16 | INFO | train | epoch 022 | loss 5.503 | nll_loss 2.309 | mask_ins 0.797 | word_ins_ml 3.966 | word_reposition 0.74 | ppl 45.36 | wps 10584.6 | ups 0.52 | wpb 20521.5 | bsz 255.8 | num_updates 24654 | lr 0.00022517 | gnorm 1.278 | clip 0 | loss_scale 5297 | train_wall 2119 | wall 47796
2022-07-11 22:30:55 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.641 | nll_loss 5.519 | mask_ins 1.431 | word_ins_ml 6.971 | word_reposition 1.239 | ppl 798.24 | wps 25441.8 | wpb 2367.6 | bsz 32 | num_updates 24654 | best_loss 9.641
2022-07-11 22:31:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 22 @ 24654 updates, score 9.641) (writing took 6.702088952995837 seconds)
2022-07-11 22:32:29 | INFO | train_inner | epoch 023:     46 / 1122 loss=5.511, nll_loss=2.315, mask_ins=0.802, word_ins_ml=3.971, word_reposition=0.739, ppl=45.61, wps=8613, ups=0.43, wpb=20247.3, bsz=253.8, num_updates=24700, lr=0.000224961, gnorm=1.295, clip=0, loss_scale=8192, train_wall=189, wall=47929
2022-07-11 22:35:38 | INFO | train_inner | epoch 023:    146 / 1122 loss=5.46, nll_loss=2.283, mask_ins=0.783, word_ins_ml=3.942, word_reposition=0.735, ppl=44.01, wps=10920, ups=0.53, wpb=20670.4, bsz=256, num_updates=24800, lr=0.000224507, gnorm=1.302, clip=0, loss_scale=8192, train_wall=188, wall=48118
2022-07-11 22:38:48 | INFO | train_inner | epoch 023:    246 / 1122 loss=5.455, nll_loss=2.261, mask_ins=0.797, word_ins_ml=3.924, word_reposition=0.735, ppl=43.88, wps=10802.8, ups=0.53, wpb=20547.6, bsz=256, num_updates=24900, lr=0.000224055, gnorm=1.247, clip=0, loss_scale=8192, train_wall=189, wall=48309
2022-07-11 22:40:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 22:42:00 | INFO | train_inner | epoch 023:    347 / 1122 loss=5.534, nll_loss=2.333, mask_ins=0.799, word_ins_ml=3.987, word_reposition=0.747, ppl=46.32, wps=10710.2, ups=0.52, wpb=20574.3, bsz=256, num_updates=25000, lr=0.000223607, gnorm=1.262, clip=0, loss_scale=6448, train_wall=191, wall=48501
2022-07-11 22:45:11 | INFO | train_inner | epoch 023:    447 / 1122 loss=5.456, nll_loss=2.275, mask_ins=0.792, word_ins_ml=3.935, word_reposition=0.729, ppl=43.89, wps=10736.8, ups=0.52, wpb=20472.5, bsz=256, num_updates=25100, lr=0.000223161, gnorm=1.251, clip=0, loss_scale=4096, train_wall=190, wall=48691
2022-07-11 22:48:21 | INFO | train_inner | epoch 023:    547 / 1122 loss=5.487, nll_loss=2.294, mask_ins=0.797, word_ins_ml=3.952, word_reposition=0.737, ppl=44.83, wps=10839.9, ups=0.53, wpb=20538.4, bsz=256, num_updates=25200, lr=0.000222718, gnorm=1.251, clip=0, loss_scale=4096, train_wall=189, wall=48881
2022-07-11 22:51:29 | INFO | train_inner | epoch 023:    647 / 1122 loss=5.478, nll_loss=2.284, mask_ins=0.797, word_ins_ml=3.943, word_reposition=0.737, ppl=44.56, wps=10896, ups=0.53, wpb=20571.3, bsz=256, num_updates=25300, lr=0.000222277, gnorm=1.261, clip=0, loss_scale=4096, train_wall=188, wall=49070
2022-07-11 22:54:39 | INFO | train_inner | epoch 023:    747 / 1122 loss=5.454, nll_loss=2.264, mask_ins=0.796, word_ins_ml=3.925, word_reposition=0.732, ppl=43.84, wps=10793.5, ups=0.53, wpb=20516.3, bsz=256, num_updates=25400, lr=0.000221839, gnorm=1.272, clip=0, loss_scale=4096, train_wall=189, wall=49260
2022-07-11 22:57:49 | INFO | train_inner | epoch 023:    847 / 1122 loss=5.476, nll_loss=2.299, mask_ins=0.786, word_ins_ml=3.957, word_reposition=0.733, ppl=44.52, wps=10829.6, ups=0.53, wpb=20523.9, bsz=256, num_updates=25500, lr=0.000221404, gnorm=1.31, clip=0, loss_scale=5366, train_wall=189, wall=49449
2022-07-11 23:00:59 | INFO | train_inner | epoch 023:    947 / 1122 loss=5.481, nll_loss=2.291, mask_ins=0.795, word_ins_ml=3.95, word_reposition=0.736, ppl=44.66, wps=10858.5, ups=0.53, wpb=20637.2, bsz=256, num_updates=25600, lr=0.000220971, gnorm=1.253, clip=0, loss_scale=8192, train_wall=189, wall=49639
2022-07-11 23:01:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 23:04:10 | INFO | train_inner | epoch 023:   1048 / 1122 loss=5.475, nll_loss=2.297, mask_ins=0.786, word_ins_ml=3.954, word_reposition=0.735, ppl=44.48, wps=10699.6, ups=0.52, wpb=20415.6, bsz=256, num_updates=25700, lr=0.000220541, gnorm=1.276, clip=0, loss_scale=5272, train_wall=190, wall=49830
2022-07-11 23:06:29 | INFO | train | epoch 023 | loss 5.477 | nll_loss 2.289 | mask_ins 0.793 | word_ins_ml 3.948 | word_reposition 0.736 | ppl 44.53 | wps 10575.6 | ups 0.52 | wpb 20521.4 | bsz 255.8 | num_updates 25774 | lr 0.000220224 | gnorm 1.274 | clip 0 | loss_scale 5790 | train_wall 2118 | wall 49969
2022-07-11 23:07:08 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.545 | nll_loss 5.422 | mask_ins 1.429 | word_ins_ml 6.881 | word_reposition 1.234 | ppl 746.88 | wps 25388.3 | wpb 2367.6 | bsz 32 | num_updates 25774 | best_loss 9.545
2022-07-11 23:07:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 23 @ 25774 updates, score 9.545) (writing took 5.892726732417941 seconds)
2022-07-11 23:08:03 | INFO | train_inner | epoch 024:     26 / 1122 loss=5.453, nll_loss=2.267, mask_ins=0.795, word_ins_ml=3.927, word_reposition=0.731, ppl=43.81, wps=8677.6, ups=0.43, wpb=20230.2, bsz=253.8, num_updates=25800, lr=0.000220113, gnorm=1.354, clip=0, loss_scale=4096, train_wall=187, wall=50063
2022-07-11 23:11:12 | INFO | train_inner | epoch 024:    126 / 1122 loss=5.472, nll_loss=2.278, mask_ins=0.796, word_ins_ml=3.938, word_reposition=0.738, ppl=44.38, wps=10828.9, ups=0.53, wpb=20514.1, bsz=256, num_updates=25900, lr=0.000219687, gnorm=1.282, clip=0, loss_scale=4096, train_wall=189, wall=50253
2022-07-11 23:14:22 | INFO | train_inner | epoch 024:    226 / 1122 loss=5.459, nll_loss=2.272, mask_ins=0.788, word_ins_ml=3.933, word_reposition=0.738, ppl=44, wps=10818.6, ups=0.53, wpb=20511.1, bsz=256, num_updates=26000, lr=0.000219265, gnorm=1.311, clip=0, loss_scale=4096, train_wall=189, wall=50442
2022-07-11 23:17:30 | INFO | train_inner | epoch 024:    326 / 1122 loss=5.423, nll_loss=2.254, mask_ins=0.78, word_ins_ml=3.917, word_reposition=0.726, ppl=42.9, wps=10872.6, ups=0.53, wpb=20467.7, bsz=256, num_updates=26100, lr=0.000218844, gnorm=1.288, clip=0, loss_scale=4096, train_wall=187, wall=50630
2022-07-11 23:20:41 | INFO | train_inner | epoch 024:    426 / 1122 loss=5.499, nll_loss=2.307, mask_ins=0.793, word_ins_ml=3.963, word_reposition=0.743, ppl=45.22, wps=10821.3, ups=0.53, wpb=20607.8, bsz=256, num_updates=26200, lr=0.000218426, gnorm=1.285, clip=0, loss_scale=6554, train_wall=190, wall=50821
2022-07-11 23:23:50 | INFO | train_inner | epoch 024:    526 / 1122 loss=5.4, nll_loss=2.213, mask_ins=0.789, word_ins_ml=3.88, word_reposition=0.731, ppl=42.22, wps=10859.2, ups=0.53, wpb=20575.5, bsz=256, num_updates=26300, lr=0.00021801, gnorm=1.253, clip=0, loss_scale=8192, train_wall=189, wall=51010
2022-07-11 23:26:59 | INFO | train_inner | epoch 024:    626 / 1122 loss=5.503, nll_loss=2.311, mask_ins=0.803, word_ins_ml=3.966, word_reposition=0.734, ppl=45.35, wps=10816.5, ups=0.53, wpb=20441.2, bsz=256, num_updates=26400, lr=0.000217597, gnorm=1.28, clip=0, loss_scale=8192, train_wall=188, wall=51199
2022-07-11 23:30:08 | INFO | train_inner | epoch 024:    726 / 1122 loss=5.5, nll_loss=2.326, mask_ins=0.785, word_ins_ml=3.98, word_reposition=0.735, ppl=45.26, wps=10911.7, ups=0.53, wpb=20587.1, bsz=256, num_updates=26500, lr=0.000217186, gnorm=1.306, clip=0, loss_scale=8192, train_wall=188, wall=51388
2022-07-11 23:33:17 | INFO | train_inner | epoch 024:    826 / 1122 loss=5.453, nll_loss=2.268, mask_ins=0.793, word_ins_ml=3.928, word_reposition=0.732, ppl=43.81, wps=10878.5, ups=0.53, wpb=20564.1, bsz=256, num_updates=26600, lr=0.000216777, gnorm=1.293, clip=0, loss_scale=8192, train_wall=188, wall=51577
2022-07-11 23:33:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 23:36:28 | INFO | train_inner | epoch 024:    927 / 1122 loss=5.48, nll_loss=2.283, mask_ins=0.796, word_ins_ml=3.942, word_reposition=0.742, ppl=44.62, wps=10773.7, ups=0.52, wpb=20613.5, bsz=256, num_updates=26700, lr=0.000216371, gnorm=1.264, clip=0, loss_scale=4299, train_wall=190, wall=51768
2022-07-11 23:39:38 | INFO | train_inner | epoch 024:   1027 / 1122 loss=5.487, nll_loss=2.299, mask_ins=0.796, word_ins_ml=3.956, word_reposition=0.735, ppl=44.86, wps=10774.7, ups=0.53, wpb=20500.1, bsz=256, num_updates=26800, lr=0.000215967, gnorm=1.265, clip=0, loss_scale=4096, train_wall=189, wall=51959
2022-07-11 23:42:38 | INFO | train | epoch 024 | loss 5.466 | nll_loss 2.278 | mask_ins 0.792 | word_ins_ml 3.938 | word_reposition 0.736 | ppl 44.19 | wps 10605.8 | ups 0.52 | wpb 20521.2 | bsz 255.8 | num_updates 26895 | lr 0.000215585 | gnorm 1.286 | clip 0 | loss_scale 5794 | train_wall 2115 | wall 52138
2022-07-11 23:43:17 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.578 | nll_loss 5.434 | mask_ins 1.418 | word_ins_ml 6.898 | word_reposition 1.263 | ppl 764.53 | wps 25388.8 | wpb 2367.6 | bsz 32 | num_updates 26895 | best_loss 9.545
2022-07-11 23:43:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 24 @ 26895 updates, score 9.578) (writing took 3.45451639033854 seconds)
2022-07-11 23:43:30 | INFO | train_inner | epoch 025:      5 / 1122 loss=5.452, nll_loss=2.259, mask_ins=0.792, word_ins_ml=3.921, word_reposition=0.739, ppl=43.78, wps=8826.5, ups=0.43, wpb=20445.4, bsz=253.8, num_updates=26900, lr=0.000215565, gnorm=1.315, clip=0, loss_scale=4096, train_wall=188, wall=52190
2022-07-11 23:46:40 | INFO | train_inner | epoch 025:    105 / 1122 loss=5.47, nll_loss=2.296, mask_ins=0.785, word_ins_ml=3.953, word_reposition=0.732, ppl=44.32, wps=10874.2, ups=0.53, wpb=20614.8, bsz=256, num_updates=27000, lr=0.000215166, gnorm=1.277, clip=0, loss_scale=4096, train_wall=189, wall=52380
2022-07-11 23:49:49 | INFO | train_inner | epoch 025:    205 / 1122 loss=5.454, nll_loss=2.267, mask_ins=0.789, word_ins_ml=3.927, word_reposition=0.738, ppl=43.84, wps=10808.7, ups=0.53, wpb=20495.2, bsz=256, num_updates=27100, lr=0.000214768, gnorm=1.262, clip=0, loss_scale=4096, train_wall=189, wall=52569
2022-07-11 23:52:59 | INFO | train_inner | epoch 025:    305 / 1122 loss=5.436, nll_loss=2.261, mask_ins=0.783, word_ins_ml=3.922, word_reposition=0.731, ppl=43.29, wps=10896, ups=0.53, wpb=20648.4, bsz=256, num_updates=27200, lr=0.000214373, gnorm=1.272, clip=0, loss_scale=7537, train_wall=189, wall=52759
2022-07-11 23:56:09 | INFO | train_inner | epoch 025:    405 / 1122 loss=5.438, nll_loss=2.255, mask_ins=0.786, word_ins_ml=3.917, word_reposition=0.735, ppl=43.34, wps=10807.7, ups=0.52, wpb=20586.5, bsz=256, num_updates=27300, lr=0.00021398, gnorm=1.272, clip=0, loss_scale=8192, train_wall=190, wall=52949
2022-07-11 23:58:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 23:59:21 | INFO | train_inner | epoch 025:    506 / 1122 loss=5.459, nll_loss=2.26, mask_ins=0.801, word_ins_ml=3.921, word_reposition=0.737, ppl=43.98, wps=10682.2, ups=0.52, wpb=20478.5, bsz=256, num_updates=27400, lr=0.000213589, gnorm=1.264, clip=0, loss_scale=7178, train_wall=191, wall=53141
2022-07-12 00:02:30 | INFO | train_inner | epoch 025:    606 / 1122 loss=5.394, nll_loss=2.21, mask_ins=0.787, word_ins_ml=3.877, word_reposition=0.73, ppl=42.05, wps=10811.4, ups=0.53, wpb=20452.5, bsz=256, num_updates=27500, lr=0.000213201, gnorm=1.243, clip=0, loss_scale=4096, train_wall=188, wall=53330
2022-07-12 00:05:39 | INFO | train_inner | epoch 025:    706 / 1122 loss=5.441, nll_loss=2.265, mask_ins=0.787, word_ins_ml=3.925, word_reposition=0.729, ppl=43.44, wps=10850.7, ups=0.53, wpb=20508.5, bsz=256, num_updates=27600, lr=0.000212814, gnorm=1.205, clip=0, loss_scale=4096, train_wall=188, wall=53519
2022-07-12 00:08:49 | INFO | train_inner | epoch 025:    806 / 1122 loss=5.431, nll_loss=2.248, mask_ins=0.786, word_ins_ml=3.911, word_reposition=0.735, ppl=43.15, wps=10888.7, ups=0.53, wpb=20723.9, bsz=256, num_updates=27700, lr=0.00021243, gnorm=1.281, clip=0, loss_scale=4096, train_wall=190, wall=53710
2022-07-12 00:10:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-12 00:12:00 | INFO | train_inner | epoch 025:    907 / 1122 loss=5.437, nll_loss=2.262, mask_ins=0.786, word_ins_ml=3.923, word_reposition=0.729, ppl=43.34, wps=10676.8, ups=0.52, wpb=20394.8, bsz=256, num_updates=27800, lr=0.000212047, gnorm=1.256, clip=0, loss_scale=3042, train_wall=190, wall=53901
2022-07-12 00:15:10 | INFO | train_inner | epoch 025:   1007 / 1122 loss=5.452, nll_loss=2.275, mask_ins=0.783, word_ins_ml=3.934, word_reposition=0.734, ppl=43.77, wps=10785.4, ups=0.53, wpb=20462.1, bsz=256, num_updates=27900, lr=0.000211667, gnorm=1.275, clip=0, loss_scale=2048, train_wall=189, wall=54090
2022-07-12 00:18:20 | INFO | train_inner | epoch 025:   1107 / 1122 loss=5.445, nll_loss=2.268, mask_ins=0.789, word_ins_ml=3.927, word_reposition=0.728, ppl=43.56, wps=10752.5, ups=0.53, wpb=20428.5, bsz=256, num_updates=28000, lr=0.000211289, gnorm=1.277, clip=0, loss_scale=2048, train_wall=189, wall=54280
2022-07-12 00:18:48 | INFO | train | epoch 025 | loss 5.441 | nll_loss 2.26 | mask_ins 0.787 | word_ins_ml 3.921 | word_reposition 0.733 | ppl 43.44 | wps 10590.3 | ups 0.52 | wpb 20521.1 | bsz 255.8 | num_updates 28015 | lr 0.000211232 | gnorm 1.266 | clip 0 | loss_scale 4558 | train_wall 2119 | wall 54309
2022-07-12 00:19:27 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.574 | nll_loss 5.472 | mask_ins 1.429 | word_ins_ml 6.933 | word_reposition 1.212 | ppl 762.34 | wps 25424.6 | wpb 2367.6 | bsz 32 | num_updates 28015 | best_loss 9.545
2022-07-12 00:19:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 25 @ 28015 updates, score 9.574) (writing took 3.3695839988067746 seconds)
2022-07-12 00:22:12 | INFO | train_inner | epoch 026:     85 / 1122 loss=5.424, nll_loss=2.248, mask_ins=0.78, word_ins_ml=3.911, word_reposition=0.733, ppl=42.92, wps=8843.8, ups=0.43, wpb=20466, bsz=253.8, num_updates=28100, lr=0.000210912, gnorm=1.315, clip=0, loss_scale=2048, train_wall=188, wall=54512
2022-07-12 00:25:20 | INFO | train_inner | epoch 026:    185 / 1122 loss=5.401, nll_loss=2.217, mask_ins=0.785, word_ins_ml=3.882, word_reposition=0.734, ppl=42.25, wps=10860.3, ups=0.53, wpb=20477.2, bsz=256, num_updates=28200, lr=0.000210538, gnorm=1.239, clip=0, loss_scale=2048, train_wall=188, wall=54700
2022-07-12 00:28:30 | INFO | train_inner | epoch 026:    285 / 1122 loss=5.365, nll_loss=2.194, mask_ins=0.779, word_ins_ml=3.863, word_reposition=0.723, ppl=41.21, wps=10787.2, ups=0.53, wpb=20434.3, bsz=256, num_updates=28300, lr=0.000210166, gnorm=1.252, clip=0, loss_scale=2867, train_wall=189, wall=54890
2022-07-12 00:31:40 | INFO | train_inner | epoch 026:    385 / 1122 loss=5.408, nll_loss=2.229, mask_ins=0.786, word_ins_ml=3.893, word_reposition=0.729, ppl=42.46, wps=10839.8, ups=0.53, wpb=20622.4, bsz=256, num_updates=28400, lr=0.000209795, gnorm=1.258, clip=0, loss_scale=4096, train_wall=189, wall=55080
2022-07-12 00:34:49 | INFO | train_inner | epoch 026:    485 / 1122 loss=5.458, nll_loss=2.278, mask_ins=0.789, word_ins_ml=3.938, word_reposition=0.731, ppl=43.96, wps=10857.1, ups=0.53, wpb=20545.6, bsz=256, num_updates=28500, lr=0.000209427, gnorm=1.261, clip=0, loss_scale=4096, train_wall=188, wall=55269
2022-07-12 00:37:58 | INFO | train_inner | epoch 026:    585 / 1122 loss=5.417, nll_loss=2.255, mask_ins=0.777, word_ins_ml=3.917, word_reposition=0.723, ppl=42.74, wps=10777, ups=0.53, wpb=20392.9, bsz=256, num_updates=28600, lr=0.000209061, gnorm=1.27, clip=0, loss_scale=4096, train_wall=188, wall=55459
2022-07-12 00:41:07 | INFO | train_inner | epoch 026:    685 / 1122 loss=5.389, nll_loss=2.216, mask_ins=0.778, word_ins_ml=3.881, word_reposition=0.73, ppl=41.92, wps=10881.1, ups=0.53, wpb=20583.7, bsz=256, num_updates=28700, lr=0.000208696, gnorm=1.238, clip=0, loss_scale=4096, train_wall=188, wall=55648
2022-07-12 00:44:16 | INFO | train_inner | epoch 026:    785 / 1122 loss=5.37, nll_loss=2.194, mask_ins=0.781, word_ins_ml=3.862, word_reposition=0.728, ppl=41.37, wps=10906.6, ups=0.53, wpb=20593.2, bsz=256, num_updates=28800, lr=0.000208333, gnorm=1.231, clip=0, loss_scale=5243, train_wall=188, wall=55837
2022-07-12 00:47:27 | INFO | train_inner | epoch 026:    885 / 1122 loss=5.42, nll_loss=2.244, mask_ins=0.783, word_ins_ml=3.906, word_reposition=0.731, ppl=42.81, wps=10821.7, ups=0.52, wpb=20619.8, bsz=256, num_updates=28900, lr=0.000207973, gnorm=1.248, clip=0, loss_scale=8192, train_wall=190, wall=56027
2022-07-12 00:48:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 00:50:37 | INFO | train_inner | epoch 026:    986 / 1122 loss=5.418, nll_loss=2.245, mask_ins=0.779, word_ins_ml=3.907, word_reposition=0.732, ppl=42.76, wps=10813, ups=0.53, wpb=20565.4, bsz=256, num_updates=29000, lr=0.000207614, gnorm=1.262, clip=0, loss_scale=6002, train_wall=189, wall=56217
2022-07-12 00:53:45 | INFO | train_inner | epoch 026:   1086 / 1122 loss=5.388, nll_loss=2.221, mask_ins=0.773, word_ins_ml=3.886, word_reposition=0.729, ppl=41.88, wps=10908.7, ups=0.53, wpb=20540.1, bsz=256, num_updates=29100, lr=0.000207257, gnorm=1.264, clip=0, loss_scale=4096, train_wall=187, wall=56406
2022-07-12 00:54:53 | INFO | train | epoch 026 | loss 5.409 | nll_loss 2.235 | mask_ins 0.781 | word_ins_ml 3.898 | word_reposition 0.729 | ppl 42.48 | wps 10629 | ups 0.52 | wpb 20520.5 | bsz 255.8 | num_updates 29136 | lr 0.000207129 | gnorm 1.258 | clip 0 | loss_scale 4288 | train_wall 2113 | wall 56473
2022-07-12 00:55:31 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.556 | nll_loss 5.47 | mask_ins 1.431 | word_ins_ml 6.93 | word_reposition 1.194 | ppl 752.5 | wps 25428.2 | wpb 2367.6 | bsz 32 | num_updates 29136 | best_loss 9.545
2022-07-12 00:55:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 26 @ 29136 updates, score 9.556) (writing took 3.1712883226573467 seconds)
2022-07-12 00:57:36 | INFO | train_inner | epoch 027:     64 / 1122 loss=5.441, nll_loss=2.259, mask_ins=0.789, word_ins_ml=3.92, word_reposition=0.732, ppl=43.44, wps=8828.3, ups=0.43, wpb=20368.4, bsz=253.8, num_updates=29200, lr=0.000206901, gnorm=1.315, clip=0, loss_scale=4096, train_wall=188, wall=56636
2022-07-12 01:00:46 | INFO | train_inner | epoch 027:    164 / 1122 loss=5.386, nll_loss=2.21, mask_ins=0.788, word_ins_ml=3.876, word_reposition=0.722, ppl=41.81, wps=10890.6, ups=0.53, wpb=20673.7, bsz=256, num_updates=29300, lr=0.000206548, gnorm=1.261, clip=0, loss_scale=4096, train_wall=189, wall=56826
2022-07-12 01:03:55 | INFO | train_inner | epoch 027:    264 / 1122 loss=5.416, nll_loss=2.239, mask_ins=0.782, word_ins_ml=3.901, word_reposition=0.733, ppl=42.7, wps=10872.1, ups=0.53, wpb=20611.8, bsz=256, num_updates=29400, lr=0.000206197, gnorm=1.26, clip=0, loss_scale=4096, train_wall=189, wall=57016
2022-07-12 01:07:05 | INFO | train_inner | epoch 027:    364 / 1122 loss=5.413, nll_loss=2.246, mask_ins=0.781, word_ins_ml=3.908, word_reposition=0.725, ppl=42.62, wps=10764, ups=0.53, wpb=20372.2, bsz=256, num_updates=29500, lr=0.000205847, gnorm=1.252, clip=0, loss_scale=5816, train_wall=188, wall=57205
2022-07-12 01:08:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 01:10:15 | INFO | train_inner | epoch 027:    465 / 1122 loss=5.408, nll_loss=2.226, mask_ins=0.788, word_ins_ml=3.89, word_reposition=0.731, ppl=42.47, wps=10714.9, ups=0.52, wpb=20423.3, bsz=256, num_updates=29600, lr=0.000205499, gnorm=1.234, clip=0, loss_scale=5556, train_wall=190, wall=57396
2022-07-12 01:13:24 | INFO | train_inner | epoch 027:    565 / 1122 loss=5.392, nll_loss=2.209, mask_ins=0.779, word_ins_ml=3.875, word_reposition=0.738, ppl=41.98, wps=10868.6, ups=0.53, wpb=20522.7, bsz=256, num_updates=29700, lr=0.000205152, gnorm=1.246, clip=0, loss_scale=4096, train_wall=188, wall=57584
2022-07-12 01:16:33 | INFO | train_inner | epoch 027:    665 / 1122 loss=5.433, nll_loss=2.252, mask_ins=0.782, word_ins_ml=3.913, word_reposition=0.737, ppl=43.19, wps=10889.5, ups=0.53, wpb=20565.7, bsz=256, num_updates=29800, lr=0.000204808, gnorm=1.292, clip=0, loss_scale=4096, train_wall=188, wall=57773
2022-07-12 01:19:44 | INFO | train_inner | epoch 027:    765 / 1122 loss=5.416, nll_loss=2.242, mask_ins=0.778, word_ins_ml=3.905, word_reposition=0.733, ppl=42.7, wps=10795.6, ups=0.52, wpb=20586.8, bsz=256, num_updates=29900, lr=0.000204465, gnorm=1.25, clip=0, loss_scale=4096, train_wall=190, wall=57964
2022-07-12 01:22:53 | INFO | train_inner | epoch 027:    865 / 1122 loss=5.362, nll_loss=2.2, mask_ins=0.775, word_ins_ml=3.866, word_reposition=0.721, ppl=41.12, wps=10850.1, ups=0.53, wpb=20534.2, bsz=256, num_updates=30000, lr=0.000204124, gnorm=1.282, clip=0, loss_scale=4096, train_wall=188, wall=58153
2022-07-12 01:25:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 01:26:04 | INFO | train_inner | epoch 027:    966 / 1122 loss=5.354, nll_loss=2.197, mask_ins=0.771, word_ins_ml=3.864, word_reposition=0.72, ppl=40.91, wps=10779.7, ups=0.52, wpb=20592, bsz=256, num_updates=30100, lr=0.000203785, gnorm=1.228, clip=0, loss_scale=5029, train_wall=190, wall=58344
2022-07-12 01:29:13 | INFO | train_inner | epoch 027:   1066 / 1122 loss=5.384, nll_loss=2.214, mask_ins=0.779, word_ins_ml=3.879, word_reposition=0.726, ppl=41.75, wps=10838.3, ups=0.53, wpb=20526.4, bsz=256, num_updates=30200, lr=0.000203447, gnorm=1.261, clip=0, loss_scale=4096, train_wall=189, wall=58534
2022-07-12 01:30:59 | INFO | train | epoch 027 | loss 5.396 | nll_loss 2.223 | mask_ins 0.78 | word_ins_ml 3.887 | word_reposition 0.729 | ppl 42.11 | wps 10610.5 | ups 0.52 | wpb 20522 | bsz 255.8 | num_updates 30256 | lr 0.000203259 | gnorm 1.264 | clip 0 | loss_scale 4465 | train_wall 2115 | wall 58639
2022-07-12 01:31:38 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.486 | nll_loss 5.439 | mask_ins 1.395 | word_ins_ml 6.896 | word_reposition 1.195 | ppl 716.99 | wps 25476.1 | wpb 2367.6 | bsz 32 | num_updates 30256 | best_loss 9.486
2022-07-12 01:31:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 27 @ 30256 updates, score 9.486) (writing took 5.871002439409494 seconds)
2022-07-12 01:33:06 | INFO | train_inner | epoch 028:     44 / 1122 loss=5.36, nll_loss=2.191, mask_ins=0.775, word_ins_ml=3.859, word_reposition=0.726, ppl=41.06, wps=8716.6, ups=0.43, wpb=20298, bsz=253.8, num_updates=30300, lr=0.000203111, gnorm=1.323, clip=0, loss_scale=4096, train_wall=187, wall=58766
2022-07-12 01:36:16 | INFO | train_inner | epoch 028:    144 / 1122 loss=5.431, nll_loss=2.253, mask_ins=0.786, word_ins_ml=3.914, word_reposition=0.732, ppl=43.14, wps=10853.6, ups=0.53, wpb=20563.8, bsz=256, num_updates=30400, lr=0.000202777, gnorm=1.239, clip=0, loss_scale=4096, train_wall=189, wall=58956
2022-07-12 01:39:25 | INFO | train_inner | epoch 028:    244 / 1122 loss=5.407, nll_loss=2.231, mask_ins=0.784, word_ins_ml=3.894, word_reposition=0.729, ppl=42.44, wps=10842.5, ups=0.53, wpb=20520.1, bsz=256, num_updates=30500, lr=0.000202444, gnorm=1.29, clip=0, loss_scale=4096, train_wall=188, wall=59145
2022-07-12 01:42:35 | INFO | train_inner | epoch 028:    344 / 1122 loss=5.383, nll_loss=2.207, mask_ins=0.782, word_ins_ml=3.873, word_reposition=0.727, ppl=41.72, wps=10785.2, ups=0.53, wpb=20477.9, bsz=256, num_updates=30600, lr=0.000202113, gnorm=1.263, clip=0, loss_scale=4874, train_wall=189, wall=59335
2022-07-12 01:45:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 01:45:47 | INFO | train_inner | epoch 028:    445 / 1122 loss=5.395, nll_loss=2.223, mask_ins=0.78, word_ins_ml=3.887, word_reposition=0.728, ppl=42.08, wps=10729.6, ups=0.52, wpb=20644.7, bsz=256, num_updates=30700, lr=0.000201784, gnorm=1.238, clip=0, loss_scale=8030, train_wall=191, wall=59528
2022-07-12 01:48:57 | INFO | train_inner | epoch 028:    545 / 1122 loss=5.34, nll_loss=2.183, mask_ins=0.773, word_ins_ml=3.851, word_reposition=0.716, ppl=40.5, wps=10835.2, ups=0.53, wpb=20535.3, bsz=256, num_updates=30800, lr=0.000201456, gnorm=1.248, clip=0, loss_scale=4096, train_wall=189, wall=59717
2022-07-12 01:52:07 | INFO | train_inner | epoch 028:    645 / 1122 loss=5.38, nll_loss=2.215, mask_ins=0.781, word_ins_ml=3.879, word_reposition=0.72, ppl=41.66, wps=10759.4, ups=0.53, wpb=20480.3, bsz=256, num_updates=30900, lr=0.000201129, gnorm=1.279, clip=0, loss_scale=4096, train_wall=189, wall=59907
2022-07-12 01:55:17 | INFO | train_inner | epoch 028:    745 / 1122 loss=5.393, nll_loss=2.235, mask_ins=0.77, word_ins_ml=3.898, word_reposition=0.725, ppl=42.01, wps=10771.7, ups=0.53, wpb=20440.1, bsz=256, num_updates=31000, lr=0.000200805, gnorm=1.239, clip=0, loss_scale=4096, train_wall=189, wall=60097
2022-07-12 01:58:26 | INFO | train_inner | epoch 028:    845 / 1122 loss=5.354, nll_loss=2.196, mask_ins=0.764, word_ins_ml=3.863, word_reposition=0.728, ppl=40.91, wps=10841.5, ups=0.53, wpb=20477.9, bsz=256, num_updates=31100, lr=0.000200482, gnorm=1.291, clip=0, loss_scale=4096, train_wall=188, wall=60286
2022-07-12 02:01:35 | INFO | train_inner | epoch 028:    945 / 1122 loss=5.378, nll_loss=2.217, mask_ins=0.773, word_ins_ml=3.882, word_reposition=0.723, ppl=41.6, wps=10834, ups=0.53, wpb=20527.8, bsz=256, num_updates=31200, lr=0.00020016, gnorm=1.27, clip=0, loss_scale=4096, train_wall=189, wall=60475
2022-07-12 02:04:44 | INFO | train_inner | epoch 028:   1045 / 1122 loss=5.41, nll_loss=2.247, mask_ins=0.776, word_ins_ml=3.908, word_reposition=0.725, ppl=42.51, wps=10892.7, ups=0.53, wpb=20572.4, bsz=256, num_updates=31300, lr=0.00019984, gnorm=1.235, clip=0, loss_scale=7864, train_wall=188, wall=60664
2022-07-12 02:07:10 | INFO | train | epoch 028 | loss 5.388 | nll_loss 2.22 | mask_ins 0.778 | word_ins_ml 3.884 | word_reposition 0.726 | ppl 41.86 | wps 10595.7 | ups 0.52 | wpb 20519.1 | bsz 255.8 | num_updates 31377 | lr 0.000199595 | gnorm 1.264 | clip 0 | loss_scale 5136 | train_wall 2117 | wall 60810
2022-07-12 02:07:49 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.6 | nll_loss 5.44 | mask_ins 1.426 | word_ins_ml 6.894 | word_reposition 1.28 | ppl 775.95 | wps 25397.3 | wpb 2367.6 | bsz 32 | num_updates 31377 | best_loss 9.486
2022-07-12 02:07:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 28 @ 31377 updates, score 9.6) (writing took 3.581385428085923 seconds)
2022-07-12 02:08:36 | INFO | train_inner | epoch 029:     23 / 1122 loss=5.418, nll_loss=2.24, mask_ins=0.783, word_ins_ml=3.902, word_reposition=0.732, ppl=42.75, wps=8837.6, ups=0.43, wpb=20458.6, bsz=253.8, num_updates=31400, lr=0.000199522, gnorm=1.333, clip=0, loss_scale=8192, train_wall=188, wall=60896
2022-07-12 02:11:46 | INFO | train_inner | epoch 029:    123 / 1122 loss=5.356, nll_loss=2.206, mask_ins=0.763, word_ins_ml=3.872, word_reposition=0.721, ppl=40.95, wps=10806.6, ups=0.52, wpb=20587.5, bsz=256, num_updates=31500, lr=0.000199205, gnorm=1.215, clip=0, loss_scale=8192, train_wall=190, wall=61086
2022-07-12 02:14:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 02:14:59 | INFO | train_inner | epoch 029:    224 / 1122 loss=5.349, nll_loss=2.173, mask_ins=0.779, word_ins_ml=3.842, word_reposition=0.728, ppl=40.76, wps=10654.7, ups=0.52, wpb=20551.2, bsz=256, num_updates=31600, lr=0.000198889, gnorm=1.252, clip=0, loss_scale=6894, train_wall=192, wall=61279
2022-07-12 02:18:09 | INFO | train_inner | epoch 029:    324 / 1122 loss=5.387, nll_loss=2.225, mask_ins=0.773, word_ins_ml=3.888, word_reposition=0.726, ppl=41.85, wps=10796.1, ups=0.53, wpb=20502.1, bsz=256, num_updates=31700, lr=0.000198575, gnorm=1.25, clip=0, loss_scale=4096, train_wall=189, wall=61469
2022-07-12 02:21:18 | INFO | train_inner | epoch 029:    424 / 1122 loss=5.398, nll_loss=2.231, mask_ins=0.783, word_ins_ml=3.894, word_reposition=0.722, ppl=42.17, wps=10834.3, ups=0.53, wpb=20515.2, bsz=256, num_updates=31800, lr=0.000198263, gnorm=1.267, clip=0, loss_scale=4096, train_wall=188, wall=61659
2022-07-12 02:24:27 | INFO | train_inner | epoch 029:    524 / 1122 loss=5.354, nll_loss=2.2, mask_ins=0.77, word_ins_ml=3.867, word_reposition=0.717, ppl=40.91, wps=10868, ups=0.53, wpb=20477.4, bsz=256, num_updates=31900, lr=0.000197952, gnorm=1.254, clip=0, loss_scale=4096, train_wall=188, wall=61847
2022-07-12 02:27:35 | INFO | train_inner | epoch 029:    624 / 1122 loss=5.372, nll_loss=2.203, mask_ins=0.778, word_ins_ml=3.869, word_reposition=0.725, ppl=41.4, wps=10915.9, ups=0.53, wpb=20602.9, bsz=256, num_updates=32000, lr=0.000197642, gnorm=1.254, clip=0, loss_scale=4096, train_wall=188, wall=62036
2022-07-12 02:30:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 02:30:46 | INFO | train_inner | epoch 029:    725 / 1122 loss=5.348, nll_loss=2.188, mask_ins=0.775, word_ins_ml=3.855, word_reposition=0.717, ppl=40.72, wps=10746.6, ups=0.52, wpb=20508.6, bsz=256, num_updates=32100, lr=0.000197334, gnorm=1.254, clip=0, loss_scale=4664, train_wall=190, wall=62227
2022-07-12 02:33:56 | INFO | train_inner | epoch 029:    825 / 1122 loss=5.373, nll_loss=2.215, mask_ins=0.769, word_ins_ml=3.88, word_reposition=0.725, ppl=41.44, wps=10814, ups=0.53, wpb=20498.8, bsz=256, num_updates=32200, lr=0.000197028, gnorm=1.294, clip=0, loss_scale=4096, train_wall=189, wall=62416
2022-07-12 02:37:05 | INFO | train_inner | epoch 029:    925 / 1122 loss=5.386, nll_loss=2.225, mask_ins=0.776, word_ins_ml=3.888, word_reposition=0.722, ppl=41.82, wps=10904.1, ups=0.53, wpb=20672.6, bsz=256, num_updates=32300, lr=0.000196722, gnorm=1.292, clip=0, loss_scale=4096, train_wall=189, wall=62606
2022-07-12 02:40:15 | INFO | train_inner | epoch 029:   1025 / 1122 loss=5.388, nll_loss=2.215, mask_ins=0.781, word_ins_ml=3.879, word_reposition=0.727, ppl=41.87, wps=10802.3, ups=0.53, wpb=20465.2, bsz=256, num_updates=32400, lr=0.000196419, gnorm=1.238, clip=0, loss_scale=4096, train_wall=189, wall=62795
2022-07-12 02:42:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-12 02:43:17 | INFO | train | epoch 029 | loss 5.37 | nll_loss 2.208 | mask_ins 0.774 | word_ins_ml 3.873 | word_reposition 0.723 | ppl 41.35 | wps 10593.6 | ups 0.52 | wpb 20521.8 | bsz 255.8 | num_updates 32496 | lr 0.000196128 | gnorm 1.262 | clip 0 | loss_scale 4802 | train_wall 2116 | wall 62978
2022-07-12 02:43:56 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.432 | nll_loss 5.315 | mask_ins 1.409 | word_ins_ml 6.786 | word_reposition 1.237 | ppl 690.84 | wps 25376.2 | wpb 2367.6 | bsz 32 | num_updates 32496 | best_loss 9.432
2022-07-12 02:44:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 29 @ 32496 updates, score 9.432) (writing took 5.9772264920175076 seconds)
2022-07-12 02:44:10 | INFO | train_inner | epoch 030:      4 / 1122 loss=5.347, nll_loss=2.194, mask_ins=0.766, word_ins_ml=3.86, word_reposition=0.721, ppl=40.69, wps=8656.2, ups=0.43, wpb=20353.6, bsz=253.8, num_updates=32500, lr=0.000196116, gnorm=1.298, clip=0, loss_scale=3508, train_wall=189, wall=63030
2022-07-12 02:47:20 | INFO | train_inner | epoch 030:    104 / 1122 loss=5.372, nll_loss=2.202, mask_ins=0.777, word_ins_ml=3.868, word_reposition=0.727, ppl=41.42, wps=10803.7, ups=0.53, wpb=20509.4, bsz=256, num_updates=32600, lr=0.000195815, gnorm=1.245, clip=0, loss_scale=2048, train_wall=189, wall=63220
2022-07-12 02:50:29 | INFO | train_inner | epoch 030:    204 / 1122 loss=5.402, nll_loss=2.234, mask_ins=0.773, word_ins_ml=3.896, word_reposition=0.733, ppl=42.28, wps=10842.9, ups=0.53, wpb=20499.5, bsz=256, num_updates=32700, lr=0.000195515, gnorm=1.238, clip=0, loss_scale=2048, train_wall=188, wall=63409
2022-07-12 02:53:37 | INFO | train_inner | epoch 030:    304 / 1122 loss=5.368, nll_loss=2.208, mask_ins=0.774, word_ins_ml=3.873, word_reposition=0.72, ppl=41.29, wps=10881.6, ups=0.53, wpb=20519.1, bsz=256, num_updates=32800, lr=0.000195217, gnorm=1.292, clip=0, loss_scale=2048, train_wall=188, wall=63598
2022-07-12 02:56:47 | INFO | train_inner | epoch 030:    404 / 1122 loss=5.35, nll_loss=2.192, mask_ins=0.768, word_ins_ml=3.859, word_reposition=0.722, ppl=40.79, wps=10786, ups=0.53, wpb=20424.8, bsz=256, num_updates=32900, lr=0.00019492, gnorm=1.295, clip=0, loss_scale=2048, train_wall=189, wall=63787
2022-07-12 02:59:56 | INFO | train_inner | epoch 030:    504 / 1122 loss=5.354, nll_loss=2.194, mask_ins=0.772, word_ins_ml=3.861, word_reposition=0.722, ppl=40.91, wps=10906.5, ups=0.53, wpb=20681.9, bsz=256, num_updates=33000, lr=0.000194625, gnorm=1.251, clip=0, loss_scale=2396, train_wall=189, wall=63977
2022-07-12 03:01:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-12 03:03:08 | INFO | train_inner | epoch 030:    605 / 1122 loss=5.361, nll_loss=2.218, mask_ins=0.764, word_ins_ml=3.882, word_reposition=0.714, ppl=41.1, wps=10731.3, ups=0.52, wpb=20499.9, bsz=256, num_updates=33100, lr=0.000194331, gnorm=1.249, clip=0, loss_scale=2859, train_wall=190, wall=64168
2022-07-12 03:06:16 | INFO | train_inner | epoch 030:    705 / 1122 loss=5.367, nll_loss=2.201, mask_ins=0.779, word_ins_ml=3.867, word_reposition=0.721, ppl=41.27, wps=10810.1, ups=0.53, wpb=20403.3, bsz=256, num_updates=33200, lr=0.000194038, gnorm=1.294, clip=0, loss_scale=2048, train_wall=188, wall=64356
2022-07-12 03:09:25 | INFO | train_inner | epoch 030:    805 / 1122 loss=5.371, nll_loss=2.215, mask_ins=0.774, word_ins_ml=3.879, word_reposition=0.719, ppl=41.39, wps=10887.5, ups=0.53, wpb=20557.9, bsz=256, num_updates=33300, lr=0.000193746, gnorm=1.306, clip=0, loss_scale=2048, train_wall=188, wall=64545
2022-07-12 03:12:34 | INFO | train_inner | epoch 030:    905 / 1122 loss=5.327, nll_loss=2.168, mask_ins=0.771, word_ins_ml=3.837, word_reposition=0.719, ppl=40.15, wps=10877.5, ups=0.53, wpb=20561.5, bsz=256, num_updates=33400, lr=0.000193456, gnorm=1.285, clip=0, loss_scale=2048, train_wall=188, wall=64734
2022-07-12 03:15:44 | INFO | train_inner | epoch 030:   1005 / 1122 loss=5.347, nll_loss=2.193, mask_ins=0.766, word_ins_ml=3.86, word_reposition=0.722, ppl=40.7, wps=10901.3, ups=0.53, wpb=20658.3, bsz=256, num_updates=33500, lr=0.000193167, gnorm=1.233, clip=0, loss_scale=2048, train_wall=189, wall=64924
2022-07-12 03:18:53 | INFO | train_inner | epoch 030:   1105 / 1122 loss=5.324, nll_loss=2.176, mask_ins=0.765, word_ins_ml=3.845, word_reposition=0.714, ppl=40.04, wps=10845.7, ups=0.53, wpb=20552.3, bsz=256, num_updates=33600, lr=0.000192879, gnorm=1.241, clip=0, loss_scale=3052, train_wall=189, wall=65113
2022-07-12 03:19:25 | INFO | train | epoch 030 | loss 5.356 | nll_loss 2.198 | mask_ins 0.771 | word_ins_ml 3.864 | word_reposition 0.721 | ppl 40.95 | wps 10612.8 | ups 0.52 | wpb 20521 | bsz 255.8 | num_updates 33617 | lr 0.00019283 | gnorm 1.27 | clip 0 | loss_scale 2273 | train_wall 2113 | wall 65145
2022-07-12 03:20:04 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.542 | nll_loss 5.412 | mask_ins 1.416 | word_ins_ml 6.873 | word_reposition 1.253 | ppl 745.27 | wps 25322.5 | wpb 2367.6 | bsz 32 | num_updates 33617 | best_loss 9.432
2022-07-12 03:20:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 30 @ 33617 updates, score 9.542) (writing took 3.416326677426696 seconds)
2022-07-12 03:22:46 | INFO | train_inner | epoch 031:     83 / 1122 loss=5.333, nll_loss=2.171, mask_ins=0.765, word_ins_ml=3.84, word_reposition=0.728, ppl=40.3, wps=8810.5, ups=0.43, wpb=20501.6, bsz=253.8, num_updates=33700, lr=0.000192593, gnorm=1.31, clip=0, loss_scale=4096, train_wall=189, wall=65346
2022-07-12 03:25:55 | INFO | train_inner | epoch 031:    183 / 1122 loss=5.326, nll_loss=2.17, mask_ins=0.765, word_ins_ml=3.839, word_reposition=0.722, ppl=40.11, wps=10892.7, ups=0.53, wpb=20652.3, bsz=256, num_updates=33800, lr=0.000192308, gnorm=1.264, clip=0, loss_scale=4096, train_wall=189, wall=65536
2022-07-12 03:29:05 | INFO | train_inner | epoch 031:    283 / 1122 loss=5.364, nll_loss=2.2, mask_ins=0.773, word_ins_ml=3.865, word_reposition=0.726, ppl=41.19, wps=10798.4, ups=0.53, wpb=20422.3, bsz=256, num_updates=33900, lr=0.000192024, gnorm=1.276, clip=0, loss_scale=4096, train_wall=188, wall=65725
2022-07-12 03:32:15 | INFO | train_inner | epoch 031:    383 / 1122 loss=5.379, nll_loss=2.22, mask_ins=0.773, word_ins_ml=3.884, word_reposition=0.723, ppl=41.61, wps=10805.2, ups=0.53, wpb=20564.7, bsz=256, num_updates=34000, lr=0.000191741, gnorm=1.258, clip=0, loss_scale=4096, train_wall=190, wall=65915
2022-07-12 03:34:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 03:35:26 | INFO | train_inner | epoch 031:    484 / 1122 loss=5.348, nll_loss=2.184, mask_ins=0.769, word_ins_ml=3.851, word_reposition=0.728, ppl=40.72, wps=10685.2, ups=0.52, wpb=20456.4, bsz=256, num_updates=34100, lr=0.00019146, gnorm=1.274, clip=0, loss_scale=4704, train_wall=191, wall=66107
2022-07-12 03:38:36 | INFO | train_inner | epoch 031:    584 / 1122 loss=5.325, nll_loss=2.169, mask_ins=0.766, word_ins_ml=3.838, word_reposition=0.722, ppl=40.09, wps=10874.4, ups=0.53, wpb=20579.5, bsz=256, num_updates=34200, lr=0.00019118, gnorm=1.243, clip=0, loss_scale=4096, train_wall=188, wall=66296
2022-07-12 03:41:46 | INFO | train_inner | epoch 031:    684 / 1122 loss=5.316, nll_loss=2.17, mask_ins=0.76, word_ins_ml=3.839, word_reposition=0.717, ppl=39.82, wps=10818.2, ups=0.53, wpb=20592.7, bsz=256, num_updates=34300, lr=0.000190901, gnorm=1.276, clip=0, loss_scale=4096, train_wall=190, wall=66486
2022-07-12 03:44:57 | INFO | train_inner | epoch 031:    784 / 1122 loss=5.359, nll_loss=2.198, mask_ins=0.765, word_ins_ml=3.864, word_reposition=0.73, ppl=41.04, wps=10739.5, ups=0.52, wpb=20542.1, bsz=256, num_updates=34400, lr=0.000190623, gnorm=1.266, clip=0, loss_scale=4096, train_wall=190, wall=66677
2022-07-12 03:48:06 | INFO | train_inner | epoch 031:    884 / 1122 loss=5.318, nll_loss=2.175, mask_ins=0.757, word_ins_ml=3.844, word_reposition=0.717, ppl=39.9, wps=10851.7, ups=0.53, wpb=20495, bsz=256, num_updates=34500, lr=0.000190347, gnorm=1.237, clip=0, loss_scale=4096, train_wall=188, wall=66866
2022-07-12 03:51:15 | INFO | train_inner | epoch 031:    984 / 1122 loss=5.342, nll_loss=2.192, mask_ins=0.766, word_ins_ml=3.858, word_reposition=0.719, ppl=40.56, wps=10814, ups=0.53, wpb=20432.9, bsz=256, num_updates=34600, lr=0.000190071, gnorm=1.311, clip=0, loss_scale=4547, train_wall=188, wall=67055
2022-07-12 03:54:25 | INFO | train_inner | epoch 031:   1084 / 1122 loss=5.33, nll_loss=2.186, mask_ins=0.764, word_ins_ml=3.853, word_reposition=0.713, ppl=40.24, wps=10799.7, ups=0.53, wpb=20510.8, bsz=256, num_updates=34700, lr=0.000189797, gnorm=1.288, clip=0, loss_scale=8192, train_wall=189, wall=67245
2022-07-12 03:55:36 | INFO | train | epoch 031 | loss 5.343 | nll_loss 2.187 | mask_ins 0.766 | word_ins_ml 3.854 | word_reposition 0.722 | ppl 40.58 | wps 10596.5 | ups 0.52 | wpb 20521.8 | bsz 255.8 | num_updates 34738 | lr 0.000189694 | gnorm 1.275 | clip 0 | loss_scale 4695 | train_wall 2119 | wall 67316
2022-07-12 03:56:15 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.385 | nll_loss 5.323 | mask_ins 1.399 | word_ins_ml 6.785 | word_reposition 1.2 | ppl 668.74 | wps 25306.1 | wpb 2367.6 | bsz 32 | num_updates 34738 | best_loss 9.385
2022-07-12 03:56:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 31 @ 34738 updates, score 9.385) (writing took 6.11579913739115 seconds)
2022-07-12 03:56:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 03:58:20 | INFO | train_inner | epoch 032:     63 / 1122 loss=5.349, nll_loss=2.208, mask_ins=0.765, word_ins_ml=3.873, word_reposition=0.712, ppl=40.76, wps=8650.1, ups=0.43, wpb=20351.7, bsz=253.8, num_updates=34800, lr=0.000189525, gnorm=1.333, clip=0, loss_scale=6367, train_wall=189, wall=67480
2022-07-12 04:01:29 | INFO | train_inner | epoch 032:    163 / 1122 loss=5.339, nll_loss=2.188, mask_ins=0.768, word_ins_ml=3.855, word_reposition=0.716, ppl=40.47, wps=10824.9, ups=0.53, wpb=20449.3, bsz=256, num_updates=34900, lr=0.000189253, gnorm=1.268, clip=0, loss_scale=4096, train_wall=188, wall=67669
2022-07-12 04:04:39 | INFO | train_inner | epoch 032:    263 / 1122 loss=5.311, nll_loss=2.165, mask_ins=0.76, word_ins_ml=3.834, word_reposition=0.717, ppl=39.71, wps=10821.3, ups=0.53, wpb=20518, bsz=256, num_updates=35000, lr=0.000188982, gnorm=1.245, clip=0, loss_scale=4096, train_wall=189, wall=67859
2022-07-12 04:07:49 | INFO | train_inner | epoch 032:    363 / 1122 loss=5.329, nll_loss=2.172, mask_ins=0.765, word_ins_ml=3.84, word_reposition=0.724, ppl=40.18, wps=10900, ups=0.53, wpb=20691.4, bsz=256, num_updates=35100, lr=0.000188713, gnorm=1.246, clip=0, loss_scale=4096, train_wall=189, wall=68049
2022-07-12 04:10:58 | INFO | train_inner | epoch 032:    463 / 1122 loss=5.347, nll_loss=2.178, mask_ins=0.772, word_ins_ml=3.845, word_reposition=0.731, ppl=40.71, wps=10951.3, ups=0.53, wpb=20731.7, bsz=256, num_updates=35200, lr=0.000188445, gnorm=1.285, clip=0, loss_scale=4096, train_wall=189, wall=68238
2022-07-12 04:14:07 | INFO | train_inner | epoch 032:    563 / 1122 loss=5.331, nll_loss=2.18, mask_ins=0.764, word_ins_ml=3.847, word_reposition=0.719, ppl=40.25, wps=10876.1, ups=0.53, wpb=20533.4, bsz=256, num_updates=35300, lr=0.000188177, gnorm=1.248, clip=0, loss_scale=5448, train_wall=188, wall=68427
2022-07-12 04:17:15 | INFO | train_inner | epoch 032:    663 / 1122 loss=5.36, nll_loss=2.21, mask_ins=0.77, word_ins_ml=3.874, word_reposition=0.716, ppl=41.07, wps=10866.9, ups=0.53, wpb=20462, bsz=256, num_updates=35400, lr=0.000187912, gnorm=1.291, clip=0, loss_scale=8192, train_wall=187, wall=68615
2022-07-12 04:20:24 | INFO | train_inner | epoch 032:    763 / 1122 loss=5.304, nll_loss=2.157, mask_ins=0.766, word_ins_ml=3.827, word_reposition=0.711, ppl=39.52, wps=10748.7, ups=0.53, wpb=20357.7, bsz=256, num_updates=35500, lr=0.000187647, gnorm=1.221, clip=0, loss_scale=8192, train_wall=189, wall=68805
2022-07-12 04:21:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 04:23:35 | INFO | train_inner | epoch 032:    864 / 1122 loss=5.297, nll_loss=2.156, mask_ins=0.759, word_ins_ml=3.826, word_reposition=0.712, ppl=39.31, wps=10698.9, ups=0.52, wpb=20405.6, bsz=256, num_updates=35600, lr=0.000187383, gnorm=1.301, clip=0, loss_scale=5556, train_wall=190, wall=68995
2022-07-12 04:26:44 | INFO | train_inner | epoch 032:    964 / 1122 loss=5.312, nll_loss=2.166, mask_ins=0.763, word_ins_ml=3.835, word_reposition=0.714, ppl=39.74, wps=10920, ups=0.53, wpb=20599.8, bsz=256, num_updates=35700, lr=0.00018712, gnorm=1.259, clip=0, loss_scale=4096, train_wall=188, wall=69184
2022-07-12 04:29:53 | INFO | train_inner | epoch 032:   1064 / 1122 loss=5.344, nll_loss=2.189, mask_ins=0.769, word_ins_ml=3.855, word_reposition=0.72, ppl=40.62, wps=10886.1, ups=0.53, wpb=20569.2, bsz=256, num_updates=35800, lr=0.000186859, gnorm=1.257, clip=0, loss_scale=4096, train_wall=188, wall=69373
2022-07-12 04:31:42 | INFO | train | epoch 032 | loss 5.328 | nll_loss 2.178 | mask_ins 0.765 | word_ins_ml 3.846 | word_reposition 0.718 | ppl 40.18 | wps 10608.6 | ups 0.52 | wpb 20518.9 | bsz 255.8 | num_updates 35858 | lr 0.000186708 | gnorm 1.265 | clip 0 | loss_scale 5144 | train_wall 2112 | wall 69482
2022-07-12 04:32:21 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.455 | nll_loss 5.329 | mask_ins 1.421 | word_ins_ml 6.794 | word_reposition 1.24 | ppl 701.63 | wps 25377.9 | wpb 2367.6 | bsz 32 | num_updates 35858 | best_loss 9.385
2022-07-12 04:32:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 32 @ 35858 updates, score 9.455) (writing took 3.057621207088232 seconds)
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
train.sh: line 42: early-exit: command not found
