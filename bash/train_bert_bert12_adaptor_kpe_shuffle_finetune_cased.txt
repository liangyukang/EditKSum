nohup: ignoring input
2022-08-15 14:51:14 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:13068
2022-08-15 14:51:14 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:13068
2022-08-15 14:51:14 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:13068
2022-08-15 14:51:14 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:13068
2022-08-15 14:51:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-08-15 14:51:15 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-08-15 14:51:15 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-08-15 14:51:15 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-08-15 14:51:15 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-15 14:51:15 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-08-15 14:51:15 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-15 14:51:15 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-08-15 14:51:15 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-15 14:51:15 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-08-15 14:51:15 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-15 14:51:15 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-08-15 14:51:19 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:13068', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='keywords', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-08-15 14:51:19 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-08-15 14:51:19 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
start load cached examples valid ...
0it [00:00, ?it/s]2022-08-15 14:51:19 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.source
2022-08-15 14:51:19 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.target
2022-08-15 14:51:19 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
start load cached examples valid ...
0it [00:00, ?it/s]0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]379it [00:00, 3784.15it/s]364it [00:00, 3636.66it/s]362it [00:00, 3612.32it/s]382it [00:00, 3818.42it/s]758it [00:00, 3411.84it/s]724it [00:00, 3370.14it/s]728it [00:00, 3382.54it/s]764it [00:00, 3432.43it/s]1080it [00:00, 3449.71it/s]1086it [00:00, 3466.20it/s]1102it [00:00, 3403.46it/s]1124it [00:00, 3501.90it/s]1444it [00:00, 3277.27it/s]1426it [00:00, 3244.36it/s]1434it [00:00, 3261.62it/s]1476it [00:00, 3351.85it/s]1825it [00:00, 3459.05it/s]1808it [00:00, 3439.01it/s]1816it [00:00, 3449.34it/s]1860it [00:00, 3517.93it/s]2208it [00:00, 3579.86it/s]2182it [00:00, 3537.13it/s]2198it [00:00, 3568.58it/s]2221it [00:00, 3417.26it/s]2538it [00:00, 3455.34it/s]2557it [00:00, 3476.68it/s]2568it [00:00, 3428.18it/s]2613it [00:00, 3571.58it/s]2904it [00:00, 3515.95it/s]2957it [00:00, 3567.44it/s]2926it [00:00, 3539.35it/s]3002it [00:00, 3668.80it/s]3257it [00:00, 3426.54it/s]3282it [00:00, 3443.64it/s]3316it [00:00, 3465.06it/s]3371it [00:00, 3537.68it/s]3635it [00:01, 3531.66it/s]3660it [00:01, 3541.66it/s]3694it [00:01, 3557.48it/s]3753it [00:01, 3618.21it/s]3990it [00:01, 3426.33it/s]4016it [00:01, 3438.62it/s]4052it [00:01, 3405.30it/s]4117it [00:01, 3496.51it/s]4346it [00:01, 3463.93it/s]4375it [00:01, 3482.78it/s]4423it [00:01, 3491.69it/s]4490it [00:01, 3563.24it/s]4712it [00:01, 3519.83it/s]4744it [00:01, 3543.49it/s]4775it [00:01, 3387.26it/s]4848it [00:01, 3444.70it/s]5065it [00:01, 3409.94it/s]5100it [00:01, 3424.96it/s]5131it [00:01, 3434.68it/s]5222it [00:01, 3529.16it/s]5428it [00:01, 3473.77it/s]5463it [00:01, 3483.49it/s]5494it [00:01, 3490.62it/s]5582it [00:01, 3375.54it/s]5777it [00:01, 3281.57it/s]5813it [00:01, 3282.40it/s]5845it [00:01, 3319.99it/s]5922it [00:01, 3357.17it/s]6122it [00:01, 3327.43it/s]6157it [00:01, 3325.41it/s]6188it [00:01, 3349.58it/s]6262it [00:01, 3368.03it/s]6457it [00:01, 3197.78it/s]6492it [00:01, 3201.81it/s]6525it [00:02, 2002.88it/s]6600it [00:02, 2011.17it/s]6815it [00:02, 2035.31it/s]6779it [00:02, 2013.64it/s]6870it [00:02, 2288.91it/s]6951it [00:02, 2307.74it/s]7161it [00:02, 2325.68it/s]7124it [00:02, 2303.67it/s]7203it [00:02, 2516.95it/s]7301it [00:02, 2569.57it/s]7445it [00:02, 2420.20it/s]7407it [00:02, 2399.70it/s]7506it [00:02, 2605.21it/s]7611it [00:02, 2626.27it/s]7792it [00:02, 2673.84it/s]7750it [00:02, 2647.03it/s]7856it [00:02, 2830.42it/s]7964it [00:02, 2850.73it/s]8140it [00:02, 2880.65it/s]8094it [00:02, 2848.21it/s]8280it [00:02, 2878.41it/s]8186it [00:02, 2759.25it/s]8455it [00:02, 2894.17it/s]8406it [00:02, 2858.94it/s]8636it [00:02, 3060.80it/s]8539it [00:02, 2958.37it/s]8782it [00:02, 2994.99it/s]8730it [00:02, 2961.85it/s]8975it [00:02, 3150.46it/s]8892it [00:02, 3112.14it/s]9096it [00:02, 2928.24it/s]9041it [00:02, 2903.15it/s]9303it [00:02, 3091.34it/s]9218it [00:02, 3067.13it/s]9445it [00:03, 3083.30it/s]9385it [00:03, 3051.35it/s]9660it [00:03, 3225.87it/s]9552it [00:03, 3142.18it/s]9795it [00:03, 3200.73it/s]9733it [00:03, 3171.73it/s]9990it [00:03, 3152.16it/s]9874it [00:03, 3080.60it/s]10122it [00:03, 3076.57it/s]10057it [00:03, 3063.23it/s]10325it [00:03, 3208.49it/s]10225it [00:03, 3201.89it/s]10467it [00:03, 3181.90it/s]10403it [00:03, 3175.49it/s]10678it [00:03, 3301.18it/s]10576it [00:03, 3289.02it/s]10725it [00:03, 3094.99it/s]10790it [00:03, 3096.08it/s]11012it [00:03, 3190.37it/s]10909it [00:03, 3149.35it/s]11071it [00:03, 3196.93it/s]11138it [00:03, 3203.94it/s]11362it [00:03, 3279.10it/s]11257it [00:03, 3241.10it/s]11401it [00:03, 3226.11it/s]11470it [00:03, 3234.99it/s]11693it [00:03, 3133.40it/s]11584it [00:03, 3148.46it/s]11726it [00:03, 3133.60it/s]11796it [00:03, 3136.99it/s]12043it [00:03, 3236.43it/s]11934it [00:03, 3247.60it/s]12071it [00:03, 3224.21it/s]12144it [00:03, 3233.10it/s]12271it [00:03, 3281.80it/s]12372it [00:03, 3156.86it/s]12396it [00:03, 3135.93it/s]12470it [00:03, 3095.18it/s]12713it [00:04, 3224.89it/s]12601it [00:04, 3179.72it/s]12730it [00:04, 3194.65it/s]12818it [00:04, 3203.58it/s]13062it [00:04, 3300.57it/s]12950it [00:04, 3266.77it/s]13075it [00:04, 3268.32it/s]13161it [00:04, 3267.28it/s]13368it [00:04, 3166.82it/s]
2022-08-15 14:51:23 | INFO | root | success load 13368 data
2022-08-15 14:51:23 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-08-15 14:51:23 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-08-15 14:51:23 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-08-15 14:51:23 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-08-15 14:51:23 | INFO | transformer.tokenization_utils | loading file None
2022-08-15 14:51:23 | INFO | transformer.tokenization_utils | loading file None
2022-08-15 14:51:23 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
13279it [00:04, 3147.55it/s]13368it [00:04, 3132.61it/s]
13368it [00:04, 3131.06it/s]
13368it [00:04, 3117.83it/s]
2022-08-15 14:51:24 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-15 14:51:24 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-15 14:51:24 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-08-15 14:51:27 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-08-15 14:51:27 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-08-15 14:51:27 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-15 14:51:27 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-15 14:51:27 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
2022-08-15 14:51:29 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-08-15 14:51:29 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-08-15 14:51:29 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-08-15 14:51:29 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-08-15 14:51:29 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 142456835)
2022-08-15 14:51:29 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-08-15 14:51:29 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-08-15 14:51:30 | INFO | fairseq_cli.train | training on 4 GPUs
2022-08-15 14:51:30 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-08-15 14:51:31 | INFO | fairseq.trainer | loaded checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 27 @ 30253 updates)
2022-08-15 14:51:31 | INFO | fairseq.trainer | loading train data for epoch 27
2022-08-15 14:51:31 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.source
2022-08-15 14:51:31 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.target
2022-08-15 14:51:31 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]303it [00:00, 3023.08it/s]632it [00:00, 3177.96it/s]950it [00:00, 3155.93it/s]1266it [00:00, 2963.28it/s]1578it [00:00, 3016.11it/s]1881it [00:00, 2824.30it/s]2175it [00:00, 2858.00it/s]2485it [00:00, 2930.18it/s]2780it [00:00, 2846.33it/s]3101it [00:01, 2952.13it/s]3417it [00:01, 3011.20it/s]3720it [00:01, 2906.30it/s]4027it [00:01, 2952.56it/s]4350it [00:01, 2887.74it/s]4663it [00:01, 2956.16it/s]4982it [00:01, 3023.15it/s]5286it [00:01, 2910.47it/s]5638it [00:01, 3084.96it/s]6007it [00:01, 3259.96it/s]6335it [00:02, 3115.32it/s]6703it [00:02, 3274.54it/s]7033it [00:03, 971.98it/s] 7386it [00:03, 1250.98it/s]7746it [00:03, 1567.42it/s]8047it [00:03, 1766.06it/s]8413it [00:03, 2114.96it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]8727it [00:03, 2313.46it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]360it [00:00, 3578.97it/s]9081it [00:03, 2593.14it/s]354it [00:00, 3536.03it/s]724it [00:00, 3611.64it/s]9452it [00:03, 2866.65it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]717it [00:00, 3587.87it/s]1086it [00:00, 3418.10it/s]9790it [00:03, 2922.90it/s]357it [00:00, 3563.44it/s]1076it [00:00, 3371.71it/s]1445it [00:00, 3480.22it/s]10147it [00:04, 3093.95it/s]723it [00:00, 3619.50it/s]1424it [00:00, 3411.65it/s]1824it [00:00, 3587.64it/s]10484it [00:04, 3112.08it/s]1085it [00:00, 3252.21it/s]1799it [00:00, 3529.53it/s]10832it [00:04, 3212.27it/s]2184it [00:00, 3421.83it/s]1452it [00:00, 3404.92it/s]2153it [00:00, 3385.86it/s]2553it [00:00, 3504.44it/s]11168it [00:04, 3189.98it/s]1810it [00:00, 3464.43it/s]2511it [00:00, 3446.19it/s]11520it [00:04, 3283.56it/s]2905it [00:00, 3367.25it/s]2159it [00:00, 3351.53it/s]2857it [00:00, 3333.25it/s]11892it [00:04, 3408.09it/s]3275it [00:00, 3464.92it/s]2520it [00:00, 3429.67it/s]3228it [00:00, 3445.44it/s]12239it [00:04, 3330.71it/s]3624it [00:01, 3343.49it/s]2865it [00:00, 3303.90it/s]3574it [00:01, 3296.86it/s]12594it [00:04, 3393.90it/s]3983it [00:01, 3412.47it/s]3230it [00:00, 3397.05it/s]3915it [00:01, 3329.28it/s]12937it [00:04, 3326.96it/s]4328it [00:01, 3422.86it/s]3572it [00:01, 3314.49it/s]4274it [00:01, 3403.39it/s]13312it [00:04, 3448.51it/s]4672it [00:01, 3320.00it/s]3927it [00:01, 3382.17it/s]4616it [00:01, 3278.42it/s]13659it [00:05, 3371.95it/s]5023it [00:01, 3373.27it/s]4284it [00:01, 3436.86it/s]4969it [00:01, 3350.70it/s]14031it [00:05, 3471.53it/s]5362it [00:01, 3317.37it/s]4629it [00:01, 3327.08it/s]5306it [00:01, 3264.01it/s]14407it [00:05, 3554.28it/s]5720it [00:01, 3393.61it/s]4982it [00:01, 3384.81it/s]5659it [00:01, 3339.53it/s]14764it [00:05, 3447.69it/s]6061it [00:01, 3305.20it/s]5322it [00:01, 3280.98it/s]6019it [00:01, 3414.56it/s]15138it [00:05, 3531.49it/s]6423it [00:01, 3395.91it/s]5689it [00:01, 3390.88it/s]6362it [00:01, 3296.20it/s]6792it [00:01, 3480.16it/s]15493it [00:05, 3380.71it/s]6030it [00:01, 3291.48it/s]6722it [00:01, 3381.61it/s]15834it [00:05, 3327.15it/s]6390it [00:01, 3378.47it/s]16179it [00:05, 3240.46it/s]6751it [00:01, 3443.46it/s]7141it [00:02, 1101.98it/s]7510it [00:02, 1406.04it/s]7062it [00:02, 994.27it/s] 7822it [00:03, 1651.97it/s]7429it [00:03, 1284.71it/s]8194it [00:03, 2003.66it/s]7097it [00:02, 1034.11it/s]7779it [00:03, 1553.22it/s]8556it [00:03, 2319.89it/s]7457it [00:02, 1320.57it/s]8147it [00:03, 1890.63it/s]8888it [00:03, 2519.47it/s]7779it [00:03, 1573.58it/s]16505it [00:06, 832.23it/s] 8496it [00:03, 2185.64it/s]9244it [00:03, 2763.91it/s]8150it [00:03, 1922.89it/s]16885it [00:07, 1110.41it/s]8821it [00:03, 2381.47it/s]8508it [00:03, 2236.16it/s]9581it [00:03, 2858.66it/s]17191it [00:07, 1340.62it/s]9184it [00:03, 2665.67it/s]8837it [00:03, 2432.26it/s]9953it [00:03, 3081.73it/s]17560it [00:07, 1679.60it/s]9517it [00:03, 2783.01it/s]9197it [00:03, 2695.56it/s]10299it [00:03, 3079.87it/s]17929it [00:07, 1961.52it/s]9885it [00:03, 3012.00it/s]9531it [00:03, 2809.92it/s]10676it [00:03, 3266.85it/s]18298it [00:07, 2292.21it/s]10235it [00:03, 3140.63it/s]9906it [00:03, 3051.80it/s]11048it [00:03, 3392.94it/s]18680it [00:07, 2619.11it/s]10577it [00:03, 3139.20it/s]10264it [00:03, 3193.55it/s]11401it [00:04, 3350.13it/s]19025it [00:07, 2739.75it/s]10942it [00:04, 3279.98it/s]10611it [00:03, 3192.03it/s]11770it [00:04, 3445.23it/s]19387it [00:07, 2956.16it/s]11285it [00:04, 3237.74it/s]10973it [00:04, 3311.09it/s]12122it [00:04, 3380.64it/s]19730it [00:07, 2996.41it/s]11655it [00:04, 3366.59it/s]11319it [00:04, 3275.95it/s]12491it [00:04, 3468.41it/s]20091it [00:08, 3157.81it/s]12000it [00:04, 3273.14it/s]11673it [00:04, 3348.84it/s]12842it [00:04, 3402.95it/s]20449it [00:08, 3155.75it/s]12379it [00:04, 3419.38it/s]12016it [00:04, 3285.79it/s]13218it [00:04, 3504.03it/s]20824it [00:08, 3317.76it/s]12752it [00:04, 3507.24it/s]12382it [00:04, 3391.75it/s]13601it [00:04, 3598.19it/s]21181it [00:08, 3388.36it/s]13107it [00:04, 3425.39it/s]12760it [00:04, 3502.52it/s]13963it [00:04, 3514.83it/s]21530it [00:08, 3320.59it/s]13485it [00:04, 3527.08it/s]13114it [00:04, 3440.93it/s]14333it [00:04, 3568.62it/s]21891it [00:08, 3401.57it/s]13841it [00:04, 3436.49it/s]13480it [00:04, 3502.51it/s]14692it [00:04, 3482.03it/s]22237it [00:08, 3326.33it/s]14220it [00:04, 3529.61it/s]13833it [00:04, 3435.19it/s]15080it [00:05, 3597.24it/s]22619it [00:08, 3417.91it/s]14575it [00:05, 3425.64it/s]14195it [00:04, 3488.02it/s]15442it [00:05, 3442.18it/s]22969it [00:08, 3351.17it/s]14943it [00:05, 3497.61it/s]14546it [00:05, 3413.25it/s]15821it [00:05, 3539.44it/s]23335it [00:08, 3438.37it/s]15323it [00:05, 3583.54it/s]14930it [00:05, 3535.86it/s]16179it [00:05, 3415.81it/s]23689it [00:09, 3467.43it/s]15683it [00:05, 3449.12it/s]15294it [00:05, 3565.07it/s]24038it [00:09, 3374.92it/s]16048it [00:05, 3504.36it/s]15652it [00:05, 3442.43it/s]24393it [00:09, 3423.79it/s]16008it [00:05, 3474.65it/s]24737it [00:09, 3326.54it/s]25108it [00:09, 3436.64it/s]25461it [00:09, 3463.26it/s]25809it [00:09, 3374.08it/s]26156it [00:09, 3399.53it/s]26497it [00:09, 3308.46it/s]26869it [00:10, 3425.57it/s]16523it [00:06, 973.38it/s] 27213it [00:10, 3345.88it/s]16907it [00:06, 1270.50it/s]16400it [00:06, 1000.08it/s]27591it [00:10, 3470.68it/s]17232it [00:06, 1525.79it/s]16773it [00:06, 1289.08it/s]27968it [00:10, 3556.86it/s]17619it [00:06, 1889.78it/s]17092it [00:06, 1538.58it/s]17946it [00:06, 2137.57it/s]16357it [00:06, 858.46it/s] 17469it [00:06, 1890.99it/s]18330it [00:06, 2487.89it/s]16735it [00:06, 1131.01it/s]17850it [00:06, 2243.87it/s]18710it [00:07, 2784.70it/s]17089it [00:06, 1401.45it/s]18191it [00:06, 2437.60it/s]19065it [00:07, 2909.14it/s]17470it [00:06, 1745.19it/s]18571it [00:07, 2744.84it/s]19447it [00:07, 3139.53it/s]17832it [00:07, 2062.33it/s]18917it [00:07, 2848.70it/s]19804it [00:07, 3166.18it/s]18169it [00:07, 2296.50it/s]19291it [00:07, 3073.14it/s]20182it [00:07, 3332.12it/s]18555it [00:07, 2634.51it/s]19639it [00:07, 3101.86it/s]20538it [00:07, 3297.53it/s]18904it [00:07, 2745.48it/s]20005it [00:07, 3252.43it/s]20915it [00:07, 3429.23it/s]19282it [00:07, 2997.94it/s]20379it [00:07, 3380.96it/s]21289it [00:07, 3369.68it/s]19630it [00:07, 3056.85it/s]20734it [00:07, 3305.59it/s]21671it [00:07, 3494.33it/s]20009it [00:07, 3251.18it/s]21106it [00:07, 3421.34it/s]22038it [00:07, 3542.96it/s]20361it [00:07, 3301.80it/s]21457it [00:07, 3318.35it/s]22398it [00:08, 3465.59it/s]20711it [00:07, 3272.90it/s]21834it [00:08, 3445.89it/s]28325it [00:11, 704.46it/s] 22781it [00:08, 3570.08it/s]21083it [00:07, 3397.38it/s]22184it [00:08, 3356.65it/s]28694it [00:11, 933.89it/s]23142it [00:08, 3470.78it/s]21433it [00:08, 3273.17it/s]22568it [00:08, 3492.68it/s]28995it [00:11, 1139.25it/s]23503it [00:08, 3509.85it/s]21811it [00:08, 3415.27it/s]22946it [00:08, 3575.49it/s]29302it [00:12, 1378.21it/s]23856it [00:08, 3418.89it/s]22159it [00:08, 3346.33it/s]23307it [00:08, 3414.66it/s]29668it [00:12, 1720.66it/s]24232it [00:08, 3516.96it/s]22546it [00:08, 3493.69it/s]23672it [00:08, 3480.65it/s]29986it [00:12, 1968.74it/s]24607it [00:08, 3584.15it/s]22905it [00:08, 3518.96it/s]24023it [00:08, 3383.81it/s]30357it [00:12, 2318.05it/s]24967it [00:08, 3436.16it/s]23260it [00:08, 3416.74it/s]24395it [00:08, 3477.96it/s]30689it [00:12, 2502.62it/s]25341it [00:08, 3522.89it/s]23631it [00:08, 3501.09it/s]24745it [00:08, 3348.80it/s]31057it [00:12, 2782.11it/s]25696it [00:09, 3426.04it/s]23984it [00:08, 3370.76it/s]25113it [00:08, 3441.47it/s]31426it [00:12, 3011.60it/s]26054it [00:09, 3469.33it/s]24363it [00:08, 3488.14it/s]25483it [00:09, 3514.95it/s]31774it [00:12, 3035.22it/s]26403it [00:09, 3381.62it/s]24714it [00:09, 3390.05it/s]25837it [00:09, 3405.15it/s]32139it [00:12, 3198.48it/s]26775it [00:09, 3478.22it/s]25075it [00:09, 3452.59it/s]26191it [00:09, 3442.09it/s]32484it [00:13, 3144.85it/s]27157it [00:09, 3577.51it/s]25448it [00:09, 3532.14it/s]26537it [00:09, 3349.87it/s]32845it [00:13, 3270.96it/s]27516it [00:09, 3453.44it/s]25803it [00:09, 3394.08it/s]26898it [00:09, 3424.40it/s]33186it [00:13, 3217.48it/s]27892it [00:09, 3540.89it/s]26173it [00:09, 3480.04it/s]27242it [00:09, 3339.77it/s]33541it [00:13, 3310.79it/s]26523it [00:09, 3388.58it/s]27621it [00:09, 3467.52it/s]33916it [00:13, 3436.59it/s]26885it [00:09, 3452.38it/s]27985it [00:09, 3515.72it/s]34266it [00:13, 3338.34it/s]27232it [00:09, 3377.05it/s]34605it [00:13, 3310.12it/s]27599it [00:09, 3459.87it/s]34939it [00:13, 3198.97it/s]27981it [00:09, 3564.27it/s]35309it [00:13, 3340.32it/s]35646it [00:13, 3236.18it/s]36004it [00:14, 3331.45it/s]36376it [00:14, 3441.15it/s]36722it [00:14, 3284.29it/s]28248it [00:10, 943.12it/s] 37080it [00:14, 3366.07it/s]28617it [00:10, 1216.92it/s]37419it [00:14, 3274.27it/s]28919it [00:10, 1434.57it/s]37767it [00:14, 3330.88it/s]29295it [00:10, 1784.47it/s]28338it [00:10, 866.21it/s] 38140it [00:14, 3445.12it/s]29665it [00:11, 2122.72it/s]28708it [00:11, 1131.58it/s]29998it [00:11, 2342.84it/s]38486it [00:14, 3264.60it/s]29009it [00:11, 1353.23it/s]30372it [00:11, 2652.48it/s]38856it [00:14, 3386.20it/s]29368it [00:11, 1675.75it/s]30714it [00:11, 2786.86it/s]39198it [00:15, 3292.80it/s]29739it [00:11, 2022.63it/s]28339it [00:11, 791.11it/s] 31076it [00:11, 2996.38it/s]39555it [00:15, 3371.68it/s]30070it [00:11, 2243.19it/s]28691it [00:11, 1023.69it/s]31439it [00:11, 3053.31it/s]39895it [00:15, 3294.94it/s]30440it [00:11, 2557.85it/s]28997it [00:11, 1244.57it/s]31817it [00:11, 3245.40it/s]40266it [00:15, 3411.89it/s]30778it [00:11, 2691.76it/s]29370it [00:11, 1578.09it/s]32195it [00:11, 3392.75it/s]40614it [00:15, 3431.05it/s]31139it [00:11, 2917.96it/s]29744it [00:11, 1925.34it/s]40959it [00:15, 3350.32it/s]32552it [00:11, 3290.45it/s]31478it [00:11, 2973.60it/s]30078it [00:11, 2144.65it/s]41312it [00:15, 3399.78it/s]32916it [00:12, 3387.13it/s]31850it [00:11, 3173.06it/s]30450it [00:11, 2470.11it/s]41653it [00:15, 3316.56it/s]33265it [00:12, 3323.43it/s]32228it [00:12, 3338.50it/s]30788it [00:11, 2630.74it/s]42021it [00:15, 3421.43it/s]33636it [00:12, 3432.75it/s]32582it [00:12, 3243.79it/s]31141it [00:12, 2847.80it/s]33985it [00:12, 3378.18it/s]42365it [00:15, 3293.79it/s]32944it [00:12, 3348.37it/s]31479it [00:12, 2927.00it/s]34352it [00:12, 3461.30it/s]42738it [00:16, 3418.51it/s]33290it [00:12, 3278.61it/s]31854it [00:12, 3144.78it/s]34723it [00:12, 3530.81it/s]43101it [00:16, 3477.69it/s]33660it [00:12, 3397.18it/s]32230it [00:12, 3312.81it/s]35079it [00:12, 3412.80it/s]34006it [00:12, 3295.07it/s]32584it [00:12, 3228.99it/s]35449it [00:12, 3474.60it/s]34380it [00:12, 3416.78it/s]32948it [00:12, 3340.76it/s]35799it [00:12, 3378.98it/s]34749it [00:12, 3494.55it/s]33294it [00:12, 3282.32it/s]36175it [00:12, 3486.21it/s]35102it [00:12, 3380.26it/s]33662it [00:12, 3360.24it/s]36526it [00:13, 3379.90it/s]35475it [00:13, 3480.06it/s]34005it [00:12, 3315.18it/s]36887it [00:13, 3444.99it/s]35826it [00:13, 3343.41it/s]34379it [00:13, 3435.30it/s]37255it [00:13, 3512.77it/s]36197it [00:13, 3446.60it/s]34749it [00:13, 3510.10it/s]37608it [00:13, 3397.15it/s]36544it [00:13, 3335.99it/s]35103it [00:13, 3348.02it/s]37978it [00:13, 3483.92it/s]36911it [00:13, 3430.31it/s]35473it [00:13, 3447.90it/s]38328it [00:13, 3368.40it/s]37265it [00:13, 3461.41it/s]35821it [00:13, 3363.96it/s]38688it [00:13, 3434.44it/s]37613it [00:13, 3341.72it/s]36191it [00:13, 3458.69it/s]39033it [00:13, 3350.69it/s]37984it [00:13, 3447.14it/s]36539it [00:13, 3303.09it/s]39414it [00:13, 3480.26it/s]38331it [00:13, 3346.43it/s]36907it [00:13, 3408.54it/s]39769it [00:14, 3499.20it/s]38691it [00:14, 3417.18it/s]37279it [00:13, 3496.99it/s]40120it [00:14, 3400.11it/s]39035it [00:14, 3274.93it/s]37631it [00:13, 3330.01it/s]40494it [00:14, 3496.99it/s]39414it [00:14, 3419.35it/s]38003it [00:14, 3439.45it/s]40845it [00:14, 3399.79it/s]39783it [00:14, 3495.26it/s]38350it [00:14, 3354.96it/s]41207it [00:14, 3461.85it/s]40135it [00:14, 3383.38it/s]38713it [00:14, 3430.89it/s]41555it [00:14, 3373.28it/s]43451it [00:18, 526.51it/s] 40499it [00:14, 3455.95it/s]39058it [00:14, 3300.97it/s]41930it [00:14, 3479.30it/s]43825it [00:18, 717.69it/s]40847it [00:14, 3351.57it/s]39433it [00:14, 3428.51it/s]42307it [00:14, 3561.83it/s]44138it [00:18, 906.64it/s]41222it [00:14, 3463.99it/s]39804it [00:14, 3509.27it/s]42665it [00:14, 3423.00it/s]44515it [00:18, 1194.74it/s]41570it [00:14, 3354.12it/s]40157it [00:14, 3344.70it/s]43032it [00:14, 3493.89it/s]44889it [00:18, 1514.30it/s]41927it [00:14, 3415.52it/s]40531it [00:14, 3455.18it/s]45225it [00:18, 1776.39it/s]42300it [00:15, 3506.56it/s]40879it [00:14, 3367.85it/s]45595it [00:18, 2116.75it/s]42652it [00:15, 3371.80it/s]41252it [00:15, 3471.05it/s]45937it [00:18, 2332.28it/s]43019it [00:15, 3455.07it/s]46304it [00:19, 2625.82it/s]41601it [00:15, 3330.16it/s]41977it [00:15, 3451.99it/s]46647it [00:19, 2748.57it/s]42354it [00:15, 3541.08it/s]47019it [00:19, 2989.57it/s]47396it [00:19, 3192.98it/s]42710it [00:15, 3412.42it/s]43077it [00:15, 3483.46it/s]47750it [00:19, 3168.40it/s]48103it [00:19, 3266.19it/s]48448it [00:19, 3240.46it/s]48818it [00:19, 3368.67it/s]49165it [00:19, 3305.05it/s]49530it [00:19, 3402.05it/s]49907it [00:20, 3506.96it/s]43383it [00:16, 660.79it/s] 50262it [00:20, 3339.19it/s]43755it [00:16, 883.44it/s]50628it [00:20, 3428.99it/s]44109it [00:16, 1123.01it/s]50975it [00:20, 3346.01it/s]44473it [00:16, 1418.91it/s]51313it [00:20, 3272.88it/s]44852it [00:16, 1759.27it/s]51669it [00:20, 3232.67it/s]45188it [00:17, 2014.10it/s]52041it [00:20, 3369.32it/s]43367it [00:17, 594.76it/s] 45562it [00:17, 2349.14it/s]52414it [00:20, 3471.38it/s]43736it [00:17, 800.68it/s]45906it [00:17, 2538.42it/s]52763it [00:20, 3366.30it/s]44109it [00:17, 1039.58it/s]46277it [00:17, 2810.79it/s]53134it [00:21, 3464.49it/s]44484it [00:17, 1334.71it/s]46630it [00:17, 2874.66it/s]53483it [00:21, 3326.96it/s]44860it [00:17, 1660.64it/s]47003it [00:17, 3091.79it/s]43428it [00:17, 589.53it/s] 53850it [00:21, 3423.75it/s]45196it [00:17, 1898.26it/s]47381it [00:17, 3274.69it/s]43788it [00:17, 786.42it/s]54195it [00:21, 3324.59it/s]45567it [00:17, 2231.55it/s]47736it [00:17, 3212.11it/s]44109it [00:17, 992.02it/s]54554it [00:21, 3398.38it/s]45906it [00:17, 2425.96it/s]48103it [00:17, 3335.67it/s]44485it [00:17, 1292.52it/s]54909it [00:21, 3440.22it/s]46273it [00:17, 2706.82it/s]44861it [00:17, 1623.77it/s]48452it [00:17, 3297.31it/s]55255it [00:21, 3325.79it/s]46630it [00:17, 2809.61it/s]48831it [00:18, 3435.69it/s]45197it [00:17, 1866.64it/s]55619it [00:21, 3416.02it/s]46983it [00:18, 2990.26it/s]45565it [00:17, 2199.57it/s]49183it [00:18, 3326.57it/s]55963it [00:21, 3289.62it/s]47360it [00:18, 3193.91it/s]45903it [00:18, 2402.60it/s]49560it [00:18, 3450.01it/s]56330it [00:21, 3395.55it/s]47710it [00:18, 3172.55it/s]46271it [00:18, 2690.31it/s]49933it [00:18, 3527.95it/s]56701it [00:22, 3485.72it/s]48079it [00:18, 3313.52it/s]50290it [00:18, 3423.13it/s]46630it [00:18, 2806.41it/s]57052it [00:22, 3352.71it/s]48427it [00:18, 3262.39it/s]50659it [00:18, 3498.69it/s]47001it [00:18, 3031.86it/s]57421it [00:22, 3447.10it/s]48800it [00:18, 3388.68it/s]47359it [00:18, 3174.60it/s]51012it [00:18, 3376.03it/s]57768it [00:22, 3350.23it/s]49150it [00:18, 3271.81it/s]47707it [00:18, 3162.72it/s]51383it [00:18, 3469.15it/s]58122it [00:22, 3403.69it/s]49522it [00:18, 3397.59it/s]48078it [00:18, 3311.09it/s]51733it [00:18, 3377.12it/s]58464it [00:22, 3303.17it/s]49894it [00:18, 3488.80it/s]48426it [00:18, 3267.75it/s]52104it [00:19, 3470.09it/s]58832it [00:22, 3409.66it/s]50248it [00:19, 3378.57it/s]48801it [00:18, 3401.40it/s]52462it [00:19, 3499.58it/s]59183it [00:22, 3436.80it/s]50616it [00:19, 3462.32it/s]49150it [00:19, 3331.89it/s]52814it [00:19, 3397.46it/s]59528it [00:22, 3326.35it/s]50966it [00:19, 3365.98it/s]53186it [00:19, 3489.28it/s]49490it [00:19, 3286.59it/s]59896it [00:23, 3427.51it/s]51320it [00:19, 3412.50it/s]49862it [00:19, 3408.80it/s]53537it [00:19, 3387.51it/s]60241it [00:23, 3289.92it/s]51670it [00:19, 3327.04it/s]53910it [00:19, 3476.32it/s]50207it [00:19, 3318.86it/s]60610it [00:23, 3402.13it/s]52040it [00:19, 3432.04it/s]50576it [00:19, 3423.38it/s]54259it [00:19, 3369.97it/s]60953it [00:23, 3308.22it/s]52410it [00:19, 3508.80it/s]54629it [00:19, 3462.90it/s]50921it [00:19, 3330.98it/s]61311it [00:23, 3384.12it/s]52763it [00:19, 3384.50it/s]54999it [00:19, 3529.58it/s]51296it [00:19, 3449.65it/s]61675it [00:23, 3456.05it/s]53117it [00:19, 3426.90it/s]51667it [00:19, 3524.93it/s]55354it [00:19, 3406.33it/s]53462it [00:19, 3325.30it/s]55701it [00:20, 3423.96it/s]52022it [00:19, 3400.98it/s]53828it [00:20, 3420.32it/s]52389it [00:19, 3477.07it/s]56045it [00:20, 3349.32it/s]54190it [00:20, 3336.86it/s]56418it [00:20, 3458.18it/s]52739it [00:20, 3326.23it/s]54562it [00:20, 3444.71it/s]56765it [00:20, 3374.19it/s]53109it [00:20, 3430.60it/s]54931it [00:20, 3515.38it/s]57139it [00:20, 3479.25it/s]53455it [00:20, 3334.23it/s]55284it [00:20, 3330.42it/s]57506it [00:20, 3505.11it/s]53824it [00:20, 3434.70it/s]55644it [00:20, 3406.45it/s]57858it [00:20, 3403.93it/s]54190it [00:20, 3356.50it/s]55987it [00:20, 3302.27it/s]58227it [00:20, 3485.76it/s]54549it [00:20, 3420.93it/s]56360it [00:20, 3420.02it/s]58577it [00:20, 3380.24it/s]54920it [00:20, 3501.64it/s]56710it [00:20, 3327.38it/s]58944it [00:21, 3462.77it/s]55272it [00:20, 3374.25it/s]57068it [00:21, 3397.58it/s]59292it [00:21, 3328.98it/s]55635it [00:20, 3445.05it/s]57440it [00:21, 3489.56it/s]59653it [00:21, 3408.55it/s]55982it [00:21, 3339.34it/s]57791it [00:21, 3375.49it/s]60022it [00:21, 3489.48it/s]56348it [00:21, 3429.89it/s]58160it [00:21, 3464.36it/s]60373it [00:21, 3386.99it/s]56710it [00:21, 3335.57it/s]58508it [00:21, 3336.12it/s]60729it [00:21, 3435.37it/s]57084it [00:21, 3450.29it/s]58872it [00:21, 3420.34it/s]61074it [00:21, 3356.31it/s]57457it [00:21, 3529.50it/s]59220it [00:21, 3437.08it/s]61444it [00:21, 3455.11it/s]57812it [00:21, 3375.62it/s]59565it [00:21, 3319.95it/s]61791it [00:21, 3355.54it/s]58180it [00:21, 3453.03it/s]59930it [00:21, 3413.35it/s]58528it [00:21, 3353.25it/s]60273it [00:21, 3312.13it/s]58891it [00:21, 3431.34it/s]60637it [00:22, 3404.41it/s]59236it [00:21, 3334.24it/s]60979it [00:22, 3267.68it/s]59601it [00:22, 3423.85it/s]62022it [00:25, 438.74it/s] 61347it [00:22, 3384.73it/s]59954it [00:22, 3452.02it/s]62378it [00:26, 595.73it/s]61708it [00:22, 3447.51it/s]60301it [00:22, 3342.57it/s]62685it [00:26, 761.71it/s]60672it [00:22, 3446.70it/s]63053it [00:26, 1016.99it/s]61019it [00:22, 3350.11it/s]63407it [00:26, 1297.10it/s]61392it [00:22, 3458.18it/s]63732it [00:26, 1550.90it/s]61750it [00:22, 3342.44it/s]64104it [00:26, 1902.46it/s]64439it [00:26, 2122.54it/s]64813it [00:26, 2458.08it/s]65178it [00:26, 2627.23it/s]65518it [00:27, 2811.83it/s]65886it [00:27, 3032.69it/s]66230it [00:27, 3044.42it/s]66583it [00:27, 3174.16it/s]66922it [00:27, 3152.56it/s]67292it [00:27, 3304.57it/s]62128it [00:24, 490.37it/s] 67642it [00:27, 3358.75it/s]62504it [00:24, 675.63it/s]67987it [00:27, 3291.73it/s]62817it [00:24, 859.34it/s]68356it [00:27, 3403.37it/s]63190it [00:24, 1136.61it/s]68702it [00:27, 3263.62it/s]63505it [00:24, 1378.01it/s]69071it [00:28, 3382.89it/s]63875it [00:24, 1719.99it/s]69413it [00:28, 3296.21it/s]64246it [00:24, 2065.61it/s]69765it [00:28, 3357.83it/s]62055it [00:24, 477.10it/s] 64589it [00:24, 2297.40it/s]70141it [00:28, 3472.24it/s]62426it [00:24, 653.30it/s]64948it [00:24, 2578.42it/s]70491it [00:28, 3368.70it/s]62730it [00:24, 825.57it/s]65290it [00:24, 2718.74it/s]70841it [00:28, 3405.99it/s]63098it [00:24, 1092.46it/s]62087it [00:24, 507.73it/s] 65654it [00:25, 2947.10it/s]71184it [00:28, 3307.28it/s]63468it [00:25, 1399.45it/s]62457it [00:24, 693.05it/s]66018it [00:25, 3009.05it/s]71548it [00:28, 3399.97it/s]63799it [00:25, 1654.77it/s]62765it [00:24, 875.76it/s]66383it [00:25, 3178.08it/s]71898it [00:28, 3273.46it/s]64156it [00:25, 1977.97it/s]63134it [00:25, 1153.24it/s]66750it [00:25, 3311.64it/s]72263it [00:29, 3378.47it/s]64489it [00:25, 2203.83it/s]63482it [00:25, 1440.37it/s]67101it [00:25, 3268.27it/s]72623it [00:29, 3412.95it/s]64861it [00:25, 2528.44it/s]63805it [00:25, 1683.78it/s]67468it [00:25, 3379.51it/s]72966it [00:29, 3343.45it/s]65200it [00:25, 2665.01it/s]64170it [00:25, 2026.19it/s]67817it [00:25, 3299.27it/s]73330it [00:29, 3426.99it/s]65562it [00:25, 2899.95it/s]64500it [00:25, 2249.39it/s]68183it [00:25, 3400.43it/s]73674it [00:29, 3320.74it/s]65932it [00:25, 3107.67it/s]64871it [00:25, 2568.59it/s]68538it [00:25, 3314.68it/s]74054it [00:29, 3457.84it/s]66281it [00:25, 3100.84it/s]65209it [00:25, 2700.00it/s]68909it [00:25, 3426.42it/s]74418it [00:29, 3327.59it/s]66648it [00:25, 3250.32it/s]65553it [00:25, 2884.26it/s]69275it [00:26, 3493.27it/s]74787it [00:29, 3429.15it/s]65922it [00:25, 3094.75it/s]66993it [00:26, 3220.81it/s]69628it [00:26, 3385.27it/s]75152it [00:29, 3442.84it/s]67350it [00:26, 3317.44it/s]66266it [00:26, 3089.60it/s]69999it [00:26, 3476.35it/s]75498it [00:29, 3336.84it/s]66632it [00:26, 3245.71it/s]67698it [00:26, 3257.87it/s]70349it [00:26, 3224.84it/s]75866it [00:30, 3432.21it/s]68063it [00:26, 3366.85it/s]66975it [00:26, 3204.98it/s]70717it [00:26, 3349.38it/s]68425it [00:26, 3437.57it/s]76211it [00:30, 3295.80it/s]67344it [00:26, 3338.98it/s]71058it [00:26, 3267.60it/s]67693it [00:26, 3380.13it/s]76543it [00:30, 3211.72it/s]68774it [00:26, 3320.82it/s]71426it [00:26, 3381.69it/s]76896it [00:30, 3300.23it/s]69129it [00:26, 3385.15it/s]68039it [00:26, 3298.65it/s]71794it [00:26, 3465.21it/s]68401it [00:26, 3388.97it/s]77228it [00:30, 3185.43it/s]69471it [00:26, 3227.05it/s]72143it [00:26, 3362.42it/s]77583it [00:30, 3286.99it/s]68744it [00:26, 3314.60it/s]69826it [00:26, 3317.60it/s]72508it [00:27, 3443.80it/s]69104it [00:26, 3396.58it/s]77914it [00:30, 3147.85it/s]70161it [00:27, 3227.26it/s]72855it [00:27, 3362.21it/s]69447it [00:26, 3310.26it/s]78266it [00:30, 3252.35it/s]70486it [00:27, 3106.69it/s]73226it [00:27, 3459.99it/s]69795it [00:27, 3357.99it/s]70835it [00:27, 3212.10it/s]78619it [00:30, 3101.17it/s]73579it [00:27, 3373.33it/s]70164it [00:27, 3452.53it/s]78969it [00:31, 3210.65it/s]71159it [00:27, 3080.16it/s]73956it [00:27, 3485.75it/s]70511it [00:27, 3341.91it/s]79316it [00:31, 3269.27it/s]71508it [00:27, 3195.39it/s]74325it [00:27, 3544.21it/s]70873it [00:27, 3421.07it/s]71863it [00:27, 3294.48it/s]79646it [00:31, 3129.32it/s]74681it [00:27, 3424.60it/s]71217it [00:27, 3313.76it/s]80001it [00:31, 3247.02it/s]72195it [00:27, 3134.98it/s]75034it [00:27, 3454.64it/s]71569it [00:27, 3355.83it/s]72547it [00:27, 3241.34it/s]80329it [00:31, 3081.21it/s]75381it [00:27, 3341.14it/s]71906it [00:27, 3275.65it/s]72874it [00:27, 3130.32it/s]80687it [00:31, 3218.13it/s]75746it [00:27, 3429.43it/s]72263it [00:27, 3358.83it/s]73227it [00:28, 3243.36it/s]81036it [00:31, 3295.18it/s]76099it [00:28, 3307.31it/s]72629it [00:27, 3445.34it/s]81369it [00:31, 3122.77it/s]73579it [00:28, 3132.60it/s]76462it [00:28, 3398.36it/s]72975it [00:28, 3362.77it/s]81716it [00:31, 3220.04it/s]73940it [00:28, 3265.70it/s]76825it [00:28, 3463.00it/s]73337it [00:28, 3437.34it/s]74290it [00:28, 3330.28it/s]82041it [00:32, 3061.89it/s]77173it [00:28, 3343.84it/s]73682it [00:28, 3311.27it/s]82392it [00:32, 3186.36it/s]74626it [00:28, 3166.54it/s]77532it [00:28, 3412.46it/s]74056it [00:28, 3433.80it/s]82745it [00:32, 3284.12it/s]74973it [00:28, 3251.42it/s]77875it [00:28, 3321.71it/s]74419it [00:28, 3327.94it/s]75301it [00:28, 3109.88it/s]78239it [00:28, 3411.65it/s]83077it [00:32, 3104.42it/s]74785it [00:28, 3419.55it/s]75649it [00:28, 3213.42it/s]78606it [00:28, 3486.36it/s]83427it [00:32, 3214.53it/s]75146it [00:28, 3472.67it/s]75994it [00:28, 3278.69it/s]78956it [00:28, 3368.64it/s]83752it [00:32, 3087.41it/s]75495it [00:28, 3348.34it/s]79319it [00:29, 3438.68it/s]76324it [00:28, 3119.10it/s]84088it [00:32, 3163.65it/s]75840it [00:28, 3376.20it/s]76674it [00:29, 3225.88it/s]84440it [00:32, 3264.60it/s]79665it [00:29, 3340.12it/s]76179it [00:28, 3263.42it/s]80031it [00:29, 3431.97it/s]84769it [00:32, 3134.95it/s]77000it [00:29, 3091.45it/s]76541it [00:29, 3365.20it/s]85101it [00:33, 3186.88it/s]77351it [00:29, 3208.54it/s]80376it [00:29, 3296.56it/s]76902it [00:29, 3434.70it/s]77690it [00:29, 3258.80it/s]80743it [00:29, 3401.89it/s]77247it [00:29, 3321.87it/s]81106it [00:29, 3466.44it/s]78018it [00:29, 3116.04it/s]77611it [00:29, 3413.23it/s]78368it [00:29, 3223.45it/s]81455it [00:29, 3346.00it/s]77954it [00:29, 3262.84it/s]81820it [00:29, 3430.51it/s]78693it [00:29, 3083.25it/s]78320it [00:29, 3373.55it/s]82165it [00:29, 3342.98it/s]79042it [00:29, 3195.71it/s]78660it [00:29, 3280.70it/s]82529it [00:29, 3426.20it/s]79391it [00:29, 3277.56it/s]79021it [00:29, 3372.62it/s]82873it [00:30, 3334.94it/s]79721it [00:30, 3133.70it/s]79385it [00:29, 3447.62it/s]83236it [00:30, 3418.22it/s]80067it [00:30, 3224.98it/s]79732it [00:30, 3332.71it/s]83583it [00:30, 3433.03it/s]80392it [00:30, 3053.60it/s]80077it [00:30, 3364.68it/s]83928it [00:30, 3342.31it/s]80743it [00:30, 3181.12it/s]80415it [00:30, 3282.43it/s]84291it [00:30, 3424.26it/s]81089it [00:30, 3260.00it/s]80781it [00:30, 3390.86it/s]84635it [00:30, 3333.27it/s]81418it [00:30, 3116.56it/s]81139it [00:30, 3281.53it/s]84997it [00:30, 3415.14it/s]81763it [00:30, 3209.09it/s]81500it [00:30, 3374.08it/s]85340it [00:30, 3312.16it/s]82087it [00:30, 3075.69it/s]81864it [00:30, 3449.17it/s]82438it [00:30, 3197.25it/s]82211it [00:30, 3293.34it/s]82790it [00:31, 3289.15it/s]82574it [00:30, 3388.17it/s]83122it [00:31, 3135.01it/s]82915it [00:30, 3293.13it/s]83474it [00:31, 3242.38it/s]83280it [00:31, 3393.47it/s]83801it [00:31, 3077.32it/s]83644it [00:31, 3464.39it/s]84150it [00:31, 3191.69it/s]83992it [00:31, 3348.11it/s]84339it [00:31, 3382.41it/s]84499it [00:31, 3077.57it/s]84850it [00:31, 3196.33it/s]84679it [00:31, 3282.08it/s]85199it [00:31, 3274.05it/s]85045it [00:31, 3389.76it/s]85386it [00:31, 3290.82it/s]85422it [00:36, 300.18it/s] 85767it [00:36, 417.76it/s]86129it [00:36, 579.88it/s]86422it [00:36, 735.02it/s]85673it [00:33, 443.61it/s] 86773it [00:36, 977.84it/s]86038it [00:33, 610.70it/s]87088it [00:36, 1197.34it/s]86351it [00:33, 785.27it/s]87449it [00:37, 1522.18it/s]86715it [00:33, 1042.36it/s]87788it [00:37, 1821.60it/s]87077it [00:33, 1336.17it/s]88110it [00:37, 2035.49it/s]87403it [00:33, 1594.74it/s]88468it [00:37, 2355.46it/s]87756it [00:33, 1915.38it/s]88793it [00:37, 2472.43it/s]88087it [00:33, 2155.55it/s]89135it [00:37, 2699.17it/s]88457it [00:33, 2482.85it/s]89479it [00:37, 2888.04it/s]88795it [00:34, 2637.08it/s]89807it [00:37, 2866.52it/s]89164it [00:34, 2895.08it/s]90137it [00:37, 2982.39it/s]89527it [00:34, 3084.48it/s]90456it [00:38, 2880.08it/s]89875it [00:34, 3096.27it/s]90798it [00:38, 3022.52it/s]90234it [00:34, 3230.60it/s]85530it [00:34, 383.55it/s] 91121it [00:38, 3078.52it/s]90578it [00:34, 3122.57it/s]85877it [00:34, 525.00it/s]91438it [00:38, 2941.04it/s]90929it [00:34, 3226.81it/s]86235it [00:34, 712.65it/s]91777it [00:38, 3057.57it/s]91280it [00:34, 3304.57it/s]85717it [00:34, 363.67it/s] 86530it [00:34, 888.40it/s]92102it [00:38, 3110.40it/s]91619it [00:34, 3208.36it/s]86082it [00:34, 505.94it/s]86878it [00:34, 1153.90it/s]92418it [00:38, 2962.36it/s]91969it [00:35, 3289.35it/s]86387it [00:34, 654.09it/s]87185it [00:35, 1381.42it/s]92758it [00:38, 3083.36it/s]86746it [00:34, 879.57it/s]92303it [00:35, 3197.56it/s]87537it [00:35, 1708.08it/s]93071it [00:38, 2909.14it/s]92654it [00:35, 3284.95it/s]87089it [00:35, 1113.06it/s]87891it [00:35, 2035.75it/s]93412it [00:38, 3046.06it/s]92986it [00:35, 3197.64it/s]87457it [00:35, 1426.11it/s]88217it [00:35, 2199.13it/s]93750it [00:39, 3140.32it/s]93338it [00:35, 3288.91it/s]87824it [00:35, 1758.46it/s]88574it [00:35, 2497.95it/s]93676it [00:35, 3314.54it/s]94068it [00:39, 2991.71it/s]88160it [00:35, 2005.67it/s]88898it [00:35, 2586.35it/s]94388it [00:39, 3049.03it/s]88530it [00:35, 2338.75it/s]94010it [00:35, 3213.67it/s]89248it [00:35, 2810.65it/s]94361it [00:35, 3298.76it/s]94696it [00:39, 2912.50it/s]88870it [00:35, 2510.49it/s]89592it [00:35, 2973.28it/s]95032it [00:39, 3036.84it/s]89216it [00:35, 2733.16it/s]94693it [00:35, 3202.00it/s]89922it [00:35, 2907.52it/s]95353it [00:39, 3085.47it/s]89576it [00:35, 2950.30it/s]95043it [00:35, 3285.97it/s]90262it [00:35, 3039.76it/s]95392it [00:36, 3343.81it/s]89918it [00:35, 2983.55it/s]95664it [00:39, 2940.91it/s]90584it [00:36, 2917.68it/s]90274it [00:35, 3135.66it/s]95728it [00:36, 3227.06it/s]96001it [00:39, 3060.07it/s]90919it [00:36, 3033.45it/s]96326it [00:39, 3113.77it/s]96072it [00:36, 3264.28it/s]90613it [00:36, 3081.60it/s]91238it [00:36, 3074.80it/s]96400it [00:36, 3178.07it/s]90965it [00:36, 3202.16it/s]96640it [00:40, 2960.54it/s]91553it [00:36, 2930.87it/s]96749it [00:36, 3267.25it/s]96976it [00:40, 3073.16it/s]91299it [00:36, 3023.59it/s]91886it [00:36, 3041.25it/s]97099it [00:36, 3333.85it/s]91650it [00:36, 3155.22it/s]97286it [00:40, 2905.59it/s]92196it [00:36, 2909.68it/s]97434it [00:36, 3232.70it/s]92001it [00:36, 3253.98it/s]97626it [00:40, 3041.20it/s]92530it [00:36, 3028.33it/s]97783it [00:36, 3304.99it/s]97962it [00:40, 3131.80it/s]92334it [00:36, 3166.00it/s]92867it [00:36, 3125.62it/s]98115it [00:36, 3198.16it/s]92682it [00:36, 3254.00it/s]98278it [00:40, 2976.59it/s]93183it [00:36, 2953.22it/s]98465it [00:37, 3281.27it/s]98598it [00:40, 3039.37it/s]93012it [00:36, 3158.91it/s]93522it [00:37, 3075.38it/s]98816it [00:37, 3345.98it/s]93345it [00:36, 3206.29it/s]98905it [00:40, 2914.21it/s]93834it [00:37, 2926.95it/s]99152it [00:37, 3194.63it/s]93698it [00:37, 3298.35it/s]99243it [00:40, 3043.05it/s]94166it [00:37, 3034.23it/s]99500it [00:37, 3276.07it/s]94030it [00:37, 3182.44it/s]99562it [00:41, 3084.95it/s]94488it [00:37, 3085.59it/s]99830it [00:37, 3179.94it/s]94382it [00:37, 3277.67it/s]99873it [00:41, 2941.55it/s]94800it [00:37, 2927.54it/s]100178it [00:37, 3263.45it/s]94712it [00:37, 3176.25it/s]100205it [00:41, 3047.63it/s]95138it [00:37, 3054.58it/s]100528it [00:37, 3182.55it/s]95046it [00:37, 3220.69it/s]100528it [00:41, 2887.66it/s]95472it [00:37, 3136.12it/s]100879it [00:37, 3274.99it/s]95396it [00:37, 3299.64it/s]100868it [00:41, 3029.22it/s]95789it [00:37, 2958.32it/s]101228it [00:37, 3333.04it/s]95728it [00:37, 3184.97it/s]101207it [00:41, 3129.04it/s]96124it [00:37, 3066.98it/s]101563it [00:37, 3222.42it/s]96077it [00:37, 3272.18it/s]101523it [00:41, 2927.81it/s]96434it [00:38, 2924.39it/s]101896it [00:38, 3252.68it/s]96406it [00:37, 3170.78it/s]101858it [00:41, 3040.13it/s]96770it [00:38, 3044.56it/s]102223it [00:38, 3127.67it/s]96756it [00:37, 3264.33it/s]102198it [00:41, 3140.24it/s]97102it [00:38, 3122.30it/s]102572it [00:38, 3230.71it/s]97099it [00:38, 3310.94it/s]102516it [00:42, 2922.90it/s]97417it [00:38, 2927.59it/s]102922it [00:38, 3306.80it/s]97432it [00:38, 3199.74it/s]102852it [00:42, 3042.88it/s]97750it [00:38, 3039.70it/s]103255it [00:38, 3209.67it/s]97779it [00:38, 3275.41it/s]103161it [00:42, 2921.46it/s]103606it [00:38, 3294.83it/s]98058it [00:38, 2901.22it/s]98108it [00:38, 3168.64it/s]103496it [00:42, 3040.47it/s]98390it [00:38, 3016.23it/s]103937it [00:38, 3186.52it/s]98456it [00:38, 3256.93it/s]103821it [00:42, 3099.69it/s]98724it [00:38, 3106.63it/s]104286it [00:38, 3272.11it/s]98806it [00:38, 3326.28it/s]104134it [00:42, 2953.84it/s]104622it [00:38, 3295.79it/s]99038it [00:38, 2950.97it/s]99140it [00:38, 3208.42it/s]104464it [00:42, 3049.57it/s]104953it [00:39, 3189.06it/s]99374it [00:38, 3065.02it/s]99492it [00:38, 3297.00it/s]104772it [00:42, 2915.81it/s]105301it [00:39, 3271.84it/s]99824it [00:38, 3194.63it/s]99689it [00:39, 2917.03it/s]105107it [00:42, 3036.34it/s]105630it [00:39, 3182.98it/s]100173it [00:39, 3278.26it/s]100026it [00:39, 3041.12it/s]105444it [00:42, 3129.70it/s]105979it [00:39, 3270.06it/s]100512it [00:39, 3310.40it/s]100359it [00:39, 3121.21it/s]105760it [00:43, 2949.10it/s]106330it [00:39, 3338.50it/s]100845it [00:39, 3192.21it/s]100674it [00:39, 2896.43it/s]106096it [00:43, 3062.03it/s]106665it [00:39, 3226.24it/s]101196it [00:39, 3282.58it/s]101013it [00:39, 3031.25it/s]107001it [00:39, 3262.18it/s]106408it [00:43, 2922.41it/s]101526it [00:39, 3164.77it/s]101346it [00:39, 3114.08it/s]106751it [00:43, 3061.98it/s]107329it [00:39, 3173.40it/s]101875it [00:39, 3257.68it/s]101661it [00:39, 2945.63it/s]107070it [00:43, 3097.81it/s]107681it [00:39, 3272.22it/s]102209it [00:39, 3152.67it/s]101996it [00:39, 3056.82it/s]108034it [00:39, 3346.91it/s]107383it [00:43, 2956.80it/s]102561it [00:39, 3255.34it/s]102306it [00:39, 2906.94it/s]107723it [00:43, 3080.97it/s]108370it [00:40, 3234.48it/s]102912it [00:39, 3328.79it/s]102636it [00:40, 3016.32it/s]108057it [00:43, 3152.80it/s]108720it [00:40, 3310.02it/s]103247it [00:39, 3215.45it/s]102973it [00:40, 3115.64it/s]109053it [00:40, 3220.15it/s]108375it [00:43, 2988.97it/s]103597it [00:40, 3295.92it/s]103288it [00:40, 2945.19it/s]109405it [00:40, 3304.35it/s]108677it [00:44, 2937.19it/s]103929it [00:40, 3184.80it/s]103619it [00:40, 3045.51it/s]109742it [00:40, 3321.82it/s]108973it [00:44, 2850.46it/s]104279it [00:40, 3274.21it/s]103927it [00:40, 2781.06it/s]110076it [00:40, 3218.77it/s]109311it [00:44, 2997.77it/s]104630it [00:40, 3341.70it/s]104257it [00:40, 2921.22it/s]110426it [00:40, 3298.99it/s]109647it [00:44, 3099.15it/s]104966it [00:40, 3211.41it/s]104592it [00:40, 3038.65it/s]110758it [00:40, 3201.17it/s]109959it [00:44, 2951.69it/s]105313it [00:40, 3284.41it/s]104901it [00:40, 2900.06it/s]111111it [00:40, 3294.96it/s]110298it [00:44, 3074.38it/s]105644it [00:40, 3181.16it/s]105232it [00:40, 3013.96it/s]111449it [00:41, 3187.58it/s]105992it [00:40, 3265.36it/s]110608it [00:44, 2929.95it/s]111800it [00:41, 3278.96it/s]105569it [00:41, 2893.82it/s]106342it [00:40, 3332.70it/s]110949it [00:44, 3063.68it/s]112150it [00:41, 3341.61it/s]105902it [00:41, 3013.34it/s]111287it [00:44, 3153.88it/s]106677it [00:41, 3198.09it/s]106234it [00:41, 3098.06it/s]112486it [00:41, 3204.71it/s]107027it [00:41, 3283.01it/s]111605it [00:45, 2960.78it/s]112834it [00:41, 3281.66it/s]106548it [00:41, 2941.63it/s]107357it [00:41, 3172.05it/s]111939it [00:45, 3066.09it/s]106854it [00:41, 2973.55it/s]113164it [00:41, 3186.09it/s]107708it [00:41, 3266.90it/s]112278it [00:45, 3156.52it/s]107170it [00:41, 3026.40it/s]113514it [00:41, 3274.04it/s]108060it [00:41, 3338.58it/s]112597it [00:45, 2968.97it/s]113866it [00:41, 3343.36it/s]107475it [00:41, 2844.49it/s]108396it [00:41, 3206.11it/s]112934it [00:45, 3079.67it/s]114202it [00:41, 3239.79it/s]107812it [00:41, 2991.07it/s]108752it [00:41, 3306.52it/s]113246it [00:45, 2955.94it/s]114553it [00:41, 3316.08it/s]108115it [00:41, 2866.88it/s]109085it [00:41, 3192.88it/s]113582it [00:45, 3068.70it/s]108446it [00:42, 2990.19it/s]109435it [00:41, 3280.38it/s]113917it [00:45, 3146.64it/s]108782it [00:42, 3093.88it/s]109765it [00:41, 3284.03it/s]114235it [00:45, 2960.51it/s]109095it [00:42, 2945.82it/s]110095it [00:42, 3185.05it/s]114578it [00:45, 3089.80it/s]109432it [00:42, 3065.23it/s]110450it [00:42, 3287.90it/s]110781it [00:42, 3195.07it/s]109769it [00:42, 2935.27it/s]111139it [00:42, 3304.00it/s]110086it [00:42, 2999.38it/s]110428it [00:42, 3117.81it/s]111471it [00:42, 3203.30it/s]111825it [00:42, 3298.82it/s]110743it [00:42, 2969.71it/s]112182it [00:42, 3376.15it/s]111084it [00:42, 3090.96it/s]112521it [00:42, 3253.07it/s]111419it [00:43, 3162.59it/s]112876it [00:42, 3337.16it/s]111738it [00:43, 2994.29it/s]113212it [00:43, 3175.35it/s]112078it [00:43, 3106.95it/s]113566it [00:43, 3276.21it/s]112392it [00:43, 2945.48it/s]113922it [00:43, 3356.56it/s]112729it [00:43, 3063.17it/s]114260it [00:43, 3244.70it/s]113041it [00:43, 3076.51it/s]114618it [00:43, 3338.27it/s]113351it [00:43, 2930.18it/s]113690it [00:43, 3058.48it/s]113999it [00:43, 2927.52it/s]114345it [00:43, 3077.22it/s]114687it [00:44, 3174.05it/s]114886it [00:45, 318.30it/s] 115243it [00:45, 443.15it/s]115588it [00:45, 599.45it/s]115890it [00:45, 766.68it/s]116249it [00:45, 1020.23it/s]116564it [00:45, 1256.28it/s]116925it [00:45, 1583.65it/s]117285it [00:46, 1917.45it/s]117621it [00:46, 2149.68it/s]117982it [00:46, 2457.23it/s]118319it [00:46, 2574.35it/s]118679it [00:46, 2821.20it/s]119037it [00:46, 3014.89it/s]119378it [00:46, 3024.67it/s]119731it [00:46, 3160.93it/s]120068it [00:46, 3127.52it/s]114891it [00:50, 224.46it/s] 120427it [00:46, 3255.33it/s]114954it [00:46, 320.94it/s] 115250it [00:50, 320.91it/s]120764it [00:47, 3188.29it/s]115310it [00:46, 444.85it/s]115608it [00:50, 449.06it/s]121120it [00:47, 3292.57it/s]115665it [00:47, 605.76it/s]115899it [00:50, 579.48it/s]121460it [00:47, 3323.16it/s]115963it [00:47, 769.22it/s]116254it [00:50, 789.04it/s]121797it [00:47, 3237.83it/s]116324it [00:47, 1023.79it/s]116563it [00:51, 997.32it/s]122153it [00:47, 3328.79it/s]116923it [00:51, 1297.76it/s]116641it [00:47, 1258.24it/s]122489it [00:47, 3244.29it/s]117273it [00:51, 1609.40it/s]117000it [00:47, 1581.52it/s]122849it [00:47, 3344.23it/s]117358it [00:47, 1911.45it/s]117604it [00:51, 1857.29it/s]123204it [00:47, 3403.50it/s]117958it [00:51, 2177.67it/s]117693it [00:47, 2135.98it/s]123546it [00:47, 3284.50it/s]118028it [00:47, 2391.02it/s]118289it [00:51, 2371.04it/s]123885it [00:48, 3312.92it/s]118647it [00:51, 2649.15it/s]118356it [00:47, 2543.98it/s]124218it [00:48, 3223.51it/s]118987it [00:51, 2833.82it/s]118713it [00:47, 2793.58it/s]124572it [00:48, 3313.59it/s]115007it [00:48, 251.94it/s] 119068it [00:48, 2988.30it/s]119322it [00:51, 2838.10it/s]124928it [00:48, 3382.95it/s]115344it [00:48, 351.10it/s]119406it [00:48, 2986.21it/s]119671it [00:52, 3008.14it/s]125268it [00:48, 3265.30it/s]115682it [00:48, 482.71it/s]119756it [00:48, 3124.00it/s]120000it [00:52, 2997.20it/s]125622it [00:48, 3343.65it/s]115965it [00:48, 617.10it/s]120089it [00:48, 3078.60it/s]120343it [00:52, 3113.87it/s]125958it [00:48, 3247.46it/s]116295it [00:48, 822.88it/s]120445it [00:48, 3210.99it/s]120700it [00:52, 3240.74it/s]126310it [00:48, 3324.98it/s]116588it [00:48, 1019.06it/s]120778it [00:48, 3141.24it/s]121036it [00:52, 3172.13it/s]126644it [00:48, 3187.14it/s]116931it [00:48, 1313.90it/s]121132it [00:48, 3253.54it/s]121388it [00:52, 3270.78it/s]126997it [00:48, 3282.69it/s]117260it [00:48, 1607.92it/s]121481it [00:48, 3278.40it/s]121722it [00:52, 3157.57it/s]127353it [00:49, 3362.26it/s]117570it [00:49, 1817.90it/s]122077it [00:52, 3267.53it/s]121814it [00:48, 3107.38it/s]127691it [00:49, 3256.36it/s]117912it [00:49, 2131.08it/s]122435it [00:52, 3355.65it/s]122164it [00:49, 3215.45it/s]128045it [00:49, 3335.65it/s]118237it [00:49, 2251.11it/s]122774it [00:52, 3253.84it/s]122490it [00:49, 3152.97it/s]128381it [00:49, 3243.35it/s]118583it [00:49, 2526.98it/s]123111it [00:53, 3286.28it/s]122845it [00:49, 3265.97it/s]128738it [00:49, 3325.56it/s]118921it [00:49, 2735.19it/s]123199it [00:49, 3343.27it/s]123442it [00:53, 3191.47it/s]129093it [00:49, 3390.03it/s]119240it [00:49, 2690.09it/s]123794it [00:53, 3285.11it/s]123536it [00:49, 3224.35it/s]129434it [00:49, 3248.33it/s]119574it [00:49, 2857.18it/s]123883it [00:49, 3292.25it/s]124125it [00:53, 3023.04it/s]129785it [00:49, 3322.59it/s]119917it [00:49, 2815.43it/s]124215it [00:49, 3184.32it/s]124449it [00:53, 3080.66it/s]130119it [00:49, 3229.10it/s]120249it [00:49, 2948.44it/s]124570it [00:49, 3287.04it/s]124761it [00:53, 3048.94it/s]130473it [00:50, 3317.16it/s]120590it [00:49, 3074.86it/s]124922it [00:49, 3352.90it/s]125069it [00:53, 2915.53it/s]130827it [00:50, 3380.52it/s]120909it [00:50, 2945.65it/s]125259it [00:49, 3188.47it/s]125402it [00:53, 3029.33it/s]131167it [00:50, 3266.56it/s]121240it [00:50, 3045.43it/s]125609it [00:50, 3274.61it/s]125736it [00:53, 3116.99it/s]131518it [00:50, 3335.76it/s]121580it [00:50, 3143.51it/s]125939it [00:50, 3185.66it/s]126050it [00:54, 2969.55it/s]131853it [00:50, 3195.62it/s]121900it [00:50, 2992.07it/s]126290it [00:50, 3277.26it/s]126383it [00:54, 3070.42it/s]132206it [00:50, 3290.49it/s]122240it [00:50, 3105.26it/s]126637it [00:50, 3163.73it/s]126693it [00:54, 2864.83it/s]132537it [00:50, 3199.94it/s]122555it [00:50, 2956.77it/s]126988it [00:50, 3260.73it/s]127026it [00:54, 2992.77it/s]132892it [00:50, 3297.57it/s]127341it [00:50, 3337.85it/s]122899it [00:50, 3089.30it/s]127363it [00:54, 3098.73it/s]133245it [00:50, 3363.92it/s]123234it [00:50, 3163.16it/s]127677it [00:50, 3223.43it/s]127677it [00:54, 2950.59it/s]133583it [00:50, 3255.36it/s]128030it [00:50, 3309.61it/s]123554it [00:50, 2988.49it/s]128011it [00:54, 3057.51it/s]133938it [00:51, 3339.39it/s]123892it [00:51, 3096.94it/s]128363it [00:50, 3194.69it/s]134274it [00:51, 3237.56it/s]128320it [00:54, 2849.71it/s]128704it [00:51, 3255.80it/s]124206it [00:51, 2957.21it/s]134632it [00:51, 3333.50it/s]128661it [00:54, 3002.27it/s]129058it [00:51, 3337.96it/s]124537it [00:51, 3051.83it/s]134972it [00:51, 3351.31it/s]128995it [00:55, 3097.38it/s]129394it [00:51, 3214.66it/s]124879it [00:51, 3154.57it/s]135309it [00:51, 3263.76it/s]129309it [00:55, 2941.05it/s]129748it [00:51, 3307.01it/s]125198it [00:51, 2996.42it/s]135668it [00:51, 3348.84it/s]129647it [00:55, 3059.11it/s]130081it [00:51, 3197.12it/s]125522it [00:51, 3064.49it/s]136004it [00:51, 3260.75it/s]129958it [00:55, 3071.49it/s]130436it [00:51, 3297.55it/s]125832it [00:51, 2940.61it/s]136360it [00:51, 3346.49it/s]130268it [00:55, 2932.27it/s]130790it [00:51, 3366.39it/s]126171it [00:51, 3065.53it/s]136715it [00:51, 3405.02it/s]130599it [00:55, 3037.87it/s]131129it [00:51, 3244.62it/s]126505it [00:51, 3142.74it/s]137057it [00:52, 3290.95it/s]130906it [00:55, 2894.77it/s]131482it [00:51, 3324.11it/s]137414it [00:52, 3369.99it/s]126822it [00:52, 2954.43it/s]131240it [00:55, 3019.04it/s]131816it [00:51, 3210.98it/s]127159it [00:52, 3068.46it/s]137753it [00:52, 3262.81it/s]131547it [00:55, 3031.25it/s]132161it [00:52, 3245.12it/s]138096it [00:52, 3310.30it/s]127477it [00:52, 2945.72it/s]131853it [00:56, 2901.56it/s]132515it [00:52, 3329.85it/s]138429it [00:52, 3221.26it/s]127818it [00:52, 3074.94it/s]132182it [00:56, 3010.02it/s]132850it [00:52, 3213.92it/s]138784it [00:52, 3315.16it/s]128137it [00:52, 3105.97it/s]132517it [00:56, 2888.29it/s]133202it [00:52, 3301.12it/s]139141it [00:52, 3389.00it/s]128450it [00:52, 2970.49it/s]132856it [00:56, 3026.08it/s]133534it [00:52, 3194.85it/s]139481it [00:52, 3277.23it/s]128789it [00:52, 3088.23it/s]133164it [00:56, 3040.70it/s]133886it [00:52, 3287.02it/s]139837it [00:52, 3356.20it/s]129127it [00:52, 3170.69it/s]133471it [00:56, 2904.34it/s]134217it [00:52, 3188.27it/s]140174it [00:52, 3253.21it/s]129447it [00:52, 2966.84it/s]133806it [00:56, 3028.45it/s]134570it [00:52, 3285.87it/s]140530it [00:53, 3339.15it/s]129788it [00:53, 3088.91it/s]134140it [00:56, 3115.80it/s]134923it [00:52, 3355.00it/s]140886it [00:53, 3400.74it/s]130101it [00:53, 2962.22it/s]134454it [00:56, 2965.19it/s]135260it [00:53, 3245.57it/s]141228it [00:53, 3249.38it/s]130442it [00:53, 3085.38it/s]134763it [00:56, 2998.64it/s]135617it [00:53, 3337.72it/s]141583it [00:53, 3335.14it/s]130783it [00:53, 3177.41it/s]135065it [00:57, 2870.93it/s]135953it [00:53, 3174.43it/s]141919it [00:53, 3229.98it/s]131104it [00:53, 2980.83it/s]135408it [00:57, 3026.13it/s]136307it [00:53, 3276.23it/s]142276it [00:53, 3325.34it/s]131441it [00:53, 3088.89it/s]135745it [00:57, 3122.30it/s]136660it [00:53, 3348.55it/s]142611it [00:53, 3218.16it/s]131754it [00:53, 2937.10it/s]136060it [00:57, 2967.41it/s]136997it [00:53, 3238.43it/s]142969it [00:53, 3321.11it/s]132096it [00:53, 3070.43it/s]136395it [00:57, 3015.93it/s]137348it [00:53, 3312.03it/s]143325it [00:53, 3388.95it/s]132422it [00:53, 3122.81it/s]137681it [00:53, 3213.29it/s]136717it [00:57, 2887.92it/s]143666it [00:54, 3274.88it/s]132738it [00:53, 2967.55it/s]138040it [00:53, 3319.57it/s]137058it [00:57, 3031.25it/s]144023it [00:54, 3358.34it/s]133078it [00:54, 3088.30it/s]138394it [00:53, 3381.09it/s]137394it [00:57, 3123.01it/s]144361it [00:54, 3216.85it/s]133390it [00:54, 2940.59it/s]138734it [00:54, 3259.68it/s]137709it [00:57, 2963.93it/s]144723it [00:54, 3331.38it/s]133717it [00:54, 3030.51it/s]139090it [00:54, 3344.50it/s]138047it [00:58, 3078.99it/s]145080it [00:54, 3399.93it/s]134059it [00:54, 3140.12it/s]138373it [00:58, 3129.49it/s]139426it [00:54, 3198.61it/s]145422it [00:54, 3289.47it/s]134376it [00:54, 2989.81it/s]139783it [00:54, 3301.77it/s]138689it [00:58, 2960.56it/s]145781it [00:54, 3375.68it/s]134717it [00:54, 3105.92it/s]140116it [00:54, 3194.35it/s]139027it [00:58, 3071.09it/s]146121it [00:54, 3280.53it/s]135038it [00:54, 2935.95it/s]140473it [00:54, 3298.74it/s]146476it [00:54, 3357.24it/s]139338it [00:58, 2945.25it/s]135386it [00:54, 3085.17it/s]140827it [00:54, 3366.49it/s]139676it [00:58, 3065.20it/s]146814it [00:54, 3261.05it/s]135730it [00:54, 3184.05it/s]141166it [00:54, 3255.56it/s]140007it [00:58, 3133.68it/s]147173it [00:55, 3353.37it/s]136052it [00:55, 3027.31it/s]141521it [00:54, 3337.41it/s]147517it [00:55, 3377.11it/s]140323it [00:58, 2985.98it/s]136391it [00:55, 3128.90it/s]141857it [00:55, 3230.56it/s]140658it [00:58, 3086.66it/s]147856it [00:55, 3275.75it/s]142213it [00:55, 3322.52it/s]136717it [00:55, 3164.41it/s]148208it [00:55, 3345.01it/s]140970it [00:59, 2946.76it/s]137036it [00:55, 3003.20it/s]142570it [00:55, 3391.75it/s]148544it [00:55, 3250.87it/s]141299it [00:59, 3041.02it/s]137373it [00:55, 3106.11it/s]142911it [00:55, 3274.62it/s]148898it [00:55, 3328.26it/s]141634it [00:59, 3127.55it/s]143249it [00:55, 3303.07it/s]137687it [00:55, 2971.69it/s]149256it [00:55, 3398.83it/s]141949it [00:59, 2983.67it/s]138011it [00:55, 3045.94it/s]143581it [00:55, 3206.89it/s]149597it [00:55, 3293.87it/s]142288it [00:59, 3097.86it/s]138355it [00:55, 3157.81it/s]143935it [00:55, 3302.02it/s]149950it [00:55, 3359.62it/s]142601it [00:59, 2926.78it/s]138674it [00:55, 3007.10it/s]144278it [00:55, 3211.37it/s]150288it [00:56, 3270.56it/s]142945it [00:59, 3069.42it/s]139015it [00:56, 3119.74it/s]144637it [00:55, 3317.33it/s]150630it [00:56, 3311.42it/s]143283it [00:59, 3157.59it/s]144993it [00:55, 3387.53it/s]150987it [00:56, 3386.63it/s]139330it [00:56, 2934.68it/s]143602it [00:59, 2992.69it/s]145334it [00:56, 3268.91it/s]139671it [00:56, 3064.64it/s]143927it [00:59, 3062.19it/s]145692it [00:56, 3356.88it/s]140013it [00:56, 3164.57it/s]144264it [01:00, 3150.04it/s]146030it [00:56, 3256.65it/s]140333it [00:56, 3015.08it/s]144582it [01:00, 3005.57it/s]146386it [00:56, 3341.25it/s]140655it [00:56, 3071.32it/s]144928it [01:00, 3132.45it/s]146728it [00:56, 3359.76it/s]140965it [00:56, 2947.86it/s]145244it [01:00, 2970.88it/s]147066it [00:56, 3252.53it/s]141308it [00:56, 3083.55it/s]145588it [01:00, 3100.14it/s]147425it [00:56, 3348.38it/s]141651it [00:56, 3180.75it/s]145927it [01:00, 3182.11it/s]147762it [00:56, 3234.27it/s]141972it [00:57, 3011.84it/s]146248it [01:00, 3007.97it/s]148111it [00:56, 3305.23it/s]142303it [00:57, 3095.63it/s]146578it [01:00, 3089.83it/s]148468it [00:57, 3380.14it/s]142616it [00:57, 2950.97it/s]146890it [01:00, 2951.67it/s]148808it [00:57, 3245.01it/s]142953it [00:57, 3066.84it/s]147224it [01:01, 3058.44it/s]149168it [00:57, 3345.98it/s]143292it [00:57, 3156.68it/s]147567it [01:01, 3155.21it/s]149505it [00:57, 3243.82it/s]143611it [00:57, 3002.16it/s]147885it [01:01, 2982.73it/s]149862it [00:57, 3336.51it/s]143957it [00:57, 3128.71it/s]148222it [01:01, 3089.75it/s]150198it [00:57, 3193.60it/s]144278it [00:57, 2974.37it/s]148534it [01:01, 2954.04it/s]150557it [00:57, 3304.76it/s]144628it [00:57, 3118.00it/s]148871it [01:01, 3068.94it/s]150916it [00:57, 3385.53it/s]144977it [00:57, 3223.11it/s]149203it [01:01, 3138.18it/s]145303it [00:58, 3032.09it/s]149520it [01:01, 2992.96it/s]145647it [00:58, 3145.28it/s]149858it [01:01, 3099.88it/s]145966it [00:58, 3002.40it/s]150171it [01:02, 2913.38it/s]146312it [00:58, 3129.00it/s]150512it [01:02, 3049.18it/s]146658it [00:58, 3214.20it/s]150857it [01:02, 3155.14it/s]146983it [00:58, 3060.77it/s]147324it [00:58, 3157.72it/s]147643it [00:58, 3011.56it/s]147992it [00:58, 3145.17it/s]148338it [00:59, 3230.98it/s]148664it [00:59, 3068.75it/s]149011it [00:59, 3180.80it/s]149332it [00:59, 3002.73it/s]149680it [00:59, 3134.68it/s]150024it [00:59, 3220.35it/s]150349it [00:59, 3065.22it/s]150699it [00:59, 3185.55it/s]151021it [00:59, 3028.99it/s]151327it [01:00, 260.37it/s] 151671it [01:00, 359.73it/s]151976it [01:00, 474.70it/s]152336it [01:00, 654.50it/s]152699it [01:00, 880.39it/s]153022it [01:00, 1104.82it/s]153384it [01:00, 1411.60it/s]153715it [01:01, 1675.93it/s]154078it [01:01, 2014.68it/s]154428it [01:01, 2254.48it/s]154785it [01:01, 2527.48it/s]155151it [01:01, 2793.49it/s]155495it [01:01, 2875.23it/s]155859it [01:01, 3073.12it/s]156202it [01:01, 3074.92it/s]156567it [01:01, 3230.74it/s]156933it [01:02, 3349.32it/s]157282it [01:02, 3281.18it/s]157648it [01:02, 3384.85it/s]157995it [01:02, 3267.57it/s]158359it [01:02, 3370.92it/s]151257it [01:02, 243.92it/s] 158701it [01:02, 3288.53it/s]151619it [01:02, 342.09it/s]159073it [01:02, 3410.52it/s]151907it [01:02, 443.16it/s]159440it [01:02, 3484.95it/s]152269it [01:02, 614.36it/s]159791it [01:02, 3379.09it/s]152630it [01:02, 828.40it/s]160153it [01:02, 3447.98it/s]152952it [01:02, 1043.12it/s]160500it [01:03, 3346.49it/s]153311it [01:02, 1338.77it/s]160865it [01:03, 3432.90it/s]153639it [01:03, 1595.03it/s]161210it [01:03, 3300.25it/s]154005it [01:03, 1940.18it/s]161572it [01:03, 3390.86it/s]154365it [01:03, 2260.01it/s]161944it [01:03, 3485.77it/s]154707it [01:03, 2453.18it/s]162295it [01:03, 3372.67it/s]155056it [01:03, 2692.31it/s]162667it [01:03, 3472.32it/s]155393it [01:03, 2790.47it/s]163016it [01:03, 3371.14it/s]155757it [01:03, 3007.33it/s]163386it [01:03, 3464.59it/s]156108it [01:03, 3016.70it/s]163734it [01:04, 3366.10it/s]156472it [01:03, 3183.00it/s]164103it [01:04, 3456.43it/s]156836it [01:03, 3307.70it/s]164454it [01:04, 3470.58it/s]157182it [01:04, 3234.22it/s]164803it [01:04, 3379.27it/s]157549it [01:04, 3356.37it/s]165179it [01:04, 3487.39it/s]157894it [01:04, 3265.89it/s]165529it [01:04, 3399.95it/s]158247it [01:04, 3338.42it/s]165904it [01:04, 3501.04it/s]158613it [01:04, 3429.68it/s]166256it [01:04, 3409.45it/s]158960it [01:04, 3326.46it/s]166629it [01:04, 3500.69it/s]151176it [01:08, 168.01it/s] 159328it [01:04, 3422.23it/s]167002it [01:04, 3566.11it/s]151518it [01:08, 237.53it/s]159673it [01:04, 3322.88it/s]167360it [01:05, 3440.30it/s]151839it [01:08, 325.60it/s]160028it [01:04, 3386.49it/s]167718it [01:05, 3478.68it/s]152117it [01:08, 424.28it/s]160369it [01:05, 3291.64it/s]168068it [01:05, 3387.35it/s]152458it [01:08, 587.61it/s]160728it [01:05, 3370.75it/s]168435it [01:05, 3466.80it/s]152752it [01:09, 752.11it/s]161091it [01:05, 3445.18it/s]168783it [01:05, 3377.76it/s]153097it [01:09, 1003.45it/s]161437it [01:05, 3294.76it/s]169148it [01:05, 3454.90it/s]153442it [01:09, 1290.54it/s]151328it [01:05, 184.09it/s] 161799it [01:05, 3385.79it/s]169513it [01:05, 3510.01it/s]151673it [01:05, 261.20it/s]153758it [01:09, 1510.87it/s]162140it [01:05, 3293.75it/s]169865it [01:05, 3391.35it/s]151929it [01:05, 336.21it/s]154099it [01:09, 1825.26it/s]162504it [01:05, 3391.74it/s]170238it [01:05, 3486.80it/s]152274it [01:05, 474.89it/s]154426it [01:09, 2021.88it/s]162845it [01:05, 3290.36it/s]170588it [01:06, 3374.23it/s]152620it [01:05, 653.45it/s]154775it [01:09, 2326.89it/s]163211it [01:05, 3395.35it/s]170953it [01:06, 3452.43it/s]152921it [01:06, 831.10it/s]155123it [01:09, 2590.89it/s]163579it [01:05, 3476.21it/s]171300it [01:06, 3349.63it/s]153265it [01:06, 1092.57it/s]163928it [01:06, 3348.88it/s]155447it [01:09, 2580.54it/s]171664it [01:06, 3431.29it/s]153587it [01:06, 1327.55it/s]164296it [01:06, 3441.40it/s]155791it [01:10, 2793.33it/s]172035it [01:06, 3509.76it/s]153920it [01:06, 1625.45it/s]164642it [01:06, 3323.27it/s]156107it [01:10, 2755.29it/s]172388it [01:06, 3396.03it/s]154268it [01:06, 1949.75it/s]165001it [01:06, 3399.02it/s]156455it [01:10, 2944.95it/s]172744it [01:06, 3440.96it/s]154588it [01:06, 2121.73it/s]156803it [01:10, 3089.95it/s]165348it [01:06, 3307.18it/s]173090it [01:06, 3362.87it/s]154932it [01:06, 2406.08it/s]165714it [01:06, 3407.70it/s]157128it [01:10, 2944.02it/s]173454it [01:06, 3440.70it/s]166083it [01:06, 3488.55it/s]155267it [01:06, 2497.41it/s]157473it [01:10, 3081.09it/s]173800it [01:06, 3345.15it/s]155617it [01:06, 2734.69it/s]166434it [01:06, 3375.47it/s]157791it [01:10, 2959.06it/s]174168it [01:07, 3440.74it/s]155967it [01:07, 2926.27it/s]166813it [01:06, 3492.50it/s]158136it [01:10, 3092.41it/s]174544it [01:07, 3531.67it/s]156293it [01:07, 2838.90it/s]167164it [01:07, 3364.13it/s]158486it [01:10, 3200.23it/s]174899it [01:07, 3369.17it/s]156637it [01:07, 2996.32it/s]167531it [01:07, 3449.29it/s]158812it [01:11, 3051.86it/s]175274it [01:07, 3477.15it/s]167878it [01:07, 3334.95it/s]156956it [01:07, 2904.02it/s]159160it [01:11, 3170.93it/s]175624it [01:07, 3369.78it/s]168241it [01:07, 3418.59it/s]157305it [01:07, 3061.53it/s]159482it [01:11, 3033.24it/s]175987it [01:07, 3441.48it/s]168613it [01:07, 3504.48it/s]157656it [01:07, 3185.78it/s]159827it [01:11, 3149.17it/s]176333it [01:07, 3340.43it/s]168965it [01:07, 3376.39it/s]157984it [01:07, 3047.38it/s]160173it [01:11, 3237.00it/s]176701it [01:07, 3436.54it/s]169329it [01:07, 3451.30it/s]158331it [01:07, 3163.03it/s]160500it [01:11, 3079.52it/s]177067it [01:07, 3499.38it/s]169676it [01:07, 3331.23it/s]158653it [01:07, 2987.28it/s]160844it [01:11, 3179.53it/s]177419it [01:08, 3226.89it/s]170040it [01:07, 3417.61it/s]159005it [01:08, 3132.91it/s]161165it [01:11, 3034.30it/s]177784it [01:08, 3343.32it/s]170387it [01:07, 3331.37it/s]159359it [01:08, 3247.52it/s]161513it [01:11, 3158.27it/s]178123it [01:08, 3294.12it/s]170753it [01:08, 3423.87it/s]159688it [01:08, 3086.66it/s]161855it [01:11, 3231.53it/s]178492it [01:08, 3404.93it/s]171118it [01:08, 3488.07it/s]160036it [01:08, 3195.23it/s]162181it [01:12, 3078.21it/s]178836it [01:08, 3339.26it/s]171469it [01:08, 3332.11it/s]160360it [01:08, 3054.17it/s]162530it [01:12, 3193.76it/s]179202it [01:08, 3430.79it/s]171836it [01:08, 3426.66it/s]160707it [01:08, 3170.13it/s]162853it [01:12, 3045.55it/s]179566it [01:08, 3490.71it/s]172181it [01:08, 3325.12it/s]161056it [01:08, 3261.43it/s]163207it [01:12, 3182.29it/s]179917it [01:08, 3373.66it/s]172551it [01:08, 3431.94it/s]161385it [01:08, 3062.17it/s]163558it [01:12, 3276.02it/s]180297it [01:08, 3495.78it/s]172907it [01:08, 3341.87it/s]161736it [01:08, 3186.38it/s]163889it [01:12, 3067.76it/s]180649it [01:08, 3380.61it/s]173276it [01:08, 3438.47it/s]162059it [01:09, 3050.18it/s]164242it [01:12, 3194.47it/s]181023it [01:09, 3482.79it/s]173643it [01:08, 3505.37it/s]162410it [01:09, 3177.44it/s]164566it [01:12, 3049.98it/s]181373it [01:09, 3366.11it/s]173995it [01:09, 3366.63it/s]162765it [01:09, 3282.00it/s]164925it [01:12, 3198.79it/s]181729it [01:09, 3418.92it/s]174361it [01:09, 3449.65it/s]163097it [01:09, 3129.96it/s]165277it [01:13, 3285.36it/s]182104it [01:09, 3512.65it/s]174708it [01:09, 3331.37it/s]163449it [01:09, 3239.85it/s]182457it [01:09, 3407.96it/s]165609it [01:13, 3141.06it/s]175076it [01:09, 3430.71it/s]182834it [01:09, 3510.53it/s]163776it [01:09, 3040.57it/s]165968it [01:13, 3265.54it/s]175428it [01:09, 3315.96it/s]164130it [01:09, 3178.68it/s]183187it [01:09, 3396.04it/s]166298it [01:13, 3123.11it/s]175798it [01:09, 3425.02it/s]164483it [01:09, 3276.31it/s]183554it [01:09, 3474.22it/s]166653it [01:13, 3241.72it/s]176155it [01:09, 3466.57it/s]164814it [01:09, 3123.30it/s]183903it [01:09, 3375.23it/s]166980it [01:13, 3144.19it/s]176504it [01:09, 3346.59it/s]165172it [01:09, 3251.22it/s]184268it [01:10, 3452.76it/s]167297it [01:13, 3038.78it/s]176876it [01:09, 3452.58it/s]184641it [01:10, 3531.69it/s]165501it [01:10, 3106.62it/s]167652it [01:13, 3181.65it/s]177223it [01:09, 3333.21it/s]184996it [01:10, 3427.45it/s]165858it [01:10, 3234.17it/s]167973it [01:13, 3047.33it/s]177587it [01:10, 3420.12it/s]185359it [01:10, 3484.95it/s]166187it [01:10, 3040.72it/s]168329it [01:14, 3188.93it/s]177946it [01:10, 3468.78it/s]185709it [01:10, 3377.28it/s]166545it [01:10, 3187.73it/s]168683it [01:14, 3289.27it/s]178295it [01:10, 3362.68it/s]186082it [01:10, 3477.46it/s]166907it [01:10, 3308.24it/s]169015it [01:14, 3127.32it/s]178660it [01:10, 3444.08it/s]186432it [01:10, 3346.33it/s]169364it [01:14, 3227.48it/s]167242it [01:10, 3140.14it/s]179006it [01:10, 3339.71it/s]186824it [01:10, 3507.54it/s]167597it [01:10, 3254.17it/s]169690it [01:14, 3072.56it/s]179369it [01:10, 3422.31it/s]187187it [01:10, 3390.30it/s]167926it [01:10, 3107.75it/s]170040it [01:14, 3191.07it/s]179713it [01:10, 3329.96it/s]187559it [01:10, 3482.52it/s]168286it [01:10, 3245.33it/s]180074it [01:10, 3408.53it/s]170387it [01:14, 3076.41it/s]187937it [01:11, 3566.51it/s]168621it [01:11, 3273.90it/s]180453it [01:10, 3517.77it/s]170733it [01:14, 3182.21it/s]188296it [01:11, 3451.83it/s]168951it [01:11, 3120.77it/s]171087it [01:14, 3273.58it/s]180806it [01:11, 3392.80it/s]188668it [01:11, 3527.21it/s]169305it [01:11, 3237.75it/s]181167it [01:11, 3450.36it/s]171417it [01:14, 3123.51it/s]189023it [01:11, 3410.66it/s]169632it [01:11, 3085.15it/s]171764it [01:15, 3220.65it/s]181514it [01:11, 3326.66it/s]189381it [01:11, 3457.89it/s]169982it [01:11, 3201.29it/s]181879it [01:11, 3417.29it/s]172089it [01:15, 3082.73it/s]189729it [01:11, 3363.50it/s]170342it [01:11, 3313.11it/s]182223it [01:11, 3346.73it/s]172446it [01:15, 3218.49it/s]190104it [01:11, 3474.26it/s]182596it [01:11, 3455.58it/s]170676it [01:11, 3145.25it/s]172800it [01:15, 3309.77it/s]190474it [01:11, 3537.65it/s]182966it [01:11, 3526.00it/s]171009it [01:11, 3196.08it/s]173134it [01:15, 3127.03it/s]190829it [01:11, 3416.75it/s]183320it [01:11, 3411.69it/s]171332it [01:11, 3057.96it/s]173484it [01:15, 3231.74it/s]191199it [01:12, 3496.06it/s]183688it [01:11, 3482.46it/s]171682it [01:12, 3181.27it/s]173811it [01:15, 3088.17it/s]191550it [01:12, 3396.74it/s]172040it [01:12, 3293.05it/s]184038it [01:11, 3373.07it/s]174167it [01:15, 3211.78it/s]191931it [01:12, 3513.13it/s]184414it [01:12, 3482.12it/s]172372it [01:12, 3123.49it/s]174525it [01:15, 3315.97it/s]192284it [01:12, 3363.77it/s]184764it [01:12, 3335.93it/s]172727it [01:12, 3242.63it/s]174860it [01:16, 3125.42it/s]192661it [01:12, 3477.33it/s]185135it [01:12, 3441.85it/s]173055it [01:12, 3103.87it/s]175216it [01:16, 3244.65it/s]193037it [01:12, 3558.20it/s]185495it [01:12, 3485.44it/s]173409it [01:12, 3225.59it/s]175544it [01:16, 3045.55it/s]193395it [01:12, 3439.24it/s]185846it [01:12, 3380.90it/s]173748it [01:12, 3046.33it/s]175896it [01:16, 3175.28it/s]193767it [01:12, 3514.18it/s]186220it [01:12, 3483.32it/s]174105it [01:12, 3188.55it/s]176245it [01:16, 3264.19it/s]194121it [01:12, 3424.28it/s]186570it [01:12, 3373.03it/s]174467it [01:12, 3310.39it/s]176575it [01:16, 3099.83it/s]194498it [01:12, 3521.33it/s]186959it [01:12, 3520.79it/s]174802it [01:13, 3140.53it/s]176933it [01:16, 3231.98it/s]194852it [01:13, 3413.99it/s]187313it [01:12, 3398.63it/s]175156it [01:13, 3251.54it/s]177260it [01:16, 3077.72it/s]195226it [01:13, 3505.48it/s]187686it [01:13, 3491.54it/s]175485it [01:13, 3105.26it/s]177606it [01:16, 3182.91it/s]195587it [01:13, 3405.19it/s]188037it [01:13, 3391.98it/s]175841it [01:13, 3231.10it/s]195950it [01:13, 3468.85it/s]177947it [01:17, 3066.70it/s]188392it [01:13, 3436.11it/s]176168it [01:13, 3236.62it/s]196322it [01:13, 3539.48it/s]178292it [01:17, 3173.11it/s]188764it [01:13, 3518.41it/s]176494it [01:13, 3087.40it/s]178647it [01:17, 3273.39it/s]189118it [01:13, 3381.32it/s]176852it [01:13, 3225.28it/s]178977it [01:17, 3119.77it/s]189487it [01:13, 3468.40it/s]177178it [01:13, 3064.56it/s]179325it [01:17, 3221.08it/s]189836it [01:13, 3360.17it/s]177530it [01:13, 3189.76it/s]179650it [01:17, 3082.12it/s]190211it [01:13, 3471.74it/s]177894it [01:13, 3316.28it/s]179996it [01:17, 3188.19it/s]190560it [01:13, 3350.71it/s]178229it [01:14, 3157.21it/s]180364it [01:17, 3327.97it/s]190928it [01:13, 3443.57it/s]178553it [01:14, 3180.29it/s]180700it [01:17, 3134.40it/s]191298it [01:14, 3517.56it/s]178874it [01:14, 3058.27it/s]181041it [01:18, 3210.00it/s]191652it [01:14, 3394.81it/s]179226it [01:14, 3188.21it/s]181366it [01:18, 3066.08it/s]192027it [01:14, 3460.77it/s]179586it [01:14, 3304.02it/s]181721it [01:18, 3199.09it/s]192375it [01:14, 3356.21it/s]179919it [01:14, 3124.24it/s]182075it [01:18, 3293.68it/s]192757it [01:14, 3487.92it/s]180284it [01:14, 3270.51it/s]182407it [01:18, 3132.85it/s]193108it [01:14, 3378.62it/s]180615it [01:14, 3098.03it/s]182771it [01:18, 3273.36it/s]193477it [01:14, 3466.71it/s]180953it [01:14, 3176.12it/s]193849it [01:14, 3539.72it/s]183102it [01:18, 3110.77it/s]181304it [01:15, 3270.86it/s]183458it [01:18, 3234.02it/s]194205it [01:14, 3430.89it/s]181634it [01:15, 3114.49it/s]183814it [01:18, 3325.06it/s]194580it [01:15, 3521.16it/s]181992it [01:15, 3244.21it/s]194934it [01:15, 3397.57it/s]184150it [01:18, 3118.05it/s]182320it [01:15, 3104.81it/s]195319it [01:15, 3524.44it/s]184507it [01:19, 3241.80it/s]182683it [01:15, 3251.35it/s]195674it [01:15, 3406.44it/s]184835it [01:19, 3109.66it/s]183012it [01:15, 3093.69it/s]196020it [01:15, 3419.52it/s]185190it [01:19, 3230.93it/s]183374it [01:15, 3240.42it/s]196390it [01:15, 3500.33it/s]185517it [01:19, 3052.38it/s]183705it [01:15, 3258.03it/s]185878it [01:19, 3206.75it/s]184034it [01:15, 3096.09it/s]186245it [01:19, 3335.90it/s]184394it [01:16, 3236.01it/s]186582it [01:19, 3133.84it/s]184721it [01:16, 3104.59it/s]186958it [01:19, 3306.32it/s]185082it [01:16, 3245.80it/s]187294it [01:19, 3147.75it/s]185430it [01:16, 3312.19it/s]187653it [01:20, 3270.13it/s]185764it [01:16, 3149.74it/s]188013it [01:20, 3362.09it/s]186111it [01:16, 3239.40it/s]188353it [01:20, 3185.20it/s]186438it [01:16, 3093.54it/s]188714it [01:20, 3302.38it/s]186818it [01:16, 3291.05it/s]189048it [01:20, 3124.81it/s]187173it [01:16, 3364.71it/s]189405it [01:20, 3247.58it/s]187513it [01:16, 3189.35it/s]189734it [01:20, 3045.27it/s]187879it [01:17, 3321.61it/s]190096it [01:20, 3201.36it/s]188215it [01:17, 3159.70it/s]190451it [01:20, 3297.11it/s]188556it [01:17, 3229.41it/s]190785it [01:21, 3137.71it/s]188882it [01:17, 3084.04it/s]191137it [01:21, 3244.23it/s]189240it [01:17, 3222.38it/s]191465it [01:21, 3092.64it/s]189594it [01:17, 3313.00it/s]191825it [01:21, 3233.34it/s]189928it [01:17, 3153.48it/s]192187it [01:21, 3341.43it/s]190293it [01:17, 3292.13it/s]192525it [01:21, 3144.87it/s]190626it [01:17, 3129.80it/s]192893it [01:21, 3292.41it/s]190965it [01:18, 3201.68it/s]193227it [01:21, 3152.57it/s]191319it [01:18, 3297.90it/s]193572it [01:21, 3234.08it/s]191652it [01:18, 3150.44it/s]193907it [01:22, 3114.46it/s]192018it [01:18, 3289.91it/s]194271it [01:22, 3260.64it/s]192350it [01:18, 3146.35it/s]194632it [01:22, 3358.61it/s]196678it [01:18, 227.09it/s] 192716it [01:18, 3289.04it/s]197054it [01:18, 319.21it/s]194971it [01:22, 3175.21it/s]193068it [01:18, 3143.96it/s]197370it [01:18, 421.20it/s]195329it [01:22, 3286.18it/s]193423it [01:18, 3255.07it/s]197739it [01:18, 580.67it/s]195661it [01:22, 3137.86it/s]193768it [01:18, 3305.94it/s]198122it [01:19, 792.30it/s]196011it [01:22, 3236.69it/s]194102it [01:19, 3163.67it/s]198460it [01:19, 1006.57it/s]196366it [01:22, 3324.24it/s]194464it [01:19, 3290.50it/s]198822it [01:19, 1288.36it/s]194796it [01:19, 3134.42it/s]199163it [01:19, 1556.12it/s]195172it [01:19, 3308.60it/s]199544it [01:19, 1913.75it/s]195530it [01:19, 3384.04it/s]199892it [01:19, 2156.44it/s]200260it [01:19, 2468.22it/s]195872it [01:19, 3203.24it/s]200628it [01:19, 2741.82it/s]196207it [01:19, 3242.56it/s]200981it [01:19, 2850.47it/s]201342it [01:19, 3040.84it/s]201689it [01:20, 3090.85it/s]202072it [01:20, 3289.34it/s]202425it [01:20, 3298.35it/s]202810it [01:20, 3451.08it/s]203193it [01:20, 3557.64it/s]203558it [01:20, 3424.10it/s]203928it [01:20, 3500.68it/s]204284it [01:20, 3409.36it/s]204660it [01:20, 3508.54it/s]205015it [01:20, 3426.59it/s]205393it [01:21, 3526.10it/s]205748it [01:21, 3431.93it/s]206119it [01:21, 3511.32it/s]196742it [01:21, 205.54it/s] 206499it [01:21, 3594.53it/s]197123it [01:21, 291.75it/s]206860it [01:21, 3468.34it/s]197441it [01:21, 387.17it/s]207215it [01:21, 3491.03it/s]197823it [01:21, 541.89it/s]207566it [01:21, 3395.44it/s]198176it [01:21, 718.19it/s]207949it [01:21, 3520.00it/s]198557it [01:21, 962.64it/s]208303it [01:21, 3455.89it/s]198934it [01:21, 1247.32it/s]208666it [01:22, 3504.23it/s]199285it [01:21, 1515.02it/s]209046it [01:22, 3590.27it/s]199667it [01:21, 1864.37it/s]209406it [01:22, 3471.56it/s]200021it [01:22, 2092.12it/s]209775it [01:22, 3532.33it/s]200392it [01:22, 2411.08it/s]210130it [01:22, 3414.29it/s]200739it [01:22, 2582.48it/s]210502it [01:22, 3499.94it/s]201112it [01:22, 2851.14it/s]210854it [01:22, 3392.44it/s]201482it [01:22, 3063.13it/s]211214it [01:22, 3451.73it/s]201837it [01:22, 3101.28it/s]211587it [01:22, 3530.10it/s]202217it [01:22, 3286.51it/s]211942it [01:22, 3423.11it/s]202572it [01:22, 3286.43it/s]212317it [01:23, 3507.57it/s]202957it [01:22, 3442.20it/s]212669it [01:23, 3407.99it/s]203315it [01:23, 3364.30it/s]213044it [01:23, 3504.19it/s]203671it [01:23, 3417.19it/s]213396it [01:23, 3400.49it/s]204038it [01:23, 3488.74it/s]213749it [01:23, 3436.88it/s]204393it [01:23, 3380.00it/s]214123it [01:23, 3522.44it/s]204766it [01:23, 3478.09it/s]214477it [01:23, 3408.26it/s]205118it [01:23, 3378.30it/s]214843it [01:23, 3480.22it/s]205495it [01:23, 3489.88it/s]215193it [01:23, 3388.48it/s]205847it [01:23, 3390.61it/s]215570it [01:24, 3496.55it/s]206226it [01:23, 3504.71it/s]215921it [01:24, 3368.52it/s]206579it [01:23, 3415.48it/s]216301it [01:24, 3489.97it/s]206955it [01:24, 3514.28it/s]216657it [01:24, 3405.53it/s]207319it [01:24, 3548.47it/s]217046it [01:24, 3542.74it/s]207676it [01:24, 3432.42it/s]217419it [01:24, 3596.03it/s]208056it [01:24, 3536.98it/s]217780it [01:24, 3514.43it/s]208412it [01:24, 3457.18it/s]218167it [01:24, 3616.98it/s]208791it [01:24, 3551.21it/s]218530it [01:24, 3468.07it/s]209148it [01:24, 3447.75it/s]218915it [01:24, 3575.28it/s]209517it [01:24, 3515.26it/s]219275it [01:25, 3503.19it/s]209889it [01:24, 3573.45it/s]219655it [01:25, 3586.41it/s]210248it [01:25, 3425.02it/s]220017it [01:25, 3510.38it/s]210623it [01:25, 3516.18it/s]220414it [01:25, 3641.38it/s]210977it [01:25, 3394.46it/s]220796it [01:25, 3691.57it/s]211334it [01:25, 3444.12it/s]221167it [01:25, 3535.98it/s]211680it [01:25, 3346.11it/s]221557it [01:25, 3633.39it/s]212054it [01:25, 3456.33it/s]221923it [01:25, 3540.30it/s]212425it [01:25, 3529.12it/s]222295it [01:25, 3591.20it/s]212780it [01:25, 3419.69it/s]222656it [01:26, 3477.04it/s]213153it [01:25, 3508.23it/s]223027it [01:26, 3543.63it/s]213506it [01:25, 3393.03it/s]223383it [01:26, 3431.43it/s]213879it [01:26, 3487.30it/s]223745it [01:26, 3483.32it/s]214230it [01:26, 3374.88it/s]196701it [01:30, 152.42it/s] 224125it [01:26, 3573.36it/s]214598it [01:26, 3461.75it/s]197059it [01:30, 216.09it/s]224484it [01:26, 3437.11it/s]214946it [01:26, 3460.30it/s]197336it [01:30, 281.66it/s]224850it [01:26, 3500.54it/s]215294it [01:26, 3369.63it/s]197693it [01:30, 398.57it/s]225202it [01:26, 3400.48it/s]215663it [01:26, 3461.31it/s]198068it [01:30, 560.55it/s]225570it [01:26, 3478.75it/s]216011it [01:26, 3349.23it/s]198387it [01:30, 724.56it/s]225920it [01:26, 3363.50it/s]216387it [01:26, 3466.54it/s]198748it [01:30, 966.53it/s]226275it [01:27, 3416.14it/s]216736it [01:26, 3365.52it/s]199073it [01:30, 1192.74it/s]226641it [01:27, 3486.61it/s]217118it [01:26, 3494.74it/s]196534it [01:27, 146.69it/s] 199431it [01:30, 1504.74it/s]226991it [01:27, 3346.00it/s]217489it [01:27, 3554.80it/s]196907it [01:27, 211.62it/s]199786it [01:31, 1825.40it/s]227353it [01:27, 3424.27it/s]197267it [01:27, 297.07it/s]217846it [01:27, 3463.28it/s]200121it [01:31, 2034.75it/s]227698it [01:27, 3321.72it/s]218227it [01:27, 3563.41it/s]197566it [01:27, 389.24it/s]200448it [01:31, 2283.78it/s]228062it [01:27, 3410.50it/s]197934it [01:27, 544.42it/s]218585it [01:27, 3444.00it/s]200769it [01:31, 2403.31it/s]228417it [01:27, 3291.09it/s]218950it [01:27, 3502.06it/s]198247it [01:27, 702.55it/s]201122it [01:31, 2668.69it/s]228782it [01:27, 3391.82it/s]198605it [01:27, 938.90it/s]219302it [01:27, 3432.10it/s]201473it [01:31, 2880.69it/s]229147it [01:27, 3464.46it/s]198961it [01:27, 1214.05it/s]219677it [01:27, 3522.08it/s]201803it [01:31, 2874.56it/s]229496it [01:28, 3378.75it/s]220031it [01:27, 3444.37it/s]199292it [01:28, 1459.60it/s]202163it [01:31, 3066.85it/s]229864it [01:28, 3464.86it/s]220424it [01:27, 3582.81it/s]199658it [01:28, 1799.75it/s]202493it [01:31, 3020.41it/s]230212it [01:28, 3349.10it/s]220802it [01:28, 3637.59it/s]199991it [01:28, 1988.80it/s]202858it [01:31, 3193.65it/s]230574it [01:28, 3425.53it/s]221167it [01:28, 3512.88it/s]200345it [01:28, 2295.99it/s]203192it [01:32, 3234.51it/s]230933it [01:28, 3472.19it/s]221551it [01:28, 3605.93it/s]200698it [01:28, 2434.74it/s]203525it [01:32, 3100.77it/s]231282it [01:28, 3382.52it/s]221913it [01:28, 3489.41it/s]201054it [01:28, 2692.92it/s]203879it [01:32, 3222.80it/s]231644it [01:28, 3449.88it/s]222279it [01:28, 3537.28it/s]201405it [01:28, 2893.71it/s]231991it [01:28, 3343.62it/s]204208it [01:32, 3082.43it/s]222634it [01:28, 3375.30it/s]201738it [01:28, 2888.51it/s]232356it [01:28, 3429.39it/s]204565it [01:32, 3216.90it/s]223002it [01:28, 3459.28it/s]202099it [01:28, 3078.41it/s]232701it [01:28, 3322.18it/s]204896it [01:32, 3080.28it/s]223365it [01:28, 3508.36it/s]202431it [01:28, 3003.10it/s]233071it [01:29, 3429.83it/s]205261it [01:32, 3237.35it/s]223718it [01:28, 3386.05it/s]202798it [01:29, 3175.44it/s]233432it [01:29, 3481.44it/s]205617it [01:32, 3326.79it/s]224086it [01:28, 3469.76it/s]203160it [01:29, 3299.45it/s]233782it [01:29, 3377.61it/s]205953it [01:32, 3122.10it/s]224435it [01:29, 3365.63it/s]203501it [01:29, 3156.64it/s]234149it [01:29, 3460.92it/s]206312it [01:33, 3252.09it/s]224801it [01:29, 3447.73it/s]203855it [01:29, 3262.24it/s]234497it [01:29, 3352.62it/s]206642it [01:33, 3117.55it/s]225148it [01:29, 3341.65it/s]204188it [01:29, 3131.82it/s]234860it [01:29, 3430.50it/s]206997it [01:33, 3237.12it/s]225515it [01:29, 3435.24it/s]204544it [01:29, 3249.91it/s]235205it [01:29, 3337.44it/s]207354it [01:33, 3331.78it/s]225875it [01:29, 3480.31it/s]204898it [01:29, 3113.58it/s]235563it [01:29, 3405.66it/s]207691it [01:33, 3181.49it/s]226225it [01:29, 3304.17it/s]205247it [01:29, 3216.83it/s]235919it [01:29, 3450.45it/s]208051it [01:33, 3297.68it/s]226588it [01:29, 3394.22it/s]205599it [01:29, 3300.04it/s]236265it [01:30, 3342.96it/s]208384it [01:33, 3179.69it/s]226930it [01:29, 3274.47it/s]205933it [01:30, 3169.96it/s]236626it [01:30, 3419.37it/s]208728it [01:33, 3250.76it/s]227290it [01:29, 3366.19it/s]206293it [01:30, 3289.48it/s]236970it [01:30, 3332.28it/s]209084it [01:33, 3338.33it/s]227629it [01:30, 3268.65it/s]237333it [01:30, 3415.95it/s]206625it [01:30, 3145.47it/s]227993it [01:30, 3373.34it/s]209420it [01:34, 3176.45it/s]206984it [01:30, 3268.21it/s]237676it [01:30, 3310.14it/s]228355it [01:30, 3443.10it/s]209767it [01:34, 3259.09it/s]207344it [01:30, 3360.62it/s]238040it [01:30, 3403.82it/s]228701it [01:30, 3327.95it/s]210096it [01:34, 3100.50it/s]238391it [01:30, 3433.81it/s]207683it [01:30, 3151.40it/s]229064it [01:30, 3412.40it/s]210447it [01:34, 3214.19it/s]238736it [01:30, 3326.88it/s]208044it [01:30, 3277.55it/s]229407it [01:30, 3323.85it/s]210776it [01:34, 3060.98it/s]239097it [01:30, 3407.88it/s]208376it [01:30, 3159.69it/s]229777it [01:30, 3431.24it/s]211129it [01:34, 3190.31it/s]239439it [01:30, 3302.35it/s]208735it [01:30, 3278.88it/s]230122it [01:30, 3276.16it/s]211469it [01:34, 3248.23it/s]239807it [01:31, 3409.28it/s]209098it [01:31, 3156.89it/s]230486it [01:30, 3377.88it/s]211797it [01:34, 3104.27it/s]240169it [01:31, 3469.37it/s]209453it [01:31, 3264.44it/s]230852it [01:30, 3458.75it/s]212148it [01:34, 3216.91it/s]240518it [01:31, 3323.77it/s]209803it [01:31, 3330.46it/s]231200it [01:31, 3356.31it/s]212473it [01:35, 3066.85it/s]240880it [01:31, 3407.53it/s]210139it [01:31, 3123.51it/s]231567it [01:31, 3440.15it/s]212827it [01:35, 3198.90it/s]241223it [01:31, 3325.28it/s]210494it [01:31, 3239.81it/s]231913it [01:31, 3334.19it/s]213182it [01:35, 3297.52it/s]241587it [01:31, 3415.04it/s]232275it [01:31, 3414.70it/s]210822it [01:31, 3089.45it/s]213515it [01:35, 3129.11it/s]241930it [01:31, 3337.68it/s]211176it [01:31, 3214.09it/s]232618it [01:31, 3305.61it/s]213870it [01:35, 3247.15it/s]242295it [01:31, 3426.21it/s]211530it [01:31, 3306.22it/s]232984it [01:31, 3406.97it/s]214198it [01:35, 3054.11it/s]242660it [01:31, 3489.55it/s]233358it [01:31, 3502.97it/s]211864it [01:31, 3158.08it/s]214551it [01:35, 3185.36it/s]243010it [01:32, 3341.29it/s]212215it [01:32, 3256.43it/s]233710it [01:31, 3335.16it/s]214899it [01:35, 3268.98it/s]243380it [01:32, 3442.34it/s]234077it [01:31, 3428.30it/s]212544it [01:32, 3055.19it/s]215230it [01:35, 3122.96it/s]243726it [01:32, 3347.61it/s]234422it [01:32, 3321.77it/s]212903it [01:32, 3201.42it/s]215585it [01:35, 3242.71it/s]244092it [01:32, 3434.78it/s]234785it [01:32, 3408.96it/s]213258it [01:32, 3299.15it/s]215913it [01:36, 3092.33it/s]244437it [01:32, 3324.52it/s]235137it [01:32, 3303.04it/s]213592it [01:32, 3131.32it/s]216271it [01:36, 3226.59it/s]244799it [01:32, 3409.08it/s]235499it [01:32, 3391.36it/s]213948it [01:32, 3249.60it/s]216627it [01:36, 3319.86it/s]245163it [01:32, 3474.69it/s]235862it [01:32, 3458.21it/s]214277it [01:32, 3116.88it/s]216962it [01:36, 3179.82it/s]245512it [01:32, 3336.22it/s]236210it [01:32, 3335.28it/s]214628it [01:32, 3224.28it/s]217301it [01:36, 3239.11it/s]245875it [01:32, 3418.05it/s]236568it [01:32, 3403.56it/s]214978it [01:32, 3030.91it/s]217628it [01:36, 3133.03it/s]246219it [01:32, 3258.60it/s]236910it [01:32, 3306.85it/s]215341it [01:33, 3194.23it/s]217994it [01:36, 3282.26it/s]246584it [01:33, 3366.89it/s]237270it [01:32, 3389.72it/s]215699it [01:33, 3301.11it/s]246923it [01:33, 3282.70it/s]218336it [01:36, 3150.19it/s]237615it [01:33, 3405.23it/s]216034it [01:33, 3150.35it/s]247291it [01:33, 3395.79it/s]218711it [01:36, 3316.72it/s]237957it [01:33, 3294.38it/s]216400it [01:33, 3292.64it/s]247656it [01:33, 3468.27it/s]219075it [01:37, 3409.01it/s]238316it [01:33, 3379.47it/s]216734it [01:33, 3153.26it/s]248005it [01:33, 3330.86it/s]219419it [01:37, 3255.31it/s]238656it [01:33, 3272.71it/s]217103it [01:33, 3302.46it/s]248367it [01:33, 3406.39it/s]219786it [01:37, 3370.30it/s]239014it [01:33, 3358.44it/s]217456it [01:33, 3366.39it/s]248710it [01:33, 3317.56it/s]220126it [01:37, 3177.79it/s]239352it [01:33, 3257.91it/s]249074it [01:33, 3409.90it/s]217796it [01:33, 3179.04it/s]220505it [01:37, 3346.13it/s]239716it [01:33, 3365.57it/s]218169it [01:33, 3331.93it/s]249417it [01:33, 3317.76it/s]220857it [01:37, 3205.25it/s]240078it [01:33, 3437.30it/s]249780it [01:34, 3406.37it/s]218506it [01:33, 3184.25it/s]221221it [01:37, 3325.73it/s]240423it [01:33, 3302.85it/s]250145it [01:34, 3476.74it/s]218882it [01:34, 3344.16it/s]221604it [01:37, 3466.70it/s]240784it [01:33, 3390.68it/s]250494it [01:34, 3349.72it/s]219220it [01:34, 3206.75it/s]221954it [01:37, 3290.36it/s]241125it [01:34, 3261.79it/s]250863it [01:34, 3446.76it/s]219591it [01:34, 3345.94it/s]222309it [01:38, 3360.49it/s]241489it [01:34, 3369.34it/s]251210it [01:34, 3345.22it/s]219958it [01:34, 3436.91it/s]222649it [01:38, 3186.34it/s]241857it [01:34, 3293.56it/s]251575it [01:34, 3430.08it/s]220305it [01:34, 3225.76it/s]222988it [01:38, 3242.13it/s]242220it [01:34, 3386.50it/s]251936it [01:34, 3481.67it/s]220682it [01:34, 3376.86it/s]223341it [01:38, 3323.68it/s]242583it [01:34, 3455.32it/s]252286it [01:34, 3381.13it/s]221024it [01:34, 3225.83it/s]223676it [01:38, 3155.43it/s]242931it [01:34, 3336.02it/s]252635it [01:34, 3411.54it/s]221393it [01:34, 3354.74it/s]224033it [01:38, 3271.06it/s]243297it [01:34, 3426.13it/s]252978it [01:34, 3319.91it/s]221733it [01:34, 3240.05it/s]224364it [01:38, 3119.59it/s]243642it [01:34, 3328.63it/s]253342it [01:35, 3411.85it/s]222094it [01:35, 3342.64it/s]224715it [01:38, 3228.69it/s]244005it [01:34, 3414.35it/s]253685it [01:35, 3321.92it/s]222445it [01:35, 3390.01it/s]244368it [01:35, 3474.86it/s]225057it [01:38, 3077.93it/s]222787it [01:35, 3137.81it/s]225407it [01:38, 3192.25it/s]244717it [01:35, 3301.86it/s]223144it [01:35, 3256.05it/s]225745it [01:39, 3243.80it/s]245077it [01:35, 3383.49it/s]223474it [01:35, 3095.75it/s]245418it [01:35, 3279.85it/s]226073it [01:39, 3069.24it/s]223831it [01:35, 3226.38it/s]245780it [01:35, 3376.08it/s]226421it [01:39, 3183.42it/s]224196it [01:35, 3344.84it/s]246120it [01:35, 3285.08it/s]226743it [01:39, 3025.91it/s]224534it [01:35, 3164.27it/s]246483it [01:35, 3382.26it/s]227087it [01:39, 3131.25it/s]224887it [01:35, 3264.92it/s]246850it [01:35, 3463.90it/s]227433it [01:39, 3222.56it/s]247198it [01:35, 3346.21it/s]225217it [01:36, 3114.35it/s]227758it [01:39, 3068.53it/s]247562it [01:35, 3430.22it/s]225547it [01:36, 3165.79it/s]228103it [01:39, 3175.24it/s]225895it [01:36, 3253.77it/s]247907it [01:36, 3320.34it/s]228424it [01:39, 2985.67it/s]248272it [01:36, 3412.55it/s]226223it [01:36, 3088.69it/s]228777it [01:40, 3135.10it/s]226574it [01:36, 3206.65it/s]248615it [01:36, 3262.25it/s]229123it [01:40, 3225.60it/s]248985it [01:36, 3384.09it/s]226898it [01:36, 3032.55it/s]229449it [01:40, 3100.32it/s]249348it [01:36, 3453.06it/s]227248it [01:36, 3160.83it/s]229802it [01:40, 3219.93it/s]249696it [01:36, 3341.12it/s]227578it [01:36, 3017.21it/s]230127it [01:40, 3054.11it/s]250062it [01:36, 3431.96it/s]227931it [01:36, 3158.66it/s]230478it [01:40, 3180.22it/s]250407it [01:36, 3330.80it/s]228257it [01:36, 3186.10it/s]230827it [01:40, 3268.12it/s]250778it [01:36, 3438.28it/s]228579it [01:37, 3042.02it/s]231157it [01:40, 3078.75it/s]251124it [01:37, 3322.66it/s]228934it [01:37, 3183.23it/s]231507it [01:40, 3195.31it/s]251487it [01:37, 3409.50it/s]229258it [01:37, 3031.20it/s]231830it [01:41, 3053.16it/s]251852it [01:37, 3477.93it/s]229623it [01:37, 3203.97it/s]232180it [01:41, 3176.86it/s]252202it [01:37, 3319.07it/s]229973it [01:37, 3286.47it/s]232528it [01:41, 3261.31it/s]252562it [01:37, 3398.54it/s]230305it [01:37, 3118.12it/s]232857it [01:41, 3100.09it/s]252904it [01:37, 3293.23it/s]230626it [01:37, 3141.79it/s]233209it [01:41, 3217.88it/s]253267it [01:37, 3386.11it/s]230943it [01:37, 3010.26it/s]233534it [01:41, 3061.10it/s]253617it [01:37, 3294.69it/s]231305it [01:37, 3181.45it/s]233872it [01:41, 3149.61it/s]231661it [01:38, 3287.41it/s]234227it [01:41, 3258.79it/s]231993it [01:38, 3116.91it/s]234556it [01:41, 3096.03it/s]232345it [01:38, 3228.76it/s]234902it [01:42, 3197.98it/s]232671it [01:38, 3075.17it/s]235225it [01:42, 3042.37it/s]232997it [01:38, 3126.36it/s]235570it [01:42, 3156.23it/s]233357it [01:38, 3261.37it/s]235920it [01:42, 3253.58it/s]233686it [01:38, 3096.56it/s]236248it [01:42, 3094.56it/s]234038it [01:38, 3215.51it/s]236572it [01:42, 3134.32it/s]234363it [01:38, 3086.65it/s]236888it [01:42, 3013.45it/s]234716it [01:39, 3209.04it/s]237237it [01:42, 3145.88it/s]235072it [01:39, 3308.14it/s]237589it [01:42, 3251.12it/s]235406it [01:39, 3072.68it/s]237917it [01:42, 3085.76it/s]235759it [01:39, 3198.50it/s]238268it [01:43, 3203.09it/s]236084it [01:39, 3054.45it/s]238592it [01:43, 3043.89it/s]236438it [01:39, 3188.95it/s]238943it [01:43, 3172.76it/s]236795it [01:39, 3295.57it/s]239292it [01:43, 3262.22it/s]237128it [01:39, 3125.23it/s]239621it [01:43, 3046.34it/s]237485it [01:39, 3248.13it/s]239972it [01:43, 3173.48it/s]237814it [01:40, 3021.29it/s]240294it [01:43, 3033.09it/s]238168it [01:40, 3162.37it/s]240642it [01:43, 3155.65it/s]240997it [01:43, 3267.12it/s]238498it [01:40, 3021.09it/s]238854it [01:40, 3167.76it/s]241327it [01:44, 3106.99it/s]239207it [01:40, 3268.52it/s]241677it [01:44, 3216.10it/s]242002it [01:44, 3072.85it/s]239538it [01:40, 3101.46it/s]242339it [01:44, 3155.26it/s]239894it [01:40, 3228.50it/s]242691it [01:44, 3259.03it/s]240221it [01:40, 3069.33it/s]240565it [01:40, 3172.17it/s]243020it [01:44, 3083.84it/s]240921it [01:41, 3280.56it/s]243375it [01:44, 3213.50it/s]243700it [01:44, 3066.25it/s]241252it [01:41, 3119.71it/s]244051it [01:44, 3189.03it/s]241608it [01:41, 3241.58it/s]241936it [01:41, 3108.36it/s]244377it [01:45, 3039.01it/s]242294it [01:41, 3239.42it/s]244728it [01:45, 3169.75it/s]242641it [01:41, 3304.59it/s]245065it [01:45, 3226.09it/s]242974it [01:41, 3144.38it/s]245391it [01:45, 3065.66it/s]243330it [01:41, 3259.49it/s]245742it [01:45, 3189.11it/s]243659it [01:41, 3114.26it/s]246064it [01:45, 3041.07it/s]244014it [01:41, 3236.06it/s]246414it [01:45, 3167.88it/s]254019it [01:42, 162.26it/s] 244363it [01:42, 3307.94it/s]246763it [01:45, 3259.07it/s]254384it [01:42, 230.95it/s]254686it [01:42, 307.27it/s]244697it [01:42, 3109.32it/s]247092it [01:45, 3098.81it/s]255050it [01:42, 432.64it/s]245049it [01:42, 3222.28it/s]247441it [01:45, 3208.21it/s]255366it [01:42, 570.67it/s]245375it [01:42, 3047.41it/s]247765it [01:46, 3016.46it/s]255735it [01:42, 782.14it/s]245732it [01:42, 3191.87it/s]248115it [01:46, 3148.90it/s]256102it [01:42, 1036.92it/s]248467it [01:46, 3252.40it/s]246058it [01:42, 3051.22it/s]256440it [01:42, 1283.03it/s]246413it [01:42, 3188.58it/s]248796it [01:46, 3093.00it/s]256807it [01:42, 1608.65it/s]246758it [01:42, 3261.75it/s]249148it [01:46, 3211.35it/s]257147it [01:42, 1874.44it/s]247088it [01:42, 3112.10it/s]249473it [01:46, 3068.14it/s]257510it [01:43, 2201.47it/s]247443it [01:43, 3234.57it/s]249823it [01:46, 3187.86it/s]257877it [01:43, 2509.50it/s]250177it [01:46, 3287.26it/s]247770it [01:43, 3083.93it/s]258226it [01:43, 2666.14it/s]248124it [01:43, 3209.71it/s]250509it [01:46, 3083.16it/s]258593it [01:43, 2909.69it/s]248476it [01:43, 3297.56it/s]250862it [01:47, 3205.97it/s]258940it [01:43, 2959.23it/s]248809it [01:43, 3105.94it/s]251187it [01:47, 3059.74it/s]259307it [01:43, 3145.63it/s]249166it [01:43, 3234.86it/s]251535it [01:47, 3176.12it/s]259652it [01:43, 3143.38it/s]251882it [01:47, 3257.56it/s]249493it [01:43, 3076.53it/s]260023it [01:43, 3298.26it/s]249843it [01:43, 3192.43it/s]252211it [01:47, 2993.02it/s]260390it [01:43, 3401.08it/s]250201it [01:43, 3302.32it/s]252522it [01:47, 3023.07it/s]260742it [01:44, 3320.71it/s]250535it [01:44, 3148.13it/s]252829it [01:47, 2925.88it/s]261108it [01:44, 3414.92it/s]250857it [01:44, 3168.30it/s]253168it [01:47, 3054.91it/s]261457it [01:44, 3294.58it/s]253523it [01:47, 3193.84it/s]251177it [01:44, 3037.28it/s]261828it [01:44, 3409.70it/s]251530it [01:44, 3175.11it/s]262174it [01:44, 3332.06it/s]251885it [01:44, 3280.91it/s]262543it [01:44, 3433.36it/s]252216it [01:44, 3139.65it/s]262914it [01:44, 3512.32it/s]252568it [01:44, 3238.17it/s]263268it [01:44, 3390.96it/s]252895it [01:44, 3067.80it/s]263621it [01:44, 3430.04it/s]253245it [01:44, 3188.04it/s]263966it [01:44, 3338.26it/s]253601it [01:44, 3291.76it/s]253949it [01:44, 158.67it/s] 264338it [01:45, 3447.63it/s]254313it [01:44, 225.47it/s]264685it [01:45, 3358.21it/s]254596it [01:45, 294.93it/s]265058it [01:45, 3464.20it/s]254963it [01:45, 418.80it/s]265433it [01:45, 3545.56it/s]255328it [01:45, 579.75it/s]265789it [01:45, 3439.79it/s]255650it [01:45, 753.27it/s]266148it [01:45, 3480.73it/s]256013it [01:45, 1002.06it/s]266498it [01:45, 3388.34it/s]256345it [01:45, 1248.56it/s]266874it [01:45, 3493.86it/s]256709it [01:45, 1571.10it/s]267225it [01:45, 3400.73it/s]257047it [01:45, 1836.61it/s]267600it [01:46, 3500.33it/s]257411it [01:45, 2170.58it/s]267966it [01:46, 3404.03it/s]257781it [01:45, 2490.36it/s]268343it [01:46, 3506.55it/s]258130it [01:46, 2652.04it/s]268705it [01:46, 3537.83it/s]258482it [01:46, 2861.98it/s]269060it [01:46, 3425.30it/s]258825it [01:46, 2940.74it/s]269433it [01:46, 3511.83it/s]259197it [01:46, 3146.27it/s]269786it [01:46, 3401.63it/s]259566it [01:46, 3142.09it/s]270166it [01:46, 3514.69it/s]259940it [01:46, 3303.09it/s]270519it [01:46, 3407.15it/s]260312it [01:46, 3419.64it/s]270891it [01:46, 3495.56it/s]260667it [01:46, 3329.73it/s]271247it [01:47, 3513.87it/s]261038it [01:46, 3435.03it/s]271600it [01:47, 3401.74it/s]261389it [01:47, 3349.74it/s]271971it [01:47, 3490.26it/s]261764it [01:47, 3463.28it/s]272322it [01:47, 3380.43it/s]262115it [01:47, 3299.77it/s]272692it [01:47, 3470.77it/s]262487it [01:47, 3415.53it/s]273041it [01:47, 3372.39it/s]262857it [01:47, 3496.25it/s]273410it [01:47, 3461.90it/s]263210it [01:47, 3381.75it/s]273769it [01:47, 3498.21it/s]263579it [01:47, 3468.16it/s]274120it [01:47, 3400.14it/s]263929it [01:47, 3367.09it/s]274487it [01:47, 3477.79it/s]264304it [01:47, 3475.95it/s]274836it [01:48, 3374.24it/s]264654it [01:47, 3365.76it/s]275209it [01:48, 3475.75it/s]265025it [01:48, 3463.57it/s]275558it [01:48, 3383.63it/s]265395it [01:48, 3532.00it/s]275915it [01:48, 3434.76it/s]265750it [01:48, 3366.50it/s]276286it [01:48, 3513.43it/s]266115it [01:48, 3446.65it/s]276639it [01:48, 3394.84it/s]266462it [01:48, 3344.96it/s]277007it [01:48, 3476.07it/s]266838it [01:48, 3461.73it/s]277356it [01:48, 3378.74it/s]267187it [01:48, 3358.95it/s]277726it [01:48, 3469.59it/s]267559it [01:48, 3460.49it/s]278075it [01:49, 3368.83it/s]267926it [01:48, 3504.98it/s]278430it [01:49, 3420.74it/s]268278it [01:49, 3396.14it/s]278800it [01:49, 3500.05it/s]268649it [01:49, 3484.28it/s]279152it [01:49, 3391.15it/s]268999it [01:49, 3369.14it/s]279521it [01:49, 3476.45it/s]269368it [01:49, 3459.68it/s]279870it [01:49, 3372.55it/s]269716it [01:49, 3363.22it/s]280238it [01:49, 3458.53it/s]270094it [01:49, 3482.00it/s]280586it [01:49, 3362.12it/s]270465it [01:49, 3546.49it/s]280944it [01:49, 3423.05it/s]270821it [01:49, 3416.89it/s]281313it [01:49, 3499.43it/s]271186it [01:49, 3475.53it/s]281664it [01:50, 3383.39it/s]271535it [01:49, 3346.83it/s]282033it [01:50, 3469.71it/s]271904it [01:50, 3444.22it/s]282382it [01:50, 3363.61it/s]272251it [01:50, 3336.62it/s]282746it [01:50, 3441.23it/s]272616it [01:50, 3425.83it/s]283092it [01:50, 3340.87it/s]272987it [01:50, 3506.70it/s]283444it [01:50, 3391.49it/s]283812it [01:50, 3473.75it/s]273340it [01:50, 3333.06it/s]273711it [01:50, 3437.68it/s]284161it [01:50, 3370.86it/s]284528it [01:50, 3456.11it/s]274058it [01:50, 3345.75it/s]274429it [01:50, 3448.61it/s]284875it [01:51, 3356.97it/s]285241it [01:51, 3442.74it/s]274776it [01:50, 3339.39it/s]275148it [01:51, 3447.40it/s]285606it [01:51, 3309.13it/s]275502it [01:51, 3473.56it/s]285976it [01:51, 3418.32it/s]275851it [01:51, 3360.59it/s]286345it [01:51, 3495.07it/s]276223it [01:51, 3462.76it/s]286697it [01:51, 3393.93it/s]276571it [01:51, 3346.20it/s]287072it [01:51, 3493.80it/s]287112it [01:51, 2570.71it/s]
276940it [01:51, 3442.41it/s]277286it [01:51, 3351.27it/s]277623it [01:51, 3307.37it/s]277993it [01:51, 3420.31it/s]278337it [01:51, 3317.84it/s]278711it [01:52, 3438.18it/s]279057it [01:52, 3347.91it/s]279430it [01:52, 3456.07it/s]279777it [01:52, 3352.74it/s]280146it [01:52, 3444.58it/s]280502it [01:52, 3477.12it/s]280851it [01:52, 3377.18it/s]281220it [01:52, 3466.40it/s]281568it [01:52, 3362.38it/s]281938it [01:53, 3458.23it/s]282286it [01:53, 3343.36it/s]282641it [01:53, 3400.46it/s]283010it [01:53, 3483.21it/s]283360it [01:53, 3362.76it/s]283698it [01:53, 3341.13it/s]284034it [01:53, 3182.87it/s]284381it [01:53, 3263.24it/s]284727it [01:53, 3318.24it/s]285061it [01:53, 3239.99it/s]285426it [01:54, 3356.23it/s]285763it [01:54, 3258.91it/s]253523it [01:58, 3193.84it/s]253687it [01:58, 90.84it/s]  286129it [01:54, 3372.38it/s]254036it [01:58, 136.71it/s]286468it [01:54, 3261.08it/s]254366it [01:58, 196.54it/s]286813it [01:54, 3314.99it/s]253933it [01:54, 114.62it/s] 287112it [01:54, 2505.92it/s]
254696it [01:58, 276.70it/s]254281it [01:54, 162.28it/s]255040it [01:58, 390.67it/s]254549it [01:54, 212.12it/s]255366it [01:58, 526.40it/s]254902it [01:54, 303.48it/s]255722it [01:58, 723.03it/s]255252it [01:55, 424.46it/s]256077it [01:58, 962.48it/s]255562it [01:55, 558.27it/s]256409it [01:59, 1193.48it/s]255900it [01:55, 749.83it/s]256762it [01:59, 1500.07it/s]256212it [01:55, 947.16it/s]256572it [01:55, 1241.22it/s]257090it [01:59, 1711.81it/s]256928it [01:55, 1557.84it/s]257436it [01:59, 2022.82it/s]257792it [01:59, 2334.54it/s]257257it [01:55, 1785.76it/s]257608it [01:55, 2103.17it/s]258122it [01:59, 2448.19it/s]258474it [01:59, 2699.27it/s]257933it [01:55, 2259.03it/s]258273it [01:56, 2513.30it/s]258800it [01:59, 2723.03it/s]258629it [01:56, 2766.34it/s]259157it [01:59, 2938.69it/s]259509it [01:59, 3094.27it/s]258959it [01:56, 2777.19it/s]259316it [01:56, 2981.18it/s]259842it [02:00, 2955.64it/s]260196it [02:00, 3109.53it/s]259644it [01:56, 2917.67it/s]260002it [01:56, 3094.58it/s]260521it [02:00, 3001.19it/s]260354it [01:56, 3210.99it/s]260876it [02:00, 3148.45it/s]261238it [02:00, 3280.84it/s]260688it [01:56, 3078.53it/s]261039it [01:56, 3197.19it/s]261573it [02:00, 3121.14it/s]261892it [02:00, 2954.00it/s]261367it [01:57, 2849.33it/s]261700it [01:57, 2975.48it/s]262193it [02:00, 2824.27it/s]262052it [01:57, 3122.86it/s]262515it [02:00, 2929.75it/s]262871it [02:01, 3103.75it/s]262373it [01:57, 3014.79it/s]262730it [01:57, 3168.82it/s]263186it [02:01, 2997.36it/s]263537it [02:01, 3140.19it/s]263053it [01:57, 3045.74it/s]263405it [01:57, 3177.37it/s]263855it [02:01, 3021.65it/s]263750it [01:57, 3254.99it/s]264211it [02:01, 3172.38it/s]264567it [02:01, 3283.18it/s]264079it [01:57, 3100.97it/s]264393it [01:58, 3042.73it/s]264898it [02:01, 2839.68it/s]264700it [01:58, 2801.07it/s]265219it [02:01, 2936.64it/s]265054it [01:58, 3000.84it/s]265523it [02:01, 2868.87it/s]265412it [01:58, 3161.63it/s]265877it [02:02, 3051.10it/s]265734it [01:58, 3043.90it/s]266232it [02:02, 3189.69it/s]266090it [01:58, 3188.30it/s]266557it [02:02, 3070.65it/s]266413it [01:58, 3043.73it/s]266913it [02:02, 3206.99it/s]266773it [01:58, 3197.52it/s]267238it [02:02, 3075.51it/s]267598it [02:02, 3220.95it/s]267126it [01:58, 3075.07it/s]267951it [02:02, 3259.39it/s]267488it [01:59, 3223.76it/s]267850it [01:59, 3335.19it/s]268280it [02:02, 3127.46it/s]268640it [02:02, 3259.35it/s]268187it [01:59, 3179.39it/s]268545it [01:59, 3290.81it/s]268969it [02:03, 3096.23it/s]269321it [02:03, 3213.65it/s]268878it [01:59, 3127.56it/s]269225it [01:59, 3220.74it/s]269646it [02:03, 3070.39it/s]269578it [01:59, 3307.63it/s]270006it [02:03, 3217.53it/s]270363it [02:03, 3316.56it/s]269912it [01:59, 3163.31it/s]270268it [01:59, 3274.52it/s]270698it [02:03, 3085.43it/s]270599it [01:59, 3115.30it/s]271054it [02:03, 3216.77it/s]270952it [02:00, 3230.43it/s]271381it [02:03, 3079.97it/s]271306it [02:00, 3317.47it/s]271733it [02:03, 3202.45it/s]271641it [02:00, 3145.37it/s]272090it [02:04, 3304.61it/s]271989it [02:00, 3109.08it/s]272424it [02:04, 3122.03it/s]272772it [02:04, 3221.90it/s]272303it [02:00, 2992.41it/s]272653it [02:00, 3131.34it/s]273098it [02:04, 3067.61it/s]273450it [02:04, 3192.71it/s]273006it [02:00, 3242.87it/s]273800it [02:04, 3279.64it/s]273333it [02:00, 3083.42it/s]273691it [02:00, 3222.25it/s]274131it [02:04, 3116.89it/s]274484it [02:04, 3232.30it/s]274017it [02:01, 3074.80it/s]274373it [02:01, 3210.34it/s]274811it [02:04, 3078.75it/s]275166it [02:04, 3209.04it/s]274698it [02:01, 3061.32it/s]275055it [02:01, 3201.70it/s]275525it [02:05, 3316.93it/s]275413it [02:01, 3308.59it/s]275860it [02:05, 3112.85it/s]276215it [02:05, 3233.99it/s]275747it [02:01, 3134.14it/s]276101it [02:01, 3247.23it/s]276543it [02:05, 3076.05it/s]276894it [02:05, 3194.50it/s]276429it [02:01, 3076.63it/s]276776it [02:01, 3183.92it/s]277217it [02:05, 3038.54it/s]277132it [02:02, 3288.55it/s]277562it [02:05, 3152.64it/s]277906it [02:05, 3229.47it/s]277464it [02:02, 3091.90it/s]277811it [02:02, 3196.05it/s]278232it [02:05, 3031.74it/s]278583it [02:06, 3163.38it/s]278135it [02:02, 3044.43it/s]278490it [02:02, 3183.54it/s]278904it [02:06, 3020.68it/s]278845it [02:02, 3285.51it/s]279255it [02:06, 3156.09it/s]279608it [02:06, 3261.87it/s]279177it [02:02, 3125.43it/s]279524it [02:02, 3220.65it/s]279938it [02:06, 3083.21it/s]279850it [02:02, 3076.06it/s]280287it [02:06, 3195.25it/s]280199it [02:03, 3191.71it/s]280610it [02:06, 3028.86it/s]280554it [02:03, 3292.51it/s]280966it [02:06, 3174.28it/s]280886it [02:03, 3134.47it/s]281315it [02:06, 3261.80it/s]281234it [02:03, 3231.72it/s]281645it [02:07, 3092.44it/s]281560it [02:03, 3081.86it/s]281997it [02:07, 3210.90it/s]281911it [02:03, 3200.62it/s]282322it [02:07, 3055.06it/s]282673it [02:07, 3181.56it/s]282247it [02:03, 3019.54it/s]283012it [02:07, 3240.79it/s]282592it [02:03, 3135.75it/s]282943it [02:03, 3239.19it/s]283339it [02:07, 3074.02it/s]283691it [02:07, 3197.60it/s]283271it [02:03, 3080.49it/s]283619it [02:04, 3191.06it/s]284014it [02:07, 3040.47it/s]283942it [02:04, 3051.66it/s]284368it [02:07, 3177.67it/s]284302it [02:04, 3202.92it/s]284723it [02:08, 3283.56it/s]284660it [02:04, 3309.94it/s]285055it [02:08, 3159.65it/s]284994it [02:04, 3217.07it/s]285402it [02:08, 3245.86it/s]285339it [02:04, 3281.20it/s]285729it [02:08, 3158.92it/s]285670it [02:04, 3200.58it/s]286089it [02:08, 3284.03it/s]286030it [02:04, 3313.36it/s]286446it [02:08, 3195.09it/s]286394it [02:04, 3405.90it/s]286809it [02:08, 3317.21it/s]287112it [02:08, 2230.28it/s]
2022-08-15 14:53:40 | INFO | root | success load 287112 data
2022-08-15 14:53:40 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-08-15 14:53:40 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-08-15 14:53:40 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-08-15 14:53:40 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-08-15 14:53:40 | INFO | transformer.tokenization_utils | loading file None
2022-08-15 14:53:40 | INFO | transformer.tokenization_utils | loading file None
2022-08-15 14:53:40 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
286736it [02:05, 3291.72it/s]287104it [02:05, 3402.83it/s]287112it [02:05, 2294.34it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-08-15 14:56:06 | INFO | train_inner | epoch 028:     47 / 1122 loss=8.119, nll_loss=2.716, mask_ins=1.608, word_ins_ml=4.324, word_reposition=1.368, kpe=0.818, ppl=277.98, wps=3847.3, ups=0.19, wpb=20316, bsz=253.8, num_updates=30300, lr=0.000203111, gnorm=1.79, clip=0, loss_scale=512, train_wall=268, wall=0
2022-08-15 15:01:06 | INFO | train_inner | epoch 028:    147 / 1122 loss=9.983, nll_loss=3.277, mask_ins=2.374, word_ins_ml=4.823, word_reposition=1.976, kpe=0.811, ppl=1012.33, wps=6834.7, ups=0.33, wpb=20530.9, bsz=256, num_updates=30400, lr=0.000202777, gnorm=1.743, clip=0, loss_scale=512, train_wall=257, wall=0
2022-08-15 15:06:06 | INFO | train_inner | epoch 028:    247 / 1122 loss=9.792, nll_loss=3.194, mask_ins=2.329, word_ins_ml=4.749, word_reposition=1.908, kpe=0.807, ppl=886.72, wps=6829.5, ups=0.33, wpb=20521.8, bsz=256, num_updates=30500, lr=0.000202444, gnorm=1.735, clip=0, loss_scale=512, train_wall=257, wall=0
2022-08-15 15:11:08 | INFO | train_inner | epoch 028:    347 / 1122 loss=9.827, nll_loss=3.208, mask_ins=2.332, word_ins_ml=4.762, word_reposition=1.919, kpe=0.814, ppl=908.33, wps=6790.4, ups=0.33, wpb=20504.6, bsz=256, num_updates=30600, lr=0.000202113, gnorm=1.763, clip=0, loss_scale=512, train_wall=257, wall=0
2022-08-15 15:16:26 | INFO | train_inner | epoch 028:    447 / 1122 loss=9.73, nll_loss=3.167, mask_ins=2.309, word_ins_ml=4.725, word_reposition=1.885, kpe=0.81, ppl=849.26, wps=6520.6, ups=0.32, wpb=20679.8, bsz=256, num_updates=30700, lr=0.000201784, gnorm=1.906, clip=0, loss_scale=512, train_wall=274, wall=0
2022-08-15 15:23:50 | INFO | train_inner | epoch 028:    547 / 1122 loss=nan, nll_loss=3.18, mask_ins=2.3, word_ins_ml=4.737, word_reposition=1.893, kpe=nan, ppl=nan, wps=4621.2, ups=0.23, wpb=20522.7, bsz=256, num_updates=30800, lr=0.000201456, gnorm=1.854, clip=0, loss_scale=696, train_wall=397, wall=0
2022-08-15 15:30:41 | INFO | train_inner | epoch 028:    647 / 1122 loss=nan, nll_loss=3.183, mask_ins=2.301, word_ins_ml=4.739, word_reposition=1.904, kpe=nan, ppl=nan, wps=4971, ups=0.24, wpb=20460.7, bsz=256, num_updates=30900, lr=0.000201129, gnorm=1.933, clip=0, loss_scale=1024, train_wall=364, wall=0
2022-08-15 15:37:34 | INFO | train_inner | epoch 028:    747 / 1122 loss=9.722, nll_loss=3.193, mask_ins=2.284, word_ins_ml=4.749, word_reposition=1.874, kpe=0.816, ppl=844.52, wps=4950.8, ups=0.24, wpb=20452, bsz=256, num_updates=31000, lr=0.000200805, gnorm=1.875, clip=0, loss_scale=1024, train_wall=364, wall=0
2022-08-15 15:39:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-15 15:44:40 | INFO | train_inner | epoch 028:    848 / 1122 loss=9.705, nll_loss=3.156, mask_ins=2.279, word_ins_ml=4.715, word_reposition=1.895, kpe=0.816, ppl=834.74, wps=4813.8, ups=0.24, wpb=20480.9, bsz=256, num_updates=31100, lr=0.000200482, gnorm=1.96, clip=0, loss_scale=634, train_wall=373, wall=0
2022-08-15 15:51:46 | INFO | train_inner | epoch 028:    948 / 1122 loss=9.733, nll_loss=3.203, mask_ins=2.287, word_ins_ml=4.758, word_reposition=1.872, kpe=0.816, ppl=850.97, wps=4808.9, ups=0.23, wpb=20503.7, bsz=256, num_updates=31200, lr=0.00020016, gnorm=1.839, clip=0, loss_scale=512, train_wall=373, wall=0
2022-08-15 15:58:52 | INFO | train_inner | epoch 028:   1048 / 1122 loss=9.833, nll_loss=3.25, mask_ins=2.297, word_ins_ml=4.799, word_reposition=1.92, kpe=0.817, ppl=912.38, wps=4837.3, ups=0.24, wpb=20580.9, bsz=256, num_updates=31300, lr=0.00019984, gnorm=1.856, clip=0, loss_scale=512, train_wall=372, wall=0
2022-08-15 16:04:03 | INFO | train | epoch 028 | loss nan | nll_loss 3.201 | mask_ins 2.317 | word_ins_ml 4.755 | word_reposition 1.911 | kpe nan | ppl nan | wps 5187 | ups 0.25 | wpb 20520 | bsz 255.8 | num_updates 31374 | lr 0.000199604 | gnorm 1.858 | clip 0 | loss_scale 631 | train_wall 3683 | wall 0
2022-08-15 16:06:03 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 16.046 | nll_loss 7.379 | mask_ins 3.171 | word_ins_ml 8.606 | word_reposition 2.831 | kpe 1.438 | ppl 67660.6 | wps 8257.2 | wpb 2367.6 | bsz 32 | num_updates 31374 | best_loss 10.679
2022-08-15 16:06:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 28 @ 31374 updates, score 16.046) (writing took 17.463121488690376 seconds)
2022-08-15 16:08:10 | INFO | train_inner | epoch 029:     26 / 1122 loss=9.679, nll_loss=3.14, mask_ins=2.272, word_ins_ml=4.702, word_reposition=1.889, kpe=0.815, ppl=819.74, wps=3668, ups=0.18, wpb=20460.3, bsz=253.8, num_updates=31400, lr=0.000199522, gnorm=1.889, clip=0, loss_scale=512, train_wall=369, wall=0
2022-08-15 16:15:13 | INFO | train_inner | epoch 029:    126 / 1122 loss=9.668, nll_loss=3.153, mask_ins=2.271, word_ins_ml=4.713, word_reposition=1.88, kpe=0.804, ppl=813.73, wps=4861.8, ups=0.24, wpb=20585, bsz=256, num_updates=31500, lr=0.000199205, gnorm=1.916, clip=0, loss_scale=512, train_wall=371, wall=0
2022-08-15 16:22:16 | INFO | train_inner | epoch 029:    226 / 1122 loss=9.608, nll_loss=3.113, mask_ins=2.27, word_ins_ml=4.678, word_reposition=1.858, kpe=0.801, ppl=780.39, wps=4856.6, ups=0.24, wpb=20532, bsz=256, num_updates=31600, lr=0.000198889, gnorm=1.897, clip=0, loss_scale=845, train_wall=369, wall=0
2022-08-15 16:29:22 | INFO | train_inner | epoch 029:    326 / 1122 loss=9.69, nll_loss=3.181, mask_ins=2.269, word_ins_ml=4.738, word_reposition=1.877, kpe=0.805, ppl=825.76, wps=4808.4, ups=0.23, wpb=20518, bsz=256, num_updates=31700, lr=0.000198575, gnorm=1.898, clip=0, loss_scale=1024, train_wall=372, wall=0
2022-08-15 16:36:23 | INFO | train_inner | epoch 029:    426 / 1122 loss=nan, nll_loss=3.164, mask_ins=2.282, word_ins_ml=4.723, word_reposition=1.88, kpe=nan, ppl=nan, wps=4881.1, ups=0.24, wpb=20503.4, bsz=256, num_updates=31800, lr=0.000198263, gnorm=1.932, clip=0, loss_scale=1024, train_wall=368, wall=0
2022-08-15 16:43:23 | INFO | train_inner | epoch 029:    526 / 1122 loss=9.643, nll_loss=3.131, mask_ins=2.264, word_ins_ml=4.694, word_reposition=1.88, kpe=0.805, ppl=799.26, wps=4879.4, ups=0.24, wpb=20502.6, bsz=256, num_updates=31900, lr=0.000197952, gnorm=1.886, clip=0, loss_scale=1024, train_wall=368, wall=0
2022-08-15 16:51:14 | INFO | train_inner | epoch 029:    626 / 1122 loss=nan, nll_loss=3.173, mask_ins=2.259, word_ins_ml=4.731, word_reposition=1.882, kpe=nan, ppl=nan, wps=4373.6, ups=0.21, wpb=20591.6, bsz=256, num_updates=32000, lr=0.000197642, gnorm=1.869, clip=0, loss_scale=1024, train_wall=418, wall=0
2022-08-15 16:52:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-15 16:58:25 | INFO | train_inner | epoch 029:    727 / 1122 loss=9.604, nll_loss=3.147, mask_ins=2.253, word_ins_ml=4.708, word_reposition=1.834, kpe=0.809, ppl=778.44, wps=4756.2, ups=0.23, wpb=20505.7, bsz=256, num_updates=32100, lr=0.000197334, gnorm=1.932, clip=0, loss_scale=578, train_wall=377, wall=0
2022-08-15 17:05:28 | INFO | train_inner | epoch 029:    827 / 1122 loss=9.642, nll_loss=3.171, mask_ins=2.239, word_ins_ml=4.729, word_reposition=1.867, kpe=0.806, ppl=798.95, wps=4849.7, ups=0.24, wpb=20515.7, bsz=256, num_updates=32200, lr=0.000197028, gnorm=1.888, clip=0, loss_scale=512, train_wall=369, wall=0
2022-08-15 17:12:29 | INFO | train_inner | epoch 029:    927 / 1122 loss=9.631, nll_loss=3.137, mask_ins=2.259, word_ins_ml=4.699, word_reposition=1.865, kpe=0.807, ppl=792.95, wps=4897.6, ups=0.24, wpb=20635.2, bsz=256, num_updates=32300, lr=0.000196722, gnorm=1.822, clip=0, loss_scale=512, train_wall=369, wall=0
2022-08-15 17:19:40 | INFO | train_inner | epoch 029:   1027 / 1122 loss=9.667, nll_loss=3.168, mask_ins=2.266, word_ins_ml=4.726, word_reposition=1.865, kpe=0.81, ppl=812.98, wps=4746.8, ups=0.23, wpb=20477.5, bsz=256, num_updates=32400, lr=0.000196419, gnorm=1.806, clip=0, loss_scale=512, train_wall=377, wall=0
2022-08-15 17:26:21 | INFO | train | epoch 029 | loss nan | nll_loss 3.152 | mask_ins 2.263 | word_ins_ml 4.713 | word_reposition 1.87 | kpe nan | ppl nan | wps 4658 | ups 0.23 | wpb 20520.2 | bsz 255.8 | num_updates 32495 | lr 0.000196131 | gnorm 1.883 | clip 0 | loss_scale 730 | train_wall 4204 | wall 0
2022-08-15 17:28:22 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 15.93 | nll_loss 7.305 | mask_ins 3.086 | word_ins_ml 8.542 | word_reposition 2.838 | kpe 1.465 | ppl 62440.8 | wps 8206.8 | wpb 2367.6 | bsz 32 | num_updates 32495 | best_loss 10.679
2022-08-15 17:28:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 29 @ 32495 updates, score 15.93) (writing took 25.33769472129643 seconds)
2022-08-15 17:29:09 | INFO | train_inner | epoch 030:      5 / 1122 loss=9.592, nll_loss=3.113, mask_ins=2.256, word_ins_ml=4.678, word_reposition=1.853, kpe=0.804, ppl=771.5, wps=3577.5, ups=0.18, wpb=20350.3, bsz=253.8, num_updates=32500, lr=0.000196116, gnorm=1.876, clip=0, loss_scale=512, train_wall=370, wall=0
2022-08-15 17:36:14 | INFO | train_inner | epoch 030:    105 / 1122 loss=9.608, nll_loss=3.131, mask_ins=2.25, word_ins_ml=4.694, word_reposition=1.868, kpe=0.795, ppl=780.33, wps=4831.1, ups=0.24, wpb=20507.2, bsz=256, num_updates=32600, lr=0.000195815, gnorm=1.797, clip=0, loss_scale=901, train_wall=371, wall=0
2022-08-15 17:43:12 | INFO | train_inner | epoch 030:    205 / 1122 loss=9.573, nll_loss=3.128, mask_ins=2.243, word_ins_ml=4.691, word_reposition=1.843, kpe=0.795, ppl=761.49, wps=4905.7, ups=0.24, wpb=20499.9, bsz=256, num_updates=32700, lr=0.000195515, gnorm=1.815, clip=0, loss_scale=1024, train_wall=367, wall=0
2022-08-15 17:50:15 | INFO | train_inner | epoch 030:    305 / 1122 loss=9.579, nll_loss=3.115, mask_ins=2.247, word_ins_ml=4.68, word_reposition=1.857, kpe=0.794, ppl=764.76, wps=4846.7, ups=0.24, wpb=20511.5, bsz=256, num_updates=32800, lr=0.000195217, gnorm=1.826, clip=0, loss_scale=1024, train_wall=371, wall=0
2022-08-15 17:57:34 | INFO | train_inner | epoch 030:    405 / 1122 loss=9.581, nll_loss=3.105, mask_ins=2.246, word_ins_ml=4.672, word_reposition=1.869, kpe=0.795, ppl=765.85, wps=4648.3, ups=0.23, wpb=20424.7, bsz=256, num_updates=32900, lr=0.00019492, gnorm=1.85, clip=0, loss_scale=1024, train_wall=386, wall=0
2022-08-15 18:04:44 | INFO | train_inner | epoch 030:    505 / 1122 loss=nan, nll_loss=3.09, mask_ins=2.231, word_ins_ml=4.657, word_reposition=1.847, kpe=nan, ppl=nan, wps=4816.4, ups=0.23, wpb=20680.8, bsz=256, num_updates=33000, lr=0.000194625, gnorm=1.834, clip=0, loss_scale=1024, train_wall=375, wall=0
2022-08-15 18:11:57 | INFO | train_inner | epoch 030:    605 / 1122 loss=9.608, nll_loss=3.151, mask_ins=2.247, word_ins_ml=4.712, word_reposition=1.846, kpe=0.803, ppl=780.5, wps=4729.4, ups=0.23, wpb=20502.4, bsz=256, num_updates=33100, lr=0.000194331, gnorm=1.84, clip=0, loss_scale=1679, train_wall=377, wall=0
2022-08-15 18:19:44 | INFO | train_inner | epoch 030:    705 / 1122 loss=9.554, nll_loss=3.127, mask_ins=2.232, word_ins_ml=4.69, word_reposition=1.834, kpe=0.798, ppl=751.57, wps=4367.8, ups=0.21, wpb=20403.3, bsz=256, num_updates=33200, lr=0.000194038, gnorm=1.798, clip=0, loss_scale=2048, train_wall=413, wall=0
2022-08-15 18:26:57 | INFO | train_inner | epoch 030:    805 / 1122 loss=9.521, nll_loss=3.102, mask_ins=2.238, word_ins_ml=4.669, word_reposition=1.816, kpe=0.799, ppl=734.8, wps=4752.5, ups=0.23, wpb=20557.9, bsz=256, num_updates=33300, lr=0.000193746, gnorm=1.848, clip=0, loss_scale=2048, train_wall=378, wall=0
2022-08-15 18:33:56 | INFO | train_inner | epoch 030:    905 / 1122 loss=9.554, nll_loss=3.106, mask_ins=2.247, word_ins_ml=4.671, word_reposition=1.833, kpe=0.803, ppl=751.81, wps=4903.5, ups=0.24, wpb=20561.5, bsz=256, num_updates=33400, lr=0.000193456, gnorm=1.971, clip=0, loss_scale=2048, train_wall=368, wall=0
2022-08-15 18:36:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-15 18:41:04 | INFO | train_inner | epoch 030:   1006 / 1122 loss=9.514, nll_loss=3.091, mask_ins=2.227, word_ins_ml=4.658, word_reposition=1.83, kpe=0.799, ppl=731.25, wps=4833.5, ups=0.23, wpb=20672.3, bsz=256, num_updates=33500, lr=0.000193167, gnorm=1.821, clip=0, loss_scale=1440, train_wall=375, wall=0
2022-08-15 18:45:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-15 18:48:14 | INFO | train_inner | epoch 030:   1107 / 1122 loss=nan, nll_loss=3.084, mask_ins=2.235, word_ins_ml=4.653, word_reposition=1.816, kpe=nan, ppl=nan, wps=4780.2, ups=0.23, wpb=20569.5, bsz=256, num_updates=33600, lr=0.000192879, gnorm=1.924, clip=0, loss_scale=836, train_wall=377, wall=0
2022-08-15 18:49:17 | INFO | train | epoch 030 | loss nan | nll_loss 3.11 | mask_ins 2.24 | word_ins_ml 4.676 | word_reposition 1.841 | kpe nan | ppl nan | wps 4618.9 | ups 0.23 | wpb 20521.1 | bsz 255.8 | num_updates 33615 | lr 0.000192836 | gnorm 1.853 | clip 0 | loss_scale 1357 | train_wall 4234 | wall 0
2022-08-15 18:51:16 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 15.977 | nll_loss 7.308 | mask_ins 3.119 | word_ins_ml 8.542 | word_reposition 2.831 | kpe 1.484 | ppl 64477.7 | wps 8280.8 | wpb 2367.6 | bsz 32 | num_updates 33615 | best_loss 10.679
2022-08-15 18:51:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 30 @ 33615 updates, score 15.977) (writing took 11.557706084102392 seconds)
2022-08-15 18:57:30 | INFO | train_inner | epoch 031:     85 / 1122 loss=nan, nll_loss=3.079, mask_ins=2.23, word_ins_ml=4.648, word_reposition=1.83, kpe=nan, ppl=nan, wps=3687.5, ups=0.18, wpb=20489.9, bsz=253.8, num_updates=33700, lr=0.000192593, gnorm=1.896, clip=0, loss_scale=512, train_wall=371, wall=0
2022-08-15 19:04:33 | INFO | train_inner | epoch 031:    185 / 1122 loss=9.463, nll_loss=3.063, mask_ins=2.222, word_ins_ml=4.634, word_reposition=1.821, kpe=0.786, ppl=705.64, wps=4881, ups=0.24, wpb=20640.5, bsz=256, num_updates=33800, lr=0.000192308, gnorm=1.884, clip=0, loss_scale=512, train_wall=371, wall=0
2022-08-15 19:11:36 | INFO | train_inner | epoch 031:    285 / 1122 loss=9.529, nll_loss=3.117, mask_ins=2.239, word_ins_ml=4.682, word_reposition=1.818, kpe=0.791, ppl=739.03, wps=4823.1, ups=0.24, wpb=20395.9, bsz=256, num_updates=33900, lr=0.000192024, gnorm=1.998, clip=0, loss_scale=512, train_wall=371, wall=0
2022-08-15 19:18:38 | INFO | train_inner | epoch 031:    385 / 1122 loss=9.547, nll_loss=3.12, mask_ins=2.234, word_ins_ml=4.685, word_reposition=1.839, kpe=0.789, ppl=747.91, wps=4876.6, ups=0.24, wpb=20609.5, bsz=256, num_updates=34000, lr=0.000191741, gnorm=1.94, clip=0, loss_scale=512, train_wall=370, wall=0
2022-08-15 19:25:47 | INFO | train_inner | epoch 031:    485 / 1122 loss=9.519, nll_loss=3.116, mask_ins=2.225, word_ins_ml=4.681, word_reposition=1.828, kpe=0.786, ppl=733.8, wps=4771.3, ups=0.23, wpb=20430.5, bsz=256, num_updates=34100, lr=0.00019146, gnorm=1.861, clip=0, loss_scale=640, train_wall=375, wall=0
2022-08-15 19:32:50 | INFO | train_inner | epoch 031:    585 / 1122 loss=9.52, nll_loss=3.09, mask_ins=2.223, word_ins_ml=4.658, word_reposition=1.848, kpe=0.792, ppl=734.37, wps=4858.1, ups=0.24, wpb=20593.3, bsz=256, num_updates=34200, lr=0.00019118, gnorm=1.951, clip=0, loss_scale=1024, train_wall=372, wall=0
2022-08-15 19:39:50 | INFO | train_inner | epoch 031:    685 / 1122 loss=9.472, nll_loss=3.09, mask_ins=2.209, word_ins_ml=4.658, word_reposition=1.813, kpe=0.792, ppl=710.04, wps=4899.1, ups=0.24, wpb=20575.6, bsz=256, num_updates=34300, lr=0.000190901, gnorm=1.859, clip=0, loss_scale=1024, train_wall=370, wall=0
2022-08-15 19:47:39 | INFO | train_inner | epoch 031:    785 / 1122 loss=9.49, nll_loss=3.079, mask_ins=2.215, word_ins_ml=4.647, word_reposition=1.834, kpe=0.794, ppl=718.97, wps=4388.4, ups=0.21, wpb=20544.3, bsz=256, num_updates=34400, lr=0.000190623, gnorm=1.931, clip=0, loss_scale=1024, train_wall=414, wall=0
2022-08-15 19:54:40 | INFO | train_inner | epoch 031:    885 / 1122 loss=9.486, nll_loss=3.074, mask_ins=2.231, word_ins_ml=4.644, word_reposition=1.818, kpe=0.794, ppl=717.23, wps=4859.3, ups=0.24, wpb=20488.2, bsz=256, num_updates=34500, lr=0.000190347, gnorm=1.99, clip=0, loss_scale=1024, train_wall=371, wall=0
2022-08-15 20:01:45 | INFO | train_inner | epoch 031:    985 / 1122 loss=9.512, nll_loss=3.089, mask_ins=2.229, word_ins_ml=4.657, word_reposition=1.833, kpe=0.793, ppl=730.23, wps=4809, ups=0.24, wpb=20436.6, bsz=256, num_updates=34600, lr=0.000190071, gnorm=1.889, clip=0, loss_scale=1157, train_wall=371, wall=0
2022-08-15 20:03:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-15 20:08:49 | INFO | train_inner | epoch 031:   1086 / 1122 loss=9.476, nll_loss=3.062, mask_ins=2.217, word_ins_ml=4.633, word_reposition=1.833, kpe=0.793, ppl=712.1, wps=4850.8, ups=0.24, wpb=20544.2, bsz=256, num_updates=34700, lr=0.000189797, gnorm=1.893, clip=0, loss_scale=1237, train_wall=371, wall=0
2022-08-15 20:11:20 | INFO | train | epoch 031 | loss nan | nll_loss 3.09 | mask_ins 2.225 | word_ins_ml 4.658 | word_reposition 1.829 | kpe nan | ppl nan | wps 4672.7 | ups 0.23 | wpb 20521.5 | bsz 255.8 | num_updates 34736 | lr 0.000189699 | gnorm 1.932 | clip 0 | loss_scale 845 | train_wall 4204 | wall 0
2022-08-15 20:13:19 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 16.058 | nll_loss 7.344 | mask_ins 3.181 | word_ins_ml 8.578 | word_reposition 2.824 | kpe 1.474 | ppl 68202.2 | wps 8318.3 | wpb 2367.6 | bsz 32 | num_updates 34736 | best_loss 10.679
2022-08-15 20:13:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 31 @ 34736 updates, score 16.058) (writing took 11.520106762647629 seconds)
2022-08-15 20:18:01 | INFO | train_inner | epoch 032:     64 / 1122 loss=nan, nll_loss=3.099, mask_ins=2.22, word_ins_ml=4.666, word_reposition=1.853, kpe=nan, ppl=nan, wps=3684.6, ups=0.18, wpb=20329.3, bsz=253.8, num_updates=34800, lr=0.000189525, gnorm=2.134, clip=0, loss_scale=1024, train_wall=369, wall=0
2022-08-15 20:25:00 | INFO | train_inner | epoch 032:    164 / 1122 loss=9.457, nll_loss=3.072, mask_ins=2.216, word_ins_ml=4.642, word_reposition=1.82, kpe=0.779, ppl=702.76, wps=4877.1, ups=0.24, wpb=20452.3, bsz=256, num_updates=34900, lr=0.000189253, gnorm=2.033, clip=0, loss_scale=1024, train_wall=367, wall=0
2022-08-15 20:32:07 | INFO | train_inner | epoch 032:    264 / 1122 loss=9.434, nll_loss=3.075, mask_ins=2.209, word_ins_ml=4.645, word_reposition=1.8, kpe=0.78, ppl=691.74, wps=4811.3, ups=0.23, wpb=20522.9, bsz=256, num_updates=35000, lr=0.000188982, gnorm=1.978, clip=0, loss_scale=1024, train_wall=374, wall=0
2022-08-15 20:39:11 | INFO | train_inner | epoch 032:    364 / 1122 loss=9.478, nll_loss=3.072, mask_ins=2.222, word_ins_ml=4.642, word_reposition=1.83, kpe=0.784, ppl=713.13, wps=4870.4, ups=0.24, wpb=20677.4, bsz=256, num_updates=35100, lr=0.000188713, gnorm=2.005, clip=0, loss_scale=1024, train_wall=371, wall=0
2022-08-15 20:39:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-15 20:46:21 | INFO | train_inner | epoch 032:    465 / 1122 loss=9.406, nll_loss=3.033, mask_ins=2.205, word_ins_ml=4.607, word_reposition=1.816, kpe=0.778, ppl=678.5, wps=4817.7, ups=0.23, wpb=20717, bsz=256, num_updates=35200, lr=0.000188445, gnorm=1.904, clip=0, loss_scale=537, train_wall=377, wall=0
2022-08-15 20:53:20 | INFO | train_inner | epoch 032:    565 / 1122 loss=nan, nll_loss=3.057, mask_ins=2.197, word_ins_ml=4.629, word_reposition=1.806, kpe=nan, ppl=nan, wps=4899.2, ups=0.24, wpb=20539.5, bsz=256, num_updates=35300, lr=0.000188177, gnorm=1.953, clip=0, loss_scale=512, train_wall=368, wall=0
2022-08-15 21:00:24 | INFO | train_inner | epoch 032:    665 / 1122 loss=9.519, nll_loss=3.089, mask_ins=2.22, word_ins_ml=4.657, word_reposition=1.856, kpe=0.786, ppl=733.59, wps=4825.8, ups=0.24, wpb=20442.8, bsz=256, num_updates=35400, lr=0.000187912, gnorm=1.93, clip=0, loss_scale=512, train_wall=371, wall=0
2022-08-15 21:07:27 | INFO | train_inner | epoch 032:    765 / 1122 loss=9.416, nll_loss=3.044, mask_ins=2.202, word_ins_ml=4.617, word_reposition=1.812, kpe=0.785, ppl=683.31, wps=4814.1, ups=0.24, wpb=20354.5, bsz=256, num_updates=35500, lr=0.000187647, gnorm=1.917, clip=0, loss_scale=512, train_wall=368, wall=0
2022-08-15 21:15:25 | INFO | train_inner | epoch 032:    865 / 1122 loss=9.462, nll_loss=3.089, mask_ins=2.201, word_ins_ml=4.658, word_reposition=1.817, kpe=0.787, ppl=705.39, wps=4275.1, ups=0.21, wpb=20436.6, bsz=256, num_updates=35600, lr=0.000187383, gnorm=1.955, clip=0, loss_scale=512, train_wall=424, wall=0
2022-08-15 21:22:24 | INFO | train_inner | epoch 032:    965 / 1122 loss=9.478, nll_loss=3.071, mask_ins=2.219, word_ins_ml=4.641, word_reposition=1.833, kpe=0.785, ppl=713.06, wps=4923.8, ups=0.24, wpb=20616.7, bsz=256, num_updates=35700, lr=0.00018712, gnorm=1.96, clip=0, loss_scale=942, train_wall=368, wall=0
2022-08-15 21:29:31 | INFO | train_inner | epoch 032:   1065 / 1122 loss=9.419, nll_loss=3.054, mask_ins=2.203, word_ins_ml=4.626, word_reposition=1.803, kpe=0.787, ppl=684.73, wps=4812.9, ups=0.23, wpb=20562.5, bsz=256, num_updates=35800, lr=0.000186859, gnorm=1.829, clip=0, loss_scale=1024, train_wall=374, wall=0
2022-08-15 21:33:32 | INFO | train | epoch 032 | loss nan | nll_loss 3.067 | mask_ins 2.209 | word_ins_ml 4.638 | word_reposition 1.823 | kpe nan | ppl nan | wps 4663.6 | ups 0.23 | wpb 20518.9 | bsz 255.8 | num_updates 35857 | lr 0.00018671 | gnorm 1.953 | clip 0 | loss_scale 790 | train_wall 4211 | wall 0
2022-08-15 21:35:31 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 15.871 | nll_loss 7.326 | mask_ins 3.085 | word_ins_ml 8.561 | word_reposition 2.753 | kpe 1.471 | ppl 59919.6 | wps 8314.6 | wpb 2367.6 | bsz 32 | num_updates 35857 | best_loss 10.679
2022-08-15 21:35:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 32 @ 35857 updates, score 15.871) (writing took 10.577913781628013 seconds)
2022-08-15 21:38:42 | INFO | train_inner | epoch 033:     43 / 1122 loss=nan, nll_loss=3.074, mask_ins=2.194, word_ins_ml=4.644, word_reposition=1.847, kpe=nan, ppl=nan, wps=3712.6, ups=0.18, wpb=20445.2, bsz=253.8, num_updates=35900, lr=0.000186598, gnorm=2.042, clip=0, loss_scale=1024, train_wall=370, wall=0
2022-08-15 21:45:44 | INFO | train_inner | epoch 033:    143 / 1122 loss=9.42, nll_loss=3.059, mask_ins=2.204, word_ins_ml=4.631, word_reposition=1.813, kpe=0.771, ppl=684.81, wps=4898.4, ups=0.24, wpb=20673.6, bsz=256, num_updates=36000, lr=0.000186339, gnorm=1.888, clip=0, loss_scale=1024, train_wall=369, wall=0
2022-08-15 21:52:44 | INFO | train_inner | epoch 033:    243 / 1122 loss=9.409, nll_loss=3.039, mask_ins=2.216, word_ins_ml=4.613, word_reposition=1.808, kpe=0.772, ppl=679.99, wps=4882.2, ups=0.24, wpb=20521.6, bsz=256, num_updates=36100, lr=0.000186081, gnorm=1.952, clip=0, loss_scale=1024, train_wall=369, wall=0
2022-08-15 21:56:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-15 21:59:50 | INFO | train_inner | epoch 033:    344 / 1122 loss=9.37, nll_loss=3.032, mask_ins=2.192, word_ins_ml=4.606, word_reposition=1.801, kpe=0.771, ppl=661.74, wps=4828.5, ups=0.23, wpb=20583.4, bsz=256, num_updates=36200, lr=0.000185824, gnorm=1.856, clip=0, loss_scale=1328, train_wall=373, wall=0
2022-08-15 22:06:53 | INFO | train_inner | epoch 033:    444 / 1122 loss=9.424, nll_loss=3.054, mask_ins=2.21, word_ins_ml=4.626, word_reposition=1.813, kpe=0.775, ppl=686.75, wps=4840.1, ups=0.24, wpb=20454.3, bsz=256, num_updates=36300, lr=0.000185567, gnorm=1.904, clip=0, loss_scale=1024, train_wall=371, wall=0
2022-08-15 22:13:53 | INFO | train_inner | epoch 033:    544 / 1122 loss=9.361, nll_loss=3.034, mask_ins=2.196, word_ins_ml=4.608, word_reposition=1.782, kpe=0.775, ppl=657.61, wps=4888.3, ups=0.24, wpb=20514.6, bsz=256, num_updates=36400, lr=0.000185312, gnorm=1.884, clip=0, loss_scale=1024, train_wall=368, wall=0
2022-08-15 22:20:54 | INFO | train_inner | epoch 033:    644 / 1122 loss=nan, nll_loss=3.053, mask_ins=2.195, word_ins_ml=4.626, word_reposition=1.803, kpe=nan, ppl=nan, wps=4865.7, ups=0.24, wpb=20498, bsz=256, num_updates=36500, lr=0.000185058, gnorm=1.874, clip=0, loss_scale=1024, train_wall=369, wall=0
2022-08-15 22:27:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-15 22:28:11 | INFO | train_inner | epoch 033:    745 / 1122 loss=9.41, nll_loss=3.055, mask_ins=2.204, word_ins_ml=4.627, word_reposition=1.802, kpe=0.777, ppl=680.41, wps=4689.1, ups=0.23, wpb=20486.8, bsz=256, num_updates=36600, lr=0.000184805, gnorm=1.914, clip=0, loss_scale=989, train_wall=384, wall=0
2022-08-15 22:35:49 | INFO | train_inner | epoch 033:    845 / 1122 loss=9.414, nll_loss=3.065, mask_ins=2.198, word_ins_ml=4.635, word_reposition=1.799, kpe=0.781, ppl=682.08, wps=4471.4, ups=0.22, wpb=20472.5, bsz=256, num_updates=36700, lr=0.000184553, gnorm=1.897, clip=0, loss_scale=512, train_wall=406, wall=0
2022-08-15 22:43:11 | INFO | train_inner | epoch 033:    945 / 1122 loss=9.381, nll_loss=3.042, mask_ins=2.197, word_ins_ml=4.615, word_reposition=1.792, kpe=0.777, ppl=666.88, wps=4642.7, ups=0.23, wpb=20527.3, bsz=256, num_updates=36800, lr=0.000184302, gnorm=1.8, clip=0, loss_scale=512, train_wall=390, wall=0
2022-08-15 22:50:14 | INFO | train_inner | epoch 033:   1045 / 1122 loss=nan, nll_loss=3.031, mask_ins=2.195, word_ins_ml=4.605, word_reposition=1.817, kpe=nan, ppl=nan, wps=4879.9, ups=0.24, wpb=20634.9, bsz=256, num_updates=36900, lr=0.000184053, gnorm=1.838, clip=0, loss_scale=512, train_wall=369, wall=0
2022-08-15 22:55:37 | INFO | train | epoch 033 | loss nan | nll_loss 3.047 | mask_ins 2.2 | word_ins_ml 4.619 | word_reposition 1.804 | kpe nan | ppl nan | wps 4667.1 | ups 0.23 | wpb 20520.9 | bsz 255.8 | num_updates 36977 | lr 0.000183861 | gnorm 1.887 | clip 0 | loss_scale 876 | train_wall 4210 | wall 0
2022-08-15 22:57:35 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 15.936 | nll_loss 7.383 | mask_ins 3.085 | word_ins_ml 8.62 | word_reposition 2.811 | kpe 1.42 | ppl 62672.8 | wps 8357.3 | wpb 2367.6 | bsz 32 | num_updates 36977 | best_loss 10.679
2022-08-15 22:57:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 33 @ 36977 updates, score 15.936) (writing took 12.321573160588741 seconds)
2022-08-15 22:59:26 | INFO | train_inner | epoch 034:     23 / 1122 loss=9.384, nll_loss=3.04, mask_ins=2.189, word_ins_ml=4.613, word_reposition=1.803, kpe=0.779, ppl=668.35, wps=3679.4, ups=0.18, wpb=20336.3, bsz=253.8, num_updates=37000, lr=0.000183804, gnorm=1.923, clip=0, loss_scale=512, train_wall=370, wall=0
2022-08-15 23:06:28 | INFO | train_inner | epoch 034:    123 / 1122 loss=9.408, nll_loss=3.043, mask_ins=2.207, word_ins_ml=4.617, word_reposition=1.825, kpe=0.759, ppl=679.32, wps=4857.5, ups=0.24, wpb=20459.5, bsz=256, num_updates=37100, lr=0.000183556, gnorm=1.921, clip=0, loss_scale=512, train_wall=370, wall=0
2022-08-15 23:13:26 | INFO | train_inner | epoch 034:    223 / 1122 loss=9.356, nll_loss=3.009, mask_ins=2.194, word_ins_ml=4.586, word_reposition=1.813, kpe=0.763, ppl=655.33, wps=4891.4, ups=0.24, wpb=20445.8, bsz=256, num_updates=37200, lr=0.000183309, gnorm=1.84, clip=0, loss_scale=998, train_wall=367, wall=0
2022-08-15 23:20:29 | INFO | train_inner | epoch 034:    323 / 1122 loss=9.295, nll_loss=2.972, mask_ins=2.174, word_ins_ml=4.553, word_reposition=1.805, kpe=0.763, ppl=628.17, wps=4850.2, ups=0.24, wpb=20529.2, bsz=256, num_updates=37300, lr=0.000183063, gnorm=1.888, clip=0, loss_scale=1024, train_wall=370, wall=0
2022-08-15 23:27:25 | INFO | train_inner | epoch 034:    423 / 1122 loss=nan, nll_loss=3.016, mask_ins=2.183, word_ins_ml=4.591, word_reposition=1.789, kpe=nan, ppl=nan, wps=4925.9, ups=0.24, wpb=20500.5, bsz=256, num_updates=37400, lr=0.000182818, gnorm=1.875, clip=0, loss_scale=1024, train_wall=367, wall=0
2022-08-15 23:34:31 | INFO | train_inner | epoch 034:    523 / 1122 loss=9.399, nll_loss=3.044, mask_ins=2.195, word_ins_ml=4.617, word_reposition=1.82, kpe=0.767, ppl=675.31, wps=4811.7, ups=0.23, wpb=20513.5, bsz=256, num_updates=37500, lr=0.000182574, gnorm=1.891, clip=0, loss_scale=1024, train_wall=373, wall=0
2022-08-15 23:41:29 | INFO | train_inner | epoch 034:    623 / 1122 loss=9.375, nll_loss=3.024, mask_ins=2.186, word_ins_ml=4.599, word_reposition=1.819, kpe=0.77, ppl=663.9, wps=4922, ups=0.24, wpb=20581.6, bsz=256, num_updates=37600, lr=0.000182331, gnorm=1.906, clip=0, loss_scale=1024, train_wall=367, wall=0
2022-08-15 23:45:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-15 23:48:34 | INFO | train_inner | epoch 034:    724 / 1122 loss=9.385, nll_loss=3.027, mask_ins=2.204, word_ins_ml=4.602, word_reposition=1.808, kpe=0.771, ppl=668.48, wps=4871, ups=0.24, wpb=20678.1, bsz=256, num_updates=37700, lr=0.000182089, gnorm=1.913, clip=0, loss_scale=1460, train_wall=372, wall=0
2022-08-15 23:55:35 | INFO | train_inner | epoch 034:    824 / 1122 loss=9.289, nll_loss=2.996, mask_ins=2.163, word_ins_ml=4.574, word_reposition=1.783, kpe=0.769, ppl=625.37, wps=4883.2, ups=0.24, wpb=20565.7, bsz=256, num_updates=37800, lr=0.000181848, gnorm=1.882, clip=0, loss_scale=1024, train_wall=368, wall=0
2022-08-16 00:03:27 | INFO | train_inner | epoch 034:    924 / 1122 loss=9.346, nll_loss=3.01, mask_ins=2.193, word_ins_ml=4.587, word_reposition=1.792, kpe=0.774, ppl=650.78, wps=4354.7, ups=0.21, wpb=20545.8, bsz=256, num_updates=37900, lr=0.000181608, gnorm=1.852, clip=0, loss_scale=1024, train_wall=418, wall=0
2022-08-16 00:10:31 | INFO | train_inner | epoch 034:   1024 / 1122 loss=9.36, nll_loss=3.03, mask_ins=2.191, word_ins_ml=4.605, word_reposition=1.79, kpe=0.774, ppl=657.16, wps=4846.6, ups=0.24, wpb=20542.3, bsz=256, num_updates=38000, lr=0.000181369, gnorm=1.897, clip=0, loss_scale=1024, train_wall=370, wall=0
2022-08-16 00:17:25 | INFO | train | epoch 034 | loss nan | nll_loss 3.013 | mask_ins 2.189 | word_ins_ml 4.59 | word_reposition 1.802 | kpe nan | ppl nan | wps 4687.1 | ups 0.23 | wpb 20519.7 | bsz 255.8 | num_updates 38098 | lr 0.000181136 | gnorm 1.888 | clip 0 | loss_scale 1005 | train_wall 4190 | wall 0
2022-08-16 00:19:23 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 15.91 | nll_loss 7.311 | mask_ins 3.105 | word_ins_ml 8.544 | word_reposition 2.779 | kpe 1.481 | ppl 61555.7 | wps 8336.5 | wpb 2367.6 | bsz 32 | num_updates 38098 | best_loss 10.679
2022-08-16 00:19:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 34 @ 38098 updates, score 15.91) (writing took 27.390879997983575 seconds)
2022-08-16 00:19:59 | INFO | train_inner | epoch 035:      2 / 1122 loss=9.267, nll_loss=2.962, mask_ins=2.184, word_ins_ml=4.545, word_reposition=1.769, kpe=0.769, ppl=615.99, wps=3573.7, ups=0.18, wpb=20315.4, bsz=253.8, num_updates=38100, lr=0.000181131, gnorm=1.895, clip=0, loss_scale=1024, train_wall=369, wall=0
2022-08-16 00:26:58 | INFO | train_inner | epoch 035:    102 / 1122 loss=nan, nll_loss=2.985, mask_ins=2.187, word_ins_ml=4.565, word_reposition=1.797, kpe=nan, ppl=nan, wps=4907.5, ups=0.24, wpb=20527.2, bsz=256, num_updates=38200, lr=0.000180894, gnorm=1.897, clip=0, loss_scale=1321, train_wall=367, wall=0
2022-08-16 00:34:08 | INFO | train_inner | epoch 035:    202 / 1122 loss=9.31, nll_loss=3.003, mask_ins=2.187, word_ins_ml=4.582, word_reposition=1.789, kpe=0.753, ppl=634.89, wps=4762.4, ups=0.23, wpb=20509.2, bsz=256, num_updates=38300, lr=0.000180657, gnorm=1.956, clip=0, loss_scale=2048, train_wall=377, wall=0
2022-08-16 00:41:17 | INFO | train_inner | epoch 035:    302 / 1122 loss=9.335, nll_loss=2.999, mask_ins=2.194, word_ins_ml=4.577, word_reposition=1.805, kpe=0.759, ppl=645.75, wps=4804, ups=0.23, wpb=20577.7, bsz=256, num_updates=38400, lr=0.000180422, gnorm=1.893, clip=0, loss_scale=2048, train_wall=373, wall=0
2022-08-16 00:41:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-16 00:48:21 | INFO | train_inner | epoch 035:    403 / 1122 loss=9.338, nll_loss=3.018, mask_ins=2.189, word_ins_ml=4.594, word_reposition=1.795, kpe=0.761, ppl=647.32, wps=4843.8, ups=0.24, wpb=20561.6, bsz=256, num_updates=38500, lr=0.000180187, gnorm=1.911, clip=0, loss_scale=1034, train_wall=372, wall=0
2022-08-16 00:55:22 | INFO | train_inner | epoch 035:    503 / 1122 loss=nan, nll_loss=2.986, mask_ins=2.184, word_ins_ml=4.566, word_reposition=1.799, kpe=nan, ppl=nan, wps=4894.2, ups=0.24, wpb=20616, bsz=256, num_updates=38600, lr=0.000179954, gnorm=1.904, clip=0, loss_scale=1024, train_wall=369, wall=0
2022-08-16 01:02:23 | INFO | train_inner | epoch 035:    603 / 1122 loss=9.245, nll_loss=2.964, mask_ins=2.166, word_ins_ml=4.545, word_reposition=1.776, kpe=0.759, ppl=606.92, wps=4860.5, ups=0.24, wpb=20423.3, bsz=256, num_updates=38700, lr=0.000179721, gnorm=1.879, clip=0, loss_scale=1024, train_wall=368, wall=0
2022-08-16 01:04:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-16 01:09:24 | INFO | train_inner | epoch 035:    704 / 1122 loss=9.301, nll_loss=3.002, mask_ins=2.171, word_ins_ml=4.58, word_reposition=1.791, kpe=0.76, ppl=630.97, wps=4885.5, ups=0.24, wpb=20598.3, bsz=256, num_updates=38800, lr=0.00017949, gnorm=1.867, clip=0, loss_scale=659, train_wall=371, wall=0
2022-08-16 01:16:28 | INFO | train_inner | epoch 035:    804 / 1122 loss=9.274, nll_loss=2.972, mask_ins=2.173, word_ins_ml=4.553, word_reposition=1.786, kpe=0.761, ppl=618.95, wps=4823.2, ups=0.24, wpb=20463.2, bsz=256, num_updates=38900, lr=0.000179259, gnorm=1.914, clip=0, loss_scale=512, train_wall=371, wall=0
2022-08-16 01:23:29 | INFO | train_inner | epoch 035:    904 / 1122 loss=9.307, nll_loss=2.998, mask_ins=2.168, word_ins_ml=4.576, word_reposition=1.799, kpe=0.763, ppl=633.32, wps=4869, ups=0.24, wpb=20487.3, bsz=256, num_updates=39000, lr=0.000179029, gnorm=1.937, clip=0, loss_scale=512, train_wall=369, wall=0
2022-08-16 01:31:20 | INFO | train_inner | epoch 035:   1004 / 1122 loss=9.341, nll_loss=3.01, mask_ins=2.195, word_ins_ml=4.587, word_reposition=1.795, kpe=0.765, ppl=648.62, wps=4361.3, ups=0.21, wpb=20524.1, bsz=256, num_updates=39100, lr=0.0001788, gnorm=1.895, clip=0, loss_scale=512, train_wall=419, wall=0
2022-08-16 01:38:27 | INFO | train_inner | epoch 035:   1104 / 1122 loss=9.232, nll_loss=2.946, mask_ins=2.166, word_ins_ml=4.53, word_reposition=1.775, kpe=0.762, ppl=601.52, wps=4816.4, ups=0.23, wpb=20590.7, bsz=256, num_updates=39200, lr=0.000178571, gnorm=1.965, clip=0, loss_scale=512, train_wall=374, wall=0
2022-08-16 01:39:42 | INFO | train | epoch 035 | loss nan | nll_loss 2.987 | mask_ins 2.18 | word_ins_ml 4.567 | word_reposition 1.79 | kpe nan | ppl nan | wps 4654.7 | ups 0.23 | wpb 20520.5 | bsz 255.8 | num_updates 39218 | lr 0.00017853 | gnorm 1.915 | clip 0 | loss_scale 1010 | train_wall 4204 | wall 0
2022-08-16 01:41:41 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 15.9 | nll_loss 7.276 | mask_ins 3.13 | word_ins_ml 8.509 | word_reposition 2.804 | kpe 1.457 | ppl 61146.9 | wps 8311.4 | wpb 2367.6 | bsz 32 | num_updates 39218 | best_loss 10.679
2022-08-16 01:41:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 35 @ 39218 updates, score 15.9) (writing took 12.840051019564271 seconds)
2022-08-16 01:47:45 | INFO | train_inner | epoch 036:     82 / 1122 loss=nan, nll_loss=2.945, mask_ins=2.172, word_ins_ml=4.529, word_reposition=1.76, kpe=nan, ppl=nan, wps=3660.8, ups=0.18, wpb=20396.5, bsz=253.8, num_updates=39300, lr=0.000178344, gnorm=2.022, clip=0, loss_scale=819, train_wall=372, wall=0
2022-08-16 01:54:49 | INFO | train_inner | epoch 036:    182 / 1122 loss=9.258, nll_loss=2.967, mask_ins=2.184, word_ins_ml=4.549, word_reposition=1.779, kpe=0.746, ppl=612.25, wps=4856.1, ups=0.24, wpb=20591.2, bsz=256, num_updates=39400, lr=0.000178118, gnorm=1.935, clip=0, loss_scale=1024, train_wall=372, wall=0
2022-08-16 01:56:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-16 02:01:54 | INFO | train_inner | epoch 036:    283 / 1122 loss=9.258, nll_loss=2.985, mask_ins=2.162, word_ins_ml=4.565, word_reposition=1.781, kpe=0.751, ppl=612.39, wps=4839.2, ups=0.24, wpb=20569.5, bsz=256, num_updates=39500, lr=0.000177892, gnorm=1.984, clip=0, loss_scale=634, train_wall=373, wall=0
2022-08-16 02:08:56 | INFO | train_inner | epoch 036:    383 / 1122 loss=9.236, nll_loss=2.962, mask_ins=2.17, word_ins_ml=4.544, word_reposition=1.772, kpe=0.75, ppl=602.94, wps=4877.1, ups=0.24, wpb=20583.6, bsz=256, num_updates=39600, lr=0.000177667, gnorm=1.983, clip=0, loss_scale=512, train_wall=370, wall=0
2022-08-16 02:15:56 | INFO | train_inner | epoch 036:    483 / 1122 loss=9.263, nll_loss=2.975, mask_ins=2.171, word_ins_ml=4.555, word_reposition=1.786, kpe=0.75, ppl=614.28, wps=4859.5, ups=0.24, wpb=20442.8, bsz=256, num_updates=39700, lr=0.000177443, gnorm=1.914, clip=0, loss_scale=512, train_wall=368, wall=0
2022-08-16 02:23:01 | INFO | train_inner | epoch 036:    583 / 1122 loss=9.249, nll_loss=2.974, mask_ins=2.163, word_ins_ml=4.555, word_reposition=1.783, kpe=0.749, ppl=608.65, wps=4819.3, ups=0.24, wpb=20456.6, bsz=256, num_updates=39800, lr=0.00017722, gnorm=1.934, clip=0, loss_scale=512, train_wall=370, wall=0
2022-08-16 02:30:05 | INFO | train_inner | epoch 036:    683 / 1122 loss=nan, nll_loss=2.982, mask_ins=2.178, word_ins_ml=4.562, word_reposition=1.794, kpe=nan, ppl=nan, wps=4854, ups=0.24, wpb=20600.5, bsz=256, num_updates=39900, lr=0.000176998, gnorm=1.902, clip=0, loss_scale=512, train_wall=370, wall=0
2022-08-16 02:37:05 | INFO | train_inner | epoch 036:    783 / 1122 loss=9.24, nll_loss=2.956, mask_ins=2.157, word_ins_ml=4.539, word_reposition=1.792, kpe=0.753, ppl=604.83, wps=4877.2, ups=0.24, wpb=20469.7, bsz=256, num_updates=40000, lr=0.000176777, gnorm=1.895, clip=0, loss_scale=845, train_wall=367, wall=0
2022-08-16 02:44:08 | INFO | train_inner | epoch 036:    883 / 1122 loss=9.271, nll_loss=2.98, mask_ins=2.168, word_ins_ml=4.56, word_reposition=1.786, kpe=0.756, ppl=617.61, wps=4828.4, ups=0.24, wpb=20436.5, bsz=256, num_updates=40100, lr=0.000176556, gnorm=1.958, clip=0, loss_scale=1024, train_wall=370, wall=0
2022-08-16 02:51:15 | INFO | train_inner | epoch 036:    983 / 1122 loss=9.301, nll_loss=2.969, mask_ins=2.172, word_ins_ml=4.55, word_reposition=1.822, kpe=0.757, ppl=630.91, wps=4836, ups=0.23, wpb=20656.3, bsz=256, num_updates=40200, lr=0.000176336, gnorm=1.952, clip=0, loss_scale=1024, train_wall=375, wall=0
2022-08-16 02:59:15 | INFO | train_inner | epoch 036:   1083 / 1122 loss=9.234, nll_loss=2.958, mask_ins=2.159, word_ins_ml=4.54, word_reposition=1.778, kpe=0.757, ppl=602.34, wps=4282.8, ups=0.21, wpb=20544, bsz=256, num_updates=40300, lr=0.000176117, gnorm=2.006, clip=0, loss_scale=1024, train_wall=424, wall=0
2022-08-16 03:01:54 | INFO | train | epoch 036 | loss nan | nll_loss 2.971 | mask_ins 2.169 | word_ins_ml 4.552 | word_reposition 1.786 | kpe nan | ppl nan | wps 4664.2 | ups 0.23 | wpb 20521 | bsz 255.8 | num_updates 40339 | lr 0.000176032 | gnorm 1.954 | clip 0 | loss_scale 780 | train_wall 4205 | wall 0
2022-08-16 03:03:51 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 15.718 | nll_loss 7.198 | mask_ins 3.048 | word_ins_ml 8.439 | word_reposition 2.739 | kpe 1.491 | ppl 53883.3 | wps 8451 | wpb 2367.6 | bsz 32 | num_updates 40339 | best_loss 10.679
2022-08-16 03:04:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 36 @ 40339 updates, score 15.718) (writing took 18.43056231737137 seconds)
2022-08-16 03:08:28 | INFO | train_inner | epoch 037:     61 / 1122 loss=9.264, nll_loss=2.996, mask_ins=2.17, word_ins_ml=4.575, word_reposition=1.77, kpe=0.748, ppl=614.6, wps=3666.3, ups=0.18, wpb=20280.5, bsz=253.8, num_updates=40400, lr=0.000175899, gnorm=2.002, clip=0, loss_scale=1024, train_wall=367, wall=0
2022-08-16 03:14:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-16 03:15:41 | INFO | train_inner | epoch 037:    162 / 1122 loss=nan, nll_loss=2.927, mask_ins=2.153, word_ins_ml=4.513, word_reposition=1.758, kpe=nan, ppl=nan, wps=4743.7, ups=0.23, wpb=20535.4, bsz=256, num_updates=40500, lr=0.000175682, gnorm=1.962, clip=0, loss_scale=1409, train_wall=375, wall=0
2022-08-16 03:22:43 | INFO | train_inner | epoch 037:    262 / 1122 loss=9.209, nll_loss=2.953, mask_ins=2.159, word_ins_ml=4.536, word_reposition=1.77, kpe=0.743, ppl=591.74, wps=4881.4, ups=0.24, wpb=20600.7, bsz=256, num_updates=40600, lr=0.000175466, gnorm=2.034, clip=0, loss_scale=1024, train_wall=368, wall=0
2022-08-16 03:29:46 | INFO | train_inner | epoch 037:    362 / 1122 loss=9.199, nll_loss=2.946, mask_ins=2.158, word_ins_ml=4.53, word_reposition=1.766, kpe=0.744, ppl=587.74, wps=4872.2, ups=0.24, wpb=20579.1, bsz=256, num_updates=40700, lr=0.00017525, gnorm=2.026, clip=0, loss_scale=1024, train_wall=370, wall=0
2022-08-16 03:36:45 | INFO | train_inner | epoch 037:    462 / 1122 loss=9.244, nll_loss=2.964, mask_ins=2.169, word_ins_ml=4.546, word_reposition=1.787, kpe=0.742, ppl=606.54, wps=4897.2, ups=0.24, wpb=20516.4, bsz=256, num_updates=40800, lr=0.000175035, gnorm=2.185, clip=0, loss_scale=1024, train_wall=368, wall=0
2022-08-16 03:43:52 | INFO | train_inner | epoch 037:    562 / 1122 loss=9.253, nll_loss=2.969, mask_ins=2.176, word_ins_ml=4.551, word_reposition=1.777, kpe=0.749, ppl=610.21, wps=4794.6, ups=0.23, wpb=20513.5, bsz=256, num_updates=40900, lr=0.000174821, gnorm=2.132, clip=0, loss_scale=1024, train_wall=374, wall=0
2022-08-16 03:50:53 | INFO | train_inner | epoch 037:    662 / 1122 loss=9.284, nll_loss=2.984, mask_ins=2.175, word_ins_ml=4.564, word_reposition=1.791, kpe=0.753, ppl=623.37, wps=4901.5, ups=0.24, wpb=20602.5, bsz=256, num_updates=41000, lr=0.000174608, gnorm=2.294, clip=0, loss_scale=1065, train_wall=369, wall=0
2022-08-16 03:57:59 | INFO | train_inner | epoch 037:    762 / 1122 loss=9.272, nll_loss=2.995, mask_ins=2.167, word_ins_ml=4.573, word_reposition=1.784, kpe=0.748, ppl=618.12, wps=4827.7, ups=0.23, wpb=20575.8, bsz=256, num_updates=41100, lr=0.000174395, gnorm=2.024, clip=0, loss_scale=2048, train_wall=372, wall=0
2022-08-16 04:04:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-16 04:05:09 | INFO | train_inner | epoch 037:    863 / 1122 loss=9.179, nll_loss=2.928, mask_ins=2.16, word_ins_ml=4.514, word_reposition=1.761, kpe=0.744, ppl=579.68, wps=4774.8, ups=0.23, wpb=20545, bsz=256, num_updates=41200, lr=0.000174183, gnorm=2, clip=0, loss_scale=1926, train_wall=376, wall=0
2022-08-16 04:10:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-16 04:12:13 | INFO | train_inner | epoch 037:    964 / 1122 loss=9.24, nll_loss=2.959, mask_ins=2.156, word_ins_ml=4.542, word_reposition=1.796, kpe=0.747, ppl=604.84, wps=4844.8, ups=0.24, wpb=20551.5, bsz=256, num_updates=41300, lr=0.000173972, gnorm=2.049, clip=0, loss_scale=902, train_wall=373, wall=0
2022-08-16 04:19:10 | INFO | train_inner | epoch 037:   1064 / 1122 loss=nan, nll_loss=2.946, mask_ins=2.157, word_ins_ml=4.53, word_reposition=1.765, kpe=nan, ppl=nan, wps=4906.5, ups=0.24, wpb=20461.2, bsz=256, num_updates=41400, lr=0.000173762, gnorm=1.945, clip=0, loss_scale=512, train_wall=367, wall=0
2022-08-16 04:23:38 | INFO | train | epoch 037 | loss nan | nll_loss 2.958 | mask_ins 2.162 | word_ins_ml 4.541 | word_reposition 1.775 | kpe nan | ppl nan | wps 4682.1 | ups 0.23 | wpb 20520 | bsz 255.8 | num_updates 41458 | lr 0.00017364 | gnorm 2.06 | clip 0 | loss_scale 1152 | train_wall 4177 | wall 0
2022-08-16 04:25:36 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 15.975 | nll_loss 7.309 | mask_ins 3.15 | word_ins_ml 8.541 | word_reposition 2.775 | kpe 1.509 | ppl 64416.7 | wps 8438.3 | wpb 2367.6 | bsz 32 | num_updates 41458 | best_loss 10.679
2022-08-16 04:25:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 37 @ 41458 updates, score 15.975) (writing took 16.154470497742295 seconds)
2022-08-16 04:29:11 | INFO | train_inner | epoch 038:     42 / 1122 loss=9.19, nll_loss=2.931, mask_ins=2.148, word_ins_ml=4.517, word_reposition=1.784, kpe=0.742, ppl=584.09, wps=3389.2, ups=0.17, wpb=20339.2, bsz=253.8, num_updates=41500, lr=0.000173553, gnorm=2.056, clip=0, loss_scale=512, train_wall=416, wall=0
2022-08-16 04:36:06 | INFO | train_inner | epoch 038:    142 / 1122 loss=nan, nll_loss=2.952, mask_ins=2.169, word_ins_ml=4.535, word_reposition=1.801, kpe=nan, ppl=nan, wps=4957.4, ups=0.24, wpb=20602.9, bsz=256, num_updates=41600, lr=0.000173344, gnorm=1.953, clip=0, loss_scale=512, train_wall=366, wall=0
2022-08-16 04:43:13 | INFO | train_inner | epoch 038:    242 / 1122 loss=9.229, nll_loss=2.96, mask_ins=2.161, word_ins_ml=4.542, word_reposition=1.79, kpe=0.735, ppl=599.91, wps=4836.7, ups=0.23, wpb=20646.4, bsz=256, num_updates=41700, lr=0.000173136, gnorm=2.029, clip=0, loss_scale=512, train_wall=372, wall=0
2022-08-16 04:50:21 | INFO | train_inner | epoch 038:    342 / 1122 loss=9.106, nll_loss=2.89, mask_ins=2.157, word_ins_ml=4.48, word_reposition=1.735, kpe=0.734, ppl=550.98, wps=4766.4, ups=0.23, wpb=20389, bsz=256, num_updates=41800, lr=0.000172929, gnorm=1.996, clip=0, loss_scale=573, train_wall=373, wall=0
2022-08-16 04:57:23 | INFO | train_inner | epoch 038:    442 / 1122 loss=9.217, nll_loss=2.956, mask_ins=2.152, word_ins_ml=4.539, word_reposition=1.788, kpe=0.737, ppl=595.12, wps=4843.5, ups=0.24, wpb=20424.1, bsz=256, num_updates=41900, lr=0.000172722, gnorm=2.033, clip=0, loss_scale=1024, train_wall=370, wall=0
2022-08-16 05:04:26 | INFO | train_inner | epoch 038:    542 / 1122 loss=9.156, nll_loss=2.929, mask_ins=2.151, word_ins_ml=4.515, word_reposition=1.754, kpe=0.736, ppl=570.51, wps=4837.5, ups=0.24, wpb=20461.8, bsz=256, num_updates=42000, lr=0.000172516, gnorm=2.129, clip=0, loss_scale=1024, train_wall=371, wall=0
2022-08-16 05:11:26 | INFO | train_inner | epoch 038:    642 / 1122 loss=9.22, nll_loss=2.971, mask_ins=2.153, word_ins_ml=4.552, word_reposition=1.769, kpe=0.746, ppl=596.26, wps=4900.7, ups=0.24, wpb=20584.5, bsz=256, num_updates=42100, lr=0.000172311, gnorm=2.138, clip=0, loss_scale=1024, train_wall=368, wall=0
2022-08-16 05:18:23 | INFO | train_inner | epoch 038:    742 / 1122 loss=9.215, nll_loss=2.944, mask_ins=2.156, word_ins_ml=4.528, word_reposition=1.792, kpe=0.74, ppl=594.38, wps=4926.1, ups=0.24, wpb=20564.9, bsz=256, num_updates=42200, lr=0.000172107, gnorm=2.054, clip=0, loss_scale=1024, train_wall=368, wall=0
2022-08-16 05:25:20 | INFO | train_inner | epoch 038:    842 / 1122 loss=9.155, nll_loss=2.912, mask_ins=2.141, word_ins_ml=4.499, word_reposition=1.772, kpe=0.742, ppl=569.92, wps=4925.8, ups=0.24, wpb=20557.5, bsz=256, num_updates=42300, lr=0.000171904, gnorm=1.968, clip=0, loss_scale=1024, train_wall=367, wall=0
2022-08-16 05:32:20 | INFO | train_inner | epoch 038:    942 / 1122 loss=9.197, nll_loss=2.945, mask_ins=2.151, word_ins_ml=4.529, word_reposition=1.774, kpe=0.742, ppl=586.74, wps=4901.8, ups=0.24, wpb=20586.9, bsz=256, num_updates=42400, lr=0.000171701, gnorm=2.052, clip=0, loss_scale=2048, train_wall=369, wall=0
2022-08-16 05:32:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-16 05:39:27 | INFO | train_inner | epoch 038:   1043 / 1122 loss=nan, nll_loss=2.924, mask_ins=2.144, word_ins_ml=4.511, word_reposition=1.745, kpe=nan, ppl=nan, wps=4804.4, ups=0.23, wpb=20488.5, bsz=256, num_updates=42500, lr=0.000171499, gnorm=2.002, clip=0, loss_scale=1095, train_wall=373, wall=0
2022-08-16 05:45:04 | INFO | train | epoch 038 | loss nan | nll_loss 2.939 | mask_ins 2.153 | word_ins_ml 4.524 | word_reposition 1.773 | kpe nan | ppl nan | wps 4708.4 | ups 0.23 | wpb 20521.3 | bsz 255.8 | num_updates 42579 | lr 0.000171339 | gnorm 2.035 | clip 0 | loss_scale 971 | train_wall 4168 | wall 0
2022-08-16 05:47:04 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 15.999 | nll_loss 7.359 | mask_ins 3.135 | word_ins_ml 8.586 | word_reposition 2.762 | kpe 1.516 | ppl 65491.3 | wps 8287.8 | wpb 2367.6 | bsz 32 | num_updates 42579 | best_loss 10.679
2022-08-16 05:47:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 38 @ 42579 updates, score 15.999) (writing took 11.61637020483613 seconds)
2022-08-16 05:48:43 | INFO | train_inner | epoch 039:     21 / 1122 loss=9.184, nll_loss=2.94, mask_ins=2.147, word_ins_ml=4.524, word_reposition=1.775, kpe=0.738, ppl=581.53, wps=3678.3, ups=0.18, wpb=20465.9, bsz=253.8, num_updates=42600, lr=0.000171297, gnorm=2.02, clip=0, loss_scale=1024, train_wall=372, wall=0
2022-08-16 05:56:37 | INFO | train_inner | epoch 039:    121 / 1122 loss=9.136, nll_loss=2.912, mask_ins=2.151, word_ins_ml=4.5, word_reposition=1.764, kpe=0.721, ppl=562.66, wps=4315, ups=0.21, wpb=20429.3, bsz=256, num_updates=42700, lr=0.000171096, gnorm=1.982, clip=0, loss_scale=1024, train_wall=420, wall=0
2022-08-16 05:56:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-16 06:03:43 | INFO | train_inner | epoch 039:    222 / 1122 loss=9.169, nll_loss=2.924, mask_ins=2.146, word_ins_ml=4.511, word_reposition=1.784, kpe=0.729, ppl=575.82, wps=4813.7, ups=0.23, wpb=20510.1, bsz=256, num_updates=42800, lr=0.000170896, gnorm=2.038, clip=0, loss_scale=522, train_wall=373, wall=0
2022-08-16 06:10:51 | INFO | train_inner | epoch 039:    322 / 1122 loss=9.157, nll_loss=2.923, mask_ins=2.147, word_ins_ml=4.509, word_reposition=1.772, kpe=0.728, ppl=570.8, wps=4805.9, ups=0.23, wpb=20561, bsz=256, num_updates=42900, lr=0.000170697, gnorm=1.992, clip=0, loss_scale=512, train_wall=374, wall=0
2022-08-16 06:17:52 | INFO | train_inner | epoch 039:    422 / 1122 loss=9.14, nll_loss=2.921, mask_ins=2.146, word_ins_ml=4.508, word_reposition=1.752, kpe=0.734, ppl=564.31, wps=4864.7, ups=0.24, wpb=20494.8, bsz=256, num_updates=43000, lr=0.000170499, gnorm=2.241, clip=0, loss_scale=512, train_wall=369, wall=0
2022-08-16 06:24:55 | INFO | train_inner | epoch 039:    522 / 1122 loss=9.168, nll_loss=2.938, mask_ins=2.146, word_ins_ml=4.522, word_reposition=1.768, kpe=0.733, ppl=575.4, wps=4861.7, ups=0.24, wpb=20588.2, bsz=256, num_updates=43100, lr=0.000170301, gnorm=2.059, clip=0, loss_scale=512, train_wall=371, wall=0
2022-08-16 06:32:01 | INFO | train_inner | epoch 039:    622 / 1122 loss=nan, nll_loss=2.945, mask_ins=2.151, word_ins_ml=4.529, word_reposition=1.769, kpe=nan, ppl=nan, wps=4793.2, ups=0.24, wpb=20387.3, bsz=256, num_updates=43200, lr=0.000170103, gnorm=1.965, clip=0, loss_scale=512, train_wall=371, wall=0
2022-08-16 06:39:05 | INFO | train_inner | epoch 039:    722 / 1122 loss=9.09, nll_loss=2.895, mask_ins=2.138, word_ins_ml=4.485, word_reposition=1.735, kpe=0.733, ppl=544.88, wps=4849.8, ups=0.24, wpb=20585.9, bsz=256, num_updates=43300, lr=0.000169907, gnorm=1.994, clip=0, loss_scale=957, train_wall=370, wall=0
2022-08-16 06:46:11 | INFO | train_inner | epoch 039:    822 / 1122 loss=nan, nll_loss=2.909, mask_ins=2.15, word_ins_ml=4.497, word_reposition=1.758, kpe=nan, ppl=nan, wps=4837.5, ups=0.23, wpb=20591.4, bsz=256, num_updates=43400, lr=0.000169711, gnorm=1.95, clip=0, loss_scale=1024, train_wall=374, wall=0
2022-08-16 06:53:15 | INFO | train_inner | epoch 039:    922 / 1122 loss=9.122, nll_loss=2.908, mask_ins=2.142, word_ins_ml=4.496, word_reposition=1.747, kpe=0.736, ppl=557.17, wps=4848.2, ups=0.24, wpb=20547.1, bsz=256, num_updates=43500, lr=0.000169516, gnorm=2.054, clip=0, loss_scale=1024, train_wall=371, wall=0
2022-08-16 07:00:16 | INFO | train_inner | epoch 039:   1022 / 1122 loss=9.109, nll_loss=2.875, mask_ins=2.139, word_ins_ml=4.466, word_reposition=1.773, kpe=0.731, ppl=552.19, wps=4898.5, ups=0.24, wpb=20637.4, bsz=256, num_updates=43600, lr=0.000169321, gnorm=2.014, clip=0, loss_scale=1024, train_wall=369, wall=0
2022-08-16 07:07:12 | INFO | train_inner | epoch 039:   1122 / 1122 loss=9.11, nll_loss=2.897, mask_ins=2.136, word_ins_ml=4.486, word_reposition=1.754, kpe=0.734, ppl=552.44, wps=4893.1, ups=0.24, wpb=20346.6, bsz=253.8, num_updates=43700, lr=0.000169128, gnorm=2.099, clip=0, loss_scale=1024, train_wall=365, wall=0
2022-08-16 07:07:12 | INFO | train | epoch 039 | loss nan | nll_loss 2.912 | mask_ins 2.145 | word_ins_ml 4.5 | word_reposition 1.762 | kpe nan | ppl nan | wps 4668.2 | ups 0.23 | wpb 20521.2 | bsz 255.8 | num_updates 43700 | lr 0.000169128 | gnorm 2.034 | clip 0 | loss_scale 790 | train_wall 4206 | wall 0
2022-08-16 07:09:10 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 15.811 | nll_loss 7.276 | mask_ins 3.077 | word_ins_ml 8.509 | word_reposition 2.774 | kpe 1.452 | ppl 57486 | wps 8375.4 | wpb 2367.6 | bsz 32 | num_updates 43700 | best_loss 10.679
2022-08-16 07:09:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_shuffle_finetune_cased/checkpoint_last.pt (epoch 39 @ 43700 updates, score 15.811) (writing took 28.143323509022593 seconds)
2022-08-16 07:12:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-16 07:16:43 | INFO | train_inner | epoch 040:    101 / 1122 loss=9.096, nll_loss=2.897, mask_ins=2.141, word_ins_ml=4.487, word_reposition=1.751, kpe=0.717, ppl=547.18, wps=3613.7, ups=0.18, wpb=20634.9, bsz=256, num_updates=43800, lr=0.000168934, gnorm=2.032, clip=0, loss_scale=1156, train_wall=373, wall=0
2022-08-16 07:24:31 | INFO | train_inner | epoch 040:    201 / 1122 loss=9.133, nll_loss=2.936, mask_ins=2.141, word_ins_ml=4.521, word_reposition=1.754, kpe=0.718, ppl=561.64, wps=4384.4, ups=0.21, wpb=20541.4, bsz=256, num_updates=43900, lr=0.000168742, gnorm=2.045, clip=0, loss_scale=1024, train_wall=417, wall=0
2022-08-16 07:31:34 | INFO | train_inner | epoch 040:    301 / 1122 loss=9.145, nll_loss=2.912, mask_ins=2.148, word_ins_ml=4.5, word_reposition=1.78, kpe=0.718, ppl=566.17, wps=4869.4, ups=0.24, wpb=20576.8, bsz=256, num_updates=44000, lr=0.00016855, gnorm=2.077, clip=0, loss_scale=1024, train_wall=370, wall=0
2022-08-16 07:38:31 | INFO | train_inner | epoch 040:    401 / 1122 loss=9.113, nll_loss=2.899, mask_ins=2.133, word_ins_ml=4.488, word_reposition=1.769, kpe=0.722, ppl=553.54, wps=4910.8, ups=0.24, wpb=20456.4, bsz=256, num_updates=44100, lr=0.000168359, gnorm=2.052, clip=0, loss_scale=1024, train_wall=366, wall=0
2022-08-16 07:45:28 | INFO | train_inner | epoch 040:    501 / 1122 loss=nan, nll_loss=2.903, mask_ins=2.134, word_ins_ml=4.492, word_reposition=1.781, kpe=nan, ppl=nan, wps=4906.4, ups=0.24, wpb=20491.5, bsz=256, num_updates=44200, lr=0.000168168, gnorm=2.04, clip=0, loss_scale=1024, train_wall=366, wall=0
2022-08-16 07:52:25 | INFO | train_inner | epoch 040:    601 / 1122 loss=9.094, nll_loss=2.896, mask_ins=2.136, word_ins_ml=4.485, word_reposition=1.749, kpe=0.723, ppl=546.32, wps=4931.6, ups=0.24, wpb=20544.1, bsz=256, num_updates=44300, lr=0.000167978, gnorm=2.028, clip=0, loss_scale=1546, train_wall=366, wall=0
2022-08-16 07:53:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-16 07:59:26 | INFO | train_inner | epoch 040:    702 / 1122 loss=9.073, nll_loss=2.88, mask_ins=2.133, word_ins_ml=4.471, word_reposition=1.743, kpe=0.727, ppl=538.7, wps=4897.5, ups=0.24, wpb=20615.4, bsz=256, num_updates=44400, lr=0.000167789, gnorm=2.051, clip=0, loss_scale=1125, train_wall=370, wall=0
2022-08-16 08:06:28 | INFO | train_inner | epoch 040:    802 / 1122 loss=9.093, nll_loss=2.916, mask_ins=2.145, word_ins_ml=4.503, word_reposition=1.717, kpe=0.728, ppl=546.18, wps=4845.1, ups=0.24, wpb=20466.6, bsz=256, num_updates=44500, lr=0.0001676, gnorm=2.082, clip=0, loss_scale=1024, train_wall=369, wall=0
2022-08-16 08:13:26 | INFO | train_inner | epoch 040:    902 / 1122 loss=9.087, nll_loss=2.87, mask_ins=2.132, word_ins_ml=4.462, word_reposition=1.766, kpe=0.726, ppl=543.67, wps=4916.5, ups=0.24, wpb=20544.1, bsz=256, num_updates=44600, lr=0.000167412, gnorm=2.087, clip=0, loss_scale=1024, train_wall=367, wall=0
2022-08-16 08:20:23 | INFO | train_inner | epoch 040:   1002 / 1122 loss=9.129, nll_loss=2.919, mask_ins=2.14, word_ins_ml=4.506, word_reposition=1.756, kpe=0.728, ppl=559.98, wps=4914.3, ups=0.24, wpb=20492.2, bsz=256, num_updates=44700, lr=0.000167225, gnorm=2.036, clip=0, loss_scale=1024, train_wall=366, wall=0
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
train.sh: line 42: -exit: command not found
