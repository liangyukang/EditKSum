nohup: ignoring input
2022-07-26 11:28:51 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:12973
2022-07-26 11:28:51 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:12973
2022-07-26 11:28:51 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:12973
2022-07-26 11:28:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-26 11:28:51 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:12973
2022-07-26 11:28:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-26 11:28:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-26 11:28:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-26 11:28:52 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-26 11:28:52 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-26 11:28:52 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-26 11:28:52 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-26 11:28:52 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-26 11:28:52 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-26 11:28:52 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-26 11:28:52 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-26 11:28:56 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:12973', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='keyword', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-26 11:28:56 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-26 11:28:56 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-07-26 11:28:56 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.source
2022-07-26 11:28:56 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.target
2022-07-26 11:28:56 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]391it [00:00, 3892.90it/s]385it [00:00, 3843.07it/s]390it [00:00, 3891.92it/s]390it [00:00, 3899.16it/s]781it [00:00, 3545.93it/s]770it [00:00, 3463.59it/s]780it [00:00, 3536.48it/s]780it [00:00, 3470.57it/s]1151it [00:00, 3609.37it/s]1119it [00:00, 3466.29it/s]1136it [00:00, 3537.13it/s]1150it [00:00, 3565.34it/s]1514it [00:00, 3482.94it/s]1491it [00:00, 3437.77it/s]1467it [00:00, 3341.84it/s]1509it [00:00, 3438.41it/s]1908it [00:00, 3636.87it/s]1870it [00:00, 3557.54it/s]1855it [00:00, 3523.75it/s]1886it [00:00, 3547.86it/s]2274it [00:00, 3555.40it/s]2236it [00:00, 3502.92it/s]2243it [00:00, 3476.54it/s]2236it [00:00, 3441.69it/s]2675it [00:00, 3697.03it/s]2641it [00:00, 3673.96it/s]2644it [00:00, 3643.26it/s]2618it [00:00, 3557.76it/s]3023it [00:00, 3718.47it/s]3061it [00:00, 3611.25it/s]3027it [00:00, 3700.08it/s]3009it [00:00, 3665.17it/s]3455it [00:00, 3708.22it/s]3396it [00:00, 3624.72it/s]3399it [00:00, 3593.47it/s]3378it [00:00, 3549.03it/s]3845it [00:01, 3765.06it/s]3778it [00:01, 3665.50it/s]3779it [00:01, 3653.58it/s]3754it [00:01, 3610.39it/s]4146it [00:01, 3582.34it/s]4223it [00:01, 3641.98it/s]4146it [00:01, 3561.60it/s]4117it [00:01, 3510.84it/s]4607it [00:01, 3699.25it/s]4534it [00:01, 3667.67it/s]4530it [00:01, 3640.82it/s]4481it [00:01, 3546.84it/s]4979it [00:01, 3582.08it/s]4902it [00:01, 3509.53it/s]4896it [00:01, 3494.26it/s]4837it [00:01, 3420.14it/s]5362it [00:01, 3653.44it/s]5284it [00:01, 3598.76it/s]5273it [00:01, 3573.39it/s]5216it [00:01, 3525.68it/s]5575it [00:01, 3542.16it/s]5729it [00:01, 3500.02it/s]5646it [00:01, 3487.01it/s]5632it [00:01, 3431.97it/s]6007it [00:01, 3520.55it/s]6081it [00:01, 3488.43it/s]5989it [00:01, 3468.93it/s]5931it [00:01, 3375.83it/s]6361it [00:01, 3504.72it/s]6349it [00:01, 3504.56it/s]6274it [00:01, 3391.23it/s]6432it [00:01, 3349.08it/s]6769it [00:02, 2169.84it/s]6701it [00:02, 2124.15it/s]6713it [00:02, 2090.10it/s]6615it [00:02, 2018.91it/s]7115it [00:02, 2436.46it/s]7057it [00:02, 2413.54it/s]7070it [00:02, 2384.57it/s]6969it [00:02, 2318.48it/s]7413it [00:02, 2560.08it/s]7359it [00:02, 2531.93it/s]7372it [00:02, 2508.03it/s]7305it [00:02, 2546.80it/s]7773it [00:02, 2814.61it/s]7705it [00:02, 2753.92it/s]7733it [00:02, 2770.15it/s]7612it [00:02, 2593.83it/s]8120it [00:02, 2982.88it/s]8061it [00:02, 2958.86it/s]8080it [00:02, 2948.21it/s]7965it [00:02, 2825.68it/s]8444it [00:02, 3000.38it/s]8407it [00:02, 2976.35it/s]8387it [00:02, 2952.75it/s]8279it [00:02, 2829.37it/s]8801it [00:02, 3154.77it/s]8768it [00:02, 3146.90it/s]8744it [00:02, 3119.73it/s]8633it [00:02, 3017.44it/s]9131it [00:02, 3095.28it/s]9073it [00:02, 3093.75it/s]9100it [00:02, 3096.51it/s]8977it [00:02, 3133.79it/s]9490it [00:02, 3233.29it/s]9416it [00:02, 3187.34it/s]9460it [00:02, 3235.19it/s]9304it [00:02, 3086.11it/s]9837it [00:03, 3299.17it/s]9776it [00:03, 3305.11it/s]9822it [00:03, 3344.05it/s]9657it [00:03, 3210.80it/s]10173it [00:03, 3219.33it/s]10114it [00:03, 3223.46it/s]10164it [00:03, 3244.68it/s]9986it [00:03, 3112.28it/s]10532it [00:03, 3323.85it/s]10463it [00:03, 3298.91it/s]10522it [00:03, 3338.67it/s]10339it [00:03, 3228.91it/s]10868it [00:03, 3204.11it/s]10797it [00:03, 3214.89it/s]10861it [00:03, 3250.89it/s]10693it [00:03, 3316.98it/s]11226it [00:03, 3309.05it/s]11154it [00:03, 3315.86it/s]11208it [00:03, 3313.08it/s]11029it [00:03, 3179.32it/s]11560it [00:03, 3160.53it/s]11502it [00:03, 3361.19it/s]11546it [00:03, 3236.88it/s]11383it [00:03, 3281.70it/s]11918it [00:03, 3278.84it/s]11841it [00:03, 3259.28it/s]11905it [00:03, 3337.73it/s]11715it [00:03, 3179.13it/s]12279it [00:03, 3371.97it/s]12203it [00:03, 3361.79it/s]12256it [00:03, 3386.10it/s]12059it [00:03, 3252.58it/s]12619it [00:03, 3245.80it/s]12597it [00:03, 3284.90it/s]12541it [00:03, 3245.46it/s]12387it [00:03, 3173.06it/s]12975it [00:03, 3333.43it/s]12954it [00:03, 3365.99it/s]12901it [00:03, 3345.79it/s]12744it [00:04, 3284.62it/s]13311it [00:04, 3214.74it/s]13238it [00:04, 3248.10it/s]13368it [00:04, 3259.07it/s]
2022-07-26 11:29:00 | INFO | root | success load 13368 data
2022-07-26 11:29:00 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-26 11:29:00 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-26 11:29:00 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-26 11:29:00 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-26 11:29:00 | INFO | transformer.tokenization_utils | loading file None
2022-07-26 11:29:00 | INFO | transformer.tokenization_utils | loading file None
2022-07-26 11:29:00 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
13293it [00:04, 3250.21it/s]13368it [00:04, 3246.74it/s]
13088it [00:04, 3327.73it/s]13368it [00:04, 3236.58it/s]
13368it [00:04, 3170.86it/s]
2022-07-26 11:29:01 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-26 11:29:01 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-26 11:29:01 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-26 11:29:04 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-26 11:29:04 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-26 11:29:04 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-26 11:29:04 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-26 11:29:04 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
2022-07-26 11:29:06 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-26 11:29:06 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-07-26 11:29:06 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-07-26 11:29:06 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-26 11:29:06 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 142456835)
2022-07-26 11:29:06 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-07-26 11:29:06 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-26 11:29:11 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-26 11:29:11 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-26 11:29:13 | INFO | fairseq.trainer | loaded checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_last.pt (epoch 12 @ 13447 updates)
2022-07-26 11:29:13 | INFO | fairseq.trainer | loading train data for epoch 12
2022-07-26 11:29:13 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.source
2022-07-26 11:29:13 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.target
2022-07-26 11:29:13 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]357it [00:00, 3565.26it/s]714it [00:00, 3533.30it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]352it [00:00, 3513.24it/s]1068it [00:00, 3358.40it/s]363it [00:00, 3624.43it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]718it [00:00, 3596.90it/s]1436it [00:00, 3479.24it/s]726it [00:00, 3596.56it/s]357it [00:00, 3560.56it/s]1815it [00:00, 3587.57it/s]1078it [00:00, 3381.77it/s]1086it [00:00, 3409.81it/s]733it [00:00, 3674.91it/s]1436it [00:00, 3455.02it/s]2175it [00:00, 3423.84it/s]1445it [00:00, 3476.92it/s]1101it [00:00, 3406.15it/s]1808it [00:00, 3546.84it/s]2541it [00:00, 3495.94it/s]1823it [00:00, 3583.21it/s]1473it [00:00, 3523.53it/s]2164it [00:00, 3386.27it/s]2893it [00:00, 3376.83it/s]2183it [00:00, 3428.23it/s]1830it [00:00, 3384.56it/s]2531it [00:00, 3474.68it/s]3261it [00:00, 3465.56it/s]2558it [00:00, 3527.61it/s]2211it [00:00, 3521.98it/s]2881it [00:00, 3333.52it/s]3610it [00:01, 3357.57it/s]2913it [00:00, 3408.84it/s]2577it [00:00, 3565.47it/s]3244it [00:00, 3419.86it/s]3962it [00:01, 3404.24it/s]3282it [00:00, 3490.62it/s]2935it [00:00, 3444.71it/s]4327it [00:01, 3476.10it/s]3588it [00:01, 3289.29it/s]3633it [00:01, 3375.65it/s]3300it [00:00, 3498.42it/s]3942it [00:01, 3361.65it/s]4676it [00:01, 3369.96it/s]3996it [00:01, 3447.42it/s]3652it [00:01, 3387.36it/s]4298it [00:01, 3417.45it/s]5036it [00:01, 3434.38it/s]4350it [00:01, 3355.28it/s]4013it [00:01, 3450.58it/s]4642it [00:01, 3325.01it/s]5381it [00:01, 3314.96it/s]4724it [00:01, 3466.25it/s]4360it [00:01, 3355.46it/s]4996it [00:01, 3385.36it/s]5743it [00:01, 3402.33it/s]5088it [00:01, 3515.96it/s]4724it [00:01, 3436.58it/s]6085it [00:01, 3318.14it/s]5336it [00:01, 3216.41it/s]5441it [00:01, 3399.32it/s]5095it [00:01, 3514.47it/s]6450it [00:01, 3407.82it/s]5690it [00:01, 3308.19it/s]5805it [00:01, 3468.77it/s]5448it [00:01, 3383.49it/s]6814it [00:01, 3474.55it/s]6030it [00:01, 3251.76it/s]6154it [00:01, 3367.32it/s]5819it [00:01, 3476.69it/s]6393it [00:01, 3359.69it/s]6529it [00:01, 3476.64it/s]6169it [00:01, 3356.31it/s]6762it [00:01, 3453.79it/s]6879it [00:02, 3361.97it/s]6544it [00:01, 3467.82it/s]6893it [00:02, 3345.06it/s]7163it [00:02, 1082.65it/s]7537it [00:02, 1389.25it/s]7217it [00:02, 1111.71it/s]7848it [00:03, 1630.99it/s]7109it [00:02, 1009.27it/s]7588it [00:02, 1420.70it/s]8222it [00:03, 1984.72it/s]7230it [00:02, 1080.55it/s]7477it [00:03, 1300.16it/s]7896it [00:03, 1661.52it/s]8599it [00:03, 2329.31it/s]7589it [00:02, 1371.88it/s]7781it [00:03, 1534.23it/s]8263it [00:03, 2005.98it/s]8936it [00:03, 2525.95it/s]7900it [00:03, 1618.61it/s]8150it [00:03, 1881.33it/s]8619it [00:03, 2252.11it/s]9306it [00:03, 2799.97it/s]8266it [00:03, 1961.77it/s]8521it [00:03, 2222.82it/s]8987it [00:03, 2558.63it/s]9649it [00:03, 2907.03it/s]8606it [00:03, 2240.05it/s]8853it [00:03, 2416.50it/s]9365it [00:03, 2844.72it/s]10029it [00:03, 3139.23it/s]8927it [00:03, 2425.12it/s]9222it [00:03, 2707.40it/s]9710it [00:03, 2907.39it/s]10379it [00:03, 3164.42it/s]9287it [00:03, 2700.43it/s]9561it [00:03, 2813.89it/s]10056it [00:03, 3049.87it/s]10745it [00:03, 3299.64it/s]9617it [00:03, 2795.83it/s]9931it [00:03, 3040.17it/s]11122it [00:03, 3430.03it/s]10393it [00:03, 3069.69it/s]9979it [00:03, 3009.34it/s]10289it [00:03, 3183.50it/s]10759it [00:03, 3230.55it/s]11479it [00:04, 3374.91it/s]10313it [00:03, 3040.66it/s]10636it [00:03, 3176.33it/s]11853it [00:04, 3478.70it/s]11139it [00:03, 3215.25it/s]10675it [00:03, 3198.93it/s]10995it [00:04, 3290.97it/s]12209it [00:04, 3413.82it/s]11519it [00:04, 3376.43it/s]11048it [00:03, 3346.79it/s]11339it [00:04, 3248.94it/s]12584it [00:04, 3508.16it/s]11881it [00:04, 3444.14it/s]11396it [00:04, 3261.39it/s]11695it [00:04, 3335.59it/s]12939it [00:04, 3443.30it/s]12233it [00:04, 3372.78it/s]11765it [00:04, 3382.20it/s]12037it [00:04, 3281.61it/s]13330it [00:04, 3576.91it/s]12609it [00:04, 3482.64it/s]12111it [00:04, 3295.60it/s]12416it [00:04, 3426.05it/s]13691it [00:04, 3500.31it/s]12962it [00:04, 3428.30it/s]12494it [00:04, 3445.05it/s]12785it [00:04, 3501.14it/s]14066it [00:04, 3572.51it/s]13341it [00:04, 3530.19it/s]12844it [00:04, 3348.58it/s]13139it [00:04, 3443.54it/s]14452it [00:04, 3655.94it/s]13697it [00:04, 3470.94it/s]13235it [00:04, 3508.48it/s]13510it [00:04, 3520.72it/s]14080it [00:04, 3566.46it/s]14820it [00:05, 3555.58it/s]13604it [00:04, 3560.46it/s]13865it [00:04, 3443.60it/s]14471it [00:04, 3665.77it/s]15208it [00:05, 3647.72it/s]13963it [00:04, 3416.74it/s]14244it [00:04, 3543.97it/s]14839it [00:04, 3542.71it/s]15575it [00:05, 3482.36it/s]14351it [00:04, 3547.17it/s]14600it [00:05, 3420.25it/s]15219it [00:05, 3614.84it/s]15954it [00:05, 3569.36it/s]14709it [00:05, 3415.90it/s]14983it [00:05, 3536.54it/s]15582it [00:05, 3501.85it/s]15100it [00:05, 3555.93it/s]15339it [00:05, 3415.95it/s]15934it [00:05, 3453.98it/s]15459it [00:05, 3411.20it/s]15706it [00:05, 3486.91it/s]15835it [00:05, 3509.60it/s]16066it [00:05, 3517.72it/s]16189it [00:05, 3351.52it/s]16313it [00:06, 1016.52it/s]16705it [00:06, 1321.72it/s]17088it [00:06, 1617.54it/s]16420it [00:06, 1080.42it/s]17472it [00:06, 1961.17it/s]16281it [00:06, 878.62it/s] 16801it [00:06, 1390.56it/s]17846it [00:06, 2282.97it/s]16674it [00:06, 1167.31it/s]17125it [00:06, 1648.31it/s]18192it [00:06, 2493.00it/s]17057it [00:06, 1484.09it/s]17491it [00:06, 1982.55it/s]18574it [00:06, 2790.92it/s]17378it [00:06, 1719.41it/s]17870it [00:06, 2328.74it/s]16527it [00:06, 825.67it/s] 18927it [00:07, 2910.65it/s]17764it [00:06, 2083.83it/s]18210it [00:06, 2515.25it/s]16911it [00:06, 1096.59it/s]19312it [00:07, 3147.39it/s]18101it [00:06, 2321.57it/s]18590it [00:06, 2813.02it/s]17222it [00:06, 1325.54it/s]19669it [00:07, 3155.96it/s]18477it [00:07, 2634.16it/s]18937it [00:07, 2891.03it/s]17607it [00:06, 1678.37it/s]20054it [00:07, 3340.32it/s]18823it [00:07, 2760.26it/s]19315it [00:07, 3118.45it/s]17928it [00:07, 1933.39it/s]20434it [00:07, 3466.32it/s]19202it [00:07, 3015.94it/s]18315it [00:07, 2304.82it/s]19664it [00:07, 3133.76it/s]20798it [00:07, 3393.91it/s]19579it [00:07, 3212.88it/s]18680it [00:07, 2593.63it/s]20028it [00:07, 3268.46it/s]21180it [00:07, 3511.74it/s]19937it [00:07, 3205.86it/s]20398it [00:07, 3387.32it/s]19028it [00:07, 2744.65it/s]21541it [00:07, 3412.67it/s]20314it [00:07, 3359.77it/s]19408it [00:07, 3004.34it/s]20751it [00:07, 3298.40it/s]21922it [00:07, 3522.32it/s]20670it [00:07, 3293.55it/s]21123it [00:07, 3415.65it/s]19760it [00:07, 3061.08it/s]22280it [00:07, 3440.42it/s]21041it [00:07, 3408.15it/s]20140it [00:07, 3257.16it/s]21473it [00:07, 3349.69it/s]22674it [00:08, 3582.09it/s]21393it [00:07, 3334.71it/s]21835it [00:07, 3426.35it/s]20494it [00:07, 3190.11it/s]23036it [00:08, 3466.20it/s]21776it [00:07, 3474.68it/s]22183it [00:08, 3336.64it/s]20870it [00:07, 3344.89it/s]23407it [00:08, 3533.96it/s]22130it [00:08, 3368.70it/s]21248it [00:07, 3466.56it/s]22552it [00:08, 3435.49it/s]23784it [00:08, 3599.66it/s]22505it [00:08, 3475.77it/s]22928it [00:08, 3528.99it/s]21607it [00:08, 3390.94it/s]24146it [00:08, 3497.45it/s]22894it [00:08, 3593.49it/s]21987it [00:08, 3506.64it/s]23284it [00:08, 3391.12it/s]24522it [00:08, 3571.38it/s]23257it [00:08, 3452.03it/s]23653it [00:08, 3475.28it/s]22345it [00:08, 3351.20it/s]24881it [00:08, 3444.70it/s]23632it [00:08, 3534.83it/s]24003it [00:08, 3382.13it/s]22731it [00:08, 3492.94it/s]25257it [00:08, 3532.43it/s]23989it [00:08, 3442.88it/s]24364it [00:08, 3445.87it/s]23086it [00:08, 3404.72it/s]25612it [00:08, 3440.41it/s]24359it [00:08, 3515.20it/s]24711it [00:08, 3338.18it/s]23456it [00:08, 3486.03it/s]25984it [00:09, 3519.39it/s]24713it [00:08, 3422.87it/s]25069it [00:08, 3401.47it/s]23808it [00:08, 3385.42it/s]26338it [00:09, 3417.83it/s]25071it [00:08, 3466.19it/s]25437it [00:08, 3481.48it/s]24165it [00:08, 3437.20it/s]26706it [00:09, 3491.60it/s]25448it [00:09, 3554.37it/s]25787it [00:09, 3361.34it/s]24541it [00:08, 3529.95it/s]27087it [00:09, 3582.87it/s]25805it [00:09, 3463.89it/s]26151it [00:09, 3438.91it/s]24896it [00:09, 3408.31it/s]27447it [00:09, 3492.85it/s]26173it [00:09, 3524.77it/s]26497it [00:09, 3344.98it/s]25270it [00:09, 3501.10it/s]27826it [00:09, 3578.53it/s]26527it [00:09, 3390.93it/s]26854it [00:09, 3408.66it/s]25622it [00:09, 3399.99it/s]26899it [00:09, 3484.19it/s]27197it [00:09, 3345.43it/s]25977it [00:09, 3441.02it/s]27249it [00:09, 3409.40it/s]27561it [00:09, 3429.92it/s]26328it [00:09, 3338.50it/s]27592it [00:09, 3392.98it/s]27938it [00:09, 3529.22it/s]26703it [00:09, 3454.73it/s]27976it [00:09, 3522.58it/s]27083it [00:09, 3552.07it/s]27440it [00:09, 3452.69it/s]27815it [00:09, 3537.57it/s]28185it [00:10, 917.37it/s] 28550it [00:10, 1181.64it/s]28917it [00:10, 1457.53it/s]29301it [00:10, 1803.75it/s]29676it [00:11, 2138.72it/s]30017it [00:11, 2357.92it/s]30386it [00:11, 2646.98it/s]30731it [00:11, 2784.55it/s]28292it [00:11, 676.61it/s] 31104it [00:11, 3020.06it/s]28171it [00:11, 828.30it/s] 28330it [00:11, 699.96it/s] 28658it [00:11, 899.09it/s]28529it [00:11, 1072.68it/s]31452it [00:11, 3065.73it/s]28679it [00:11, 914.03it/s]28957it [00:11, 1100.31it/s]28903it [00:11, 1372.75it/s]31833it [00:11, 3264.87it/s]28991it [00:11, 1131.95it/s]29327it [00:11, 1414.50it/s]32206it [00:11, 3392.02it/s]29369it [00:11, 1459.39it/s]29217it [00:11, 1613.59it/s]29689it [00:11, 1738.75it/s]29735it [00:11, 1789.58it/s]29586it [00:11, 1955.63it/s]32564it [00:11, 3326.64it/s]30018it [00:11, 1990.76it/s]29915it [00:11, 2178.90it/s]30067it [00:11, 2023.81it/s]32934it [00:11, 3431.09it/s]30386it [00:11, 2324.25it/s]30289it [00:11, 2507.98it/s]30434it [00:11, 2350.05it/s]33287it [00:12, 3359.15it/s]30724it [00:11, 2493.51it/s]30770it [00:11, 2539.04it/s]30627it [00:11, 2659.65it/s]33663it [00:12, 3472.60it/s]31089it [00:12, 2760.32it/s]31134it [00:12, 2800.36it/s]30999it [00:11, 2918.54it/s]34016it [00:12, 3388.19it/s]31439it [00:12, 2861.02it/s]31360it [00:12, 3096.45it/s]31475it [00:12, 2882.42it/s]34399it [00:12, 3512.86it/s]31811it [00:12, 3081.80it/s]31854it [00:12, 3117.59it/s]31708it [00:12, 3111.60it/s]34775it [00:12, 3582.70it/s]32187it [00:12, 3262.04it/s]32222it [00:12, 3269.74it/s]32081it [00:12, 3279.90it/s]35136it [00:12, 3462.50it/s]32540it [00:12, 3221.62it/s]32575it [00:12, 3248.57it/s]32430it [00:12, 3227.28it/s]35508it [00:12, 3535.52it/s]32902it [00:12, 3330.57it/s]32931it [00:12, 3335.21it/s]32793it [00:12, 3338.42it/s]35864it [00:12, 3402.33it/s]33249it [00:12, 3262.93it/s]33278it [00:12, 3288.69it/s]33138it [00:12, 3274.25it/s]36244it [00:12, 3514.75it/s]33612it [00:12, 3364.50it/s]33641it [00:12, 3384.14it/s]33509it [00:12, 3395.58it/s]36598it [00:13, 3405.64it/s]33958it [00:12, 3307.43it/s]33871it [00:12, 3457.97it/s]33987it [00:12, 3314.15it/s]36969it [00:13, 3492.53it/s]34329it [00:13, 3422.02it/s]34365it [00:13, 3446.11it/s]34222it [00:12, 3374.42it/s]37321it [00:13, 3379.73it/s]34696it [00:13, 3493.36it/s]34726it [00:13, 3490.99it/s]34592it [00:12, 3466.97it/s]37690it [00:13, 3466.52it/s]35049it [00:13, 3372.35it/s]35079it [00:13, 3384.22it/s]34942it [00:13, 3352.97it/s]38068it [00:13, 3555.18it/s]35417it [00:13, 3459.39it/s]35444it [00:13, 3459.01it/s]35303it [00:13, 3425.66it/s]38425it [00:13, 3438.82it/s]35766it [00:13, 3359.48it/s]35793it [00:13, 3369.64it/s]35648it [00:13, 3339.12it/s]38795it [00:13, 3512.03it/s]36132it [00:13, 3445.10it/s]36158it [00:13, 3449.15it/s]36022it [00:13, 3452.04it/s]39148it [00:13, 3391.46it/s]36479it [00:13, 3327.86it/s]36392it [00:13, 3521.34it/s]36505it [00:13, 3310.75it/s]39527it [00:13, 3505.08it/s]36843it [00:13, 3415.40it/s]36875it [00:13, 3420.02it/s]36746it [00:13, 3363.60it/s]39880it [00:13, 3413.86it/s]37208it [00:13, 3482.86it/s]37237it [00:13, 3476.49it/s]37113it [00:13, 3450.02it/s]40257it [00:14, 3512.27it/s]37558it [00:13, 3340.44it/s]37587it [00:13, 3372.07it/s]37460it [00:13, 3338.68it/s]40627it [00:14, 3566.12it/s]37921it [00:14, 3423.21it/s]37947it [00:14, 3435.43it/s]37828it [00:13, 3435.67it/s]40985it [00:14, 3430.82it/s]38266it [00:14, 3345.99it/s]38292it [00:14, 3370.78it/s]38174it [00:14, 3326.61it/s]41363it [00:14, 3530.70it/s]38611it [00:14, 3375.44it/s]38631it [00:14, 3308.45it/s]38529it [00:14, 3390.20it/s]41718it [00:14, 3430.66it/s]38977it [00:14, 3457.36it/s]38904it [00:14, 3493.37it/s]38998it [00:14, 3289.46it/s]42093it [00:14, 3521.10it/s]39324it [00:14, 3373.23it/s]39377it [00:14, 3430.18it/s]39255it [00:14, 3396.19it/s]42447it [00:14, 3434.77it/s]39689it [00:14, 3453.46it/s]39750it [00:14, 3514.57it/s]39612it [00:14, 3446.09it/s]42817it [00:14, 3509.00it/s]40036it [00:14, 3354.65it/s]40103it [00:14, 3413.31it/s]39958it [00:14, 3353.19it/s]43187it [00:14, 3563.89it/s]40401it [00:14, 3438.85it/s]40329it [00:14, 3456.00it/s]40446it [00:14, 3375.28it/s]40747it [00:14, 3176.04it/s]40785it [00:14, 3303.52it/s]40677it [00:14, 3345.38it/s]41111it [00:15, 3303.89it/s]41162it [00:15, 3436.22it/s]41038it [00:14, 3419.90it/s]41482it [00:15, 3419.13it/s]41414it [00:14, 3517.42it/s]41518it [00:15, 3352.21it/s]41828it [00:15, 3327.65it/s]41892it [00:15, 3462.53it/s]41767it [00:15, 3405.33it/s]42196it [00:15, 3428.07it/s]42266it [00:15, 3540.44it/s]42138it [00:15, 3493.10it/s]42542it [00:15, 3350.14it/s]42489it [00:15, 3367.87it/s]42622it [00:15, 3393.20it/s]42906it [00:15, 3430.86it/s]42857it [00:15, 3454.91it/s]42989it [00:15, 3470.73it/s]43251it [00:15, 3313.73it/s]43205it [00:15, 3342.47it/s]43545it [00:16, 680.41it/s] 43925it [00:16, 911.50it/s]44246it [00:16, 1131.36it/s]44614it [00:16, 1437.75it/s]44948it [00:16, 1703.58it/s]45327it [00:16, 2062.64it/s]45702it [00:17, 2394.91it/s]46051it [00:17, 2565.91it/s]46411it [00:17, 2807.45it/s]43585it [00:17, 653.77it/s] 46756it [00:17, 2865.23it/s]43956it [00:17, 881.34it/s]47131it [00:17, 3091.72it/s]44259it [00:17, 1087.86it/s]47476it [00:17, 3122.80it/s]44629it [00:17, 1403.30it/s]47849it [00:17, 3288.42it/s]43541it [00:17, 551.80it/s] 43338it [00:17, 528.68it/s] 44948it [00:17, 1660.17it/s]48216it [00:17, 3393.40it/s]43915it [00:17, 752.83it/s]43699it [00:17, 711.21it/s]45315it [00:17, 2007.79it/s]48570it [00:17, 3338.15it/s]44227it [00:17, 948.39it/s]44080it [00:17, 952.89it/s]45680it [00:17, 2332.97it/s]48947it [00:18, 3459.27it/s]44591it [00:17, 1232.54it/s]44392it [00:17, 1169.14it/s]46019it [00:17, 2492.73it/s]49301it [00:18, 3382.25it/s]44947it [00:17, 1512.80it/s]44768it [00:17, 1493.11it/s]46380it [00:17, 2753.80it/s]49680it [00:18, 3497.23it/s]45321it [00:17, 1858.51it/s]45099it [00:18, 1760.69it/s]46718it [00:18, 2847.75it/s]50035it [00:18, 3376.44it/s]45693it [00:17, 2196.44it/s]45460it [00:18, 2088.75it/s]47083it [00:18, 3054.38it/s]50407it [00:18, 3472.75it/s]46037it [00:18, 2381.17it/s]45798it [00:18, 2318.92it/s]47442it [00:18, 3198.02it/s]50777it [00:18, 3538.01it/s]46404it [00:18, 2667.62it/s]46157it [00:18, 2600.61it/s]47788it [00:18, 3168.94it/s]51134it [00:18, 3451.84it/s]46746it [00:18, 2783.00it/s]46528it [00:18, 2867.21it/s]48153it [00:18, 3300.35it/s]51507it [00:18, 3529.97it/s]47119it [00:18, 3019.87it/s]46877it [00:18, 2928.51it/s]48497it [00:18, 3252.50it/s]51862it [00:18, 3403.68it/s]47467it [00:18, 3036.93it/s]47256it [00:18, 3151.87it/s]48868it [00:18, 3378.61it/s]52232it [00:18, 3487.36it/s]47836it [00:18, 3209.47it/s]47606it [00:18, 3139.14it/s]49214it [00:18, 3284.06it/s]52583it [00:19, 3396.29it/s]48209it [00:18, 3352.27it/s]47978it [00:18, 3296.36it/s]49583it [00:18, 3397.95it/s]52958it [00:19, 3495.26it/s]48562it [00:18, 3294.41it/s]48326it [00:19, 3244.70it/s]49951it [00:19, 3478.02it/s]53332it [00:19, 3564.22it/s]48919it [00:18, 3370.26it/s]48705it [00:19, 3394.69it/s]50303it [00:19, 3367.70it/s]53690it [00:19, 3415.66it/s]49266it [00:19, 3304.04it/s]49074it [00:19, 3478.25it/s]50666it [00:19, 3442.21it/s]54069it [00:19, 3520.63it/s]49637it [00:19, 3419.02it/s]49430it [00:19, 3386.37it/s]51013it [00:19, 3323.13it/s]54423it [00:19, 3430.73it/s]49987it [00:19, 3329.47it/s]49797it [00:19, 3466.40it/s]51377it [00:19, 3413.23it/s]54795it [00:19, 3512.22it/s]50358it [00:19, 3436.03it/s]50148it [00:19, 3382.25it/s]51721it [00:19, 3319.21it/s]55148it [00:19, 3377.17it/s]50718it [00:19, 3481.00it/s]50508it [00:19, 3442.26it/s]52088it [00:19, 3410.98it/s]55514it [00:19, 3456.64it/s]51069it [00:19, 3388.76it/s]50855it [00:19, 3358.98it/s]52443it [00:19, 3451.01it/s]55868it [00:20, 3361.06it/s]51441it [00:19, 3481.93it/s]51223it [00:19, 3449.78it/s]52790it [00:19, 3342.73it/s]56247it [00:20, 3481.34it/s]51792it [00:19, 3371.73it/s]51596it [00:19, 3529.13it/s]53156it [00:19, 3433.40it/s]56621it [00:20, 3555.64it/s]52160it [00:19, 3459.88it/s]51951it [00:20, 3386.05it/s]53501it [00:20, 3329.87it/s]56979it [00:20, 3424.43it/s]52508it [00:20, 3356.76it/s]52328it [00:20, 3495.31it/s]53865it [00:20, 3416.66it/s]57356it [00:20, 3521.51it/s]52881it [00:20, 3462.14it/s]52680it [00:20, 3360.52it/s]54209it [00:20, 3314.77it/s]57710it [00:20, 3421.24it/s]53248it [00:20, 3521.52it/s]53056it [00:20, 3473.85it/s]54575it [00:20, 3413.60it/s]58086it [00:20, 3516.73it/s]53602it [00:20, 3363.68it/s]53406it [00:20, 3372.03it/s]54938it [00:20, 3476.14it/s]58440it [00:20, 3401.64it/s]53977it [00:20, 3464.53it/s]53774it [00:20, 3457.26it/s]55287it [00:20, 3345.40it/s]58801it [00:20, 3458.73it/s]54326it [00:20, 3380.33it/s]54154it [00:20, 3554.01it/s]55634it [00:20, 3380.84it/s]59168it [00:20, 3518.66it/s]54694it [00:20, 3464.71it/s]54511it [00:20, 3424.00it/s]55974it [00:20, 3233.28it/s]59522it [00:21, 3400.06it/s]55042it [00:20, 3352.09it/s]54883it [00:20, 3507.48it/s]56344it [00:20, 3365.40it/s]59892it [00:21, 3486.04it/s]55404it [00:20, 3427.66it/s]55236it [00:21, 3349.65it/s]56708it [00:21, 3291.21it/s]60243it [00:21, 3388.31it/s]55771it [00:20, 3497.68it/s]55602it [00:21, 3435.84it/s]57078it [00:21, 3404.73it/s]60605it [00:21, 3454.61it/s]56123it [00:21, 3342.60it/s]55948it [00:21, 3344.36it/s]57434it [00:21, 3448.94it/s]60952it [00:21, 3358.82it/s]56495it [00:21, 3449.33it/s]56327it [00:21, 3469.01it/s]57781it [00:21, 3349.95it/s]61328it [00:21, 3473.91it/s]56842it [00:21, 3357.85it/s]56690it [00:21, 3515.22it/s]58147it [00:21, 3438.94it/s]61696it [00:21, 3533.22it/s]57214it [00:21, 3461.32it/s]57043it [00:21, 3429.34it/s]58493it [00:21, 3323.42it/s]57562it [00:21, 3364.37it/s]57414it [00:21, 3509.35it/s]58855it [00:21, 3408.11it/s]57934it [00:21, 3463.86it/s]57767it [00:21, 3363.48it/s]59204it [00:21, 3430.83it/s]58299it [00:21, 3515.42it/s]58138it [00:21, 3462.05it/s]59549it [00:21, 3318.50it/s]58652it [00:21, 3392.09it/s]58487it [00:21, 3355.99it/s]59910it [00:21, 3401.46it/s]59015it [00:21, 3458.28it/s]58825it [00:22, 3317.10it/s]60252it [00:22, 3305.58it/s]59363it [00:22, 3300.51it/s]59195it [00:22, 3424.72it/s]60603it [00:22, 3362.56it/s]59725it [00:22, 3390.87it/s]59539it [00:22, 3330.20it/s]60941it [00:22, 3270.63it/s]60068it [00:22, 3298.50it/s]59910it [00:22, 3437.20it/s]61311it [00:22, 3391.92it/s]60434it [00:22, 3400.26it/s]60256it [00:22, 3346.97it/s]61670it [00:22, 3448.18it/s]60806it [00:22, 3492.44it/s]60627it [00:22, 3449.39it/s]61157it [00:22, 3377.12it/s]60974it [00:22, 3323.76it/s]61525it [00:22, 3463.08it/s]61347it [00:22, 3438.41it/s]61714it [00:22, 3504.22it/s]62051it [00:23, 489.33it/s] 62427it [00:24, 668.74it/s]62735it [00:24, 843.89it/s]63109it [00:24, 1115.98it/s]63481it [00:24, 1424.06it/s]63816it [00:24, 1676.73it/s]64187it [00:24, 2020.19it/s]64526it [00:24, 2250.42it/s]64898it [00:24, 2564.99it/s]65242it [00:24, 2697.75it/s]65593it [00:24, 2896.84it/s]62016it [00:24, 465.52it/s] 65963it [00:25, 3103.98it/s]62386it [00:24, 638.70it/s]61873it [00:24, 502.26it/s] 66311it [00:25, 3105.00it/s]62696it [00:25, 813.45it/s]62230it [00:24, 676.42it/s]66680it [00:25, 3262.61it/s]63062it [00:25, 1075.48it/s]62602it [00:24, 904.83it/s]67026it [00:25, 3205.41it/s]63420it [00:25, 1366.61it/s]62913it [00:25, 1118.22it/s]67397it [00:25, 3345.23it/s]63747it [00:25, 1623.61it/s]63287it [00:25, 1436.58it/s]62066it [00:25, 444.16it/s] 67743it [00:25, 3287.73it/s]64115it [00:25, 1967.97it/s]63616it [00:25, 1694.04it/s]62428it [00:25, 603.79it/s]68109it [00:25, 3391.07it/s]64450it [00:25, 2198.30it/s]63987it [00:25, 2040.72it/s]62733it [00:25, 767.65it/s]68474it [00:25, 3464.43it/s]64809it [00:25, 2494.52it/s]64337it [00:25, 2276.84it/s]63102it [00:25, 1022.79it/s]68825it [00:25, 3361.28it/s]65172it [00:25, 2759.87it/s]64708it [00:25, 2586.52it/s]63460it [00:25, 1306.43it/s]69190it [00:26, 3442.95it/s]65517it [00:25, 2851.67it/s]65078it [00:25, 2848.88it/s]63787it [00:25, 1563.32it/s]69538it [00:26, 3345.63it/s]65881it [00:25, 3054.05it/s]65429it [00:25, 2896.88it/s]64144it [00:25, 1889.16it/s]69907it [00:26, 3443.73it/s]66224it [00:26, 3075.83it/s]65798it [00:25, 3100.18it/s]64477it [00:26, 2130.54it/s]70254it [00:26, 3360.03it/s]66577it [00:26, 3196.86it/s]66144it [00:26, 3110.34it/s]64848it [00:26, 2461.35it/s]70624it [00:26, 3455.43it/s]66917it [00:26, 3181.10it/s]66509it [00:26, 3255.97it/s]65188it [00:26, 2604.20it/s]70986it [00:26, 3496.45it/s]67283it [00:26, 3313.34it/s]66857it [00:26, 3218.85it/s]65552it [00:26, 2854.69it/s]71337it [00:26, 3378.52it/s]67647it [00:26, 3406.89it/s]67231it [00:26, 3363.00it/s]65910it [00:26, 3040.48it/s]71704it [00:26, 3461.33it/s]67996it [00:26, 3328.18it/s]67600it [00:26, 3454.67it/s]66255it [00:26, 3059.48it/s]72052it [00:26, 3362.22it/s]68350it [00:26, 3387.47it/s]67954it [00:26, 3339.10it/s]66623it [00:26, 3226.88it/s]72407it [00:26, 3415.94it/s]68694it [00:26, 3304.69it/s]68322it [00:26, 3434.29it/s]66968it [00:26, 3175.59it/s]72750it [00:27, 3331.28it/s]69058it [00:26, 3400.76it/s]68671it [00:26, 3337.15it/s]67337it [00:26, 3317.29it/s]73125it [00:27, 3451.32it/s]69401it [00:26, 3310.50it/s]69036it [00:26, 3424.85it/s]67694it [00:27, 3388.37it/s]73497it [00:27, 3527.73it/s]69752it [00:27, 3365.55it/s]69382it [00:26, 3315.66it/s]68042it [00:27, 3309.53it/s]73851it [00:27, 3429.58it/s]70124it [00:27, 3467.71it/s]69749it [00:27, 3415.22it/s]68407it [00:27, 3406.41it/s]74223it [00:27, 3511.42it/s]70473it [00:27, 3365.12it/s]70124it [00:27, 3509.49it/s]68753it [00:27, 3302.15it/s]74576it [00:27, 3403.84it/s]70836it [00:27, 3439.82it/s]70477it [00:27, 3379.25it/s]69118it [00:27, 3401.25it/s]74943it [00:27, 3479.01it/s]71182it [00:27, 3328.73it/s]70832it [00:27, 3427.57it/s]69462it [00:27, 3277.70it/s]75293it [00:27, 3372.47it/s]71533it [00:27, 3378.24it/s]71177it [00:27, 3321.93it/s]69832it [00:27, 3397.52it/s]75661it [00:27, 3460.05it/s]71897it [00:27, 3305.75it/s]71547it [00:27, 3421.90it/s]70194it [00:27, 3460.28it/s]76009it [00:27, 3462.94it/s]72261it [00:27, 3399.19it/s]71897it [00:27, 3336.97it/s]70543it [00:27, 3360.38it/s]76357it [00:28, 3347.01it/s]72628it [00:27, 3476.34it/s]72261it [00:27, 3423.00it/s]70907it [00:27, 3439.74it/s]76726it [00:28, 3436.58it/s]72977it [00:28, 3391.85it/s]72633it [00:27, 3507.12it/s]71253it [00:28, 3313.75it/s]77071it [00:28, 3336.44it/s]73330it [00:28, 3429.85it/s]72985it [00:28, 3412.38it/s]71617it [00:28, 3405.37it/s]77439it [00:28, 3433.12it/s]73674it [00:28, 3354.53it/s]73355it [00:28, 3492.93it/s]71960it [00:28, 3307.01it/s]77784it [00:28, 3337.41it/s]74051it [00:28, 3473.18it/s]73706it [00:28, 3368.54it/s]72318it [00:28, 3384.08it/s]78153it [00:28, 3436.27it/s]74417it [00:28, 3362.59it/s]74083it [00:28, 3482.85it/s]72688it [00:28, 3473.64it/s]78523it [00:28, 3511.59it/s]74769it [00:28, 3405.46it/s]74433it [00:28, 3370.73it/s]73037it [00:28, 3362.41it/s]78876it [00:28, 3381.48it/s]75129it [00:28, 3460.62it/s]74801it [00:28, 3458.66it/s]73404it [00:28, 3449.39it/s]79242it [00:28, 3460.49it/s]75477it [00:28, 3343.50it/s]75168it [00:28, 3518.93it/s]73751it [00:28, 3338.21it/s]79590it [00:29, 3311.20it/s]75842it [00:28, 3430.60it/s]75522it [00:28, 3380.78it/s]74127it [00:28, 3457.42it/s]79958it [00:29, 3413.90it/s]76187it [00:28, 3303.14it/s]75888it [00:28, 3459.89it/s]74475it [00:29, 3356.48it/s]80302it [00:29, 3319.66it/s]76536it [00:29, 3355.95it/s]76236it [00:28, 3307.65it/s]74833it [00:29, 3418.45it/s]80673it [00:29, 3429.30it/s]76897it [00:29, 3428.29it/s]76600it [00:29, 3400.47it/s]75196it [00:29, 3479.13it/s]81038it [00:29, 3491.82it/s]77242it [00:29, 3323.73it/s]76943it [00:29, 3296.80it/s]75546it [00:29, 3335.39it/s]81389it [00:29, 3372.08it/s]77608it [00:29, 3419.10it/s]77310it [00:29, 3402.56it/s]75914it [00:29, 3433.31it/s]81754it [00:29, 3450.15it/s]77952it [00:29, 3313.77it/s]77681it [00:29, 3488.92it/s]76260it [00:29, 3322.29it/s]82101it [00:29, 3354.09it/s]78300it [00:29, 3360.86it/s]78032it [00:29, 3370.16it/s]76614it [00:29, 3384.19it/s]82466it [00:29, 3436.01it/s]78638it [00:29, 3277.51it/s]78401it [00:29, 3460.56it/s]76954it [00:29, 3296.71it/s]82816it [00:30, 3344.65it/s]78999it [00:29, 3372.34it/s]78749it [00:29, 3345.13it/s]77311it [00:29, 3372.69it/s]83179it [00:30, 3426.31it/s]79360it [00:29, 3439.48it/s]79101it [00:29, 3392.54it/s]77677it [00:29, 3455.12it/s]83523it [00:30, 3381.13it/s]79705it [00:30, 3324.73it/s]79457it [00:29, 3297.15it/s]78024it [00:30, 3345.62it/s]83863it [00:30, 3307.16it/s]80069it [00:30, 3414.50it/s]79828it [00:30, 3411.92it/s]78380it [00:30, 3406.23it/s]84226it [00:30, 3399.68it/s]80412it [00:30, 3305.94it/s]80195it [00:30, 3485.53it/s]78722it [00:30, 3313.93it/s]84567it [00:30, 3310.84it/s]80778it [00:30, 3405.78it/s]80546it [00:30, 3379.94it/s]79082it [00:30, 3396.25it/s]84937it [00:30, 3420.60it/s]81138it [00:30, 3300.54it/s]80917it [00:30, 3473.41it/s]79449it [00:30, 3473.94it/s]85302it [00:30, 3484.95it/s]81500it [00:30, 3389.50it/s]81266it [00:30, 3348.85it/s]79798it [00:30, 3368.53it/s]81863it [00:30, 3458.41it/s]81629it [00:30, 3428.17it/s]80160it [00:30, 3440.61it/s]82211it [00:30, 3352.75it/s]81977it [00:30, 3302.70it/s]80506it [00:30, 3353.16it/s]82573it [00:30, 3428.95it/s]82345it [00:30, 3408.55it/s]80862it [00:30, 3412.17it/s]82918it [00:30, 3331.77it/s]82715it [00:30, 3491.89it/s]81205it [00:31, 3307.32it/s]83284it [00:31, 3423.41it/s]83066it [00:30, 3373.31it/s]81566it [00:31, 3393.62it/s]83647it [00:31, 3481.66it/s]83434it [00:31, 3460.82it/s]81933it [00:31, 3473.14it/s]83997it [00:31, 3366.52it/s]83782it [00:31, 3349.40it/s]82282it [00:31, 3353.23it/s]84358it [00:31, 3434.02it/s]84147it [00:31, 3431.60it/s]82620it [00:31, 3359.02it/s]84703it [00:31, 3336.71it/s]82957it [00:31, 3284.23it/s]84497it [00:31, 3311.69it/s]85064it [00:31, 3414.84it/s]83319it [00:31, 3380.05it/s]84862it [00:31, 3406.61it/s]85228it [00:31, 3478.55it/s]83659it [00:31, 3288.68it/s]84029it [00:31, 3404.70it/s]84395it [00:31, 3478.30it/s]84744it [00:32, 3314.76it/s]85111it [00:32, 3414.91it/s]85652it [00:33, 447.36it/s] 86017it [00:33, 610.95it/s]86327it [00:33, 780.13it/s]86695it [00:33, 1036.91it/s]87059it [00:33, 1329.07it/s]87389it [00:33, 1578.57it/s]87761it [00:33, 1926.99it/s]88097it [00:33, 2166.64it/s]88468it [00:33, 2490.12it/s]88810it [00:34, 2648.24it/s]89181it [00:34, 2906.70it/s]89546it [00:34, 3097.80it/s]89897it [00:34, 3110.42it/s]90260it [00:34, 3249.99it/s]85407it [00:34, 3315.37it/s]90607it [00:34, 3156.52it/s]85740it [00:34, 383.98it/s] 90962it [00:34, 3264.45it/s]86107it [00:34, 532.93it/s]85578it [00:34, 386.17it/s] 91301it [00:34, 3181.46it/s]86416it [00:34, 688.48it/s]85942it [00:34, 529.94it/s]91656it [00:34, 3283.82it/s]86779it [00:34, 923.48it/s]86246it [00:34, 678.77it/s]92011it [00:35, 3357.63it/s]87088it [00:34, 1143.36it/s]86616it [00:34, 914.76it/s]92352it [00:35, 3243.35it/s]87450it [00:34, 1459.10it/s]86980it [00:34, 1188.44it/s]92707it [00:35, 3330.22it/s]87822it [00:35, 1806.85it/s]87309it [00:34, 1441.24it/s]93044it [00:35, 3232.44it/s]85455it [00:35, 367.54it/s] 88160it [00:35, 2056.24it/s]87681it [00:35, 1784.75it/s]93399it [00:35, 3320.53it/s]85818it [00:35, 506.36it/s]88530it [00:35, 2388.27it/s]88019it [00:35, 2036.02it/s]93754it [00:35, 3384.36it/s]86183it [00:35, 686.46it/s]88872it [00:35, 2562.49it/s]88391it [00:35, 2372.26it/s]86488it [00:35, 864.66it/s]94095it [00:35, 3234.60it/s]89239it [00:35, 2826.53it/s]88762it [00:35, 2668.17it/s]86844it [00:35, 1127.25it/s]94451it [00:35, 3325.18it/s]89602it [00:35, 3028.66it/s]89113it [00:35, 2777.74it/s]87164it [00:35, 1364.86it/s]94786it [00:35, 3212.62it/s]89951it [00:35, 3051.76it/s]89474it [00:35, 2984.68it/s]87532it [00:35, 1705.23it/s]95139it [00:35, 3299.40it/s]90307it [00:35, 3182.75it/s]89819it [00:35, 3020.36it/s]87895it [00:35, 2038.84it/s]95486it [00:36, 3198.61it/s]90650it [00:35, 3134.88it/s]90183it [00:35, 3185.45it/s]88234it [00:35, 2257.59it/s]95837it [00:36, 3285.62it/s]90994it [00:36, 3217.75it/s]90527it [00:35, 3131.33it/s]88604it [00:36, 2571.01it/s]96192it [00:36, 3359.67it/s]91329it [00:36, 3149.26it/s]90885it [00:35, 3253.15it/s]88945it [00:36, 2681.72it/s]96530it [00:36, 3247.77it/s]91679it [00:36, 3247.75it/s]91240it [00:36, 3335.44it/s]89306it [00:36, 2909.36it/s]96882it [00:36, 3324.45it/s]92031it [00:36, 3324.17it/s]91584it [00:36, 3227.61it/s]89644it [00:36, 2947.10it/s]97216it [00:36, 3193.54it/s]92369it [00:36, 3218.26it/s]91929it [00:36, 3290.05it/s]90010it [00:36, 3135.65it/s]97570it [00:36, 3290.21it/s]92722it [00:36, 3305.41it/s]92264it [00:36, 3199.73it/s]90367it [00:36, 3252.77it/s]97923it [00:36, 3359.26it/s]93056it [00:36, 3206.72it/s]92617it [00:36, 3292.97it/s]90712it [00:36, 3158.61it/s]98261it [00:36, 3244.02it/s]93409it [00:36, 3298.73it/s]92967it [00:36, 3202.74it/s]91065it [00:36, 3261.44it/s]98613it [00:37, 3321.84it/s]93763it [00:36, 3366.64it/s]93325it [00:36, 3307.12it/s]91402it [00:36, 3180.61it/s]98947it [00:37, 3215.92it/s]94102it [00:36, 3094.45it/s]93681it [00:36, 3378.65it/s]91756it [00:37, 3280.03it/s]99303it [00:37, 3312.32it/s]94457it [00:37, 3218.44it/s]94022it [00:36, 3256.86it/s]92110it [00:37, 3353.21it/s]99657it [00:37, 3377.50it/s]94784it [00:37, 3150.95it/s]94371it [00:37, 3321.15it/s]92450it [00:37, 3219.16it/s]99997it [00:37, 3257.82it/s]95133it [00:37, 3245.06it/s]94706it [00:37, 3221.60it/s]92805it [00:37, 3311.13it/s]100349it [00:37, 3332.43it/s]95483it [00:37, 3315.46it/s]95061it [00:37, 3315.05it/s]93140it [00:37, 3216.20it/s]100684it [00:37, 3201.51it/s]95817it [00:37, 3201.66it/s]95416it [00:37, 3380.66it/s]93496it [00:37, 3311.83it/s]101035it [00:37, 3288.72it/s]96167it [00:37, 3285.97it/s]95756it [00:37, 3250.96it/s]93830it [00:37, 3191.74it/s]101366it [00:37, 3193.38it/s]96498it [00:37, 3182.00it/s]96107it [00:37, 3324.10it/s]94183it [00:37, 3287.90it/s]101716it [00:37, 3274.98it/s]96846it [00:37, 3264.82it/s]96442it [00:37, 3209.09it/s]94539it [00:37, 3365.96it/s]102069it [00:38, 3347.37it/s]97175it [00:37, 3166.75it/s]96797it [00:37, 3304.20it/s]94878it [00:37, 3252.97it/s]102406it [00:38, 3242.23it/s]97527it [00:38, 3265.58it/s]97141it [00:37, 3341.48it/s]95231it [00:38, 3332.25it/s]102758it [00:38, 3320.44it/s]97879it [00:38, 3336.87it/s]97477it [00:38, 3228.84it/s]95566it [00:38, 3191.63it/s]103092it [00:38, 3214.75it/s]98215it [00:38, 3221.89it/s]97831it [00:38, 3316.86it/s]95919it [00:38, 3287.51it/s]103445it [00:38, 3303.79it/s]98566it [00:38, 3304.46it/s]98165it [00:38, 3207.54it/s]96275it [00:38, 3363.90it/s]103789it [00:38, 3341.98it/s]98898it [00:38, 3173.17it/s]98518it [00:38, 3298.88it/s]96614it [00:38, 3250.77it/s]104125it [00:38, 3232.10it/s]99250it [00:38, 3269.70it/s]98850it [00:38, 3194.86it/s]96966it [00:38, 3327.08it/s]104474it [00:38, 3303.93it/s]99601it [00:38, 3338.40it/s]99207it [00:38, 3300.59it/s]97301it [00:38, 3208.86it/s]104806it [00:38, 3205.21it/s]99937it [00:38, 3225.50it/s]99563it [00:38, 3374.03it/s]97652it [00:38, 3293.35it/s]105155it [00:39, 3286.59it/s]100288it [00:38, 3306.06it/s]99902it [00:38, 3226.32it/s]98005it [00:38, 3361.39it/s]105506it [00:39, 3350.27it/s]100621it [00:38, 3200.50it/s]100256it [00:38, 3313.86it/s]98343it [00:39, 3247.93it/s]105843it [00:39, 3244.02it/s]100973it [00:39, 3289.73it/s]100590it [00:38, 3214.74it/s]98697it [00:39, 3330.72it/s]106195it [00:39, 3322.08it/s]101324it [00:39, 3352.13it/s]100945it [00:39, 3308.64it/s]106529it [00:39, 3220.16it/s]99032it [00:39, 3205.27it/s]101661it [00:39, 3225.39it/s]101297it [00:39, 3367.13it/s]106883it [00:39, 3311.20it/s]99389it [00:39, 3308.37it/s]101998it [00:39, 3261.36it/s]101636it [00:39, 3243.92it/s]107226it [00:39, 3345.01it/s]99722it [00:39, 3211.60it/s]102326it [00:39, 3171.88it/s]101989it [00:39, 3324.48it/s]100077it [00:39, 3307.50it/s]107562it [00:39, 3235.92it/s]102676it [00:39, 3265.92it/s]102324it [00:39, 3189.51it/s]107919it [00:39, 3332.13it/s]100432it [00:39, 3377.06it/s]103030it [00:39, 3342.97it/s]102680it [00:39, 3292.37it/s]108254it [00:39, 3230.16it/s]100772it [00:39, 3235.83it/s]103366it [00:39, 3222.75it/s]103036it [00:39, 3367.62it/s]108611it [00:40, 3325.46it/s]101125it [00:39, 3317.61it/s]103716it [00:39, 3299.91it/s]103375it [00:39, 3242.51it/s]108945it [00:40, 3222.55it/s]101459it [00:39, 3211.28it/s]104048it [00:40, 3196.01it/s]103727it [00:39, 3320.30it/s]109298it [00:40, 3310.09it/s]101811it [00:40, 3298.80it/s]104396it [00:40, 3276.02it/s]104061it [00:40, 3213.66it/s]109652it [00:40, 3375.87it/s]102167it [00:40, 3372.92it/s]104728it [00:40, 3181.04it/s]104414it [00:40, 3301.80it/s]109991it [00:40, 3258.95it/s]102506it [00:40, 3234.14it/s]105068it [00:40, 3241.92it/s]104746it [00:40, 3196.05it/s]110336it [00:40, 3311.76it/s]102861it [00:40, 3323.06it/s]105416it [00:40, 3310.73it/s]105089it [00:40, 3260.71it/s]110669it [00:40, 3224.76it/s]103196it [00:40, 3217.39it/s]105749it [00:40, 3205.07it/s]105443it [00:40, 3339.62it/s]111027it [00:40, 3322.24it/s]103551it [00:40, 3310.47it/s]106098it [00:40, 3284.74it/s]105779it [00:40, 3220.82it/s]111386it [00:40, 3398.73it/s]103887it [00:40, 3212.70it/s]106428it [00:40, 3184.42it/s]106133it [00:40, 3312.08it/s]111727it [00:41, 3282.22it/s]104235it [00:40, 3288.18it/s]106781it [00:40, 3282.40it/s]106466it [00:40, 3212.34it/s]112087it [00:41, 3373.22it/s]104587it [00:40, 3355.12it/s]107133it [00:40, 3349.30it/s]106824it [00:40, 3317.00it/s]112426it [00:41, 3266.46it/s]104924it [00:41, 3248.88it/s]107470it [00:41, 3233.92it/s]107177it [00:40, 3377.25it/s]112780it [00:41, 3343.40it/s]105277it [00:41, 3328.77it/s]107825it [00:41, 3324.35it/s]107516it [00:41, 3264.17it/s]113127it [00:41, 3247.68it/s]105612it [00:41, 3195.12it/s]108159it [00:41, 3195.15it/s]107863it [00:41, 3322.25it/s]113481it [00:41, 3330.54it/s]105966it [00:41, 3292.81it/s]108510it [00:41, 3284.61it/s]108197it [00:41, 3212.28it/s]113827it [00:41, 3358.62it/s]106322it [00:41, 3367.57it/s]108865it [00:41, 3359.30it/s]108554it [00:41, 3313.53it/s]114164it [00:41, 3261.06it/s]106661it [00:41, 3250.17it/s]109203it [00:41, 3247.48it/s]108911it [00:41, 3387.88it/s]114522it [00:41, 3351.63it/s]107017it [00:41, 3337.75it/s]109556it [00:41, 3326.43it/s]109252it [00:41, 3263.01it/s]114859it [00:41, 3240.63it/s]107353it [00:41, 3197.32it/s]109891it [00:41, 3212.79it/s]109606it [00:41, 3339.99it/s]107709it [00:41, 3299.17it/s]110243it [00:41, 3300.32it/s]109942it [00:41, 3228.12it/s]108068it [00:41, 3381.61it/s]110595it [00:42, 3363.12it/s]110286it [00:41, 3286.07it/s]108408it [00:42, 3263.10it/s]110933it [00:42, 3237.57it/s]110617it [00:42, 3191.06it/s]108770it [00:42, 3363.21it/s]111285it [00:42, 3317.51it/s]110972it [00:42, 3293.58it/s]109109it [00:42, 3232.15it/s]111619it [00:42, 3175.81it/s]111327it [00:42, 3365.98it/s]109467it [00:42, 3330.78it/s]111970it [00:42, 3268.61it/s]111665it [00:42, 3238.56it/s]109803it [00:42, 3222.55it/s]112299it [00:42, 3175.21it/s]112019it [00:42, 3324.68it/s]110160it [00:42, 3321.19it/s]112648it [00:42, 3264.66it/s]112354it [00:42, 3218.94it/s]110516it [00:42, 3389.91it/s]112999it [00:42, 3333.77it/s]112707it [00:42, 3302.75it/s]110857it [00:42, 3248.02it/s]113334it [00:42, 3231.42it/s]113052it [00:42, 3344.32it/s]111216it [00:42, 3345.05it/s]113683it [00:42, 3304.27it/s]113388it [00:42, 3223.54it/s]111553it [00:43, 3239.12it/s]114015it [00:43, 3197.60it/s]113740it [00:42, 3307.87it/s]111913it [00:43, 3341.17it/s]114366it [00:43, 3285.95it/s]114073it [00:43, 3202.92it/s]112273it [00:43, 3415.12it/s]114705it [00:43, 3315.56it/s]114428it [00:43, 3302.26it/s]112617it [00:43, 3274.70it/s]114783it [00:43, 3373.89it/s]112977it [00:43, 3366.40it/s]113316it [00:43, 3263.92it/s]113676it [00:43, 3358.41it/s]114014it [00:43, 3254.64it/s]114367it [00:43, 3330.97it/s]114727it [00:43, 3408.38it/s]115185it [00:45, 315.23it/s] 115544it [00:45, 440.49it/s]115845it [00:45, 573.62it/s]116197it [00:45, 776.57it/s]116555it [00:45, 1012.97it/s]116918it [00:45, 1305.85it/s]117279it [00:45, 1623.33it/s]117612it [00:46, 1873.92it/s]117972it [00:46, 2198.06it/s]118307it [00:46, 2385.56it/s]118667it [00:46, 2661.41it/s]119026it [00:46, 2888.50it/s]119369it [00:46, 2925.45it/s]119715it [00:46, 3065.29it/s]120050it [00:46, 3056.34it/s]115038it [00:46, 319.46it/s] 120405it [00:46, 3191.23it/s]115391it [00:46, 443.89it/s]120755it [00:46, 3146.74it/s]115716it [00:46, 588.45it/s]121112it [00:47, 3262.51it/s]116071it [00:46, 793.98it/s]121471it [00:47, 3353.34it/s]116429it [00:47, 1045.28it/s]121813it [00:47, 3247.73it/s]116748it [00:47, 1277.15it/s]115122it [00:46, 291.89it/s] 122170it [00:47, 3337.22it/s]117104it [00:47, 1594.10it/s]115475it [00:47, 404.96it/s]122508it [00:47, 3245.10it/s]117429it [00:47, 1836.21it/s]115766it [00:47, 523.97it/s]122867it [00:47, 3341.17it/s]117783it [00:47, 2157.35it/s]116123it [00:47, 717.50it/s]123215it [00:47, 3378.94it/s]118133it [00:47, 2442.09it/s]116480it [00:47, 954.82it/s]123555it [00:47, 3270.47it/s]118467it [00:47, 2575.36it/s]116799it [00:47, 1181.97it/s]123912it [00:47, 3353.88it/s]118820it [00:47, 2806.60it/s]117145it [00:47, 1479.74it/s]115070it [00:47, 286.13it/s] 124250it [00:48, 3258.21it/s]119152it [00:47, 2848.20it/s]117467it [00:47, 1727.71it/s]115409it [00:47, 391.24it/s]124605it [00:48, 3337.17it/s]119499it [00:47, 3011.23it/s]117824it [00:47, 2060.97it/s]115715it [00:48, 513.31it/s]119849it [00:48, 3143.45it/s]124955it [00:48, 3247.15it/s]118177it [00:47, 2362.47it/s]116069it [00:48, 700.03it/s]125310it [00:48, 3331.69it/s]120184it [00:48, 3065.69it/s]118512it [00:48, 2517.87it/s]116425it [00:48, 931.65it/s]125668it [00:48, 3401.81it/s]120534it [00:48, 3186.12it/s]118865it [00:48, 2757.07it/s]116742it [00:48, 1156.98it/s]126010it [00:48, 3295.39it/s]120864it [00:48, 3115.48it/s]119197it [00:48, 2816.59it/s]117090it [00:48, 1454.93it/s]126359it [00:48, 3348.84it/s]121214it [00:48, 3222.56it/s]119548it [00:48, 2997.22it/s]117413it [00:48, 1708.49it/s]126696it [00:48, 3254.22it/s]121565it [00:48, 3304.07it/s]119890it [00:48, 3111.33it/s]117772it [00:48, 2045.74it/s]127051it [00:48, 3338.89it/s]121901it [00:48, 3194.20it/s]120224it [00:48, 3061.61it/s]118125it [00:48, 2347.61it/s]127406it [00:48, 3398.47it/s]122251it [00:48, 3281.46it/s]120581it [00:48, 3202.05it/s]118461it [00:48, 2515.49it/s]127747it [00:49, 3291.49it/s]122583it [00:48, 3184.83it/s]120914it [00:48, 3124.55it/s]118810it [00:48, 2746.94it/s]128104it [00:49, 3369.68it/s]122935it [00:48, 3280.50it/s]121269it [00:48, 3242.02it/s]119143it [00:49, 2826.27it/s]128443it [00:49, 3265.01it/s]121601it [00:48, 3150.77it/s]123276it [00:49, 3157.79it/s]119496it [00:49, 3009.32it/s]128803it [00:49, 3360.17it/s]121956it [00:49, 3262.04it/s]123627it [00:49, 3255.07it/s]119854it [00:49, 3164.04it/s]129156it [00:49, 3260.19it/s]122312it [00:49, 3346.16it/s]123976it [00:49, 3321.20it/s]120195it [00:49, 3137.77it/s]129514it [00:49, 3350.48it/s]124311it [00:49, 3213.32it/s]122650it [00:49, 3203.79it/s]120546it [00:49, 3232.68it/s]129858it [00:49, 3375.78it/s]124662it [00:49, 3297.12it/s]123006it [00:49, 3303.94it/s]120882it [00:49, 3184.00it/s]130197it [00:49, 3277.73it/s]124994it [00:49, 3189.76it/s]123340it [00:49, 3192.56it/s]121240it [00:49, 3294.61it/s]130555it [00:49, 3363.97it/s]125343it [00:49, 3273.16it/s]123693it [00:49, 3286.82it/s]121595it [00:49, 3367.89it/s]130893it [00:50, 3254.81it/s]125694it [00:49, 3339.46it/s]124046it [00:49, 3348.58it/s]121937it [00:49, 3246.96it/s]131251it [00:50, 3346.67it/s]126030it [00:49, 3228.36it/s]124383it [00:49, 3230.23it/s]122277it [00:49, 3290.04it/s]131606it [00:50, 3401.10it/s]126369it [00:50, 3274.04it/s]124737it [00:49, 3317.64it/s]122610it [00:50, 3193.51it/s]131948it [00:50, 3291.41it/s]126698it [00:50, 3178.77it/s]125071it [00:50, 3176.18it/s]122965it [00:50, 3293.06it/s]132304it [00:50, 3368.51it/s]127046it [00:50, 3264.88it/s]125425it [00:50, 3277.44it/s]123297it [00:50, 3187.03it/s]132643it [00:50, 3257.73it/s]127398it [00:50, 3338.45it/s]125781it [00:50, 3357.85it/s]123651it [00:50, 3285.52it/s]132991it [00:50, 3318.92it/s]127733it [00:50, 3217.94it/s]126119it [00:50, 3234.99it/s]123993it [00:50, 3322.64it/s]133347it [00:50, 3386.90it/s]128084it [00:50, 3299.52it/s]126471it [00:50, 3315.55it/s]124327it [00:50, 3205.51it/s]133687it [00:50, 3270.06it/s]128416it [00:50, 3192.39it/s]126805it [00:50, 3197.81it/s]124677it [00:50, 3288.90it/s]134046it [00:50, 3361.91it/s]128768it [00:50, 3285.50it/s]127159it [00:50, 3293.00it/s]125008it [00:50, 3186.17it/s]134384it [00:51, 3251.18it/s]129121it [00:50, 3356.16it/s]127491it [00:50, 3191.42it/s]125358it [00:50, 3275.21it/s]134744it [00:51, 3351.06it/s]129458it [00:51, 3199.80it/s]127834it [00:50, 3257.51it/s]125699it [00:51, 3312.52it/s]135081it [00:51, 3248.77it/s]129809it [00:51, 3285.91it/s]128188it [00:50, 3336.88it/s]126032it [00:51, 3206.70it/s]135444it [00:51, 3357.55it/s]130140it [00:51, 3180.61it/s]128524it [00:51, 3229.13it/s]126384it [00:51, 3296.38it/s]135806it [00:51, 3423.63it/s]130492it [00:51, 3274.87it/s]128877it [00:51, 3313.44it/s]126715it [00:51, 3193.41it/s]136150it [00:51, 3316.17it/s]130836it [00:51, 3183.33it/s]129210it [00:51, 3210.96it/s]127068it [00:51, 3289.49it/s]136499it [00:51, 3365.46it/s]131188it [00:51, 3276.75it/s]129564it [00:51, 3304.66it/s]127413it [00:51, 3335.02it/s]136837it [00:51, 3270.26it/s]131538it [00:51, 3340.31it/s]129919it [00:51, 3374.62it/s]127748it [00:51, 3234.84it/s]137197it [00:51, 3363.57it/s]131874it [00:51, 3232.73it/s]130258it [00:51, 3221.70it/s]128101it [00:51, 3317.92it/s]137556it [00:52, 3267.89it/s]132223it [00:51, 3306.01it/s]130612it [00:51, 3311.92it/s]128434it [00:51, 3212.00it/s]137917it [00:52, 3363.70it/s]132556it [00:51, 3178.53it/s]130946it [00:51, 3203.91it/s]128788it [00:51, 3305.18it/s]138278it [00:52, 3433.92it/s]132907it [00:52, 3271.28it/s]131301it [00:51, 3302.29it/s]129131it [00:52, 3339.71it/s]138623it [00:52, 3314.39it/s]133258it [00:52, 3337.98it/s]131655it [00:52, 3368.65it/s]129467it [00:52, 3226.80it/s]138984it [00:52, 3397.94it/s]133594it [00:52, 3221.33it/s]131994it [00:52, 3237.76it/s]129820it [00:52, 3312.15it/s]139326it [00:52, 3296.60it/s]133946it [00:52, 3305.63it/s]132347it [00:52, 3320.99it/s]130153it [00:52, 3208.53it/s]139676it [00:52, 3354.83it/s]134279it [00:52, 3204.44it/s]132681it [00:52, 3210.27it/s]130500it [00:52, 3282.85it/s]140038it [00:52, 3431.53it/s]134631it [00:52, 3292.22it/s]133024it [00:52, 3272.78it/s]130836it [00:52, 3198.08it/s]140383it [00:52, 3312.96it/s]134985it [00:52, 3362.38it/s]133356it [00:52, 3173.78it/s]131194it [00:52, 3307.22it/s]140743it [00:52, 3394.97it/s]135323it [00:52, 3247.18it/s]133711it [00:52, 3279.79it/s]131552it [00:52, 3384.84it/s]141084it [00:53, 3287.68it/s]135670it [00:52, 3308.62it/s]134067it [00:52, 3359.80it/s]131892it [00:52, 3270.85it/s]141444it [00:53, 3377.02it/s]136003it [00:53, 3217.29it/s]134405it [00:52, 3245.00it/s]132237it [00:53, 3322.21it/s]141784it [00:53, 3273.66it/s]136356it [00:53, 3304.64it/s]134758it [00:52, 3326.02it/s]132571it [00:53, 3241.25it/s]142146it [00:53, 3372.18it/s]136710it [00:53, 3371.37it/s]135093it [00:53, 3221.32it/s]132931it [00:53, 3342.46it/s]142507it [00:53, 3440.18it/s]137049it [00:53, 3248.96it/s]135451it [00:53, 3323.22it/s]133287it [00:53, 3405.16it/s]142853it [00:53, 3323.00it/s]137402it [00:53, 3327.70it/s]135793it [00:53, 3350.57it/s]133629it [00:53, 3304.33it/s]143204it [00:53, 3376.62it/s]137737it [00:53, 3221.60it/s]136130it [00:53, 3237.85it/s]133982it [00:53, 3368.51it/s]143543it [00:53, 3279.29it/s]138092it [00:53, 3313.63it/s]136485it [00:53, 3327.10it/s]134320it [00:53, 3266.31it/s]143904it [00:53, 3373.13it/s]138425it [00:53, 3210.61it/s]136820it [00:53, 3219.09it/s]134678it [00:53, 3355.65it/s]144262it [00:54, 3431.37it/s]138778it [00:53, 3300.80it/s]137176it [00:53, 3316.20it/s]135036it [00:53, 3260.02it/s]144607it [00:54, 3328.36it/s]139123it [00:53, 3342.50it/s]137533it [00:53, 3389.27it/s]135399it [00:53, 3364.24it/s]144969it [00:54, 3412.50it/s]139459it [00:54, 3241.22it/s]137874it [00:53, 3264.58it/s]135762it [00:54, 3438.55it/s]145312it [00:54, 3308.14it/s]139811it [00:54, 3321.31it/s]138224it [00:54, 3305.24it/s]136108it [00:54, 3326.63it/s]145676it [00:54, 3395.88it/s]140145it [00:54, 3221.06it/s]138556it [00:54, 3199.17it/s]136468it [00:54, 3402.90it/s]146017it [00:54, 3308.72it/s]140497it [00:54, 3298.84it/s]138912it [00:54, 3300.86it/s]136810it [00:54, 3275.55it/s]146368it [00:54, 3364.93it/s]140851it [00:54, 3368.69it/s]139244it [00:54, 3204.20it/s]137170it [00:54, 3366.39it/s]146726it [00:54, 3423.81it/s]141189it [00:54, 3261.81it/s]139602it [00:54, 3310.69it/s]137532it [00:54, 3438.15it/s]147070it [00:54, 3322.95it/s]141542it [00:54, 3337.52it/s]139958it [00:54, 3381.38it/s]137878it [00:54, 3317.65it/s]147430it [00:54, 3401.94it/s]141877it [00:54, 3224.07it/s]140298it [00:54, 3251.83it/s]138239it [00:54, 3399.60it/s]147772it [00:55, 3305.50it/s]142222it [00:54, 3288.64it/s]140654it [00:54, 3338.10it/s]138581it [00:54, 3288.46it/s]148130it [00:55, 3384.33it/s]142575it [00:54, 3356.29it/s]140990it [00:54, 3228.69it/s]138942it [00:55, 3378.70it/s]148476it [00:55, 3282.82it/s]142912it [00:55, 3245.42it/s]141344it [00:54, 3316.64it/s]139282it [00:55, 3282.83it/s]148836it [00:55, 3371.87it/s]143266it [00:55, 3328.80it/s]141700it [00:55, 3385.62it/s]139627it [00:55, 3329.13it/s]149199it [00:55, 3444.99it/s]143601it [00:55, 3223.31it/s]142040it [00:55, 3264.75it/s]139990it [00:55, 3414.51it/s]149545it [00:55, 3327.75it/s]143954it [00:55, 3310.96it/s]142397it [00:55, 3350.93it/s]140333it [00:55, 3295.35it/s]149895it [00:55, 3377.16it/s]144287it [00:55, 3209.96it/s]142734it [00:55, 3246.34it/s]140694it [00:55, 3385.72it/s]150234it [00:55, 3276.55it/s]144645it [00:55, 3313.60it/s]143086it [00:55, 3324.58it/s]141035it [00:55, 3285.12it/s]150596it [00:55, 3372.88it/s]145001it [00:55, 3382.75it/s]143436it [00:55, 3227.00it/s]141397it [00:55, 3380.10it/s]150959it [00:56, 3447.58it/s]145341it [00:55, 3240.20it/s]143794it [00:55, 3327.12it/s]141756it [00:55, 3272.93it/s]145699it [00:55, 3334.87it/s]144149it [00:55, 3390.29it/s]142118it [00:55, 3369.94it/s]146035it [00:56, 3239.38it/s]144490it [00:55, 3239.17it/s]142481it [00:56, 3442.47it/s]146387it [00:56, 3319.42it/s]144852it [00:56, 3346.23it/s]142827it [00:56, 3293.47it/s]146741it [00:56, 3381.22it/s]145189it [00:56, 3245.98it/s]143190it [00:56, 3387.00it/s]147081it [00:56, 3266.50it/s]145550it [00:56, 3347.54it/s]143531it [00:56, 3282.02it/s]147438it [00:56, 3352.16it/s]145912it [00:56, 3425.95it/s]143893it [00:56, 3378.30it/s]147775it [00:56, 3251.10it/s]146257it [00:56, 3295.99it/s]144253it [00:56, 3441.01it/s]148127it [00:56, 3324.75it/s]146614it [00:56, 3373.61it/s]144599it [00:56, 3322.50it/s]148475it [00:56, 3368.60it/s]146954it [00:56, 3260.84it/s]144963it [00:56, 3412.87it/s]148813it [00:56, 3248.83it/s]147313it [00:56, 3354.38it/s]145306it [00:56, 3308.39it/s]149168it [00:56, 3334.49it/s]147651it [00:56, 3205.01it/s]145654it [00:57, 3356.60it/s]149503it [00:57, 3230.64it/s]148009it [00:56, 3309.98it/s]145991it [00:57, 3271.56it/s]149859it [00:57, 3322.84it/s]148367it [00:57, 3385.89it/s]146350it [00:57, 3362.03it/s]150193it [00:57, 3217.69it/s]148708it [00:57, 3260.86it/s]146711it [00:57, 3432.65it/s]150546it [00:57, 3304.55it/s]149066it [00:57, 3350.21it/s]147056it [00:57, 3321.50it/s]150903it [00:57, 3379.35it/s]149403it [00:57, 3238.40it/s]147420it [00:57, 3412.59it/s]149765it [00:57, 3345.02it/s]147763it [00:57, 3299.24it/s]150125it [00:57, 3416.82it/s]148123it [00:57, 3384.05it/s]150469it [00:57, 3284.93it/s]148476it [00:57, 3270.29it/s]150811it [00:57, 3322.96it/s]148805it [00:57, 3243.31it/s]149169it [00:58, 3356.85it/s]149507it [00:58, 3261.69it/s]149871it [00:58, 3369.77it/s]150210it [00:58, 3267.42it/s]150574it [00:58, 3372.95it/s]150936it [00:58, 3444.43it/s]151305it [01:00, 263.65it/s] 151669it [01:00, 368.45it/s]151971it [01:00, 481.03it/s]152332it [01:00, 659.98it/s]152696it [01:00, 884.98it/s]153022it [01:00, 1106.16it/s]153385it [01:00, 1411.53it/s]153716it [01:00, 1672.73it/s]154080it [01:01, 2011.51it/s]154425it [01:01, 2244.89it/s]154791it [01:01, 2550.61it/s]155158it [01:01, 2813.18it/s]155506it [01:01, 2868.49it/s]155864it [01:01, 3050.35it/s]156205it [01:01, 3032.70it/s]156570it [01:01, 3197.07it/s]156937it [01:01, 3326.96it/s]157285it [01:02, 3251.46it/s]157643it [01:02, 3342.72it/s]151243it [01:02, 244.33it/s] 157986it [01:02, 3258.56it/s]151603it [01:02, 342.69it/s]158350it [01:02, 3364.37it/s]151906it [01:02, 450.28it/s]158692it [01:02, 3250.38it/s]152265it [01:02, 620.62it/s]159057it [01:02, 3362.80it/s]152624it [01:02, 834.18it/s]159425it [01:02, 3453.68it/s]151145it [01:02, 242.87it/s] 152946it [01:02, 1050.07it/s]159774it [01:02, 3322.21it/s]151503it [01:02, 340.84it/s]153294it [01:02, 1333.71it/s]160139it [01:02, 3415.72it/s]151863it [01:02, 471.98it/s]153619it [01:02, 1590.00it/s]160483it [01:02, 3319.46it/s]152162it [01:02, 608.55it/s]153980it [01:02, 1928.84it/s]160851it [01:03, 3421.92it/s]152520it [01:02, 822.48it/s]154337it [01:02, 2245.79it/s]161196it [01:03, 3330.81it/s]152836it [01:02, 1033.72it/s]151282it [01:02, 256.32it/s] 154676it [01:03, 2435.76it/s]161562it [01:03, 3423.97it/s]153183it [01:02, 1318.51it/s]151645it [01:03, 358.17it/s]155034it [01:03, 2699.96it/s]161932it [01:03, 3502.09it/s]153541it [01:03, 1641.58it/s]151950it [01:03, 469.44it/s]155372it [01:03, 2791.25it/s]152315it [01:03, 646.74it/s]162284it [01:03, 3387.51it/s]153873it [01:03, 1892.12it/s]155731it [01:03, 2995.12it/s]152669it [01:03, 860.17it/s]162656it [01:03, 3483.00it/s]154233it [01:03, 2220.22it/s]156089it [01:03, 3151.21it/s]152993it [01:03, 1081.05it/s]154567it [01:03, 2402.14it/s]163006it [01:03, 3375.90it/s]156432it [01:03, 3100.91it/s]153358it [01:03, 1387.26it/s]163367it [01:03, 3441.46it/s]154926it [01:03, 2676.33it/s]156791it [01:03, 3234.77it/s]153691it [01:03, 1649.57it/s]163713it [01:03, 3350.95it/s]155265it [01:03, 2769.58it/s]157130it [01:03, 3178.48it/s]154057it [01:03, 1991.82it/s]164082it [01:04, 3446.30it/s]155623it [01:03, 2975.30it/s]157490it [01:03, 3295.80it/s]154420it [01:03, 2311.37it/s]164446it [01:04, 3502.19it/s]155978it [01:03, 3129.02it/s]157828it [01:03, 3214.34it/s]154765it [01:03, 2498.73it/s]164798it [01:04, 3394.33it/s]156319it [01:03, 3072.57it/s]158185it [01:04, 3313.86it/s]155129it [01:04, 2763.87it/s]165172it [01:04, 3493.98it/s]156675it [01:03, 3205.28it/s]158545it [01:04, 3395.13it/s]155472it [01:04, 2836.33it/s]165523it [01:04, 3399.58it/s]157011it [01:04, 3153.54it/s]158889it [01:04, 3284.29it/s]155819it [01:04, 2999.01it/s]165898it [01:04, 3499.31it/s]157365it [01:04, 3254.12it/s]159250it [01:04, 3377.40it/s]156155it [01:04, 3007.53it/s]157716it [01:04, 3325.71it/s]166250it [01:04, 3393.59it/s]159591it [01:04, 3259.96it/s]156517it [01:04, 3172.64it/s]166612it [01:04, 3457.39it/s]158055it [01:04, 3210.87it/s]159934it [01:04, 3307.65it/s]156875it [01:04, 3282.31it/s]166985it [01:04, 3536.64it/s]158411it [01:04, 3308.86it/s]160292it [01:04, 3384.69it/s]157218it [01:04, 3205.30it/s]167340it [01:04, 3414.58it/s]158746it [01:04, 3199.23it/s]160633it [01:04, 3279.34it/s]157579it [01:04, 3318.90it/s]167712it [01:05, 3501.74it/s]159096it [01:04, 3281.68it/s]160991it [01:04, 3364.96it/s]157919it [01:04, 3246.70it/s]168064it [01:05, 3397.08it/s]159427it [01:04, 3118.15it/s]161330it [01:05, 3257.32it/s]158273it [01:05, 3329.40it/s]168433it [01:05, 3480.20it/s]159742it [01:04, 3084.22it/s]161688it [01:05, 3347.39it/s]158626it [01:05, 3212.77it/s]168783it [01:05, 3299.36it/s]160073it [01:05, 3145.96it/s]162025it [01:05, 3243.21it/s]158952it [01:05, 3216.92it/s]169151it [01:05, 3405.38it/s]160390it [01:05, 3082.40it/s]162380it [01:05, 3329.18it/s]159322it [01:05, 3354.93it/s]169520it [01:05, 3484.75it/s]160750it [01:05, 3228.88it/s]162738it [01:05, 3401.16it/s]159661it [01:05, 3271.32it/s]169871it [01:05, 3360.79it/s]161085it [01:05, 3262.25it/s]163080it [01:05, 3274.32it/s]160026it [01:05, 3370.89it/s]170234it [01:05, 3436.27it/s]161413it [01:05, 3196.88it/s]163441it [01:05, 3368.06it/s]160365it [01:05, 3289.75it/s]161775it [01:05, 3318.68it/s]170580it [01:05, 3340.81it/s]163780it [01:05, 3279.89it/s]160730it [01:05, 3365.85it/s]170948it [01:06, 3436.29it/s]162108it [01:05, 3219.06it/s]164144it [01:05, 3380.88it/s]161097it [01:05, 3452.04it/s]162473it [01:05, 3342.21it/s]171294it [01:06, 3353.60it/s]164503it [01:05, 3441.08it/s]161444it [01:05, 3343.51it/s]171657it [01:06, 3431.07it/s]162826it [01:05, 3257.33it/s]164849it [01:06, 3345.24it/s]161817it [01:06, 3452.98it/s]172030it [01:06, 3518.01it/s]163193it [01:05, 3374.84it/s]165212it [01:06, 3425.72it/s]162164it [01:06, 3352.16it/s]172383it [01:06, 3395.43it/s]163555it [01:06, 3445.56it/s]165556it [01:06, 3331.34it/s]162535it [01:06, 3454.36it/s]172752it [01:06, 3479.64it/s]163901it [01:06, 3313.27it/s]165919it [01:06, 3415.09it/s]162882it [01:06, 3336.46it/s]173102it [01:06, 3397.31it/s]164263it [01:06, 3398.34it/s]166262it [01:06, 3301.65it/s]163252it [01:06, 3438.91it/s]173460it [01:06, 3448.91it/s]164605it [01:06, 3138.71it/s]166630it [01:06, 3408.59it/s]163608it [01:06, 3472.04it/s]173806it [01:06, 3347.68it/s]164981it [01:06, 3309.82it/s]166995it [01:06, 3476.86it/s]163957it [01:06, 3367.83it/s]174179it [01:06, 3455.36it/s]165336it [01:06, 3376.35it/s]167344it [01:06, 3366.28it/s]164330it [01:06, 3472.03it/s]174558it [01:07, 3551.09it/s]165678it [01:06, 3211.68it/s]167710it [01:06, 3449.42it/s]164679it [01:06, 3370.15it/s]174915it [01:07, 3413.77it/s]166048it [01:06, 3347.59it/s]168057it [01:07, 3338.13it/s]165058it [01:07, 3488.30it/s]175288it [01:07, 3504.52it/s]166387it [01:06, 3254.90it/s]168425it [01:07, 3434.83it/s]165409it [01:07, 3386.33it/s]175641it [01:07, 3396.79it/s]166761it [01:07, 3391.01it/s]168770it [01:07, 3330.26it/s]165782it [01:07, 3483.56it/s]176010it [01:07, 3478.55it/s]167103it [01:07, 3300.26it/s]169132it [01:07, 3411.20it/s]166158it [01:07, 3562.16it/s]176360it [01:07, 3369.40it/s]167473it [01:07, 3412.38it/s]169482it [01:07, 3436.35it/s]166516it [01:07, 3425.91it/s]176734it [01:07, 3473.56it/s]167841it [01:07, 3489.46it/s]169827it [01:07, 3315.75it/s]166892it [01:07, 3519.56it/s]177091it [01:07, 3499.72it/s]168192it [01:07, 3347.73it/s]170188it [01:07, 3398.43it/s]167246it [01:07, 3399.16it/s]177443it [01:07, 3379.90it/s]168538it [01:07, 3379.08it/s]170530it [01:07, 3289.20it/s]167620it [01:07, 3494.49it/s]177810it [01:07, 3460.59it/s]168878it [01:07, 3196.05it/s]170893it [01:07, 3385.74it/s]167972it [01:07, 3390.22it/s]178158it [01:08, 3367.77it/s]169244it [01:07, 3323.88it/s]171234it [01:07, 3286.38it/s]168349it [01:07, 3497.44it/s]178515it [01:08, 3425.28it/s]169580it [01:07, 3207.00it/s]171584it [01:08, 3345.82it/s]168705it [01:08, 3387.14it/s]178859it [01:08, 3186.58it/s]169942it [01:08, 3322.73it/s]171942it [01:08, 3412.52it/s]169075it [01:08, 3474.26it/s]179224it [01:08, 3314.69it/s]170314it [01:08, 3435.42it/s]172285it [01:08, 3298.07it/s]169431it [01:08, 3498.73it/s]179596it [01:08, 3427.87it/s]170660it [01:08, 3321.30it/s]172645it [01:08, 3372.86it/s]169783it [01:08, 3383.68it/s]179942it [01:08, 3321.54it/s]171025it [01:08, 3413.74it/s]172984it [01:08, 3280.37it/s]170158it [01:08, 3487.35it/s]180314it [01:08, 3434.09it/s]171369it [01:08, 3286.11it/s]173345it [01:08, 3374.61it/s]170509it [01:08, 3379.35it/s]180660it [01:08, 3325.28it/s]171733it [01:08, 3385.62it/s]173706it [01:08, 3442.42it/s]170880it [01:08, 3471.93it/s]181033it [01:08, 3439.77it/s]172074it [01:08, 3298.04it/s]174052it [01:08, 3316.89it/s]171229it [01:08, 3365.10it/s]181380it [01:09, 3335.33it/s]172446it [01:08, 3418.45it/s]174418it [01:08, 3413.17it/s]171599it [01:08, 3459.96it/s]181748it [01:09, 3432.98it/s]172817it [01:08, 3502.29it/s]174761it [01:09, 3310.88it/s]171973it [01:09, 3539.65it/s]182125it [01:09, 3528.82it/s]173169it [01:08, 3388.44it/s]175123it [01:09, 3399.25it/s]172329it [01:09, 3379.88it/s]182480it [01:09, 3409.20it/s]173534it [01:09, 3463.14it/s]175465it [01:09, 3284.30it/s]172704it [01:09, 3483.30it/s]182857it [01:09, 3510.69it/s]173882it [01:09, 3345.88it/s]175828it [01:09, 3380.65it/s]173055it [01:09, 3390.10it/s]183210it [01:09, 3398.20it/s]174250it [01:09, 3439.83it/s]176173it [01:09, 3399.44it/s]173429it [01:09, 3489.63it/s]183584it [01:09, 3495.14it/s]174596it [01:09, 3332.45it/s]176515it [01:09, 3281.71it/s]173780it [01:09, 3384.15it/s]183936it [01:09, 3361.63it/s]174961it [01:09, 3422.54it/s]176879it [01:09, 3384.32it/s]174156it [01:09, 3490.57it/s]184316it [01:09, 3478.14it/s]175331it [01:09, 3502.95it/s]177219it [01:09, 3272.12it/s]174534it [01:09, 3572.30it/s]184666it [01:10, 3381.96it/s]175683it [01:09, 3377.20it/s]177575it [01:09, 3352.57it/s]174893it [01:09, 3420.88it/s]185042it [01:10, 3489.70it/s]176046it [01:09, 3449.25it/s]177942it [01:09, 3442.16it/s]175260it [01:09, 3490.68it/s]185403it [01:10, 3524.25it/s]176393it [01:09, 3331.23it/s]178288it [01:10, 3333.14it/s]175611it [01:10, 3378.22it/s]185757it [01:10, 3404.79it/s]176762it [01:10, 3433.01it/s]178647it [01:10, 3403.18it/s]175984it [01:10, 3476.24it/s]186136it [01:10, 3513.39it/s]177107it [01:10, 3316.25it/s]178989it [01:10, 3303.47it/s]176334it [01:10, 3365.63it/s]186489it [01:10, 3407.00it/s]177464it [01:10, 3386.24it/s]179334it [01:10, 3344.53it/s]176705it [01:10, 3463.92it/s]186881it [01:10, 3551.92it/s]177834it [01:10, 3474.84it/s]179670it [01:10, 3255.96it/s]177073it [01:10, 3525.39it/s]187238it [01:10, 3406.32it/s]178183it [01:10, 3378.37it/s]180029it [01:10, 3351.81it/s]177427it [01:10, 3413.16it/s]187609it [01:10, 3491.26it/s]178547it [01:10, 3451.40it/s]180397it [01:10, 3443.83it/s]177796it [01:10, 3491.78it/s]187989it [01:10, 3579.20it/s]178894it [01:10, 3359.88it/s]180743it [01:10, 3321.52it/s]178147it [01:10, 3388.30it/s]188349it [01:11, 3462.16it/s]179260it [01:10, 3445.41it/s]181110it [01:10, 3420.34it/s]178514it [01:10, 3467.08it/s]188726it [01:11, 3547.56it/s]179626it [01:10, 3343.57it/s]181454it [01:11, 3289.76it/s]178863it [01:11, 3388.01it/s]189083it [01:11, 3421.69it/s]179991it [01:10, 3430.23it/s]181819it [01:11, 3391.37it/s]179231it [01:11, 3470.37it/s]189460it [01:11, 3519.46it/s]180372it [01:11, 3539.67it/s]182160it [01:11, 3299.94it/s]179606it [01:11, 3549.34it/s]189814it [01:11, 3410.80it/s]180728it [01:11, 3360.92it/s]182511it [01:11, 3358.97it/s]179962it [01:11, 3417.79it/s]190196it [01:11, 3519.69it/s]181101it [01:11, 3464.30it/s]182880it [01:11, 3452.69it/s]180342it [01:11, 3526.08it/s]190550it [01:11, 3402.64it/s]181450it [01:11, 3336.07it/s]183227it [01:11, 3332.40it/s]180697it [01:11, 3403.76it/s]190906it [01:11, 3445.22it/s]181824it [01:11, 3449.47it/s]183590it [01:11, 3416.19it/s]181064it [01:11, 3479.18it/s]191278it [01:11, 3522.06it/s]182172it [01:11, 3361.00it/s]183934it [01:11, 3304.69it/s]181414it [01:11, 3372.45it/s]191632it [01:12, 3414.70it/s]182541it [01:11, 3454.07it/s]184300it [01:11, 3405.78it/s]181787it [01:11, 3474.75it/s]192016it [01:12, 3526.83it/s]182916it [01:11, 3530.52it/s]184665it [01:11, 3476.28it/s]182146it [01:11, 3381.18it/s]192371it [01:12, 3417.83it/s]183271it [01:11, 3404.54it/s]185014it [01:12, 3354.24it/s]182517it [01:12, 3473.06it/s]192756it [01:12, 3541.18it/s]183640it [01:12, 3483.92it/s]185368it [01:12, 3406.36it/s]182898it [01:12, 3569.84it/s]193112it [01:12, 3432.59it/s]183990it [01:12, 3334.57it/s]185711it [01:12, 3274.91it/s]183257it [01:12, 3459.30it/s]193482it [01:12, 3508.24it/s]184365it [01:12, 3449.20it/s]186079it [01:12, 3389.37it/s]183627it [01:12, 3527.44it/s]193856it [01:12, 3574.67it/s]184713it [01:12, 3347.82it/s]186420it [01:12, 3298.58it/s]183982it [01:12, 3401.60it/s]194215it [01:12, 3445.64it/s]185086it [01:12, 3450.91it/s]186799it [01:12, 3436.94it/s]184360it [01:12, 3508.36it/s]194592it [01:12, 3536.48it/s]185445it [01:12, 3490.14it/s]187160it [01:12, 3486.83it/s]184713it [01:12, 3416.38it/s]194948it [01:12, 3428.80it/s]185796it [01:12, 3373.22it/s]187511it [01:12, 3382.35it/s]185088it [01:12, 3510.16it/s]195336it [01:13, 3557.16it/s]186177it [01:12, 3498.40it/s]187886it [01:12, 3487.20it/s]185451it [01:12, 3543.37it/s]195694it [01:13, 3443.72it/s]186529it [01:12, 3373.36it/s]188237it [01:13, 3357.93it/s]185807it [01:13, 3430.42it/s]196041it [01:13, 3395.32it/s]186873it [01:12, 3391.00it/s]188575it [01:13, 3343.20it/s]186189it [01:13, 3541.75it/s]196382it [01:13, 3261.46it/s]187214it [01:13, 3289.05it/s]188911it [01:13, 3236.78it/s]186545it [01:13, 3399.55it/s]187587it [01:13, 3413.35it/s]189276it [01:13, 3354.45it/s]186916it [01:13, 3486.57it/s]187956it [01:13, 3489.13it/s]189627it [01:13, 3399.22it/s]187267it [01:13, 3372.34it/s]188307it [01:13, 3378.63it/s]189969it [01:13, 3324.94it/s]187644it [01:13, 3484.90it/s]188686it [01:13, 3496.16it/s]190342it [01:13, 3440.55it/s]188024it [01:13, 3575.53it/s]189038it [01:13, 3368.47it/s]190688it [01:13, 3336.47it/s]188384it [01:13, 3448.39it/s]189411it [01:13, 3469.54it/s]191050it [01:13, 3417.89it/s]188761it [01:13, 3540.22it/s]189760it [01:13, 3347.41it/s]191394it [01:13, 3312.42it/s]189117it [01:13, 3416.04it/s]190133it [01:13, 3455.92it/s]191762it [01:14, 3416.94it/s]189481it [01:14, 3479.47it/s]190490it [01:14, 3486.70it/s]192136it [01:14, 3509.19it/s]189831it [01:14, 3332.92it/s]190841it [01:14, 3366.67it/s]192489it [01:14, 3372.12it/s]190211it [01:14, 3464.38it/s]191211it [01:14, 3461.93it/s]192862it [01:14, 3472.24it/s]190560it [01:14, 3362.21it/s]191559it [01:14, 3357.19it/s]193211it [01:14, 3385.44it/s]190933it [01:14, 3467.03it/s]191943it [01:14, 3494.67it/s]193571it [01:14, 3445.89it/s]191306it [01:14, 3542.49it/s]193917it [01:14, 3371.04it/s]192295it [01:14, 3373.99it/s]191662it [01:14, 3428.30it/s]194294it [01:14, 3484.71it/s]192674it [01:14, 3491.11it/s]192045it [01:14, 3541.43it/s]194665it [01:14, 3548.58it/s]193049it [01:14, 3564.03it/s]192401it [01:14, 3403.05it/s]195021it [01:15, 3417.49it/s]193407it [01:14, 3425.68it/s]192786it [01:15, 3526.41it/s]195407it [01:15, 3544.83it/s]193767it [01:14, 3474.03it/s]193141it [01:15, 3419.71it/s]194117it [01:15, 3375.74it/s]195764it [01:15, 3397.29it/s]193514it [01:15, 3507.28it/s]194495it [01:15, 3490.84it/s]196128it [01:15, 3464.64it/s]193896it [01:15, 3597.32it/s]196477it [01:15, 3370.30it/s]194846it [01:15, 3378.24it/s]194258it [01:15, 3477.97it/s]195236it [01:15, 3525.78it/s]194637it [01:15, 3565.16it/s]195591it [01:15, 3414.88it/s]194996it [01:15, 3442.06it/s]195958it [01:15, 3486.60it/s]195377it [01:15, 3546.16it/s]196329it [01:15, 3550.33it/s]195734it [01:15, 3419.45it/s]196101it [01:16, 3490.73it/s]196452it [01:16, 3395.02it/s]196710it [01:18, 204.60it/s] 197083it [01:18, 291.43it/s]197404it [01:19, 390.61it/s]197782it [01:19, 547.55it/s]198166it [01:19, 751.71it/s]198502it [01:19, 959.70it/s]198880it [01:19, 1252.06it/s]199225it [01:19, 1519.90it/s]199617it [01:19, 1889.81it/s]199971it [01:19, 2134.76it/s]200343it [01:19, 2452.36it/s]200695it [01:19, 2618.99it/s]201070it [01:20, 2885.10it/s]201432it [01:20, 3070.22it/s]201786it [01:20, 3107.99it/s]202165it [01:20, 3291.05it/s]202519it [01:20, 3299.59it/s]202908it [01:20, 3464.81it/s]203268it [01:20, 3395.63it/s]203639it [01:20, 3482.76it/s]204011it [01:20, 3549.74it/s]204372it [01:20, 3440.23it/s]204748it [01:21, 3530.82it/s]205105it [01:21, 3416.84it/s]205466it [01:21, 3469.57it/s]196816it [01:21, 196.70it/s] 205816it [01:21, 3386.56it/s]197192it [01:21, 279.60it/s]206200it [01:21, 3514.82it/s]197500it [01:21, 369.67it/s]206575it [01:21, 3426.15it/s]197872it [01:21, 516.81it/s]206951it [01:21, 3519.63it/s]198185it [01:21, 669.93it/s]207331it [01:21, 3600.22it/s]196793it [01:21, 205.51it/s] 198560it [01:21, 908.97it/s]207693it [01:21, 3474.17it/s]197169it [01:21, 291.30it/s]198931it [01:21, 1188.65it/s]208072it [01:22, 3562.34it/s]197488it [01:21, 387.93it/s]196686it [01:21, 194.13it/s] 199273it [01:21, 1452.97it/s]208430it [01:22, 3482.77it/s]197874it [01:21, 545.72it/s]197062it [01:21, 273.75it/s]199639it [01:22, 1784.05it/s]208809it [01:22, 3570.59it/s]198195it [01:22, 707.61it/s]197379it [01:21, 363.27it/s]199983it [01:22, 2028.60it/s]198575it [01:22, 955.03it/s]209168it [01:22, 3466.71it/s]197749it [01:22, 504.37it/s]200345it [01:22, 2341.13it/s]198949it [01:22, 1240.81it/s]209537it [01:22, 3528.49it/s]198133it [01:22, 694.36it/s]200695it [01:22, 2527.84it/s]209893it [01:22, 3535.09it/s]199298it [01:22, 1513.03it/s]198472it [01:22, 891.54it/s]201056it [01:22, 2781.29it/s]199673it [01:22, 1855.08it/s]210248it [01:22, 3407.71it/s]198837it [01:22, 1157.65it/s]201397it [01:22, 2928.78it/s]210621it [01:22, 3498.63it/s]200024it [01:22, 2103.26it/s]199178it [01:22, 1414.60it/s]201737it [01:22, 2928.82it/s]200394it [01:22, 2422.91it/s]210973it [01:22, 3387.04it/s]199556it [01:22, 1759.84it/s]202096it [01:22, 3102.70it/s]211343it [01:22, 3475.48it/s]200743it [01:22, 2600.27it/s]199902it [01:22, 2005.80it/s]202432it [01:22, 3151.90it/s]201114it [01:22, 2861.95it/s]211692it [01:23, 3375.24it/s]200269it [01:22, 2328.30it/s]202795it [01:22, 3282.05it/s]201486it [01:22, 3078.83it/s]212066it [01:23, 3478.69it/s]200635it [01:22, 2615.96it/s]203172it [01:23, 3420.76it/s]212438it [01:23, 3548.41it/s]201842it [01:23, 3124.36it/s]200985it [01:22, 2739.55it/s]203525it [01:23, 3329.13it/s]202224it [01:23, 3311.77it/s]212795it [01:23, 3438.92it/s]201349it [01:23, 2961.00it/s]203887it [01:23, 3411.29it/s]213164it [01:23, 3509.85it/s]202581it [01:23, 3292.72it/s]201695it [01:23, 3007.78it/s]204234it [01:23, 3318.39it/s]202968it [01:23, 3453.13it/s]213517it [01:23, 3398.32it/s]202060it [01:23, 3177.78it/s]204603it [01:23, 3424.39it/s]213894it [01:23, 3502.85it/s]203327it [01:23, 3381.60it/s]202405it [01:23, 3190.88it/s]204949it [01:23, 3335.76it/s]203701it [01:23, 3481.52it/s]214246it [01:23, 3392.41it/s]202787it [01:23, 3364.19it/s]205318it [01:23, 3435.43it/s]204057it [01:23, 3372.02it/s]214587it [01:23, 3356.65it/s]203163it [01:23, 3475.97it/s]205685it [01:23, 3502.36it/s]204431it [01:23, 3475.12it/s]214956it [01:24, 3450.87it/s]203522it [01:23, 3367.61it/s]206038it [01:23, 3365.87it/s]204805it [01:23, 3550.72it/s]215303it [01:24, 3366.27it/s]203888it [01:23, 3449.74it/s]206384it [01:24, 3390.42it/s]215671it [01:24, 3455.09it/s]205164it [01:24, 3408.76it/s]204240it [01:23, 3338.22it/s]206725it [01:24, 3221.17it/s]205533it [01:24, 3486.94it/s]216018it [01:24, 3353.27it/s]204599it [01:24, 3409.34it/s]207094it [01:24, 3353.26it/s]216394it [01:24, 3469.93it/s]205885it [01:24, 3315.61it/s]204944it [01:24, 3098.77it/s]207432it [01:24, 3281.46it/s]216743it [01:24, 3373.52it/s]206264it [01:24, 3447.80it/s]205311it [01:24, 3252.11it/s]207793it [01:24, 3373.47it/s]217126it [01:24, 3502.31it/s]206612it [01:24, 3368.43it/s]205672it [01:24, 3350.90it/s]208165it [01:24, 3473.72it/s]206985it [01:24, 3470.46it/s]217495it [01:24, 3395.44it/s]206013it [01:24, 3289.04it/s]208514it [01:24, 3381.90it/s]207359it [01:24, 3547.61it/s]217885it [01:24, 3538.28it/s]206388it [01:24, 3418.07it/s]208885it [01:24, 3474.50it/s]218265it [01:24, 3608.71it/s]207716it [01:24, 3434.94it/s]206734it [01:24, 3325.08it/s]209234it [01:24, 3387.14it/s]208090it [01:24, 3521.85it/s]218628it [01:25, 3506.72it/s]207106it [01:24, 3436.44it/s]209585it [01:24, 3421.81it/s]219012it [01:25, 3601.71it/s]208444it [01:24, 3451.76it/s]207453it [01:24, 3330.34it/s]209936it [01:25, 3310.86it/s]208820it [01:25, 3540.06it/s]219374it [01:25, 3474.42it/s]207826it [01:24, 3444.03it/s]210301it [01:25, 3405.74it/s]219755it [01:25, 3570.33it/s]209176it [01:25, 3452.84it/s]208193it [01:25, 3509.18it/s]210665it [01:25, 3471.43it/s]209544it [01:25, 3517.56it/s]220114it [01:25, 3481.35it/s]208546it [01:25, 3403.51it/s]211014it [01:25, 3346.16it/s]209915it [01:25, 3572.73it/s]220505it [01:25, 3604.31it/s]208920it [01:25, 3498.93it/s]211368it [01:25, 3400.15it/s]210274it [01:25, 3430.71it/s]220867it [01:25, 3501.04it/s]209272it [01:25, 3391.01it/s]211710it [01:25, 3308.55it/s]210641it [01:25, 3498.88it/s]221244it [01:25, 3576.94it/s]209637it [01:25, 3463.16it/s]212078it [01:25, 3414.99it/s]221640it [01:25, 3685.63it/s]210993it [01:25, 3369.45it/s]209985it [01:25, 3341.66it/s]212441it [01:25, 3474.93it/s]211365it [01:25, 3469.22it/s]222010it [01:26, 3550.76it/s]210354it [01:25, 3440.16it/s]212790it [01:25, 3336.69it/s]222380it [01:26, 3593.36it/s]211714it [01:25, 3377.78it/s]210719it [01:25, 3499.53it/s]213156it [01:26, 3420.43it/s]212090it [01:26, 3485.41it/s]222741it [01:26, 3481.60it/s]211071it [01:25, 3365.17it/s]213500it [01:26, 3320.25it/s]223112it [01:26, 3545.08it/s]212454it [01:26, 3390.08it/s]211429it [01:26, 3426.02it/s]213865it [01:26, 3412.27it/s]212828it [01:26, 3488.51it/s]223468it [01:26, 3419.66it/s]211774it [01:26, 3320.59it/s]214208it [01:26, 3309.63it/s]213182it [01:26, 3503.24it/s]223843it [01:26, 3513.38it/s]212141it [01:26, 3419.30it/s]214570it [01:26, 3398.05it/s]224207it [01:26, 3549.22it/s]213534it [01:26, 3287.78it/s]212485it [01:26, 3316.38it/s]214933it [01:26, 3463.78it/s]224564it [01:26, 3418.26it/s]213878it [01:26, 3329.54it/s]212856it [01:26, 3427.73it/s]215281it [01:26, 3362.05it/s]224932it [01:26, 3490.86it/s]214214it [01:26, 3265.60it/s]213225it [01:26, 3495.12it/s]215648it [01:26, 3450.83it/s]225283it [01:26, 3385.14it/s]214582it [01:26, 3383.47it/s]213576it [01:26, 3364.20it/s]215995it [01:26, 3310.83it/s]225654it [01:27, 3477.16it/s]214923it [01:26, 3388.24it/s]213947it [01:26, 3462.13it/s]216370it [01:26, 3435.22it/s]215264it [01:26, 3319.52it/s]226004it [01:27, 3359.15it/s]214295it [01:26, 3311.39it/s]216716it [01:27, 3337.25it/s]215640it [01:27, 3445.45it/s]226370it [01:27, 3443.37it/s]214659it [01:26, 3404.46it/s]217090it [01:27, 3450.36it/s]215986it [01:27, 3348.92it/s]226735it [01:27, 3338.39it/s]215002it [01:27, 3298.14it/s]217453it [01:27, 3500.91it/s]216368it [01:27, 3484.62it/s]227095it [01:27, 3410.71it/s]215373it [01:27, 3414.01it/s]217805it [01:27, 3426.37it/s]227458it [01:27, 3471.20it/s]216718it [01:27, 3364.14it/s]215738it [01:27, 3481.19it/s]218184it [01:27, 3531.06it/s]217057it [01:27, 3189.79it/s]227807it [01:27, 3261.00it/s]216088it [01:27, 3360.81it/s]218539it [01:27, 3408.73it/s]217424it [01:27, 3315.87it/s]228170it [01:27, 3362.30it/s]216454it [01:27, 3445.73it/s]218914it [01:27, 3506.14it/s]216801it [01:27, 3344.97it/s]217759it [01:27, 2997.28it/s]228510it [01:27, 2964.28it/s]219267it [01:27, 3386.26it/s]217181it [01:27, 3475.04it/s]218138it [01:27, 3209.68it/s]228875it [01:28, 3145.22it/s]219641it [01:27, 3486.89it/s]218467it [01:27, 3205.22it/s]217531it [01:27, 3348.96it/s]229240it [01:28, 3282.25it/s]220016it [01:28, 3426.72it/s]218853it [01:28, 3389.43it/s]217916it [01:27, 3491.08it/s]229577it [01:28, 3252.36it/s]220403it [01:28, 3551.39it/s]218293it [01:28, 3570.67it/s]219197it [01:28, 3328.49it/s]229945it [01:28, 3372.48it/s]220782it [01:28, 3620.15it/s]219584it [01:28, 3481.66it/s]218652it [01:28, 3462.22it/s]230287it [01:28, 3287.93it/s]221146it [01:28, 3491.62it/s]219969it [01:28, 3587.61it/s]219031it [01:28, 3556.54it/s]230652it [01:28, 3389.08it/s]221529it [01:28, 3588.04it/s]220331it [01:28, 3516.43it/s]219389it [01:28, 3455.35it/s]230994it [01:28, 3320.08it/s]221890it [01:28, 3491.00it/s]220719it [01:28, 3620.68it/s]219769it [01:28, 3553.34it/s]231364it [01:28, 3428.46it/s]222256it [01:28, 3532.25it/s]221083it [01:28, 3507.47it/s]231721it [01:28, 3469.03it/s]220126it [01:28, 3449.96it/s]222611it [01:28, 3411.97it/s]221474it [01:28, 3621.72it/s]220516it [01:28, 3577.05it/s]232070it [01:29, 3355.26it/s]222977it [01:28, 3481.57it/s]221838it [01:28, 3529.45it/s]232439it [01:29, 3451.05it/s]220876it [01:28, 3431.80it/s]223327it [01:28, 3481.71it/s]222207it [01:29, 3554.51it/s]221252it [01:28, 3522.43it/s]232786it [01:29, 3348.36it/s]223677it [01:29, 3365.15it/s]222564it [01:29, 3446.42it/s]221641it [01:28, 3627.41it/s]233161it [01:29, 3462.73it/s]224046it [01:29, 3455.85it/s]222936it [01:29, 3524.44it/s]233509it [01:29, 3367.00it/s]222006it [01:29, 3488.21it/s]224393it [01:29, 3347.92it/s]223307it [01:29, 3576.33it/s]233878it [01:29, 3459.08it/s]222371it [01:29, 3533.71it/s]224758it [01:29, 3431.80it/s]223666it [01:29, 3447.80it/s]234250it [01:29, 3534.49it/s]222727it [01:29, 3413.24it/s]225103it [01:29, 3325.98it/s]224047it [01:29, 3549.87it/s]234605it [01:29, 3401.07it/s]223095it [01:29, 3480.69it/s]225469it [01:29, 3419.99it/s]224404it [01:29, 3437.27it/s]234974it [01:29, 3483.04it/s]223445it [01:29, 3359.01it/s]225826it [01:29, 3463.07it/s]224774it [01:29, 3508.08it/s]235324it [01:29, 3374.63it/s]223802it [01:29, 3417.25it/s]226174it [01:29, 3340.24it/s]225127it [01:29, 3378.86it/s]235688it [01:30, 3450.37it/s]224179it [01:29, 3516.55it/s]226530it [01:29, 3402.37it/s]225499it [01:29, 3474.95it/s]236035it [01:30, 3325.75it/s]224533it [01:29, 3384.57it/s]226872it [01:30, 3284.50it/s]225866it [01:30, 3530.76it/s]236398it [01:30, 3412.56it/s]224900it [01:29, 3463.55it/s]227228it [01:30, 3361.37it/s]226221it [01:30, 3401.93it/s]236766it [01:30, 3488.68it/s]225248it [01:30, 3342.42it/s]227576it [01:30, 3259.01it/s]226590it [01:30, 3483.60it/s]237117it [01:30, 3373.07it/s]225613it [01:30, 3429.97it/s]227933it [01:30, 3345.20it/s]237484it [01:30, 3458.13it/s]226940it [01:30, 3347.66it/s]225958it [01:30, 3302.94it/s]228294it [01:30, 3420.83it/s]227303it [01:30, 3426.11it/s]237832it [01:30, 3336.29it/s]226318it [01:30, 3385.80it/s]228638it [01:30, 3314.44it/s]227648it [01:30, 3327.70it/s]238198it [01:30, 3427.04it/s]226681it [01:30, 3454.56it/s]228999it [01:30, 3397.20it/s]228005it [01:30, 3394.51it/s]238543it [01:30, 3319.96it/s]227028it [01:30, 3280.06it/s]229341it [01:30, 3295.75it/s]228375it [01:30, 3480.97it/s]238908it [01:31, 3413.49it/s]227389it [01:30, 3372.48it/s]229710it [01:30, 3408.36it/s]228725it [01:30, 3372.71it/s]239270it [01:31, 3472.84it/s]230070it [01:30, 3463.58it/s]227729it [01:30, 3271.12it/s]229094it [01:31, 3461.91it/s]239619it [01:31, 3356.59it/s]228091it [01:30, 3369.81it/s]230418it [01:31, 3335.23it/s]229442it [01:31, 3374.77it/s]239977it [01:31, 3418.31it/s]228430it [01:31, 3272.57it/s]230779it [01:31, 3414.07it/s]229814it [01:31, 3466.77it/s]240321it [01:31, 3317.73it/s]228794it [01:31, 3376.98it/s]231122it [01:31, 3331.26it/s]230162it [01:31, 3359.26it/s]240689it [01:31, 3419.92it/s]229157it [01:31, 3449.79it/s]231485it [01:31, 3416.76it/s]230524it [01:31, 3433.57it/s]241033it [01:31, 3328.09it/s]229504it [01:31, 3355.36it/s]231828it [01:31, 3311.71it/s]230888it [01:31, 3491.40it/s]241405it [01:31, 3439.54it/s]229871it [01:31, 3444.49it/s]232190it [01:31, 3398.26it/s]231239it [01:31, 3398.14it/s]241778it [01:31, 3521.75it/s]232551it [01:31, 3458.43it/s]230217it [01:31, 3295.48it/s]231608it [01:31, 3482.42it/s]242132it [01:31, 3399.78it/s]230578it [01:31, 3384.39it/s]232898it [01:31, 3312.27it/s]231958it [01:31, 3377.77it/s]242498it [01:32, 3472.72it/s]233266it [01:31, 3415.39it/s]230935it [01:31, 3288.46it/s]232326it [01:31, 3463.32it/s]242847it [01:32, 3360.27it/s]231308it [01:31, 3412.58it/s]233610it [01:32, 3315.64it/s]232674it [01:32, 3362.13it/s]243217it [01:32, 3455.90it/s]231672it [01:31, 3477.34it/s]233973it [01:32, 3405.20it/s]233042it [01:32, 3451.87it/s]243565it [01:32, 3343.27it/s]232022it [01:32, 3346.79it/s]234316it [01:32, 3314.19it/s]233416it [01:32, 3533.09it/s]243923it [01:32, 3410.78it/s]232386it [01:32, 3428.60it/s]234672it [01:32, 3382.83it/s]233771it [01:32, 3372.21it/s]244284it [01:32, 3468.19it/s]235037it [01:32, 3458.72it/s]232731it [01:32, 3293.71it/s]234111it [01:32, 3353.75it/s]244633it [01:32, 3317.36it/s]233063it [01:32, 3299.44it/s]235385it [01:32, 3320.86it/s]234448it [01:32, 3266.97it/s]244999it [01:32, 3412.64it/s]233417it [01:32, 3367.50it/s]235741it [01:32, 3388.93it/s]234785it [01:32, 3289.10it/s]245343it [01:32, 3298.41it/s]233756it [01:32, 3249.80it/s]236082it [01:32, 3282.88it/s]235135it [01:32, 3230.45it/s]245707it [01:33, 3393.73it/s]234122it [01:32, 3366.77it/s]236439it [01:32, 3364.45it/s]235499it [01:32, 3345.68it/s]246055it [01:33, 3297.16it/s]236799it [01:32, 3432.59it/s]234461it [01:32, 3268.20it/s]235865it [01:33, 3434.01it/s]246421it [01:33, 3399.17it/s]234823it [01:32, 3368.34it/s]237144it [01:33, 3309.88it/s]236210it [01:33, 3335.01it/s]246790it [01:33, 3481.86it/s]237506it [01:33, 3396.52it/s]235162it [01:33, 3271.87it/s]236566it [01:33, 3398.39it/s]247140it [01:33, 3370.04it/s]235522it [01:33, 3365.93it/s]237848it [01:33, 3280.40it/s]236907it [01:33, 3311.45it/s]247507it [01:33, 3455.42it/s]235884it [01:33, 3438.37it/s]238206it [01:33, 3365.54it/s]237273it [01:33, 3410.89it/s]247855it [01:33, 3320.20it/s]236230it [01:33, 3294.50it/s]238545it [01:33, 3257.52it/s]237640it [01:33, 3484.08it/s]248219it [01:33, 3409.12it/s]236590it [01:33, 3381.10it/s]238904it [01:33, 3350.68it/s]237990it [01:33, 3369.41it/s]248575it [01:33, 3309.12it/s]239261it [01:33, 3413.65it/s]236930it [01:33, 3279.76it/s]238350it [01:33, 3435.52it/s]248942it [01:33, 3408.92it/s]237290it [01:33, 3370.69it/s]239604it [01:33, 3271.62it/s]238695it [01:33, 3319.71it/s]249306it [01:34, 3473.74it/s]237650it [01:33, 3434.80it/s]239965it [01:33, 3365.91it/s]239030it [01:33, 3326.35it/s]249655it [01:34, 3350.29it/s]240304it [01:34, 3264.81it/s]237995it [01:33, 3245.07it/s]239364it [01:34, 3094.95it/s]250026it [01:34, 3452.62it/s]240665it [01:34, 3361.12it/s]238352it [01:33, 3334.98it/s]239732it [01:34, 3257.72it/s]250373it [01:34, 3352.28it/s]238688it [01:34, 3239.95it/s]241016it [01:34, 3269.81it/s]240097it [01:34, 3369.01it/s]250745it [01:34, 3452.78it/s]239048it [01:34, 3342.17it/s]241382it [01:34, 3380.72it/s]240438it [01:34, 3278.25it/s]251095it [01:34, 3346.11it/s]241748it [01:34, 3460.04it/s]239385it [01:34, 3213.95it/s]240805it [01:34, 3382.51it/s]251461it [01:34, 3434.15it/s]242096it [01:34, 3339.03it/s]239751it [01:34, 3338.63it/s]241146it [01:34, 3307.66it/s]251814it [01:34, 3460.42it/s]242459it [01:34, 3421.24it/s]240109it [01:34, 3407.51it/s]241509it [01:34, 3398.87it/s]252162it [01:34, 3352.36it/s]242803it [01:34, 3310.95it/s]240452it [01:34, 3297.88it/s]241855it [01:34, 3284.94it/s]252521it [01:35, 3418.84it/s]243166it [01:34, 3401.42it/s]240812it [01:34, 3383.28it/s]242220it [01:34, 3386.77it/s]252865it [01:35, 3307.88it/s]243528it [01:34, 3464.75it/s]241152it [01:34, 3287.76it/s]242587it [01:35, 3466.90it/s]253231it [01:35, 3408.70it/s]241512it [01:34, 3374.93it/s]243876it [01:35, 3343.64it/s]242936it [01:35, 3346.11it/s]253597it [01:35, 3481.48it/s]244235it [01:35, 3412.84it/s]241855it [01:35, 3285.99it/s]243305it [01:35, 3443.74it/s]242209it [01:35, 3356.43it/s]244578it [01:35, 3298.88it/s]243652it [01:35, 3343.02it/s]242572it [01:35, 3435.63it/s]244937it [01:35, 3381.23it/s]244021it [01:35, 3442.47it/s]245277it [01:35, 3281.48it/s]242917it [01:35, 3312.49it/s]244375it [01:35, 3323.24it/s]245638it [01:35, 3374.09it/s]243283it [01:35, 3410.93it/s]244731it [01:35, 3388.43it/s]245989it [01:35, 3412.45it/s]243626it [01:35, 3272.45it/s]245100it [01:35, 3473.95it/s]246332it [01:35, 3307.33it/s]243992it [01:35, 3381.52it/s]245449it [01:35, 3348.24it/s]246691it [01:35, 3386.86it/s]244354it [01:35, 3449.29it/s]245819it [01:35, 3447.60it/s]247031it [01:36, 3299.32it/s]244701it [01:35, 3331.71it/s]246166it [01:36, 3340.86it/s]247392it [01:36, 3386.55it/s]245067it [01:35, 3425.74it/s]246536it [01:36, 3441.89it/s]247736it [01:36, 3293.66it/s]245412it [01:36, 3286.22it/s]246895it [01:36, 3338.75it/s]248100it [01:36, 3391.14it/s]245778it [01:36, 3390.02it/s]247266it [01:36, 3443.10it/s]248463it [01:36, 3458.81it/s]246120it [01:36, 3297.52it/s]247622it [01:36, 3476.35it/s]248810it [01:36, 3343.93it/s]246483it [01:36, 3392.07it/s]247972it [01:36, 3310.08it/s]249177it [01:36, 3436.03it/s]246849it [01:36, 3467.79it/s]248339it [01:36, 3410.83it/s]249522it [01:36, 3327.38it/s]247198it [01:36, 3347.80it/s]248683it [01:36, 3306.71it/s]249882it [01:36, 3405.17it/s]247564it [01:36, 3435.87it/s]249049it [01:36, 3405.81it/s]250247it [01:36, 3475.53it/s]247910it [01:36, 3331.86it/s]249415it [01:37, 3309.28it/s]250596it [01:37, 3370.91it/s]248276it [01:36, 3423.76it/s]250959it [01:37, 3444.97it/s]249782it [01:37, 3409.68it/s]248620it [01:37, 3280.17it/s]250154it [01:37, 3498.03it/s]251305it [01:37, 3343.19it/s]248991it [01:37, 3400.50it/s]251661it [01:37, 3404.03it/s]250506it [01:37, 3365.79it/s]249355it [01:37, 3469.42it/s]250879it [01:37, 3468.45it/s]252003it [01:37, 3305.10it/s]249704it [01:37, 3346.03it/s]251228it [01:37, 3355.45it/s]252335it [01:37, 3233.32it/s]250073it [01:37, 3443.02it/s]251593it [01:37, 3436.95it/s]252694it [01:37, 3333.16it/s]250420it [01:37, 3334.98it/s]253029it [01:37, 3257.48it/s]251939it [01:37, 3314.97it/s]250791it [01:37, 3439.91it/s]253390it [01:37, 3357.79it/s]252312it [01:37, 3431.90it/s]251137it [01:37, 3324.12it/s]252676it [01:37, 3490.14it/s]251490it [01:37, 3382.33it/s]253027it [01:38, 3372.31it/s]251855it [01:37, 3458.63it/s]253385it [01:38, 3430.76it/s]252203it [01:38, 3349.67it/s]252565it [01:38, 3424.87it/s]252909it [01:38, 3329.08it/s]253277it [01:38, 3429.16it/s]253622it [01:38, 3324.57it/s]253947it [01:42, 169.84it/s] 254310it [01:42, 239.05it/s]254609it [01:42, 315.46it/s]254975it [01:42, 443.08it/s]255339it [01:42, 608.30it/s]255665it [01:42, 787.10it/s]256031it [01:42, 1042.90it/s]256365it [01:42, 1289.98it/s]256733it [01:42, 1618.72it/s]257072it [01:43, 1848.41it/s]257427it [01:43, 2162.89it/s]257792it [01:43, 2473.30it/s]258134it [01:43, 2563.38it/s]258503it [01:43, 2830.49it/s]258840it [01:43, 2911.64it/s]259211it [01:43, 3119.76it/s]259564it [01:43, 3112.05it/s]259936it [01:43, 3276.51it/s]260308it [01:43, 3399.99it/s]260661it [01:44, 3313.57it/s]261034it [01:44, 3430.76it/s]261385it [01:44, 3320.64it/s]261762it [01:44, 3446.07it/s]262112it [01:44, 3350.90it/s]262486it [01:44, 3459.34it/s]262857it [01:44, 3531.29it/s]253727it [01:44, 165.46it/s] 263213it [01:44, 3411.12it/s]254093it [01:44, 235.64it/s]263583it [01:44, 3492.33it/s]254456it [01:44, 330.28it/s]263935it [01:45, 3388.99it/s]254758it [01:44, 433.29it/s]264314it [01:45, 3501.19it/s]255124it [01:45, 600.66it/s]264666it [01:45, 3395.76it/s]255445it [01:45, 777.50it/s]265036it [01:45, 3482.82it/s]255815it [01:45, 1038.67it/s]265412it [01:45, 3561.04it/s]256180it [01:45, 1334.00it/s]265770it [01:45, 3417.25it/s]256522it [01:45, 1602.05it/s]266141it [01:45, 3500.06it/s]253730it [01:45, 157.31it/s] 256888it [01:45, 1939.08it/s]254096it [01:45, 222.65it/s]266493it [01:45, 3389.84it/s]257231it [01:45, 2170.44it/s]254458it [01:45, 310.85it/s]266863it [01:45, 3477.81it/s]257590it [01:45, 2467.05it/s]254763it [01:45, 408.27it/s]267213it [01:46, 3376.34it/s]257930it [01:45, 2590.50it/s]255130it [01:45, 566.53it/s]267588it [01:46, 3480.93it/s]258285it [01:45, 2820.36it/s]255451it [01:45, 735.21it/s]267961it [01:46, 3552.46it/s]258647it [01:46, 3024.43it/s]255821it [01:46, 985.71it/s]253956it [01:45, 150.76it/s] 268318it [01:46, 3432.86it/s]258990it [01:46, 3045.90it/s]256188it [01:46, 1274.71it/s]254316it [01:46, 213.58it/s]268690it [01:46, 3514.67it/s]259345it [01:46, 3182.50it/s]256531it [01:46, 1544.16it/s]254615it [01:46, 284.03it/s]269043it [01:46, 3402.95it/s]259685it [01:46, 3141.97it/s]256904it [01:46, 1887.71it/s]254982it [01:46, 402.48it/s]269414it [01:46, 3490.19it/s]260047it [01:46, 3273.49it/s]255347it [01:46, 556.98it/s]257249it [01:46, 2136.56it/s]269765it [01:46, 3357.08it/s]255671it [01:46, 725.34it/s]260405it [01:46, 3200.08it/s]257604it [01:46, 2424.70it/s]270140it [01:46, 3466.80it/s]256036it [01:46, 968.26it/s]260766it [01:46, 3314.32it/s]257945it [01:46, 2598.83it/s]270489it [01:46, 3366.30it/s]261129it [01:46, 3402.46it/s]256369it [01:46, 1207.98it/s]258314it [01:46, 2860.40it/s]270860it [01:47, 3462.71it/s]256737it [01:46, 1530.38it/s]258687it [01:46, 3082.26it/s]261475it [01:46, 3288.16it/s]271229it [01:47, 3526.73it/s]261839it [01:47, 3386.99it/s]257075it [01:46, 1793.74it/s]259040it [01:47, 3116.66it/s]271584it [01:47, 3411.11it/s]257424it [01:46, 2100.03it/s]259408it [01:47, 3268.31it/s]262182it [01:47, 3265.12it/s]271954it [01:47, 3491.85it/s]257789it [01:47, 2417.42it/s]262537it [01:47, 3345.13it/s]259759it [01:47, 3235.74it/s]272305it [01:47, 3372.05it/s]260127it [01:47, 3357.79it/s]262875it [01:47, 3287.59it/s]258133it [01:47, 2549.04it/s]272673it [01:47, 3459.03it/s]258493it [01:47, 2797.89it/s]260476it [01:47, 3261.63it/s]263206it [01:47, 3135.30it/s]273021it [01:47, 3353.37it/s]258831it [01:47, 2879.91it/s]260846it [01:47, 3382.75it/s]263566it [01:47, 3265.32it/s]273384it [01:47, 3430.42it/s]259199it [01:47, 3088.46it/s]261218it [01:47, 3477.51it/s]263896it [01:47, 3221.57it/s]273742it [01:47, 3473.21it/s]261572it [01:47, 3382.53it/s]264253it [01:47, 3321.04it/s]259563it [01:47, 3088.10it/s]274091it [01:47, 3377.75it/s]261944it [01:47, 3475.36it/s]259928it [01:47, 3239.39it/s]264605it [01:47, 3239.80it/s]274458it [01:48, 3460.17it/s]260296it [01:47, 3359.57it/s]262295it [01:47, 3364.54it/s]264968it [01:47, 3348.68it/s]274806it [01:48, 3347.62it/s]262665it [01:48, 3458.18it/s]265329it [01:48, 3422.63it/s]260646it [01:47, 3247.22it/s]275174it [01:48, 3439.93it/s]261017it [01:48, 3376.56it/s]263014it [01:48, 3363.49it/s]265673it [01:48, 3311.85it/s]275524it [01:48, 3354.81it/s]263373it [01:48, 3425.97it/s]266038it [01:48, 3408.54it/s]261363it [01:48, 3295.50it/s]275892it [01:48, 3447.52it/s]263747it [01:48, 3515.09it/s]261732it [01:48, 3405.81it/s]266381it [01:48, 3315.91it/s]276261it [01:48, 3516.35it/s]264101it [01:48, 3410.42it/s]266750it [01:48, 3421.73it/s]262084it [01:48, 3307.14it/s]276614it [01:48, 3395.25it/s]264475it [01:48, 3505.32it/s]267119it [01:48, 3497.32it/s]262453it [01:48, 3414.45it/s]276984it [01:48, 3481.42it/s]262822it [01:48, 3493.39it/s]264828it [01:48, 3395.78it/s]267470it [01:48, 3383.72it/s]277334it [01:48, 3374.71it/s]265200it [01:48, 3488.48it/s]267835it [01:48, 3458.92it/s]263174it [01:48, 3370.30it/s]277688it [01:49, 3420.04it/s]265551it [01:48, 3380.28it/s]263539it [01:48, 3447.98it/s]268183it [01:48, 3273.07it/s]278044it [01:49, 3320.76it/s]265924it [01:49, 3480.47it/s]268524it [01:49, 3309.18it/s]263886it [01:48, 3317.92it/s]278410it [01:49, 3416.61it/s]264254it [01:48, 3420.80it/s]266284it [01:49, 3352.84it/s]268858it [01:49, 3243.29it/s]278781it [01:49, 3499.85it/s]266660it [01:49, 3466.84it/s]269222it [01:49, 3356.75it/s]264604it [01:49, 3322.96it/s]279133it [01:49, 3387.32it/s]267033it [01:49, 3540.06it/s]269585it [01:49, 3433.74it/s]264972it [01:49, 3422.90it/s]279501it [01:49, 3470.00it/s]267389it [01:49, 3433.17it/s]265344it [01:49, 3506.76it/s]269930it [01:49, 3336.56it/s]279850it [01:49, 3359.65it/s]267761it [01:49, 3513.52it/s]270295it [01:49, 3425.64it/s]265697it [01:49, 3389.38it/s]280214it [01:49, 3433.36it/s]268114it [01:49, 3417.12it/s]266067it [01:49, 3476.76it/s]270639it [01:49, 3296.55it/s]280564it [01:49, 3341.29it/s]268491it [01:49, 3516.56it/s]271003it [01:49, 3394.54it/s]266417it [01:49, 3364.04it/s]280936it [01:49, 3448.32it/s]268845it [01:49, 3410.69it/s]266774it [01:49, 3421.80it/s]271345it [01:49, 3297.79it/s]281303it [01:50, 3511.24it/s]269204it [01:49, 3460.75it/s]271707it [01:49, 3389.35it/s]267124it [01:49, 3324.21it/s]281656it [01:50, 3357.72it/s]269574it [01:50, 3520.89it/s]272070it [01:50, 3459.20it/s]267493it [01:49, 3428.14it/s]282021it [01:50, 3440.34it/s]269928it [01:50, 3419.11it/s]267862it [01:50, 3503.59it/s]272418it [01:50, 3330.08it/s]282367it [01:50, 3337.12it/s]270301it [01:50, 3508.09it/s]272778it [01:50, 3405.27it/s]268214it [01:50, 3387.41it/s]282733it [01:50, 3428.58it/s]270653it [01:50, 3399.66it/s]268584it [01:50, 3476.23it/s]273121it [01:50, 3303.13it/s]283084it [01:50, 3321.79it/s]271024it [01:50, 3486.80it/s]273482it [01:50, 3390.43it/s]268934it [01:50, 3356.63it/s]283449it [01:50, 3413.34it/s]271374it [01:50, 3383.55it/s]269298it [01:50, 3435.15it/s]273845it [01:50, 3295.09it/s]283816it [01:50, 3486.90it/s]271741it [01:50, 3464.22it/s]274206it [01:50, 3383.01it/s]269644it [01:50, 3325.41it/s]284167it [01:50, 3365.47it/s]272099it [01:50, 3496.73it/s]274566it [01:50, 3445.37it/s]270004it [01:50, 3403.41it/s]284533it [01:51, 3447.92it/s]272450it [01:50, 3376.84it/s]270373it [01:50, 3484.35it/s]274913it [01:50, 3327.70it/s]284880it [01:51, 3341.19it/s]272819it [01:51, 3466.78it/s]270723it [01:50, 3362.81it/s]275248it [01:51, 3250.75it/s]285247it [01:51, 3435.12it/s]273168it [01:51, 3360.49it/s]271091it [01:50, 3451.21it/s]275575it [01:51, 3153.40it/s]285604it [01:51, 3309.83it/s]273539it [01:51, 3458.86it/s]271438it [01:51, 3335.71it/s]275940it [01:51, 3292.95it/s]285974it [01:51, 3418.55it/s]273887it [01:51, 3364.26it/s]271804it [01:51, 3425.86it/s]276300it [01:51, 3379.59it/s]286344it [01:51, 3498.85it/s]274258it [01:51, 3462.43it/s]276640it [01:51, 3287.05it/s]272164it [01:51, 3323.07it/s]286696it [01:51, 3393.74it/s]274625it [01:51, 3521.27it/s]276993it [01:51, 3356.50it/s]272527it [01:51, 3408.35it/s]287071it [01:51, 3494.58it/s]287112it [01:51, 2567.97it/s]
2022-07-26 11:31:05 | INFO | root | success load 287112 data
2022-07-26 11:31:05 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-26 11:31:05 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-26 11:31:05 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-26 11:31:05 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-26 11:31:05 | INFO | transformer.tokenization_utils | loading file None
2022-07-26 11:31:05 | INFO | transformer.tokenization_utils | loading file None
2022-07-26 11:31:05 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
274979it [01:51, 3375.39it/s]272892it [01:51, 3477.26it/s]277330it [01:51, 3267.26it/s]275356it [01:51, 3486.55it/s]277688it [01:51, 3356.79it/s]273242it [01:51, 3316.25it/s]275707it [01:51, 3383.66it/s]273608it [01:51, 3412.08it/s]278045it [01:51, 3258.89it/s]276079it [01:51, 3477.75it/s]278405it [01:51, 3354.17it/s]273952it [01:51, 3310.47it/s]276429it [01:52, 3374.51it/s]278768it [01:52, 3433.45it/s]274315it [01:51, 3400.30it/s]276797it [01:52, 3461.92it/s]274680it [01:52, 3471.64it/s]279113it [01:52, 3323.36it/s]277175it [01:52, 3553.67it/s]279475it [01:52, 3407.95it/s]275029it [01:52, 3351.41it/s]277532it [01:52, 3392.51it/s]275391it [01:52, 3428.08it/s]279818it [01:52, 3292.18it/s]277903it [01:52, 3482.40it/s]280174it [01:52, 3366.32it/s]275736it [01:52, 3311.41it/s]278254it [01:52, 3360.59it/s]280532it [01:52, 3426.92it/s]276091it [01:52, 3378.21it/s]278627it [01:52, 3464.24it/s]276431it [01:52, 3288.65it/s]280876it [01:52, 3177.93it/s]278976it [01:52, 3374.66it/s]276794it [01:52, 3384.80it/s]281198it [01:52, 3189.38it/s]279346it [01:52, 3466.19it/s]277162it [01:52, 3469.44it/s]281520it [01:52, 3150.38it/s]279719it [01:53, 3542.71it/s]277511it [01:52, 3337.22it/s]281881it [01:53, 3280.27it/s]280075it [01:53, 3410.14it/s]277877it [01:52, 3427.72it/s]282241it [01:53, 3372.28it/s]280437it [01:53, 3467.87it/s]282580it [01:53, 3266.99it/s]278222it [01:53, 3303.99it/s]280786it [01:53, 3367.64it/s]282938it [01:53, 3356.59it/s]278588it [01:53, 3403.41it/s]281154it [01:53, 3455.51it/s]283276it [01:53, 3255.53it/s]278931it [01:53, 3298.92it/s]281501it [01:53, 3358.88it/s]283624it [01:53, 3318.27it/s]279285it [01:53, 3366.37it/s]281870it [01:53, 3451.84it/s]279651it [01:53, 3448.90it/s]283958it [01:53, 3227.44it/s]282217it [01:53, 3441.94it/s]284320it [01:53, 3339.08it/s]279998it [01:53, 3317.96it/s]282563it [01:53, 3266.20it/s]284677it [01:53, 3403.65it/s]280354it [01:53, 3384.58it/s]282922it [01:53, 3356.55it/s]285019it [01:54, 3288.52it/s]280694it [01:53, 3288.00it/s]283260it [01:54, 3244.94it/s]285380it [01:54, 3379.92it/s]281055it [01:53, 3378.87it/s]283627it [01:54, 3364.64it/s]285720it [01:54, 3277.48it/s]281404it [01:54, 3279.25it/s]283966it [01:54, 3283.62it/s]286082it [01:54, 3374.85it/s]281767it [01:54, 3378.16it/s]284336it [01:54, 3402.33it/s]282119it [01:54, 3418.54it/s]286445it [01:54, 3283.43it/s]284702it [01:54, 3475.10it/s]286811it [01:54, 3389.07it/s]282463it [01:54, 3308.00it/s]287112it [01:54, 2505.05it/s]
285051it [01:54, 3373.26it/s]282825it [01:54, 3396.80it/s]285421it [01:54, 3467.48it/s]283167it [01:54, 3282.64it/s]285770it [01:54, 3349.13it/s]283532it [01:54, 3386.44it/s]286131it [01:54, 3423.91it/s]283896it [01:54, 3459.32it/s]286475it [01:55, 3341.14it/s]284244it [01:54, 3346.17it/s]286849it [01:55, 3455.25it/s]284606it [01:54, 3423.16it/s]287112it [01:55, 2492.50it/s]
284950it [01:55, 3313.83it/s]285306it [01:55, 3383.99it/s]285646it [01:55, 3286.38it/s]286015it [01:55, 3401.33it/s]286380it [01:55, 3472.20it/s]286729it [01:55, 3349.04it/s]287103it [01:55, 3458.83it/s]287112it [01:55, 2480.69it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-26 11:33:48 | INFO | train_inner | epoch 013:     53 / 1122 loss=8.784, nll_loss=3.183, mask_ins=1.674, word_ins_ml=4.745, word_reposition=1.493, kpe=0.871, ppl=440.84, wps=4065.2, ups=0.2, wpb=20393.7, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=1.958, clip=0, loss_scale=512, train_wall=260, wall=0
2022-07-26 11:37:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-26 11:38:45 | INFO | train_inner | epoch 013:    154 / 1122 loss=8.753, nll_loss=3.168, mask_ins=1.666, word_ins_ml=4.732, word_reposition=1.489, kpe=0.866, ppl=431.37, wps=6923.9, ups=0.34, wpb=20601.7, bsz=256, num_updates=13600, lr=0.00030317, gnorm=1.739, clip=0, loss_scale=446, train_wall=256, wall=0
2022-07-26 11:43:37 | INFO | train_inner | epoch 013:    254 / 1122 loss=8.762, nll_loss=3.175, mask_ins=1.666, word_ins_ml=4.738, word_reposition=1.493, kpe=0.865, ppl=434.02, wps=7017.8, ups=0.34, wpb=20454.1, bsz=256, num_updates=13700, lr=0.000302061, gnorm=1.617, clip=0, loss_scale=256, train_wall=251, wall=0
2022-07-26 11:48:28 | INFO | train_inner | epoch 013:    354 / 1122 loss=nan, nll_loss=3.192, mask_ins=1.648, word_ins_ml=4.753, word_reposition=1.484, kpe=nan, ppl=nan, wps=7058.5, ups=0.34, wpb=20530.2, bsz=256, num_updates=13800, lr=0.000300965, gnorm=1.571, clip=0, loss_scale=256, train_wall=251, wall=0
2022-07-26 11:53:20 | INFO | train_inner | epoch 013:    454 / 1122 loss=8.752, nll_loss=3.176, mask_ins=1.663, word_ins_ml=4.739, word_reposition=1.483, kpe=0.867, ppl=431.02, wps=7055.1, ups=0.34, wpb=20590.5, bsz=256, num_updates=13900, lr=0.00029988, gnorm=1.611, clip=0, loss_scale=256, train_wall=251, wall=0
2022-07-26 11:58:36 | INFO | train_inner | epoch 013:    554 / 1122 loss=8.723, nll_loss=3.161, mask_ins=1.653, word_ins_ml=4.726, word_reposition=1.481, kpe=0.863, ppl=422.65, wps=6493.3, ups=0.32, wpb=20525.6, bsz=256, num_updates=14000, lr=0.000298807, gnorm=1.574, clip=0, loss_scale=256, train_wall=276, wall=0
2022-07-26 12:03:26 | INFO | train_inner | epoch 013:    654 / 1122 loss=8.716, nll_loss=3.173, mask_ins=1.647, word_ins_ml=4.737, word_reposition=1.466, kpe=0.867, ppl=420.51, wps=7048, ups=0.34, wpb=20467.9, bsz=256, num_updates=14100, lr=0.000297746, gnorm=1.586, clip=0, loss_scale=292, train_wall=251, wall=0
2022-07-26 12:08:16 | INFO | train_inner | epoch 013:    754 / 1122 loss=nan, nll_loss=3.188, mask_ins=1.636, word_ins_ml=4.749, word_reposition=1.48, kpe=nan, ppl=nan, wps=7093.7, ups=0.35, wpb=20534.2, bsz=256, num_updates=14200, lr=0.000296695, gnorm=1.56, clip=0, loss_scale=512, train_wall=251, wall=0
2022-07-26 12:13:07 | INFO | train_inner | epoch 013:    854 / 1122 loss=8.752, nll_loss=3.179, mask_ins=1.65, word_ins_ml=4.741, word_reposition=1.498, kpe=0.863, ppl=431.28, wps=7038.2, ups=0.34, wpb=20509.8, bsz=256, num_updates=14300, lr=0.000295656, gnorm=1.538, clip=0, loss_scale=512, train_wall=252, wall=0
2022-07-26 12:17:57 | INFO | train_inner | epoch 013:    954 / 1122 loss=8.702, nll_loss=3.132, mask_ins=1.65, word_ins_ml=4.7, word_reposition=1.483, kpe=0.869, ppl=416.35, wps=7063.7, ups=0.34, wpb=20475.1, bsz=256, num_updates=14400, lr=0.000294628, gnorm=1.597, clip=0, loss_scale=512, train_wall=250, wall=0
2022-07-26 12:22:49 | INFO | train_inner | epoch 013:   1054 / 1122 loss=8.659, nll_loss=3.137, mask_ins=1.643, word_ins_ml=4.704, word_reposition=1.444, kpe=0.868, ppl=404.3, wps=7077.6, ups=0.34, wpb=20651, bsz=256, num_updates=14500, lr=0.00029361, gnorm=1.649, clip=0, loss_scale=512, train_wall=252, wall=0
2022-07-26 12:26:07 | INFO | train | epoch 013 | loss nan | nll_loss 3.168 | mask_ins 1.653 | word_ins_ml 4.732 | word_reposition 1.48 | kpe nan | ppl nan | wps 6572.8 | ups 0.32 | wpb 20520.5 | bsz 255.8 | num_updates 14568 | lr 0.000292924 | gnorm 1.613 | clip 0 | loss_scale 395 | train_wall 2852 | wall 0
2022-07-26 12:27:29 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 15.372 | nll_loss 7.257 | mask_ins 2.723 | word_ins_ml 8.486 | word_reposition 2.753 | kpe 1.409 | ppl 42395.7 | wps 12187.7 | wpb 2367.6 | bsz 32 | num_updates 14568 | best_loss 15.153
2022-07-26 12:27:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_last.pt (epoch 13 @ 14568 updates, score 15.372) (writing took 13.015959592536092 seconds)
2022-07-26 12:29:14 | INFO | train_inner | epoch 014:     32 / 1122 loss=8.716, nll_loss=3.151, mask_ins=1.647, word_ins_ml=4.716, word_reposition=1.485, kpe=0.868, ppl=420.64, wps=5289.2, ups=0.26, wpb=20393.2, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=1.56, clip=0, loss_scale=522, train_wall=252, wall=0
2022-07-26 12:34:05 | INFO | train_inner | epoch 014:    132 / 1122 loss=8.695, nll_loss=3.137, mask_ins=1.657, word_ins_ml=4.704, word_reposition=1.481, kpe=0.853, ppl=414.43, wps=7064.6, ups=0.34, wpb=20514.1, bsz=256, num_updates=14700, lr=0.000291606, gnorm=1.585, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-26 12:38:54 | INFO | train_inner | epoch 014:    232 / 1122 loss=8.664, nll_loss=3.137, mask_ins=1.635, word_ins_ml=4.704, word_reposition=1.471, kpe=0.853, ppl=405.67, wps=7087.5, ups=0.35, wpb=20534.4, bsz=256, num_updates=14800, lr=0.000290619, gnorm=1.606, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-26 12:43:45 | INFO | train_inner | epoch 014:    332 / 1122 loss=nan, nll_loss=3.119, mask_ins=1.637, word_ins_ml=4.688, word_reposition=1.473, kpe=nan, ppl=nan, wps=7081.9, ups=0.34, wpb=20558.7, bsz=256, num_updates=14900, lr=0.000289642, gnorm=1.6, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-26 12:48:34 | INFO | train_inner | epoch 014:    432 / 1122 loss=8.679, nll_loss=3.154, mask_ins=1.639, word_ins_ml=4.718, word_reposition=1.465, kpe=0.857, ppl=409.94, wps=7086.9, ups=0.35, wpb=20527.2, bsz=256, num_updates=15000, lr=0.000288675, gnorm=1.613, clip=0, loss_scale=1024, train_wall=250, wall=0
2022-07-26 12:53:23 | INFO | train_inner | epoch 014:    532 / 1122 loss=8.613, nll_loss=3.092, mask_ins=1.632, word_ins_ml=4.664, word_reposition=1.464, kpe=0.852, ppl=391.45, wps=7090.2, ups=0.35, wpb=20504.7, bsz=256, num_updates=15100, lr=0.000287718, gnorm=1.62, clip=0, loss_scale=1024, train_wall=250, wall=0
2022-07-26 12:58:51 | INFO | train_inner | epoch 014:    632 / 1122 loss=8.633, nll_loss=3.103, mask_ins=1.638, word_ins_ml=4.673, word_reposition=1.465, kpe=0.857, ppl=397.13, wps=6261.7, ups=0.31, wpb=20491.1, bsz=256, num_updates=15200, lr=0.00028677, gnorm=1.628, clip=0, loss_scale=1946, train_wall=287, wall=0
2022-07-26 13:03:41 | INFO | train_inner | epoch 014:    732 / 1122 loss=8.617, nll_loss=3.101, mask_ins=1.629, word_ins_ml=4.672, word_reposition=1.459, kpe=0.857, ppl=392.58, wps=7125.9, ups=0.34, wpb=20660.2, bsz=256, num_updates=15300, lr=0.000285831, gnorm=1.499, clip=0, loss_scale=2048, train_wall=251, wall=0
2022-07-26 13:08:32 | INFO | train_inner | epoch 014:    832 / 1122 loss=8.692, nll_loss=3.146, mask_ins=1.643, word_ins_ml=4.712, word_reposition=1.477, kpe=0.861, ppl=413.66, wps=7001.9, ups=0.34, wpb=20403.9, bsz=256, num_updates=15400, lr=0.000284901, gnorm=1.513, clip=0, loss_scale=2048, train_wall=252, wall=0
2022-07-26 13:13:26 | INFO | train_inner | epoch 014:    932 / 1122 loss=8.696, nll_loss=3.123, mask_ins=1.654, word_ins_ml=4.691, word_reposition=1.488, kpe=0.863, ppl=414.64, wps=6973.9, ups=0.34, wpb=20518.6, bsz=256, num_updates=15500, lr=0.000283981, gnorm=1.492, clip=0, loss_scale=2048, train_wall=254, wall=0
2022-07-26 13:18:19 | INFO | train_inner | epoch 014:   1032 / 1122 loss=8.653, nll_loss=3.128, mask_ins=1.64, word_ins_ml=4.695, word_reposition=1.46, kpe=0.859, ppl=402.66, wps=7026.5, ups=0.34, wpb=20542.7, bsz=256, num_updates=15600, lr=0.000283069, gnorm=1.477, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-07-26 13:22:40 | INFO | train | epoch 014 | loss nan | nll_loss 3.124 | mask_ins 1.641 | word_ins_ml 4.692 | word_reposition 1.47 | kpe nan | ppl nan | wps 6786.5 | ups 0.33 | wpb 20521.1 | bsz 255.8 | num_updates 15690 | lr 0.000282256 | gnorm 1.56 | clip 0 | loss_scale 1664 | train_wall 2856 | wall 0
2022-07-26 13:24:03 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 15.538 | nll_loss 7.343 | mask_ins 2.736 | word_ins_ml 8.571 | word_reposition 2.783 | kpe 1.447 | ppl 47564.9 | wps 11875.8 | wpb 2367.6 | bsz 32 | num_updates 15690 | best_loss 15.153
2022-07-26 13:24:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_last.pt (epoch 14 @ 15690 updates, score 15.538) (writing took 13.516270014457405 seconds)
2022-07-26 13:24:47 | INFO | train_inner | epoch 015:     10 / 1122 loss=nan, nll_loss=3.122, mask_ins=1.643, word_ins_ml=4.69, word_reposition=1.465, kpe=nan, ppl=nan, wps=5267, ups=0.26, wpb=20450.5, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=1.538, clip=0, loss_scale=3645, train_wall=252, wall=0
2022-07-26 13:28:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-26 13:29:45 | INFO | train_inner | epoch 015:    111 / 1122 loss=8.637, nll_loss=3.107, mask_ins=1.645, word_ins_ml=4.677, word_reposition=1.473, kpe=0.842, ppl=397.99, wps=6925.2, ups=0.34, wpb=20670.8, bsz=256, num_updates=15800, lr=0.000281272, gnorm=1.487, clip=0, loss_scale=3488, train_wall=258, wall=0
2022-07-26 13:34:40 | INFO | train_inner | epoch 015:    211 / 1122 loss=8.589, nll_loss=3.1, mask_ins=1.622, word_ins_ml=4.67, word_reposition=1.456, kpe=0.84, ppl=385.15, wps=6981.2, ups=0.34, wpb=20568.6, bsz=256, num_updates=15900, lr=0.000280386, gnorm=1.486, clip=0, loss_scale=2048, train_wall=255, wall=0
2022-07-26 13:39:30 | INFO | train_inner | epoch 015:    311 / 1122 loss=nan, nll_loss=3.063, mask_ins=1.626, word_ins_ml=4.638, word_reposition=1.466, kpe=nan, ppl=nan, wps=7091.7, ups=0.35, wpb=20538, bsz=256, num_updates=16000, lr=0.000279508, gnorm=1.493, clip=0, loss_scale=2048, train_wall=250, wall=0
2022-07-26 13:44:23 | INFO | train_inner | epoch 015:    411 / 1122 loss=8.584, nll_loss=3.072, mask_ins=1.633, word_ins_ml=4.645, word_reposition=1.462, kpe=0.845, ppl=383.76, wps=7010.3, ups=0.34, wpb=20551.2, bsz=256, num_updates=16100, lr=0.000278639, gnorm=1.493, clip=0, loss_scale=2048, train_wall=254, wall=0
2022-07-26 13:46:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-26 13:49:20 | INFO | train_inner | epoch 015:    512 / 1122 loss=8.571, nll_loss=3.073, mask_ins=1.629, word_ins_ml=4.646, word_reposition=1.451, kpe=0.845, ppl=380.19, wps=6887.3, ups=0.34, wpb=20467, bsz=256, num_updates=16200, lr=0.000277778, gnorm=1.497, clip=0, loss_scale=1490, train_wall=257, wall=0
2022-07-26 13:54:17 | INFO | train_inner | epoch 015:    612 / 1122 loss=nan, nll_loss=3.108, mask_ins=1.638, word_ins_ml=4.677, word_reposition=1.464, kpe=nan, ppl=nan, wps=6917, ups=0.34, wpb=20545.4, bsz=256, num_updates=16300, lr=0.000276924, gnorm=1.546, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-26 13:59:53 | INFO | train_inner | epoch 015:    712 / 1122 loss=8.617, nll_loss=3.105, mask_ins=1.63, word_ins_ml=4.675, word_reposition=1.465, kpe=0.847, ppl=392.68, wps=6089.3, ups=0.3, wpb=20458.1, bsz=256, num_updates=16400, lr=0.000276079, gnorm=1.543, clip=0, loss_scale=1024, train_wall=294, wall=0
2022-07-26 14:04:48 | INFO | train_inner | epoch 015:    812 / 1122 loss=8.615, nll_loss=3.104, mask_ins=1.628, word_ins_ml=4.674, word_reposition=1.46, kpe=0.853, ppl=392.05, wps=6952.6, ups=0.34, wpb=20536.7, bsz=256, num_updates=16500, lr=0.000275241, gnorm=1.522, clip=0, loss_scale=1024, train_wall=256, wall=0
2022-07-26 14:09:40 | INFO | train_inner | epoch 015:    912 / 1122 loss=8.595, nll_loss=3.082, mask_ins=1.626, word_ins_ml=4.654, word_reposition=1.469, kpe=0.847, ppl=386.67, wps=7028.4, ups=0.34, wpb=20492.2, bsz=256, num_updates=16600, lr=0.000274411, gnorm=1.496, clip=0, loss_scale=1024, train_wall=251, wall=0
2022-07-26 14:14:34 | INFO | train_inner | epoch 015:   1012 / 1122 loss=8.608, nll_loss=3.099, mask_ins=1.626, word_ins_ml=4.668, word_reposition=1.466, kpe=0.847, ppl=390.11, wps=6978, ups=0.34, wpb=20522.4, bsz=256, num_updates=16700, lr=0.000273588, gnorm=1.48, clip=0, loss_scale=1464, train_wall=254, wall=0
2022-07-26 14:19:29 | INFO | train_inner | epoch 015:   1112 / 1122 loss=8.561, nll_loss=3.048, mask_ins=1.623, word_ins_ml=4.623, word_reposition=1.466, kpe=0.849, ppl=377.72, wps=6992.4, ups=0.34, wpb=20605.1, bsz=256, num_updates=16800, lr=0.000272772, gnorm=1.466, clip=0, loss_scale=2048, train_wall=254, wall=0
2022-07-26 14:19:57 | INFO | train | epoch 015 | loss nan | nll_loss 3.087 | mask_ins 1.63 | word_ins_ml 4.659 | word_reposition 1.463 | kpe nan | ppl nan | wps 6687.6 | ups 0.33 | wpb 20521.7 | bsz 255.8 | num_updates 16810 | lr 0.000272691 | gnorm 1.504 | clip 0 | loss_scale 1729 | train_wall 2890 | wall 0
2022-07-26 14:21:19 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 15.221 | nll_loss 7.194 | mask_ins 2.689 | word_ins_ml 8.433 | word_reposition 2.652 | kpe 1.448 | ppl 38196.8 | wps 12028.9 | wpb 2367.6 | bsz 32 | num_updates 16810 | best_loss 15.153
2022-07-26 14:21:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_goldenkeywords_cased/checkpoint_last.pt (epoch 15 @ 16810 updates, score 15.221) (writing took 15.905262529850006 seconds)
2022-07-26 14:25:58 | INFO | train_inner | epoch 016:     90 / 1122 loss=nan, nll_loss=3.045, mask_ins=1.617, word_ins_ml=4.622, word_reposition=1.446, kpe=nan, ppl=nan, wps=5207.1, ups=0.26, wpb=20272.3, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=1.522, clip=0, loss_scale=2048, train_wall=251, wall=0
2022-07-26 14:30:50 | INFO | train_inner | epoch 016:    190 / 1122 loss=8.548, nll_loss=3.068, mask_ins=1.624, word_ins_ml=4.641, word_reposition=1.453, kpe=0.83, ppl=374.32, wps=7023.4, ups=0.34, wpb=20493.9, bsz=256, num_updates=17000, lr=0.000271163, gnorm=1.532, clip=0, loss_scale=2048, train_wall=252, wall=0
2022-07-26 14:35:41 | INFO | train_inner | epoch 016:    290 / 1122 loss=8.499, nll_loss=3.022, mask_ins=1.612, word_ins_ml=4.601, word_reposition=1.453, kpe=0.833, ppl=361.8, wps=7087.8, ups=0.34, wpb=20656.5, bsz=256, num_updates=17100, lr=0.000270369, gnorm=1.531, clip=0, loss_scale=2048, train_wall=252, wall=0
2022-07-26 14:40:40 | INFO | train_inner | epoch 016:    390 / 1122 loss=8.521, nll_loss=3.052, mask_ins=1.615, word_ins_ml=4.627, word_reposition=1.445, kpe=0.834, ppl=367.34, wps=6849.4, ups=0.34, wpb=20436.9, bsz=256, num_updates=17200, lr=0.000269582, gnorm=1.522, clip=0, loss_scale=2683, train_wall=258, wall=0
2022-07-26 14:45:35 | INFO | train_inner | epoch 016:    490 / 1122 loss=8.517, nll_loss=3.037, mask_ins=1.608, word_ins_ml=4.613, word_reposition=1.459, kpe=0.836, ppl=366.24, wps=6991.9, ups=0.34, wpb=20636.2, bsz=256, num_updates=17300, lr=0.000268802, gnorm=1.507, clip=0, loss_scale=4096, train_wall=255, wall=0
2022-07-26 14:45:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-26 14:50:32 | INFO | train_inner | epoch 016:    591 / 1122 loss=8.528, nll_loss=3.048, mask_ins=1.613, word_ins_ml=4.623, word_reposition=1.454, kpe=0.839, ppl=369.25, wps=6918.5, ups=0.34, wpb=20538.7, bsz=256, num_updates=17400, lr=0.000268028, gnorm=1.537, clip=0, loss_scale=2048, train_wall=256, wall=0
2022-07-26 14:51:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-26 14:55:26 | INFO | train_inner | epoch 016:    692 / 1122 loss=8.485, nll_loss=3.034, mask_ins=1.602, word_ins_ml=4.61, word_reposition=1.433, kpe=0.839, ppl=358.28, wps=6988.7, ups=0.34, wpb=20587.1, bsz=256, num_updates=17500, lr=0.000267261, gnorm=1.51, clip=0, loss_scale=1227, train_wall=254, wall=0
2022-07-26 14:56:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
train.sh: line 43: token: command not found
