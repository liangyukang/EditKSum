nohup: ignoring input
2022-08-06 04:34:52 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:11416
2022-08-06 04:34:52 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:11416
2022-08-06 04:34:53 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:11416
2022-08-06 04:34:53 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:11416
2022-08-06 04:34:53 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-08-06 04:34:53 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-08-06 04:34:53 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-08-06 04:34:53 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-08-06 04:34:53 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-06 04:34:53 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-08-06 04:34:53 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-06 04:34:53 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-08-06 04:34:53 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-06 04:34:53 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-08-06 04:34:53 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-06 04:34:53 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-08-06 04:34:57 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=32, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=32, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:11416', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_kpe_cased_Ggw', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-Ggw', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510_Ggw', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-08-06 04:34:57 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-08-06 04:34:57 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-08-06 04:34:57 | INFO | fairseq.data.data_utils | loaded 189612 examples from: ../data-bin-bert-cased-Ggw/valid.source-target.source
start load cached examples valid ...
0it [00:00, ?it/s]2022-08-06 04:34:57 | INFO | fairseq.data.data_utils | loaded 189612 examples from: ../data-bin-bert-cased-Ggw/valid.source-target.target
2022-08-06 04:34:57 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-Ggw valid source-target 189612 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]2136it [00:00, 21351.93it/s]2077it [00:00, 20758.14it/s]2127it [00:00, 21258.46it/s]2122it [00:00, 21213.90it/s]4156it [00:00, 20775.64it/s]4272it [00:00, 20815.11it/s]4244it [00:00, 21208.07it/s]4253it [00:00, 20683.47it/s]6264it [00:00, 20913.92it/s]6437it [00:00, 21164.92it/s]6437it [00:00, 21105.15it/s]6365it [00:00, 20922.10it/s]8356it [00:00, 14935.04it/s]8458it [00:00, 15147.93it/s]8555it [00:00, 14960.96it/s]8549it [00:00, 14948.67it/s]10494it [00:00, 16783.52it/s]10633it [00:00, 17049.84it/s]10736it [00:00, 16898.09it/s]10715it [00:00, 16846.17it/s]12597it [00:00, 18017.93it/s]12665it [00:00, 18002.16it/s]12957it [00:00, 18426.86it/s]12932it [00:00, 18377.41it/s]14716it [00:00, 18946.71it/s]14819it [00:00, 19041.83it/s]15120it [00:00, 19359.97it/s]14999it [00:00, 19044.83it/s]16709it [00:01, 13743.27it/s]16815it [00:01, 13697.23it/s]17164it [00:01, 13559.43it/s]17007it [00:01, 13410.39it/s]18822it [00:01, 15453.95it/s]18959it [00:01, 15480.69it/s]19282it [00:01, 15268.68it/s]19158it [00:01, 15234.87it/s]20895it [00:01, 16764.68it/s]21052it [00:01, 16829.57it/s]21348it [00:01, 16575.63it/s]21282it [00:01, 16697.32it/s]23005it [00:01, 17900.04it/s]23201it [00:01, 18047.46it/s]23495it [00:01, 17835.60it/s]23420it [00:01, 17904.26it/s]25112it [00:01, 12269.27it/s]25165it [00:01, 12167.01it/s]25457it [00:01, 11740.77it/s]25387it [00:01, 11662.45it/s]27268it [00:01, 14153.53it/s]27356it [00:01, 14149.92it/s]27646it [00:01, 13736.38it/s]27597it [00:01, 13709.91it/s]29294it [00:01, 15524.41it/s]29377it [00:01, 15514.51it/s]29843it [00:01, 15546.98it/s]29748it [00:01, 15417.38it/s]31359it [00:01, 16769.52it/s]31518it [00:01, 16949.84it/s]31983it [00:01, 16944.54it/s]31823it [00:01, 16686.29it/s]33465it [00:02, 17872.97it/s]33626it [00:02, 18013.84it/s]34057it [00:02, 17909.07it/s]33955it [00:02, 17862.57it/s]35428it [00:02, 10966.31it/s]35609it [00:02, 11079.80it/s]36051it [00:02, 10437.63it/s]37589it [00:02, 12954.61it/s]35950it [00:02, 10158.71it/s]37796it [00:02, 13070.80it/s]38235it [00:02, 12459.06it/s]39714it [00:02, 14700.63it/s]38132it [00:02, 12180.21it/s]39992it [00:02, 14943.07it/s]40452it [00:02, 14420.00it/s]41829it [00:02, 16191.67it/s]40327it [00:02, 14119.41it/s]42106it [00:02, 16375.74it/s]42738it [00:02, 16310.36it/s]44006it [00:02, 17576.04it/s]42565it [00:02, 15942.39it/s]44340it [00:02, 17855.21it/s]44865it [00:02, 17511.64it/s]46182it [00:02, 18655.47it/s]44778it [00:02, 17424.47it/s]46554it [00:02, 18976.11it/s]47106it [00:02, 18769.94it/s]48339it [00:02, 19446.58it/s]47037it [00:02, 18692.43it/s]48717it [00:02, 19628.53it/s]50423it [00:03, 10221.50it/s]49144it [00:03, 9584.66it/s] 49207it [00:03, 9216.11it/s] 50819it [00:03, 9579.97it/s] 52500it [00:03, 12022.95it/s]51306it [00:03, 11478.99it/s]51355it [00:03, 11109.20it/s]52971it [00:03, 11493.41it/s]54609it [00:03, 13800.61it/s]53477it [00:03, 13367.50it/s]53467it [00:03, 12917.45it/s]55074it [00:03, 13269.97it/s]56724it [00:03, 15408.49it/s]55544it [00:03, 14891.76it/s]55598it [00:03, 14638.56it/s]57260it [00:03, 15074.48it/s]58835it [00:03, 16765.66it/s]57814it [00:03, 16684.46it/s]57881it [00:03, 16495.15it/s]59496it [00:03, 16755.07it/s]60972it [00:03, 17934.00it/s]60033it [00:03, 18043.46it/s]60109it [00:03, 17909.22it/s]61706it [00:03, 18078.88it/s]63146it [00:04, 18945.64it/s]62227it [00:04, 19017.57it/s]62282it [00:04, 18898.40it/s]63925it [00:04, 19154.13it/s]65353it [00:04, 19804.32it/s]64495it [00:04, 20008.24it/s]64559it [00:04, 19943.40it/s]66056it [00:04, 9167.00it/s] 67467it [00:04, 9056.56it/s] 68256it [00:04, 11128.46it/s]66658it [00:04, 8768.39it/s] 66728it [00:04, 8708.41it/s] 69606it [00:04, 10946.15it/s]70414it [00:04, 13000.93it/s]68884it [00:04, 10736.83it/s]68958it [00:04, 10677.83it/s]71694it [00:04, 12729.48it/s]72537it [00:04, 14677.55it/s]71031it [00:04, 12587.34it/s]71057it [00:04, 12446.44it/s]73816it [00:04, 14463.07it/s]74718it [00:04, 16284.82it/s]72958it [00:05, 13906.70it/s]73218it [00:05, 14224.15it/s]75785it [00:05, 15643.16it/s]76839it [00:05, 17484.64it/s]75112it [00:05, 15586.33it/s]75417it [00:05, 15924.68it/s]77918it [00:05, 17027.92it/s]79002it [00:05, 18553.55it/s]77290it [00:05, 17067.15it/s]77577it [00:05, 17277.38it/s]80011it [00:05, 18035.53it/s]81121it [00:05, 19263.10it/s]79452it [00:05, 18226.06it/s]79723it [00:05, 18339.10it/s]82123it [00:05, 18864.68it/s]83265it [00:05, 19867.08it/s]81617it [00:05, 19080.25it/s]81850it [00:05, 19118.15it/s]84177it [00:05, 19146.73it/s]85395it [00:05, 20272.13it/s]83780it [00:05, 19780.72it/s]84016it [00:05, 19817.80it/s]86302it [00:05, 19739.81it/s]85939it [00:05, 20290.16it/s]86140it [00:05, 19800.29it/s]87512it [00:06, 7969.97it/s] 89641it [00:06, 9807.84it/s]88363it [00:06, 6968.69it/s] 91781it [00:06, 11716.55it/s]88069it [00:06, 7454.73it/s] 88220it [00:06, 7319.92it/s] 90458it [00:06, 8719.07it/s]93948it [00:06, 13611.99it/s]90225it [00:06, 9280.37it/s]90368it [00:06, 9139.53it/s]92592it [00:06, 10615.81it/s]96094it [00:06, 15289.34it/s]92430it [00:06, 11272.89it/s]92577it [00:06, 11148.06it/s]94723it [00:06, 12516.89it/s]98239it [00:06, 16730.73it/s]94603it [00:06, 13178.79it/s]94447it [00:06, 12386.39it/s]96828it [00:06, 14245.37it/s]100369it [00:06, 17874.40it/s]96799it [00:06, 14992.30it/s]96657it [00:06, 14368.10it/s]98956it [00:06, 15823.63it/s]102563it [00:06, 18945.47it/s]98952it [00:06, 16480.41it/s]98807it [00:06, 15977.62it/s]101047it [00:06, 17058.35it/s]104717it [00:06, 19649.46it/s]101097it [00:06, 17699.42it/s]101002it [00:06, 17432.83it/s]103211it [00:07, 18237.43it/s]106887it [00:07, 20189.42it/s]103317it [00:07, 18866.83it/s]103186it [00:07, 18570.94it/s]105332it [00:07, 19031.14it/s]109020it [00:07, 20192.52it/s]105486it [00:07, 19628.92it/s]105363it [00:07, 19431.71it/s]107489it [00:07, 19734.09it/s]107704it [00:07, 20338.80it/s]111120it [00:07, 19921.49it/s]107587it [00:07, 20204.27it/s]109599it [00:07, 19575.77it/s]109870it [00:07, 20059.17it/s]109740it [00:07, 20016.01it/s]111652it [00:07, 19477.48it/s]111969it [00:07, 19740.56it/s]111835it [00:07, 19951.61it/s]113169it [00:08, 6616.56it/s] 115278it [00:08, 8330.98it/s]113667it [00:08, 6460.75it/s] 117407it [00:08, 10206.54it/s]115740it [00:08, 8138.52it/s]119538it [00:08, 12108.87it/s]114009it [00:08, 6058.32it/s] 117804it [00:08, 9936.00it/s]121691it [00:08, 13960.58it/s]116133it [00:08, 7712.96it/s]113896it [00:08, 5488.23it/s] 119923it [00:08, 11853.56it/s]123866it [00:08, 15667.24it/s]118216it [00:08, 9480.38it/s]116037it [00:08, 7076.08it/s]122062it [00:08, 13696.22it/s]126098it [00:08, 17181.45it/s]120398it [00:08, 11467.92it/s]118073it [00:08, 8732.45it/s]124241it [00:08, 15462.35it/s]128299it [00:08, 18403.84it/s]122626it [00:08, 13492.80it/s]120259it [00:08, 10717.21it/s]126431it [00:08, 16992.14it/s]130479it [00:08, 19305.80it/s]124745it [00:08, 15116.12it/s]122424it [00:08, 12658.50it/s]128623it [00:08, 18240.32it/s]132678it [00:08, 20042.26it/s]127006it [00:08, 16851.05it/s]124624it [00:09, 14549.12it/s]130759it [00:09, 19069.39it/s]134835it [00:09, 20449.75it/s]129248it [00:09, 18235.14it/s]126768it [00:09, 16092.48it/s]132940it [00:09, 19822.43it/s]137060it [00:09, 20965.86it/s]131435it [00:09, 19186.93it/s]129024it [00:09, 17659.43it/s]135101it [00:09, 20324.59it/s]139276it [00:09, 21312.15it/s]133595it [00:09, 19829.43it/s]131243it [00:09, 18823.48it/s]137287it [00:09, 20764.01it/s]141464it [00:09, 21417.86it/s]135820it [00:09, 20505.07it/s]133468it [00:09, 19742.95it/s]139506it [00:09, 21178.14it/s]143646it [00:09, 21311.40it/s]138062it [00:09, 21051.23it/s]135687it [00:09, 20421.03it/s]141690it [00:09, 21370.79it/s]140341it [00:09, 21552.91it/s]137877it [00:09, 20682.63it/s]143868it [00:09, 21154.45it/s]142563it [00:09, 21640.55it/s]140131it [00:09, 21214.69it/s]144774it [00:09, 21265.10it/s]142335it [00:09, 21451.73it/s]144534it [00:09, 21501.48it/s]145805it [00:10, 5826.88it/s] 148013it [00:10, 7495.83it/s]146012it [00:10, 5816.65it/s] 150132it [00:10, 9247.75it/s]148139it [00:10, 7408.31it/s]152316it [00:10, 11192.78it/s]150221it [00:10, 9128.30it/s]154331it [00:10, 12811.93it/s]152371it [00:10, 11025.74it/s]156464it [00:10, 14560.02it/s]146935it [00:11, 5084.73it/s] 154482it [00:11, 12849.43it/s]158572it [00:11, 16038.05it/s]149104it [00:11, 6578.47it/s]156497it [00:11, 14347.79it/s]160685it [00:11, 17282.52it/s]151304it [00:11, 8334.39it/s]158565it [00:11, 15782.60it/s]162798it [00:11, 18277.85it/s]153415it [00:11, 10121.19it/s]160642it [00:11, 17001.11it/s]164880it [00:11, 18885.89it/s]146722it [00:11, 4319.11it/s] 155575it [00:11, 12032.16it/s]162675it [00:11, 17800.99it/s]167032it [00:11, 19615.58it/s]148931it [00:11, 5696.50it/s]157720it [00:11, 13842.05it/s]164811it [00:11, 18756.49it/s]169168it [00:11, 20109.15it/s]151091it [00:11, 7281.76it/s]159875it [00:11, 15501.37it/s]166908it [00:11, 19369.78it/s]171276it [00:11, 20326.02it/s]153250it [00:11, 9065.18it/s]161959it [00:11, 16688.45it/s]168978it [00:11, 19638.06it/s]173378it [00:11, 20287.16it/s]155346it [00:11, 10862.75it/s]164147it [00:11, 17934.79it/s]171038it [00:11, 19912.85it/s]175468it [00:11, 20464.98it/s]157479it [00:11, 12721.53it/s]166352it [00:11, 19016.99it/s]173123it [00:11, 20185.46it/s]177595it [00:11, 20698.85it/s]159628it [00:12, 14495.45it/s]168484it [00:12, 19645.77it/s]175190it [00:12, 20119.74it/s]179851it [00:12, 21247.76it/s]161723it [00:12, 15942.60it/s]170616it [00:12, 19885.59it/s]177296it [00:12, 20394.73it/s]182028it [00:12, 21400.97it/s]163804it [00:12, 17083.09it/s]172727it [00:12, 20231.86it/s]179473it [00:12, 20800.68it/s]184181it [00:12, 21248.56it/s]165974it [00:12, 18267.69it/s]174864it [00:12, 20557.21it/s]181613it [00:12, 20976.65it/s]168155it [00:12, 19217.25it/s]176980it [00:12, 20731.17it/s]183724it [00:12, 21016.09it/s]170282it [00:12, 19747.84it/s]179250it [00:12, 21309.58it/s]185849it [00:12, 21085.14it/s]172438it [00:12, 20259.98it/s]181412it [00:12, 20858.75it/s]174571it [00:12, 20004.83it/s]183579it [00:12, 21094.70it/s]176711it [00:12, 20402.20it/s]185756it [00:12, 21291.46it/s]178944it [00:12, 20960.56it/s]181182it [00:13, 21372.37it/s]183396it [00:13, 21598.46it/s]185577it [00:13, 20848.77it/s]186315it [00:13, 4369.87it/s] 188449it [00:13, 5728.36it/s]187964it [00:13, 4746.23it/s] 189612it [00:13, 13685.20it/s]
189612it [00:13, 13643.20it/s]
2022-08-06 04:35:11 | INFO | root | success load 189612 data
2022-08-06 04:35:11 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-08-06 04:35:11 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-08-06 04:35:11 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-08-06 04:35:11 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-08-06 04:35:11 | INFO | transformer.tokenization_utils | loading file None
2022-08-06 04:35:11 | INFO | transformer.tokenization_utils | loading file None
2022-08-06 04:35:11 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
2022-08-06 04:35:11 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-06 04:35:11 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-06 04:35:11 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
187898it [00:14, 4478.11it/s] 189612it [00:14, 13285.23it/s]
187682it [00:14, 3845.08it/s] 189612it [00:14, 12680.77it/s]
2022-08-06 04:35:15 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-08-06 04:35:15 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-08-06 04:35:15 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-06 04:35:15 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-06 04:35:15 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
2022-08-06 04:35:17 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-08-06 04:35:17 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 668
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 404
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']Trained parameters: len 668
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 404
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']
2022-08-06 04:35:17 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-08-06 04:35:17 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-08-06 04:35:17 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 380755715)
2022-08-06 04:35:17 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 146472707)
2022-08-06 04:35:17 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 234283008)
Trained parameters: len 668
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 404
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']2022-08-06 04:35:18 | INFO | fairseq_cli.train | training on 4 GPUs
2022-08-06 04:35:18 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 32
2022-08-06 04:35:18 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt
2022-08-06 04:35:18 | INFO | fairseq.trainer | loading train data for epoch 1
2022-08-06 04:35:18 | INFO | fairseq.data.data_utils | loaded 3803212 examples from: ../data-bin-bert-cased-Ggw/train.source-target.source
2022-08-06 04:35:18 | INFO | fairseq.data.data_utils | loaded 3803212 examples from: ../data-bin-bert-cased-Ggw/train.source-target.target
2022-08-06 04:35:18 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-Ggw train source-target 3803212 examples
start load cached examples train ...
0it [00:00, ?it/s]
start load cached examples train ...
0it [00:00, ?it/s]2130it [00:00, 21291.14it/s]2157it [00:00, 21563.44it/s]4260it [00:00, 21166.72it/s]
start load cached examples train ...
0it [00:00, ?it/s]4314it [00:00, 21459.43it/s]6379it [00:00, 21104.17it/s]2140it [00:00, 21377.75it/s]6460it [00:00, 21359.85it/s]8500it [00:00, 21145.25it/s]Trained parameters: len 668
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 404
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']4278it [00:00, 21349.69it/s]8597it [00:00, 20845.69it/s]10615it [00:00, 20977.01it/s]6413it [00:00, 21123.55it/s]10708it [00:00, 20937.07it/s]12720it [00:00, 21000.67it/s]8553it [00:00, 21228.00it/s]14821it [00:00, 20824.67it/s]12803it [00:00, 20670.82it/s]10677it [00:00, 21190.70it/s]16904it [00:00, 20803.56it/s]14872it [00:00, 20648.83it/s]12797it [00:00, 21180.96it/s]18985it [00:00, 20798.45it/s]16938it [00:00, 20556.55it/s]14916it [00:00, 21085.86it/s]
start load cached examples train ...
0it [00:00, ?it/s]21119it [00:01, 20964.29it/s]19053it [00:00, 20739.53it/s]17039it [00:00, 21129.37it/s]2180it [00:00, 21620.69it/s]23231it [00:01, 21010.28it/s]21205it [00:01, 20976.16it/s]19153it [00:00, 20549.50it/s]4405it [00:00, 21985.08it/s]25333it [00:01, 20968.06it/s]23304it [00:01, 20864.23it/s]21220it [00:01, 20583.27it/s]6604it [00:00, 21739.13it/s]27430it [00:01, 20864.34it/s]25478it [00:01, 21128.17it/s]23281it [00:01, 20381.63it/s]8782it [00:00, 21754.19it/s]29519it [00:01, 20871.21it/s]27592it [00:01, 20929.46it/s]25399it [00:01, 20619.14it/s]10958it [00:00, 21665.20it/s]31607it [00:01, 20769.00it/s]29766it [00:01, 21167.77it/s]27501it [00:01, 20736.05it/s]13125it [00:00, 21564.35it/s]33685it [00:01, 20763.04it/s]31884it [00:01, 21125.08it/s]29641it [00:01, 20934.43it/s]15302it [00:00, 21629.42it/s]35764it [00:01, 20768.36it/s]34046it [00:01, 21272.68it/s]31750it [00:01, 20978.87it/s]17466it [00:00, 21557.80it/s]37841it [00:01, 20655.89it/s]36174it [00:01, 21267.20it/s]33887it [00:01, 21094.97it/s]19660it [00:00, 21675.38it/s]39907it [00:01, 20649.27it/s]38302it [00:01, 21015.16it/s]35998it [00:01, 21048.95it/s]21841it [00:01, 21714.24it/s]41973it [00:02, 20498.25it/s]40405it [00:01, 20845.06it/s]38130it [00:01, 21129.15it/s]24020it [00:01, 21658.08it/s]44102it [00:02, 20733.64it/s]42552it [00:02, 21028.44it/s]40244it [00:01, 20999.07it/s]26186it [00:01, 21584.40it/s]44656it [00:02, 20884.38it/s]42428it [00:02, 21027.93it/s]28360it [00:01, 21622.89it/s]44566it [00:02, 21131.03it/s]30555it [00:01, 21719.12it/s]32728it [00:01, 21712.70it/s]34900it [00:01, 21662.84it/s]37067it [00:01, 21636.17it/s]39231it [00:01, 21538.57it/s]41444it [00:01, 21714.77it/s]43620it [00:02, 21724.91it/s]45793it [00:02, 21578.77it/s]46176it [00:04, 2737.27it/s] 48303it [00:04, 3724.34it/s]46746it [00:04, 2703.26it/s] 50270it [00:04, 4851.80it/s]48869it [00:04, 3666.41it/s]52361it [00:04, 6324.92it/s]50937it [00:04, 4842.53it/s]54399it [00:04, 7952.56it/s]52954it [00:04, 6220.09it/s]56397it [00:04, 9660.21it/s]55027it [00:04, 7870.76it/s]58490it [00:05, 11559.62it/s]57069it [00:04, 9624.87it/s]60475it [00:05, 13122.41it/s]59190it [00:05, 11553.19it/s]62579it [00:05, 14835.80it/s]61234it [00:05, 13259.49it/s]64592it [00:05, 15979.53it/s]63263it [00:05, 14773.36it/s]66668it [00:05, 17178.08it/s]65285it [00:05, 16052.67it/s]68764it [00:05, 18176.12it/s]67451it [00:05, 17461.43it/s]70807it [00:05, 18702.61it/s]69516it [00:05, 18268.94it/s]72903it [00:05, 19333.76it/s]71595it [00:05, 18957.68it/s]74955it [00:05, 19601.40it/s]47952it [00:04, 2428.82it/s] 73661it [00:05, 19322.79it/s]77040it [00:05, 19961.53it/s]50131it [00:04, 3313.16it/s]75810it [00:05, 19940.34it/s]46680it [00:05, 1824.61it/s] 79096it [00:06, 20062.55it/s]52308it [00:05, 4444.62it/s]77941it [00:06, 20336.58it/s]48874it [00:05, 2536.84it/s]81209it [00:06, 20352.98it/s]54339it [00:05, 5722.00it/s]80038it [00:06, 20495.22it/s]50954it [00:05, 3416.16it/s]83349it [00:06, 20660.16it/s]56517it [00:05, 7378.56it/s]82132it [00:06, 20578.81it/s]53077it [00:06, 4562.42it/s]85437it [00:06, 20544.59it/s]58632it [00:05, 9147.38it/s]84222it [00:06, 20598.68it/s]55127it [00:06, 5904.18it/s]87594it [00:06, 20847.72it/s]60805it [00:05, 11097.64it/s]86391it [00:06, 20913.80it/s]57245it [00:06, 7543.82it/s]89690it [00:06, 20572.90it/s]62943it [00:05, 12961.66it/s]88565it [00:06, 21156.71it/s]59290it [00:06, 9259.34it/s]91835it [00:06, 20829.26it/s]65102it [00:05, 14735.81it/s]61413it [00:06, 11169.78it/s]90692it [00:06, 21052.08it/s]93925it [00:06, 20700.84it/s]67214it [00:05, 16082.04it/s]63571it [00:06, 13071.49it/s]92806it [00:06, 21006.16it/s]96080it [00:06, 20951.67it/s]69402it [00:05, 17494.87it/s]65643it [00:06, 14608.68it/s]94913it [00:06, 20915.19it/s]98206it [00:06, 21042.21it/s]71554it [00:05, 18533.46it/s]67802it [00:06, 16209.39it/s]97098it [00:06, 21190.85it/s]100344it [00:07, 21141.20it/s]73711it [00:06, 19350.39it/s]69907it [00:06, 17402.69it/s]99241it [00:07, 21261.56it/s]102463it [00:07, 21154.99it/s]75867it [00:06, 19963.38it/s]72001it [00:06, 18257.49it/s]101370it [00:07, 21111.25it/s]104644it [00:07, 21348.44it/s]78011it [00:06, 20171.38it/s]74176it [00:07, 19202.18it/s]103541it [00:07, 21284.81it/s]106780it [00:07, 21304.39it/s]80209it [00:06, 20688.97it/s]76288it [00:07, 19585.60it/s]105671it [00:07, 21059.58it/s]108929it [00:07, 21296.75it/s]82360it [00:06, 20926.74it/s]78418it [00:07, 20071.01it/s]107855it [00:07, 21290.20it/s]84571it [00:06, 21209.02it/s]80524it [00:07, 20193.97it/s]86813it [00:06, 21564.98it/s]82716it [00:07, 20693.84it/s]88997it [00:06, 21568.72it/s]84912it [00:07, 21064.38it/s]91173it [00:06, 21545.61it/s]87055it [00:07, 20526.47it/s]93363it [00:06, 21650.21it/s]89208it [00:07, 20815.55it/s]95569it [00:07, 21770.08it/s]91388it [00:07, 21102.30it/s]97795it [00:07, 21915.38it/s]93607it [00:08, 21421.90it/s]99992it [00:07, 21875.24it/s]95773it [00:08, 21491.12it/s]102183it [00:07, 21734.58it/s]97930it [00:08, 21467.52it/s]104387it [00:07, 21824.45it/s]100083it [00:08, 21366.73it/s]106576it [00:07, 21842.43it/s]102224it [00:08, 21177.22it/s]108813it [00:07, 21999.15it/s]104470it [00:08, 21556.44it/s]106629it [00:08, 21451.13it/s]108780it [00:08, 21465.92it/s]111060it [00:10, 2411.65it/s] 113143it [00:10, 3260.46it/s]109986it [00:10, 2335.27it/s] 115188it [00:10, 4322.14it/s]112027it [00:10, 3140.03it/s]117374it [00:10, 5744.05it/s]114213it [00:10, 4259.46it/s]119481it [00:10, 7334.90it/s]116415it [00:10, 5657.21it/s]121523it [00:10, 9024.33it/s]118538it [00:10, 7230.84it/s]123519it [00:10, 10727.42it/s]120664it [00:10, 9000.39it/s]125532it [00:10, 12438.42it/s]122697it [00:10, 10698.21it/s]127646it [00:10, 14232.76it/s]124823it [00:10, 12584.42it/s]129801it [00:11, 15897.94it/s]126876it [00:11, 14156.44it/s]131871it [00:11, 16976.79it/s]129053it [00:11, 15865.32it/s]134017it [00:11, 18135.29it/s]131194it [00:11, 17210.00it/s]136100it [00:11, 18787.82it/s]133295it [00:11, 18083.56it/s]138281it [00:11, 19624.83it/s]135466it [00:11, 19052.97it/s]140432it [00:11, 20157.58it/s]137634it [00:11, 19779.15it/s]142552it [00:11, 20279.82it/s]139763it [00:11, 20188.88it/s]144681it [00:11, 20571.02it/s]141890it [00:11, 20275.93it/s]146790it [00:11, 20529.59it/s]111014it [00:10, 2127.61it/s] 144061it [00:11, 20688.61it/s]148880it [00:11, 20636.50it/s]113199it [00:10, 2909.21it/s]146197it [00:11, 20882.60it/s]150998it [00:12, 20730.93it/s]115454it [00:11, 3962.53it/s]148324it [00:12, 20890.42it/s]153090it [00:12, 20534.78it/s]117674it [00:11, 5260.39it/s]150441it [00:12, 20942.73it/s]155198it [00:12, 20623.48it/s]119865it [00:11, 6796.66it/s]152555it [00:12, 20880.91it/s]157270it [00:12, 20234.08it/s]121971it [00:11, 8457.86it/s]154657it [00:12, 20749.75it/s]159398it [00:12, 20514.14it/s]124040it [00:11, 10144.87it/s]156823it [00:12, 21018.10it/s]161555it [00:12, 20824.30it/s]126085it [00:11, 11875.39it/s]158932it [00:12, 20972.83it/s]163643it [00:12, 20657.08it/s]128291it [00:11, 13850.23it/s]110928it [00:12, 1786.08it/s] 161035it [00:12, 20986.95it/s]165810it [00:12, 20954.30it/s]130469it [00:11, 15569.43it/s]113180it [00:12, 2494.85it/s]163184it [00:12, 21134.35it/s]167909it [00:12, 20740.50it/s]132651it [00:11, 17047.89it/s]115365it [00:12, 3393.74it/s]165300it [00:12, 20972.40it/s]170055it [00:12, 20949.94it/s]134882it [00:12, 18375.88it/s]117570it [00:12, 4556.50it/s]167400it [00:12, 20961.07it/s]172152it [00:13, 20901.61it/s]137047it [00:12, 19104.16it/s]119756it [00:12, 5971.07it/s]169548it [00:13, 21112.93it/s]174244it [00:13, 20720.93it/s]139267it [00:12, 19948.78it/s]121819it [00:13, 7503.78it/s]171661it [00:13, 20908.94it/s]176326it [00:13, 20749.88it/s]141496it [00:12, 20604.21it/s]123987it [00:13, 9346.12it/s]173753it [00:13, 20713.41it/s]178402it [00:13, 20593.47it/s]143683it [00:12, 20949.09it/s]126080it [00:13, 11158.36it/s]175888it [00:13, 20901.15it/s]180495it [00:13, 20691.88it/s]145903it [00:12, 21311.01it/s]128285it [00:13, 13148.41it/s]177980it [00:13, 20655.48it/s]182636it [00:13, 20904.40it/s]148099it [00:12, 21399.52it/s]130466it [00:13, 14940.49it/s]180133it [00:13, 20911.67it/s]184728it [00:13, 20703.26it/s]150285it [00:12, 21192.78it/s]132604it [00:13, 16260.57it/s]182226it [00:13, 20678.88it/s]186900it [00:13, 21002.46it/s]152459it [00:12, 21351.48it/s]134831it [00:13, 17727.01it/s]184421it [00:13, 21052.10it/s]154617it [00:12, 21351.36it/s]137035it [00:13, 18840.66it/s]186579it [00:13, 21204.44it/s]156821it [00:13, 21554.42it/s]139240it [00:13, 19696.32it/s]158997it [00:13, 21614.74it/s]141415it [00:13, 19655.48it/s]161169it [00:13, 21643.29it/s]143590it [00:14, 20235.48it/s]163340it [00:13, 21492.93it/s]145786it [00:14, 20723.38it/s]165514it [00:13, 21564.90it/s]147935it [00:14, 20907.76it/s]167748it [00:13, 21794.82it/s]150121it [00:14, 21185.02it/s]169930it [00:13, 21778.74it/s]152279it [00:14, 21213.66it/s]172110it [00:13, 21648.41it/s]154428it [00:14, 21071.30it/s]174277it [00:13, 21436.85it/s]156629it [00:14, 21345.14it/s]176422it [00:13, 21415.12it/s]158778it [00:14, 21239.18it/s]178574it [00:14, 21443.67it/s]160980it [00:14, 21469.46it/s]180719it [00:14, 21286.93it/s]163153it [00:14, 21546.21it/s]182920it [00:14, 21398.18it/s]165313it [00:15, 21391.62it/s]185132it [00:14, 21611.94it/s]167540it [00:15, 21651.57it/s]187294it [00:14, 21383.28it/s]169709it [00:15, 21323.82it/s]171891it [00:15, 21469.00it/s]174049it [00:15, 21498.08it/s]176201it [00:15, 21503.36it/s]178353it [00:15, 21438.20it/s]180501it [00:15, 21449.28it/s]182647it [00:15, 21266.26it/s]184834it [00:15, 21445.26it/s]187066it [00:16, 21705.16it/s]189002it [00:17, 1982.17it/s] 191069it [00:17, 2703.02it/s]193121it [00:17, 3633.48it/s]188701it [00:17, 1973.08it/s] 195248it [00:17, 4857.83it/s]190821it [00:17, 2704.63it/s]197295it [00:17, 6266.34it/s]192930it [00:17, 3652.21it/s]199375it [00:17, 7925.44it/s]195040it [00:17, 4848.96it/s]201478it [00:17, 9762.66it/s]197046it [00:17, 6206.28it/s]203542it [00:17, 11577.66it/s]199176it [00:17, 7912.91it/s]205620it [00:17, 13348.27it/s]201351it [00:17, 9836.93it/s]207712it [00:18, 14980.37it/s]203411it [00:17, 11620.03it/s]209789it [00:18, 16342.57it/s]205577it [00:18, 13541.48it/s]211858it [00:18, 17436.88it/s]207663it [00:18, 15014.01it/s]214019it [00:18, 18538.82it/s]209827it [00:18, 16560.94it/s]216113it [00:18, 18995.09it/s]211979it [00:18, 17801.34it/s]218184it [00:18, 19371.82it/s]214092it [00:18, 18535.57it/s]220269it [00:18, 19789.76it/s]216268it [00:18, 19411.67it/s]222336it [00:18, 19935.70it/s]218388it [00:18, 19882.24it/s]224463it [00:18, 20322.92it/s]220505it [00:18, 20052.10it/s]226554it [00:18, 20425.21it/s]222625it [00:18, 20381.35it/s]228661it [00:19, 20613.05it/s]224809it [00:18, 20802.95it/s]230822it [00:19, 20906.75it/s]226936it [00:19, 20842.67it/s]232929it [00:19, 20604.48it/s]229053it [00:19, 20933.00it/s]235047it [00:19, 20772.01it/s]231170it [00:19, 20839.08it/s]189434it [00:18, 1701.29it/s] 237150it [00:19, 20846.11it/s]233286it [00:19, 20931.37it/s]191597it [00:18, 2350.28it/s]239241it [00:19, 20746.77it/s]235430it [00:19, 21081.48it/s]193798it [00:18, 3224.18it/s]241378it [00:19, 20685.53it/s]237547it [00:19, 21094.03it/s]195972it [00:18, 4329.75it/s]243532it [00:19, 20936.52it/s]239676it [00:19, 21151.52it/s]198110it [00:18, 5663.89it/s]245708it [00:19, 21176.95it/s]241796it [00:19, 21132.15it/s]200326it [00:18, 7327.34it/s]247828it [00:19, 20985.37it/s]244012it [00:19, 21436.72it/s]202490it [00:19, 9129.00it/s]249952it [00:20, 21059.06it/s]246158it [00:19, 21318.55it/s]204681it [00:19, 11077.18it/s]252060it [00:20, 20980.04it/s]248361it [00:20, 21530.06it/s]206873it [00:19, 13015.18it/s]254159it [00:20, 20815.13it/s]250516it [00:20, 21238.00it/s]209044it [00:19, 14782.78it/s]256267it [00:20, 20892.27it/s]189238it [00:20, 1674.16it/s] 252642it [00:20, 21003.16it/s]211208it [00:19, 16326.86it/s]258392it [00:20, 20996.96it/s]191390it [00:20, 2306.22it/s]254744it [00:20, 20902.06it/s]213439it [00:19, 17782.06it/s]260493it [00:20, 20766.79it/s]193588it [00:20, 3162.41it/s]256836it [00:20, 20899.12it/s]215666it [00:19, 18939.20it/s]262633it [00:20, 20952.53it/s]195694it [00:20, 4205.23it/s]258953it [00:20, 20978.54it/s]217859it [00:19, 19698.92it/s]264756it [00:20, 21034.82it/s]197905it [00:20, 5585.28it/s]261110it [00:20, 21131.13it/s]220046it [00:19, 20227.47it/s]266861it [00:20, 20834.01it/s]200092it [00:20, 7200.32it/s]263224it [00:20, 20987.01it/s]222225it [00:19, 20581.76it/s]268996it [00:20, 20985.60it/s]202305it [00:20, 9053.13it/s]265352it [00:20, 21071.93it/s]224394it [00:20, 20828.77it/s]271096it [00:21, 20845.97it/s]204413it [00:20, 10755.72it/s]267460it [00:20, 20943.48it/s]226558it [00:20, 21062.83it/s]273212it [00:21, 20936.46it/s]206510it [00:20, 12537.22it/s]269595it [00:21, 21062.73it/s]228720it [00:20, 21028.13it/s]275373it [00:21, 21136.47it/s]208594it [00:21, 14200.77it/s]271759it [00:21, 21232.05it/s]230863it [00:20, 21143.41it/s]277488it [00:21, 20854.14it/s]210710it [00:21, 15709.04it/s]273883it [00:21, 20995.70it/s]233005it [00:20, 21078.63it/s]279608it [00:21, 20955.69it/s]212792it [00:21, 16888.58it/s]276068it [00:21, 21248.80it/s]235133it [00:20, 20164.59it/s]281772it [00:21, 21156.32it/s]214910it [00:21, 17932.02it/s]278209it [00:21, 21296.49it/s]237255it [00:20, 20466.19it/s]283889it [00:21, 20882.51it/s]217032it [00:21, 18807.79it/s]280340it [00:21, 21189.08it/s]239332it [00:20, 20552.09it/s]286046it [00:21, 21085.41it/s]282494it [00:21, 21291.34it/s]219125it [00:21, 19259.92it/s]241476it [00:20, 20810.34it/s]284633it [00:21, 21318.34it/s]221203it [00:21, 19515.42it/s]243596it [00:20, 20923.63it/s]286766it [00:21, 21151.35it/s]223310it [00:21, 19870.41it/s]245748it [00:21, 21099.02it/s]225435it [00:21, 20267.76it/s]247880it [00:21, 21162.86it/s]227517it [00:21, 20219.15it/s]250000it [00:21, 21091.29it/s]229641it [00:22, 20516.82it/s]252112it [00:21, 20880.87it/s]231756it [00:22, 20700.98it/s]254203it [00:21, 20665.89it/s]233846it [00:22, 20743.08it/s]256280it [00:21, 20694.87it/s]235935it [00:22, 20744.54it/s]258390it [00:21, 20814.43it/s]238032it [00:22, 20808.39it/s]260491it [00:21, 20871.59it/s]240120it [00:22, 20821.71it/s]262597it [00:21, 20926.42it/s]242280it [00:22, 21046.74it/s]264691it [00:21, 20792.94it/s]244389it [00:22, 20885.08it/s]266813it [00:22, 20917.52it/s]246550it [00:22, 21094.66it/s]268920it [00:22, 20961.12it/s]248662it [00:22, 21081.06it/s]271064it [00:22, 21103.35it/s]250772it [00:23, 21043.72it/s]273175it [00:22, 21043.23it/s]252878it [00:23, 20769.72it/s]275328it [00:22, 21187.37it/s]255010it [00:23, 20931.16it/s]277447it [00:22, 21033.98it/s]257105it [00:23, 20899.73it/s]279590it [00:22, 21064.33it/s]259234it [00:23, 21014.55it/s]281743it [00:22, 21201.77it/s]261337it [00:23, 20973.16it/s]283864it [00:22, 21190.95it/s]263435it [00:23, 20855.08it/s]286002it [00:22, 21245.16it/s]265529it [00:23, 20879.49it/s]267673it [00:23, 21046.21it/s]269790it [00:23, 21081.16it/s]271930it [00:24, 21174.42it/s]274048it [00:24, 20781.25it/s]276216it [00:24, 21046.10it/s]278327it [00:24, 21063.66it/s]280453it [00:24, 21119.50it/s]282616it [00:24, 21271.04it/s]284744it [00:24, 21201.45it/s]286865it [00:24, 20335.48it/s]288156it [00:25, 1654.01it/s] 290290it [00:25, 2288.14it/s]292361it [00:26, 3098.52it/s]288882it [00:25, 1643.69it/s] 294518it [00:26, 4190.64it/s]291001it [00:26, 2268.55it/s]296600it [00:26, 5485.73it/s]293145it [00:26, 3105.84it/s]298737it [00:26, 7075.90it/s]295257it [00:26, 4163.69it/s]300750it [00:26, 8706.39it/s]297404it [00:26, 5503.10it/s]302833it [00:26, 10543.87it/s]299544it [00:26, 7085.65it/s]304866it [00:26, 12271.95it/s]301646it [00:26, 8820.62it/s]306993it [00:26, 14093.46it/s]303784it [00:26, 10716.87it/s]309058it [00:26, 15518.13it/s]305873it [00:26, 12521.68it/s]311142it [00:26, 16806.81it/s]308046it [00:26, 14382.88it/s]313258it [00:27, 17909.31it/s]310191it [00:26, 15965.42it/s]315347it [00:27, 18707.32it/s]312358it [00:27, 17347.78it/s]317458it [00:27, 19363.18it/s]314493it [00:27, 18375.80it/s]319546it [00:27, 19690.07it/s]316627it [00:27, 19159.03it/s]321658it [00:27, 20056.87it/s]318807it [00:27, 19889.41it/s]323741it [00:27, 20103.17it/s]320954it [00:27, 20329.03it/s]325837it [00:27, 20350.59it/s]323103it [00:27, 20661.84it/s]327944it [00:27, 20559.66it/s]325250it [00:27, 20765.18it/s]330028it [00:27, 20595.59it/s]327383it [00:27, 20814.80it/s]332149it [00:27, 20775.33it/s]329537it [00:27, 21027.19it/s]334258it [00:28, 20814.52it/s]331668it [00:27, 20817.39it/s]336349it [00:28, 20663.25it/s]333802it [00:28, 20968.65it/s]338458it [00:28, 20715.91it/s]335940it [00:28, 20994.90it/s]340596it [00:28, 20912.12it/s]338050it [00:28, 20957.30it/s]342691it [00:28, 20696.69it/s]340181it [00:28, 21060.01it/s]344850it [00:28, 20959.43it/s]342293it [00:28, 20906.26it/s]346949it [00:28, 20726.86it/s]344422it [00:28, 21019.11it/s]349091it [00:28, 20930.82it/s]346590it [00:28, 21213.98it/s]288127it [00:27, 1391.37it/s] 351196it [00:28, 20965.12it/s]348723it [00:28, 21246.83it/s]290199it [00:27, 1917.42it/s]353294it [00:28, 20841.70it/s]350850it [00:28, 21031.77it/s]292315it [00:28, 2637.56it/s]355412it [00:29, 20941.06it/s]352988it [00:29, 21132.43it/s]294454it [00:28, 3589.23it/s]357568it [00:29, 21119.97it/s]355165it [00:29, 21319.95it/s]296562it [00:28, 4771.53it/s]359681it [00:29, 20856.04it/s]357298it [00:29, 21013.12it/s]298651it [00:28, 6190.80it/s]361777it [00:29, 20883.32it/s]359460it [00:29, 21129.37it/s]300660it [00:28, 7725.86it/s]363867it [00:29, 20873.73it/s]361593it [00:29, 21186.76it/s]302743it [00:28, 9526.09it/s]365955it [00:29, 20614.02it/s]363713it [00:29, 21123.61it/s]304832it [00:28, 11389.74it/s]368023it [00:29, 20632.00it/s]365826it [00:29, 21009.48it/s]306890it [00:28, 13133.94it/s]370135it [00:29, 20776.18it/s]367928it [00:29, 20912.97it/s]309031it [00:28, 14898.40it/s]372214it [00:29, 20491.24it/s]370020it [00:29, 20851.15it/s]311123it [00:28, 16303.46it/s]288907it [00:29, 1354.26it/s] 374334it [00:29, 20697.89it/s]372110it [00:29, 20863.56it/s]313202it [00:29, 17377.11it/s]291054it [00:29, 1894.19it/s]376469it [00:30, 20889.08it/s]374197it [00:30, 20806.82it/s]315275it [00:29, 18217.07it/s]293143it [00:29, 2595.67it/s]378559it [00:30, 20793.24it/s]376361it [00:30, 21052.67it/s]317401it [00:29, 19045.87it/s]295203it [00:30, 3497.83it/s]380706it [00:30, 20993.34it/s]378497it [00:30, 21143.99it/s]319504it [00:29, 19601.27it/s]297311it [00:30, 4672.23it/s]382849it [00:30, 21121.77it/s]380671it [00:30, 21313.01it/s]321615it [00:29, 20030.98it/s]299411it [00:30, 6094.48it/s]384962it [00:30, 20715.79it/s]382803it [00:30, 21112.38it/s]323712it [00:29, 19976.61it/s]301500it [00:30, 7717.13it/s]387080it [00:30, 20850.89it/s]384915it [00:30, 20991.24it/s]325795it [00:29, 20221.45it/s]303583it [00:30, 9504.03it/s]389167it [00:30, 20835.40it/s]387048it [00:30, 21090.58it/s]327876it [00:29, 20393.18it/s]305613it [00:30, 11163.37it/s]391252it [00:30, 20708.80it/s]389158it [00:30, 20930.67it/s]330030it [00:29, 20729.71it/s]307687it [00:30, 12958.46it/s]393348it [00:30, 20782.07it/s]391312it [00:30, 21103.11it/s]332127it [00:29, 20794.48it/s]309812it [00:30, 14709.54it/s]395474it [00:30, 20924.02it/s]393423it [00:30, 20923.24it/s]334242it [00:30, 20899.27it/s]311879it [00:30, 16090.49it/s]397567it [00:31, 20595.59it/s]395577it [00:31, 21105.10it/s]336344it [00:30, 20565.55it/s]314001it [00:30, 17366.24it/s]399718it [00:31, 20862.01it/s]397689it [00:31, 20872.32it/s]338446it [00:30, 20696.98it/s]316079it [00:31, 18259.77it/s]401823it [00:31, 20917.60it/s]399821it [00:31, 21002.83it/s]340523it [00:30, 20713.45it/s]318156it [00:31, 18724.44it/s]403916it [00:31, 20804.93it/s]401923it [00:31, 20909.77it/s]342629it [00:30, 20814.84it/s]320219it [00:31, 19253.45it/s]406021it [00:31, 20876.03it/s]404055it [00:31, 21030.54it/s]344719it [00:30, 20837.27it/s]322341it [00:31, 19810.55it/s]408178it [00:31, 20984.84it/s]406238it [00:31, 21266.20it/s]346806it [00:30, 20738.37it/s]324416it [00:31, 20010.87it/s]410277it [00:31, 20894.04it/s]408366it [00:31, 21081.87it/s]348891it [00:30, 20770.40it/s]326506it [00:31, 20266.91it/s]410555it [00:31, 21321.26it/s]351007it [00:30, 20885.46it/s]328599it [00:31, 20459.84it/s]353097it [00:30, 20861.26it/s]330759it [00:31, 20795.42it/s]355248it [00:31, 21053.38it/s]332863it [00:31, 20368.69it/s]357354it [00:31, 20943.81it/s]334952it [00:31, 20518.68it/s]359449it [00:31, 20840.81it/s]337018it [00:32, 20493.47it/s]361534it [00:31, 20623.33it/s]339110it [00:32, 20616.95it/s]363597it [00:31, 20624.25it/s]341201it [00:32, 20703.05it/s]365660it [00:31, 20592.65it/s]343301it [00:32, 20787.61it/s]367723it [00:31, 20602.93it/s]345384it [00:32, 20390.33it/s]369784it [00:31, 20412.07it/s]347471it [00:32, 20528.91it/s]371870it [00:31, 20543.31it/s]349541it [00:32, 20576.89it/s]373925it [00:31, 20511.53it/s]351656it [00:32, 20745.17it/s]376090it [00:32, 20850.26it/s]353757it [00:32, 20821.69it/s]378219it [00:32, 20981.11it/s]355899it [00:32, 20998.82it/s]380414it [00:32, 21268.88it/s]358000it [00:33, 20878.81it/s]382542it [00:32, 21007.29it/s]360131it [00:33, 21005.22it/s]384650it [00:32, 21026.26it/s]362233it [00:33, 20852.30it/s]386754it [00:32, 20902.96it/s]364330it [00:33, 20884.79it/s]388850it [00:32, 20917.62it/s]366419it [00:33, 20533.35it/s]390943it [00:32, 20893.38it/s]368513it [00:33, 20652.48it/s]393033it [00:32, 20626.93it/s]370580it [00:33, 20638.21it/s]395105it [00:32, 20653.69it/s]372667it [00:33, 20705.66it/s]397226it [00:33, 20817.64it/s]374747it [00:33, 20731.77it/s]399315it [00:33, 20838.66it/s]376842it [00:33, 20794.49it/s]401436it [00:33, 20948.21it/s]378966it [00:34, 20927.30it/s]403532it [00:33, 20927.30it/s]381127it [00:34, 21129.61it/s]405661it [00:33, 20957.76it/s]383241it [00:34, 21071.13it/s]407757it [00:33, 20802.08it/s]385354it [00:34, 21086.72it/s]409862it [00:33, 20873.69it/s]387463it [00:34, 20710.50it/s]389581it [00:34, 20847.77it/s]391670it [00:34, 20857.58it/s]393791it [00:34, 20961.00it/s]395888it [00:34, 20870.02it/s]397976it [00:35, 20709.08it/s]400061it [00:35, 20750.09it/s]402199it [00:35, 20935.71it/s]404312it [00:35, 20992.20it/s]406470it [00:35, 21167.26it/s]408588it [00:35, 20932.01it/s]410718it [00:35, 21039.95it/s]412367it [00:36, 1363.46it/s] 414429it [00:36, 1882.95it/s]412688it [00:36, 1399.30it/s] 416609it [00:36, 2622.03it/s]414912it [00:36, 1965.58it/s]418722it [00:36, 3553.73it/s]416966it [00:36, 2659.60it/s]420848it [00:37, 4740.53it/s]419141it [00:36, 3626.98it/s]422948it [00:37, 6162.31it/s]421291it [00:37, 4833.38it/s]425081it [00:37, 7846.55it/s]423372it [00:37, 6229.10it/s]427201it [00:37, 9674.51it/s]425562it [00:37, 7975.76it/s]429330it [00:37, 11574.03it/s]427731it [00:37, 9859.41it/s]431416it [00:37, 13330.67it/s]429928it [00:37, 11845.22it/s]433534it [00:37, 15001.70it/s]432054it [00:37, 13515.28it/s]435644it [00:37, 16423.95it/s]434242it [00:37, 15286.34it/s]437787it [00:37, 17674.68it/s]436402it [00:37, 16752.25it/s]439961it [00:37, 18743.47it/s]438537it [00:37, 17760.04it/s]442092it [00:38, 19313.14it/s]440743it [00:37, 18880.44it/s]444248it [00:38, 19940.43it/s]442893it [00:38, 19591.32it/s]446376it [00:38, 20263.01it/s]445098it [00:38, 20276.11it/s]448531it [00:38, 20633.89it/s]447261it [00:38, 20367.91it/s]450663it [00:38, 20540.90it/s]449412it [00:38, 20643.81it/s]452777it [00:38, 20713.74it/s]451576it [00:38, 20930.05it/s]454883it [00:38, 20746.48it/s]453718it [00:38, 20949.04it/s]456982it [00:38, 20815.31it/s]455876it [00:38, 21131.86it/s]459081it [00:38, 20639.43it/s]458014it [00:38, 21170.58it/s]461168it [00:38, 20703.32it/s]460171it [00:38, 21286.84it/s]463247it [00:39, 20505.09it/s]462312it [00:38, 21038.27it/s]465388it [00:39, 20768.92it/s]464498it [00:39, 21279.16it/s]467539it [00:39, 20987.87it/s]466659it [00:39, 21376.36it/s]469653it [00:39, 21031.85it/s]468802it [00:39, 21388.82it/s]471759it [00:39, 20941.28it/s]470995it [00:39, 21549.90it/s]473901it [00:39, 21083.10it/s]473168it [00:39, 21601.46it/s]476069it [00:39, 21260.17it/s]475370it [00:39, 21725.04it/s]478197it [00:39, 21108.13it/s]477555it [00:39, 21761.57it/s]480387it [00:39, 21343.43it/s]479758it [00:39, 21840.48it/s]482523it [00:39, 21081.20it/s]481965it [00:39, 21907.50it/s]484157it [00:39, 21876.47it/s]484689it [00:40, 21209.01it/s]486858it [00:40, 21349.60it/s]486372it [00:40, 21859.78it/s]488560it [00:40, 21863.15it/s]488994it [00:40, 21194.07it/s]490747it [00:40, 21695.21it/s]491115it [00:40, 21100.82it/s]492960it [00:40, 21823.88it/s]493269it [00:40, 21229.55it/s]495447it [00:40, 21391.84it/s]495143it [00:40, 21800.08it/s]497324it [00:40, 21754.89it/s]497587it [00:40, 21147.36it/s]499726it [00:40, 21216.66it/s]499500it [00:40, 21293.59it/s]501849it [00:40, 20986.41it/s]501667it [00:40, 21402.70it/s]503866it [00:40, 21573.75it/s]504009it [00:40, 21117.80it/s]506158it [00:41, 21225.30it/s]506025it [00:40, 21512.45it/s]508283it [00:41, 21230.87it/s]508207it [00:41, 21602.49it/s]510368it [00:41, 21531.83it/s]510407it [00:41, 21033.99it/s]512512it [00:41, 21034.88it/s]512522it [00:41, 21305.55it/s]514672it [00:41, 21361.87it/s]514616it [00:41, 20821.59it/s]516809it [00:41, 21342.12it/s]516709it [00:41, 20852.83it/s]411950it [00:40, 977.77it/s]  518957it [00:41, 21381.57it/s]518828it [00:41, 20952.03it/s]414120it [00:40, 1383.11it/s]520924it [00:41, 20563.48it/s]521096it [00:41, 20862.21it/s]416260it [00:40, 1927.25it/s]523030it [00:41, 20708.18it/s]523245it [00:41, 21045.44it/s]418340it [00:40, 2632.73it/s]525103it [00:41, 20709.28it/s]525352it [00:41, 21018.51it/s]420441it [00:41, 3564.35it/s]527178it [00:42, 20718.47it/s]527488it [00:41, 21116.98it/s]422531it [00:41, 4732.28it/s]529617it [00:42, 21166.72it/s]529251it [00:42, 20350.04it/s]424574it [00:41, 6112.48it/s]531342it [00:42, 20512.94it/s]531735it [00:42, 20882.75it/s]426731it [00:41, 7816.62it/s]533912it [00:42, 21144.52it/s]533409it [00:42, 20515.84it/s]428860it [00:41, 9664.65it/s]536091it [00:42, 21334.59it/s]535487it [00:42, 20591.03it/s]430947it [00:41, 11500.44it/s]538302it [00:42, 21561.88it/s]537627it [00:42, 20830.54it/s]433071it [00:41, 13347.41it/s]539771it [00:42, 21010.80it/s]540460it [00:42, 21485.38it/s]412823it [00:42, 990.14it/s]  435185it [00:41, 15007.95it/s]541878it [00:42, 21021.87it/s]542641it [00:42, 21581.26it/s]414970it [00:42, 1391.86it/s]437345it [00:41, 16545.74it/s]543981it [00:42, 20798.21it/s]544800it [00:42, 21139.64it/s]417058it [00:42, 1923.40it/s]439469it [00:41, 17719.09it/s]546080it [00:42, 20854.27it/s]546959it [00:42, 21270.30it/s]419169it [00:42, 2641.61it/s]441628it [00:42, 18737.02it/s]548246it [00:43, 21093.90it/s]549088it [00:42, 20921.79it/s]421285it [00:42, 3582.81it/s]443758it [00:42, 19327.91it/s]550356it [00:43, 20848.55it/s]551252it [00:43, 21132.54it/s]423370it [00:43, 4750.28it/s]445913it [00:42, 19949.04it/s]552546it [00:43, 21158.98it/s]553461it [00:43, 21415.34it/s]425513it [00:43, 6220.51it/s]448042it [00:42, 19948.74it/s]554664it [00:43, 21163.47it/s]555605it [00:43, 21181.00it/s]427616it [00:43, 7876.63it/s]450139it [00:42, 20240.29it/s]556782it [00:43, 20956.77it/s]557758it [00:43, 21282.44it/s]429749it [00:43, 9729.41it/s]452230it [00:42, 20340.82it/s]558879it [00:43, 20940.75it/s]559903it [00:43, 21330.36it/s]431828it [00:43, 11538.19it/s]454339it [00:42, 20556.60it/s]561043it [00:43, 21146.39it/s]562063it [00:43, 21409.65it/s]433948it [00:43, 13374.00it/s]456429it [00:42, 20589.26it/s]563171it [00:43, 21184.54it/s]564249it [00:43, 21542.70it/s]436043it [00:43, 14991.32it/s]458522it [00:42, 20689.32it/s]565290it [00:43, 21052.70it/s]438201it [00:43, 16531.01it/s]460608it [00:42, 20630.68it/s]440312it [00:43, 17669.81it/s]462711it [00:43, 20747.92it/s]442422it [00:43, 18469.42it/s]464800it [00:43, 20789.61it/s]444519it [00:44, 19104.81it/s]466946it [00:43, 20988.65it/s]446680it [00:44, 19800.77it/s]469050it [00:43, 20958.30it/s]448792it [00:44, 20073.23it/s]471178it [00:43, 21052.92it/s]450893it [00:44, 20331.10it/s]473286it [00:43, 20685.91it/s]452993it [00:44, 20442.16it/s]475441it [00:43, 20939.47it/s]455096it [00:44, 20613.89it/s]477574it [00:43, 21053.66it/s]457191it [00:44, 20602.98it/s]479729it [00:43, 21201.19it/s]459291it [00:44, 20718.48it/s]481892it [00:43, 21326.81it/s]461380it [00:44, 20629.60it/s]484026it [00:44, 21212.13it/s]463455it [00:44, 20569.77it/s]486182it [00:44, 21313.30it/s]465551it [00:45, 20683.38it/s]488314it [00:44, 21237.50it/s]467670it [00:45, 20833.62it/s]490473it [00:44, 21339.85it/s]469768it [00:45, 20876.41it/s]492608it [00:44, 21277.39it/s]471910it [00:45, 21037.43it/s]494770it [00:44, 21378.88it/s]474021it [00:45, 21056.61it/s]496909it [00:44, 21290.35it/s]476165it [00:45, 21169.50it/s]499039it [00:44, 21029.28it/s]478298it [00:45, 21216.20it/s]501181it [00:44, 21143.82it/s]480468it [00:45, 21360.89it/s]503306it [00:44, 21172.64it/s]482605it [00:45, 21290.99it/s]505449it [00:45, 21248.85it/s]484735it [00:45, 21055.00it/s]507575it [00:45, 21159.73it/s]486878it [00:46, 21164.73it/s]509722it [00:45, 21250.60it/s]488996it [00:46, 21093.93it/s]511848it [00:45, 21109.86it/s]491156it [00:46, 21241.69it/s]513960it [00:45, 21078.27it/s]493281it [00:46, 21223.86it/s]516069it [00:45, 20985.97it/s]495424it [00:46, 21284.85it/s]518183it [00:45, 21029.35it/s]497553it [00:46, 21193.95it/s]520287it [00:45, 20845.67it/s]499699it [00:46, 21270.76it/s]522372it [00:45, 19968.90it/s]501827it [00:46, 21197.82it/s]524428it [00:45, 20139.24it/s]503947it [00:46, 20968.51it/s]526527it [00:46, 20386.58it/s]506045it [00:46, 20884.80it/s]528571it [00:46, 20394.14it/s]508183it [00:47, 21031.12it/s]530728it [00:46, 20740.31it/s]510287it [00:47, 20954.18it/s]532823it [00:46, 20801.41it/s]512408it [00:47, 21027.14it/s]534959it [00:46, 20965.97it/s]514511it [00:47, 20845.77it/s]537090it [00:46, 21067.14it/s]516610it [00:47, 20756.72it/s]539235it [00:46, 21180.23it/s]518728it [00:47, 20879.87it/s]541354it [00:46, 21094.76it/s]520817it [00:47, 20771.08it/s]543486it [00:46, 21160.29it/s]522895it [00:47, 20747.38it/s]545603it [00:46, 21041.50it/s]524981it [00:47, 20778.14it/s]547726it [00:47, 21095.33it/s]527059it [00:47, 19924.24it/s]549891it [00:47, 21259.71it/s]529183it [00:48, 20303.34it/s]552018it [00:47, 21160.33it/s]531279it [00:48, 20493.16it/s]554179it [00:47, 21292.15it/s]533411it [00:48, 20668.29it/s]556309it [00:47, 20670.34it/s]535567it [00:48, 20930.08it/s]558448it [00:47, 20880.34it/s]537690it [00:48, 21018.73it/s]560560it [00:47, 20950.41it/s]539815it [00:48, 21086.58it/s]562711it [00:47, 21115.13it/s]541926it [00:48, 21009.68it/s]564825it [00:47, 21096.60it/s]544036it [00:48, 21036.27it/s]546141it [00:48, 21017.95it/s]548305it [00:48, 21203.12it/s]550426it [00:49, 21185.45it/s]566404it [00:49, 1251.22it/s] 552601it [00:49, 21353.80it/s]568500it [00:49, 1728.09it/s]554737it [00:49, 21100.46it/s]570677it [00:49, 2397.67it/s]556860it [00:49, 21138.61it/s]572889it [00:49, 3296.04it/s]558975it [00:49, 21106.95it/s]575058it [00:49, 4419.84it/s]561113it [00:49, 21187.87it/s]577162it [00:49, 5748.71it/s]563233it [00:49, 21118.29it/s]567396it [00:49, 1131.60it/s] 579357it [00:49, 7414.12it/s]565346it [00:49, 21076.13it/s]569564it [00:49, 1590.89it/s]581502it [00:49, 9211.27it/s]571733it [00:50, 2212.81it/s]583635it [00:50, 11084.75it/s]573738it [00:50, 2967.88it/s]585801it [00:50, 12997.19it/s]575860it [00:50, 4008.64it/s]587928it [00:50, 14623.82it/s]577999it [00:50, 5315.99it/s]590090it [00:50, 16203.29it/s]580007it [00:50, 6721.89it/s]592271it [00:50, 17571.11it/s]582145it [00:50, 8500.77it/s]594414it [00:50, 18565.96it/s]584237it [00:50, 10336.10it/s]596583it [00:50, 19406.99it/s]586387it [00:50, 12281.51it/s]598734it [00:50, 19836.39it/s]588515it [00:50, 14075.39it/s]600867it [00:50, 20200.90it/s]590637it [00:50, 15657.41it/s]602994it [00:50, 20161.45it/s]592739it [00:51, 16868.23it/s]605134it [00:51, 20516.79it/s]594880it [00:51, 18024.35it/s]607336it [00:51, 20953.90it/s]597060it [00:51, 19029.80it/s]609523it [00:51, 21220.60it/s]599188it [00:51, 19511.91it/s]611673it [00:51, 21123.96it/s]601316it [00:51, 20008.61it/s]613884it [00:51, 21413.28it/s]603433it [00:51, 20315.15it/s]616062it [00:51, 21519.10it/s]605547it [00:51, 20473.88it/s]618255it [00:51, 21640.52it/s]607677it [00:51, 20713.50it/s]620427it [00:51, 21352.25it/s]609864it [00:51, 21052.41it/s]622568it [00:51, 21292.14it/s]611999it [00:52, 21050.96it/s]624708it [00:51, 21323.45it/s]614163it [00:52, 21224.37it/s]626844it [00:52, 21251.17it/s]616301it [00:52, 21207.77it/s]628972it [00:52, 21065.66it/s]618442it [00:52, 21266.89it/s]631132it [00:52, 21219.04it/s]620615it [00:52, 21402.12it/s]633319it [00:52, 21409.98it/s]622761it [00:52, 21065.72it/s]635484it [00:52, 21481.24it/s]624886it [00:52, 21117.07it/s]637633it [00:52, 21382.10it/s]627002it [00:52, 20849.00it/s]639785it [00:52, 21420.33it/s]629127it [00:52, 20964.88it/s]641944it [00:52, 21469.89it/s]631269it [00:52, 21093.33it/s]644122it [00:52, 21559.88it/s]633381it [00:53, 21094.86it/s]646307it [00:52, 21645.81it/s]635524it [00:53, 21192.46it/s]648472it [00:53, 21464.70it/s]637645it [00:53, 21087.02it/s]650619it [00:53, 21461.92it/s]639786it [00:53, 21181.52it/s]652766it [00:53, 21063.75it/s]641906it [00:53, 21184.89it/s]654943it [00:53, 21271.12it/s]644025it [00:53, 20981.02it/s]657072it [00:53, 21107.64it/s]646177it [00:53, 21139.62it/s]659264it [00:53, 21347.44it/s]648341it [00:53, 21287.43it/s]661400it [00:53, 21267.04it/s]650471it [00:53, 21275.65it/s]663560it [00:53, 21361.77it/s]652599it [00:53, 21037.86it/s]665697it [00:53, 21074.70it/s]654719it [00:54, 21078.91it/s]667864it [00:53, 21249.45it/s]656883it [00:54, 21243.46it/s]670007it [00:54, 21302.55it/s]659013it [00:54, 21257.27it/s]672141it [00:54, 21311.50it/s]661140it [00:54, 20945.43it/s]674273it [00:54, 21107.82it/s]663236it [00:54, 20934.66it/s]676452it [00:54, 21309.75it/s]665331it [00:54, 20798.82it/s]678658it [00:54, 21532.89it/s]667443it [00:54, 20891.63it/s]680846it [00:54, 21635.96it/s]669560it [00:54, 20955.84it/s]683034it [00:54, 21706.67it/s]671698it [00:54, 21081.61it/s]685206it [00:54, 21577.01it/s]673807it [00:54, 20824.11it/s]687384it [00:54, 21636.49it/s]675976it [00:55, 21079.80it/s]689588it [00:54, 21755.55it/s]678105it [00:55, 21139.85it/s]691764it [00:55, 21711.85it/s]680291it [00:55, 21354.38it/s]693936it [00:55, 21445.23it/s]682428it [00:55, 21142.65it/s]696119it [00:55, 21557.02it/s]684609it [00:55, 21338.10it/s]698276it [00:55, 21557.11it/s]686750it [00:55, 21358.04it/s]700498it [00:55, 21751.73it/s]688888it [00:55, 21361.47it/s]702674it [00:55, 21450.01it/s]691025it [00:55, 21172.06it/s]704853it [00:55, 21547.34it/s]693144it [00:55, 21174.98it/s]707030it [00:55, 21613.02it/s]695289it [00:55, 21254.90it/s]709193it [00:55, 21494.17it/s]697415it [00:56, 21204.97it/s]711343it [00:55, 21213.42it/s]699536it [00:56, 21048.22it/s]713484it [00:56, 21270.59it/s]701687it [00:56, 21183.09it/s]715691it [00:56, 21506.84it/s]703845it [00:56, 21298.69it/s]717854it [00:56, 21540.72it/s]705976it [00:56, 21063.17it/s]720009it [00:56, 21506.53it/s]708109it [00:56, 21140.83it/s]722209it [00:56, 21652.76it/s]710224it [00:56, 21071.00it/s]724375it [00:56, 21370.00it/s]712336it [00:56, 21084.77it/s]726583it [00:56, 21579.89it/s]714445it [00:56, 20831.18it/s]728777it [00:56, 21686.50it/s]716601it [00:56, 20982.98it/s]730962it [00:56, 21732.37it/s]718754it [00:57, 21144.08it/s]733174it [00:56, 21847.81it/s]566936it [00:56, 838.60it/s]  720870it [00:57, 21122.74it/s]735360it [00:57, 21402.93it/s]569030it [00:56, 1173.04it/s]722983it [00:57, 20975.72it/s]737530it [00:57, 21489.37it/s]571211it [00:56, 1652.11it/s]725111it [00:57, 21065.33it/s]739681it [00:57, 21426.98it/s]573334it [00:56, 2280.54it/s]727303it [00:57, 21318.98it/s]741825it [00:57, 21284.32it/s]575481it [00:56, 3120.47it/s]729436it [00:57, 21113.00it/s]743987it [00:57, 21381.69it/s]577658it [00:56, 4219.25it/s]731612it [00:57, 21304.40it/s]746126it [00:57, 21373.04it/s]579759it [00:56, 5522.73it/s]733753it [00:57, 21332.22it/s]748264it [00:57, 21243.03it/s]581889it [00:56, 7097.59it/s]735893it [00:57, 21351.33it/s]750439it [00:57, 21392.79it/s]583967it [00:56, 8799.25it/s]738029it [00:57, 20991.95it/s]752638it [00:57, 21568.57it/s]586110it [00:57, 10709.13it/s]740130it [00:58, 20978.96it/s]754796it [00:58, 21504.43it/s]588242it [00:57, 12593.23it/s]567454it [00:57, 842.72it/s]  742229it [00:58, 20964.17it/s]756947it [00:58, 21419.67it/s]590406it [00:57, 14423.16it/s]569529it [00:58, 1176.36it/s]744327it [00:58, 20666.40it/s]592529it [00:57, 15662.94it/s]571666it [00:58, 1648.54it/s]746453it [00:58, 20839.67it/s]594667it [00:57, 17030.54it/s]573801it [00:58, 2284.63it/s]748578it [00:58, 20961.13it/s]596775it [00:57, 18060.84it/s]575943it [00:58, 3128.97it/s]750752it [00:58, 21192.46it/s]598920it [00:57, 18963.12it/s]578060it [00:58, 4198.67it/s]752873it [00:58, 20956.34it/s]601032it [00:57, 19432.29it/s]580197it [00:58, 5539.54it/s]755000it [00:58, 21047.62it/s]603140it [00:57, 19893.91it/s]582266it [00:58, 7059.84it/s]757106it [00:58, 21003.93it/s]605241it [00:57, 20089.48it/s]584372it [00:58, 8815.22it/s]607376it [00:58, 20453.85it/s]586443it [00:58, 10612.82it/s]609503it [00:58, 20690.75it/s]588601it [00:58, 12566.22it/s]611641it [00:58, 20892.42it/s]590696it [00:59, 14167.19it/s]613802it [00:58, 21104.22it/s]592833it [00:59, 15776.85it/s]615933it [00:58, 21051.20it/s]594940it [00:59, 17057.19it/s]618053it [00:58, 20698.48it/s]597086it [00:59, 18187.27it/s]620141it [00:58, 20744.30it/s]599198it [00:59, 18949.20it/s]622225it [00:58, 20770.99it/s]601307it [00:59, 19505.00it/s]624308it [00:58, 20588.25it/s]603412it [00:59, 19826.45it/s]626371it [00:58, 20588.63it/s]605511it [00:59, 20154.27it/s]628433it [00:59, 20392.55it/s]607620it [00:59, 20423.80it/s]630516it [00:59, 20519.96it/s]609718it [00:59, 20335.78it/s]632639it [00:59, 20728.36it/s]611850it [01:00, 20623.67it/s]634766it [00:59, 20888.73it/s]613996it [01:00, 20869.33it/s]636860it [00:59, 20900.72it/s]616107it [01:00, 20938.31it/s]639005it [00:59, 21062.11it/s]618245it [01:00, 21069.09it/s]641112it [00:59, 20933.66it/s]620362it [01:00, 21053.10it/s]643206it [00:59, 20463.82it/s]622475it [01:00, 21010.91it/s]645294it [00:59, 20584.18it/s]624581it [01:00, 20827.44it/s]647414it [00:59, 20763.78it/s]626673it [01:00, 20854.51it/s]649509it [01:00, 20816.73it/s]628762it [01:00, 20719.10it/s]651613it [01:00, 20881.90it/s]630883it [01:00, 20863.46it/s]653703it [01:00, 20796.09it/s]632971it [01:01, 20404.35it/s]655807it [01:00, 20865.76it/s]635117it [01:01, 20713.79it/s]657895it [01:00, 20797.99it/s]637211it [01:01, 20777.44it/s]659976it [01:00, 20779.75it/s]639321it [01:01, 20835.58it/s]662055it [01:00, 20592.52it/s]641450it [01:01, 20968.65it/s]664133it [01:00, 20646.10it/s]643548it [01:01, 20891.29it/s]666201it [01:00, 20549.77it/s]645691it [01:01, 21046.64it/s]668277it [01:00, 20610.29it/s]647797it [01:01, 21024.18it/s]670339it [01:01, 20238.24it/s]649933it [01:01, 21124.16it/s]672386it [01:01, 20305.02it/s]652046it [01:01, 21025.36it/s]674475it [01:01, 20477.55it/s]654168it [01:02, 21081.80it/s]676545it [01:01, 20541.38it/s]656277it [01:02, 20517.17it/s]678684it [01:01, 20793.36it/s]658422it [01:02, 20789.46it/s]680800it [01:01, 20902.59it/s]660504it [01:02, 20740.10it/s]682934it [01:01, 21032.53it/s]662601it [01:02, 20806.21it/s]685038it [01:01, 20944.87it/s]664684it [01:02, 20744.26it/s]687167it [01:01, 21045.28it/s]666799it [01:02, 20863.90it/s]689272it [01:01, 20983.55it/s]668887it [01:02, 20827.32it/s]691399it [01:02, 21068.73it/s]671002it [01:02, 20921.98it/s]693507it [01:02, 21000.56it/s]673095it [01:02, 20858.39it/s]695608it [01:02, 20365.25it/s]675210it [01:03, 20943.58it/s]697719it [01:02, 20580.83it/s]677316it [01:03, 20977.37it/s]699818it [01:02, 20701.17it/s]679414it [01:03, 20623.62it/s]701966it [01:02, 20930.44it/s]681478it [01:03, 20066.75it/s]704072it [01:02, 20964.72it/s]683637it [01:03, 20509.85it/s]706205it [01:02, 21073.10it/s]685754it [01:03, 20700.79it/s]708314it [01:02, 20870.56it/s]687901it [01:03, 20922.18it/s]710403it [01:02, 20822.86it/s]690005it [01:03, 20955.03it/s]712487it [01:03, 20683.65it/s]692148it [01:03, 21096.14it/s]714609it [01:03, 20841.10it/s]694259it [01:03, 21042.99it/s]716694it [01:03, 20812.51it/s]696368it [01:04, 21053.85it/s]718818it [01:03, 20938.52it/s]698475it [01:04, 21003.24it/s]720913it [01:03, 20386.89it/s]700640it [01:04, 21194.34it/s]723028it [01:03, 20609.07it/s]702760it [01:04, 20690.11it/s]725111it [01:03, 20673.95it/s]704886it [01:04, 20857.30it/s]727247it [01:03, 20877.16it/s]706998it [01:04, 20933.56it/s]729348it [01:03, 20915.30it/s]709094it [01:04, 20832.02it/s]731480it [01:03, 21032.88it/s]711183it [01:04, 20846.50it/s]733585it [01:04, 20949.15it/s]713269it [01:04, 20767.14it/s]735681it [01:04, 20929.98it/s]715415it [01:04, 20970.59it/s]737775it [01:04, 20847.88it/s]717513it [01:05, 20950.42it/s]759090it [01:05, 978.93it/s]  739861it [01:04, 20837.60it/s]719626it [01:05, 21000.12it/s]761211it [01:05, 1365.03it/s]741946it [01:04, 20519.49it/s]721727it [01:05, 20934.21it/s]763388it [01:05, 1906.59it/s]744024it [01:04, 20595.70it/s]765533it [01:05, 2620.41it/s]723821it [01:05, 20550.01it/s]746085it [01:04, 20180.08it/s]767722it [01:05, 3576.04it/s]725936it [01:05, 20724.96it/s]748198it [01:04, 20457.41it/s]769864it [01:05, 4756.54it/s]728105it [01:05, 21009.05it/s]750288it [01:04, 20584.64it/s]772027it [01:05, 6213.49it/s]730231it [01:05, 21080.45it/s]752416it [01:05, 20790.19it/s]774143it [01:05, 7854.53it/s]732389it [01:05, 21229.22it/s]754497it [01:05, 20731.23it/s]776280it [01:06, 9686.76it/s]734513it [01:05, 21159.74it/s]759207it [01:06, 935.20it/s]  756589it [01:05, 20785.70it/s]736637it [01:05, 21182.75it/s]778386it [01:06, 11403.59it/s]761198it [01:06, 1287.86it/s]758669it [01:05, 20703.04it/s]738756it [01:06, 21029.15it/s]780512it [01:06, 13238.22it/s]763327it [01:06, 1805.20it/s]782671it [01:06, 14994.47it/s]740885it [01:06, 21105.22it/s]765430it [01:06, 2490.22it/s]784798it [01:06, 16443.34it/s]742996it [01:06, 20852.43it/s]767581it [01:06, 3411.43it/s]786951it [01:06, 17703.49it/s]745099it [01:06, 20902.57it/s]769590it [01:06, 4488.02it/s]789091it [01:06, 18669.68it/s]747190it [01:06, 20438.30it/s]771717it [01:06, 5907.33it/s]791294it [01:06, 19582.82it/s]749347it [01:06, 20769.55it/s]773792it [01:06, 7507.23it/s]793456it [01:06, 20152.24it/s]751453it [01:06, 20853.35it/s]775890it [01:06, 9304.18it/s]795610it [01:06, 20523.03it/s]753562it [01:06, 20917.79it/s]777943it [01:07, 11061.41it/s]797762it [01:07, 20560.60it/s]755675it [01:06, 20978.79it/s]780021it [01:07, 12864.67it/s]799892it [01:07, 20774.31it/s]757774it [01:07, 20927.38it/s]782150it [01:07, 14629.67it/s]802042it [01:07, 20985.70it/s]784231it [01:07, 16051.92it/s]804178it [01:07, 21093.04it/s]786325it [01:07, 17261.46it/s]806374it [01:07, 21347.75it/s]788417it [01:07, 18216.52it/s]808536it [01:07, 21428.45it/s]790589it [01:07, 19168.25it/s]810743it [01:07, 21618.92it/s]792700it [01:07, 19582.35it/s]812914it [01:07, 21637.97it/s]794808it [01:07, 20004.38it/s]815084it [01:07, 21617.48it/s]796909it [01:07, 20293.54it/s]817251it [01:07, 21318.14it/s]799009it [01:08, 20424.87it/s]819397it [01:08, 21358.02it/s]801134it [01:08, 20666.68it/s]821561it [01:08, 21440.80it/s]803236it [01:08, 20731.22it/s]823723it [01:08, 21493.90it/s]805379it [01:08, 20935.85it/s]825874it [01:08, 21467.00it/s]807511it [01:08, 21049.01it/s]828050it [01:08, 21553.07it/s]809687it [01:08, 21258.58it/s]830212it [01:08, 21567.25it/s]811822it [01:08, 21094.07it/s]832427it [01:08, 21738.15it/s]813989it [01:08, 21262.53it/s]834602it [01:08, 21626.41it/s]816120it [01:08, 21220.52it/s]836766it [01:08, 21403.93it/s]818246it [01:09, 20838.62it/s]838908it [01:08, 21260.83it/s]820353it [01:09, 20904.10it/s]841046it [01:09, 21293.71it/s]822504it [01:09, 21082.82it/s]843230it [01:09, 21455.07it/s]824621it [01:09, 21106.28it/s]845376it [01:09, 21453.14it/s]826734it [01:09, 21074.07it/s]847558it [01:09, 21560.41it/s]828843it [01:09, 20438.51it/s]849726it [01:09, 21595.09it/s]830987it [01:09, 20728.94it/s]851912it [01:09, 21642.80it/s]833177it [01:09, 21071.66it/s]854109it [01:09, 21739.83it/s]835292it [01:09, 21092.73it/s]856284it [01:09, 21733.97it/s]837435it [01:09, 21191.65it/s]858458it [01:09, 21596.18it/s]839556it [01:10, 20765.67it/s]860650it [01:09, 21692.25it/s]841702it [01:10, 20967.77it/s]862832it [01:10, 21693.46it/s]843822it [01:10, 21036.30it/s]865013it [01:10, 21725.44it/s]845980it [01:10, 21196.51it/s]867186it [01:10, 21615.40it/s]848102it [01:10, 21188.01it/s]869370it [01:10, 21681.11it/s]850222it [01:10, 21154.70it/s]871539it [01:10, 21647.01it/s]852355it [01:10, 21206.57it/s]873752it [01:10, 21767.50it/s]854484it [01:10, 21230.86it/s]875977it [01:10, 21909.54it/s]856608it [01:10, 21169.49it/s]878186it [01:10, 21962.59it/s]858759it [01:10, 21270.34it/s]880383it [01:10, 21791.55it/s]860940it [01:11, 21424.27it/s]882563it [01:10, 21771.78it/s]863083it [01:11, 21183.14it/s]884741it [01:11, 21728.54it/s]865203it [01:11, 21143.29it/s]886937it [01:11, 21796.79it/s]867318it [01:11, 21075.39it/s]889117it [01:11, 21766.44it/s]869426it [01:11, 21011.87it/s]891294it [01:11, 21703.52it/s]871544it [01:11, 21060.55it/s]893465it [01:11, 21580.02it/s]873732it [01:11, 21304.81it/s]895624it [01:11, 21470.06it/s]875863it [01:11, 21198.77it/s]897772it [01:11, 21429.16it/s]878020it [01:11, 21301.12it/s]899935it [01:11, 21487.30it/s]880185it [01:11, 21403.31it/s]902133it [01:11, 21633.83it/s]882326it [01:12, 21183.91it/s]904297it [01:11, 21623.35it/s]884477it [01:12, 21279.31it/s]906495it [01:12, 21727.32it/s]886628it [01:12, 21345.68it/s]908668it [01:12, 21667.80it/s]888763it [01:12, 21293.35it/s]910835it [01:12, 21534.24it/s]890893it [01:12, 21152.92it/s]912990it [01:12, 21537.28it/s]893018it [01:12, 21178.83it/s]915144it [01:12, 21499.29it/s]895137it [01:12, 20921.05it/s]917295it [01:12, 21320.75it/s]897270it [01:12, 20961.86it/s]919477it [01:12, 21467.01it/s]899441it [01:12, 21183.38it/s]921722it [01:12, 21758.17it/s]901560it [01:12, 20974.24it/s]924000it [01:12, 22060.97it/s]903715it [01:13, 21143.58it/s]926218it [01:12, 22095.75it/s]905847it [01:13, 21194.64it/s]928458it [01:13, 22186.36it/s]907968it [01:13, 21130.20it/s]930753it [01:13, 22412.79it/s]910082it [01:13, 21070.05it/s]933008it [01:13, 22452.42it/s]912213it [01:13, 21138.56it/s]935254it [01:13, 22293.16it/s]914328it [01:13, 21095.44it/s]937514it [01:13, 22381.80it/s]916438it [01:13, 20974.66it/s]939753it [01:13, 22053.75it/s]918538it [01:13, 20980.28it/s]942007it [01:13, 22195.10it/s]920782it [01:13, 21415.34it/s]944269it [01:13, 22319.50it/s]922924it [01:13, 21378.38it/s]946506it [01:13, 22333.75it/s]925111it [01:14, 21523.37it/s]948756it [01:13, 22381.71it/s]927352it [01:14, 21786.94it/s]951018it [01:14, 22452.18it/s]929531it [01:14, 21614.32it/s]953264it [01:14, 22226.27it/s]931756it [01:14, 21801.38it/s]955488it [01:14, 21928.41it/s]934004it [01:14, 22003.35it/s]957683it [01:14, 21912.22it/s]936205it [01:14, 21803.67it/s]959882it [01:14, 21933.46it/s]938414it [01:14, 21884.67it/s]962076it [01:14, 21912.22it/s]940603it [01:14, 21656.43it/s]964286it [01:14, 21966.93it/s]942817it [01:14, 21798.48it/s]966494it [01:14, 21999.65it/s]944998it [01:14, 21591.23it/s]968695it [01:14, 21898.74it/s]947190it [01:15, 21687.89it/s]970912it [01:14, 21978.72it/s]949396it [01:15, 21797.10it/s]973111it [01:15, 21765.55it/s]951591it [01:15, 21840.03it/s]975311it [01:15, 21831.75it/s]953776it [01:15, 21666.19it/s]977495it [01:15, 21689.26it/s]955944it [01:15, 21409.81it/s]979665it [01:15, 21524.94it/s]958086it [01:15, 21389.21it/s]981851it [01:15, 21622.84it/s]960268it [01:15, 21516.52it/s]984014it [01:15, 21624.61it/s]962421it [01:15, 21510.22it/s]986271it [01:15, 21905.02it/s]964573it [01:15, 21226.91it/s]988517it [01:15, 22069.45it/s]966759it [01:15, 21412.28it/s]990725it [01:15, 21961.23it/s]968908it [01:16, 21433.79it/s]992997it [01:15, 22186.78it/s]995240it [01:16, 22258.56it/s]971053it [01:16, 21261.75it/s]997486it [01:16, 22317.49it/s]973180it [01:16, 21194.24it/s]999718it [01:16, 22271.24it/s]975330it [01:16, 21283.61it/s]977483it [01:16, 21354.62it/s]979619it [01:16, 21085.50it/s]981787it [01:16, 21258.48it/s]983929it [01:16, 21304.96it/s]986061it [01:16, 21302.26it/s]988275it [01:16, 21549.45it/s]990493it [01:17, 21736.92it/s]992689it [01:17, 21802.03it/s]994902it [01:17, 21898.73it/s]997093it [01:17, 21764.22it/s]758669it [01:16, 20703.04it/s]758671it [01:16, 427.28it/s]  999270it [01:17, 21733.37it/s]760810it [01:16, 686.30it/s]762902it [01:16, 1036.94it/s]765104it [01:16, 1544.14it/s]767247it [01:16, 2207.91it/s]769428it [01:16, 3100.04it/s]771570it [01:17, 4216.72it/s]773721it [01:17, 5603.36it/s]775863it [01:17, 7217.22it/s]757774it [01:18, 20927.38it/s]758672it [01:18, 511.40it/s]  777995it [01:17, 8995.81it/s]760816it [01:18, 770.80it/s]780131it [01:17, 10899.88it/s]762930it [01:18, 1125.19it/s]782255it [01:17, 12738.38it/s]765092it [01:18, 1622.22it/s]784372it [01:17, 14462.25it/s]767218it [01:18, 2277.56it/s]786489it [01:17, 15822.26it/s]769389it [01:18, 3163.81it/s]788637it [01:17, 17191.53it/s]771516it [01:18, 4270.40it/s]790778it [01:18, 18275.05it/s]773647it [01:18, 5639.38it/s]792970it [01:18, 19252.00it/s]775774it [01:18, 7231.42it/s]795110it [01:18, 19750.53it/s]777896it [01:19, 9021.06it/s]797239it [01:18, 20173.60it/s]780011it [01:19, 10865.77it/s]799367it [01:18, 20353.23it/s]782154it [01:19, 12771.83it/s]801493it [01:18, 20614.09it/s]784270it [01:19, 14450.33it/s]803610it [01:18, 20713.24it/s]786391it [01:19, 15930.71it/s]805732it [01:18, 20860.35it/s]788494it [01:19, 17117.27it/s]807910it [01:18, 21131.37it/s]790635it [01:19, 18221.35it/s]810043it [01:18, 20960.42it/s]792816it [01:19, 19184.16it/s]812214it [01:19, 21180.63it/s]794946it [01:19, 19706.95it/s]814343it [01:19, 21159.44it/s]797074it [01:19, 20151.94it/s]816514it [01:19, 21322.52it/s]799199it [01:20, 20350.88it/s]818652it [01:19, 21290.42it/s]801326it [01:20, 20615.34it/s]820808it [01:19, 21369.54it/s]803443it [01:20, 20719.24it/s]822948it [01:19, 21355.39it/s]805582it [01:20, 20914.52it/s]825086it [01:19, 21214.62it/s]807710it [01:20, 21022.24it/s]827257it [01:19, 21361.14it/s]809832it [01:20, 21022.70it/s]829395it [01:19, 21306.74it/s]811967it [01:20, 21117.46it/s]831527it [01:19, 21239.60it/s]814112it [01:20, 21153.28it/s]833652it [01:20, 21135.81it/s]816278it [01:20, 21302.46it/s]835797it [01:20, 21227.85it/s]818413it [01:20, 21291.01it/s]837921it [01:20, 21136.92it/s]820560it [01:21, 21343.06it/s]840037it [01:20, 21143.37it/s]822697it [01:21, 21289.51it/s]842152it [01:20, 21033.83it/s]824861it [01:21, 21391.85it/s]844271it [01:20, 21079.55it/s]827002it [01:21, 21291.69it/s]846380it [01:20, 21078.81it/s]829133it [01:21, 21171.81it/s]848526it [01:20, 21190.22it/s]831267it [01:21, 21221.16it/s]850652it [01:20, 21203.60it/s]833432it [01:21, 21274.94it/s]852773it [01:20, 21165.58it/s]835572it [01:21, 21311.92it/s]854890it [01:21, 21037.71it/s]837704it [01:21, 21177.91it/s]857015it [01:21, 21099.58it/s]839823it [01:21, 21163.22it/s]859171it [01:21, 21235.49it/s]841940it [01:22, 21134.04it/s]861295it [01:21, 21143.83it/s]844101it [01:22, 21273.90it/s]863424it [01:21, 21186.76it/s]846229it [01:22, 21208.94it/s]865543it [01:21, 21084.30it/s]848387it [01:22, 21318.88it/s]867657it [01:21, 21100.29it/s]850520it [01:22, 21031.96it/s]869768it [01:21, 21029.63it/s]852682it [01:22, 21198.47it/s]871931it [01:21, 21207.88it/s]854803it [01:22, 21189.30it/s]874071it [01:21, 21264.03it/s]856952it [01:22, 21170.90it/s]876233it [01:22, 21369.91it/s]859111it [01:22, 21294.15it/s]878371it [01:22, 21082.42it/s]861244it [01:22, 21303.44it/s]880481it [01:22, 20915.86it/s]863375it [01:23, 21301.72it/s]882583it [01:22, 20944.62it/s]865506it [01:23, 21134.91it/s]884679it [01:22, 20873.85it/s]867620it [01:23, 21132.59it/s]886809it [01:22, 20998.57it/s]869734it [01:23, 20759.70it/s]888926it [01:22, 21048.89it/s]871888it [01:23, 20988.29it/s]891034it [01:22, 21054.45it/s]874037it [01:23, 21135.63it/s]893140it [01:22, 20918.97it/s]876223it [01:23, 21350.33it/s]895237it [01:22, 20933.43it/s]878367it [01:23, 21375.27it/s]897331it [01:23, 20259.94it/s]880506it [01:23, 21334.59it/s]899487it [01:23, 20637.96it/s]882651it [01:24, 21366.54it/s]901556it [01:23, 20462.35it/s]1001946it [01:24, 912.29it/s] 884789it [01:24, 21335.84it/s]903694it [01:23, 20730.01it/s]1004113it [01:24, 1269.35it/s]886930it [01:24, 21355.86it/s]905770it [01:23, 20688.95it/s]1006330it [01:24, 1770.62it/s]889066it [01:24, 21276.28it/s]907916it [01:23, 20914.55it/s]1008429it [01:24, 2407.30it/s]891194it [01:24, 20933.09it/s]910009it [01:23, 20849.22it/s]1010645it [01:24, 3299.82it/s]893289it [01:24, 20852.60it/s]912118it [01:23, 20919.94it/s]1012838it [01:24, 4429.42it/s]895382it [01:24, 20868.47it/s]914211it [01:23, 20847.07it/s]1015023it [01:24, 5818.09it/s]897471it [01:24, 20872.03it/s]916330it [01:23, 20946.44it/s]1017219it [01:24, 7467.85it/s]899620it [01:24, 21054.03it/s]918426it [01:24, 20725.63it/s]1019416it [01:25, 9315.73it/s]901726it [01:24, 21031.29it/s]920621it [01:24, 21087.13it/s]1021662it [01:25, 11334.08it/s]903842it [01:25, 21067.35it/s]922822it [01:24, 21357.82it/s]1023961it [01:25, 13434.16it/s]905969it [01:25, 21127.23it/s]924959it [01:24, 21239.18it/s]1026253it [01:25, 15375.58it/s]908141it [01:25, 21302.08it/s]927150it [01:24, 21438.06it/s]1028491it [01:25, 16955.23it/s]910272it [01:25, 21192.31it/s]929369it [01:24, 21659.53it/s]1030773it [01:25, 18386.81it/s]912392it [01:25, 20823.74it/s]931646it [01:24, 21988.99it/s]1033046it [01:25, 19509.43it/s]914535it [01:25, 21000.01it/s]933868it [01:24, 22057.22it/s]1035316it [01:25, 20368.38it/s]916637it [01:25, 20959.93it/s]936118it [01:24, 22188.32it/s]1037616it [01:25, 21098.63it/s]918785it [01:25, 21112.65it/s]938338it [01:24, 22140.53it/s]1039889it [01:25, 21547.17it/s]921008it [01:25, 21442.94it/s]940553it [01:25, 21865.86it/s]1042160it [01:26, 21846.30it/s]923257it [01:25, 21754.40it/s]942771it [01:25, 21958.56it/s]1044440it [01:26, 22122.11it/s]925459it [01:26, 21831.18it/s]1001444it [01:26, 810.07it/s] 945009it [01:25, 22082.83it/s]1046711it [01:26, 22274.11it/s]927658it [01:26, 21878.06it/s]1003473it [01:26, 1114.89it/s]947218it [01:25, 22025.97it/s]1049026it [01:26, 22531.00it/s]929905it [01:26, 22052.25it/s]1005603it [01:26, 1556.23it/s]949422it [01:25, 21945.33it/s]1051309it [01:26, 22575.55it/s]932111it [01:26, 21818.14it/s]1007741it [01:26, 2156.69it/s]951617it [01:25, 21608.49it/s]1053587it [01:26, 22279.15it/s]934334it [01:26, 21937.94it/s]1009901it [01:26, 2962.43it/s]953780it [01:25, 21464.68it/s]1055831it [01:26, 22168.35it/s]936588it [01:26, 22116.96it/s]1012097it [01:26, 4024.83it/s]955971it [01:25, 21594.49it/s]1058059it [01:26, 21945.50it/s]938801it [01:26, 21883.76it/s]1014159it [01:26, 5252.42it/s]958132it [01:25, 21545.18it/s]1060284it [01:26, 22032.86it/s]1016334it [01:26, 6829.25it/s]940991it [01:26, 21849.53it/s]960288it [01:25, 21529.63it/s]1062493it [01:26, 22008.98it/s]1018477it [01:27, 8583.11it/s]943238it [01:26, 22033.35it/s]962480it [01:26, 21642.90it/s]1064698it [01:27, 22008.89it/s]1020610it [01:27, 10449.87it/s]945442it [01:26, 21990.71it/s]964645it [01:26, 21534.16it/s]1066914it [01:27, 22052.88it/s]1022832it [01:27, 12486.19it/s]947671it [01:27, 22028.11it/s]966834it [01:26, 21638.42it/s]1069134it [01:27, 22095.43it/s]1025075it [01:27, 14456.17it/s]949895it [01:27, 22090.76it/s]968999it [01:26, 21459.88it/s]1071345it [01:27, 22056.26it/s]1027361it [01:27, 16309.68it/s]952105it [01:27, 21953.41it/s]971163it [01:26, 21510.60it/s]1073616it [01:27, 22248.71it/s]1029569it [01:27, 17668.19it/s]954301it [01:27, 21864.43it/s]973315it [01:26, 21394.63it/s]1075856it [01:27, 22291.71it/s]1031794it [01:27, 18833.63it/s]956488it [01:27, 21442.03it/s]975455it [01:26, 21300.58it/s]1078098it [01:27, 22327.71it/s]1034067it [01:27, 19872.51it/s]958634it [01:27, 21425.53it/s]977586it [01:26, 21154.24it/s]1080373it [01:27, 22451.50it/s]1036310it [01:27, 20576.73it/s]960814it [01:27, 21533.99it/s]979728it [01:26, 21230.81it/s]1082619it [01:27, 22435.42it/s]1038543it [01:27, 20779.40it/s]962970it [01:27, 21539.03it/s]981906it [01:26, 21391.41it/s]1084863it [01:27, 22164.50it/s]1040802it [01:28, 21293.10it/s]965166it [01:27, 21662.89it/s]984048it [01:27, 21399.30it/s]1087081it [01:28, 22144.89it/s]1043023it [01:28, 21556.98it/s]967333it [01:27, 21605.39it/s]986287it [01:27, 21694.08it/s]1089297it [01:28, 22073.00it/s]1045268it [01:28, 21817.50it/s]969512it [01:28, 21586.80it/s]988503it [01:27, 21831.14it/s]1091505it [01:28, 22025.78it/s]1047578it [01:28, 22193.53it/s]971720it [01:28, 21731.89it/s]990711it [01:27, 21904.59it/s]1093708it [01:28, 21978.74it/s]1049830it [01:28, 21979.99it/s]973894it [01:28, 21565.00it/s]992947it [01:27, 22040.01it/s]1095907it [01:28, 21848.68it/s]1052051it [01:28, 21936.70it/s]976079it [01:28, 21647.32it/s]995154it [01:27, 22047.00it/s]1098103it [01:28, 21805.04it/s]1054261it [01:28, 21754.28it/s]978245it [01:28, 21561.54it/s]997385it [01:27, 22124.80it/s]1100324it [01:28, 21924.56it/s]1056448it [01:28, 21478.75it/s]980402it [01:28, 21286.23it/s]999608it [01:27, 22154.67it/s]1102517it [01:28, 21566.61it/s]982532it [01:28, 21243.81it/s]1058605it [01:28, 21283.61it/s]1104703it [01:28, 21650.92it/s]984667it [01:28, 21272.32it/s]1060740it [01:28, 21289.52it/s]1106870it [01:28, 21633.14it/s]986922it [01:28, 21651.34it/s]1062894it [01:29, 21361.78it/s]1109035it [01:29, 21615.82it/s]989135it [01:28, 21792.43it/s]1065105it [01:29, 21581.76it/s]1111223it [01:29, 21693.65it/s]991352it [01:29, 21857.11it/s]1067266it [01:29, 21527.85it/s]1113459it [01:29, 21892.33it/s]993608it [01:29, 22064.53it/s]1069462it [01:29, 21655.91it/s]1115764it [01:29, 22238.22it/s]995823it [01:29, 22089.43it/s]1071629it [01:29, 21646.17it/s]1118152it [01:29, 22726.75it/s]1073834it [01:29, 21766.26it/s]998072it [01:29, 22104.20it/s]1120447it [01:29, 22792.72it/s]1076056it [01:29, 21900.59it/s]1122727it [01:29, 22498.95it/s]1078247it [01:29, 21650.04it/s]1124983it [01:29, 22424.45it/s]1080462it [01:29, 21751.33it/s]1127244it [01:29, 22478.41it/s]1082700it [01:29, 21935.93it/s]1129493it [01:29, 22301.54it/s]1084895it [01:30, 21931.54it/s]1131725it [01:30, 22306.33it/s]1087089it [01:30, 21544.36it/s]1133994it [01:30, 22420.24it/s]1089246it [01:30, 21546.37it/s]1136253it [01:30, 22466.42it/s]1091402it [01:30, 21531.96it/s]1138500it [01:30, 22415.06it/s]1093557it [01:30, 21518.79it/s]1140742it [01:30, 22163.34it/s]1095710it [01:30, 21127.43it/s]1143002it [01:30, 22291.97it/s]1097872it [01:30, 21271.13it/s]1145232it [01:30, 22289.43it/s]1100023it [01:30, 21338.80it/s]1147489it [01:30, 22371.77it/s]1102185it [01:30, 21420.10it/s]1149737it [01:30, 22402.33it/s]1104328it [01:30, 21344.86it/s]1151978it [01:30, 22353.18it/s]1106464it [01:31, 21098.39it/s]1154296it [01:31, 22597.91it/s]1108588it [01:31, 21139.11it/s]1156604it [01:31, 22739.56it/s]1110717it [01:31, 21181.65it/s]1158904it [01:31, 22814.98it/s]1112922it [01:31, 21436.57it/s]1161186it [01:31, 22108.45it/s]1115067it [01:31, 21422.73it/s]1163402it [01:31, 22020.99it/s]1117407it [01:31, 22013.35it/s]1165608it [01:31, 21946.32it/s]1119704it [01:31, 22297.98it/s]1167805it [01:31, 21944.94it/s]1121935it [01:31, 22277.70it/s]1170002it [01:31, 21820.15it/s]1124164it [01:31, 22192.81it/s]1172186it [01:31, 21698.51it/s]1126384it [01:32, 21897.79it/s]1174357it [01:31, 21667.69it/s]1128575it [01:32, 21825.41it/s]1176566it [01:32, 21791.24it/s]1130773it [01:32, 21870.39it/s]1178746it [01:32, 21705.89it/s]1132961it [01:32, 21862.80it/s]1181037it [01:32, 22063.10it/s]1135148it [01:32, 21560.07it/s]1183257it [01:32, 22101.22it/s]1137377it [01:32, 21774.31it/s]1185489it [01:32, 22165.22it/s]1139556it [01:32, 21755.70it/s]1187773it [01:32, 22360.15it/s]1141783it [01:32, 21847.13it/s]1190032it [01:32, 22427.60it/s]1143969it [01:32, 21785.63it/s]1192275it [01:32, 22420.22it/s]1146171it [01:32, 21854.07it/s]1194518it [01:32, 22292.24it/s]1148383it [01:33, 21932.70it/s]1196748it [01:32, 21882.68it/s]1150578it [01:33, 21935.83it/s]1198938it [01:33, 21835.10it/s]1152772it [01:33, 21878.85it/s]1201175it [01:33, 21991.25it/s]1155077it [01:33, 22228.13it/s]1203412it [01:33, 22101.04it/s]1157301it [01:33, 22106.79it/s]1205657it [01:33, 22203.99it/s]1159512it [01:33, 22085.89it/s]1207957it [01:33, 22439.60it/s]1161721it [01:33, 21869.86it/s]1210204it [01:33, 22446.13it/s]1163909it [01:33, 21613.56it/s]1212451it [01:33, 22451.22it/s]1166080it [01:33, 21637.59it/s]1214739it [01:33, 22579.35it/s]1168245it [01:33, 21444.97it/s]1216998it [01:33, 22340.23it/s]1170391it [01:34, 21361.70it/s]1219233it [01:34, 22339.25it/s]1172532it [01:34, 21374.70it/s]1221468it [01:34, 22295.52it/s]1174670it [01:34, 21187.13it/s]1223698it [01:34, 22247.15it/s]1176896it [01:34, 21502.97it/s]1225923it [01:34, 22231.29it/s]1179102it [01:34, 21665.86it/s]1228147it [01:34, 22221.27it/s]1181319it [01:34, 21815.84it/s]1230370it [01:34, 22100.49it/s]1183502it [01:34, 21766.91it/s]1232581it [01:34, 22101.05it/s]1185683it [01:34, 21779.22it/s]1234792it [01:34, 21827.94it/s]1187920it [01:34, 21954.91it/s]1236978it [01:34, 21836.57it/s]1190148it [01:34, 22050.52it/s]1239189it [01:34, 21916.25it/s]1192354it [01:35, 22003.52it/s]1241382it [01:35, 21869.46it/s]1194555it [01:35, 21852.70it/s]1243570it [01:35, 21813.53it/s]1196741it [01:35, 21699.75it/s]1245753it [01:35, 21815.59it/s]1198912it [01:35, 21118.01it/s]1247935it [01:35, 21690.81it/s]1201103it [01:35, 21346.57it/s]1250142it [01:35, 21713.47it/s]1203313it [01:35, 21563.53it/s]1252390it [01:35, 21939.41it/s]1205561it [01:35, 21833.16it/s]1254585it [01:35, 21729.78it/s]1207786it [01:35, 21956.58it/s]1256863it [01:35, 22025.66it/s]1209986it [01:35, 21967.16it/s]1259127it [01:35, 22206.21it/s]1212234it [01:35, 22118.76it/s]1261357it [01:35, 22233.08it/s]1214447it [01:36, 21300.55it/s]1263584it [01:36, 22178.07it/s]1216682it [01:36, 21605.63it/s]1265812it [01:36, 22207.21it/s]1218921it [01:36, 21834.21it/s]1268033it [01:36, 22087.23it/s]1221109it [01:36, 21756.49it/s]1270259it [01:36, 22138.21it/s]1223291it [01:36, 21772.72it/s]1272474it [01:36, 21820.32it/s]1225471it [01:36, 21753.50it/s]1274728it [01:36, 22032.33it/s]1227648it [01:36, 21694.07it/s]1277019it [01:36, 22292.57it/s]1229850it [01:36, 21789.52it/s]1279264it [01:36, 22336.31it/s]1232030it [01:36, 21472.96it/s]1281508it [01:36, 22363.91it/s]1234184it [01:36, 21457.40it/s]1283745it [01:36, 22328.15it/s]1236366it [01:37, 21564.60it/s]1286004it [01:37, 22405.52it/s]1238524it [01:37, 21547.42it/s]1288245it [01:37, 22295.38it/s]1240693it [01:37, 21588.19it/s]1290475it [01:37, 22100.02it/s]1242853it [01:37, 21503.68it/s]1292715it [01:37, 22187.31it/s]1245004it [01:37, 21444.85it/s]1294944it [01:37, 22213.43it/s]1247149it [01:37, 21328.21it/s]1297199it [01:37, 22311.85it/s]1249283it [01:37, 21229.41it/s]1299449it [01:37, 22365.13it/s]1251460it [01:37, 21389.16it/s]1253655it [01:37, 21553.60it/s]1255918it [01:37, 21873.21it/s]1258136it [01:38, 21964.52it/s]1260333it [01:38, 21959.39it/s]1262530it [01:38, 21844.55it/s]1264715it [01:38, 21775.01it/s]1266894it [01:38, 21777.72it/s]1269072it [01:38, 21718.77it/s]1271244it [01:38, 21628.29it/s]1273459it [01:38, 21782.54it/s]1275639it [01:38, 21785.78it/s]1277864it [01:38, 21918.00it/s]1280118it [01:39, 22102.85it/s]1282329it [01:39, 22050.13it/s]1284562it [01:39, 22131.77it/s]1286776it [01:39, 22030.73it/s]1288980it [01:39, 21816.61it/s]1291233it [01:39, 22026.41it/s]1293437it [01:39, 22003.83it/s]1295645it [01:39, 22025.28it/s]1297895it [01:39, 22166.84it/s]1300112it [01:39, 22042.00it/s]999608it [01:41, 22154.67it/s]999823it [01:41, 374.46it/s]  1001959it [01:42, 580.77it/s]1004151it [01:42, 876.30it/s]1006387it [01:42, 1293.10it/s]1008592it [01:42, 1852.88it/s]1010797it [01:42, 2600.90it/s]998072it [01:43, 22104.20it/s]999822it [01:43, 489.32it/s]  1013016it [01:42, 3586.28it/s]1002012it [01:43, 702.71it/s]1015219it [01:42, 4817.66it/s]1004217it [01:43, 1001.77it/s]1017412it [01:42, 6310.83it/s]1006412it [01:43, 1413.36it/s]1019604it [01:42, 8017.79it/s]1008595it [01:43, 1970.75it/s]1021836it [01:42, 9968.44it/s]1010777it [01:43, 2715.07it/s]1024139it [01:43, 12097.94it/s]1012981it [01:43, 3698.24it/s]1026420it [01:43, 14120.86it/s]1015166it [01:44, 4925.56it/s]1028748it [01:43, 16066.91it/s]1017352it [01:44, 6419.21it/s]1031016it [01:43, 17582.37it/s]1019534it [01:44, 8138.20it/s]1033314it [01:43, 18926.32it/s]1021713it [01:44, 10013.58it/s]1035588it [01:43, 19769.55it/s]1024004it [01:44, 12133.02it/s]1037842it [01:43, 20496.96it/s]1026271it [01:44, 14135.75it/s]1040137it [01:43, 21179.36it/s]1028603it [01:44, 16097.28it/s]1042401it [01:43, 21528.92it/s]1030892it [01:44, 17684.97it/s]1044679it [01:43, 21887.65it/s]1033160it [01:44, 18903.84it/s]1046949it [01:44, 22121.46it/s]1035424it [01:44, 19851.54it/s]1049249it [01:44, 22379.43it/s]1037683it [01:45, 20382.06it/s]1051525it [01:44, 22408.21it/s]1039965it [01:45, 21059.95it/s]1053792it [01:44, 21861.23it/s]1042213it [01:45, 21408.78it/s]1056000it [01:44, 21774.22it/s]1044471it [01:45, 21744.00it/s]1058193it [01:44, 21559.85it/s]1046812it [01:45, 22229.02it/s]1060360it [01:44, 21569.87it/s]1049102it [01:45, 22424.27it/s]1062571it [01:44, 21726.55it/s]1051382it [01:45, 22421.63it/s]1064754it [01:44, 21755.29it/s]1053650it [01:45, 22076.01it/s]1067024it [01:44, 22018.82it/s]1055877it [01:45, 21730.77it/s]1069229it [01:45, 21968.69it/s]1058065it [01:45, 21511.39it/s]1071428it [01:45, 21906.53it/s]1060252it [01:46, 21614.17it/s]1073694it [01:45, 22130.52it/s]1062421it [01:46, 21617.42it/s]1075917it [01:45, 22159.28it/s]1064592it [01:46, 21644.40it/s]1078140it [01:45, 22178.02it/s]1066885it [01:46, 22024.61it/s]1080401it [01:45, 22305.12it/s]1069091it [01:46, 22033.48it/s]1082633it [01:45, 22306.97it/s]1071297it [01:46, 21711.96it/s]1084864it [01:45, 22272.36it/s]1073531it [01:46, 21892.07it/s]1087092it [01:45, 21950.36it/s]1075752it [01:46, 21985.80it/s]1089289it [01:45, 21873.68it/s]1077973it [01:46, 22049.47it/s]1091478it [01:46, 21827.97it/s]1080215it [01:46, 22159.44it/s]1093662it [01:46, 21805.02it/s]1082443it [01:47, 22194.68it/s]1095843it [01:46, 21668.79it/s]1084664it [01:47, 22167.69it/s]1098022it [01:46, 21704.35it/s]1086882it [01:47, 22047.38it/s]1100193it [01:46, 21631.43it/s]1089088it [01:47, 21696.99it/s]1102357it [01:46, 21146.59it/s]1091294it [01:47, 21802.52it/s]1104518it [01:46, 21279.45it/s]1093476it [01:47, 21639.93it/s]1106648it [01:46, 21271.26it/s]1095641it [01:47, 21499.05it/s]1108830it [01:46, 21430.49it/s]1299449it [01:47, 22365.13it/s]1301454it [01:47, 693.65it/s]  1097813it [01:47, 21562.00it/s]1110975it [01:47, 21399.60it/s]1303740it [01:47, 992.81it/s]1099970it [01:47, 21550.26it/s]1113223it [01:47, 21718.81it/s]1306019it [01:48, 1405.11it/s]1102135it [01:47, 21576.55it/s]1115497it [01:47, 22021.70it/s]1308313it [01:48, 1971.69it/s]1104293it [01:48, 21199.27it/s]1117831it [01:47, 22413.46it/s]1310623it [01:48, 2736.97it/s]1106441it [01:48, 21280.17it/s]1120111it [01:47, 22526.62it/s]1312904it [01:48, 3708.56it/s]1108572it [01:48, 21286.78it/s]1122365it [01:47, 22275.01it/s]1315130it [01:48, 4910.05it/s]1110702it [01:48, 21272.17it/s]1124594it [01:47, 22245.25it/s]1317337it [01:48, 6373.01it/s]1112919it [01:48, 21538.04it/s]1126820it [01:47, 22142.58it/s]1319541it [01:48, 8066.92it/s]1115169it [01:48, 21823.79it/s]1129035it [01:47, 22142.51it/s]1321818it [01:48, 10039.23it/s]1117490it [01:48, 22236.70it/s]1131250it [01:47, 22098.05it/s]1324133it [01:48, 12120.97it/s]1119826it [01:48, 22570.61it/s]1133461it [01:48, 22062.77it/s]1326432it [01:48, 14147.98it/s]1122084it [01:48, 22255.31it/s]1135734it [01:48, 22260.56it/s]1328724it [01:49, 15992.91it/s]1124311it [01:48, 22233.60it/s]1137961it [01:48, 22191.15it/s]1330992it [01:49, 17356.63it/s]1126536it [01:49, 22234.90it/s]1140181it [01:48, 22080.99it/s]1333262it [01:49, 18671.79it/s]1128761it [01:49, 22082.26it/s]1142390it [01:48, 21890.33it/s]1335510it [01:49, 19634.41it/s]1130970it [01:49, 22024.11it/s]1144596it [01:48, 21940.13it/s]1337755it [01:49, 20228.33it/s]1133207it [01:49, 22125.47it/s]1146826it [01:48, 21967.03it/s]1340031it [01:49, 20927.57it/s]1135427it [01:49, 22146.49it/s]1149078it [01:48, 22128.42it/s]1342271it [01:49, 21200.39it/s]1137642it [01:49, 21807.79it/s]1151292it [01:48, 22107.51it/s]1344495it [01:49, 21337.52it/s]1139866it [01:49, 21935.04it/s]1153546it [01:48, 22155.06it/s]1346702it [01:49, 21443.04it/s]1142081it [01:49, 21996.97it/s]1155858it [01:49, 22440.99it/s]1348898it [01:49, 21293.42it/s]1144303it [01:49, 21966.88it/s]1158123it [01:49, 22500.69it/s]1351082it [01:50, 21449.90it/s]1146523it [01:49, 22034.91it/s]1160374it [01:49, 22170.04it/s]1300112it [01:50, 22042.00it/s]1301454it [01:50, 621.74it/s]  1353310it [01:50, 21692.56it/s]1148733it [01:50, 22053.00it/s]1162593it [01:49, 21613.66it/s]1303693it [01:50, 913.86it/s]1355498it [01:50, 21727.09it/s]1150967it [01:50, 22135.93it/s]1164758it [01:49, 21488.46it/s]1305933it [01:50, 1317.99it/s]1357724it [01:50, 21884.63it/s]1153181it [01:50, 22084.77it/s]1166944it [01:49, 21595.01it/s]1308158it [01:50, 1865.72it/s]1359922it [01:50, 21873.98it/s]1155467it [01:50, 22315.78it/s]1169106it [01:49, 21424.93it/s]1310408it [01:50, 2609.27it/s]1362121it [01:50, 21908.14it/s]1157699it [01:50, 22195.91it/s]1171250it [01:49, 21298.10it/s]1312639it [01:50, 3567.67it/s]1364317it [01:50, 21920.38it/s]1159919it [01:50, 22084.45it/s]1173405it [01:49, 21372.20it/s]1314847it [01:50, 4765.43it/s]1366513it [01:50, 21792.22it/s]1162128it [01:50, 21789.59it/s]1175543it [01:49, 21296.34it/s]1317034it [01:50, 6211.16it/s]1368695it [01:50, 21496.82it/s]1164308it [01:50, 21671.65it/s]1177809it [01:50, 21699.00it/s]1319209it [01:51, 7880.64it/s]1370907it [01:50, 21679.86it/s]1166476it [01:50, 21576.39it/s]1179980it [01:50, 21545.06it/s]1321475it [01:51, 9856.73it/s]1373077it [01:51, 21681.16it/s]1168635it [01:50, 21486.36it/s]1182206it [01:50, 21755.71it/s]1323670it [01:51, 11777.46it/s]1375270it [01:51, 21753.31it/s]1170784it [01:51, 21382.24it/s]1184438it [01:50, 21921.94it/s]1325889it [01:51, 13715.14it/s]1377480it [01:51, 21855.41it/s]1172923it [01:51, 21290.64it/s]1186648it [01:50, 21973.57it/s]1328178it [01:51, 15641.87it/s]1379714it [01:51, 21990.19it/s]1175053it [01:51, 21242.67it/s]1188871it [01:50, 22048.26it/s]1330405it [01:51, 17170.85it/s]1382011it [01:51, 22281.42it/s]1177178it [01:51, 21168.15it/s]1191162it [01:50, 22305.51it/s]1332631it [01:51, 18364.67it/s]1384242it [01:51, 22287.04it/s]1179438it [01:51, 21591.07it/s]1193393it [01:50, 22016.97it/s]1334850it [01:51, 19361.72it/s]1386472it [01:51, 21965.27it/s]1181668it [01:51, 21801.15it/s]1195596it [01:50, 21801.86it/s]1337067it [01:51, 19959.23it/s]1388714it [01:51, 22099.47it/s]1183849it [01:51, 21790.10it/s]1197778it [01:50, 21762.35it/s]1339278it [01:51, 20554.20it/s]1390925it [01:51, 22034.17it/s]1186087it [01:51, 21964.54it/s]1199955it [01:51, 21388.21it/s]1341488it [01:52, 20991.15it/s]1393130it [01:51, 22032.90it/s]1188292it [01:51, 21987.15it/s]1202178it [01:51, 21632.63it/s]1343692it [01:52, 21058.84it/s]1395351it [01:52, 22082.91it/s]1190522it [01:52, 22080.49it/s]1204405it [01:51, 21820.33it/s]1345871it [01:52, 21190.90it/s]1397560it [01:52, 21986.84it/s]1192734it [01:52, 22089.80it/s]1206620it [01:51, 21915.74it/s]1348042it [01:52, 21215.38it/s]1399760it [01:52, 21934.44it/s]1194944it [01:52, 21773.12it/s]1208874it [01:51, 22099.39it/s]1350200it [01:52, 21109.74it/s]1402079it [01:52, 22307.09it/s]1197123it [01:52, 21451.58it/s]1211086it [01:51, 22097.01it/s]1352393it [01:52, 21347.32it/s]1404394it [01:52, 22555.54it/s]1199270it [01:52, 21405.25it/s]1213297it [01:51, 22095.29it/s]1354546it [01:52, 21356.55it/s]1406650it [01:52, 22440.41it/s]1201445it [01:52, 21506.95it/s]1215572it [01:51, 22290.50it/s]1356735it [01:52, 21512.83it/s]1408991it [01:52, 22729.15it/s]1203689it [01:52, 21782.71it/s]1217802it [01:51, 22287.48it/s]1358896it [01:52, 21483.35it/s]1411348it [01:52, 22979.24it/s]1205903it [01:52, 21886.72it/s]1220031it [01:51, 21832.57it/s]1361070it [01:52, 21557.02it/s]1413647it [01:52, 22820.86it/s]1208143it [01:52, 22001.91it/s]1222217it [01:52, 21790.06it/s]1363231it [01:53, 21335.54it/s]1415930it [01:52, 22805.09it/s]1210387it [01:52, 22130.98it/s]1224398it [01:52, 21751.30it/s]1365369it [01:53, 21252.31it/s]1418214it [01:53, 22732.30it/s]1212601it [01:53, 22118.12it/s]1226627it [01:52, 21827.25it/s]1367526it [01:53, 21344.42it/s]1420494it [01:53, 22749.93it/s]1214814it [01:53, 21955.82it/s]1228843it [01:52, 21924.18it/s]1369663it [01:53, 21339.32it/s]1422770it [01:53, 22547.55it/s]1217028it [01:53, 22009.07it/s]1231037it [01:52, 21881.67it/s]1371817it [01:53, 21396.80it/s]1425026it [01:53, 22226.20it/s]1219230it [01:53, 21992.72it/s]1233252it [01:52, 21960.74it/s]1373960it [01:53, 21405.25it/s]1427250it [01:53, 21875.75it/s]1221430it [01:53, 21814.31it/s]1235449it [01:52, 21842.30it/s]1376102it [01:53, 21240.64it/s]1429440it [01:53, 21795.11it/s]1223612it [01:53, 21791.63it/s]1237634it [01:52, 21760.86it/s]1378294it [01:53, 21440.60it/s]1431642it [01:53, 21858.65it/s]1225798it [01:53, 21809.03it/s]1239811it [01:52, 21436.21it/s]1380487it [01:53, 21585.19it/s]1433829it [01:53, 21833.27it/s]1227997it [01:53, 21860.60it/s]1241956it [01:52, 21413.03it/s]1382720it [01:53, 21807.23it/s]1436013it [01:53, 21809.52it/s]1230184it [01:53, 21795.12it/s]1244126it [01:53, 21497.80it/s]1384909it [01:54, 21830.99it/s]1438228it [01:54, 21908.75it/s]1232409it [01:53, 21928.76it/s]1246277it [01:53, 21381.22it/s]1387093it [01:54, 21830.04it/s]1440420it [01:54, 21890.97it/s]1234603it [01:54, 21479.94it/s]1248431it [01:53, 21426.43it/s]1389277it [01:54, 21546.27it/s]1442610it [01:54, 21796.09it/s]1236754it [01:54, 21473.11it/s]1250580it [01:53, 21444.46it/s]1391433it [01:54, 21476.59it/s]1444804it [01:54, 21836.60it/s]1238941it [01:54, 21589.87it/s]1252775it [01:53, 21594.48it/s]1393619it [01:54, 21589.87it/s]1447005it [01:54, 21888.00it/s]1241102it [01:54, 21498.23it/s]1255047it [01:53, 21928.38it/s]1395779it [01:54, 21545.45it/s]1449194it [01:54, 21799.80it/s]1243265it [01:54, 21534.81it/s]1257241it [01:53, 21803.27it/s]1397946it [01:54, 21579.39it/s]1451426it [01:54, 21954.29it/s]1245420it [01:54, 21377.86it/s]1259422it [01:53, 21786.23it/s]1400105it [01:54, 21518.45it/s]1453709it [01:54, 22213.74it/s]1247559it [01:54, 21371.89it/s]1261653it [01:53, 21939.76it/s]1402274it [01:54, 21567.01it/s]1456014it [01:54, 22393.31it/s]1249697it [01:54, 21341.94it/s]1263848it [01:53, 21867.20it/s]1404572it [01:54, 21988.99it/s]1458309it [01:54, 22557.92it/s]1251897it [01:54, 21536.68it/s]1266035it [01:54, 21863.79it/s]1406848it [01:55, 22217.76it/s]1460568it [01:55, 22566.48it/s]1254051it [01:54, 21488.71it/s]1268222it [01:54, 21660.11it/s]1409134it [01:55, 22408.32it/s]1462825it [01:55, 22457.71it/s]1256286it [01:55, 21744.02it/s]1270389it [01:54, 21612.50it/s]1411443it [01:55, 22608.91it/s]1465114it [01:55, 22577.55it/s]1258530it [01:55, 21950.80it/s]1272582it [01:54, 21705.06it/s]1413705it [01:55, 22447.32it/s]1467372it [01:55, 22559.56it/s]1260726it [01:55, 21907.36it/s]1274810it [01:54, 21874.02it/s]1415951it [01:55, 22434.22it/s]1469629it [01:55, 22560.90it/s]1262917it [01:55, 21817.96it/s]1276998it [01:54, 21822.32it/s]1418199it [01:55, 22447.61it/s]1471886it [01:55, 22320.14it/s]1265107it [01:55, 21841.49it/s]1279195it [01:54, 21864.05it/s]1420444it [01:55, 22035.40it/s]1474119it [01:55, 22305.97it/s]1267292it [01:55, 21725.56it/s]1281415it [01:54, 21962.51it/s]1422650it [01:55, 21902.20it/s]1476351it [01:55, 22274.04it/s]1269465it [01:55, 21581.07it/s]1283657it [01:54, 22097.10it/s]1424842it [01:55, 21756.91it/s]1478656it [01:55, 22505.02it/s]1271642it [01:55, 21634.49it/s]1285867it [01:54, 22072.51it/s]1427019it [01:56, 21618.46it/s]1480956it [01:55, 22651.10it/s]1273806it [01:55, 21440.80it/s]1288075it [01:55, 21986.57it/s]1429182it [01:56, 21492.22it/s]1483276it [01:56, 22813.15it/s]1276083it [01:55, 21833.93it/s]1290353it [01:55, 22221.28it/s]1431342it [01:56, 21523.20it/s]1485595it [01:56, 22925.34it/s]1278299it [01:56, 21929.14it/s]1292576it [01:55, 22193.10it/s]1433495it [01:56, 21492.17it/s]1487934it [01:56, 23017.55it/s]1280517it [01:56, 22002.79it/s]1294796it [01:55, 22155.94it/s]1490262it [01:56, 23093.29it/s]1435645it [01:56, 21268.44it/s]1282739it [01:56, 22065.08it/s]1297012it [01:55, 21964.19it/s]1437782it [01:56, 21294.69it/s]1492572it [01:56, 22631.57it/s]1284946it [01:56, 22024.29it/s]1299219it [01:55, 21990.95it/s]1439953it [01:56, 21415.00it/s]1494848it [01:56, 22667.52it/s]1287149it [01:56, 21936.11it/s]1301419it [01:55, 21934.07it/s]1442095it [01:56, 21372.80it/s]1497161it [01:56, 22803.87it/s]1289382it [01:56, 22053.38it/s]1444243it [01:56, 21403.72it/s]1499443it [01:56, 22728.77it/s]1291588it [01:56, 21764.92it/s]1446400it [01:56, 21451.16it/s]1501724it [01:56, 22747.26it/s]1293803it [01:56, 21876.54it/s]1448585it [01:57, 21570.10it/s]1504000it [01:56, 22199.98it/s]1295997it [01:56, 21893.25it/s]1450743it [01:57, 21474.92it/s]1506224it [01:57, 22159.48it/s]1298213it [01:56, 21972.15it/s]1452974it [01:57, 21722.27it/s]1508443it [01:57, 22159.65it/s]1300427it [01:57, 22021.30it/s]1455210it [01:57, 21911.95it/s]1510661it [01:57, 22163.44it/s]1457470it [01:57, 22115.59it/s]1512902it [01:57, 22235.02it/s]1459705it [01:57, 22183.18it/s]1515127it [01:57, 21991.20it/s]1461924it [01:57, 22049.56it/s]1517360it [01:57, 22088.24it/s]1464149it [01:57, 22108.85it/s]1519570it [01:57, 22055.55it/s]1466361it [01:57, 22095.98it/s]1521777it [01:57, 22039.05it/s]1468571it [01:57, 21879.48it/s]1524004it [01:57, 22105.89it/s]1470786it [01:58, 21957.79it/s]1526215it [01:57, 21908.99it/s]1472983it [01:58, 21929.28it/s]1528407it [01:58, 21850.31it/s]1475186it [01:58, 21957.28it/s]1530612it [01:58, 21909.11it/s]1477382it [01:58, 21944.14it/s]1532804it [01:58, 21909.95it/s]1479625it [01:58, 22087.69it/s]1534996it [01:58, 21871.29it/s]1481916it [01:58, 22333.18it/s]1537184it [01:58, 21693.84it/s]1484150it [01:58, 22141.78it/s]1539359it [01:58, 21709.22it/s]1486417it [01:58, 22297.54it/s]1541554it [01:58, 21780.36it/s]1488719it [01:58, 22510.26it/s]1543733it [01:58, 21704.84it/s]1490971it [01:58, 22473.78it/s]1545904it [01:58, 21587.78it/s]1493219it [01:59, 22412.01it/s]1548100it [01:58, 21697.56it/s]1495492it [01:59, 22505.62it/s]1550270it [01:59, 21677.34it/s]1497743it [01:59, 22459.05it/s]1552519it [01:59, 21919.22it/s]1499990it [01:59, 22115.91it/s]1554740it [01:59, 22005.16it/s]1502213it [01:59, 22104.05it/s]1556941it [01:59, 21806.61it/s]1504425it [01:59, 21773.13it/s]1559223it [01:59, 22106.51it/s]1506604it [01:59, 21697.98it/s]1561459it [01:59, 22179.94it/s]1508818it [01:59, 21825.83it/s]1563686it [01:59, 22205.76it/s]1511002it [01:59, 21776.80it/s]1565973it [01:59, 22403.37it/s]1513181it [01:59, 21730.49it/s]1568224it [01:59, 22433.46it/s]1515430it [02:00, 21955.20it/s]1570485it [01:59, 22485.98it/s]1517626it [02:00, 21634.06it/s]1572776it [02:00, 22545.90it/s]1519791it [02:00, 21619.80it/s]1575086it [02:00, 22702.18it/s]1521954it [02:00, 21492.26it/s]1577357it [02:00, 22602.18it/s]1524105it [02:00, 21497.16it/s]1579618it [02:00, 22236.41it/s]1526291it [02:00, 21602.10it/s]1581883it [02:00, 22355.59it/s]1528452it [02:00, 21518.57it/s]1584128it [02:00, 22381.58it/s]1530615it [02:00, 21549.46it/s]1586368it [02:00, 22384.89it/s]1532771it [02:00, 21259.48it/s]1588614it [02:00, 22405.96it/s]1534955it [02:00, 21430.30it/s]1590857it [02:00, 22412.53it/s]1537112it [02:01, 21470.25it/s]1593099it [02:00, 22362.51it/s]1539260it [02:01, 21442.76it/s]1595382it [02:01, 22499.80it/s]1541417it [02:01, 21478.16it/s]1597643it [02:01, 22531.26it/s]1543566it [02:01, 21400.78it/s]1599897it [02:01, 22491.78it/s]1545707it [02:01, 21362.63it/s]1602147it [02:01, 22267.22it/s]1604404it [02:01, 22355.45it/s]1547844it [02:01, 21076.52it/s]1606656it [02:01, 22404.05it/s]1550002it [02:01, 21223.83it/s]1608897it [02:01, 22383.80it/s]1552164it [02:01, 21341.16it/s]1611196it [02:01, 22562.52it/s]1554362it [02:01, 21529.88it/s]1613463it [02:01, 22591.48it/s]1556574it [02:01, 21702.64it/s]1615723it [02:01, 22590.24it/s]1558779it [02:02, 21805.23it/s]1617984it [02:02, 22595.54it/s]1560996it [02:02, 21913.34it/s]1563188it [02:02, 21903.55it/s]1620244it [02:02, 22407.76it/s]1565379it [02:02, 21654.41it/s]1622486it [02:02, 21874.78it/s]1567606it [02:02, 21836.46it/s]1624709it [02:02, 21977.94it/s]1569817it [02:02, 21916.83it/s]1626909it [02:02, 21881.04it/s]1572045it [02:02, 22024.49it/s]1629099it [02:02, 21813.21it/s]1574287it [02:02, 22139.82it/s]1631361it [02:02, 22050.06it/s]1576502it [02:02, 22135.23it/s]1633587it [02:02, 22111.12it/s]1578716it [02:02, 22075.22it/s]1635839it [02:02, 22232.61it/s]1580924it [02:03, 21843.51it/s]1638075it [02:02, 22268.03it/s]1583130it [02:03, 21905.63it/s]1640303it [02:03, 22167.38it/s]1585367it [02:03, 22043.68it/s]1642521it [02:03, 22088.61it/s]1587572it [02:03, 21978.50it/s]1644738it [02:03, 22107.71it/s]1589771it [02:03, 21933.82it/s]1646949it [02:03, 22024.54it/s]1591991it [02:03, 22009.71it/s]1649218it [02:03, 22142.03it/s]1594193it [02:03, 22007.49it/s]1651499it [02:03, 22340.60it/s]1596405it [02:03, 22039.54it/s]1653737it [02:03, 22350.59it/s]1598610it [02:03, 21860.72it/s]1655991it [02:03, 22406.52it/s]1600822it [02:03, 21936.28it/s]1658232it [02:03, 22355.85it/s]1603032it [02:04, 21983.07it/s]1660480it [02:03, 22391.08it/s]1605268it [02:04, 22094.59it/s]1662720it [02:04, 22141.44it/s]1607483it [02:04, 22109.44it/s]1664935it [02:04, 22076.99it/s]1609712it [02:04, 22161.25it/s]1667144it [02:04, 21891.55it/s]1611929it [02:04, 22140.65it/s]1669334it [02:04, 21763.45it/s]1614144it [02:04, 21918.48it/s]1671511it [02:04, 21723.03it/s]1616402it [02:04, 22114.33it/s]1673684it [02:04, 21669.10it/s]1618614it [02:04, 22028.58it/s]1675875it [02:04, 21737.75it/s]1620818it [02:04, 21874.66it/s]1678049it [02:04, 21671.86it/s]1623006it [02:04, 21872.04it/s]1625194it [02:05, 21808.10it/s]1627376it [02:05, 21644.19it/s]1629541it [02:05, 21424.81it/s]1631745it [02:05, 21604.14it/s]1633974it [02:05, 21807.11it/s]1636190it [02:05, 21911.87it/s]1638382it [02:05, 21869.12it/s]1640574it [02:05, 21880.52it/s]1642763it [02:05, 21724.03it/s]1644978it [02:06, 21847.91it/s]1647164it [02:06, 21453.09it/s]1649367it [02:06, 21622.61it/s]1651609it [02:06, 21857.96it/s]1653797it [02:06, 21855.60it/s]1656006it [02:06, 21922.28it/s]1658264it [02:06, 22117.33it/s]1660477it [02:06, 22118.90it/s]1662690it [02:06, 21577.57it/s]1664852it [02:06, 21587.72it/s]1667013it [02:07, 21483.06it/s]1669173it [02:07, 21515.91it/s]1671326it [02:07, 21431.18it/s]1673483it [02:07, 21471.20it/s]1675631it [02:07, 21375.47it/s]1677769it [02:07, 21200.33it/s]1301419it [02:14, 21934.07it/s]1301459it [02:14, 275.97it/s]  1303751it [02:14, 446.24it/s]1306027it [02:14, 683.39it/s]1300427it [02:15, 22021.30it/s]1301454it [02:15, 333.44it/s]  1308319it [02:14, 1015.96it/s]1303712it [02:15, 503.40it/s]1310610it [02:14, 1473.95it/s]1305963it [02:15, 741.00it/s]1312876it [02:14, 2085.84it/s]1308237it [02:15, 1075.15it/s]1315112it [02:14, 2885.67it/s]1310525it [02:15, 1538.23it/s]1317314it [02:15, 3892.41it/s]1312781it [02:15, 2154.32it/s]1319463it [02:15, 5138.78it/s]1315007it [02:15, 2959.14it/s]1321727it [02:15, 6746.60it/s]1317203it [02:16, 3990.42it/s]1324024it [02:15, 8628.89it/s]1319394it [02:16, 5243.56it/s]1326264it [02:15, 10586.66it/s]1321650it [02:16, 6848.62it/s]1328548it [02:15, 12652.47it/s]1323946it [02:16, 8732.36it/s]1330818it [02:15, 14603.66it/s]1326186it [02:16, 10688.57it/s]1333068it [02:15, 16272.76it/s]1328449it [02:16, 12714.98it/s]1335309it [02:15, 17679.57it/s]1330713it [02:16, 14651.69it/s]1337543it [02:15, 18563.70it/s]1332954it [02:16, 16293.78it/s]1339767it [02:16, 19524.10it/s]1335186it [02:16, 17662.17it/s]1341971it [02:16, 20100.24it/s]1337437it [02:16, 18883.85it/s]1678049it [02:17, 21671.86it/s]1678688it [02:17, 457.12it/s]  1344162it [02:16, 20546.29it/s]1339668it [02:17, 19357.66it/s]1680890it [02:17, 699.50it/s]1346347it [02:16, 20624.74it/s]1341847it [02:17, 19965.94it/s]1683068it [02:17, 1032.50it/s]1348501it [02:16, 20809.69it/s]1344020it [02:17, 20414.29it/s]1685274it [02:17, 1496.69it/s]1350709it [02:16, 21172.93it/s]1346189it [02:17, 20646.65it/s]1687456it [02:17, 2117.01it/s]1352883it [02:16, 21336.40it/s]1348363it [02:17, 20958.29it/s]1689628it [02:17, 2936.89it/s]1355050it [02:16, 21051.16it/s]1350523it [02:17, 21087.20it/s]1691832it [02:17, 4013.93it/s]1357210it [02:16, 21209.08it/s]1352693it [02:17, 21250.86it/s]1694013it [02:17, 5340.21it/s]1359395it [02:16, 21396.29it/s]1354866it [02:17, 21389.78it/s]1696311it [02:17, 7037.63it/s]1361558it [02:17, 21463.52it/s]1357028it [02:17, 21432.17it/s]1698583it [02:18, 8932.73it/s]1363713it [02:17, 21477.24it/s]1359188it [02:18, 21087.87it/s]1700816it [02:18, 10864.03it/s]1365873it [02:17, 21511.55it/s]1361332it [02:18, 21189.27it/s]1703065it [02:18, 12877.95it/s]1368029it [02:17, 21467.77it/s]1363518it [02:18, 21386.96it/s]1705338it [02:18, 14832.19it/s]1370207it [02:17, 21560.08it/s]1365664it [02:18, 21262.47it/s]1707598it [02:18, 16543.57it/s]1372366it [02:17, 21494.46it/s]1709844it [02:18, 17951.61it/s]1367813it [02:18, 21278.48it/s]1374517it [02:17, 21099.06it/s]1369978it [02:18, 21387.67it/s]1712088it [02:18, 19015.89it/s]1376650it [02:17, 21165.28it/s]1372120it [02:18, 21373.14it/s]1714321it [02:18, 19556.97it/s]1378846it [02:17, 21398.79it/s]1374299it [02:18, 21497.24it/s]1716513it [02:18, 20087.02it/s]1381118it [02:17, 21786.19it/s]1376450it [02:18, 20929.82it/s]1718722it [02:18, 20644.19it/s]1383325it [02:18, 21867.89it/s]1378686it [02:18, 21346.88it/s]1720925it [02:19, 21037.09it/s]1385517it [02:18, 21879.94it/s]1380900it [02:19, 21579.19it/s]1723158it [02:19, 21410.69it/s]1387729it [02:18, 21951.08it/s]1383091it [02:19, 21675.32it/s]1725389it [02:19, 21672.96it/s]1389925it [02:18, 21854.36it/s]1385314it [02:19, 21838.83it/s]1727602it [02:19, 21525.23it/s]1392115it [02:18, 21865.11it/s]1387500it [02:19, 21777.11it/s]1729849it [02:19, 21801.10it/s]1394302it [02:18, 21326.72it/s]1389679it [02:19, 21713.16it/s]1732088it [02:19, 21973.38it/s]1396448it [02:18, 21356.64it/s]1391865it [02:19, 21756.53it/s]1734338it [02:19, 22126.34it/s]1398620it [02:18, 21463.33it/s]1394042it [02:19, 21713.54it/s]1736648it [02:19, 22318.12it/s]1400791it [02:18, 21535.94it/s]1738888it [02:19, 22241.64it/s]1396214it [02:19, 21240.47it/s]1403098it [02:18, 21984.37it/s]1398347it [02:19, 21263.76it/s]1741118it [02:19, 22104.37it/s]1405298it [02:19, 21955.50it/s]1400522it [02:19, 21407.17it/s]1743333it [02:20, 22053.86it/s]1407539it [02:19, 22088.64it/s]1402812it [02:20, 21850.53it/s]1745542it [02:20, 21946.77it/s]1409846it [02:19, 22380.42it/s]1677769it [02:20, 21200.33it/s]1678685it [02:20, 456.91it/s]  1405086it [02:20, 22115.25it/s]1747755it [02:20, 21999.34it/s]1412151it [02:19, 22579.32it/s]1680850it [02:20, 687.13it/s]1407379it [02:20, 22356.76it/s]1750039it [02:20, 22249.07it/s]1414410it [02:19, 22093.10it/s]1682984it [02:20, 1002.63it/s]1409712it [02:20, 22645.94it/s]1752266it [02:20, 22193.98it/s]1416670it [02:19, 22239.95it/s]1685149it [02:20, 1443.70it/s]1411980it [02:20, 22653.89it/s]1754505it [02:20, 22250.53it/s]1418933it [02:19, 22354.40it/s]1687321it [02:20, 2044.14it/s]1414246it [02:20, 22466.33it/s]1756759it [02:20, 22334.51it/s]1421170it [02:19, 22340.78it/s]1689491it [02:20, 2841.91it/s]1758993it [02:20, 22300.14it/s]1416494it [02:20, 22090.53it/s]1423406it [02:19, 22077.51it/s]1691647it [02:20, 3864.89it/s]1761224it [02:20, 22294.27it/s]1418707it [02:20, 22100.55it/s]1425616it [02:19, 21976.54it/s]1693794it [02:21, 5128.86it/s]1763485it [02:20, 22387.66it/s]1420919it [02:20, 22095.09it/s]1427815it [02:20, 21779.56it/s]1696032it [02:21, 6745.02it/s]1765724it [02:21, 22228.28it/s]1423130it [02:20, 22049.08it/s]1429994it [02:20, 21644.68it/s]1698267it [02:21, 8586.75it/s]1767973it [02:21, 22304.26it/s]1425336it [02:21, 21785.92it/s]1432160it [02:20, 21477.14it/s]1700514it [02:21, 10589.93it/s]1770204it [02:21, 22295.03it/s]1427516it [02:21, 21654.87it/s]1434309it [02:20, 21442.94it/s]1702718it [02:21, 12546.50it/s]1772434it [02:21, 22183.58it/s]1429683it [02:21, 21630.40it/s]1436497it [02:20, 21569.26it/s]1704934it [02:21, 14430.97it/s]1774653it [02:21, 22063.91it/s]1431847it [02:21, 21540.09it/s]1438655it [02:20, 21546.86it/s]1707183it [02:21, 16195.04it/s]1776860it [02:21, 21953.10it/s]1434022it [02:21, 21598.95it/s]1440833it [02:20, 21613.73it/s]1709402it [02:21, 17445.27it/s]1779056it [02:21, 21865.51it/s]1436183it [02:21, 21049.19it/s]1442995it [02:20, 21531.44it/s]1711594it [02:21, 18484.34it/s]1781243it [02:21, 21825.27it/s]1438360it [02:21, 21258.26it/s]1445149it [02:20, 21460.89it/s]1713774it [02:21, 19306.62it/s]1783431it [02:21, 21839.41it/s]1440509it [02:21, 21325.16it/s]1447376it [02:21, 21701.11it/s]1715947it [02:22, 19814.92it/s]1785616it [02:21, 21771.71it/s]1442644it [02:21, 21288.37it/s]1449547it [02:21, 21562.02it/s]1718102it [02:22, 20286.05it/s]1787799it [02:22, 21788.17it/s]1444805it [02:21, 21382.02it/s]1451802it [02:21, 21854.86it/s]1720269it [02:22, 20678.76it/s]1789978it [02:22, 21548.70it/s]1446963it [02:22, 21439.70it/s]1454066it [02:21, 22087.84it/s]1722434it [02:22, 20954.49it/s]1792134it [02:22, 21536.84it/s]1449173it [02:22, 21635.70it/s]1456294it [02:21, 22145.03it/s]1724594it [02:22, 20983.96it/s]1794302it [02:22, 21577.72it/s]1451366it [02:22, 21721.06it/s]1458538it [02:21, 22232.11it/s]1726761it [02:22, 21183.00it/s]1796468it [02:22, 21599.72it/s]1453539it [02:22, 21433.96it/s]1460811it [02:21, 22380.51it/s]1728947it [02:22, 21380.76it/s]1798652it [02:22, 21669.34it/s]1455817it [02:22, 21830.67it/s]1463050it [02:21, 22236.15it/s]1731130it [02:22, 21510.92it/s]1800820it [02:22, 21657.38it/s]1458044it [02:22, 21958.49it/s]1465274it [02:21, 21988.84it/s]1733343it [02:22, 21692.74it/s]1802986it [02:22, 21526.02it/s]1460271it [02:22, 22049.48it/s]1467539it [02:21, 22183.64it/s]1735597it [02:22, 21943.05it/s]1805154it [02:22, 21570.47it/s]1462490it [02:22, 22090.50it/s]1469759it [02:22, 22187.90it/s]1737830it [02:23, 22057.21it/s]1807312it [02:22, 21536.63it/s]1464706it [02:22, 22108.60it/s]1471979it [02:22, 22178.13it/s]1740042it [02:23, 21854.97it/s]1809482it [02:23, 21583.78it/s]1466934it [02:22, 22088.40it/s]1474219it [02:22, 22239.72it/s]1742232it [02:23, 21479.54it/s]1811646it [02:23, 21600.19it/s]1469189it [02:23, 22224.24it/s]1476444it [02:22, 22167.31it/s]1744385it [02:23, 21409.11it/s]1813807it [02:23, 21459.45it/s]1471412it [02:23, 22191.17it/s]1478700it [02:22, 22282.84it/s]1746541it [02:23, 21452.67it/s]1816005it [02:23, 21614.21it/s]1473632it [02:23, 21753.04it/s]1481020it [02:22, 22556.80it/s]1748765it [02:23, 21685.07it/s]1818264it [02:23, 21903.68it/s]1475819it [02:23, 21785.39it/s]1483276it [02:22, 22396.75it/s]1750994it [02:23, 21857.43it/s]1820488it [02:23, 22002.39it/s]1478043it [02:23, 21918.48it/s]1485569it [02:22, 22554.61it/s]1753268it [02:23, 22120.20it/s]1822689it [02:23, 21907.58it/s]1480344it [02:23, 22241.97it/s]1487906it [02:22, 22796.66it/s]1755482it [02:23, 22036.95it/s]1824881it [02:23, 21894.98it/s]1482614it [02:23, 22377.84it/s]1490187it [02:22, 22792.06it/s]1757687it [02:23, 21656.41it/s]1827090it [02:23, 21950.83it/s]1484902it [02:23, 22526.19it/s]1492467it [02:23, 22576.08it/s]1759897it [02:24, 21785.89it/s]1829328it [02:23, 22076.24it/s]1487251it [02:23, 22812.81it/s]1494726it [02:23, 22521.84it/s]1762096it [02:24, 21843.65it/s]1831607it [02:24, 22287.15it/s]1489613it [02:23, 23051.25it/s]1496992it [02:23, 22562.60it/s]1764333it [02:24, 21997.78it/s]1833859it [02:24, 22355.23it/s]1491919it [02:24, 22995.48it/s]1499249it [02:23, 22222.06it/s]1766543it [02:24, 22026.89it/s]1836122it [02:24, 22437.14it/s]1494219it [02:24, 22695.98it/s]1501485it [02:23, 22260.42it/s]1768747it [02:24, 21982.71it/s]1838366it [02:24, 22238.29it/s]1496515it [02:24, 22773.68it/s]1503712it [02:23, 22081.09it/s]1770946it [02:24, 21883.14it/s]1840591it [02:24, 21939.28it/s]1498816it [02:24, 22842.16it/s]1505921it [02:23, 21800.64it/s]1773135it [02:24, 21752.91it/s]1842786it [02:24, 21847.33it/s]1501103it [02:24, 22847.76it/s]1508103it [02:23, 21772.29it/s]1775311it [02:24, 21359.98it/s]1844985it [02:24, 21889.17it/s]1503389it [02:24, 22640.34it/s]1510297it [02:23, 21819.59it/s]1777488it [02:24, 21476.70it/s]1847175it [02:24, 21828.05it/s]1505654it [02:24, 22368.63it/s]1512480it [02:23, 21753.79it/s]1779638it [02:24, 21481.35it/s]1849379it [02:24, 21890.26it/s]1507905it [02:24, 22408.19it/s]1514705it [02:24, 21900.24it/s]1781796it [02:25, 21509.81it/s]1851617it [02:24, 22033.58it/s]1510147it [02:24, 22008.95it/s]1516896it [02:24, 21696.72it/s]1783948it [02:25, 21364.01it/s]1853821it [02:25, 21776.14it/s]1512362it [02:24, 22048.71it/s]1519067it [02:24, 21516.26it/s]1786091it [02:25, 21379.96it/s]1856014it [02:25, 21821.49it/s]1514632it [02:25, 22240.89it/s]1521241it [02:24, 21581.08it/s]1788230it [02:25, 21302.90it/s]1858258it [02:25, 22005.06it/s]1516905it [02:25, 22385.33it/s]1523400it [02:24, 21526.83it/s]1790361it [02:25, 21062.33it/s]1860485it [02:25, 22083.43it/s]1519145it [02:25, 22175.71it/s]1525576it [02:24, 21594.12it/s]1792473it [02:25, 21079.18it/s]1862718it [02:25, 22151.36it/s]1521364it [02:25, 22168.37it/s]1527736it [02:24, 21486.91it/s]1794604it [02:25, 21081.51it/s]1864934it [02:25, 21888.39it/s]1523582it [02:25, 21926.33it/s]1529885it [02:24, 21478.75it/s]1796759it [02:25, 21218.24it/s]1867124it [02:25, 21778.51it/s]1525776it [02:25, 21921.66it/s]1532034it [02:24, 21144.24it/s]1798882it [02:25, 21209.64it/s]1869329it [02:25, 21856.75it/s]1527990it [02:25, 21983.96it/s]1534175it [02:24, 21222.34it/s]1801052it [02:25, 21353.66it/s]1871516it [02:25, 21722.61it/s]1530189it [02:25, 21562.33it/s]1536352it [02:25, 21382.73it/s]1803188it [02:26, 21321.91it/s]1873689it [02:26, 21623.22it/s]1532401it [02:25, 21724.04it/s]1538493it [02:25, 21390.48it/s]1805321it [02:26, 21115.51it/s]1875852it [02:26, 21616.41it/s]1534607it [02:26, 21820.61it/s]1540648it [02:25, 21437.03it/s]1807436it [02:26, 21124.06it/s]1878014it [02:26, 21360.51it/s]1536791it [02:26, 21822.33it/s]1542793it [02:25, 21312.31it/s]1809562it [02:26, 21163.21it/s]1880168it [02:26, 21412.26it/s]1538990it [02:26, 21869.22it/s]1544925it [02:25, 21307.21it/s]1811691it [02:26, 21198.81it/s]1882326it [02:26, 21461.50it/s]1541178it [02:26, 21813.22it/s]1547057it [02:25, 21169.40it/s]1813824it [02:26, 21236.32it/s]1884488it [02:26, 21486.39it/s]1543373it [02:26, 21852.80it/s]1549175it [02:25, 21040.01it/s]1815996it [02:26, 21380.29it/s]1886649it [02:26, 21520.63it/s]1545559it [02:26, 21715.41it/s]1551289it [02:25, 21062.85it/s]1818221it [02:26, 21639.26it/s]1888813it [02:26, 21555.31it/s]1547731it [02:26, 21663.67it/s]1553459it [02:25, 21230.63it/s]1820405it [02:26, 21698.14it/s]1890969it [02:26, 21398.02it/s]1549898it [02:26, 21451.94it/s]1555662it [02:25, 21467.01it/s]1822575it [02:26, 21274.65it/s]1893125it [02:26, 21445.08it/s]1552101it [02:26, 21621.39it/s]1557840it [02:26, 21559.50it/s]1824747it [02:27, 21403.55it/s]1895277it [02:27, 21463.37it/s]1554339it [02:26, 21845.48it/s]1560048it [02:26, 21711.30it/s]1826966it [02:27, 21634.48it/s]1897432it [02:27, 21486.78it/s]1556592it [02:27, 22047.21it/s]1562226it [02:26, 21729.19it/s]1829169it [02:27, 21751.15it/s]1899581it [02:27, 21451.89it/s]1558835it [02:27, 22158.75it/s]1564400it [02:26, 21475.85it/s]1831435it [02:27, 22019.58it/s]1901727it [02:27, 21249.64it/s]1561052it [02:27, 22127.28it/s]1566596it [02:26, 21618.22it/s]1833638it [02:27, 21935.15it/s]1903981it [02:27, 21631.32it/s]1563313it [02:27, 22270.72it/s]1568796it [02:26, 21730.64it/s]1835873it [02:27, 22058.58it/s]1906199it [02:27, 21794.02it/s]1565573it [02:27, 22367.47it/s]1571049it [02:26, 21967.63it/s]1838080it [02:27, 21737.04it/s]1908379it [02:27, 21781.01it/s]1567836it [02:27, 22445.38it/s]1573269it [02:26, 22034.36it/s]1840256it [02:27, 21620.26it/s]1910558it [02:27, 21724.73it/s]1570081it [02:27, 22232.99it/s]1575494it [02:26, 22098.37it/s]1842425it [02:27, 21639.60it/s]1912770it [02:27, 21840.64it/s]1572361it [02:27, 22399.25it/s]1577715it [02:26, 22130.34it/s]1844590it [02:27, 21545.68it/s]1914955it [02:27, 21537.97it/s]1574602it [02:27, 21857.84it/s]1579929it [02:27, 22046.84it/s]1846746it [02:28, 21484.50it/s]1917143it [02:28, 21637.94it/s]1576872it [02:27, 22103.88it/s]1582134it [02:27, 21657.90it/s]1848919it [02:28, 21556.66it/s]1919308it [02:28, 21611.22it/s]1579086it [02:28, 22041.02it/s]1584376it [02:27, 21882.16it/s]1851086it [02:28, 21587.16it/s]1921470it [02:28, 21565.50it/s]1581292it [02:28, 22034.60it/s]1586566it [02:27, 21820.06it/s]1853245it [02:28, 21511.67it/s]1923653it [02:28, 21643.11it/s]1583522it [02:28, 22112.19it/s]1588749it [02:27, 21736.60it/s]1855410it [02:28, 21552.22it/s]1925859it [02:28, 21764.58it/s]1585735it [02:28, 22046.13it/s]1590983it [02:27, 21913.19it/s]1857604it [02:28, 21635.19it/s]1928061it [02:28, 21838.80it/s]1587941it [02:28, 21675.43it/s]1593175it [02:27, 21859.91it/s]1859827it [02:28, 21812.29it/s]1930314it [02:28, 22044.70it/s]1590111it [02:28, 21633.20it/s]1595408it [02:27, 21997.75it/s]1862026it [02:28, 21863.93it/s]1932538it [02:28, 22101.79it/s]1592276it [02:28, 21409.93it/s]1597609it [02:27, 21990.20it/s]1864213it [02:28, 21715.73it/s]1934749it [02:28, 22100.69it/s]1594419it [02:28, 21375.13it/s]1599809it [02:27, 21949.59it/s]1866385it [02:28, 21489.98it/s]1936960it [02:28, 21985.33it/s]1596558it [02:28, 21239.75it/s]1602005it [02:28, 21716.37it/s]1868535it [02:29, 21456.67it/s]1939159it [02:29, 21722.79it/s]1598783it [02:28, 21536.91it/s]1604195it [02:28, 21769.05it/s]1870682it [02:29, 21455.97it/s]1941358it [02:29, 21801.50it/s]1600972it [02:29, 21641.60it/s]1606380it [02:28, 21771.13it/s]1872828it [02:29, 21297.19it/s]1943539it [02:29, 21793.09it/s]1608595it [02:28, 21883.78it/s]1603137it [02:29, 21387.20it/s]1874959it [02:29, 20795.39it/s]1945769it [02:29, 21943.91it/s]1610796it [02:28, 21919.70it/s]1605368it [02:29, 21656.90it/s]1877051it [02:29, 20831.05it/s]1947973it [02:29, 21969.69it/s]1613033it [02:28, 22052.39it/s]1607535it [02:29, 21404.30it/s]1879181it [02:29, 20969.01it/s]1950171it [02:29, 21910.82it/s]1615239it [02:28, 22000.69it/s]1609736it [02:29, 21471.42it/s]1881296it [02:29, 21021.11it/s]1952363it [02:29, 21837.67it/s]1617440it [02:28, 21975.53it/s]1611884it [02:29, 21389.37it/s]1883465it [02:29, 21218.52it/s]1954547it [02:29, 21821.84it/s]1619638it [02:28, 21611.86it/s]1614085it [02:29, 21570.64it/s]1885588it [02:29, 21151.75it/s]1956730it [02:29, 21760.17it/s]1621801it [02:28, 21429.62it/s]1616249it [02:29, 21496.14it/s]1887718it [02:29, 21195.66it/s]1958929it [02:29, 21826.42it/s]1623958it [02:29, 21469.49it/s]1618400it [02:29, 21339.09it/s]1889839it [02:30, 20475.42it/s]1961112it [02:30, 21823.64it/s]1626106it [02:29, 21373.54it/s]1620551it [02:29, 21387.06it/s]1892003it [02:30, 20813.64it/s]1963339it [02:30, 21956.70it/s]1628244it [02:29, 21338.32it/s]1622691it [02:30, 21049.41it/s]1894098it [02:30, 20852.52it/s]1965535it [02:30, 21756.27it/s]1630439it [02:29, 21517.13it/s]1624856it [02:30, 21136.20it/s]1896196it [02:30, 20889.39it/s]1967737it [02:30, 21831.90it/s]1632616it [02:29, 21590.65it/s]1626971it [02:30, 20962.85it/s]1898300it [02:30, 20931.05it/s]1970128it [02:30, 22449.93it/s]1634840it [02:29, 21783.01it/s]1629074it [02:30, 20981.18it/s]1900444it [02:30, 21005.33it/s]1972495it [02:30, 22814.09it/s]1637040it [02:29, 21844.45it/s]1631204it [02:30, 21074.16it/s]1902666it [02:30, 21364.61it/s]1974798it [02:30, 22876.44it/s]1639225it [02:29, 21383.17it/s]1633341it [02:30, 20940.40it/s]1904868it [02:30, 21558.36it/s]1977087it [02:30, 22520.68it/s]1641396it [02:29, 21479.22it/s]1635566it [02:30, 21324.39it/s]1907044it [02:30, 21618.18it/s]1979341it [02:30, 22325.28it/s]1643546it [02:29, 21451.22it/s]1637728it [02:30, 21410.61it/s]1909207it [02:31, 21215.72it/s]1981575it [02:30, 22109.62it/s]1645737it [02:30, 21585.58it/s]1639870it [02:30, 20873.53it/s]1911364it [02:31, 21252.36it/s]1983787it [02:31, 21983.45it/s]1647897it [02:30, 21565.63it/s]1642076it [02:30, 21212.79it/s]1913541it [02:31, 21404.45it/s]1985987it [02:31, 21966.89it/s]1650108it [02:30, 21726.82it/s]1644201it [02:31, 21207.66it/s]1915694it [02:31, 21441.43it/s]1988195it [02:31, 22000.04it/s]1652325it [02:30, 21858.37it/s]1646448it [02:31, 21579.99it/s]1917851it [02:31, 21477.64it/s]1990396it [02:31, 21905.41it/s]1654530it [02:30, 21913.13it/s]1648608it [02:31, 21549.31it/s]1920000it [02:31, 21445.45it/s]1992684it [02:31, 22192.58it/s]1656759it [02:30, 22023.96it/s]1650800it [02:31, 21658.06it/s]1922145it [02:31, 21281.29it/s]1994917it [02:31, 22233.17it/s]1658962it [02:30, 21792.59it/s]1653061it [02:31, 21940.83it/s]1924282it [02:31, 21306.70it/s]1997167it [02:31, 22309.37it/s]1661142it [02:30, 21753.90it/s]1655338it [02:31, 22187.12it/s]1926485it [02:31, 21475.79it/s]1999464it [02:31, 22506.56it/s]1663318it [02:30, 21687.07it/s]1657558it [02:31, 21864.13it/s]1928734it [02:31, 21775.52it/s]2001746it [02:31, 22599.62it/s]1665488it [02:31, 21466.29it/s]1659836it [02:31, 22134.33it/s]1930958it [02:32, 21913.02it/s]2004007it [02:31, 22505.89it/s]1667636it [02:31, 21460.40it/s]1933195it [02:32, 22049.36it/s]1662051it [02:31, 20829.46it/s]2006296it [02:32, 22618.49it/s]1669783it [02:31, 21375.73it/s]1935401it [02:32, 21671.29it/s]1664265it [02:32, 21202.92it/s]2008653it [02:32, 22902.53it/s]1671921it [02:31, 21265.05it/s]1937570it [02:32, 21650.47it/s]1666421it [02:32, 21304.22it/s]2010944it [02:32, 22900.64it/s]1674057it [02:31, 21292.20it/s]1939758it [02:32, 21717.37it/s]1668594it [02:32, 21428.40it/s]2013235it [02:32, 22805.99it/s]1676187it [02:31, 21133.92it/s]1941931it [02:32, 21635.73it/s]1670797it [02:32, 21604.18it/s]2015516it [02:32, 22752.92it/s]1678301it [02:31, 20891.43it/s]1944120it [02:32, 21709.50it/s]1672963it [02:32, 21572.65it/s]2017792it [02:32, 22427.54it/s]1946292it [02:32, 21505.58it/s]1675124it [02:32, 21406.59it/s]2020044it [02:32, 22454.11it/s]1948479it [02:32, 21610.82it/s]1677277it [02:32, 21441.90it/s]2022291it [02:32, 22186.82it/s]1950661it [02:32, 21671.91it/s]2024511it [02:32, 22086.52it/s]1952842it [02:33, 21712.19it/s]2026721it [02:32, 22012.82it/s]1955041it [02:33, 21794.76it/s]2028923it [02:33, 21755.61it/s]1957221it [02:33, 21481.82it/s]2031100it [02:33, 21709.95it/s]1959371it [02:33, 21441.89it/s]2033272it [02:33, 21675.58it/s]1961562it [02:33, 21580.38it/s]2035571it [02:33, 22063.49it/s]1963721it [02:33, 21569.22it/s]2037895it [02:33, 22411.61it/s]1965934it [02:33, 21733.39it/s]2040212it [02:33, 22636.18it/s]1968108it [02:33, 21614.77it/s]2042522it [02:33, 22774.23it/s]1970431it [02:33, 22095.48it/s]2044800it [02:33, 22405.99it/s]1972776it [02:33, 22499.09it/s]2047043it [02:33, 22209.11it/s]1975079it [02:34, 22655.01it/s]2049266it [02:33, 22095.86it/s]1977345it [02:34, 22479.36it/s]2051477it [02:34, 22091.50it/s]1979594it [02:34, 22184.65it/s]2053687it [02:34, 21910.22it/s]1981814it [02:34, 21782.54it/s]2055902it [02:34, 21979.10it/s]1983995it [02:34, 21656.18it/s]2058245it [02:34, 22407.70it/s]1986162it [02:34, 21530.29it/s]2060487it [02:34, 22372.75it/s]1988358it [02:34, 21655.25it/s]2062774it [02:34, 22520.73it/s]1990547it [02:34, 21724.45it/s]2065090it [02:34, 22644.85it/s]1992726it [02:34, 21741.58it/s]2067355it [02:34, 22389.75it/s]1994925it [02:34, 21813.02it/s]2069595it [02:34, 22095.85it/s]1997124it [02:35, 21863.93it/s]2071806it [02:35, 21917.28it/s]1999373it [02:35, 22049.76it/s]2073999it [02:35, 21880.62it/s]2001622it [02:35, 22179.47it/s]2076188it [02:35, 21571.08it/s]2003841it [02:35, 22099.60it/s]2078393it [02:35, 21710.47it/s]2006120it [02:35, 22304.55it/s]2080569it [02:35, 21721.40it/s]2008396it [02:35, 22440.02it/s]2082800it [02:35, 21892.64it/s]2010646it [02:35, 22455.34it/s]2085056it [02:35, 22088.91it/s]2012921it [02:35, 22540.99it/s]2087303it [02:35, 22200.31it/s]2015176it [02:35, 22339.92it/s]2089524it [02:35, 22092.12it/s]2017411it [02:35, 22071.51it/s]2091760it [02:35, 22165.12it/s]2019666it [02:36, 22212.12it/s]2093977it [02:36, 21962.20it/s]2021889it [02:36, 21960.73it/s]2096174it [02:36, 21879.36it/s]2024087it [02:36, 21724.37it/s]2098410it [02:36, 22020.21it/s]2026269it [02:36, 21750.48it/s]2100646it [02:36, 22118.76it/s]2028445it [02:36, 21553.48it/s]2102890it [02:36, 22169.16it/s]2030602it [02:36, 21497.17it/s]2105108it [02:36, 22011.80it/s]2032753it [02:36, 21454.47it/s]2107310it [02:36, 21976.43it/s]2034967it [02:36, 21657.44it/s]2109509it [02:36, 21978.58it/s]2037270it [02:36, 22065.42it/s]2111708it [02:36, 21955.93it/s]2039489it [02:36, 22100.95it/s]2113972it [02:36, 22158.86it/s]2041745it [02:37, 22235.75it/s]2116189it [02:37, 22125.11it/s]2044005it [02:37, 22341.67it/s]2118475it [02:37, 22342.52it/s]2046240it [02:37, 21987.67it/s]2120760it [02:37, 22491.82it/s]2048441it [02:37, 21780.96it/s]2123041it [02:37, 22584.56it/s]2050621it [02:37, 21572.89it/s]2125300it [02:37, 22248.84it/s]2052780it [02:37, 21469.74it/s]2127527it [02:37, 22075.83it/s]2054930it [02:37, 21477.82it/s]2129736it [02:37, 21820.52it/s]2057189it [02:37, 21806.79it/s]2131920it [02:37, 21770.21it/s]2059462it [02:37, 22078.94it/s]2134098it [02:37, 21757.80it/s]2061740it [02:37, 22285.99it/s]2136318it [02:37, 21886.31it/s]2063970it [02:38, 22224.32it/s]2138508it [02:38, 21817.69it/s]2066193it [02:38, 22153.78it/s]2140691it [02:38, 21760.86it/s]2068409it [02:38, 21928.16it/s]2142868it [02:38, 21603.36it/s]2070603it [02:38, 21675.64it/s]2145044it [02:38, 21647.04it/s]2072772it [02:38, 21537.37it/s]2147241it [02:38, 21742.78it/s]2074927it [02:38, 21442.17it/s]2149416it [02:38, 21669.85it/s]2077072it [02:38, 21416.90it/s]2079261it [02:38, 21555.36it/s]2081434it [02:38, 21606.49it/s]2083635it [02:39, 21722.59it/s]2085808it [02:39, 21688.01it/s]2088042it [02:39, 21880.17it/s]2090285it [02:39, 22018.02it/s]2092487it [02:39, 21887.72it/s]2094677it [02:39, 21679.95it/s]2096846it [02:39, 21539.86it/s]2099021it [02:39, 21600.75it/s]2101218it [02:39, 21707.49it/s]2103441it [02:39, 21860.25it/s]2105628it [02:40, 21816.27it/s]2107821it [02:40, 21848.09it/s]2110006it [02:40, 21542.04it/s]2112195it [02:40, 21638.65it/s]2114450it [02:40, 21907.64it/s]2116647it [02:40, 21924.64it/s]2118923it [02:40, 22172.73it/s]2121141it [02:40, 22124.44it/s]2123362it [02:40, 22147.76it/s]2125578it [02:40, 21931.94it/s]2127772it [02:41, 21869.04it/s]2129960it [02:41, 21731.62it/s]2132134it [02:41, 21540.28it/s]2134289it [02:41, 21506.68it/s]2136468it [02:41, 21589.56it/s]2138628it [02:41, 21543.28it/s]2140786it [02:41, 21551.56it/s]2142964it [02:41, 21619.28it/s]2145127it [02:41, 21450.12it/s]2147299it [02:41, 21529.48it/s]2149453it [02:42, 21448.82it/s]1677277it [02:51, 21441.90it/s]1678686it [02:51, 334.44it/s]  1680910it [02:51, 494.83it/s]1683081it [02:51, 714.44it/s]1678301it [02:51, 20891.43it/s]1678691it [02:51, 270.07it/s]  1685251it [02:52, 1019.95it/s]1680897it [02:51, 425.42it/s]1687468it [02:52, 1451.13it/s]1683067it [02:51, 639.92it/s]1689660it [02:52, 2030.38it/s]1685299it [02:51, 947.75it/s]1691889it [02:52, 2817.57it/s]1687466it [02:51, 1358.32it/s]1694083it [02:52, 3816.26it/s]1689668it [02:51, 1928.33it/s]1696373it [02:52, 5147.46it/s]1698699it [02:52, 6788.68it/s]1691823it [02:51, 2666.87it/s]1694051it [02:51, 3672.68it/s]1700942it [02:52, 8542.30it/s]1696205it [02:52, 4890.53it/s]1703199it [02:52, 10507.60it/s]1698500it [02:52, 6494.43it/s]1705513it [02:52, 12614.46it/s]1700771it [02:52, 8319.97it/s]1707768it [02:53, 14423.70it/s]1710020it [02:53, 16159.20it/s]1702988it [02:52, 10180.49it/s]1705258it [02:52, 12238.42it/s]1712257it [02:53, 17566.62it/s]1714486it [02:53, 18414.47it/s]1707472it [02:52, 13899.37it/s]1709763it [02:52, 15799.59it/s]1716670it [02:53, 19201.53it/s]1711964it [02:52, 17209.88it/s]1718897it [02:53, 20026.96it/s]1721097it [02:53, 20575.18it/s]1714161it [02:52, 18320.60it/s]1723288it [02:53, 20734.39it/s]1716348it [02:53, 18956.08it/s]1725574it [02:53, 21343.48it/s]1718526it [02:53, 19714.03it/s]1720702it [02:53, 20280.15it/s]1727777it [02:53, 21363.06it/s]1722957it [02:53, 20923.81it/s]1729996it [02:54, 21594.72it/s]1732243it [02:54, 21850.64it/s]1725148it [02:53, 21078.88it/s]1734528it [02:54, 22145.58it/s]1727399it [02:53, 21493.17it/s]1736761it [02:54, 22157.18it/s]1729619it [02:53, 21699.34it/s]1739035it [02:54, 22327.91it/s]1731825it [02:53, 21331.16it/s]1741277it [02:54, 22036.24it/s]1734128it [02:53, 21825.36it/s]2149416it [02:54, 21669.85it/s]2150001it [02:54, 345.36it/s]  1743488it [02:54, 21946.18it/s]1736403it [02:53, 22096.15it/s]2152195it [02:54, 531.16it/s]1738661it [02:54, 22238.54it/s]1745688it [02:54, 21776.72it/s]2154371it [02:54, 788.84it/s]1747914it [02:54, 21918.49it/s]1740895it [02:54, 22112.68it/s]2156576it [02:55, 1151.30it/s]1750109it [02:54, 21905.47it/s]1743114it [02:54, 21564.79it/s]2158757it [02:55, 1640.23it/s]1752432it [02:55, 22297.66it/s]1745288it [02:54, 21613.84it/s]2160928it [02:55, 2298.61it/s]1754664it [02:55, 22170.80it/s]1747544it [02:54, 21890.37it/s]2163137it [02:55, 3184.58it/s]1756883it [02:55, 22110.84it/s]1749738it [02:54, 21838.50it/s]2165307it [02:55, 4296.91it/s]1759095it [02:55, 22054.32it/s]1752030it [02:54, 22157.08it/s]2167497it [02:55, 5688.71it/s]1761340it [02:55, 22170.12it/s]1754296it [02:54, 22304.60it/s]2169672it [02:55, 7315.63it/s]1763558it [02:55, 22059.77it/s]1756566it [02:54, 22419.04it/s]2171846it [02:55, 9141.02it/s]1765847it [02:55, 22304.77it/s]1758810it [02:54, 20797.56it/s]2174019it [02:55, 11007.15it/s]1768078it [02:55, 22136.24it/s]1761042it [02:55, 21228.77it/s]2176188it [02:55, 12915.36it/s]1770293it [02:55, 21453.27it/s]1763314it [02:55, 21657.90it/s]2178390it [02:56, 14765.20it/s]1772443it [02:56, 21295.39it/s]1765561it [02:55, 21891.88it/s]2180563it [02:56, 16332.59it/s]1774582it [02:56, 21321.54it/s]1767822it [02:55, 22100.99it/s]2182841it [02:56, 17900.82it/s]1776778it [02:56, 21509.12it/s]1770060it [02:55, 22182.01it/s]2185209it [02:56, 19389.15it/s]1778931it [02:56, 21453.54it/s]1772285it [02:55, 22129.69it/s]2187514it [02:56, 20372.14it/s]1781078it [02:56, 21246.24it/s]1774503it [02:55, 22032.79it/s]2189782it [02:56, 20851.37it/s]1783204it [02:56, 21215.23it/s]1776746it [02:55, 22147.79it/s]2192032it [02:56, 21163.30it/s]1785363it [02:56, 21323.67it/s]1778963it [02:55, 21672.38it/s]2194265it [02:56, 21477.65it/s]1787497it [02:56, 20923.62it/s]1781170it [02:55, 21684.54it/s]2196496it [02:56, 21477.14it/s]1789627it [02:56, 21033.94it/s]1783350it [02:56, 21714.26it/s]2198708it [02:56, 21662.74it/s]1791732it [02:56, 20759.22it/s]1785524it [02:56, 21362.07it/s]2200916it [02:57, 21674.89it/s]1793841it [02:57, 20853.66it/s]1787695it [02:56, 21462.73it/s]2203113it [02:57, 21696.62it/s]1796009it [02:57, 21095.26it/s]1789851it [02:56, 21486.35it/s]2205303it [02:57, 21481.95it/s]1798120it [02:57, 21027.91it/s]1792057it [02:56, 21655.97it/s]2207542it [02:57, 21746.77it/s]1800280it [02:57, 21197.02it/s]1794224it [02:56, 21312.29it/s]2209820it [02:57, 22050.27it/s]1802480it [02:57, 21435.45it/s]1796381it [02:56, 21387.29it/s]2212137it [02:57, 22381.39it/s]1804659it [02:57, 21540.70it/s]1798571it [02:56, 21538.38it/s]2214409it [02:57, 22479.92it/s]1806814it [02:57, 21406.38it/s]1800726it [02:56, 21245.01it/s]2216668it [02:57, 22512.37it/s]1808956it [02:57, 21340.93it/s]1802939it [02:56, 21505.76it/s]2218939it [02:57, 22569.93it/s]1811103it [02:57, 21378.05it/s]1805105it [02:57, 21551.06it/s]2149453it [02:58, 21448.82it/s]2149996it [02:58, 343.73it/s]  2221199it [02:58, 22279.96it/s]1813242it [02:57, 21204.53it/s]1807262it [02:57, 21309.70it/s]2152165it [02:58, 529.41it/s]2223457it [02:58, 22368.24it/s]1815431it [02:58, 21405.84it/s]1809447it [02:57, 21468.39it/s]2154318it [02:58, 787.17it/s]2225696it [02:58, 22360.17it/s]1817659it [02:58, 21663.94it/s]1811626it [02:57, 21561.52it/s]2156470it [02:58, 1144.17it/s]2227961it [02:58, 22445.83it/s]1819826it [02:58, 21630.34it/s]2158613it [02:58, 1631.31it/s]1813783it [02:57, 21297.25it/s]2230207it [02:58, 22319.17it/s]1821990it [02:58, 21608.62it/s]2160796it [02:58, 2301.36it/s]1815989it [02:57, 21521.50it/s]2232440it [02:58, 22134.40it/s]1824152it [02:58, 21518.51it/s]2162950it [02:58, 3171.79it/s]1818253it [02:57, 21852.59it/s]2234655it [02:58, 22104.08it/s]1826358it [02:58, 21678.84it/s]1820491it [02:57, 22009.40it/s]2165115it [02:58, 4291.09it/s]2236866it [02:58, 21803.06it/s]1828564it [02:58, 21789.86it/s]2167266it [02:58, 5650.75it/s]1822693it [02:57, 21647.43it/s]2239069it [02:58, 21868.23it/s]1830779it [02:58, 21895.81it/s]2169398it [02:58, 7248.56it/s]1824868it [02:57, 21677.37it/s]2241307it [02:58, 22019.43it/s]1833065it [02:58, 22026.62it/s]2171582it [02:59, 9104.78it/s]1827163it [02:58, 22052.67it/s]2243527it [02:59, 22069.13it/s]1835338it [02:58, 22235.92it/s]2173729it [02:59, 10941.06it/s]1829370it [02:58, 21872.16it/s]2245840it [02:59, 22383.59it/s]1837562it [02:59, 21522.60it/s]2175930it [02:59, 12925.07it/s]1831652it [02:58, 22152.57it/s]2248180it [02:59, 22685.75it/s]1839720it [02:59, 21082.94it/s]2178076it [02:59, 14657.30it/s]1833919it [02:58, 22304.10it/s]2250480it [02:59, 22777.16it/s]1841833it [02:59, 20961.59it/s]1836151it [02:58, 22183.90it/s]2180221it [02:59, 16067.70it/s]2252759it [02:59, 22575.58it/s]1843960it [02:59, 21050.02it/s]2182345it [02:59, 17297.67it/s]1838371it [02:58, 22055.84it/s]2255049it [02:59, 22669.43it/s]1846100it [02:59, 21150.74it/s]2184613it [02:59, 18679.19it/s]1840578it [02:58, 21850.98it/s]2257363it [02:59, 22806.71it/s]1848292it [02:59, 21375.66it/s]2186866it [02:59, 19712.34it/s]1842764it [02:58, 21796.56it/s]2259645it [02:59, 22772.37it/s]1850457it [02:59, 21455.34it/s]2189114it [02:59, 20422.89it/s]1844967it [02:58, 21862.70it/s]2261923it [02:59, 22768.20it/s]1852650it [02:59, 21595.69it/s]2191328it [02:59, 20907.51it/s]1847154it [02:59, 21615.67it/s]2264201it [02:59, 22679.27it/s]1854834it [02:59, 21667.10it/s]2193534it [03:00, 20992.57it/s]1849353it [02:59, 21725.94it/s]2266470it [03:00, 22478.17it/s]1857002it [02:59, 21614.47it/s]1851531it [02:59, 21739.39it/s]2195714it [03:00, 20737.31it/s]2268719it [03:00, 21978.69it/s]1859274it [03:00, 21942.90it/s]1853747it [02:59, 21862.95it/s]2197876it [03:00, 20915.38it/s]2270962it [03:00, 22109.83it/s]1861539it [03:00, 22153.35it/s]1855934it [02:59, 21732.60it/s]2200008it [03:00, 20844.16it/s]2273269it [03:00, 22391.83it/s]1863755it [03:00, 21906.16it/s]1858172it [02:59, 21916.37it/s]2202140it [03:00, 20981.33it/s]2275511it [03:00, 22383.08it/s]1865974it [03:00, 21988.96it/s]1860365it [02:59, 21867.90it/s]2204259it [03:00, 20968.38it/s]2277755it [03:00, 22397.05it/s]1868174it [03:00, 21719.40it/s]1862615it [02:59, 22053.22it/s]2206424it [03:00, 21168.97it/s]2279996it [03:00, 22345.09it/s]1870363it [03:00, 21767.17it/s]1864821it [02:59, 21767.78it/s]2208694it [03:00, 21622.20it/s]2282232it [03:00, 22174.34it/s]1872541it [03:00, 21515.12it/s]1866999it [02:59, 21691.97it/s]2210864it [03:00, 21499.77it/s]2284451it [03:00, 21843.15it/s]1874694it [03:00, 21264.55it/s]1869169it [03:00, 21617.56it/s]2213095it [03:01, 21739.30it/s]2286761it [03:00, 22211.69it/s]1876905it [03:00, 21512.84it/s]1871332it [03:00, 21547.20it/s]2215273it [03:01, 21722.33it/s]2288984it [03:01, 21849.44it/s]1879075it [03:00, 21567.72it/s]2217510it [03:01, 21912.81it/s]1873488it [03:00, 21413.14it/s]2291171it [03:01, 21281.72it/s]1881233it [03:01, 21352.05it/s]2219704it [03:01, 21871.90it/s]1875630it [03:00, 21387.00it/s]2293303it [03:01, 21008.42it/s]1883469it [03:01, 21648.68it/s]2221913it [03:01, 21934.19it/s]1877769it [03:00, 21296.75it/s]2295407it [03:01, 20652.89it/s]1885635it [03:01, 21359.73it/s]2224132it [03:01, 22007.97it/s]1879903it [03:00, 21308.55it/s]2297475it [03:01, 20423.36it/s]1887842it [03:01, 21566.34it/s]2226334it [03:01, 21780.93it/s]1882034it [03:00, 21143.99it/s]2299519it [03:01, 20394.43it/s]1890000it [03:01, 21366.19it/s]2228537it [03:01, 21852.38it/s]1884234it [03:00, 21395.40it/s]2301560it [03:01, 20101.20it/s]1892187it [03:01, 21512.16it/s]1886374it [03:00, 21159.40it/s]2230724it [03:01, 21519.89it/s]2303578it [03:01, 20123.52it/s]1894373it [03:01, 21615.06it/s]1888568it [03:00, 21387.78it/s]2232904it [03:01, 21599.20it/s]2305592it [03:01, 20102.16it/s]1896536it [03:01, 21255.54it/s]1890708it [03:01, 21204.89it/s]2235066it [03:02, 21268.64it/s]2307603it [03:01, 19889.28it/s]1898664it [03:01, 21258.48it/s]1892891it [03:01, 21300.45it/s]2237267it [03:02, 21484.39it/s]2309601it [03:02, 19913.87it/s]1900792it [03:01, 21118.27it/s]2239440it [03:02, 21555.02it/s]1895022it [03:01, 21111.94it/s]2311593it [03:02, 19873.12it/s]1903063it [03:02, 21586.34it/s]1897134it [03:01, 21062.16it/s]2241597it [03:02, 21311.41it/s]2313600it [03:02, 19930.21it/s]1905232it [03:02, 21614.91it/s]1899252it [03:01, 21093.71it/s]2243819it [03:02, 21577.84it/s]2315630it [03:02, 20038.83it/s]1907433it [03:02, 21731.77it/s]1901424it [03:01, 21278.43it/s]2245984it [03:02, 21597.79it/s]2317635it [03:02, 20037.63it/s]1909650it [03:02, 21859.70it/s]1903602it [03:01, 21421.06it/s]2248277it [03:02, 21937.05it/s]2319663it [03:02, 20109.13it/s]1911837it [03:02, 21573.27it/s]1905793it [03:01, 21566.61it/s]2250485it [03:02, 21977.52it/s]2321675it [03:02, 20006.48it/s]1914022it [03:02, 21654.60it/s]1907950it [03:01, 21480.22it/s]2252776it [03:02, 22254.84it/s]2323676it [03:02, 19864.49it/s]1916189it [03:02, 21561.26it/s]1910099it [03:01, 21471.44it/s]2255027it [03:02, 22330.30it/s]2325663it [03:02, 19665.96it/s]1918368it [03:02, 21628.95it/s]2257261it [03:03, 22185.02it/s]1912247it [03:02, 21271.52it/s]2327698it [03:02, 19865.94it/s]1920532it [03:02, 21464.96it/s]2259504it [03:03, 22255.44it/s]1914435it [03:02, 21452.01it/s]2329692it [03:03, 19887.12it/s]1922692it [03:03, 21503.45it/s]2261730it [03:03, 22019.34it/s]1916581it [03:02, 21203.69it/s]2331693it [03:03, 19920.63it/s]1924922it [03:03, 21740.01it/s]2263945it [03:03, 22057.02it/s]1918763it [03:02, 21383.75it/s]2333686it [03:03, 19485.05it/s]1927097it [03:03, 21722.40it/s]1920903it [03:02, 21223.96it/s]2266152it [03:03, 21632.33it/s]2335644it [03:03, 19512.46it/s]1929353it [03:03, 21970.11it/s]1923050it [03:02, 21295.70it/s]2268330it [03:03, 21675.45it/s]2337604it [03:03, 19530.49it/s]1931551it [03:03, 21860.41it/s]1925181it [03:02, 21282.59it/s]2270531it [03:03, 21513.57it/s]2339651it [03:03, 19806.95it/s]1933843it [03:03, 22174.13it/s]1927407it [03:02, 21572.24it/s]2272778it [03:03, 21793.19it/s]2341633it [03:03, 19660.82it/s]1936061it [03:03, 21813.59it/s]1929687it [03:02, 21938.17it/s]2275049it [03:03, 22064.01it/s]2343703it [03:03, 19968.60it/s]1938271it [03:03, 21896.30it/s]1931882it [03:02, 21772.14it/s]2277257it [03:03, 21806.70it/s]2345701it [03:03, 19815.11it/s]1940518it [03:03, 22066.14it/s]1934098it [03:03, 21885.98it/s]2279440it [03:04, 21767.87it/s]2347684it [03:04, 19690.04it/s]1942726it [03:03, 21779.05it/s]1936288it [03:03, 21695.19it/s]2281618it [03:04, 21545.56it/s]2349760it [03:04, 20006.32it/s]1944908it [03:04, 21790.19it/s]1938462it [03:03, 21702.36it/s]2283791it [03:04, 21599.75it/s]2351762it [03:04, 19787.92it/s]1947088it [03:04, 21723.72it/s]1940633it [03:03, 21574.45it/s]2285973it [03:04, 21662.68it/s]2353845it [03:04, 20095.39it/s]1949262it [03:04, 21621.28it/s]1942802it [03:03, 21601.09it/s]2288140it [03:04, 21664.59it/s]2355856it [03:04, 20056.38it/s]1951425it [03:04, 21616.85it/s]1944965it [03:03, 21608.24it/s]2290307it [03:04, 20993.45it/s]2357948it [03:04, 20311.37it/s]1953588it [03:04, 21526.26it/s]1947160it [03:03, 21707.70it/s]2292411it [03:04, 20455.51it/s]2360052it [03:04, 20528.31it/s]1955741it [03:04, 21522.86it/s]1949331it [03:03, 21552.37it/s]2294462it [03:04, 20287.13it/s]2362204it [03:04, 20823.20it/s]1957894it [03:04, 21382.51it/s]1951556it [03:03, 21756.80it/s]2296494it [03:04, 19968.36it/s]2364298it [03:04, 20856.43it/s]1960033it [03:04, 21277.33it/s]1953733it [03:03, 21563.55it/s]2298494it [03:04, 19857.25it/s]2366385it [03:04, 20356.96it/s]1962250it [03:04, 21541.63it/s]1955926it [03:04, 21669.45it/s]2300482it [03:05, 19498.71it/s]2368472it [03:05, 20505.66it/s]1964405it [03:04, 21343.38it/s]1958094it [03:04, 21527.42it/s]2302439it [03:05, 19518.69it/s]2370623it [03:05, 20802.49it/s]1966652it [03:05, 21676.96it/s]1960254it [03:04, 21545.57it/s]2304412it [03:05, 19579.01it/s]2372726it [03:05, 20869.01it/s]1968962it [03:05, 22100.01it/s]1962409it [03:04, 21505.33it/s]2306371it [03:05, 19364.08it/s]2374847it [03:05, 20964.32it/s]1971224it [03:05, 22253.31it/s]1964578it [03:04, 21558.20it/s]2308345it [03:05, 19472.15it/s]2376953it [03:05, 20990.74it/s]1973603it [03:05, 22711.47it/s]1966735it [03:04, 21549.62it/s]2310294it [03:05, 19427.02it/s]2379111it [03:05, 21165.26it/s]1975875it [03:05, 22546.58it/s]1969016it [03:04, 21924.06it/s]2312238it [03:05, 19409.91it/s]2381229it [03:05, 21027.73it/s]1978131it [03:05, 22356.07it/s]1971299it [03:04, 22194.80it/s]2314180it [03:05, 19252.13it/s]2383384it [03:05, 21181.30it/s]1973646it [03:04, 22575.30it/s]1980368it [03:05, 21851.69it/s]2316193it [03:05, 19510.10it/s]2385503it [03:05, 21163.60it/s]1975904it [03:04, 22472.71it/s]1982556it [03:05, 21730.16it/s]2318174it [03:06, 19595.84it/s]2387647it [03:05, 21245.88it/s]1978152it [03:05, 22223.51it/s]1984731it [03:05, 21208.51it/s]2320135it [03:06, 19442.73it/s]2389772it [03:06, 21203.41it/s]1980376it [03:05, 21826.33it/s]1986855it [03:05, 21140.60it/s]2322114it [03:06, 19542.51it/s]2391893it [03:06, 21087.22it/s]1982570it [03:05, 21858.62it/s]1988971it [03:06, 21116.69it/s]2394002it [03:06, 21077.00it/s]2324069it [03:06, 19177.50it/s]1991107it [03:06, 21187.01it/s]1984758it [03:05, 21525.12it/s]2396134it [03:06, 21145.70it/s]2326066it [03:06, 19409.75it/s]1993227it [03:06, 21113.81it/s]1986962it [03:05, 21673.65it/s]2398281it [03:06, 21241.18it/s]2328009it [03:06, 19391.51it/s]1995398it [03:06, 21288.47it/s]1989131it [03:05, 21586.25it/s]2329954it [03:06, 19407.72it/s]2400406it [03:06, 21028.94it/s]1997720it [03:06, 21862.75it/s]1991347it [03:05, 21755.20it/s]2331984it [03:06, 19670.92it/s]2402627it [03:06, 21378.01it/s]1999927it [03:06, 21850.58it/s]1993545it [03:05, 21819.80it/s]2404766it [03:06, 21354.08it/s]2333952it [03:06, 19400.15it/s]2002241it [03:06, 22232.48it/s]1995770it [03:05, 21946.94it/s]2406919it [03:06, 21405.39it/s]2335927it [03:06, 19501.03it/s]2004608it [03:06, 22572.61it/s]1997966it [03:05, 21867.15it/s]2409060it [03:06, 21305.80it/s]2337879it [03:07, 19316.22it/s]2006866it [03:06, 22484.95it/s]2000276it [03:06, 22232.20it/s]2411191it [03:07, 21181.47it/s]2339870it [03:07, 19489.55it/s]2009206it [03:06, 22757.20it/s]2002500it [03:06, 22196.95it/s]2413314it [03:07, 21193.01it/s]2341820it [03:07, 19231.29it/s]2004791it [03:06, 22408.89it/s]2011483it [03:07, 22569.88it/s]2415472it [03:07, 21304.25it/s]2343871it [03:07, 19606.33it/s]2007125it [03:06, 22684.85it/s]2013776it [03:07, 22675.86it/s]2417641it [03:07, 21417.41it/s]2345834it [03:07, 19305.20it/s]2016045it [03:07, 22646.72it/s]2009394it [03:06, 22516.48it/s]2419783it [03:07, 21236.10it/s]2347804it [03:07, 19421.01it/s]2011683it [03:06, 22624.32it/s]2018310it [03:07, 22311.56it/s]2421956it [03:07, 21381.53it/s]2349851it [03:07, 19728.06it/s]2020569it [03:07, 22350.02it/s]2013946it [03:06, 22245.03it/s]2424095it [03:07, 21337.08it/s]2351826it [03:07, 19297.98it/s]2016240it [03:06, 22447.89it/s]2022805it [03:07, 21990.67it/s]2426254it [03:07, 21409.36it/s]2353895it [03:07, 19705.59it/s]2018487it [03:06, 22259.10it/s]2025006it [03:07, 21905.25it/s]2428396it [03:07, 21311.98it/s]2355897it [03:07, 19796.35it/s]2020715it [03:06, 22228.14it/s]2027198it [03:07, 21684.91it/s]2430528it [03:07, 21178.11it/s]2357972it [03:08, 20077.40it/s]2022939it [03:07, 21870.01it/s]2029389it [03:07, 21742.16it/s]2432656it [03:08, 21206.44it/s]2359982it [03:08, 20043.69it/s]2025128it [03:07, 21716.48it/s]2031564it [03:07, 21338.51it/s]2434777it [03:08, 21165.79it/s]2362018it [03:08, 20136.93it/s]2027301it [03:07, 21505.44it/s]2033778it [03:08, 21571.23it/s]2436932it [03:08, 21279.54it/s]2364149it [03:08, 20486.73it/s]2029487it [03:07, 21607.95it/s]2036117it [03:08, 22105.55it/s]2439061it [03:08, 21035.77it/s]2366199it [03:08, 20338.12it/s]2038330it [03:08, 22088.05it/s]2031649it [03:07, 21373.89it/s]2441205it [03:08, 21155.37it/s]2368313it [03:08, 20575.76it/s]2040678it [03:08, 22501.39it/s]2033818it [03:07, 21466.64it/s]2443322it [03:08, 21062.86it/s]2370372it [03:08, 20377.43it/s]2042994it [03:08, 22696.33it/s]2036050it [03:07, 21717.13it/s]2445476it [03:08, 21204.06it/s]2372494it [03:08, 20627.10it/s]2045265it [03:08, 22537.67it/s]2038367it [03:07, 22148.22it/s]2447695it [03:08, 21496.00it/s]2374558it [03:08, 20341.99it/s]2040643it [03:07, 22328.55it/s]2047520it [03:08, 22055.11it/s]2449884it [03:08, 21612.70it/s]2376665it [03:08, 20555.82it/s]2042877it [03:08, 21930.63it/s]2049737it [03:08, 22086.84it/s]2452046it [03:08, 21603.07it/s]2378777it [03:09, 20722.22it/s]2051948it [03:08, 22029.99it/s]2045073it [03:08, 21516.60it/s]2454225it [03:09, 21657.44it/s]2380851it [03:09, 20660.60it/s]2054153it [03:09, 21799.47it/s]2047241it [03:08, 21562.57it/s]2456399it [03:09, 21681.70it/s]2382956it [03:09, 20776.07it/s]2056409it [03:09, 22018.95it/s]2049400it [03:08, 21526.31it/s]2458604it [03:09, 21790.45it/s]2385035it [03:09, 20609.03it/s]2058717it [03:09, 22331.67it/s]2051554it [03:08, 20956.15it/s]2460789it [03:09, 21806.25it/s]2387120it [03:09, 20678.17it/s]2060985it [03:09, 22433.65it/s]2053654it [03:08, 20801.09it/s]2462970it [03:09, 21692.65it/s]2389189it [03:09, 20533.15it/s]2063230it [03:09, 21773.70it/s]2465140it [03:09, 21645.18it/s]2055737it [03:08, 20409.34it/s]2391272it [03:09, 20618.83it/s]2065533it [03:09, 22140.05it/s]2467351it [03:09, 21782.40it/s]2057991it [03:08, 21026.16it/s]2393402it [03:09, 20819.01it/s]2067752it [03:09, 21914.00it/s]2469549it [03:09, 21840.53it/s]2060243it [03:08, 21463.95it/s]2395485it [03:09, 20487.90it/s]2471791it [03:09, 22011.89it/s]2062500it [03:08, 21787.49it/s]2069947it [03:09, 21368.86it/s]2397587it [03:09, 20643.71it/s]2474070it [03:09, 22244.05it/s]2064691it [03:09, 21822.22it/s]2072089it [03:09, 21297.07it/s]2399653it [03:10, 20323.89it/s]2476313it [03:10, 22299.21it/s]2066882it [03:09, 21845.50it/s]2074295it [03:09, 21518.16it/s]2401822it [03:10, 20724.33it/s]2478544it [03:10, 22204.63it/s]2076450it [03:10, 21524.25it/s]2069069it [03:09, 21589.75it/s]2403897it [03:10, 20519.25it/s]2480765it [03:10, 21885.10it/s]2071230it [03:09, 21304.16it/s]2078605it [03:10, 20347.99it/s]2406007it [03:10, 20689.99it/s]2482955it [03:10, 21775.72it/s]2073363it [03:09, 21239.03it/s]2080810it [03:10, 20833.78it/s]2408078it [03:10, 20682.33it/s]2485134it [03:10, 21748.65it/s]2075489it [03:09, 21125.84it/s]2083055it [03:10, 21301.07it/s]2410148it [03:10, 20683.79it/s]2487310it [03:10, 21707.18it/s]2077611it [03:09, 21151.21it/s]2085281it [03:10, 21579.85it/s]2412260it [03:10, 20812.57it/s]2489482it [03:10, 21456.80it/s]2079727it [03:09, 21105.25it/s]2087593it [03:10, 22031.89it/s]2414411it [03:10, 21018.64it/s]2491684it [03:10, 21622.35it/s]2081894it [03:09, 21186.54it/s]2089880it [03:10, 22279.84it/s]2416527it [03:10, 21059.23it/s]2493858it [03:10, 21655.83it/s]2084094it [03:09, 21426.73it/s]2092113it [03:10, 22130.59it/s]2418660it [03:10, 21135.32it/s]2496089it [03:10, 21797.44it/s]2086261it [03:10, 21497.75it/s]2094330it [03:10, 22037.34it/s]2420804it [03:11, 21222.77it/s]2498286it [03:11, 21847.97it/s]2088476it [03:10, 21691.56it/s]2096537it [03:10, 21938.63it/s]2422927it [03:11, 21040.56it/s]2090669it [03:10, 21760.38it/s]2500472it [03:11, 21528.50it/s]2098733it [03:11, 21717.02it/s]2425032it [03:11, 21006.23it/s]2502636it [03:11, 21559.43it/s]2092846it [03:10, 21448.21it/s]2100997it [03:11, 21987.88it/s]2427133it [03:11, 20961.81it/s]2504849it [03:11, 21726.05it/s]2094992it [03:10, 21334.66it/s]2103198it [03:11, 21753.82it/s]2429230it [03:11, 20915.67it/s]2507069it [03:11, 21865.85it/s]2097127it [03:10, 21232.89it/s]2105410it [03:11, 21820.86it/s]2431322it [03:11, 20524.90it/s]2509257it [03:11, 21809.46it/s]2099307it [03:10, 21400.41it/s]2107657it [03:11, 22010.46it/s]2433418it [03:11, 20652.66it/s]2511439it [03:11, 21726.16it/s]2101540it [03:10, 21426.99it/s]2109859it [03:11, 21684.01it/s]2435532it [03:11, 20794.69it/s]2513612it [03:11, 21704.35it/s]2103757it [03:10, 21644.65it/s]2112100it [03:11, 21896.58it/s]2437613it [03:11, 20036.92it/s]2515783it [03:11, 21584.04it/s]2105998it [03:10, 21870.54it/s]2114292it [03:11, 21817.14it/s]2439729it [03:11, 20359.61it/s]2517967it [03:11, 21656.43it/s]2108186it [03:11, 21569.88it/s]2116560it [03:11, 22072.71it/s]2441812it [03:12, 20494.50it/s]2520187it [03:12, 21815.48it/s]2110407it [03:11, 21758.45it/s]2118851it [03:11, 22223.13it/s]2443910it [03:12, 20635.25it/s]2522369it [03:12, 21775.21it/s]2112584it [03:11, 21595.99it/s]2121164it [03:12, 22491.43it/s]2446020it [03:12, 20770.79it/s]2524547it [03:12, 21688.50it/s]2114837it [03:11, 21871.36it/s]2123414it [03:12, 22438.91it/s]2448100it [03:12, 20728.41it/s]2526717it [03:12, 21642.37it/s]2117048it [03:11, 21874.87it/s]2125659it [03:12, 22193.17it/s]2450292it [03:12, 21082.52it/s]2528882it [03:12, 21502.74it/s]2119344it [03:11, 22189.60it/s]2127880it [03:12, 22197.62it/s]2452429it [03:12, 21167.64it/s]2531067it [03:12, 21604.38it/s]2121646it [03:11, 22435.60it/s]2130101it [03:12, 21758.83it/s]2454547it [03:12, 21147.95it/s]2533228it [03:12, 21516.30it/s]2123891it [03:11, 22114.25it/s]2132287it [03:12, 21787.48it/s]2456663it [03:12, 21100.70it/s]2535380it [03:12, 21455.26it/s]2126104it [03:11, 21949.53it/s]2134468it [03:12, 21697.09it/s]2458792it [03:12, 21156.40it/s]2537526it [03:12, 21300.69it/s]2136639it [03:12, 21655.65it/s]2460944it [03:13, 21262.93it/s]2128300it [03:12, 20647.37it/s]2539657it [03:12, 21278.61it/s]2463071it [03:13, 21125.00it/s]2138806it [03:12, 21464.91it/s]2130491it [03:12, 21003.92it/s]2541786it [03:13, 21265.95it/s]2465184it [03:13, 21112.28it/s]2140962it [03:12, 21491.90it/s]2132650it [03:12, 21172.38it/s]2543968it [03:13, 21349.30it/s]2467296it [03:13, 21104.05it/s]2143148it [03:13, 21600.53it/s]2134816it [03:12, 21313.54it/s]2546198it [03:13, 21628.15it/s]2469515it [03:13, 21425.41it/s]2137021it [03:12, 21529.44it/s]2145309it [03:13, 21332.73it/s]2548362it [03:13, 21621.19it/s]2471702it [03:13, 21554.82it/s]2139192it [03:12, 21579.50it/s]2147473it [03:13, 21421.74it/s]2550629it [03:13, 21933.26it/s]2473896it [03:13, 21669.33it/s]2149630it [03:13, 21464.26it/s]2141354it [03:12, 20406.62it/s]2552823it [03:13, 21855.54it/s]2476064it [03:13, 21667.38it/s]2143516it [03:12, 20751.59it/s]2555009it [03:13, 21719.28it/s]2478260it [03:13, 21752.10it/s]2145716it [03:12, 21111.20it/s]2557182it [03:13, 21710.61it/s]2480436it [03:13, 21368.97it/s]2147868it [03:12, 21229.83it/s]2559354it [03:13, 21511.59it/s]2482590it [03:14, 21417.27it/s]2149999it [03:13, 21234.64it/s]2561599it [03:13, 21789.23it/s]2484733it [03:14, 21135.92it/s]2563809it [03:14, 21878.03it/s]2486848it [03:14, 21131.71it/s]2566075it [03:14, 22108.90it/s]2489014it [03:14, 21287.51it/s]2568287it [03:14, 22072.14it/s]2491144it [03:14, 20978.70it/s]2570499it [03:14, 22084.19it/s]2493313it [03:14, 21188.24it/s]2572708it [03:14, 22064.36it/s]2495434it [03:14, 21156.89it/s]2574964it [03:14, 22211.36it/s]2497609it [03:14, 21331.34it/s]2577205it [03:14, 22270.05it/s]2499743it [03:14, 21104.82it/s]2579433it [03:14, 22190.24it/s]2501879it [03:14, 21177.68it/s]2581705it [03:14, 22347.14it/s]2503998it [03:15, 21146.17it/s]2583940it [03:14, 22010.77it/s]2506141it [03:15, 21230.16it/s]2586143it [03:15, 21809.74it/s]2508265it [03:15, 21150.44it/s]2588325it [03:15, 21480.93it/s]2510381it [03:15, 21032.12it/s]2590475it [03:15, 21407.52it/s]2512485it [03:15, 20786.29it/s]2592672it [03:15, 21571.31it/s]2514565it [03:15, 20733.52it/s]2594870it [03:15, 21690.09it/s]2516639it [03:15, 20608.25it/s]2597076it [03:15, 21799.65it/s]2518822it [03:15, 20968.78it/s]2599257it [03:15, 21548.22it/s]2520994it [03:15, 21191.59it/s]2601413it [03:15, 21448.45it/s]2523114it [03:15, 21144.16it/s]2603609it [03:15, 21579.75it/s]2525229it [03:16, 20998.10it/s]2605850it [03:16, 21823.31it/s]2527370it [03:16, 21119.83it/s]2608033it [03:16, 21736.56it/s]2529504it [03:16, 21181.88it/s]2610277it [03:16, 21943.66it/s]2531623it [03:16, 21176.29it/s]2612489it [03:16, 21994.36it/s]2533741it [03:16, 20986.38it/s]2614689it [03:16, 21959.53it/s]2535841it [03:16, 20942.92it/s]2616923it [03:16, 22070.52it/s]2537936it [03:16, 20778.03it/s]2619131it [03:16, 21904.16it/s]2540062it [03:16, 20920.71it/s]2621349it [03:16, 21985.03it/s]2542155it [03:16, 20876.23it/s]2623595it [03:16, 22125.88it/s]2544309it [03:16, 21073.18it/s]2625854it [03:16, 22263.33it/s]2546472it [03:17, 21237.37it/s]2628081it [03:17, 22153.55it/s]2548652it [03:17, 21404.15it/s]2630342it [03:17, 22286.43it/s]2550799it [03:17, 21422.39it/s]2632571it [03:17, 22207.52it/s]2552958it [03:17, 21471.93it/s]2634795it [03:17, 22213.84it/s]2555106it [03:17, 21230.40it/s]2637087it [03:17, 22422.50it/s]2557261it [03:17, 21324.26it/s]2639330it [03:17, 22323.93it/s]2559394it [03:17, 21242.19it/s]2641563it [03:17, 22257.50it/s]2561598it [03:17, 21478.64it/s]2643789it [03:17, 22220.43it/s]2563772it [03:17, 21554.68it/s]2646012it [03:17, 22054.78it/s]2565928it [03:17, 21555.65it/s]2648218it [03:17, 21728.75it/s]2568160it [03:18, 21783.85it/s]2650392it [03:18, 21680.31it/s]2570339it [03:18, 21707.20it/s]2652561it [03:18, 21526.05it/s]2572510it [03:18, 21675.44it/s]2654728it [03:18, 21565.67it/s]2574678it [03:18, 21523.64it/s]2656912it [03:18, 21644.92it/s]2576888it [03:18, 21693.87it/s]2659077it [03:18, 21586.46it/s]2579065it [03:18, 21714.32it/s]2661236it [03:18, 21500.21it/s]2581274it [03:18, 21819.56it/s]2663387it [03:18, 21400.87it/s]2583457it [03:18, 21600.96it/s]2665577it [03:18, 21548.32it/s]2585690it [03:18, 21815.05it/s]2667749it [03:18, 21596.67it/s]2587873it [03:18, 21490.94it/s]2669980it [03:18, 21807.45it/s]2590024it [03:19, 21470.90it/s]2672161it [03:19, 21790.58it/s]2592172it [03:19, 21302.80it/s]2674391it [03:19, 21940.53it/s]2594364it [03:19, 21472.25it/s]2676671it [03:19, 22197.27it/s]2596553it [03:19, 21594.66it/s]2678891it [03:19, 22010.16it/s]2598714it [03:19, 21467.12it/s]2681093it [03:19, 21917.37it/s]2600862it [03:19, 21102.18it/s]2683298it [03:19, 21954.12it/s]2603001it [03:19, 21185.34it/s]2685494it [03:19, 21823.27it/s]2605147it [03:19, 21265.44it/s]2687677it [03:19, 21633.47it/s]2607338it [03:19, 21456.65it/s]2689936it [03:19, 21916.53it/s]2609485it [03:19, 21384.15it/s]2692178it [03:19, 22064.09it/s]2611690it [03:20, 21579.42it/s]2694389it [03:20, 22076.27it/s]2613849it [03:20, 21455.91it/s]2696643it [03:20, 22212.07it/s]2616036it [03:20, 21577.91it/s]2698865it [03:20, 21748.42it/s]2618195it [03:20, 21483.41it/s]2701082it [03:20, 21870.23it/s]2620404it [03:20, 21591.03it/s]2703367it [03:20, 22160.23it/s]2622564it [03:20, 21549.70it/s]2705635it [03:20, 22313.11it/s]2624781it [03:20, 21730.90it/s]2707868it [03:20, 22129.89it/s]2626985it [03:20, 21822.55it/s]2710156it [03:20, 22351.11it/s]2629210it [03:20, 21949.38it/s]2712395it [03:20, 22361.24it/s]2631406it [03:20, 21745.17it/s]2714646it [03:20, 22403.59it/s]2633635it [03:21, 21906.74it/s]2716887it [03:21, 22307.55it/s]2635882it [03:21, 22074.01it/s]2719119it [03:21, 21878.39it/s]2638090it [03:21, 22043.36it/s]2721309it [03:21, 21805.39it/s]2640323it [03:21, 22126.03it/s]2723559it [03:21, 22009.88it/s]2642536it [03:21, 21831.65it/s]2725808it [03:21, 22149.37it/s]2644736it [03:21, 21878.65it/s]2728024it [03:21, 21911.75it/s]2646925it [03:21, 21644.29it/s]2730304it [03:21, 22173.86it/s]2649091it [03:21, 21496.16it/s]2732544it [03:21, 22238.70it/s]2651242it [03:21, 21288.86it/s]2734778it [03:21, 22266.75it/s]2653372it [03:21, 21289.29it/s]2737006it [03:21, 22210.38it/s]2655502it [03:22, 21159.34it/s]2657728it [03:22, 21482.07it/s]2659877it [03:22, 21370.92it/s]2662015it [03:22, 21257.49it/s]2664142it [03:22, 20955.69it/s]2666367it [03:22, 21335.13it/s]2668527it [03:22, 21410.46it/s]2670751it [03:22, 21655.76it/s]2672918it [03:22, 21444.14it/s]2675154it [03:23, 21714.55it/s]2677338it [03:23, 21750.67it/s]2679525it [03:23, 21784.37it/s]2681704it [03:23, 21741.23it/s]2683879it [03:23, 21679.89it/s]2686048it [03:23, 21382.93it/s]2688273it [03:23, 21636.91it/s]2690438it [03:23, 21587.39it/s]2692659it [03:23, 21771.50it/s]2694837it [03:23, 21688.25it/s]2697007it [03:24, 21682.42it/s]2699176it [03:24, 20883.97it/s]2701273it [03:24, 20908.66it/s]2703417it [03:24, 21063.68it/s]2705527it [03:24, 21011.93it/s]2707683it [03:24, 21172.02it/s]2709844it [03:24, 21299.16it/s]2711996it [03:24, 21360.49it/s]2714222it [03:24, 21628.16it/s]2716386it [03:24, 21405.06it/s]2718528it [03:25, 21386.43it/s]2720668it [03:25, 21152.71it/s]2722833it [03:25, 21297.36it/s]2724988it [03:25, 21371.50it/s]2727126it [03:25, 21342.54it/s]2729299it [03:25, 21455.74it/s]2731455it [03:25, 21485.17it/s]2733638it [03:25, 21586.29it/s]2735850it [03:25, 21745.40it/s]2738025it [03:25, 21444.47it/s]2149630it [03:35, 21464.26it/s]2150002it [03:35, 240.29it/s]  2152151it [03:35, 373.80it/s]2154348it [03:35, 565.92it/s]2156511it [03:36, 830.24it/s]2158695it [03:36, 1200.23it/s]2160864it [03:36, 1702.91it/s]2163070it [03:36, 2392.34it/s]2165248it [03:36, 3283.60it/s]2167477it [03:36, 4457.88it/s]2169660it [03:36, 5850.40it/s]2171869it [03:36, 7531.95it/s]2174044it [03:36, 9342.53it/s]2176258it [03:36, 11327.95it/s]2178437it [03:37, 13197.35it/s]2180644it [03:37, 15019.94it/s]2182865it [03:37, 16650.28it/s]2185281it [03:37, 18426.82it/s]2187573it [03:37, 19586.85it/s]2149999it [03:36, 21234.64it/s]2150005it [03:36, 210.80it/s]  2189930it [03:37, 20660.00it/s]2152222it [03:36, 341.05it/s]2192221it [03:37, 20786.31it/s]2154408it [03:36, 521.85it/s]2194459it [03:37, 21231.02it/s]2156629it [03:37, 779.09it/s]2196696it [03:37, 21109.09it/s]2158810it [03:37, 1128.69it/s]2198887it [03:37, 21317.64it/s]2161014it [03:37, 1614.51it/s]2201076it [03:38, 21367.96it/s]2163246it [03:37, 2279.61it/s]2203260it [03:38, 21503.02it/s]2165434it [03:37, 3128.30it/s]2205439it [03:38, 21306.28it/s]2167644it [03:37, 4240.15it/s]2207703it [03:38, 21694.24it/s]2169856it [03:37, 5624.18it/s]2209993it [03:38, 22046.88it/s]2172025it [03:37, 7172.45it/s]2212292it [03:38, 22326.01it/s]2174239it [03:37, 9024.53it/s]2214533it [03:38, 22202.11it/s]2176440it [03:37, 10974.95it/s]2216791it [03:38, 22313.41it/s]2178611it [03:38, 12870.84it/s]2219058it [03:38, 22418.31it/s]2180779it [03:38, 14587.07it/s]2221332it [03:38, 22505.71it/s]2183077it [03:38, 16457.02it/s]2223585it [03:39, 22495.20it/s]2185275it [03:38, 17123.20it/s]2225836it [03:39, 22415.05it/s]2187597it [03:38, 18641.13it/s]2228101it [03:39, 22482.70it/s]2189935it [03:38, 19884.29it/s]2230351it [03:39, 22061.70it/s]2192156it [03:38, 20434.76it/s]2232560it [03:39, 21945.15it/s]2194384it [03:38, 20950.33it/s]2234784it [03:39, 22029.28it/s]2196600it [03:38, 21115.13it/s]2236999it [03:39, 22062.31it/s]2198797it [03:38, 21307.84it/s]2239225it [03:39, 22120.01it/s]2201027it [03:39, 21595.06it/s]2241438it [03:39, 21886.57it/s]2203230it [03:39, 21418.45it/s]2243644it [03:39, 21937.45it/s]2205444it [03:39, 21553.21it/s]2245960it [03:40, 22299.16it/s]2207653it [03:39, 21710.58it/s]2248282it [03:40, 22516.07it/s]2209954it [03:39, 22094.54it/s]2250642it [03:40, 22836.65it/s]2212175it [03:39, 22039.21it/s]2252968it [03:40, 22960.56it/s]2214480it [03:39, 22337.88it/s]2255265it [03:40, 22668.03it/s]2216720it [03:39, 22142.12it/s]2257533it [03:40, 22662.85it/s]2218994it [03:39, 22317.87it/s]2259856it [03:40, 22830.71it/s]2221292it [03:39, 22514.09it/s]2262142it [03:40, 22829.08it/s]2223546it [03:40, 22316.89it/s]2264426it [03:40, 22667.25it/s]2225782it [03:40, 22327.31it/s]2266694it [03:41, 21997.41it/s]2228017it [03:40, 22166.55it/s]2268902it [03:41, 22019.35it/s]2230235it [03:40, 22141.13it/s]2271167it [03:41, 22204.68it/s]2232450it [03:40, 22050.16it/s]2273482it [03:41, 22433.16it/s]2234656it [03:40, 21816.16it/s]2275771it [03:41, 22568.21it/s]2236890it [03:40, 21968.55it/s]2278030it [03:41, 22540.98it/s]2239088it [03:40, 21773.48it/s]2280286it [03:41, 22411.97it/s]2241329it [03:40, 21960.19it/s]2282529it [03:41, 20617.68it/s]2243526it [03:41, 21709.40it/s]2284816it [03:41, 21248.90it/s]2245698it [03:41, 21494.29it/s]2287063it [03:41, 21596.59it/s]2247956it [03:41, 21811.99it/s]2289241it [03:42, 21338.15it/s]2250139it [03:41, 21397.02it/s]2737006it [03:42, 22210.38it/s]2738921it [03:42, 347.48it/s]  2291388it [03:42, 20518.83it/s]2252454it [03:41, 21910.90it/s]2741127it [03:42, 497.48it/s]2293454it [03:42, 20406.34it/s]2254695it [03:41, 22055.15it/s]2743396it [03:42, 714.37it/s]2295504it [03:42, 19980.18it/s]2256931it [03:41, 22142.85it/s]2745641it [03:42, 1013.55it/s]2297509it [03:42, 19991.63it/s]2259185it [03:41, 22258.96it/s]2747872it [03:42, 1423.88it/s]2299523it [03:42, 19973.36it/s]2261428it [03:41, 22309.31it/s]2750154it [03:42, 1997.48it/s]2301524it [03:42, 19585.31it/s]2263660it [03:41, 21975.05it/s]2752393it [03:42, 2746.56it/s]2303528it [03:42, 19715.26it/s]2265924it [03:42, 22057.82it/s]2754612it [03:42, 3715.97it/s]2305503it [03:42, 19533.32it/s]2268131it [03:42, 21987.47it/s]2756821it [03:43, 4917.49it/s]2307459it [03:42, 19381.51it/s]2270331it [03:42, 21857.22it/s]2759080it [03:43, 6445.84it/s]2309440it [03:43, 19506.65it/s]2272644it [03:42, 22163.53it/s]2761321it [03:43, 8201.29it/s]2311392it [03:43, 19305.73it/s]2274890it [03:42, 22250.55it/s]2763554it [03:43, 10119.45it/s]2313385it [03:43, 19486.17it/s]2277116it [03:42, 22218.03it/s]2765801it [03:43, 12095.04it/s]2315450it [03:43, 19828.92it/s]2279339it [03:42, 22072.20it/s]2768015it [03:43, 13935.93it/s]2317435it [03:43, 19514.61it/s]2281547it [03:42, 21901.14it/s]2770217it [03:43, 15587.23it/s]2319456it [03:43, 19718.49it/s]2283738it [03:42, 21765.51it/s]2772458it [03:43, 17162.45it/s]2321430it [03:43, 19444.61it/s]2286002it [03:42, 22022.85it/s]2774690it [03:43, 18441.52it/s]2323377it [03:43, 19385.95it/s]2288205it [03:43, 21858.85it/s]2776904it [03:43, 19212.34it/s]2325317it [03:43, 19245.96it/s]2290392it [03:43, 20987.13it/s]2779190it [03:44, 20195.36it/s]2327270it [03:44, 19326.34it/s]2292498it [03:43, 20634.69it/s]2781440it [03:44, 20836.81it/s]2329271it [03:44, 19527.80it/s]2294567it [03:43, 20153.72it/s]2783677it [03:44, 21271.62it/s]2331225it [03:44, 19465.86it/s]2296587it [03:43, 19952.67it/s]2785916it [03:44, 21592.84it/s]2333173it [03:44, 19140.14it/s]2298586it [03:43, 19917.93it/s]2788149it [03:44, 21631.12it/s]2335173it [03:44, 19390.02it/s]2300580it [03:43, 19496.27it/s]2790364it [03:44, 21639.95it/s]2337114it [03:44, 19322.87it/s]2302553it [03:43, 19562.30it/s]2792613it [03:44, 21889.14it/s]2339048it [03:44, 19311.25it/s]2304555it [03:43, 19695.68it/s]2794828it [03:44, 21801.30it/s]2340980it [03:44, 19169.26it/s]2306527it [03:43, 19545.20it/s]2797038it [03:44, 21888.54it/s]2342957it [03:44, 19347.00it/s]2308483it [03:44, 19542.25it/s]2799277it [03:44, 22034.73it/s]2344893it [03:44, 19273.42it/s]2310444it [03:44, 19517.12it/s]2801490it [03:45, 21624.07it/s]2346821it [03:45, 19270.01it/s]2312397it [03:44, 19455.43it/s]2803661it [03:45, 21630.80it/s]2348869it [03:45, 19629.33it/s]2314343it [03:44, 19393.00it/s]2805915it [03:45, 21898.88it/s]2350833it [03:45, 19444.00it/s]2316293it [03:44, 19423.13it/s]2808154it [03:45, 22043.61it/s]2352779it [03:45, 19431.78it/s]2318236it [03:44, 19357.79it/s]2810362it [03:45, 21870.67it/s]2354829it [03:45, 19749.32it/s]2320173it [03:44, 19360.08it/s]2812650it [03:45, 22168.67it/s]2356845it [03:45, 19871.51it/s]2322110it [03:44, 19242.98it/s]2814884it [03:45, 22217.57it/s]2358900it [03:45, 20073.57it/s]2324035it [03:44, 19070.98it/s]2817134it [03:45, 22299.50it/s]2360939it [03:45, 20167.71it/s]2325954it [03:44, 19105.21it/s]2819449it [03:45, 22550.77it/s]2363042it [03:45, 20423.77it/s]2327944it [03:45, 19336.12it/s]2821705it [03:46, 22265.85it/s]2365141it [03:45, 20589.59it/s]2329878it [03:45, 19123.78it/s]2823960it [03:46, 22347.26it/s]2367293it [03:46, 20864.80it/s]2331865it [03:45, 19341.77it/s]2826243it [03:46, 22490.38it/s]2369385it [03:46, 20879.21it/s]2333824it [03:45, 19405.82it/s]2828493it [03:46, 22452.61it/s]2371523it [03:46, 21029.02it/s]2335766it [03:45, 19351.93it/s]2830739it [03:46, 22084.71it/s]2373626it [03:46, 20809.26it/s]2337702it [03:45, 19236.31it/s]2833012it [03:46, 22275.06it/s]2375708it [03:46, 20765.64it/s]2339667it [03:45, 19355.80it/s]2835301it [03:46, 22454.60it/s]2377785it [03:46, 20541.59it/s]2341603it [03:45, 19057.58it/s]2837548it [03:46, 22310.73it/s]2379943it [03:46, 20848.02it/s]2343579it [03:45, 19264.58it/s]2839781it [03:46, 22224.50it/s]2382029it [03:46, 20676.43it/s]2345507it [03:46, 19250.21it/s]2842005it [03:46, 21999.14it/s]2384157it [03:46, 20788.82it/s]2347433it [03:46, 18982.29it/s]2844233it [03:47, 22080.56it/s]2386237it [03:46, 20747.34it/s]2349422it [03:46, 19249.83it/s]2846454it [03:47, 22115.64it/s]2388313it [03:47, 20679.94it/s]2351349it [03:46, 19113.83it/s]2848667it [03:47, 21949.23it/s]2390382it [03:47, 20501.95it/s]2353285it [03:46, 19186.35it/s]2850863it [03:47, 21729.22it/s]2392474it [03:47, 20623.48it/s]2355367it [03:46, 19671.32it/s]2853038it [03:47, 21734.82it/s]2394537it [03:47, 20418.78it/s]2357405it [03:46, 19881.88it/s]2855269it [03:47, 21904.34it/s]2396620it [03:47, 20540.07it/s]2359441it [03:46, 20021.28it/s]2857460it [03:47, 21763.54it/s]2398675it [03:47, 20398.00it/s]2361488it [03:46, 20154.74it/s]2859639it [03:47, 21770.03it/s]2400825it [03:47, 20723.37it/s]2363563it [03:46, 20249.39it/s]2861817it [03:47, 21304.83it/s]2402899it [03:47, 20607.60it/s]2365691it [03:47, 20556.47it/s]2864077it [03:47, 21683.79it/s]2405045it [03:47, 20860.00it/s]2367747it [03:47, 20517.71it/s]2866265it [03:48, 21740.67it/s]2407137it [03:47, 20875.80it/s]2369825it [03:47, 20594.74it/s]2868441it [03:48, 21643.48it/s]2409226it [03:48, 20820.03it/s]2371885it [03:47, 20523.57it/s]2870607it [03:48, 21634.13it/s]2411309it [03:48, 20728.98it/s]2373938it [03:47, 20519.31it/s]2872825it [03:48, 21794.23it/s]2413397it [03:48, 20771.44it/s]2375991it [03:47, 20482.95it/s]2875045it [03:48, 21913.78it/s]2415477it [03:48, 20778.46it/s]2378079it [03:47, 20599.18it/s]2877237it [03:48, 21881.47it/s]2417592it [03:48, 20887.39it/s]2380165it [03:47, 20611.47it/s]2879442it [03:48, 21931.29it/s]2419681it [03:48, 20794.45it/s]2382243it [03:47, 20658.43it/s]2881636it [03:48, 21783.09it/s]2421761it [03:48, 20782.00it/s]2384316it [03:47, 20676.75it/s]2883815it [03:48, 21684.09it/s]2423840it [03:48, 20676.73it/s]2386384it [03:48, 20498.69it/s]2886043it [03:48, 21859.31it/s]2425908it [03:48, 20524.11it/s]2388435it [03:48, 20499.38it/s]2888290it [03:49, 22040.08it/s]2427961it [03:48, 20498.14it/s]2390486it [03:48, 20368.21it/s]2890495it [03:49, 21900.85it/s]2430011it [03:49, 20470.47it/s]2392574it [03:48, 20517.16it/s]2892686it [03:49, 21818.13it/s]2432102it [03:49, 20597.96it/s]2394648it [03:48, 20405.57it/s]2894952it [03:49, 22065.32it/s]2434162it [03:49, 20504.98it/s]2396791it [03:48, 20705.78it/s]2897172it [03:49, 22102.07it/s]2436259it [03:49, 20607.30it/s]2398863it [03:48, 20608.60it/s]2899383it [03:49, 21965.47it/s]2438365it [03:49, 20740.39it/s]2400966it [03:48, 20732.69it/s]2901580it [03:49, 21953.75it/s]2440440it [03:49, 20694.47it/s]2403079it [03:48, 20849.44it/s]2903776it [03:49, 21621.79it/s]2442510it [03:49, 20504.81it/s]2405165it [03:48, 20776.03it/s]2905993it [03:49, 21780.71it/s]2444565it [03:49, 20512.65it/s]2407243it [03:49, 20571.13it/s]2908173it [03:49, 21738.42it/s]2446650it [03:49, 20612.32it/s]2409301it [03:49, 20540.68it/s]2910382it [03:50, 21841.28it/s]2448802it [03:49, 20881.88it/s]2411356it [03:49, 20470.90it/s]2912567it [03:50, 21634.23it/s]2450898it [03:50, 20904.05it/s]2413429it [03:49, 20545.52it/s]2914780it [03:50, 21778.31it/s]2453013it [03:50, 20976.57it/s]2415484it [03:49, 20543.94it/s]2917055it [03:50, 22065.67it/s]2455111it [03:50, 20895.54it/s]2417572it [03:49, 20642.47it/s]2919347it [03:50, 22317.60it/s]2457201it [03:50, 20871.47it/s]2419645it [03:49, 20558.51it/s]2921605it [03:50, 22393.06it/s]2459305it [03:50, 20919.18it/s]2421732it [03:49, 20650.38it/s]2923845it [03:50, 22116.58it/s]2461454it [03:50, 21089.67it/s]2423802it [03:49, 20664.05it/s]2926151it [03:50, 22395.34it/s]2463564it [03:50, 20905.03it/s]2425879it [03:49, 20561.58it/s]2928398it [03:50, 22415.99it/s]2465655it [03:50, 20875.04it/s]2427997it [03:50, 20744.16it/s]2930661it [03:50, 22478.20it/s]2467830it [03:50, 20976.35it/s]2430072it [03:50, 20688.63it/s]2932910it [03:51, 22300.96it/s]2470082it [03:50, 21432.06it/s]2432142it [03:50, 20674.70it/s]2935141it [03:51, 22045.28it/s]2472279it [03:51, 21589.86it/s]2434210it [03:50, 20531.42it/s]2937347it [03:51, 21663.45it/s]2474455it [03:51, 21631.90it/s]2436284it [03:50, 20592.91it/s]2939539it [03:51, 21737.68it/s]2476644it [03:51, 21705.33it/s]2438344it [03:50, 20455.08it/s]2941715it [03:51, 21673.22it/s]2478815it [03:51, 21463.93it/s]2440419it [03:50, 20541.75it/s]2943891it [03:51, 21698.50it/s]2480963it [03:51, 21319.40it/s]2442474it [03:50, 20423.23it/s]2946102it [03:51, 21819.77it/s]2483096it [03:51, 21150.60it/s]2444549it [03:50, 20518.54it/s]2948285it [03:51, 21620.62it/s]2485212it [03:51, 20941.09it/s]2446671it [03:50, 20725.18it/s]2950576it [03:51, 22002.16it/s]2487336it [03:51, 21020.65it/s]2448841it [03:51, 21014.75it/s]2952862it [03:51, 22255.88it/s]2489439it [03:51, 20915.36it/s]2450977it [03:51, 21115.71it/s]2955089it [03:52, 22170.96it/s]2491536it [03:51, 20923.74it/s]2453089it [03:51, 21095.62it/s]2738025it [03:52, 21444.47it/s]2738915it [03:52, 224.84it/s]  2957323it [03:52, 22163.71it/s]2493629it [03:52, 20880.26it/s]2455229it [03:51, 21182.94it/s]2741015it [03:52, 335.99it/s]2959595it [03:52, 22329.17it/s]2495780it [03:52, 21066.50it/s]2457400it [03:51, 21338.30it/s]2743247it [03:52, 502.73it/s]2961829it [03:52, 22021.51it/s]2497887it [03:52, 20908.78it/s]2459534it [03:51, 21291.66it/s]2745409it [03:52, 729.07it/s]2964052it [03:52, 22082.04it/s]2499981it [03:52, 20916.85it/s]2461664it [03:51, 21150.30it/s]2747608it [03:52, 1049.71it/s]2966311it [03:52, 22230.22it/s]2502073it [03:52, 20680.27it/s]2463835it [03:51, 21316.86it/s]2749809it [03:52, 1491.77it/s]2968566it [03:52, 22323.72it/s]2504288it [03:52, 21111.92it/s]2465967it [03:51, 20929.09it/s]2751989it [03:52, 2084.99it/s]2970804it [03:52, 22337.51it/s]2506470it [03:52, 21321.30it/s]2468230it [03:51, 21429.36it/s]2754161it [03:52, 2863.27it/s]2973075it [03:52, 22448.54it/s]2508635it [03:52, 21416.12it/s]2470375it [03:52, 21383.44it/s]2756338it [03:53, 3884.42it/s]2975321it [03:53, 21925.00it/s]2510778it [03:52, 21100.87it/s]2472600it [03:52, 21638.76it/s]2758572it [03:53, 5206.34it/s]2977517it [03:53, 21790.65it/s]2512890it [03:53, 20879.07it/s]2474936it [03:52, 22149.72it/s]2760754it [03:53, 6748.47it/s]2979791it [03:53, 22067.73it/s]2514989it [03:53, 20911.48it/s]2477153it [03:52, 21771.07it/s]2762927it [03:53, 8472.47it/s]2982000it [03:53, 22048.90it/s]2517082it [03:53, 20795.56it/s]2479333it [03:52, 21736.61it/s]2765123it [03:53, 10398.79it/s]2984207it [03:53, 21865.18it/s]2519244it [03:53, 21038.20it/s]2481509it [03:52, 21440.15it/s]2767375it [03:53, 12453.06it/s]2986469it [03:53, 22087.12it/s]2521356it [03:53, 21060.68it/s]2483655it [03:52, 21426.67it/s]2769567it [03:53, 14077.48it/s]2988679it [03:53, 21982.74it/s]2523463it [03:53, 20902.28it/s]2485799it [03:52, 20974.98it/s]2771710it [03:53, 15647.53it/s]2990931it [03:53, 22141.53it/s]2525554it [03:53, 20782.45it/s]2487930it [03:52, 21072.50it/s]2773935it [03:53, 17198.08it/s]2993199it [03:53, 22300.51it/s]2527645it [03:53, 20819.07it/s]2490097it [03:52, 21246.45it/s]2776101it [03:53, 18154.72it/s]2995430it [03:53, 22288.76it/s]2529728it [03:53, 20817.52it/s]2492272it [03:53, 21394.39it/s]2778366it [03:54, 19333.67it/s]2997660it [03:54, 21954.49it/s]2531887it [03:53, 21046.63it/s]2494413it [03:53, 20360.42it/s]2780571it [03:54, 20071.92it/s]2999913it [03:54, 22120.79it/s]2533992it [03:54, 20759.13it/s]2496647it [03:53, 20928.55it/s]2782785it [03:54, 20650.70it/s]3002127it [03:54, 22022.17it/s]2536069it [03:54, 20672.51it/s]2498778it [03:53, 21038.45it/s]2784980it [03:54, 20861.54it/s]3004331it [03:54, 21943.77it/s]2538137it [03:54, 20505.35it/s]2500929it [03:53, 21176.17it/s]2787158it [03:54, 20900.25it/s]3006526it [03:54, 21755.26it/s]2540236it [03:54, 20647.82it/s]2503057it [03:53, 21205.16it/s]2789340it [03:54, 21166.07it/s]3008703it [03:54, 21638.21it/s]2542302it [03:54, 20535.30it/s]2505306it [03:53, 21582.41it/s]2791503it [03:54, 21209.83it/s]3010868it [03:54, 21426.36it/s]2544392it [03:54, 20641.15it/s]2507486it [03:53, 21646.05it/s]2793657it [03:54, 21290.39it/s]3013012it [03:54, 21365.63it/s]2546511it [03:54, 20803.57it/s]2509653it [03:53, 21630.42it/s]2795822it [03:54, 21393.07it/s]3015174it [03:54, 21439.99it/s]2548680it [03:54, 21065.09it/s]2511818it [03:53, 21373.24it/s]2797978it [03:55, 21129.75it/s]3017331it [03:54, 21476.50it/s]2550836it [03:54, 21210.83it/s]2513957it [03:54, 21320.82it/s]2800176it [03:55, 21376.79it/s]3019484it [03:55, 21482.39it/s]2552958it [03:54, 21128.74it/s]2516093it [03:54, 21329.89it/s]2802323it [03:55, 21334.46it/s]3021741it [03:55, 21806.14it/s]2555072it [03:55, 20916.85it/s]2518227it [03:54, 20886.08it/s]2804463it [03:55, 21345.65it/s]3023922it [03:55, 21607.77it/s]2557165it [03:55, 20133.23it/s]2520456it [03:54, 21298.49it/s]2806602it [03:55, 21273.23it/s]3026084it [03:55, 21594.35it/s]2559338it [03:55, 20594.54it/s]2522639it [03:54, 21455.01it/s]2808817it [03:55, 21533.24it/s]3028247it [03:55, 21603.12it/s]2561588it [03:55, 21150.60it/s]2524787it [03:54, 21381.89it/s]2811061it [03:55, 21803.08it/s]3030470it [03:55, 21788.08it/s]2563801it [03:55, 21436.41it/s]2526979it [03:54, 21540.41it/s]2813244it [03:55, 21602.39it/s]3032752it [03:55, 22095.77it/s]2566057it [03:55, 21766.69it/s]2529135it [03:54, 20977.28it/s]2815450it [03:55, 21737.43it/s]3034962it [03:55, 21884.19it/s]2568238it [03:55, 21703.96it/s]2531298it [03:54, 21165.91it/s]2817696it [03:55, 21951.74it/s]3037152it [03:55, 21708.19it/s]2570411it [03:55, 21136.06it/s]2533418it [03:55, 21174.53it/s]2819893it [03:56, 21761.66it/s]3039330it [03:55, 21726.87it/s]2572532it [03:55, 21156.95it/s]2535550it [03:55, 21217.22it/s]2822081it [03:56, 21793.99it/s]3041504it [03:56, 21688.59it/s]2574772it [03:55, 21522.51it/s]2537674it [03:55, 20862.54it/s]2824313it [03:56, 21947.80it/s]3043674it [03:56, 21663.58it/s]2577008it [03:56, 21767.86it/s]2539820it [03:55, 21036.30it/s]2826517it [03:56, 21972.17it/s]3045841it [03:56, 21464.68it/s]2579188it [03:56, 21127.94it/s]2541971it [03:55, 21175.49it/s]2828715it [03:56, 21712.03it/s]3047988it [03:56, 21228.69it/s]2581398it [03:56, 21410.14it/s]2544129it [03:55, 21293.48it/s]2830888it [03:56, 21716.91it/s]3050112it [03:56, 21056.85it/s]2583577it [03:56, 21520.79it/s]2833121it [03:56, 21897.26it/s]2546260it [03:55, 20058.08it/s]3052310it [03:56, 21327.50it/s]2585827it [03:56, 21802.88it/s]2835312it [03:56, 21673.33it/s]2548490it [03:55, 20698.54it/s]3054522it [03:56, 21559.61it/s]2588011it [03:56, 21579.88it/s]2837481it [03:56, 21625.01it/s]2550752it [03:55, 21254.88it/s]3056688it [03:56, 21586.92it/s]2590172it [03:56, 21505.31it/s]2839681it [03:56, 21735.53it/s]2552955it [03:55, 21480.55it/s]3058878it [03:56, 21679.47it/s]2592394it [03:56, 21716.62it/s]2841856it [03:57, 21401.61it/s]2555112it [03:56, 21490.53it/s]3061047it [03:56, 21353.86it/s]2594568it [03:56, 21541.64it/s]2844045it [03:57, 21544.64it/s]2557280it [03:56, 21544.92it/s]3063189it [03:57, 21372.24it/s]2596802it [03:56, 21776.10it/s]2846267it [03:57, 21744.33it/s]2559468it [03:56, 21642.38it/s]3065371it [03:57, 21502.02it/s]2598981it [03:57, 21615.50it/s]2848443it [03:57, 21464.65it/s]2561683it [03:56, 21792.52it/s]3067534it [03:57, 21537.14it/s]2601144it [03:57, 20875.95it/s]2563938it [03:56, 22015.88it/s]2850591it [03:57, 21237.66it/s]3069715it [03:57, 21617.36it/s]2603333it [03:57, 21168.39it/s]2852716it [03:57, 21165.51it/s]2566142it [03:56, 21893.61it/s]3071878it [03:57, 21554.18it/s]2605537it [03:57, 21423.15it/s]2568382it [03:56, 22043.81it/s]2854899it [03:57, 21358.61it/s]3074034it [03:57, 21488.33it/s]2607757it [03:57, 21651.51it/s]2570588it [03:56, 21893.99it/s]2857036it [03:57, 21142.06it/s]3076275it [03:57, 21761.45it/s]2609926it [03:57, 21220.48it/s]2572779it [03:56, 21812.65it/s]2859152it [03:57, 21126.72it/s]3078452it [03:57, 21694.58it/s]2612112it [03:57, 21407.43it/s]2574961it [03:56, 21810.95it/s]2861266it [03:57, 21113.23it/s]3080622it [03:57, 21683.12it/s]2614320it [03:57, 21604.36it/s]2577199it [03:57, 21979.82it/s]2863378it [03:58, 20862.87it/s]3082791it [03:57, 21578.48it/s]2616494it [03:57, 21642.82it/s]2579451it [03:57, 22139.88it/s]2865595it [03:58, 21244.73it/s]3084957it [03:58, 21386.52it/s]2618727it [03:57, 21764.45it/s]2581724it [03:57, 22314.99it/s]2867721it [03:58, 21162.80it/s]3087115it [03:58, 21443.23it/s]2620905it [03:58, 20697.22it/s]2869915it [03:58, 21393.43it/s]2583956it [03:57, 21930.44it/s]3089284it [03:58, 21514.86it/s]2623090it [03:58, 21025.84it/s]2586177it [03:57, 22012.65it/s]2872056it [03:58, 21137.84it/s]3091445it [03:58, 21541.62it/s]2625362it [03:58, 21517.23it/s]2588380it [03:57, 21907.11it/s]2874244it [03:58, 21354.50it/s]3093600it [03:58, 21523.40it/s]2627617it [03:58, 21812.82it/s]2876412it [03:58, 21447.15it/s]3095799it [03:58, 21660.75it/s]2590572it [03:57, 21572.06it/s]2629840it [03:58, 21935.50it/s]2592780it [03:57, 21720.78it/s]2878558it [03:58, 21117.14it/s]3097966it [03:58, 21195.99it/s]2632093it [03:58, 22110.23it/s]2595076it [03:57, 22084.81it/s]2880798it [03:58, 21495.70it/s]3100124it [03:58, 21283.10it/s]2634308it [03:58, 22075.93it/s]2882950it [03:58, 21428.95it/s]3102333it [03:58, 21521.52it/s]2597286it [03:57, 21718.75it/s]2636556it [03:58, 22195.19it/s]3104517it [03:58, 21603.78it/s]2885095it [03:59, 21152.77it/s]2599460it [03:58, 21290.31it/s]2638778it [03:58, 22129.87it/s]3106770it [03:59, 21878.60it/s]2887324it [03:59, 21484.63it/s]2601592it [03:58, 21190.19it/s]2640993it [03:59, 22102.90it/s]3108959it [03:59, 21881.31it/s]2889520it [03:59, 21624.05it/s]2603757it [03:58, 21323.89it/s]2643205it [03:59, 22025.01it/s]3111148it [03:59, 21628.70it/s]2891684it [03:59, 21401.72it/s]2605891it [03:58, 20879.67it/s]2645409it [03:59, 22009.58it/s]3113323it [03:59, 21662.98it/s]2893826it [03:59, 21396.63it/s]2608095it [03:58, 21216.11it/s]2647611it [03:59, 21550.71it/s]2896038it [03:59, 21609.66it/s]3115491it [03:59, 21260.17it/s]2610279it [03:58, 21399.40it/s]2649769it [03:59, 21548.75it/s]2898200it [03:59, 21601.85it/s]3117683it [03:59, 21452.08it/s]2612425it [03:58, 21410.45it/s]2651926it [03:59, 21456.80it/s]2900361it [03:59, 21168.42it/s]3119830it [03:59, 20995.61it/s]2614568it [03:58, 21313.52it/s]2654073it [03:59, 21176.40it/s]2902524it [03:59, 21300.55it/s]3121967it [03:59, 21102.71it/s]2616765it [03:58, 21507.27it/s]2656257it [03:59, 21368.45it/s]2904665it [03:59, 21331.64it/s]2618950it [03:58, 21607.63it/s]3124089it [03:59, 21012.47it/s]2658396it [03:59, 21336.51it/s]2906800it [04:00, 21080.88it/s]2621175it [03:59, 21793.08it/s]3126225it [03:59, 21112.37it/s]2660533it [03:59, 21345.60it/s]2908924it [04:00, 21126.18it/s]2623355it [03:59, 21787.27it/s]3128402it [04:00, 21304.88it/s]2662669it [04:00, 21214.65it/s]2625585it [03:59, 21934.82it/s]2911115it [04:00, 21282.80it/s]3130534it [04:00, 20983.94it/s]2664791it [04:00, 21002.64it/s]2627857it [03:59, 22168.26it/s]2913245it [04:00, 20995.63it/s]3132700it [04:00, 21180.60it/s]2667026it [04:00, 21399.69it/s]2630075it [03:59, 22113.24it/s]2915449it [04:00, 21302.14it/s]3134820it [04:00, 20893.87it/s]2669261it [04:00, 21681.35it/s]2632287it [03:59, 22011.21it/s]2917694it [04:00, 21642.10it/s]3137086it [04:00, 21412.57it/s]2671438it [04:00, 21705.34it/s]2634528it [03:59, 22126.88it/s]2919911it [04:00, 21796.35it/s]3139230it [04:00, 21336.87it/s]2673610it [04:00, 21514.88it/s]2636766it [03:59, 22200.88it/s]2922092it [04:00, 21507.92it/s]3141369it [04:00, 21351.65it/s]2675862it [04:00, 21812.39it/s]2639069it [03:59, 22447.30it/s]2924339it [04:00, 21790.13it/s]3143574it [04:00, 21558.62it/s]2678131it [04:00, 22071.43it/s]2641314it [03:59, 22390.43it/s]2926547it [04:00, 21874.11it/s]3145731it [04:00, 21347.88it/s]2680339it [04:00, 21778.43it/s]2643554it [04:00, 22164.84it/s]2928736it [04:01, 21714.72it/s]3147948it [04:01, 21589.71it/s]2682552it [04:00, 21879.87it/s]2645772it [04:00, 21976.73it/s]2930944it [04:01, 21822.39it/s]3150108it [04:01, 21581.25it/s]2684742it [04:01, 21751.18it/s]2647971it [04:00, 21880.97it/s]2933127it [04:01, 21639.72it/s]3152275it [04:01, 21603.35it/s]2686918it [04:01, 21351.76it/s]2650160it [04:00, 21557.87it/s]3154436it [04:01, 21390.50it/s]2935292it [04:01, 21216.84it/s]2689174it [04:01, 21704.89it/s]2652325it [04:00, 21485.14it/s]3156600it [04:01, 21461.31it/s]2937416it [04:01, 21193.64it/s]2691394it [04:01, 21848.26it/s]3158747it [04:01, 21426.49it/s]2654475it [04:00, 21308.16it/s]2939553it [04:01, 21244.01it/s]2693581it [04:01, 21652.09it/s]3160928it [04:01, 21539.17it/s]2656680it [04:00, 21525.07it/s]2941679it [04:01, 21217.76it/s]2695849it [04:01, 21954.89it/s]3163083it [04:01, 21232.14it/s]2658834it [04:00, 21236.49it/s]2943802it [04:01, 21023.91it/s]2698046it [04:01, 21485.16it/s]3165321it [04:01, 21571.15it/s]2660959it [04:00, 21198.87it/s]2945938it [04:01, 21122.55it/s]2700208it [04:01, 21515.04it/s]3167567it [04:01, 21833.53it/s]2663118it [04:01, 21312.84it/s]2948091it [04:02, 21242.50it/s]2702449it [04:01, 21777.43it/s]3169784it [04:02, 21933.25it/s]2665277it [04:01, 21393.08it/s]2950235it [04:02, 21298.08it/s]2704693it [04:01, 21971.46it/s]3171999it [04:02, 21996.85it/s]2952468it [04:02, 21606.01it/s]2667417it [04:01, 21251.91it/s]2706903it [04:02, 22008.97it/s]2954650it [04:02, 21668.66it/s]2669686it [04:01, 21678.65it/s]3174200it [04:02, 21475.67it/s]2709111it [04:02, 22027.71it/s]3176368it [04:02, 21535.42it/s]2956818it [04:02, 21197.88it/s]2671855it [04:01, 20649.44it/s]2711315it [04:02, 21897.57it/s]2959030it [04:02, 21468.63it/s]3178524it [04:02, 21482.86it/s]2674124it [04:01, 21237.50it/s]2713610it [04:02, 22207.86it/s]2961302it [04:02, 21838.75it/s]3180700it [04:02, 21564.36it/s]2676407it [04:01, 21697.86it/s]2715832it [04:02, 21791.69it/s]2963499it [04:02, 21875.27it/s]2678649it [04:01, 21909.24it/s]3182858it [04:02, 20860.75it/s]2718014it [04:02, 21670.15it/s]2965688it [04:02, 21587.33it/s]2680888it [04:01, 22049.64it/s]3184987it [04:02, 20984.37it/s]2720183it [04:02, 21474.26it/s]2967921it [04:02, 21805.17it/s]3187215it [04:02, 21361.61it/s]2683098it [04:01, 21921.41it/s]2722347it [04:02, 21521.09it/s]2970119it [04:03, 21855.90it/s]3189466it [04:02, 21700.15it/s]2685294it [04:02, 20874.35it/s]2724556it [04:02, 21689.23it/s]2972306it [04:03, 21727.09it/s]3191640it [04:03, 21626.17it/s]2687508it [04:02, 21234.14it/s]2726737it [04:02, 21724.26it/s]2974480it [04:03, 21640.57it/s]3193805it [04:03, 21464.41it/s]2689748it [04:02, 21570.83it/s]2728911it [04:03, 21532.53it/s]2976645it [04:03, 21507.02it/s]3195954it [04:03, 21245.52it/s]2692019it [04:02, 21904.64it/s]2731179it [04:03, 21871.45it/s]2978797it [04:03, 21411.84it/s]3198121it [04:03, 21367.88it/s]2694293it [04:02, 22149.10it/s]2733367it [04:03, 21760.78it/s]2980988it [04:03, 21557.15it/s]3200259it [04:03, 21122.37it/s]2696513it [04:02, 21234.09it/s]2735572it [04:03, 21845.91it/s]2983145it [04:03, 21535.76it/s]3202463it [04:03, 21391.35it/s]2698650it [04:02, 21271.05it/s]2737758it [04:03, 21543.66it/s]2985299it [04:03, 21525.04it/s]3204604it [04:03, 21197.76it/s]2700915it [04:02, 21673.46it/s]2987452it [04:03, 21490.57it/s]3206853it [04:03, 21576.84it/s]2703183it [04:02, 21969.23it/s]2989668it [04:03, 21688.22it/s]3209015it [04:03, 21585.31it/s]2705385it [04:02, 21070.01it/s]2991883it [04:04, 21824.09it/s]3211175it [04:03, 21583.63it/s]2707654it [04:03, 21535.02it/s]2994066it [04:04, 21644.63it/s]3213335it [04:04, 21405.00it/s]2709935it [04:03, 21902.56it/s]2996235it [04:04, 21648.35it/s]3215477it [04:04, 21382.87it/s]2712175it [04:03, 22047.82it/s]2998448it [04:04, 21789.62it/s]3217616it [04:04, 21217.88it/s]2714468it [04:03, 22305.68it/s]3000628it [04:04, 21323.10it/s]3219757it [04:04, 21265.51it/s]2716703it [04:03, 22093.56it/s]3002763it [04:04, 21300.61it/s]3221927it [04:04, 21359.04it/s]2718916it [04:03, 20896.72it/s]3004913it [04:04, 21359.09it/s]3224064it [04:04, 21346.47it/s]2721099it [04:03, 21160.93it/s]3007051it [04:04, 20942.89it/s]3226211it [04:04, 21382.59it/s]2723312it [04:03, 21440.56it/s]3009195it [04:04, 21088.28it/s]3228350it [04:04, 21337.92it/s]2725580it [04:03, 21800.12it/s]3011320it [04:04, 21133.29it/s]3230545it [04:04, 21519.84it/s]2727815it [04:04, 21960.41it/s]3013439it [04:05, 21147.10it/s]3232773it [04:04, 21746.70it/s]2730066it [04:04, 22121.85it/s]3234974it [04:05, 21824.76it/s]3015555it [04:05, 20728.18it/s]2732283it [04:04, 21236.61it/s]3017719it [04:05, 20994.69it/s]3237157it [04:05, 21510.28it/s]2734554it [04:04, 21662.27it/s]3019853it [04:05, 21094.37it/s]3239333it [04:05, 21583.21it/s]2736762it [04:04, 21782.24it/s]3021965it [04:05, 21076.88it/s]3241493it [04:05, 21350.25it/s]3024137it [04:05, 21267.28it/s]3243679it [04:05, 21500.14it/s]3026265it [04:05, 21183.15it/s]3245830it [04:05, 21384.12it/s]3028384it [04:05, 20901.76it/s]3247970it [04:05, 21286.18it/s]3030555it [04:05, 21139.40it/s]3250137it [04:05, 21397.56it/s]3032800it [04:05, 21527.54it/s]3252321it [04:05, 21528.72it/s]3034969it [04:06, 21572.88it/s]3254475it [04:05, 21360.24it/s]3256612it [04:06, 21103.89it/s]3037128it [04:06, 20787.33it/s]3258724it [04:06, 21057.61it/s]3039236it [04:06, 20870.28it/s]3260844it [04:06, 21098.56it/s]3041339it [04:06, 20906.06it/s]3263002it [04:06, 21240.51it/s]3043434it [04:06, 20612.26it/s]3265277it [04:06, 21688.10it/s]3045516it [04:06, 20635.68it/s]3267447it [04:06, 21613.76it/s]3047650it [04:06, 20841.15it/s]3269629it [04:06, 21672.78it/s]3049736it [04:06, 20456.79it/s]3271797it [04:06, 21418.03it/s]3051951it [04:06, 20952.68it/s]3273959it [04:06, 21475.11it/s]3054110it [04:06, 21140.34it/s]3276108it [04:06, 21364.06it/s]3056283it [04:07, 21312.19it/s]3278279it [04:07, 21466.21it/s]3058416it [04:07, 20964.21it/s]3280445it [04:07, 21521.82it/s]3060567it [04:07, 21124.11it/s]3282702it [04:07, 21832.31it/s]3062695it [04:07, 21167.38it/s]3284928it [04:07, 21920.04it/s]3064814it [04:07, 20892.06it/s]3287218it [04:07, 22199.90it/s]3066930it [04:07, 20969.09it/s]3289439it [04:07, 22119.34it/s]3069077it [04:07, 21117.05it/s]3291652it [04:07, 22039.06it/s]3071190it [04:07, 20958.01it/s]3293857it [04:07, 21922.59it/s]3073367it [04:07, 21198.33it/s]3296050it [04:07, 21788.58it/s]3075596it [04:07, 21520.27it/s]3298230it [04:07, 21758.40it/s]3077749it [04:08, 21513.20it/s]3300406it [04:08, 21505.51it/s]3079901it [04:08, 21283.92it/s]3302568it [04:08, 21520.84it/s]3082031it [04:08, 21272.84it/s]3304721it [04:08, 21489.12it/s]3084165it [04:08, 21290.10it/s]3306871it [04:08, 21482.25it/s]3086347it [04:08, 21446.42it/s]3309020it [04:08, 21409.30it/s]3088493it [04:08, 21425.50it/s]3311162it [04:08, 21366.36it/s]3090636it [04:08, 21418.84it/s]3313299it [04:08, 21328.52it/s]3092779it [04:08, 21346.31it/s]3315445it [04:08, 21364.91it/s]3094914it [04:08, 21262.75it/s]3317582it [04:08, 21282.10it/s]3097041it [04:09, 21187.14it/s]3319819it [04:09, 21606.23it/s]3099189it [04:09, 21272.24it/s]3321980it [04:09, 21563.09it/s]3101317it [04:09, 21148.33it/s]3324284it [04:09, 22003.02it/s]3103433it [04:09, 21035.79it/s]3326485it [04:09, 21994.42it/s]3105646it [04:09, 21351.09it/s]3328685it [04:09, 21742.51it/s]3107790it [04:09, 21375.68it/s]3330860it [04:09, 21581.74it/s]3109990it [04:09, 21560.40it/s]3333019it [04:09, 21497.46it/s]3112147it [04:09, 21491.93it/s]3335170it [04:09, 21472.52it/s]3114297it [04:09, 20804.77it/s]3337371it [04:09, 21630.05it/s]3116418it [04:09, 20920.76it/s]3339535it [04:09, 21622.51it/s]3118568it [04:10, 21089.34it/s]3341813it [04:10, 21965.89it/s]3120680it [04:10, 21063.28it/s]3344010it [04:10, 21875.47it/s]3122796it [04:10, 21060.80it/s]3346234it [04:10, 21980.55it/s]3124944it [04:10, 21184.08it/s]3348433it [04:10, 21670.17it/s]3127064it [04:10, 20991.64it/s]3350602it [04:10, 21629.71it/s]3129214it [04:10, 21140.84it/s]3352766it [04:10, 21620.27it/s]3131329it [04:10, 21053.10it/s]3354976it [04:10, 21760.47it/s]3133435it [04:10, 21007.07it/s]3357168it [04:10, 21725.20it/s]3135601it [04:10, 21200.59it/s]3359474it [04:10, 22121.50it/s]3137874it [04:10, 21656.36it/s]3361687it [04:10, 21874.20it/s]3140041it [04:11, 21562.07it/s]3363876it [04:11, 21876.96it/s]3142198it [04:11, 21315.37it/s]3366065it [04:11, 21637.66it/s]3144375it [04:11, 21448.80it/s]3368269it [04:11, 21753.36it/s]3146521it [04:11, 21361.22it/s]3370446it [04:11, 21660.45it/s]3148720it [04:11, 21547.50it/s]3372613it [04:11, 21635.45it/s]3150876it [04:11, 21334.07it/s]3374808it [04:11, 21655.79it/s]3153036it [04:11, 21361.84it/s]3376974it [04:11, 21572.07it/s]3155218it [04:11, 21495.57it/s]3379143it [04:11, 21604.76it/s]3157369it [04:11, 21275.16it/s]3381304it [04:11, 21523.47it/s]3159538it [04:11, 21395.52it/s]3383488it [04:11, 21613.61it/s]3161730it [04:12, 21548.73it/s]3385716it [04:12, 21809.53it/s]3163957it [04:12, 21716.52it/s]3387963it [04:12, 22004.22it/s]3166137it [04:12, 21738.69it/s]3390164it [04:12, 21849.59it/s]3168362it [04:12, 21889.98it/s]3392416it [04:12, 22047.12it/s]3170637it [04:12, 22146.28it/s]3394622it [04:12, 21751.84it/s]3172852it [04:12, 21984.80it/s]3396799it [04:12, 21726.79it/s]3175051it [04:12, 21617.90it/s]3398973it [04:12, 21552.55it/s]3177215it [04:12, 21599.51it/s]3401129it [04:12, 21502.09it/s]3179376it [04:12, 21567.54it/s]3403304it [04:12, 21573.19it/s]3181534it [04:12, 21351.43it/s]3405527it [04:12, 21768.22it/s]3183670it [04:13, 21286.39it/s]3407705it [04:13, 21553.72it/s]3185800it [04:13, 21215.25it/s]3409885it [04:13, 21625.09it/s]3188060it [04:13, 21624.01it/s]3412049it [04:13, 21308.67it/s]3190224it [04:13, 21550.94it/s]3414261it [04:13, 21545.30it/s]3192381it [04:13, 21554.11it/s]3416417it [04:13, 21401.19it/s]3194537it [04:13, 21378.98it/s]3418580it [04:13, 21466.40it/s]3196688it [04:13, 21417.65it/s]3420728it [04:13, 21426.98it/s]3198831it [04:13, 21106.89it/s]3422872it [04:13, 21418.15it/s]3200943it [04:13, 21094.69it/s]3425015it [04:13, 21315.17it/s]3203122it [04:13, 21299.46it/s]3427196it [04:13, 21461.75it/s]3205253it [04:14, 21200.38it/s]3429408it [04:14, 21614.68it/s]3207409it [04:14, 21306.84it/s]3431645it [04:14, 21839.43it/s]3209596it [04:14, 21471.63it/s]3433830it [04:14, 21547.53it/s]3211778it [04:14, 21573.99it/s]3435986it [04:14, 21535.66it/s]3213936it [04:14, 21332.47it/s]3438141it [04:14, 21350.90it/s]3216071it [04:14, 21163.41it/s]3440328it [04:14, 21448.58it/s]3218209it [04:14, 21227.02it/s]3442538it [04:14, 21641.30it/s]3220373it [04:14, 21349.36it/s]3444802it [04:14, 21937.99it/s]3222519it [04:14, 21380.37it/s]3447034it [04:14, 22050.18it/s]3224658it [04:14, 21221.57it/s]3449280it [04:14, 22172.17it/s]3226781it [04:15, 21214.55it/s]3451532it [04:15, 22275.92it/s]3228903it [04:15, 21097.85it/s]3453768it [04:15, 22248.02it/s]3231094it [04:15, 21339.01it/s]3456003it [04:15, 22277.78it/s]3233281it [04:15, 21496.39it/s]3458231it [04:15, 21785.85it/s]3235432it [04:15, 21377.99it/s]3460441it [04:15, 21878.07it/s]3237571it [04:15, 21290.90it/s]3462631it [04:15, 21871.65it/s]3239701it [04:15, 21107.29it/s]3464901it [04:15, 22116.84it/s]3241813it [04:15, 21084.11it/s]3467114it [04:15, 21911.75it/s]3243922it [04:15, 21065.25it/s]3469307it [04:15, 21880.23it/s]3246052it [04:15, 21132.05it/s]3471496it [04:15, 21638.14it/s]3248166it [04:16, 21057.86it/s]3473661it [04:16, 21614.27it/s]3250319it [04:16, 21196.74it/s]3252475it [04:16, 21201.04it/s]3254628it [04:16, 21297.37it/s]3256758it [04:16, 21162.66it/s]3258875it [04:16, 21149.23it/s]3260991it [04:16, 21035.08it/s]3263149it [04:16, 21194.67it/s]3265400it [04:16, 21585.79it/s]3267559it [04:17, 21419.52it/s]3269702it [04:17, 21314.72it/s]3271834it [04:17, 21136.61it/s]3273949it [04:17, 21045.99it/s]3276054it [04:17, 21009.13it/s]3278198it [04:17, 21051.13it/s]3280344it [04:17, 21170.10it/s]3282525it [04:17, 21358.56it/s]3284778it [04:17, 21701.76it/s]3287012it [04:17, 21890.22it/s]3289202it [04:18, 21769.21it/s]3291380it [04:18, 21650.28it/s]3293546it [04:18, 21465.73it/s]3295694it [04:18, 21373.92it/s]3297832it [04:18, 21191.97it/s]3299952it [04:18, 21182.92it/s]3302071it [04:18, 21039.07it/s]3304179it [04:18, 21049.13it/s]3306285it [04:18, 21008.85it/s]3308388it [04:18, 21014.08it/s]3310490it [04:19, 20969.00it/s]3312622it [04:19, 21072.23it/s]3314730it [04:19, 20955.88it/s]3316839it [04:19, 20882.43it/s]3319069it [04:19, 21182.02it/s]3321276it [04:19, 21443.10it/s]3323499it [04:19, 21675.69it/s]3325695it [04:19, 21757.78it/s]3327872it [04:19, 21581.69it/s]3330031it [04:19, 21412.89it/s]3332173it [04:20, 21223.73it/s]3334296it [04:20, 21208.75it/s]3336418it [04:20, 21122.54it/s]3338593it [04:20, 21308.41it/s]3340738it [04:20, 21349.54it/s]3342903it [04:20, 21436.84it/s]3345081it [04:20, 21538.39it/s]3347236it [04:20, 21347.06it/s]3349372it [04:20, 21269.30it/s]3351500it [04:20, 21155.90it/s]3353718it [04:21, 21458.33it/s]3355865it [04:21, 21337.41it/s]3358046it [04:21, 21474.90it/s]3360239it [04:21, 21608.59it/s]3362401it [04:21, 21387.10it/s]3364541it [04:21, 21254.59it/s]3366668it [04:21, 21206.48it/s]3368810it [04:21, 21268.88it/s]3370938it [04:21, 21146.75it/s]3373053it [04:21, 21096.39it/s]3375163it [04:22, 21011.81it/s]3377273it [04:22, 21035.17it/s]3379377it [04:22, 20930.28it/s]3381473it [04:22, 20936.49it/s]3383567it [04:22, 20915.99it/s]3385772it [04:22, 21161.98it/s]3388039it [04:22, 21608.94it/s]3390201it [04:22, 21449.67it/s]3392366it [04:22, 21506.72it/s]3394517it [04:22, 21338.77it/s]3396652it [04:23, 21189.13it/s]3398772it [04:23, 21118.23it/s]3400885it [04:23, 20961.62it/s]3403051it [04:23, 21166.69it/s]3405183it [04:23, 21209.85it/s]3407309it [04:23, 21222.24it/s]3409432it [04:23, 21078.71it/s]3411541it [04:23, 21019.13it/s]3413644it [04:23, 20955.00it/s]3415764it [04:23, 21026.51it/s]3417867it [04:24, 20989.97it/s]3420048it [04:24, 21232.96it/s]3422172it [04:24, 20930.96it/s]3424344it [04:24, 21162.50it/s]3426462it [04:24, 21044.01it/s]3428568it [04:24, 20272.67it/s]3430854it [04:24, 21020.49it/s]3432990it [04:24, 21116.93it/s]3435154it [04:24, 21268.36it/s]3437290it [04:24, 21293.68it/s]3439479it [04:25, 21383.97it/s]3441620it [04:25, 21228.60it/s]3443864it [04:25, 21585.97it/s]3446024it [04:25, 20737.13it/s]3448243it [04:25, 21156.72it/s]3450482it [04:25, 21517.38it/s]3452776it [04:25, 21935.49it/s]3455012it [04:25, 22061.00it/s]3457222it [04:25, 21835.64it/s]3459409it [04:26, 20617.78it/s]3461613it [04:26, 21021.85it/s]3463841it [04:26, 21384.61it/s]3466082it [04:26, 21683.54it/s]3468259it [04:26, 21280.45it/s]3470394it [04:26, 20775.79it/s]3472478it [04:26, 19910.08it/s]3474661it [04:26, 20454.14it/s]2737758it [04:28, 21543.66it/s]2738918it [04:28, 246.71it/s]  2741096it [04:28, 366.69it/s]2743384it [04:28, 544.46it/s]2745601it [04:29, 785.69it/s]2747815it [04:29, 1120.64it/s]2750061it [04:29, 1588.32it/s]2752280it [04:29, 2212.31it/s]2754485it [04:29, 3029.40it/s]2756656it [04:29, 4068.74it/s]2758925it [04:29, 5444.16it/s]2761116it [04:29, 7009.85it/s]2763317it [04:29, 8808.24it/s]2765515it [04:29, 10734.07it/s]2767719it [04:30, 12687.55it/s]2769915it [04:30, 14473.52it/s]2772100it [04:30, 15988.68it/s]2774288it [04:30, 17387.89it/s]2776575it [04:30, 18776.32it/s]2778783it [04:30, 19651.63it/s]2780993it [04:30, 20323.63it/s]2783246it [04:30, 20945.69it/s]2785467it [04:30, 21247.03it/s]2787682it [04:30, 21151.02it/s]2789927it [04:31, 21523.06it/s]2792125it [04:31, 21591.57it/s]2794316it [04:31, 21520.50it/s]2796491it [04:31, 21488.89it/s]2798656it [04:31, 21523.63it/s]2800820it [04:31, 21485.73it/s]2802977it [04:31, 21425.35it/s]2805174it [04:31, 21585.73it/s]2736762it [04:31, 21782.24it/s]2738918it [04:31, 274.38it/s]  2807350it [04:31, 21636.76it/s]2741070it [04:31, 386.82it/s]2809582it [04:31, 21838.65it/s]2743364it [04:31, 556.04it/s]2811803it [04:32, 21948.46it/s]2745575it [04:31, 784.63it/s]2814000it [04:32, 21838.47it/s]2747829it [04:31, 1109.31it/s]2816199it [04:32, 21811.88it/s]2750052it [04:31, 1550.07it/s]2818419it [04:32, 21925.36it/s]2752272it [04:31, 2148.00it/s]2820613it [04:32, 21920.22it/s]2754492it [04:31, 2937.29it/s]2822813it [04:32, 21943.37it/s]2756673it [04:31, 3947.01it/s]2825008it [04:32, 21912.04it/s]2758990it [04:31, 5313.07it/s]2827200it [04:32, 21894.29it/s]2761209it [04:32, 6835.04it/s]2829390it [04:32, 21792.87it/s]2763455it [04:32, 8644.93it/s]2831570it [04:32, 21783.09it/s]2765728it [04:32, 10644.49it/s]2833817it [04:33, 21986.91it/s]2767952it [04:32, 12514.87it/s]2836016it [04:33, 21890.63it/s]2770151it [04:32, 14313.37it/s]2838206it [04:33, 21671.72it/s]2772385it [04:32, 16045.18it/s]2840374it [04:33, 21659.12it/s]2774589it [04:32, 17353.20it/s]2842566it [04:33, 21734.76it/s]2776847it [04:32, 18662.98it/s]2844759it [04:33, 21733.79it/s]2779140it [04:32, 19787.89it/s]2846946it [04:33, 21773.12it/s]2781374it [04:32, 20304.64it/s]2849124it [04:33, 21363.94it/s]2783635it [04:33, 20945.44it/s]2851314it [04:33, 21520.77it/s]2785899it [04:33, 21425.78it/s]2853468it [04:34, 20944.13it/s]2788137it [04:33, 21515.71it/s]2855671it [04:34, 21259.11it/s]2790356it [04:33, 21551.18it/s]2857801it [04:34, 21070.01it/s]2792613it [04:33, 21847.79it/s]2859911it [04:34, 20941.02it/s]2794832it [04:33, 21770.90it/s]2862007it [04:34, 20938.75it/s]2797033it [04:33, 21596.61it/s]2864157it [04:34, 21102.78it/s]2799281it [04:33, 21855.28it/s]2866309it [04:34, 21224.28it/s]2801479it [04:33, 21791.06it/s]2868433it [04:34, 21045.90it/s]2803667it [04:33, 21487.86it/s]2870589it [04:34, 21196.56it/s]2805936it [04:34, 21837.43it/s]2872800it [04:34, 21468.08it/s]2808187it [04:34, 22033.19it/s]2875009it [04:35, 21652.74it/s]2810430it [04:34, 22149.16it/s]2877175it [04:35, 21445.96it/s]2812648it [04:34, 22070.76it/s]2879386it [04:35, 21642.33it/s]2814889it [04:34, 22169.43it/s]2881551it [04:35, 21535.42it/s]2817150it [04:34, 22299.09it/s]2883718it [04:35, 21572.90it/s]2819382it [04:34, 22229.25it/s]2885882it [04:35, 21592.04it/s]2821623it [04:34, 22282.32it/s]2888107it [04:35, 21787.20it/s]2823885it [04:34, 22382.86it/s]2890372it [04:35, 22044.17it/s]2826124it [04:34, 22226.48it/s]2892577it [04:35, 21716.37it/s]2828361it [04:35, 22268.67it/s]2894808it [04:35, 21890.93it/s]2830589it [04:35, 22204.47it/s]2896999it [04:36, 21710.70it/s]2832904it [04:35, 22485.85it/s]2899186it [04:36, 21756.51it/s]2835153it [04:35, 22224.77it/s]2901363it [04:36, 21426.80it/s]2837377it [04:35, 22135.21it/s]2903558it [04:36, 21580.34it/s]2839620it [04:35, 22221.22it/s]2905739it [04:36, 21647.65it/s]2841843it [04:35, 21906.30it/s]2907905it [04:36, 21339.40it/s]2844071it [04:35, 22014.77it/s]2910130it [04:36, 21606.04it/s]2846344it [04:35, 22224.65it/s]2912293it [04:36, 21328.27it/s]2848568it [04:36, 21570.62it/s]2914479it [04:36, 21458.73it/s]2850771it [04:36, 21704.48it/s]2916666it [04:36, 21580.13it/s]2852945it [04:36, 21331.01it/s]2918915it [04:37, 21849.47it/s]2855184it [04:36, 21640.21it/s]2921199it [04:37, 22054.69it/s]2857352it [04:36, 21410.06it/s]2923406it [04:37, 21987.72it/s]2859498it [04:36, 21422.19it/s]2925606it [04:37, 21929.96it/s]2861642it [04:36, 21326.23it/s]2927800it [04:37, 21902.60it/s]2863778it [04:36, 21333.43it/s]2929991it [04:37, 21853.07it/s]2866000it [04:36, 21594.24it/s]2932177it [04:37, 21646.59it/s]2868179it [04:36, 21648.71it/s]2934343it [04:37, 21396.01it/s]2870345it [04:37, 21493.87it/s]2936484it [04:37, 21235.22it/s]2872556it [04:37, 21674.45it/s]2938613it [04:37, 21249.02it/s]2874829it [04:37, 21987.35it/s]2940739it [04:38, 21016.85it/s]2877029it [04:37, 21574.48it/s]2942869it [04:38, 21096.57it/s]2879244it [04:37, 21743.29it/s]2944980it [04:38, 20987.32it/s]2881495it [04:37, 21969.96it/s]2947124it [04:38, 21118.52it/s]2883694it [04:37, 21881.78it/s]2949293it [04:38, 21286.79it/s]2885884it [04:37, 21810.18it/s]2951494it [04:38, 21500.20it/s]2888123it [04:37, 21981.74it/s]2953729it [04:38, 21752.15it/s]2890397it [04:37, 22202.94it/s]2955905it [04:38, 21568.63it/s]2892618it [04:38, 21803.96it/s]2958063it [04:38, 21546.00it/s]2894848it [04:38, 21948.08it/s]2960272it [04:38, 21706.42it/s]2897064it [04:38, 22009.74it/s]2962477it [04:39, 21807.20it/s]2899267it [04:38, 21679.92it/s]2964659it [04:39, 21800.59it/s]2901437it [04:38, 21619.16it/s]2966840it [04:39, 21789.73it/s]2903601it [04:38, 21609.77it/s]2969041it [04:39, 21853.59it/s]2905803it [04:38, 21730.92it/s]2971227it [04:39, 21825.75it/s]2907977it [04:38, 21393.90it/s]2973410it [04:39, 21749.44it/s]2910194it [04:38, 21621.27it/s]2975586it [04:39, 21586.20it/s]2912358it [04:38, 21619.14it/s]2977745it [04:39, 21452.66it/s]2914521it [04:39, 21392.19it/s]2979942it [04:39, 21604.24it/s]2916820it [04:39, 21863.99it/s]2982103it [04:39, 21482.56it/s]2919054it [04:39, 22004.96it/s]2984252it [04:40, 21344.26it/s]2921256it [04:39, 21898.67it/s]2986444it [04:40, 21514.33it/s]2923535it [04:39, 22162.79it/s]2988711it [04:40, 21857.35it/s]2925753it [04:39, 22151.22it/s]2990922it [04:40, 21860.96it/s]2928001it [04:39, 22248.81it/s]2993109it [04:40, 21679.30it/s]2930270it [04:39, 22262.66it/s]2995337it [04:40, 21856.63it/s]2932497it [04:39, 22050.66it/s]2997524it [04:40, 21822.70it/s]2934703it [04:39, 21654.92it/s]2999734it [04:40, 21902.53it/s]2936889it [04:40, 21714.95it/s]3001925it [04:40, 21633.62it/s]2939062it [04:40, 21716.33it/s]3004090it [04:40, 21634.63it/s]2941235it [04:40, 21517.96it/s]3006255it [04:41, 21571.08it/s]2943408it [04:40, 21579.84it/s]3008413it [04:41, 21137.99it/s]2945567it [04:40, 21367.27it/s]3010575it [04:41, 21276.34it/s]2947792it [04:40, 21626.87it/s]3012724it [04:41, 21338.05it/s]2950024it [04:40, 21830.74it/s]3014865it [04:41, 21358.52it/s]2952306it [04:40, 22123.66it/s]3017002it [04:41, 21154.19it/s]2954520it [04:40, 22040.64it/s]3019193it [04:41, 21375.36it/s]2956725it [04:40, 21963.38it/s]3021332it [04:41, 20233.55it/s]2958963it [04:41, 22085.44it/s]3023580it [04:41, 20876.72it/s]2961172it [04:41, 22080.40it/s]3025712it [04:42, 21001.17it/s]2963381it [04:41, 22032.32it/s]3027882it [04:42, 21181.35it/s]2965626it [04:41, 22156.12it/s]3030118it [04:42, 21526.75it/s]2967842it [04:41, 22101.28it/s]3032372it [04:42, 21825.04it/s]2970082it [04:41, 22190.12it/s]3034620it [04:42, 22018.85it/s]2972302it [04:41, 22150.74it/s]3036825it [04:42, 21884.25it/s]2974518it [04:41, 22080.75it/s]3039016it [04:42, 21213.29it/s]2976727it [04:41, 21731.06it/s]3041217it [04:42, 21444.61it/s]2979003it [04:41, 22033.61it/s]3043366it [04:42, 21444.21it/s]2981208it [04:42, 21889.91it/s]3045514it [04:42, 21211.20it/s]2983398it [04:42, 21782.60it/s]3047638it [04:43, 21197.15it/s]2985577it [04:42, 21719.94it/s]3049760it [04:43, 20041.30it/s]2987871it [04:42, 22079.20it/s]3051997it [04:43, 20704.20it/s]2990080it [04:42, 21979.40it/s]3054091it [04:43, 20771.52it/s]2992383it [04:42, 22290.33it/s]3056295it [04:43, 21140.29it/s]2994613it [04:42, 22126.04it/s]3058417it [04:43, 21035.39it/s]2996827it [04:42, 22074.50it/s]3060601it [04:43, 21272.16it/s]2999037it [04:42, 22081.30it/s]3062733it [04:43, 21102.54it/s]3001258it [04:42, 22118.82it/s]3064890it [04:43, 21238.71it/s]3003471it [04:43, 21575.88it/s]3067017it [04:43, 21203.25it/s]3005632it [04:43, 21511.50it/s]3069187it [04:44, 21350.61it/s]3007786it [04:43, 21503.80it/s]3071324it [04:44, 21236.30it/s]3009977it [04:43, 21623.18it/s]3073529it [04:44, 21476.45it/s]3012141it [04:43, 21358.43it/s]3075708it [04:44, 21567.24it/s]3014316it [04:43, 21472.66it/s]3077874it [04:44, 21591.88it/s]3016470it [04:43, 21485.02it/s]3080034it [04:44, 21399.70it/s]3018620it [04:43, 21334.01it/s]3082202it [04:44, 21482.28it/s]3020810it [04:43, 21495.40it/s]3084370it [04:44, 21538.57it/s]3023046it [04:44, 21752.17it/s]3086543it [04:44, 21594.92it/s]3025257it [04:44, 21855.98it/s]3088705it [04:44, 21599.73it/s]3027444it [04:44, 21704.56it/s]3090866it [04:45, 21509.99it/s]3029615it [04:44, 21212.76it/s]3093018it [04:45, 21505.36it/s]3031899it [04:44, 21689.70it/s]3095169it [04:45, 21379.83it/s]3034153it [04:44, 21939.52it/s]3097327it [04:45, 21438.14it/s]3036350it [04:44, 21769.79it/s]3099472it [04:45, 21263.29it/s]3038529it [04:44, 21722.32it/s]3101652it [04:45, 21418.86it/s]3040703it [04:44, 21684.42it/s]3103850it [04:45, 21585.85it/s]3042898it [04:44, 21760.80it/s]3106082it [04:45, 21802.99it/s]3045075it [04:45, 21278.72it/s]3108267it [04:45, 21814.98it/s]3047206it [04:45, 21228.83it/s]3473661it [04:46, 21614.27it/s]3475678it [04:46, 236.34it/s]  3110465it [04:45, 21862.24it/s]3049331it [04:45, 20372.76it/s]3477830it [04:46, 336.53it/s]3112652it [04:46, 21643.87it/s]3051560it [04:45, 20923.08it/s]3480068it [04:46, 483.77it/s]3114817it [04:46, 21625.50it/s]3053710it [04:45, 21086.42it/s]3482337it [04:46, 693.67it/s]3116980it [04:46, 21432.64it/s]3055889it [04:45, 21291.63it/s]3484546it [04:46, 978.53it/s]3119156it [04:46, 21526.91it/s]3058110it [04:45, 21560.83it/s]3486745it [04:46, 1370.77it/s]3121310it [04:46, 21374.04it/s]3060273it [04:45, 21580.28it/s]3488985it [04:46, 1917.13it/s]3123492it [04:46, 21506.04it/s]3062434it [04:45, 21361.21it/s]3491217it [04:46, 2647.29it/s]3125644it [04:46, 21378.51it/s]3064629it [04:45, 21526.94it/s]3493433it [04:46, 3592.64it/s]3127833it [04:46, 21527.63it/s]3066803it [04:46, 21589.18it/s]3495635it [04:46, 4788.42it/s]3129987it [04:46, 21397.70it/s]3069019it [04:46, 21757.46it/s]3497837it [04:47, 6238.36it/s]3132128it [04:46, 21335.49it/s]3500024it [04:47, 7917.92it/s]3071196it [04:46, 20220.87it/s]3134262it [04:47, 21237.05it/s]3502227it [04:47, 9801.13it/s]3073411it [04:46, 20766.28it/s]3136525it [04:47, 21650.10it/s]3504448it [04:47, 11788.10it/s]3075693it [04:46, 21355.81it/s]3138759it [04:47, 21854.67it/s]3506703it [04:47, 13790.51it/s]3077904it [04:46, 21573.98it/s]3140945it [04:47, 21743.85it/s]3508918it [04:47, 15451.90it/s]3080073it [04:46, 21557.51it/s]3143120it [04:47, 21606.29it/s]3511114it [04:47, 16862.04it/s]3082272it [04:46, 21683.44it/s]3145319it [04:47, 21717.56it/s]3513330it [04:47, 18163.47it/s]3084460it [04:46, 21740.52it/s]3147492it [04:47, 21603.40it/s]3515522it [04:47, 18971.54it/s]3086680it [04:46, 21842.61it/s]3149683it [04:47, 21630.24it/s]3517692it [04:47, 19658.81it/s]3088897it [04:47, 21938.64it/s]3151847it [04:47, 21613.89it/s]3519906it [04:48, 20346.18it/s]3091093it [04:47, 21865.66it/s]3154009it [04:47, 21565.44it/s]3522086it [04:48, 20549.76it/s]3093282it [04:47, 21655.11it/s]3156198it [04:48, 21659.98it/s]3524269it [04:48, 20913.58it/s]3095458it [04:47, 21684.30it/s]3158365it [04:48, 21497.55it/s]3526434it [04:48, 21099.29it/s]3097628it [04:47, 21608.27it/s]3160585it [04:48, 21703.78it/s]3528596it [04:48, 21216.31it/s]3099790it [04:47, 21458.50it/s]3162758it [04:48, 21711.31it/s]3530755it [04:48, 21290.20it/s]3101964it [04:47, 21540.72it/s]3165014it [04:48, 21962.08it/s]3532951it [04:48, 21487.07it/s]3104273it [04:47, 22001.10it/s]3167255it [04:48, 22093.45it/s]3535158it [04:48, 21657.61it/s]3106527it [04:47, 22159.40it/s]3169522it [04:48, 22264.12it/s]3537337it [04:48, 21529.04it/s]3108744it [04:47, 21869.46it/s]3171749it [04:48, 22191.33it/s]3539518it [04:48, 21585.61it/s]3111001it [04:48, 22074.20it/s]3173969it [04:48, 22096.75it/s]3541732it [04:49, 21747.92it/s]3113210it [04:48, 21917.50it/s]3176179it [04:49, 21814.40it/s]3543912it [04:49, 21701.03it/s]3115403it [04:48, 21830.22it/s]3178362it [04:49, 21791.45it/s]3546187it [04:49, 22013.30it/s]3117587it [04:48, 21626.24it/s]3180555it [04:49, 21831.57it/s]3548431it [04:49, 22139.29it/s]3119751it [04:48, 21540.53it/s]3182739it [04:49, 21777.70it/s]3550647it [04:49, 22034.32it/s]3121936it [04:48, 21631.76it/s]3184918it [04:49, 21643.03it/s]3552852it [04:49, 21863.80it/s]3124104it [04:48, 21645.43it/s]3187107it [04:49, 21715.65it/s]3555112it [04:49, 22081.74it/s]3126269it [04:48, 21334.26it/s]3189306it [04:49, 21795.85it/s]3557368it [04:49, 22223.80it/s]3128445it [04:48, 21457.55it/s]3191513it [04:49, 21877.24it/s]3559592it [04:49, 21996.87it/s]3130600it [04:48, 21484.21it/s]3193701it [04:49, 21501.03it/s]3561793it [04:49, 21886.66it/s]3132785it [04:49, 21592.33it/s]3195871it [04:49, 21559.58it/s]3563983it [04:50, 21826.46it/s]3134954it [04:49, 21618.75it/s]3198029it [04:50, 21378.67it/s]3566167it [04:50, 21713.84it/s]3137138it [04:49, 21682.74it/s]3200168it [04:50, 21362.29it/s]3568339it [04:50, 21694.80it/s]3139390it [04:49, 21929.13it/s]3202315it [04:50, 21389.04it/s]3570565it [04:50, 21860.61it/s]3141584it [04:49, 21847.39it/s]3204480it [04:50, 21464.97it/s]3572761it [04:50, 21887.20it/s]3143769it [04:49, 21683.02it/s]3206723it [04:50, 21752.61it/s]3574950it [04:50, 21727.28it/s]3145938it [04:49, 21669.84it/s]3208956it [04:50, 21923.76it/s]3577166it [04:50, 21855.42it/s]3148133it [04:49, 21750.97it/s]3211149it [04:50, 21678.24it/s]3579352it [04:50, 21827.59it/s]3150348it [04:49, 21865.94it/s]3213343it [04:50, 21752.53it/s]3581536it [04:50, 21653.01it/s]3152535it [04:50, 21581.95it/s]3215519it [04:50, 21529.79it/s]3583723it [04:50, 21716.24it/s]3154719it [04:50, 21588.29it/s]3217709it [04:50, 21637.02it/s]3585895it [04:51, 21703.59it/s]3156879it [04:50, 21344.43it/s]3219874it [04:51, 21496.08it/s]3588066it [04:51, 21672.93it/s]3159064it [04:50, 21492.78it/s]3222058it [04:51, 21597.82it/s]3590258it [04:51, 21746.24it/s]3161233it [04:50, 21550.83it/s]3224219it [04:51, 21578.49it/s]3592439it [04:51, 21692.98it/s]3163490it [04:50, 21853.09it/s]3226378it [04:51, 21576.90it/s]3594620it [04:51, 21726.83it/s]3165759it [04:50, 22100.80it/s]3228536it [04:51, 21526.77it/s]3596793it [04:51, 21654.25it/s]3168032it [04:50, 22269.00it/s]3230765it [04:51, 21752.48it/s]3599076it [04:51, 22001.70it/s]3170308it [04:50, 22414.05it/s]3232988it [04:51, 21893.52it/s]3601294it [04:51, 22052.58it/s]3172550it [04:50, 22346.76it/s]3235235it [04:51, 22065.25it/s]3603500it [04:51, 21882.12it/s]3174785it [04:51, 22303.44it/s]3237442it [04:51, 21764.18it/s]3605728it [04:52, 21999.18it/s]3177016it [04:51, 21927.02it/s]3239620it [04:51, 21680.91it/s]3607929it [04:52, 21918.04it/s]3179211it [04:51, 21759.12it/s]3241799it [04:52, 21709.96it/s]3610122it [04:52, 21685.30it/s]3181388it [04:51, 21652.15it/s]3243971it [04:52, 21599.84it/s]3612319it [04:52, 21761.98it/s]3183554it [04:51, 21559.63it/s]3246162it [04:52, 21690.61it/s]3614496it [04:52, 21748.26it/s]3185728it [04:51, 21610.65it/s]3248332it [04:52, 21580.26it/s]3616708it [04:52, 21856.01it/s]3187991it [04:51, 21912.58it/s]3250540it [04:52, 21727.89it/s]3618894it [04:52, 21727.76it/s]3190248it [04:51, 22106.10it/s]3252714it [04:52, 21573.41it/s]3621074it [04:52, 21748.43it/s]3192460it [04:51, 22022.28it/s]3254872it [04:52, 21494.72it/s]3623297it [04:52, 21888.64it/s]3194663it [04:51, 21657.83it/s]3257022it [04:52, 20932.77it/s]3625505it [04:52, 21942.74it/s]3196831it [04:52, 21619.43it/s]3259170it [04:52, 21090.81it/s]3627765it [04:53, 22137.25it/s]3199009it [04:52, 21664.70it/s]3261372it [04:52, 21362.08it/s]3629979it [04:53, 22058.04it/s]3201177it [04:52, 21463.59it/s]3263623it [04:53, 21700.58it/s]3632185it [04:53, 21782.46it/s]3203391it [04:52, 21661.29it/s]3265796it [04:53, 21217.04it/s]3634384it [04:53, 21841.57it/s]3205617it [04:52, 21837.33it/s]3267973it [04:53, 21377.01it/s]3636569it [04:53, 21777.92it/s]3207882it [04:52, 22077.07it/s]3270114it [04:53, 21354.96it/s]3638807it [04:53, 21954.01it/s]3210091it [04:52, 21962.36it/s]3272269it [04:53, 21409.32it/s]3641072it [04:53, 22161.15it/s]3212288it [04:52, 21854.34it/s]3274412it [04:53, 21353.97it/s]3643307it [04:53, 22215.22it/s]3214474it [04:52, 21825.58it/s]3276549it [04:53, 20904.89it/s]3645529it [04:53, 22109.49it/s]3216670it [04:52, 21859.67it/s]3278748it [04:53, 21221.36it/s]3647741it [04:53, 21946.00it/s]3218857it [04:53, 21713.06it/s]3280950it [04:53, 21456.75it/s]3649937it [04:54, 21848.70it/s]3221080it [04:53, 21797.15it/s]3283244it [04:53, 21834.95it/s]3652123it [04:54, 21824.05it/s]3223316it [04:53, 21961.83it/s]3285430it [04:54, 21194.92it/s]3654306it [04:54, 21739.29it/s]3225513it [04:53, 21722.82it/s]3287677it [04:54, 21566.45it/s]3656481it [04:54, 21693.14it/s]3227730it [04:53, 21848.69it/s]3289940it [04:54, 21878.08it/s]3658675it [04:54, 21765.60it/s]3229916it [04:53, 21808.36it/s]3292132it [04:54, 21824.56it/s]3660862it [04:54, 21795.71it/s]3232211it [04:53, 22145.40it/s]3294317it [04:54, 21793.66it/s]3663042it [04:54, 21621.13it/s]3234460it [04:53, 22245.23it/s]3296508it [04:54, 21827.20it/s]3665268it [04:54, 21808.97it/s]3236685it [04:53, 22024.38it/s]3298692it [04:54, 20821.20it/s]3667455it [04:54, 21826.01it/s]3238889it [04:53, 21809.89it/s]3300854it [04:54, 21050.09it/s]3669638it [04:54, 21733.61it/s]3241071it [04:54, 21799.30it/s]3303002it [04:54, 21174.48it/s]3671822it [04:55, 21761.45it/s]3243252it [04:54, 21571.33it/s]3305161it [04:54, 21293.99it/s]3673999it [04:55, 21723.61it/s]3474661it [04:55, 20454.14it/s]3475669it [04:55, 210.94it/s]  3245440it [04:54, 21621.07it/s]3307339it [04:55, 21437.39it/s]3676187it [04:55, 21769.82it/s]3477878it [04:55, 319.23it/s]3247631it [04:54, 21705.97it/s]3309496it [04:55, 21475.03it/s]3678417it [04:55, 21926.71it/s]3480021it [04:55, 467.45it/s]3249848it [04:54, 21841.22it/s]3680652it [04:55, 22051.45it/s]3311646it [04:55, 20569.06it/s]3482268it [04:55, 685.95it/s]3252033it [04:54, 21804.85it/s]3682967it [04:55, 22379.14it/s]3313786it [04:55, 20808.29it/s]3484422it [04:55, 977.33it/s]3254214it [04:54, 21715.27it/s]3685206it [04:55, 22337.03it/s]3315961it [04:55, 21082.44it/s]3486588it [04:55, 1382.27it/s]3256386it [04:54, 21614.59it/s]3687441it [04:55, 22338.19it/s]3318142it [04:55, 21296.59it/s]3488732it [04:55, 1927.45it/s]3258548it [04:54, 21545.71it/s]3689675it [04:55, 22330.59it/s]3320388it [04:55, 21640.04it/s]3490933it [04:55, 2678.72it/s]3260703it [04:54, 21526.27it/s]3322664it [04:55, 21971.59it/s]3691909it [04:55, 21985.61it/s]3493088it [04:56, 3633.88it/s]3262992it [04:55, 21931.88it/s]3324947it [04:55, 22224.75it/s]3694109it [04:56, 21909.84it/s]3495236it [04:56, 4838.30it/s]3265279it [04:55, 22211.24it/s]3696301it [04:56, 21822.90it/s]3327172it [04:56, 21543.31it/s]3497382it [04:56, 6277.69it/s]3267501it [04:55, 22012.89it/s]3698484it [04:56, 21435.17it/s]3329349it [04:56, 21608.90it/s]3499525it [04:56, 7963.91it/s]3269703it [04:55, 21898.57it/s]3700668it [04:56, 21551.88it/s]3331515it [04:56, 21610.21it/s]3501652it [04:56, 9770.62it/s]3271894it [04:55, 21790.25it/s]3702882it [04:56, 21723.54it/s]3333679it [04:56, 21578.45it/s]3503830it [04:56, 11734.74it/s]3274074it [04:55, 21603.26it/s]3705056it [04:56, 21714.76it/s]3335839it [04:56, 20694.86it/s]3505966it [04:56, 13519.75it/s]3276235it [04:55, 21578.64it/s]3707229it [04:56, 21551.27it/s]3338070it [04:56, 21160.82it/s]3508177it [04:56, 15344.07it/s]3278413it [04:55, 21636.47it/s]3709389it [04:56, 21564.11it/s]3340350it [04:56, 21638.36it/s]3510329it [04:56, 16585.21it/s]3280646it [04:55, 21840.87it/s]3711591it [04:56, 21697.63it/s]3342606it [04:56, 21908.51it/s]3512501it [04:56, 17854.95it/s]3282858it [04:55, 21922.41it/s]3713762it [04:56, 21500.31it/s]3344845it [04:56, 22049.54it/s]3514637it [04:57, 18589.75it/s]3285147it [04:56, 22211.26it/s]3715970it [04:57, 21671.03it/s]3347068it [04:56, 22101.47it/s]3516793it [04:57, 19390.91it/s]3287432it [04:56, 22399.69it/s]3718155it [04:57, 21723.86it/s]3349281it [04:57, 21967.88it/s]3518919it [04:57, 19758.33it/s]3289687it [04:56, 22442.72it/s]3720328it [04:57, 21451.52it/s]3351480it [04:57, 21635.34it/s]3521049it [04:57, 20193.62it/s]3291932it [04:56, 22076.20it/s]3722610it [04:57, 21856.14it/s]3353709it [04:57, 21827.52it/s]3523164it [04:57, 20342.93it/s]3294153it [04:56, 22114.11it/s]3724853it [04:57, 22025.25it/s]3355929it [04:57, 21937.62it/s]3525288it [04:57, 20600.49it/s]3296366it [04:56, 21957.74it/s]3727057it [04:57, 21963.43it/s]3358210it [04:57, 22196.77it/s]3527397it [04:57, 20680.00it/s]3298563it [04:56, 21848.27it/s]3729255it [04:57, 21823.77it/s]3529530it [04:57, 20869.62it/s]3360431it [04:57, 21293.83it/s]3300749it [04:56, 21711.03it/s]3731458it [04:57, 21883.39it/s]3531691it [04:57, 20913.70it/s]3362594it [04:57, 21389.55it/s]3302921it [04:56, 21677.12it/s]3733647it [04:57, 21808.43it/s]3533839it [04:57, 21080.18it/s]3364740it [04:57, 21408.07it/s]3305090it [04:56, 21643.37it/s]3735829it [04:57, 21777.41it/s]3536015it [04:58, 21280.55it/s]3366968it [04:57, 21663.35it/s]3307286it [04:57, 21735.83it/s]3738007it [04:58, 21719.46it/s]3369174it [04:57, 21778.91it/s]3538152it [04:58, 21075.71it/s]3309460it [04:57, 21513.41it/s]3740199it [04:58, 21778.46it/s]3371382it [04:58, 21866.44it/s]3540322it [04:58, 21259.48it/s]3311658it [04:57, 21650.71it/s]3742377it [04:58, 21578.99it/s]3373571it [04:58, 21784.64it/s]3542453it [04:58, 21086.46it/s]3313824it [04:57, 21578.64it/s]3744556it [04:58, 21639.38it/s]3544634it [04:58, 21300.53it/s]3315983it [04:57, 21529.61it/s]3375751it [04:58, 20776.91it/s]3746776it [04:58, 21804.46it/s]3546797it [04:58, 21396.06it/s]3318162it [04:57, 21606.78it/s]3377943it [04:58, 21105.45it/s]3748957it [04:58, 21763.44it/s]3548996it [04:58, 21571.75it/s]3320413it [04:57, 21875.41it/s]3380107it [04:58, 21261.29it/s]3751134it [04:58, 21634.13it/s]3551155it [04:58, 21353.30it/s]3322692it [04:57, 22146.82it/s]3382300it [04:58, 21457.22it/s]3753371it [04:58, 21845.12it/s]3553305it [04:58, 21395.19it/s]3324925it [04:57, 22198.91it/s]3384537it [04:58, 21726.30it/s]3755616it [04:58, 22024.24it/s]3555469it [04:58, 21455.30it/s]3327163it [04:57, 22251.87it/s]3386805it [04:58, 22009.13it/s]3757819it [04:58, 21926.83it/s]3557721it [04:59, 21770.07it/s]3329389it [04:58, 22160.86it/s]3389009it [04:58, 21442.00it/s]3760012it [04:59, 21882.91it/s]3559899it [04:59, 21424.64it/s]3331606it [04:58, 22046.55it/s]3391222it [04:58, 21641.88it/s]3762201it [04:59, 21777.01it/s]3562069it [04:59, 21504.49it/s]3333811it [04:58, 21792.59it/s]3393391it [04:59, 21596.31it/s]3764379it [04:59, 21637.43it/s]3564221it [04:59, 21166.92it/s]3336024it [04:58, 21891.46it/s]3395554it [04:59, 21480.08it/s]3766543it [04:59, 21579.27it/s]3566389it [04:59, 21238.93it/s]3338288it [04:58, 22112.62it/s]3397704it [04:59, 20633.07it/s]3768715it [04:59, 21620.14it/s]3340535it [04:58, 22217.68it/s]3568515it [04:59, 21101.15it/s]3399879it [04:59, 20954.70it/s]3770878it [04:59, 21582.61it/s]3342792it [04:58, 22322.01it/s]3570687it [04:59, 21282.21it/s]3402044it [04:59, 21156.90it/s]3773062it [04:59, 21656.57it/s]3345030it [04:58, 22339.07it/s]3572817it [04:59, 21197.34it/s]3404287it [04:59, 21529.76it/s]3775358it [04:59, 22044.66it/s]3347265it [04:58, 22260.06it/s]3574975it [04:59, 21308.29it/s]3406535it [04:59, 21809.21it/s]3777563it [04:59, 21986.05it/s]3349492it [04:59, 22032.04it/s]3577107it [05:00, 21160.01it/s]3408720it [04:59, 21706.85it/s]3779762it [04:59, 21614.63it/s]3351696it [04:59, 22002.03it/s]3579235it [05:00, 21194.37it/s]3410894it [04:59, 21204.24it/s]3781957it [05:00, 21712.28it/s]3353910it [04:59, 22042.65it/s]3581355it [05:00, 21060.94it/s]3413037it [05:00, 21269.17it/s]3784130it [05:00, 21676.12it/s]3356184it [04:59, 22248.62it/s]3583478it [05:00, 21109.68it/s]3415213it [05:00, 21413.38it/s]3786299it [05:00, 21457.55it/s]3358421it [04:59, 22277.67it/s]3585590it [05:00, 21021.05it/s]3417357it [05:00, 20966.02it/s]3788446it [05:00, 21433.27it/s]3360685it [04:59, 22384.73it/s]3587729it [05:00, 21129.43it/s]3419511it [05:00, 21133.21it/s]3790652it [05:00, 21614.81it/s]3362924it [04:59, 22287.14it/s]3589843it [05:00, 21101.26it/s]3421712it [05:00, 21390.88it/s]3792817it [05:00, 21624.16it/s]3365153it [04:59, 22114.36it/s]3591967it [05:00, 21142.22it/s]3423857it [05:00, 21407.18it/s]3794980it [05:00, 21446.75it/s]3594082it [05:00, 21009.16it/s]3367365it [04:59, 21949.99it/s]3426034it [05:00, 21514.03it/s]3797172it [05:00, 21586.81it/s]3596219it [05:00, 21114.81it/s]3369621it [04:59, 22123.11it/s]3428187it [05:00, 21053.81it/s]3799374it [05:00, 21714.94it/s]3598332it [05:01, 21117.16it/s]3371834it [05:00, 22029.64it/s]3430449it [05:00, 21512.30it/s]3801591it [05:00, 21848.90it/s]3600560it [05:01, 21461.17it/s]3374038it [05:00, 21745.57it/s]3432651it [05:00, 21661.66it/s]3803212it [05:01, 12632.69it/s]
3602707it [05:01, 21208.73it/s]3376214it [05:00, 21403.04it/s]3434820it [05:01, 21588.86it/s]3604902it [05:01, 21427.06it/s]3378412it [05:00, 21567.66it/s]3436981it [05:01, 21482.99it/s]3607046it [05:01, 21188.18it/s]3380652it [05:00, 21810.73it/s]3439189it [05:01, 21659.72it/s]3609212it [05:01, 21325.32it/s]3382835it [05:00, 21666.02it/s]3441364it [05:01, 21684.20it/s]3611346it [05:01, 21058.08it/s]3385119it [05:00, 22011.33it/s]3443668it [05:01, 22087.80it/s]3613475it [05:01, 21124.47it/s]3387403it [05:00, 22255.48it/s]3445878it [05:01, 21971.66it/s]3615646it [05:01, 21295.47it/s]3389686it [05:00, 22424.84it/s]3448127it [05:01, 22125.70it/s]3391930it [05:00, 22301.61it/s]3617777it [05:01, 21008.61it/s]3450406it [05:01, 22295.47it/s]3619969it [05:02, 21275.43it/s]3394161it [05:01, 22222.05it/s]3452636it [05:01, 22019.79it/s]3396384it [05:01, 22219.69it/s]3622098it [05:02, 21055.15it/s]3454910it [05:01, 22231.09it/s]3398607it [05:01, 22078.80it/s]3624329it [05:02, 21424.00it/s]3457135it [05:02, 22051.92it/s]3626473it [05:02, 21396.95it/s]3400816it [05:01, 21889.13it/s]3459342it [05:02, 21963.25it/s]3628653it [05:02, 21516.58it/s]3403006it [05:01, 21889.97it/s]3461576it [05:02, 22074.01it/s]3405251it [05:01, 22053.81it/s]3630806it [05:02, 21282.03it/s]3463784it [05:02, 22029.07it/s]3407457it [05:01, 22010.11it/s]3632936it [05:02, 21251.53it/s]3466018it [05:02, 22119.87it/s]3409659it [05:01, 21899.48it/s]3635062it [05:02, 21073.32it/s]3468231it [05:02, 22069.76it/s]3411850it [05:01, 21714.19it/s]3637195it [05:02, 21146.00it/s]3470439it [05:02, 20715.88it/s]3414071it [05:01, 21852.69it/s]3639339it [05:02, 21231.98it/s]3472598it [05:02, 20965.30it/s]3641586it [05:03, 21600.49it/s]3416257it [05:02, 21707.68it/s]3474774it [05:02, 21193.34it/s]3643760it [05:03, 21640.81it/s]3418481it [05:02, 21770.74it/s]3420692it [05:02, 21868.62it/s]3645925it [05:03, 21378.24it/s]3648064it [05:03, 21339.91it/s]3422880it [05:02, 21768.70it/s]3650199it [05:03, 21302.95it/s]3425058it [05:02, 21638.63it/s]3427259it [05:02, 21746.62it/s]3652330it [05:03, 21040.33it/s]3429554it [05:02, 22103.48it/s]3654499it [05:03, 21231.50it/s]3431765it [05:02, 22076.57it/s]3656624it [05:03, 21215.11it/s]3658786it [05:03, 21333.84it/s]3433973it [05:02, 21908.03it/s]3436165it [05:02, 21744.01it/s]3660920it [05:03, 21028.19it/s]3438363it [05:03, 21812.70it/s]3663040it [05:04, 21078.31it/s]3665234it [05:04, 21332.74it/s]3440545it [05:03, 21628.32it/s]3442842it [05:03, 21932.08it/s]3667369it [05:04, 21143.57it/s]3445148it [05:03, 22265.80it/s]3669560it [05:04, 21369.17it/s]3447413it [05:03, 22379.24it/s]3671717it [05:04, 21427.67it/s]3449652it [05:03, 22339.35it/s]3673861it [05:04, 21224.05it/s]3451984it [05:03, 22631.09it/s]3676024it [05:04, 21343.67it/s]3454271it [05:03, 22701.62it/s]3678228it [05:04, 21548.57it/s]3456542it [05:03, 22513.03it/s]3680466it [05:04, 21794.76it/s]3682647it [05:04, 21716.89it/s]3458794it [05:03, 22140.26it/s]3684885it [05:05, 21911.74it/s]3461010it [05:04, 22139.44it/s]3687101it [05:05, 21985.00it/s]3463262it [05:04, 22251.53it/s]3465511it [05:04, 22320.78it/s]3689300it [05:05, 21607.20it/s]3467744it [05:04, 22172.41it/s]3691463it [05:05, 21480.58it/s]3469962it [05:04, 22105.82it/s]3693613it [05:05, 21423.65it/s]3472174it [05:04, 22108.17it/s]3695757it [05:05, 21145.16it/s]3474386it [05:04, 21841.43it/s]3697919it [05:05, 21278.88it/s]3700060it [05:05, 21316.54it/s]3702252it [05:05, 21495.54it/s]3704403it [05:05, 21271.78it/s]3706559it [05:06, 21356.37it/s]3708696it [05:06, 21327.09it/s]3710851it [05:06, 21391.36it/s]3712991it [05:06, 21367.23it/s]3715147it [05:06, 21421.82it/s]3717290it [05:06, 21353.05it/s]3719426it [05:06, 21209.70it/s]3721636it [05:06, 21473.84it/s]3723784it [05:06, 21383.37it/s]3725990it [05:06, 21547.43it/s]3728158it [05:07, 21585.55it/s]3730317it [05:07, 21532.71it/s]3732471it [05:07, 21209.98it/s]3734600it [05:07, 21233.21it/s]3736804it [05:07, 21470.66it/s]3738952it [05:07, 21386.98it/s]3741092it [05:07, 20860.27it/s]3743229it [05:07, 21008.57it/s]3745373it [05:07, 21134.11it/s]3747540it [05:08, 21291.84it/s]3749671it [05:08, 21286.51it/s]3751842it [05:08, 21410.84it/s]3754000it [05:08, 21459.79it/s]3756230it [05:08, 21607.75it/s]3758409it [05:08, 21659.30it/s]3760576it [05:08, 21445.40it/s]3762736it [05:08, 21489.46it/s]3764886it [05:08, 21434.98it/s]3767030it [05:08, 21290.59it/s]3769160it [05:09, 21259.93it/s]3771343it [05:09, 21428.06it/s]3773511it [05:09, 21500.48it/s]3775751it [05:09, 21767.19it/s]3777930it [05:09, 21773.60it/s]3780108it [05:09, 21431.28it/s]3782270it [05:09, 21432.71it/s]3784431it [05:09, 21482.26it/s]3786580it [05:09, 21110.24it/s]3788744it [05:09, 21265.53it/s]3790894it [05:10, 21332.99it/s]3793046it [05:10, 21385.76it/s]3795186it [05:10, 21114.06it/s]3797366it [05:10, 21315.37it/s]3799561it [05:10, 21503.41it/s]3801717it [05:10, 21518.63it/s]3803212it [05:10, 12245.00it/s]
2022-08-06 04:40:29 | INFO | root | success load 3803212 data
2022-08-06 04:40:29 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-08-06 04:40:29 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-08-06 04:40:29 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-08-06 04:40:29 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-08-06 04:40:29 | INFO | transformer.tokenization_utils | loading file None
2022-08-06 04:40:29 | INFO | transformer.tokenization_utils | loading file None
2022-08-06 04:40:29 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
3474774it [05:30, 21193.34it/s]3475676it [05:30, 217.63it/s]  3477912it [05:30, 331.40it/s]3480164it [05:30, 493.08it/s]3482378it [05:30, 716.10it/s]3484579it [05:30, 1024.94it/s]3486777it [05:30, 1450.22it/s]3488973it [05:30, 2027.87it/s]3491207it [05:30, 2813.78it/s]3493422it [05:31, 3826.57it/s]3495630it [05:31, 5086.14it/s]3497818it [05:31, 6592.32it/s]3500035it [05:31, 8359.13it/s]3502271it [05:31, 10318.80it/s]3504469it [05:31, 12228.68it/s]3506725it [05:31, 14210.61it/s]3508933it [05:31, 15890.52it/s]3511139it [05:31, 17156.44it/s]3513342it [05:31, 18371.12it/s]3515527it [05:32, 19251.25it/s]3517708it [05:32, 19727.91it/s]3519923it [05:32, 20400.72it/s]3522096it [05:32, 20747.42it/s]3524293it [05:32, 21097.29it/s]3526471it [05:32, 21127.52it/s]3528652it [05:32, 21325.16it/s]3530869it [05:32, 21571.69it/s]3533051it [05:32, 21555.98it/s]3535264it [05:33, 21725.04it/s]3537449it [05:33, 21469.38it/s]3539605it [05:33, 20867.14it/s]3541702it [05:33, 20865.58it/s]3543796it [05:33, 20705.94it/s]3545939it [05:33, 20916.82it/s]3548035it [05:33, 20793.97it/s]3550117it [05:33, 20732.12it/s]3552192it [05:33, 20573.35it/s]3554251it [05:33, 20490.33it/s]3556413it [05:34, 20823.70it/s]3558505it [05:34, 20848.32it/s]3560591it [05:34, 20414.65it/s]3562657it [05:34, 20484.03it/s]3564716it [05:34, 20394.13it/s]3566819it [05:34, 20579.37it/s]3568879it [05:34, 20493.95it/s]3570983it [05:34, 20654.51it/s]3573113it [05:34, 20844.26it/s]3575199it [05:34, 20645.42it/s]3577297it [05:35, 20743.24it/s]3579372it [05:35, 20737.28it/s]3581447it [05:35, 20608.83it/s]3583509it [05:35, 20465.32it/s]3585620it [05:35, 20653.63it/s]3474386it [05:34, 21841.43it/s]3475672it [05:34, 214.13it/s]  3587686it [05:35, 20600.16it/s]3477745it [05:34, 310.58it/s]3589747it [05:35, 20568.31it/s]3479897it [05:34, 452.00it/s]3591805it [05:35, 20531.39it/s]3482036it [05:34, 649.57it/s]3593877it [05:35, 20587.41it/s]3484170it [05:35, 924.65it/s]3595936it [05:35, 20319.93it/s]3486295it [05:35, 1302.32it/s]3598095it [05:36, 20695.31it/s]3488387it [05:35, 1810.42it/s]3600217it [05:36, 20849.80it/s]3490474it [05:35, 2489.95it/s]3602317it [05:36, 20892.05it/s]3492555it [05:35, 3377.94it/s]3604407it [05:36, 20647.58it/s]3494633it [05:35, 4503.60it/s]3606502it [05:36, 20734.43it/s]3496707it [05:35, 5859.39it/s]3608577it [05:36, 20705.44it/s]3498782it [05:35, 7461.39it/s]3610649it [05:36, 20540.27it/s]3500871it [05:35, 9231.47it/s]3612704it [05:36, 20528.39it/s]3502961it [05:36, 11093.80it/s]3614786it [05:36, 20613.90it/s]3505072it [05:36, 12952.61it/s]3616848it [05:36, 20344.60it/s]3507174it [05:36, 14643.07it/s]3618947it [05:37, 20535.06it/s]3509271it [05:36, 16047.91it/s]3621009it [05:37, 20557.68it/s]3511352it [05:36, 17105.91it/s]3623108it [05:37, 20684.79it/s]3513428it [05:36, 18053.20it/s]3625196it [05:37, 20675.23it/s]3515496it [05:36, 18568.59it/s]3627371it [05:37, 20993.31it/s]3517571it [05:36, 19169.55it/s]3629471it [05:37, 20882.25it/s]3519625it [05:36, 19383.83it/s]3631560it [05:37, 20671.92it/s]3521716it [05:36, 19820.20it/s]3633628it [05:37, 20530.86it/s]3523768it [05:37, 19739.16it/s]3635716it [05:37, 20632.39it/s]3525853it [05:37, 20058.15it/s]3637780it [05:37, 20618.66it/s]3527894it [05:37, 19979.03it/s]3639917it [05:38, 20841.07it/s]3529986it [05:37, 20251.41it/s]3642002it [05:38, 20342.56it/s]3532029it [05:37, 20162.54it/s]3644081it [05:38, 20471.66it/s]3534135it [05:37, 20425.73it/s]3646131it [05:38, 20375.43it/s]3536187it [05:37, 20308.54it/s]3648182it [05:38, 20413.36it/s]3538288it [05:37, 20514.78it/s]3650258it [05:38, 20515.28it/s]3540345it [05:37, 20404.46it/s]3652311it [05:38, 20278.40it/s]3542410it [05:37, 20476.35it/s]3654379it [05:38, 20396.03it/s]3544551it [05:38, 20474.93it/s]3656420it [05:38, 20366.30it/s]3546736it [05:38, 20878.57it/s]3658458it [05:38, 20260.67it/s]3548836it [05:38, 20912.30it/s]3660523it [05:39, 20375.50it/s]3550929it [05:38, 20726.26it/s]3662616it [05:39, 20539.45it/s]3553003it [05:38, 20647.95it/s]3664683it [05:39, 20575.66it/s]3555154it [05:38, 20901.62it/s]3666741it [05:39, 20473.00it/s]3557286it [05:38, 21023.63it/s]3668848it [05:39, 20649.22it/s]3559390it [05:38, 20875.16it/s]3670921it [05:39, 20672.18it/s]3561479it [05:38, 20738.33it/s]3672989it [05:39, 20456.23it/s]3563554it [05:38, 20664.98it/s]3675038it [05:39, 20464.62it/s]3565621it [05:39, 20561.59it/s]3677251it [05:39, 20957.03it/s]3567716it [05:39, 20676.07it/s]3679348it [05:40, 20787.67it/s]3569784it [05:39, 20564.97it/s]3681499it [05:40, 21002.13it/s]3571841it [05:39, 20553.88it/s]3683718it [05:40, 21353.85it/s]3573941it [05:39, 20685.41it/s]3685863it [05:40, 21379.83it/s]3576010it [05:39, 20431.70it/s]3688002it [05:40, 20960.78it/s]3578085it [05:39, 20525.22it/s]3690101it [05:40, 20920.20it/s]3580139it [05:39, 20311.18it/s]3692195it [05:40, 20907.94it/s]3582216it [05:39, 20446.11it/s]3694287it [05:40, 20644.72it/s]3584262it [05:39, 20233.41it/s]3696383it [05:40, 20737.17it/s]3586354it [05:40, 20435.82it/s]3698458it [05:40, 20644.39it/s]3588399it [05:40, 20253.02it/s]3700524it [05:41, 20515.02it/s]3590507it [05:40, 20495.94it/s]3702618it [05:41, 20635.25it/s]3592558it [05:40, 20298.00it/s]3704718it [05:41, 20743.45it/s]3594611it [05:40, 20365.87it/s]3706793it [05:41, 20582.09it/s]3596649it [05:40, 20366.40it/s]3708852it [05:41, 20358.90it/s]3598814it [05:40, 20747.05it/s]3710897it [05:41, 20385.39it/s]3600890it [05:40, 20666.85it/s]3712981it [05:41, 20517.69it/s]3603008it [05:40, 20817.00it/s]3715034it [05:41, 20427.48it/s]3605091it [05:40, 20685.29it/s]3717102it [05:41, 20501.14it/s]3607162it [05:41, 20691.70it/s]3719153it [05:41, 20452.69it/s]3609232it [05:41, 20448.60it/s]3721199it [05:42, 20341.46it/s]3611298it [05:41, 20508.62it/s]3723369it [05:42, 20744.25it/s]3613350it [05:41, 20475.48it/s]3725506it [05:42, 20926.95it/s]3615398it [05:41, 20429.56it/s]3727612it [05:42, 20964.72it/s]3617442it [05:41, 20394.53it/s]3729709it [05:42, 20594.37it/s]3619482it [05:41, 20385.40it/s]3731830it [05:42, 20774.04it/s]3621587it [05:41, 20583.38it/s]3733909it [05:42, 20610.43it/s]3623646it [05:41, 20370.36it/s]3735975it [05:42, 20622.67it/s]3625841it [05:41, 20839.18it/s]3738039it [05:42, 20504.75it/s]3627926it [05:42, 20652.80it/s]3740107it [05:42, 20553.16it/s]3629993it [05:42, 20654.97it/s]3742163it [05:43, 20507.38it/s]3632060it [05:42, 20274.33it/s]3744215it [05:43, 20468.37it/s]3634160it [05:42, 20487.55it/s]3746278it [05:43, 20515.26it/s]3636211it [05:42, 20246.38it/s]3748367it [05:43, 20625.73it/s]3638369it [05:42, 20638.52it/s]3750430it [05:43, 20325.04it/s]3640457it [05:42, 20707.99it/s]3752557it [05:43, 20603.33it/s]3642660it [05:42, 21100.84it/s]3754701it [05:43, 20851.23it/s]3644772it [05:42, 20820.23it/s]3756788it [05:43, 20773.44it/s]3646893it [05:42, 20933.75it/s]3758867it [05:43, 20718.00it/s]3648988it [05:43, 20547.71it/s]3760960it [05:43, 20780.62it/s]3651093it [05:43, 20693.22it/s]3763039it [05:44, 20650.87it/s]3653165it [05:43, 20446.95it/s]3765105it [05:44, 20511.28it/s]3655264it [05:43, 20606.29it/s]3767158it [05:44, 20514.47it/s]3657327it [05:43, 20305.32it/s]3769210it [05:44, 20447.32it/s]3659422it [05:43, 20493.62it/s]3771255it [05:44, 20363.21it/s]3661473it [05:43, 20289.27it/s]3773368it [05:44, 20587.48it/s]3663570it [05:43, 20487.26it/s]3775559it [05:44, 20891.77it/s]3665620it [05:43, 20358.07it/s]3777649it [05:44, 20645.35it/s]3667732it [05:44, 20581.81it/s]3779721it [05:44, 20663.89it/s]3669792it [05:44, 20361.01it/s]3781788it [05:44, 20591.11it/s]3671897it [05:44, 20564.01it/s]3783871it [05:45, 20659.47it/s]3673955it [05:44, 20314.97it/s]3785938it [05:45, 20174.42it/s]3676064it [05:44, 20542.51it/s]3787984it [05:45, 20255.70it/s]3678120it [05:44, 20482.01it/s]3790022it [05:45, 20291.89it/s]3680283it [05:44, 20821.14it/s]3792053it [05:45, 20155.65it/s]3682376it [05:44, 20850.92it/s]3794070it [05:45, 20136.92it/s]3684571it [05:44, 21176.48it/s]3796140it [05:45, 20302.58it/s]3686690it [05:44, 21017.12it/s]3798207it [05:45, 20410.35it/s]3688816it [05:45, 21088.92it/s]3800327it [05:45, 20644.39it/s]3690926it [05:45, 20606.27it/s]3802393it [05:45, 20647.39it/s]3803212it [05:46, 10991.48it/s]
3693012it [05:45, 20679.47it/s]3695082it [05:45, 20348.91it/s]3697161it [05:45, 20474.96it/s]3699211it [05:45, 20239.09it/s]3701306it [05:45, 20447.94it/s]3703359it [05:45, 20469.42it/s]3705413it [05:45, 20488.68it/s]3707463it [05:45, 20406.12it/s]3709518it [05:46, 20446.00it/s]3711564it [05:46, 20428.04it/s]3713608it [05:46, 20319.15it/s]3715721it [05:46, 20559.10it/s]3717778it [05:46, 20223.49it/s]3719834it [05:46, 20320.94it/s]3721868it [05:46, 20296.12it/s]3724043it [05:46, 20725.95it/s]3726117it [05:46, 20603.71it/s]3728213it [05:46, 20707.61it/s]3730285it [05:47, 20478.44it/s]3732415it [05:47, 20719.39it/s]3734488it [05:47, 20419.86it/s]3736638it [05:47, 20737.07it/s]3738714it [05:47, 20390.25it/s]3740835it [05:47, 20630.57it/s]3742900it [05:47, 20401.68it/s]3745033it [05:47, 20672.98it/s]3747102it [05:47, 20521.58it/s]3749229it [05:47, 20741.60it/s]3751305it [05:48, 20527.90it/s]3753518it [05:48, 21000.53it/s]3755620it [05:48, 20834.15it/s]3757767it [05:48, 21019.43it/s]3759871it [05:48, 20687.22it/s]3762007it [05:48, 20883.72it/s]3764097it [05:48, 20671.80it/s]3766226it [05:48, 20852.72it/s]3768313it [05:48, 20542.18it/s]3770420it [05:49, 20693.90it/s]3772491it [05:49, 20667.66it/s]3774675it [05:49, 21014.26it/s]3776778it [05:49, 20892.03it/s]3778904it [05:49, 20999.22it/s]3781005it [05:49, 20599.72it/s]3783112it [05:49, 20636.14it/s]3785177it [05:49, 20399.80it/s]3787260it [05:49, 20525.15it/s]3789314it [05:49, 20357.56it/s]3791437it [05:50, 20612.36it/s]3793500it [05:50, 20380.76it/s]3795593it [05:50, 20540.84it/s]3797649it [05:50, 20400.03it/s]3799837it [05:50, 20835.63it/s]3801942it [05:50, 20897.43it/s]3803212it [05:50, 10848.18it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-08-06 04:46:22 | INFO | train_inner | epoch 001:    100 / 3715 loss=26.107, nll_loss=12.656, mask_ins=7.559, word_ins_ml=13.073, word_reposition=3.477, kpe=1.998, ppl=7.22841e+07, wps=5166.9, ups=0.35, wpb=14605, bsz=1024, num_updates=100, lr=1.0098e-05, gnorm=20.996, clip=14, loss_scale=128, train_wall=293, wall=664
2022-08-06 04:51:02 | INFO | train_inner | epoch 001:    200 / 3715 loss=20.134, nll_loss=11.209, mask_ins=4.592, word_ins_ml=11.794, word_reposition=2.49, kpe=1.257, ppl=1.15043e+06, wps=5199.3, ups=0.36, wpb=14603.1, bsz=1024, num_updates=200, lr=2.0096e-05, gnorm=17.841, clip=0, loss_scale=128, train_wall=249, wall=945
2022-08-06 04:55:43 | INFO | train_inner | epoch 001:    300 / 3715 loss=16.345, nll_loss=10.702, mask_ins=2.402, word_ins_ml=11.344, word_reposition=1.481, kpe=1.118, ppl=83242.1, wps=5261.2, ups=0.36, wpb=14772, bsz=1024, num_updates=300, lr=3.0094e-05, gnorm=5.558, clip=0, loss_scale=128, train_wall=250, wall=1226
2022-08-06 05:00:23 | INFO | train_inner | epoch 001:    400 / 3715 loss=14.977, nll_loss=10.214, mask_ins=2.044, word_ins_ml=10.925, word_reposition=0.947, kpe=1.062, ppl=32251.7, wps=5256.6, ups=0.36, wpb=14728.8, bsz=1024, num_updates=400, lr=4.0092e-05, gnorm=3.658, clip=0, loss_scale=128, train_wall=248, wall=1506
2022-08-06 05:05:03 | INFO | train_inner | epoch 001:    500 / 3715 loss=14.264, nll_loss=9.734, mask_ins=1.925, word_ins_ml=10.512, word_reposition=0.803, kpe=1.025, ppl=19673, wps=5217.1, ups=0.36, wpb=14607.3, bsz=1024, num_updates=500, lr=5.009e-05, gnorm=3.644, clip=0, loss_scale=128, train_wall=249, wall=1786
2022-08-06 05:09:43 | INFO | train_inner | epoch 001:    600 / 3715 loss=13.818, nll_loss=9.358, mask_ins=1.895, word_ins_ml=10.186, word_reposition=0.734, kpe=1.003, ppl=14438.4, wps=5219.4, ups=0.36, wpb=14581.3, bsz=1024, num_updates=600, lr=6.0088e-05, gnorm=3.389, clip=0, loss_scale=242, train_wall=248, wall=2065
2022-08-06 05:14:23 | INFO | train_inner | epoch 001:    700 / 3715 loss=13.501, nll_loss=9.061, mask_ins=1.88, word_ins_ml=9.927, word_reposition=0.704, kpe=0.989, ppl=11589.5, wps=5267.8, ups=0.36, wpb=14738.3, bsz=1024, num_updates=700, lr=7.0086e-05, gnorm=3.334, clip=0, loss_scale=256, train_wall=248, wall=2345
2022-08-06 05:19:02 | INFO | train_inner | epoch 001:    800 / 3715 loss=13.179, nll_loss=8.747, mask_ins=1.873, word_ins_ml=9.652, word_reposition=0.681, kpe=0.974, ppl=9275.68, wps=5237.6, ups=0.36, wpb=14637, bsz=1024, num_updates=800, lr=8.0084e-05, gnorm=3.332, clip=0, loss_scale=256, train_wall=248, wall=2624
2022-08-06 05:23:41 | INFO | train_inner | epoch 001:    900 / 3715 loss=12.5, nll_loss=7.991, mask_ins=1.85, word_ins_ml=8.987, word_reposition=0.7, kpe=0.963, ppl=5791.37, wps=5211.3, ups=0.36, wpb=14512.1, bsz=1024, num_updates=900, lr=9.0082e-05, gnorm=3.499, clip=0, loss_scale=256, train_wall=247, wall=2903
2022-08-06 05:28:20 | INFO | train_inner | epoch 001:   1000 / 3715 loss=11.877, nll_loss=7.391, mask_ins=1.775, word_ins_ml=8.459, word_reposition=0.689, kpe=0.954, ppl=3760.93, wps=5222.3, ups=0.36, wpb=14576.3, bsz=1024, num_updates=1000, lr=0.00010008, gnorm=3.482, clip=0, loss_scale=256, train_wall=248, wall=3182
2022-08-06 05:32:59 | INFO | train_inner | epoch 001:   1100 / 3715 loss=11.416, nll_loss=6.977, mask_ins=1.698, word_ins_ml=8.093, word_reposition=0.677, kpe=0.949, ppl=2732.55, wps=5255.8, ups=0.36, wpb=14664.5, bsz=1024, num_updates=1100, lr=0.000110078, gnorm=3.318, clip=0, loss_scale=453, train_wall=247, wall=3461
2022-08-06 05:37:39 | INFO | train_inner | epoch 001:   1200 / 3715 loss=11.121, nll_loss=6.691, mask_ins=1.667, word_ins_ml=7.838, word_reposition=0.67, kpe=0.946, ppl=2227.62, wps=5245.2, ups=0.36, wpb=14698.2, bsz=1024, num_updates=1200, lr=0.000120076, gnorm=3.267, clip=0, loss_scale=512, train_wall=249, wall=3741
2022-08-06 05:42:18 | INFO | train_inner | epoch 001:   1300 / 3715 loss=10.816, nll_loss=6.413, mask_ins=1.632, word_ins_ml=7.593, word_reposition=0.648, kpe=0.943, ppl=1803.19, wps=5258.1, ups=0.36, wpb=14674.2, bsz=1024, num_updates=1300, lr=0.000130074, gnorm=3.161, clip=0, loss_scale=512, train_wall=248, wall=4020
2022-08-06 05:46:57 | INFO | train_inner | epoch 001:   1400 / 3715 loss=10.607, nll_loss=6.227, mask_ins=1.596, word_ins_ml=7.429, word_reposition=0.644, kpe=0.938, ppl=1559.58, wps=5286.3, ups=0.36, wpb=14762.6, bsz=1024, num_updates=1400, lr=0.000140072, gnorm=3.102, clip=0, loss_scale=512, train_wall=248, wall=4300
2022-08-06 05:51:36 | INFO | train_inner | epoch 001:   1500 / 3715 loss=10.328, nll_loss=5.987, mask_ins=1.557, word_ins_ml=7.216, word_reposition=0.621, kpe=0.933, ppl=1285.06, wps=5249.3, ups=0.36, wpb=14653.9, bsz=1024, num_updates=1500, lr=0.00015007, gnorm=2.922, clip=0, loss_scale=512, train_wall=248, wall=4579
2022-08-06 05:56:16 | INFO | train_inner | epoch 001:   1600 / 3715 loss=10.08, nll_loss=5.784, mask_ins=1.51, word_ins_ml=7.037, word_reposition=0.601, kpe=0.932, ppl=1082.27, wps=5260, ups=0.36, wpb=14712.5, bsz=1024, num_updates=1600, lr=0.000160068, gnorm=2.835, clip=0, loss_scale=845, train_wall=248, wall=4858
2022-08-06 06:00:54 | INFO | train_inner | epoch 001:   1700 / 3715 loss=9.853, nll_loss=5.596, mask_ins=1.476, word_ins_ml=6.87, word_reposition=0.582, kpe=0.926, ppl=925.09, wps=5288.8, ups=0.36, wpb=14722.9, bsz=1023.8, num_updates=1700, lr=0.000170066, gnorm=2.816, clip=0, loss_scale=1024, train_wall=247, wall=5137
2022-08-06 06:05:34 | INFO | train_inner | epoch 001:   1800 / 3715 loss=9.675, nll_loss=5.453, mask_ins=1.443, word_ins_ml=6.744, word_reposition=0.569, kpe=0.92, ppl=817.57, wps=5235.9, ups=0.36, wpb=14623.7, bsz=1024, num_updates=1800, lr=0.000180064, gnorm=2.622, clip=0, loss_scale=1024, train_wall=248, wall=5416
2022-08-06 06:10:13 | INFO | train_inner | epoch 001:   1900 / 3715 loss=9.543, nll_loss=5.353, mask_ins=1.419, word_ins_ml=6.654, word_reposition=0.549, kpe=0.921, ppl=746.06, wps=5234.2, ups=0.36, wpb=14622.6, bsz=1024, num_updates=1900, lr=0.000190062, gnorm=2.594, clip=0, loss_scale=1024, train_wall=248, wall=5696
2022-08-06 06:14:52 | INFO | train_inner | epoch 001:   2000 / 3715 loss=9.431, nll_loss=5.255, mask_ins=1.402, word_ins_ml=6.566, word_reposition=0.547, kpe=0.916, ppl=690.38, wps=5280.8, ups=0.36, wpb=14718.6, bsz=1024, num_updates=2000, lr=0.00020006, gnorm=2.419, clip=0, loss_scale=1024, train_wall=247, wall=5974
2022-08-06 06:19:32 | INFO | train_inner | epoch 001:   2100 / 3715 loss=9.358, nll_loss=5.199, mask_ins=1.392, word_ins_ml=6.516, word_reposition=0.539, kpe=0.911, ppl=656.31, wps=5247.4, ups=0.36, wpb=14676.5, bsz=1024, num_updates=2100, lr=0.000210058, gnorm=2.421, clip=0, loss_scale=1567, train_wall=248, wall=6254
2022-08-06 06:24:12 | INFO | train_inner | epoch 001:   2200 / 3715 loss=9.27, nll_loss=5.129, mask_ins=1.372, word_ins_ml=6.454, word_reposition=0.53, kpe=0.913, ppl=617.37, wps=5251.5, ups=0.36, wpb=14703.8, bsz=1024, num_updates=2200, lr=0.000220056, gnorm=2.27, clip=0, loss_scale=2048, train_wall=248, wall=6534
2022-08-06 06:28:51 | INFO | train_inner | epoch 001:   2300 / 3715 loss=9.169, nll_loss=5.032, mask_ins=1.365, word_ins_ml=6.368, word_reposition=0.526, kpe=0.909, ppl=575.46, wps=5316.6, ups=0.36, wpb=14839.5, bsz=1024, num_updates=2300, lr=0.000230054, gnorm=2.235, clip=0, loss_scale=2048, train_wall=248, wall=6813
2022-08-06 06:33:30 | INFO | train_inner | epoch 001:   2400 / 3715 loss=9.093, nll_loss=4.979, mask_ins=1.351, word_ins_ml=6.32, word_reposition=0.517, kpe=0.905, ppl=546.11, wps=5240.8, ups=0.36, wpb=14630.8, bsz=1024, num_updates=2400, lr=0.000240052, gnorm=2.223, clip=0, loss_scale=2048, train_wall=248, wall=7092
2022-08-06 06:38:09 | INFO | train_inner | epoch 001:   2500 / 3715 loss=9.056, nll_loss=4.959, mask_ins=1.344, word_ins_ml=6.301, word_reposition=0.503, kpe=0.907, ppl=532.23, wps=5261, ups=0.36, wpb=14677.7, bsz=1024, num_updates=2500, lr=0.00025005, gnorm=2.124, clip=0, loss_scale=2048, train_wall=248, wall=7371
2022-08-06 06:42:50 | INFO | train_inner | epoch 001:   2600 / 3715 loss=8.998, nll_loss=4.887, mask_ins=1.337, word_ins_ml=6.237, word_reposition=0.515, kpe=0.909, ppl=511.46, wps=5254.6, ups=0.36, wpb=14766, bsz=1024, num_updates=2600, lr=0.000260048, gnorm=2.097, clip=0, loss_scale=2888, train_wall=249, wall=7652
2022-08-06 06:46:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-06 06:47:32 | INFO | train_inner | epoch 001:   2701 / 3715 loss=8.932, nll_loss=4.851, mask_ins=1.325, word_ins_ml=6.205, word_reposition=0.497, kpe=0.905, ppl=488.48, wps=5224.1, ups=0.35, wpb=14752.5, bsz=1024, num_updates=2700, lr=0.000270046, gnorm=2.091, clip=0, loss_scale=3711, train_wall=251, wall=7935
2022-08-06 06:52:13 | INFO | train_inner | epoch 001:   2801 / 3715 loss=8.887, nll_loss=4.82, mask_ins=1.318, word_ins_ml=6.176, word_reposition=0.49, kpe=0.902, ppl=473.44, wps=5229.4, ups=0.36, wpb=14679.3, bsz=1024, num_updates=2800, lr=0.000280044, gnorm=2.028, clip=0, loss_scale=2048, train_wall=248, wall=8215
2022-08-06 06:56:53 | INFO | train_inner | epoch 001:   2901 / 3715 loss=8.844, nll_loss=4.781, mask_ins=1.315, word_ins_ml=6.141, word_reposition=0.492, kpe=0.897, ppl=459.56, wps=5207, ups=0.36, wpb=14572.2, bsz=1024, num_updates=2900, lr=0.000290042, gnorm=2.054, clip=0, loss_scale=2048, train_wall=248, wall=8495
2022-08-06 07:01:33 | INFO | train_inner | epoch 001:   3001 / 3715 loss=8.827, nll_loss=4.766, mask_ins=1.31, word_ins_ml=6.127, word_reposition=0.491, kpe=0.899, ppl=454.13, wps=5245.3, ups=0.36, wpb=14696.4, bsz=1024, num_updates=3000, lr=0.00030004, gnorm=2.022, clip=0, loss_scale=2048, train_wall=248, wall=8775
2022-08-06 07:06:14 | INFO | train_inner | epoch 001:   3101 / 3715 loss=8.759, nll_loss=4.707, mask_ins=1.306, word_ins_ml=6.074, word_reposition=0.483, kpe=0.895, ppl=433.13, wps=5231.9, ups=0.36, wpb=14677.8, bsz=1024, num_updates=3100, lr=0.000310038, gnorm=1.978, clip=0, loss_scale=2048, train_wall=249, wall=9056
2022-08-06 07:10:52 | INFO | train_inner | epoch 001:   3201 / 3715 loss=8.725, nll_loss=4.693, mask_ins=1.296, word_ins_ml=6.061, word_reposition=0.475, kpe=0.892, ppl=423.13, wps=5220.1, ups=0.36, wpb=14540.1, bsz=1024, num_updates=3200, lr=0.000320036, gnorm=1.918, clip=0, loss_scale=2191, train_wall=247, wall=9335
2022-08-06 07:13:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-06 07:15:36 | INFO | train_inner | epoch 001:   3302 / 3715 loss=8.714, nll_loss=4.671, mask_ins=1.296, word_ins_ml=6.041, word_reposition=0.476, kpe=0.901, ppl=420.06, wps=5158.8, ups=0.35, wpb=14664.7, bsz=1024, num_updates=3300, lr=0.000330034, gnorm=2.121, clip=0, loss_scale=3001, train_wall=252, wall=9619
2022-08-06 07:20:16 | INFO | train_inner | epoch 001:   3402 / 3715 loss=8.652, nll_loss=4.621, mask_ins=1.283, word_ins_ml=5.997, word_reposition=0.476, kpe=0.895, ppl=402.26, wps=5214.7, ups=0.36, wpb=14570.1, bsz=1024, num_updates=3400, lr=0.000340032, gnorm=2.067, clip=0, loss_scale=2048, train_wall=248, wall=9898
2022-08-06 07:24:56 | INFO | train_inner | epoch 001:   3502 / 3715 loss=8.645, nll_loss=4.622, mask_ins=1.281, word_ins_ml=5.996, word_reposition=0.47, kpe=0.898, ppl=400.26, wps=5245.3, ups=0.36, wpb=14688.2, bsz=1024, num_updates=3500, lr=0.00035003, gnorm=1.991, clip=0, loss_scale=2048, train_wall=249, wall=10178
2022-08-06 07:29:35 | INFO | train_inner | epoch 001:   3602 / 3715 loss=8.611, nll_loss=4.591, mask_ins=1.278, word_ins_ml=5.969, word_reposition=0.469, kpe=0.896, ppl=391.11, wps=5241.8, ups=0.36, wpb=14646.4, bsz=1024, num_updates=3600, lr=0.000360028, gnorm=1.947, clip=0, loss_scale=2048, train_wall=248, wall=10458
2022-08-06 07:34:14 | INFO | train_inner | epoch 001:   3702 / 3715 loss=8.574, nll_loss=4.559, mask_ins=1.278, word_ins_ml=5.94, word_reposition=0.46, kpe=0.896, ppl=381.2, wps=5230.9, ups=0.36, wpb=14593, bsz=1024, num_updates=3700, lr=0.000370026, gnorm=2.007, clip=0, loss_scale=2048, train_wall=248, wall=10737
2022-08-06 07:34:49 | INFO | train | epoch 001 | loss 11.091 | nll_loss 6.444 | mask_ins 1.772 | word_ins_ml 7.611 | word_reposition 0.735 | kpe 0.973 | ppl 2181.87 | wps 5239.5 | ups 0.36 | wpb 14661.3 | bsz 1023.7 | num_updates 3713 | lr 0.000371326 | gnorm 3.6 | clip 0.4 | loss_scale 1282 | train_wall 9263 | wall 10771
2022-08-06 07:38:31 | INFO | valid | epoch 001 | valid on 'valid' subset | loss nan | nll_loss 4.444 | mask_ins 1.247 | word_ins_ml 5.928 | word_reposition 0.466 | kpe nan | ppl nan | wps 12397.1 | wpb 1849.4 | bsz 127.9 | num_updates 3713
2022-08-06 07:38:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 1 @ 3713 updates, score nan) (writing took 5.971434598788619 seconds)
2022-08-06 07:39:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-06 07:40:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-06 07:41:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-06 07:42:50 | INFO | train_inner | epoch 002:     90 / 3715 loss=8.662, nll_loss=4.622, mask_ins=1.281, word_ins_ml=5.995, word_reposition=0.47, kpe=0.915, ppl=404.97, wps=2807.8, ups=0.19, wpb=14479.3, bsz=1014.7, num_updates=3800, lr=0.000380024, gnorm=4.203, clip=3, loss_scale=1064, train_wall=255, wall=11252
2022-08-06 07:47:30 | INFO | train_inner | epoch 002:    190 / 3715 loss=8.624, nll_loss=4.58, mask_ins=1.274, word_ins_ml=5.958, word_reposition=0.473, kpe=0.918, ppl=394.4, wps=5256.9, ups=0.36, wpb=14700.9, bsz=1024, num_updates=3900, lr=0.000390022, gnorm=3.34, clip=2, loss_scale=256, train_wall=248, wall=11532
2022-08-06 07:52:11 | INFO | train_inner | epoch 002:    290 / 3715 loss=8.527, nll_loss=4.509, mask_ins=1.27, word_ins_ml=5.894, word_reposition=0.467, kpe=0.896, ppl=368.93, wps=5219.9, ups=0.36, wpb=14669.1, bsz=1024, num_updates=4000, lr=0.00040002, gnorm=2.32, clip=0, loss_scale=256, train_wall=249, wall=11813
2022-08-06 07:56:52 | INFO | train_inner | epoch 002:    390 / 3715 loss=8.525, nll_loss=4.516, mask_ins=1.27, word_ins_ml=5.9, word_reposition=0.462, kpe=0.893, ppl=368.48, wps=5192.9, ups=0.36, wpb=14616.3, bsz=1024, num_updates=4100, lr=0.000410018, gnorm=1.934, clip=0, loss_scale=256, train_wall=250, wall=12094
2022-08-06 08:01:32 | INFO | train_inner | epoch 002:    490 / 3715 loss=8.477, nll_loss=4.472, mask_ins=1.263, word_ins_ml=5.861, word_reposition=0.461, kpe=0.892, ppl=356.21, wps=5233.9, ups=0.36, wpb=14657.6, bsz=1024, num_updates=4200, lr=0.000420016, gnorm=1.98, clip=0, loss_scale=256, train_wall=248, wall=12374
2022-08-06 08:06:11 | INFO | train_inner | epoch 002:    590 / 3715 loss=8.537, nll_loss=4.518, mask_ins=1.272, word_ins_ml=5.901, word_reposition=0.465, kpe=0.9, ppl=371.53, wps=5279.5, ups=0.36, wpb=14734.9, bsz=1024, num_updates=4300, lr=0.000430014, gnorm=2.218, clip=0, loss_scale=287, train_wall=248, wall=12654
2022-08-06 08:10:51 | INFO | train_inner | epoch 002:    690 / 3715 loss=8.498, nll_loss=4.486, mask_ins=1.266, word_ins_ml=5.872, word_reposition=0.46, kpe=0.899, ppl=361.56, wps=5235.5, ups=0.36, wpb=14658.9, bsz=1024, num_updates=4400, lr=0.000440012, gnorm=2.356, clip=0, loss_scale=512, train_wall=248, wall=12934
2022-08-06 08:15:31 | INFO | train_inner | epoch 002:    790 / 3715 loss=8.473, nll_loss=4.47, mask_ins=1.263, word_ins_ml=5.857, word_reposition=0.455, kpe=0.897, ppl=355.28, wps=5237.4, ups=0.36, wpb=14656.5, bsz=1024, num_updates=4500, lr=0.00045001, gnorm=1.959, clip=0, loss_scale=512, train_wall=248, wall=13213
2022-08-06 08:20:12 | INFO | train_inner | epoch 002:    890 / 3715 loss=8.51, nll_loss=4.497, mask_ins=1.262, word_ins_ml=5.881, word_reposition=0.462, kpe=0.905, ppl=364.57, wps=5205.9, ups=0.36, wpb=14639.8, bsz=1023.8, num_updates=4600, lr=0.000460008, gnorm=2.575, clip=0, loss_scale=512, train_wall=250, wall=13495
2022-08-06 08:20:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-06 08:24:54 | INFO | train_inner | epoch 002:    991 / 3715 loss=8.5, nll_loss=4.478, mask_ins=1.264, word_ins_ml=5.865, word_reposition=0.468, kpe=0.903, ppl=362.04, wps=5185.9, ups=0.35, wpb=14631.1, bsz=1024, num_updates=4700, lr=0.000470006, gnorm=2.323, clip=0, loss_scale=271, train_wall=250, wall=13777
2022-08-06 08:29:35 | INFO | train_inner | epoch 002:   1091 / 3715 loss=8.475, nll_loss=4.453, mask_ins=1.26, word_ins_ml=5.841, word_reposition=0.471, kpe=0.902, ppl=355.73, wps=5243.7, ups=0.36, wpb=14722.8, bsz=1024, num_updates=4800, lr=0.000480004, gnorm=2.187, clip=0, loss_scale=256, train_wall=249, wall=14058
2022-08-06 08:34:17 | INFO | train_inner | epoch 002:   1191 / 3715 loss=8.458, nll_loss=4.445, mask_ins=1.262, word_ins_ml=5.834, word_reposition=0.461, kpe=0.901, ppl=351.53, wps=5182.3, ups=0.35, wpb=14605.9, bsz=1024, num_updates=4900, lr=0.000490002, gnorm=2.026, clip=0, loss_scale=256, train_wall=250, wall=14339
2022-08-06 08:38:59 | INFO | train_inner | epoch 002:   1291 / 3715 loss=8.482, nll_loss=4.47, mask_ins=1.26, word_ins_ml=5.856, word_reposition=0.459, kpe=0.908, ppl=357.6, wps=5219.4, ups=0.36, wpb=14701.5, bsz=1024, num_updates=5000, lr=0.0005, gnorm=2.605, clip=0, loss_scale=256, train_wall=250, wall=14621
2022-08-06 08:43:38 | INFO | train_inner | epoch 002:   1391 / 3715 loss=8.44, nll_loss=4.43, mask_ins=1.255, word_ins_ml=5.819, word_reposition=0.461, kpe=0.905, ppl=347.22, wps=5279.2, ups=0.36, wpb=14732.4, bsz=1024, num_updates=5100, lr=0.000495074, gnorm=2.085, clip=0, loss_scale=256, train_wall=247, wall=14900
2022-08-06 08:48:18 | INFO | train_inner | epoch 002:   1491 / 3715 loss=8.416, nll_loss=4.419, mask_ins=1.253, word_ins_ml=5.81, word_reposition=0.453, kpe=0.901, ppl=341.63, wps=5208.5, ups=0.36, wpb=14619.3, bsz=1024, num_updates=5200, lr=0.00049029, gnorm=2.118, clip=0, loss_scale=468, train_wall=249, wall=15181
2022-08-06 08:52:59 | INFO | train_inner | epoch 002:   1591 / 3715 loss=8.416, nll_loss=4.41, mask_ins=1.25, word_ins_ml=5.801, word_reposition=0.459, kpe=0.906, ppl=341.56, wps=5197.1, ups=0.36, wpb=14586.4, bsz=1024, num_updates=5300, lr=0.000485643, gnorm=2.34, clip=0, loss_scale=512, train_wall=249, wall=15461
2022-08-06 08:57:41 | INFO | train_inner | epoch 002:   1691 / 3715 loss=8.368, nll_loss=4.37, mask_ins=1.246, word_ins_ml=5.765, word_reposition=0.453, kpe=0.904, ppl=330.42, wps=5198.5, ups=0.35, wpb=14679.9, bsz=1024, num_updates=5400, lr=0.000481125, gnorm=2.01, clip=0, loss_scale=512, train_wall=251, wall=15744
2022-08-06 09:02:23 | INFO | train_inner | epoch 002:   1791 / 3715 loss=8.357, nll_loss=4.366, mask_ins=1.239, word_ins_ml=5.761, word_reposition=0.452, kpe=0.905, ppl=327.85, wps=5221.4, ups=0.35, wpb=14721.2, bsz=1024, num_updates=5500, lr=0.000476731, gnorm=2.031, clip=0, loss_scale=512, train_wall=250, wall=16026
2022-08-06 09:07:03 | INFO | train_inner | epoch 002:   1891 / 3715 loss=8.316, nll_loss=4.328, mask_ins=1.242, word_ins_ml=5.727, word_reposition=0.448, kpe=0.899, ppl=318.78, wps=5250.3, ups=0.36, wpb=14683.2, bsz=1024, num_updates=5600, lr=0.000472456, gnorm=1.959, clip=0, loss_scale=512, train_wall=248, wall=16305
2022-08-06 09:11:43 | INFO | train_inner | epoch 002:   1991 / 3715 loss=8.309, nll_loss=4.327, mask_ins=1.241, word_ins_ml=5.725, word_reposition=0.44, kpe=0.902, ppl=317.07, wps=5249.2, ups=0.36, wpb=14666.3, bsz=1024, num_updates=5700, lr=0.000468293, gnorm=2.015, clip=0, loss_scale=876, train_wall=248, wall=16585
2022-08-06 09:16:23 | INFO | train_inner | epoch 002:   2091 / 3715 loss=8.289, nll_loss=4.314, mask_ins=1.232, word_ins_ml=5.713, word_reposition=0.448, kpe=0.896, ppl=312.78, wps=5228.6, ups=0.36, wpb=14662.8, bsz=1024, num_updates=5800, lr=0.000464238, gnorm=1.947, clip=0, loss_scale=1024, train_wall=249, wall=16865
2022-08-06 09:21:03 | INFO | train_inner | epoch 002:   2191 / 3715 loss=8.244, nll_loss=4.28, mask_ins=1.227, word_ins_ml=5.683, word_reposition=0.439, kpe=0.894, ppl=303.08, wps=5266.7, ups=0.36, wpb=14731.5, bsz=1024, num_updates=5900, lr=0.000460287, gnorm=1.871, clip=0, loss_scale=1024, train_wall=248, wall=17145
2022-08-06 09:24:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-06 09:25:44 | INFO | train_inner | epoch 002:   2292 / 3715 loss=8.225, nll_loss=4.265, mask_ins=1.224, word_ins_ml=5.669, word_reposition=0.437, kpe=0.895, ppl=299.17, wps=5222.6, ups=0.36, wpb=14703.7, bsz=1024, num_updates=6000, lr=0.000456435, gnorm=2.248, clip=1, loss_scale=902, train_wall=250, wall=17427
2022-08-06 09:30:24 | INFO | train_inner | epoch 002:   2392 / 3715 loss=8.2, nll_loss=4.249, mask_ins=1.221, word_ins_ml=5.654, word_reposition=0.433, kpe=0.892, ppl=294.14, wps=5236.2, ups=0.36, wpb=14643.7, bsz=1024, num_updates=6100, lr=0.000452679, gnorm=2.127, clip=0, loss_scale=512, train_wall=248, wall=17706
2022-08-06 09:35:04 | INFO | train_inner | epoch 002:   2492 / 3715 loss=8.199, nll_loss=4.246, mask_ins=1.218, word_ins_ml=5.651, word_reposition=0.433, kpe=0.897, ppl=293.79, wps=5211.2, ups=0.36, wpb=14598.3, bsz=1024, num_updates=6200, lr=0.000449013, gnorm=2.217, clip=0, loss_scale=512, train_wall=249, wall=17986
2022-08-06 09:39:43 | INFO | train_inner | epoch 002:   2592 / 3715 loss=8.17, nll_loss=4.227, mask_ins=1.216, word_ins_ml=5.634, word_reposition=0.429, kpe=0.89, ppl=287.98, wps=5237.7, ups=0.36, wpb=14598.6, bsz=1024, num_updates=6300, lr=0.000445435, gnorm=1.957, clip=0, loss_scale=512, train_wall=247, wall=18265
2022-08-06 09:46:35 | INFO | train_inner | epoch 002:   2692 / 3715 loss=8.156, nll_loss=4.209, mask_ins=1.218, word_ins_ml=5.618, word_reposition=0.432, kpe=0.888, ppl=285.23, wps=3546, ups=0.24, wpb=14602.9, bsz=1024, num_updates=6400, lr=0.000441942, gnorm=1.782, clip=0, loss_scale=512, train_wall=380, wall=18677
2022-08-06 09:51:14 | INFO | train_inner | epoch 002:   2792 / 3715 loss=8.115, nll_loss=4.182, mask_ins=1.209, word_ins_ml=5.593, word_reposition=0.426, kpe=0.887, ppl=277.18, wps=5252.7, ups=0.36, wpb=14692.5, bsz=1024, num_updates=6500, lr=0.000438529, gnorm=1.746, clip=0, loss_scale=573, train_wall=249, wall=18957
2022-08-06 09:55:53 | INFO | train_inner | epoch 002:   2892 / 3715 loss=8.1, nll_loss=4.172, mask_ins=1.204, word_ins_ml=5.584, word_reposition=0.426, kpe=0.886, ppl=274.41, wps=5260.5, ups=0.36, wpb=14686.4, bsz=1024, num_updates=6600, lr=0.000435194, gnorm=1.751, clip=0, loss_scale=1024, train_wall=248, wall=19236
2022-08-06 10:00:34 | INFO | train_inner | epoch 002:   2992 / 3715 loss=8.094, nll_loss=4.167, mask_ins=1.208, word_ins_ml=5.579, word_reposition=0.425, kpe=0.881, ppl=273.22, wps=5201.4, ups=0.36, wpb=14567.4, bsz=1024, num_updates=6700, lr=0.000431934, gnorm=1.788, clip=0, loss_scale=1024, train_wall=249, wall=19516
2022-08-06 10:05:13 | INFO | train_inner | epoch 002:   3092 / 3715 loss=8.102, nll_loss=4.178, mask_ins=1.206, word_ins_ml=5.588, word_reposition=0.422, kpe=0.885, ppl=274.83, wps=5241, ups=0.36, wpb=14624.4, bsz=1024, num_updates=6800, lr=0.000428746, gnorm=1.766, clip=0, loss_scale=1024, train_wall=248, wall=19795
2022-08-06 10:09:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-06 10:09:55 | INFO | train_inner | epoch 002:   3193 / 3715 loss=8.063, nll_loss=4.127, mask_ins=1.2, word_ins_ml=5.544, word_reposition=0.428, kpe=0.892, ppl=267.47, wps=5238.7, ups=0.35, wpb=14822.2, bsz=1024, num_updates=6900, lr=0.000425628, gnorm=1.808, clip=0, loss_scale=989, train_wall=251, wall=20078
2022-08-06 10:14:35 | INFO | train_inner | epoch 002:   3293 / 3715 loss=8.095, nll_loss=4.167, mask_ins=1.205, word_ins_ml=5.578, word_reposition=0.418, kpe=0.894, ppl=273.41, wps=5234, ups=0.36, wpb=14642.2, bsz=1024, num_updates=7000, lr=0.000422577, gnorm=2.449, clip=0, loss_scale=512, train_wall=248, wall=20358
2022-08-06 10:19:14 | INFO | train_inner | epoch 002:   3393 / 3715 loss=8.057, nll_loss=4.136, mask_ins=1.193, word_ins_ml=5.551, word_reposition=0.422, kpe=0.891, ppl=266.33, wps=5271.3, ups=0.36, wpb=14702.5, bsz=1024, num_updates=7100, lr=0.000419591, gnorm=2.224, clip=0, loss_scale=512, train_wall=248, wall=20637
2022-08-06 10:23:53 | INFO | train_inner | epoch 002:   3493 / 3715 loss=8.017, nll_loss=4.101, mask_ins=1.195, word_ins_ml=5.52, word_reposition=0.417, kpe=0.885, ppl=258.99, wps=5279.9, ups=0.36, wpb=14725.5, bsz=1024, num_updates=7200, lr=0.000416667, gnorm=1.871, clip=0, loss_scale=512, train_wall=248, wall=20915
2022-08-06 10:28:32 | INFO | train_inner | epoch 002:   3593 / 3715 loss=8.029, nll_loss=4.117, mask_ins=1.196, word_ins_ml=5.533, word_reposition=0.417, kpe=0.883, ppl=261.26, wps=5245.3, ups=0.36, wpb=14607.8, bsz=1024, num_updates=7300, lr=0.000413803, gnorm=1.778, clip=0, loss_scale=512, train_wall=247, wall=21194
2022-08-06 10:33:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-06 10:33:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-06 10:33:17 | INFO | train_inner | epoch 002:   3695 / 3715 loss=7.99, nll_loss=4.082, mask_ins=1.187, word_ins_ml=5.502, word_reposition=0.42, kpe=0.882, ppl=254.22, wps=5175.9, ups=0.35, wpb=14781.4, bsz=1024, num_updates=7400, lr=0.000410997, gnorm=1.811, clip=0, loss_scale=496, train_wall=253, wall=21480
2022-08-06 10:34:11 | INFO | train | epoch 002 | loss 8.309 | nll_loss 4.333 | mask_ins 1.236 | word_ins_ml 5.731 | word_reposition 0.445 | kpe 0.896 | ppl 317.06 | wps 5050.7 | ups 0.34 | wpb 14663 | bsz 1023.7 | num_updates 7420 | lr 0.000410443 | gnorm 2.152 | clip 0.2 | loss_scale 554 | train_wall 9363 | wall 21533
2022-08-06 10:37:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss nan | nll_loss 3.884 | mask_ins 1.155 | word_ins_ml 5.418 | word_reposition 0.412 | kpe nan | ppl nan | wps 12361.8 | wpb 1849.4 | bsz 127.9 | num_updates 7420 | best_loss nan
2022-08-06 10:38:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 2 @ 7420 updates, score nan) (writing took 9.343834141269326 seconds)
2022-08-06 10:41:45 | INFO | train_inner | epoch 003:     80 / 3715 loss=7.947, nll_loss=4.037, mask_ins=1.19, word_ins_ml=5.462, word_reposition=0.419, kpe=0.876, ppl=246.85, wps=2880.9, ups=0.2, wpb=14638.4, bsz=1014.7, num_updates=7500, lr=0.000408248, gnorm=1.862, clip=0, loss_scale=128, train_wall=246, wall=21988
2022-08-06 10:46:25 | INFO | train_inner | epoch 003:    180 / 3715 loss=7.903, nll_loss=4.019, mask_ins=1.182, word_ins_ml=5.446, word_reposition=0.408, kpe=0.868, ppl=239.38, wps=5208.7, ups=0.36, wpb=14582.1, bsz=1024, num_updates=7600, lr=0.000405554, gnorm=1.857, clip=0, loss_scale=128, train_wall=249, wall=22268
2022-08-06 10:51:05 | INFO | train_inner | epoch 003:    280 / 3715 loss=7.934, nll_loss=4.047, mask_ins=1.18, word_ins_ml=5.47, word_reposition=0.407, kpe=0.877, ppl=244.59, wps=5264.4, ups=0.36, wpb=14722.8, bsz=1024, num_updates=7700, lr=0.000402911, gnorm=1.977, clip=0, loss_scale=128, train_wall=249, wall=22547
2022-08-06 10:55:44 | INFO | train_inner | epoch 003:    380 / 3715 loss=7.865, nll_loss=3.986, mask_ins=1.168, word_ins_ml=5.416, word_reposition=0.414, kpe=0.867, ppl=233.2, wps=5281.1, ups=0.36, wpb=14748.4, bsz=1024, num_updates=7800, lr=0.00040032, gnorm=1.698, clip=0, loss_scale=128, train_wall=248, wall=22827
2022-08-06 11:00:24 | INFO | train_inner | epoch 003:    480 / 3715 loss=7.886, nll_loss=4.008, mask_ins=1.172, word_ins_ml=5.436, word_reposition=0.406, kpe=0.873, ppl=236.56, wps=5252.9, ups=0.36, wpb=14683.9, bsz=1024, num_updates=7900, lr=0.000397779, gnorm=2.567, clip=1, loss_scale=128, train_wall=248, wall=23106
2022-08-06 11:05:03 | INFO | train_inner | epoch 003:    580 / 3715 loss=7.919, nll_loss=4.019, mask_ins=1.18, word_ins_ml=5.445, word_reposition=0.412, kpe=0.882, ppl=241.99, wps=5273.7, ups=0.36, wpb=14734, bsz=1024, num_updates=8000, lr=0.000395285, gnorm=2.331, clip=0, loss_scale=244, train_wall=248, wall=23385
2022-08-06 11:09:43 | INFO | train_inner | epoch 003:    680 / 3715 loss=7.895, nll_loss=4.001, mask_ins=1.175, word_ins_ml=5.43, word_reposition=0.413, kpe=0.878, ppl=238.11, wps=5254.8, ups=0.36, wpb=14707.5, bsz=1024, num_updates=8100, lr=0.000392837, gnorm=1.903, clip=0, loss_scale=256, train_wall=249, wall=23665
2022-08-06 11:14:22 | INFO | train_inner | epoch 003:    780 / 3715 loss=7.849, nll_loss=3.97, mask_ins=1.168, word_ins_ml=5.401, word_reposition=0.415, kpe=0.865, ppl=230.6, wps=5240.5, ups=0.36, wpb=14641.9, bsz=1024, num_updates=8200, lr=0.000390434, gnorm=1.879, clip=0, loss_scale=256, train_wall=248, wall=23945
2022-08-06 11:19:01 | INFO | train_inner | epoch 003:    880 / 3715 loss=7.862, nll_loss=3.982, mask_ins=1.168, word_ins_ml=5.411, word_reposition=0.412, kpe=0.871, ppl=232.6, wps=5279.6, ups=0.36, wpb=14704.1, bsz=1024, num_updates=8300, lr=0.000388075, gnorm=2.137, clip=1, loss_scale=256, train_wall=247, wall=24223
2022-08-06 11:23:41 | INFO | train_inner | epoch 003:    980 / 3715 loss=7.836, nll_loss=3.965, mask_ins=1.171, word_ins_ml=5.396, word_reposition=0.402, kpe=0.868, ppl=228.49, wps=5222.9, ups=0.36, wpb=14604.5, bsz=1024, num_updates=8400, lr=0.000385758, gnorm=1.981, clip=0, loss_scale=256, train_wall=248, wall=24503
2022-08-06 11:26:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-06 11:28:23 | INFO | train_inner | epoch 003:   1081 / 3715 loss=7.868, nll_loss=3.996, mask_ins=1.17, word_ins_ml=5.424, word_reposition=0.404, kpe=0.87, ppl=233.6, wps=5197.7, ups=0.35, wpb=14657.1, bsz=1024, num_updates=8500, lr=0.000383482, gnorm=2.014, clip=0, loss_scale=337, train_wall=251, wall=24785
2022-08-06 11:31:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-06 11:33:05 | INFO | train_inner | epoch 003:   1182 / 3715 loss=7.823, nll_loss=3.951, mask_ins=1.173, word_ins_ml=5.383, word_reposition=0.399, kpe=0.868, ppl=226.39, wps=5172.6, ups=0.35, wpb=14593.4, bsz=1024, num_updates=8600, lr=0.000381246, gnorm=2.014, clip=0, loss_scale=209, train_wall=250, wall=25067
2022-08-06 11:37:44 | INFO | train_inner | epoch 003:   1282 / 3715 loss=7.826, nll_loss=3.962, mask_ins=1.164, word_ins_ml=5.393, word_reposition=0.401, kpe=0.868, ppl=226.92, wps=5216.4, ups=0.36, wpb=14563.3, bsz=1024, num_updates=8700, lr=0.000379049, gnorm=1.709, clip=0, loss_scale=128, train_wall=248, wall=25346
2022-08-06 11:42:23 | INFO | train_inner | epoch 003:   1382 / 3715 loss=7.793, nll_loss=3.932, mask_ins=1.16, word_ins_ml=5.367, word_reposition=0.402, kpe=0.865, ppl=221.74, wps=5255.9, ups=0.36, wpb=14665.1, bsz=1024, num_updates=8800, lr=0.000376889, gnorm=1.7, clip=0, loss_scale=128, train_wall=247, wall=25625
2022-08-06 11:47:03 | INFO | train_inner | epoch 003:   1482 / 3715 loss=7.833, nll_loss=3.966, mask_ins=1.162, word_ins_ml=5.396, word_reposition=0.401, kpe=0.874, ppl=228.02, wps=5227.1, ups=0.36, wpb=14631.6, bsz=1024, num_updates=8900, lr=0.000374766, gnorm=2.26, clip=0, loss_scale=128, train_wall=248, wall=25905
2022-08-06 11:51:42 | INFO | train_inner | epoch 003:   1582 / 3715 loss=7.78, nll_loss=3.927, mask_ins=1.159, word_ins_ml=5.361, word_reposition=0.398, kpe=0.863, ppl=219.87, wps=5223.4, ups=0.36, wpb=14604.5, bsz=1024, num_updates=9000, lr=0.000372678, gnorm=1.637, clip=0, loss_scale=128, train_wall=248, wall=26185
2022-08-06 11:56:22 | INFO | train_inner | epoch 003:   1682 / 3715 loss=7.777, nll_loss=3.918, mask_ins=1.158, word_ins_ml=5.353, word_reposition=0.405, kpe=0.861, ppl=219.32, wps=5236.7, ups=0.36, wpb=14624.4, bsz=1024, num_updates=9100, lr=0.000370625, gnorm=1.633, clip=0, loss_scale=160, train_wall=248, wall=26464
2022-08-06 12:01:01 | INFO | train_inner | epoch 003:   1782 / 3715 loss=7.81, nll_loss=3.944, mask_ins=1.164, word_ins_ml=5.376, word_reposition=0.404, kpe=0.865, ppl=224.47, wps=5282.8, ups=0.36, wpb=14737.8, bsz=1024, num_updates=9200, lr=0.000368605, gnorm=1.786, clip=0, loss_scale=256, train_wall=248, wall=26743
2022-08-06 12:05:40 | INFO | train_inner | epoch 003:   1882 / 3715 loss=7.778, nll_loss=3.922, mask_ins=1.161, word_ins_ml=5.356, word_reposition=0.401, kpe=0.859, ppl=219.45, wps=5232.2, ups=0.36, wpb=14623.9, bsz=1024, num_updates=9300, lr=0.000366618, gnorm=1.713, clip=0, loss_scale=256, train_wall=248, wall=27023
2022-08-06 12:10:18 | INFO | train_inner | epoch 003:   1982 / 3715 loss=7.746, nll_loss=3.903, mask_ins=1.151, word_ins_ml=5.339, word_reposition=0.399, kpe=0.857, ppl=214.73, wps=5250.1, ups=0.36, wpb=14585.5, bsz=1023.8, num_updates=9400, lr=0.000364662, gnorm=1.705, clip=0, loss_scale=256, train_wall=246, wall=27300
2022-08-06 12:14:57 | INFO | train_inner | epoch 003:   2082 / 3715 loss=7.732, nll_loss=3.891, mask_ins=1.153, word_ins_ml=5.328, word_reposition=0.395, kpe=0.856, ppl=212.56, wps=5239.7, ups=0.36, wpb=14605.2, bsz=1024, num_updates=9500, lr=0.000362738, gnorm=1.728, clip=0, loss_scale=256, train_wall=248, wall=27579
2022-08-06 12:19:36 | INFO | train_inner | epoch 003:   2182 / 3715 loss=7.714, nll_loss=3.872, mask_ins=1.154, word_ins_ml=5.311, word_reposition=0.393, kpe=0.857, ppl=210.03, wps=5268.2, ups=0.36, wpb=14723.3, bsz=1024, num_updates=9600, lr=0.000360844, gnorm=1.615, clip=0, loss_scale=289, train_wall=248, wall=27859
2022-08-06 12:24:16 | INFO | train_inner | epoch 003:   2282 / 3715 loss=7.7, nll_loss=3.859, mask_ins=1.147, word_ins_ml=5.3, word_reposition=0.396, kpe=0.856, ppl=207.93, wps=5262.2, ups=0.36, wpb=14698.6, bsz=1024, num_updates=9700, lr=0.000358979, gnorm=1.656, clip=0, loss_scale=512, train_wall=248, wall=28138
2022-08-06 12:28:55 | INFO | train_inner | epoch 003:   2382 / 3715 loss=7.686, nll_loss=3.847, mask_ins=1.143, word_ins_ml=5.289, word_reposition=0.399, kpe=0.856, ppl=205.98, wps=5245.3, ups=0.36, wpb=14663.1, bsz=1024, num_updates=9800, lr=0.000357143, gnorm=1.959, clip=0, loss_scale=512, train_wall=248, wall=28417
2022-08-06 12:33:35 | INFO | train_inner | epoch 003:   2482 / 3715 loss=7.704, nll_loss=3.862, mask_ins=1.15, word_ins_ml=5.302, word_reposition=0.394, kpe=0.858, ppl=208.55, wps=5278.8, ups=0.36, wpb=14775.7, bsz=1024, num_updates=9900, lr=0.000355335, gnorm=1.626, clip=0, loss_scale=512, train_wall=248, wall=28697
2022-08-06 12:38:14 | INFO | train_inner | epoch 003:   2582 / 3715 loss=7.684, nll_loss=3.86, mask_ins=1.14, word_ins_ml=5.3, word_reposition=0.397, kpe=0.847, ppl=205.7, wps=5212.6, ups=0.36, wpb=14543, bsz=1024, num_updates=10000, lr=0.000353553, gnorm=1.594, clip=0, loss_scale=512, train_wall=248, wall=28976
2022-08-06 12:42:54 | INFO | train_inner | epoch 003:   2682 / 3715 loss=7.682, nll_loss=3.843, mask_ins=1.146, word_ins_ml=5.285, word_reposition=0.395, kpe=0.856, ppl=205.38, wps=5255.5, ups=0.36, wpb=14721.5, bsz=1024, num_updates=10100, lr=0.000351799, gnorm=1.599, clip=0, loss_scale=517, train_wall=249, wall=29256
2022-08-06 12:47:34 | INFO | train_inner | epoch 003:   2782 / 3715 loss=7.676, nll_loss=3.857, mask_ins=1.142, word_ins_ml=5.296, word_reposition=0.387, kpe=0.85, ppl=204.47, wps=5215.4, ups=0.36, wpb=14602.4, bsz=1024, num_updates=10200, lr=0.00035007, gnorm=1.534, clip=0, loss_scale=1024, train_wall=249, wall=29536
2022-08-06 12:52:12 | INFO | train_inner | epoch 003:   2882 / 3715 loss=7.658, nll_loss=3.839, mask_ins=1.136, word_ins_ml=5.281, word_reposition=0.393, kpe=0.849, ppl=202.03, wps=5269.8, ups=0.36, wpb=14671, bsz=1024, num_updates=10300, lr=0.000348367, gnorm=1.526, clip=0, loss_scale=1024, train_wall=247, wall=29815
2022-08-06 12:52:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-06 12:56:54 | INFO | train_inner | epoch 003:   2983 / 3715 loss=7.652, nll_loss=3.826, mask_ins=1.14, word_ins_ml=5.269, word_reposition=0.392, kpe=0.851, ppl=201.14, wps=5239, ups=0.36, wpb=14726.2, bsz=1024, num_updates=10400, lr=0.000346688, gnorm=1.536, clip=0, loss_scale=553, train_wall=249, wall=30096
2022-08-06 13:01:33 | INFO | train_inner | epoch 003:   3083 / 3715 loss=7.646, nll_loss=3.816, mask_ins=1.138, word_ins_ml=5.26, word_reposition=0.394, kpe=0.853, ppl=200.32, wps=5291.2, ups=0.36, wpb=14775, bsz=1024, num_updates=10500, lr=0.000345033, gnorm=1.587, clip=0, loss_scale=512, train_wall=248, wall=30375
2022-08-06 13:05:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-06 13:06:15 | INFO | train_inner | epoch 003:   3184 / 3715 loss=7.65, nll_loss=3.832, mask_ins=1.137, word_ins_ml=5.274, word_reposition=0.392, kpe=0.846, ppl=200.83, wps=5170.8, ups=0.35, wpb=14614.4, bsz=1024, num_updates=10600, lr=0.000343401, gnorm=1.749, clip=0, loss_scale=466, train_wall=251, wall=30658
2022-08-06 13:10:55 | INFO | train_inner | epoch 003:   3284 / 3715 loss=7.629, nll_loss=3.817, mask_ins=1.136, word_ins_ml=5.261, word_reposition=0.383, kpe=0.849, ppl=197.94, wps=5242.3, ups=0.36, wpb=14648, bsz=1024, num_updates=10700, lr=0.000341793, gnorm=1.527, clip=0, loss_scale=256, train_wall=248, wall=30937
2022-08-06 13:15:34 | INFO | train_inner | epoch 003:   3384 / 3715 loss=7.628, nll_loss=3.812, mask_ins=1.128, word_ins_ml=5.256, word_reposition=0.398, kpe=0.845, ppl=197.86, wps=5222.6, ups=0.36, wpb=14587.8, bsz=1024, num_updates=10800, lr=0.000340207, gnorm=1.498, clip=0, loss_scale=256, train_wall=248, wall=31217
2022-08-06 13:20:13 | INFO | train_inner | epoch 003:   3484 / 3715 loss=7.618, nll_loss=3.802, mask_ins=1.133, word_ins_ml=5.247, word_reposition=0.389, kpe=0.85, ppl=196.48, wps=5283.5, ups=0.36, wpb=14732.2, bsz=1024, num_updates=10900, lr=0.000338643, gnorm=1.566, clip=0, loss_scale=256, train_wall=247, wall=31495
2022-08-06 13:24:54 | INFO | train_inner | epoch 003:   3584 / 3715 loss=7.604, nll_loss=3.793, mask_ins=1.129, word_ins_ml=5.239, word_reposition=0.388, kpe=0.848, ppl=194.53, wps=5231.1, ups=0.36, wpb=14718.1, bsz=1024, num_updates=11000, lr=0.0003371, gnorm=1.73, clip=0, loss_scale=256, train_wall=250, wall=31777
2022-08-06 13:29:35 | INFO | train_inner | epoch 003:   3684 / 3715 loss=7.592, nll_loss=3.788, mask_ins=1.132, word_ins_ml=5.235, word_reposition=0.383, kpe=0.843, ppl=192.99, wps=5202.4, ups=0.36, wpb=14591.4, bsz=1024, num_updates=11100, lr=0.000335578, gnorm=1.506, clip=0, loss_scale=271, train_wall=249, wall=32057
2022-08-06 13:31:00 | INFO | train | epoch 003 | loss 7.76 | nll_loss 3.908 | mask_ins 1.155 | word_ins_ml 5.344 | word_reposition 0.4 | kpe 0.86 | ppl 216.69 | wps 5128.7 | ups 0.35 | wpb 14661.5 | bsz 1023.7 | num_updates 11131 | lr 0.00033511 | gnorm 1.779 | clip 0.1 | loss_scale 324 | train_wall 9215 | wall 32142
2022-08-06 13:34:42 | INFO | valid | epoch 003 | valid on 'valid' subset | loss nan | nll_loss 3.619 | mask_ins 1.109 | word_ins_ml 5.161 | word_reposition 0.386 | kpe nan | ppl nan | wps 12340.8 | wpb 1849.4 | bsz 127.9 | num_updates 11131 | best_loss nan
2022-08-06 13:34:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 3 @ 11131 updates, score nan) (writing took 10.017116725444794 seconds)
2022-08-06 13:38:04 | INFO | train_inner | epoch 004:     69 / 3715 loss=7.534, nll_loss=3.743, mask_ins=1.123, word_ins_ml=5.195, word_reposition=0.388, kpe=0.828, ppl=185.33, wps=2847, ups=0.2, wpb=14497.6, bsz=1014.7, num_updates=11200, lr=0.000334077, gnorm=1.518, clip=0, loss_scale=512, train_wall=246, wall=32566
2022-08-06 13:42:44 | INFO | train_inner | epoch 004:    169 / 3715 loss=7.512, nll_loss=3.724, mask_ins=1.119, word_ins_ml=5.178, word_reposition=0.387, kpe=0.829, ppl=182.57, wps=5292.4, ups=0.36, wpb=14789.4, bsz=1024, num_updates=11300, lr=0.000332595, gnorm=1.481, clip=0, loss_scale=512, train_wall=248, wall=32846
2022-08-06 13:47:24 | INFO | train_inner | epoch 004:    269 / 3715 loss=7.528, nll_loss=3.732, mask_ins=1.125, word_ins_ml=5.185, word_reposition=0.388, kpe=0.829, ppl=184.51, wps=5270.7, ups=0.36, wpb=14758.5, bsz=1024, num_updates=11400, lr=0.000331133, gnorm=1.482, clip=0, loss_scale=512, train_wall=249, wall=33126
2022-08-06 13:48:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-06 13:52:05 | INFO | train_inner | epoch 004:    370 / 3715 loss=7.505, nll_loss=3.715, mask_ins=1.122, word_ins_ml=5.17, word_reposition=0.383, kpe=0.83, ppl=181.6, wps=5193, ups=0.35, wpb=14628.4, bsz=1024, num_updates=11500, lr=0.00032969, gnorm=1.495, clip=0, loss_scale=309, train_wall=250, wall=33408
2022-08-06 13:56:45 | INFO | train_inner | epoch 004:    470 / 3715 loss=7.507, nll_loss=3.718, mask_ins=1.12, word_ins_ml=5.172, word_reposition=0.386, kpe=0.828, ppl=181.9, wps=5234.8, ups=0.36, wpb=14623.7, bsz=1024, num_updates=11600, lr=0.000328266, gnorm=1.537, clip=0, loss_scale=256, train_wall=248, wall=33687
2022-08-06 14:01:24 | INFO | train_inner | epoch 004:    570 / 3715 loss=7.533, nll_loss=3.745, mask_ins=1.125, word_ins_ml=5.196, word_reposition=0.382, kpe=0.831, ppl=185.23, wps=5222.2, ups=0.36, wpb=14591.8, bsz=1024, num_updates=11700, lr=0.00032686, gnorm=1.702, clip=0, loss_scale=256, train_wall=248, wall=33966
2022-08-06 14:06:03 | INFO | train_inner | epoch 004:    670 / 3715 loss=7.493, nll_loss=3.712, mask_ins=1.117, word_ins_ml=5.168, word_reposition=0.379, kpe=0.83, ppl=180.18, wps=5270.1, ups=0.36, wpb=14706.6, bsz=1024, num_updates=11800, lr=0.000325472, gnorm=1.481, clip=0, loss_scale=256, train_wall=248, wall=34245
2022-08-06 14:10:43 | INFO | train_inner | epoch 004:    770 / 3715 loss=7.502, nll_loss=3.717, mask_ins=1.121, word_ins_ml=5.171, word_reposition=0.382, kpe=0.827, ppl=181.24, wps=5234.6, ups=0.36, wpb=14633.3, bsz=1024, num_updates=11900, lr=0.000324102, gnorm=1.582, clip=0, loss_scale=256, train_wall=248, wall=34525
2022-08-06 14:15:22 | INFO | train_inner | epoch 004:    870 / 3715 loss=7.485, nll_loss=3.701, mask_ins=1.114, word_ins_ml=5.157, word_reposition=0.381, kpe=0.832, ppl=179.09, wps=5281.8, ups=0.36, wpb=14766.6, bsz=1024, num_updates=12000, lr=0.000322749, gnorm=1.5, clip=0, loss_scale=430, train_wall=248, wall=34805
2022-08-06 14:20:03 | INFO | train_inner | epoch 004:    970 / 3715 loss=7.489, nll_loss=3.712, mask_ins=1.115, word_ins_ml=5.167, word_reposition=0.378, kpe=0.829, ppl=179.7, wps=5196.6, ups=0.36, wpb=14590, bsz=1024, num_updates=12100, lr=0.000321412, gnorm=1.513, clip=0, loss_scale=512, train_wall=249, wall=35085
2022-08-06 14:24:41 | INFO | train_inner | epoch 004:   1070 / 3715 loss=7.48, nll_loss=3.71, mask_ins=1.115, word_ins_ml=5.165, word_reposition=0.374, kpe=0.826, ppl=178.54, wps=5249.7, ups=0.36, wpb=14621.1, bsz=1023.8, num_updates=12200, lr=0.000320092, gnorm=1.473, clip=0, loss_scale=512, train_wall=247, wall=35364
2022-08-06 14:29:20 | INFO | train_inner | epoch 004:   1170 / 3715 loss=7.507, nll_loss=3.72, mask_ins=1.122, word_ins_ml=5.173, word_reposition=0.383, kpe=0.829, ppl=181.86, wps=5277.6, ups=0.36, wpb=14688.7, bsz=1024, num_updates=12300, lr=0.000318788, gnorm=1.482, clip=0, loss_scale=512, train_wall=247, wall=35642
2022-08-06 14:34:00 | INFO | train_inner | epoch 004:   1270 / 3715 loss=7.473, nll_loss=3.686, mask_ins=1.123, word_ins_ml=5.143, word_reposition=0.377, kpe=0.83, ppl=177.7, wps=5244.4, ups=0.36, wpb=14688, bsz=1024, num_updates=12400, lr=0.0003175, gnorm=1.51, clip=0, loss_scale=512, train_wall=249, wall=35922
2022-08-06 14:38:40 | INFO | train_inner | epoch 004:   1370 / 3715 loss=7.464, nll_loss=3.693, mask_ins=1.109, word_ins_ml=5.149, word_reposition=0.38, kpe=0.826, ppl=176.51, wps=5251.3, ups=0.36, wpb=14724.3, bsz=1024, num_updates=12500, lr=0.000316228, gnorm=1.454, clip=0, loss_scale=799, train_wall=249, wall=36203
2022-08-06 14:43:20 | INFO | train_inner | epoch 004:   1470 / 3715 loss=7.464, nll_loss=3.694, mask_ins=1.113, word_ins_ml=5.15, word_reposition=0.377, kpe=0.824, ppl=176.56, wps=5235.9, ups=0.36, wpb=14659.7, bsz=1024, num_updates=12600, lr=0.00031497, gnorm=1.479, clip=0, loss_scale=1024, train_wall=248, wall=36483
2022-08-06 14:46:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-06 14:48:02 | INFO | train_inner | epoch 004:   1571 / 3715 loss=7.439, nll_loss=3.675, mask_ins=1.105, word_ins_ml=5.133, word_reposition=0.376, kpe=0.826, ppl=173.56, wps=5249.7, ups=0.36, wpb=14768.5, bsz=1024, num_updates=12700, lr=0.000313728, gnorm=1.447, clip=0, loss_scale=892, train_wall=250, wall=36764
2022-08-06 14:52:41 | INFO | train_inner | epoch 004:   1671 / 3715 loss=7.457, nll_loss=3.683, mask_ins=1.109, word_ins_ml=5.139, word_reposition=0.382, kpe=0.827, ppl=175.72, wps=5240.5, ups=0.36, wpb=14644.8, bsz=1024, num_updates=12800, lr=0.0003125, gnorm=1.476, clip=0, loss_scale=512, train_wall=248, wall=37043
2022-08-06 14:57:22 | INFO | train_inner | epoch 004:   1771 / 3715 loss=7.442, nll_loss=3.673, mask_ins=1.103, word_ins_ml=5.131, word_reposition=0.382, kpe=0.826, ppl=173.89, wps=5261.4, ups=0.36, wpb=14757.3, bsz=1024, num_updates=12900, lr=0.000311286, gnorm=1.471, clip=0, loss_scale=512, train_wall=249, wall=37324
2022-08-06 15:02:01 | INFO | train_inner | epoch 004:   1871 / 3715 loss=7.438, nll_loss=3.675, mask_ins=1.106, word_ins_ml=5.133, word_reposition=0.377, kpe=0.822, ppl=173.42, wps=5239.4, ups=0.36, wpb=14662, bsz=1024, num_updates=13000, lr=0.000310087, gnorm=1.463, clip=0, loss_scale=512, train_wall=248, wall=37604
2022-08-06 15:06:41 | INFO | train_inner | epoch 004:   1971 / 3715 loss=7.44, nll_loss=3.669, mask_ins=1.109, word_ins_ml=5.127, word_reposition=0.379, kpe=0.824, ppl=173.59, wps=5224.3, ups=0.36, wpb=14600.2, bsz=1024, num_updates=13100, lr=0.000308901, gnorm=1.477, clip=0, loss_scale=512, train_wall=248, wall=37883
2022-08-06 15:11:20 | INFO | train_inner | epoch 004:   2071 / 3715 loss=7.446, nll_loss=3.676, mask_ins=1.111, word_ins_ml=5.134, word_reposition=0.38, kpe=0.822, ppl=174.42, wps=5225.6, ups=0.36, wpb=14602.7, bsz=1024, num_updates=13200, lr=0.000307729, gnorm=1.467, clip=0, loss_scale=584, train_wall=248, wall=38163
2022-08-06 15:16:00 | INFO | train_inner | epoch 004:   2171 / 3715 loss=7.447, nll_loss=3.687, mask_ins=1.11, word_ins_ml=5.143, word_reposition=0.374, kpe=0.82, ppl=174.43, wps=5230.3, ups=0.36, wpb=14648.5, bsz=1024, num_updates=13300, lr=0.00030657, gnorm=1.487, clip=0, loss_scale=1024, train_wall=249, wall=38443
2022-08-06 15:20:40 | INFO | train_inner | epoch 004:   2271 / 3715 loss=7.435, nll_loss=3.661, mask_ins=1.107, word_ins_ml=5.12, word_reposition=0.379, kpe=0.829, ppl=173, wps=5296.3, ups=0.36, wpb=14795.9, bsz=1024, num_updates=13400, lr=0.000305424, gnorm=1.445, clip=0, loss_scale=1024, train_wall=248, wall=38722
2022-08-06 15:21:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-06 15:21:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-06 15:22:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-06 15:25:26 | INFO | train_inner | epoch 004:   2374 / 3715 loss=7.521, nll_loss=3.721, mask_ins=1.113, word_ins_ml=5.174, word_reposition=0.382, kpe=0.852, ppl=183.71, wps=5148.6, ups=0.35, wpb=14761.7, bsz=1024, num_updates=13500, lr=0.00030429, gnorm=4.28, clip=2, loss_scale=299, train_wall=255, wall=39009
2022-08-06 15:30:06 | INFO | train_inner | epoch 004:   2474 / 3715 loss=7.427, nll_loss=3.656, mask_ins=1.108, word_ins_ml=5.115, word_reposition=0.376, kpe=0.828, ppl=172.14, wps=5274.1, ups=0.36, wpb=14757.7, bsz=1024, num_updates=13600, lr=0.00030317, gnorm=1.604, clip=0, loss_scale=128, train_wall=248, wall=39289
2022-08-06 15:31:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-08-06 15:31:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-08-06 15:34:51 | INFO | train_inner | epoch 004:   2576 / 3715 loss=7.51, nll_loss=3.721, mask_ins=1.114, word_ins_ml=5.173, word_reposition=0.377, kpe=0.845, ppl=182.28, wps=5115.3, ups=0.35, wpb=14581.6, bsz=1024, num_updates=13700, lr=0.000302061, gnorm=5.068, clip=5, loss_scale=56, train_wall=253, wall=39574
2022-08-06 15:39:31 | INFO | train_inner | epoch 004:   2676 / 3715 loss=7.405, nll_loss=3.652, mask_ins=1.1, word_ins_ml=5.112, word_reposition=0.374, kpe=0.818, ppl=169.43, wps=5232.2, ups=0.36, wpb=14654.5, bsz=1024, num_updates=13800, lr=0.000300965, gnorm=1.456, clip=0, loss_scale=32, train_wall=249, wall=39854
2022-08-06 15:44:10 | INFO | train_inner | epoch 004:   2776 / 3715 loss=7.379, nll_loss=3.629, mask_ins=1.096, word_ins_ml=5.091, word_reposition=0.374, kpe=0.818, ppl=166.51, wps=5218.8, ups=0.36, wpb=14561.7, bsz=1024, num_updates=13900, lr=0.00029988, gnorm=1.457, clip=0, loss_scale=32, train_wall=248, wall=40133
2022-08-06 15:48:50 | INFO | train_inner | epoch 004:   2876 / 3715 loss=7.393, nll_loss=3.64, mask_ins=1.099, word_ins_ml=5.1, word_reposition=0.37, kpe=0.824, ppl=168.14, wps=5224.2, ups=0.36, wpb=14588.9, bsz=1024, num_updates=14000, lr=0.000298807, gnorm=1.455, clip=0, loss_scale=32, train_wall=248, wall=40412
2022-08-06 15:53:29 | INFO | train_inner | epoch 004:   2976 / 3715 loss=7.433, nll_loss=3.67, mask_ins=1.105, word_ins_ml=5.127, word_reposition=0.378, kpe=0.823, ppl=172.84, wps=5227.3, ups=0.36, wpb=14593.5, bsz=1024, num_updates=14100, lr=0.000297746, gnorm=1.585, clip=0, loss_scale=32, train_wall=248, wall=40691
2022-08-06 15:58:08 | INFO | train_inner | epoch 004:   3076 / 3715 loss=7.388, nll_loss=3.638, mask_ins=1.097, word_ins_ml=5.099, word_reposition=0.372, kpe=0.821, ppl=167.47, wps=5256.6, ups=0.36, wpb=14653.3, bsz=1024, num_updates=14200, lr=0.000296695, gnorm=1.55, clip=0, loss_scale=52, train_wall=248, wall=40970
2022-08-06 16:02:47 | INFO | train_inner | epoch 004:   3176 / 3715 loss=7.496, nll_loss=3.711, mask_ins=1.112, word_ins_ml=5.163, word_reposition=0.38, kpe=0.84, ppl=180.54, wps=5236.2, ups=0.36, wpb=14630.1, bsz=1024, num_updates=14300, lr=0.000295656, gnorm=3.309, clip=1, loss_scale=64, train_wall=248, wall=41249
2022-08-06 16:07:26 | INFO | train_inner | epoch 004:   3276 / 3715 loss=7.376, nll_loss=3.63, mask_ins=1.099, word_ins_ml=5.092, word_reposition=0.367, kpe=0.818, ppl=166.12, wps=5194, ups=0.36, wpb=14492, bsz=1024, num_updates=14400, lr=0.000294628, gnorm=1.423, clip=0, loss_scale=64, train_wall=248, wall=41528
2022-08-06 16:12:06 | INFO | train_inner | epoch 004:   3376 / 3715 loss=7.394, nll_loss=3.644, mask_ins=1.1, word_ins_ml=5.104, word_reposition=0.369, kpe=0.821, ppl=168.25, wps=5227.3, ups=0.36, wpb=14606.1, bsz=1024, num_updates=14500, lr=0.00029361, gnorm=1.856, clip=1, loss_scale=64, train_wall=248, wall=41808
2022-08-06 16:16:45 | INFO | train_inner | epoch 004:   3476 / 3715 loss=7.373, nll_loss=3.617, mask_ins=1.098, word_ins_ml=5.08, word_reposition=0.373, kpe=0.822, ppl=165.72, wps=5266.5, ups=0.36, wpb=14733.6, bsz=1024, num_updates=14600, lr=0.000292603, gnorm=1.444, clip=0, loss_scale=64, train_wall=248, wall=42088
2022-08-06 16:21:24 | INFO | train_inner | epoch 004:   3576 / 3715 loss=7.389, nll_loss=3.635, mask_ins=1.1, word_ins_ml=5.095, word_reposition=0.371, kpe=0.822, ppl=167.56, wps=5296.1, ups=0.36, wpb=14757, bsz=1024, num_updates=14700, lr=0.000291606, gnorm=1.438, clip=0, loss_scale=97, train_wall=247, wall=42366
2022-08-06 16:26:04 | INFO | train_inner | epoch 004:   3676 / 3715 loss=7.374, nll_loss=3.631, mask_ins=1.097, word_ins_ml=5.092, word_reposition=0.37, kpe=0.815, ppl=165.83, wps=5221.3, ups=0.36, wpb=14638.3, bsz=1024, num_updates=14800, lr=0.000290619, gnorm=1.433, clip=0, loss_scale=128, train_wall=249, wall=42647
2022-08-06 16:27:51 | INFO | train | epoch 004 | loss 7.454 | nll_loss 3.683 | mask_ins 1.11 | word_ins_ml 5.139 | word_reposition 0.378 | kpe 0.827 | ppl 175.33 | wps 5123.4 | ups 0.35 | wpb 14662.2 | bsz 1023.7 | num_updates 14839 | lr 0.000290237 | gnorm 1.723 | clip 0.2 | loss_scale 371 | train_wall 9216 | wall 42753
2022-08-06 16:31:32 | INFO | valid | epoch 004 | valid on 'valid' subset | loss nan | nll_loss 3.468 | mask_ins 1.072 | word_ins_ml 5.014 | word_reposition 0.374 | kpe nan | ppl nan | wps 12420 | wpb 1849.4 | bsz 127.9 | num_updates 14839 | best_loss nan
2022-08-06 16:31:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 4 @ 14839 updates, score nan) (writing took 11.345183033496141 seconds)
2022-08-06 16:34:33 | INFO | train_inner | epoch 005:     61 / 3715 loss=7.322, nll_loss=3.589, mask_ins=1.091, word_ins_ml=5.055, word_reposition=0.37, kpe=0.807, ppl=160.05, wps=2855.1, ups=0.2, wpb=14528.7, bsz=1014.7, num_updates=14900, lr=0.000289642, gnorm=1.473, clip=0, loss_scale=128, train_wall=246, wall=43156
2022-08-06 16:39:13 | INFO | train_inner | epoch 005:    161 / 3715 loss=7.294, nll_loss=3.567, mask_ins=1.091, word_ins_ml=5.036, word_reposition=0.367, kpe=0.8, ppl=156.94, wps=5231.1, ups=0.36, wpb=14622, bsz=1024, num_updates=15000, lr=0.000288675, gnorm=1.475, clip=0, loss_scale=128, train_wall=248, wall=43435
2022-08-06 16:43:52 | INFO | train_inner | epoch 005:    261 / 3715 loss=7.266, nll_loss=3.545, mask_ins=1.086, word_ins_ml=5.017, word_reposition=0.363, kpe=0.801, ppl=153.97, wps=5253.8, ups=0.36, wpb=14673.1, bsz=1024, num_updates=15100, lr=0.000287718, gnorm=1.442, clip=0, loss_scale=128, train_wall=248, wall=43714
2022-08-06 16:48:31 | INFO | train_inner | epoch 005:    361 / 3715 loss=7.303, nll_loss=3.579, mask_ins=1.091, word_ins_ml=5.046, word_reposition=0.363, kpe=0.803, ppl=157.95, wps=5263.9, ups=0.36, wpb=14684.5, bsz=1024, num_updates=15200, lr=0.00028677, gnorm=1.444, clip=0, loss_scale=179, train_wall=247, wall=43993
2022-08-06 16:53:10 | INFO | train_inner | epoch 005:    461 / 3715 loss=7.305, nll_loss=3.574, mask_ins=1.089, word_ins_ml=5.041, word_reposition=0.368, kpe=0.807, ppl=158.18, wps=5264.3, ups=0.36, wpb=14701.7, bsz=1024, num_updates=15300, lr=0.000285831, gnorm=1.49, clip=0, loss_scale=256, train_wall=248, wall=44273
2022-08-06 16:57:49 | INFO | train_inner | epoch 005:    561 / 3715 loss=7.273, nll_loss=3.547, mask_ins=1.092, word_ins_ml=5.017, word_reposition=0.362, kpe=0.803, ppl=154.69, wps=5245.8, ups=0.36, wpb=14640.9, bsz=1023.8, num_updates=15400, lr=0.000284901, gnorm=1.453, clip=0, loss_scale=256, train_wall=248, wall=44552
2022-08-06 17:02:29 | INFO | train_inner | epoch 005:    661 / 3715 loss=7.298, nll_loss=3.569, mask_ins=1.085, word_ins_ml=5.038, word_reposition=0.373, kpe=0.803, ppl=157.39, wps=5287.9, ups=0.36, wpb=14770.1, bsz=1024, num_updates=15500, lr=0.000283981, gnorm=1.433, clip=0, loss_scale=256, train_wall=248, wall=44831
2022-08-06 17:07:09 | INFO | train_inner | epoch 005:    761 / 3715 loss=7.289, nll_loss=3.558, mask_ins=1.089, word_ins_ml=5.027, word_reposition=0.373, kpe=0.801, ppl=156.44, wps=5236.7, ups=0.36, wpb=14674.9, bsz=1024, num_updates=15600, lr=0.000283069, gnorm=1.449, clip=0, loss_scale=256, train_wall=249, wall=45111
2022-08-06 17:11:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-06 17:11:50 | INFO | train_inner | epoch 005:    862 / 3715 loss=7.275, nll_loss=3.536, mask_ins=1.089, word_ins_ml=5.007, word_reposition=0.371, kpe=0.808, ppl=154.85, wps=5270.3, ups=0.36, wpb=14841.3, bsz=1024, num_updates=15700, lr=0.000282166, gnorm=1.484, clip=0, loss_scale=294, train_wall=250, wall=45393
2022-08-06 17:16:29 | INFO | train_inner | epoch 005:    962 / 3715 loss=7.299, nll_loss=3.557, mask_ins=1.096, word_ins_ml=5.027, word_reposition=0.371, kpe=0.806, ppl=157.47, wps=5308.3, ups=0.36, wpb=14778.1, bsz=1024, num_updates=15800, lr=0.000281272, gnorm=1.503, clip=0, loss_scale=256, train_wall=247, wall=45671
2022-08-06 17:21:08 | INFO | train_inner | epoch 005:   1062 / 3715 loss=7.269, nll_loss=3.545, mask_ins=1.087, word_ins_ml=5.015, word_reposition=0.368, kpe=0.799, ppl=154.26, wps=5230.2, ups=0.36, wpb=14582.3, bsz=1024, num_updates=15900, lr=0.000280386, gnorm=1.487, clip=0, loss_scale=256, train_wall=248, wall=45950
2022-08-06 17:25:47 | INFO | train_inner | epoch 005:   1162 / 3715 loss=7.268, nll_loss=3.545, mask_ins=1.089, word_ins_ml=5.016, word_reposition=0.364, kpe=0.799, ppl=154.11, wps=5244, ups=0.36, wpb=14629.7, bsz=1024, num_updates=16000, lr=0.000279508, gnorm=1.44, clip=0, loss_scale=256, train_wall=247, wall=46229
2022-08-06 17:30:27 | INFO | train_inner | epoch 005:   1262 / 3715 loss=7.281, nll_loss=3.553, mask_ins=1.086, word_ins_ml=5.023, word_reposition=0.369, kpe=0.803, ppl=155.49, wps=5245.5, ups=0.36, wpb=14700.4, bsz=1024, num_updates=16100, lr=0.000278639, gnorm=1.601, clip=0, loss_scale=256, train_wall=249, wall=46509
2022-08-06 17:35:07 | INFO | train_inner | epoch 005:   1362 / 3715 loss=7.264, nll_loss=3.537, mask_ins=1.084, word_ins_ml=5.008, word_reposition=0.373, kpe=0.799, ppl=153.67, wps=5233.8, ups=0.36, wpb=14636.5, bsz=1024, num_updates=16200, lr=0.000277778, gnorm=1.426, clip=0, loss_scale=261, train_wall=249, wall=46789
2022-08-06 17:39:47 | INFO | train_inner | epoch 005:   1462 / 3715 loss=7.262, nll_loss=3.543, mask_ins=1.085, word_ins_ml=5.013, word_reposition=0.362, kpe=0.802, ppl=153.44, wps=5224.5, ups=0.36, wpb=14644.1, bsz=1024, num_updates=16300, lr=0.000276924, gnorm=1.415, clip=0, loss_scale=512, train_wall=249, wall=47069
2022-08-06 17:44:27 | INFO | train_inner | epoch 005:   1562 / 3715 loss=7.289, nll_loss=3.557, mask_ins=1.091, word_ins_ml=5.026, word_reposition=0.367, kpe=0.805, ppl=156.39, wps=5231.7, ups=0.36, wpb=14665.3, bsz=1024, num_updates=16400, lr=0.000276079, gnorm=1.876, clip=0, loss_scale=512, train_wall=249, wall=47350
2022-08-06 17:49:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-06 17:49:10 | INFO | train_inner | epoch 005:   1663 / 3715 loss=7.236, nll_loss=3.517, mask_ins=1.081, word_ins_ml=4.99, word_reposition=0.362, kpe=0.803, ppl=150.73, wps=5197.9, ups=0.35, wpb=14694.9, bsz=1024, num_updates=16500, lr=0.000275241, gnorm=1.651, clip=0, loss_scale=507, train_wall=251, wall=47632
2022-08-06 17:53:49 | INFO | train_inner | epoch 005:   1763 / 3715 loss=7.282, nll_loss=3.559, mask_ins=1.085, word_ins_ml=5.028, word_reposition=0.366, kpe=0.803, ppl=155.63, wps=5225.2, ups=0.36, wpb=14590.1, bsz=1024, num_updates=16600, lr=0.000274411, gnorm=1.465, clip=0, loss_scale=256, train_wall=248, wall=47912
2022-08-06 17:58:28 | INFO | train_inner | epoch 005:   1863 / 3715 loss=7.246, nll_loss=3.535, mask_ins=1.08, word_ins_ml=5.005, word_reposition=0.362, kpe=0.799, ppl=151.78, wps=5253.2, ups=0.36, wpb=14626, bsz=1024, num_updates=16700, lr=0.000273588, gnorm=1.416, clip=0, loss_scale=256, train_wall=247, wall=48190
2022-08-06 18:01:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-06 18:03:10 | INFO | train_inner | epoch 005:   1964 / 3715 loss=7.267, nll_loss=3.545, mask_ins=1.088, word_ins_ml=5.015, word_reposition=0.361, kpe=0.803, ppl=154.01, wps=5197.6, ups=0.35, wpb=14692, bsz=1024, num_updates=16800, lr=0.000272772, gnorm=1.626, clip=0, loss_scale=212, train_wall=251, wall=48473
2022-08-06 18:07:49 | INFO | train_inner | epoch 005:   2064 / 3715 loss=7.225, nll_loss=3.518, mask_ins=1.076, word_ins_ml=4.99, word_reposition=0.36, kpe=0.799, ppl=149.65, wps=5268.7, ups=0.36, wpb=14705.8, bsz=1024, num_updates=16900, lr=0.000271964, gnorm=1.423, clip=0, loss_scale=128, train_wall=248, wall=48752
2022-08-06 18:12:31 | INFO | train_inner | epoch 005:   2164 / 3715 loss=7.26, nll_loss=3.537, mask_ins=1.086, word_ins_ml=5.008, word_reposition=0.367, kpe=0.8, ppl=153.31, wps=5208, ups=0.36, wpb=14646.2, bsz=1024, num_updates=17000, lr=0.000271163, gnorm=1.447, clip=0, loss_scale=128, train_wall=250, wall=49033
2022-08-06 18:17:10 | INFO | train_inner | epoch 005:   2264 / 3715 loss=7.22, nll_loss=3.51, mask_ins=1.082, word_ins_ml=4.984, word_reposition=0.357, kpe=0.796, ppl=149.1, wps=5217.9, ups=0.36, wpb=14558.5, bsz=1024, num_updates=17100, lr=0.000270369, gnorm=1.418, clip=0, loss_scale=128, train_wall=247, wall=49312
2022-08-06 18:21:49 | INFO | train_inner | epoch 005:   2364 / 3715 loss=7.26, nll_loss=3.533, mask_ins=1.081, word_ins_ml=5.003, word_reposition=0.371, kpe=0.805, ppl=153.23, wps=5312.8, ups=0.36, wpb=14821.4, bsz=1024, num_updates=17200, lr=0.000269582, gnorm=1.422, clip=0, loss_scale=128, train_wall=248, wall=49591
2022-08-06 18:26:28 | INFO | train_inner | epoch 005:   2464 / 3715 loss=7.227, nll_loss=3.514, mask_ins=1.079, word_ins_ml=4.986, word_reposition=0.36, kpe=0.801, ppl=149.79, wps=5238, ups=0.36, wpb=14655.3, bsz=1024, num_updates=17300, lr=0.000268802, gnorm=1.466, clip=0, loss_scale=157, train_wall=249, wall=49871
2022-08-06 18:31:08 | INFO | train_inner | epoch 005:   2564 / 3715 loss=7.265, nll_loss=3.548, mask_ins=1.082, word_ins_ml=5.017, word_reposition=0.364, kpe=0.802, ppl=153.78, wps=5254.2, ups=0.36, wpb=14712.6, bsz=1024, num_updates=17400, lr=0.000268028, gnorm=1.447, clip=0, loss_scale=256, train_wall=249, wall=50151
2022-08-06 18:38:49 | INFO | train_inner | epoch 005:   2664 / 3715 loss=7.234, nll_loss=3.522, mask_ins=1.078, word_ins_ml=4.994, word_reposition=0.365, kpe=0.797, ppl=150.58, wps=3173.7, ups=0.22, wpb=14621.7, bsz=1024, num_updates=17500, lr=0.000267261, gnorm=1.397, clip=0, loss_scale=256, train_wall=429, wall=50611
2022-08-06 18:43:28 | INFO | train_inner | epoch 005:   2764 / 3715 loss=7.271, nll_loss=3.542, mask_ins=1.085, word_ins_ml=5.011, word_reposition=0.372, kpe=0.802, ppl=154.41, wps=5255.2, ups=0.36, wpb=14679.9, bsz=1024, num_updates=17600, lr=0.000266501, gnorm=1.481, clip=0, loss_scale=256, train_wall=248, wall=50891
2022-08-06 18:48:08 | INFO | train_inner | epoch 005:   2864 / 3715 loss=7.228, nll_loss=3.519, mask_ins=1.08, word_ins_ml=4.991, word_reposition=0.358, kpe=0.799, ppl=149.87, wps=5256.7, ups=0.36, wpb=14693.4, bsz=1024, num_updates=17700, lr=0.000265747, gnorm=1.469, clip=0, loss_scale=256, train_wall=248, wall=51170
2022-08-06 18:52:46 | INFO | train_inner | epoch 005:   2964 / 3715 loss=7.227, nll_loss=3.52, mask_ins=1.074, word_ins_ml=4.992, word_reposition=0.362, kpe=0.799, ppl=149.77, wps=5260.7, ups=0.36, wpb=14650.2, bsz=1024, num_updates=17800, lr=0.000264999, gnorm=1.402, clip=0, loss_scale=284, train_wall=247, wall=51449
2022-08-06 18:57:27 | INFO | train_inner | epoch 005:   3064 / 3715 loss=7.23, nll_loss=3.513, mask_ins=1.081, word_ins_ml=4.986, word_reposition=0.363, kpe=0.8, ppl=150.1, wps=5244.6, ups=0.36, wpb=14708.8, bsz=1024, num_updates=17900, lr=0.000264258, gnorm=1.425, clip=0, loss_scale=512, train_wall=249, wall=51729
2022-08-06 19:02:07 | INFO | train_inner | epoch 005:   3164 / 3715 loss=7.201, nll_loss=3.508, mask_ins=1.075, word_ins_ml=4.981, word_reposition=0.35, kpe=0.795, ppl=147.08, wps=5214.1, ups=0.36, wpb=14611.6, bsz=1024, num_updates=18000, lr=0.000263523, gnorm=1.414, clip=0, loss_scale=512, train_wall=249, wall=52010
2022-08-06 19:06:45 | INFO | train_inner | epoch 005:   3264 / 3715 loss=7.192, nll_loss=3.489, mask_ins=1.076, word_ins_ml=4.964, word_reposition=0.357, kpe=0.795, ppl=146.25, wps=5240.3, ups=0.36, wpb=14582.9, bsz=1024, num_updates=18100, lr=0.000262794, gnorm=1.398, clip=0, loss_scale=512, train_wall=247, wall=52288
2022-08-06 19:11:25 | INFO | train_inner | epoch 005:   3364 / 3715 loss=7.244, nll_loss=3.53, mask_ins=1.075, word_ins_ml=5, word_reposition=0.367, kpe=0.801, ppl=151.55, wps=5248.5, ups=0.36, wpb=14663, bsz=1024, num_updates=18200, lr=0.000262071, gnorm=1.418, clip=0, loss_scale=512, train_wall=248, wall=52567
2022-08-06 19:12:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-06 19:13:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-06 19:14:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-08-06 19:16:13 | INFO | train_inner | epoch 005:   3467 / 3715 loss=7.307, nll_loss=3.575, mask_ins=1.083, word_ins_ml=5.041, word_reposition=0.375, kpe=0.807, ppl=158.33, wps=5088.1, ups=0.35, wpb=14642.2, bsz=1024, num_updates=18300, lr=0.000261354, gnorm=2.921, clip=2, loss_scale=219, train_wall=255, wall=52855
2022-08-06 19:18:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-08-06 19:20:56 | INFO | train_inner | epoch 005:   3568 / 3715 loss=7.238, nll_loss=3.527, mask_ins=1.08, word_ins_ml=4.998, word_reposition=0.358, kpe=0.802, ppl=150.94, wps=5180.7, ups=0.35, wpb=14666.2, bsz=1024, num_updates=18400, lr=0.000260643, gnorm=1.98, clip=1, loss_scale=44, train_wall=252, wall=53138
2022-08-06 19:25:35 | INFO | train_inner | epoch 005:   3668 / 3715 loss=7.196, nll_loss=3.493, mask_ins=1.074, word_ins_ml=4.967, word_reposition=0.359, kpe=0.795, ppl=146.61, wps=5222.8, ups=0.36, wpb=14576.4, bsz=1024, num_updates=18500, lr=0.000259938, gnorm=2.02, clip=1, loss_scale=32, train_wall=248, wall=53417
2022-08-06 19:27:44 | INFO | train | epoch 005 | loss 7.258 | nll_loss 3.538 | mask_ins 1.084 | word_ins_ml 5.009 | word_reposition 0.365 | kpe 0.801 | ppl 153.07 | wps 5036.9 | ups 0.34 | wpb 14661.1 | bsz 1023.7 | num_updates 18547 | lr 0.000259608 | gnorm 1.54 | clip 0.1 | loss_scale 262 | train_wall 9398 | wall 53546
2022-08-06 19:31:25 | INFO | valid | epoch 005 | valid on 'valid' subset | loss nan | nll_loss 3.4 | mask_ins 1.06 | word_ins_ml 4.944 | word_reposition 0.357 | kpe nan | ppl nan | wps 12390 | wpb 1849.4 | bsz 127.9 | num_updates 18547 | best_loss nan
2022-08-06 19:31:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 5 @ 18547 updates, score nan) (writing took 22.865413000807166 seconds)
2022-08-06 19:34:16 | INFO | train_inner | epoch 006:     53 / 3715 loss=7.156, nll_loss=3.466, mask_ins=1.071, word_ins_ml=4.944, word_reposition=0.357, kpe=0.783, ppl=142.62, wps=2769.7, ups=0.19, wpb=14430.3, bsz=1014.7, num_updates=18600, lr=0.000259238, gnorm=1.453, clip=0, loss_scale=32, train_wall=246, wall=53938
2022-08-06 19:38:55 | INFO | train_inner | epoch 006:    153 / 3715 loss=7.117, nll_loss=3.437, mask_ins=1.066, word_ins_ml=4.919, word_reposition=0.355, kpe=0.777, ppl=138.77, wps=5278.7, ups=0.36, wpb=14720.7, bsz=1024, num_updates=18700, lr=0.000258544, gnorm=1.426, clip=0, loss_scale=32, train_wall=247, wall=54217
2022-08-06 19:43:32 | INFO | train_inner | epoch 006:    253 / 3715 loss=7.108, nll_loss=3.422, mask_ins=1.07, word_ins_ml=4.905, word_reposition=0.358, kpe=0.775, ppl=137.91, wps=5257.3, ups=0.36, wpb=14605.8, bsz=1023.8, num_updates=18800, lr=0.000257855, gnorm=1.411, clip=0, loss_scale=32, train_wall=246, wall=54495
2022-08-06 19:48:13 | INFO | train_inner | epoch 006:    353 / 3715 loss=7.118, nll_loss=3.433, mask_ins=1.067, word_ins_ml=4.915, word_reposition=0.358, kpe=0.778, ppl=138.88, wps=5227.8, ups=0.36, wpb=14685.8, bsz=1024, num_updates=18900, lr=0.000257172, gnorm=1.425, clip=0, loss_scale=48, train_wall=249, wall=54776
2022-08-06 19:52:53 | INFO | train_inner | epoch 006:    453 / 3715 loss=7.115, nll_loss=3.44, mask_ins=1.068, word_ins_ml=4.921, word_reposition=0.348, kpe=0.778, ppl=138.61, wps=5199.2, ups=0.36, wpb=14538.8, bsz=1024, num_updates=19000, lr=0.000256495, gnorm=1.594, clip=0, loss_scale=64, train_wall=249, wall=55055
2022-08-06 19:57:33 | INFO | train_inner | epoch 006:    553 / 3715 loss=7.146, nll_loss=3.466, mask_ins=1.069, word_ins_ml=4.944, word_reposition=0.354, kpe=0.779, ppl=141.6, wps=5223.2, ups=0.36, wpb=14606.3, bsz=1024, num_updates=19100, lr=0.000255822, gnorm=1.525, clip=0, loss_scale=64, train_wall=248, wall=55335
2022-08-06 19:58:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-08-06 19:58:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2022-08-06 19:58:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0
2022-08-06 20:02:20 | INFO | train_inner | epoch 006:    656 / 3715 loss=7.358, nll_loss=3.591, mask_ins=1.098, word_ins_ml=5.054, word_reposition=0.411, kpe=0.796, ppl=164.11, wps=5124.4, ups=0.35, wpb=14728, bsz=1024, num_updates=19200, lr=0.000255155, gnorm=8.376, clip=4, loss_scale=14, train_wall=255, wall=55622
2022-08-06 20:06:59 | INFO | train_inner | epoch 006:    756 / 3715 loss=7.11, nll_loss=3.432, mask_ins=1.061, word_ins_ml=4.913, word_reposition=0.354, kpe=0.782, ppl=138.09, wps=5292.1, ups=0.36, wpb=14784, bsz=1024, num_updates=19300, lr=0.000254493, gnorm=2.085, clip=1, loss_scale=8, train_wall=248, wall=55902
2022-08-06 20:11:39 | INFO | train_inner | epoch 006:    856 / 3715 loss=7.132, nll_loss=3.445, mask_ins=1.07, word_ins_ml=4.925, word_reposition=0.358, kpe=0.779, ppl=140.31, wps=5279.4, ups=0.36, wpb=14740.6, bsz=1024, num_updates=19400, lr=0.000253837, gnorm=1.417, clip=0, loss_scale=8, train_wall=248, wall=56181
2022-08-06 20:16:17 | INFO | train_inner | epoch 006:    956 / 3715 loss=7.144, nll_loss=3.459, mask_ins=1.067, word_ins_ml=4.936, word_reposition=0.365, kpe=0.777, ppl=141.44, wps=5248.9, ups=0.36, wpb=14627.2, bsz=1024, num_updates=19500, lr=0.000253185, gnorm=1.409, clip=0, loss_scale=8, train_wall=247, wall=56460
2022-08-06 20:20:57 | INFO | train_inner | epoch 006:   1056 / 3715 loss=7.141, nll_loss=3.449, mask_ins=1.067, word_ins_ml=4.928, word_reposition=0.361, kpe=0.785, ppl=141.09, wps=5293, ups=0.36, wpb=14802.5, bsz=1024, num_updates=19600, lr=0.000252538, gnorm=1.441, clip=0, loss_scale=8, train_wall=248, wall=56739
2022-08-06 20:25:37 | INFO | train_inner | epoch 006:   1156 / 3715 loss=7.103, nll_loss=3.429, mask_ins=1.061, word_ins_ml=4.91, word_reposition=0.352, kpe=0.781, ppl=137.47, wps=5265.4, ups=0.36, wpb=14755.7, bsz=1024, num_updates=19700, lr=0.000251896, gnorm=1.413, clip=0, loss_scale=14, train_wall=249, wall=57020
2022-08-06 20:30:16 | INFO | train_inner | epoch 006:   1256 / 3715 loss=7.127, nll_loss=3.442, mask_ins=1.069, word_ins_ml=4.922, word_reposition=0.354, kpe=0.782, ppl=139.74, wps=5263.1, ups=0.36, wpb=14691, bsz=1024, num_updates=19800, lr=0.000251259, gnorm=1.43, clip=0, loss_scale=16, train_wall=248, wall=57299
2022-08-06 20:34:55 | INFO | train_inner | epoch 006:   1356 / 3715 loss=7.123, nll_loss=3.435, mask_ins=1.068, word_ins_ml=4.916, word_reposition=0.356, kpe=0.782, ppl=139.37, wps=5251, ups=0.36, wpb=14629.8, bsz=1024, num_updates=19900, lr=0.000250627, gnorm=1.469, clip=0, loss_scale=16, train_wall=247, wall=57577
2022-08-06 20:39:35 | INFO | train_inner | epoch 006:   1456 / 3715 loss=7.141, nll_loss=3.443, mask_ins=1.074, word_ins_ml=4.923, word_reposition=0.361, kpe=0.783, ppl=141.11, wps=5197.2, ups=0.36, wpb=14578.8, bsz=1024, num_updates=20000, lr=0.00025, gnorm=2.075, clip=1, loss_scale=16, train_wall=249, wall=57858
2022-08-06 20:44:15 | INFO | train_inner | epoch 006:   1556 / 3715 loss=7.134, nll_loss=3.451, mask_ins=1.064, word_ins_ml=4.93, word_reposition=0.356, kpe=0.784, ppl=140.42, wps=5287.7, ups=0.36, wpb=14777.6, bsz=1024, num_updates=20100, lr=0.000249377, gnorm=1.558, clip=0, loss_scale=16, train_wall=248, wall=58137
2022-08-06 20:48:56 | INFO | train_inner | epoch 006:   1656 / 3715 loss=7.115, nll_loss=3.434, mask_ins=1.064, word_ins_ml=4.915, word_reposition=0.353, kpe=0.783, ppl=138.59, wps=5223.7, ups=0.36, wpb=14669.3, bsz=1024, num_updates=20200, lr=0.000248759, gnorm=1.428, clip=0, loss_scale=27, train_wall=249, wall=58418
2022-08-06 20:53:35 | INFO | train_inner | epoch 006:   1756 / 3715 loss=7.12, nll_loss=3.442, mask_ins=1.064, word_ins_ml=4.922, word_reposition=0.354, kpe=0.78, ppl=139.09, wps=5258.9, ups=0.36, wpb=14689.8, bsz=1024, num_updates=20300, lr=0.000248146, gnorm=1.405, clip=0, loss_scale=32, train_wall=248, wall=58698
2022-08-06 20:58:14 | INFO | train_inner | epoch 006:   1856 / 3715 loss=7.13, nll_loss=3.453, mask_ins=1.063, word_ins_ml=4.932, word_reposition=0.355, kpe=0.78, ppl=140.05, wps=5227.2, ups=0.36, wpb=14577.3, bsz=1024, num_updates=20400, lr=0.000247537, gnorm=1.441, clip=0, loss_scale=32, train_wall=248, wall=58976
2022-08-06 21:02:53 | INFO | train_inner | epoch 006:   1956 / 3715 loss=7.126, nll_loss=3.438, mask_ins=1.065, word_ins_ml=4.918, word_reposition=0.36, kpe=0.783, ppl=139.67, wps=5268.1, ups=0.36, wpb=14697.1, bsz=1024, num_updates=20500, lr=0.000246932, gnorm=1.45, clip=0, loss_scale=32, train_wall=248, wall=59255
2022-08-06 21:07:32 | INFO | train_inner | epoch 006:   2056 / 3715 loss=7.124, nll_loss=3.44, mask_ins=1.064, word_ins_ml=4.92, word_reposition=0.358, kpe=0.782, ppl=139.49, wps=5257.8, ups=0.36, wpb=14652.4, bsz=1024, num_updates=20600, lr=0.000246332, gnorm=1.414, clip=0, loss_scale=32, train_wall=247, wall=59534
2022-08-06 21:12:10 | INFO | train_inner | epoch 006:   2156 / 3715 loss=7.103, nll_loss=3.429, mask_ins=1.059, word_ins_ml=4.91, word_reposition=0.351, kpe=0.782, ppl=137.5, wps=5235.3, ups=0.36, wpb=14593.9, bsz=1024, num_updates=20700, lr=0.000245737, gnorm=1.732, clip=0, loss_scale=49, train_wall=247, wall=59813
2022-08-06 21:16:50 | INFO | train_inner | epoch 006:   2256 / 3715 loss=7.123, nll_loss=3.441, mask_ins=1.064, word_ins_ml=4.92, word_reposition=0.357, kpe=0.782, ppl=139.39, wps=5273.1, ups=0.36, wpb=14729.6, bsz=1024, num_updates=20800, lr=0.000245145, gnorm=1.506, clip=0, loss_scale=64, train_wall=248, wall=60092
2022-08-06 21:19:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-08-06 21:21:32 | INFO | train_inner | epoch 006:   2357 / 3715 loss=7.113, nll_loss=3.443, mask_ins=1.057, word_ins_ml=4.922, word_reposition=0.348, kpe=0.785, ppl=138.4, wps=5186.2, ups=0.35, wpb=14646.7, bsz=1024, num_updates=20900, lr=0.000244558, gnorm=2.241, clip=1, loss_scale=52, train_wall=251, wall=60375
2022-08-06 21:26:12 | INFO | train_inner | epoch 006:   2457 / 3715 loss=7.119, nll_loss=3.425, mask_ins=1.065, word_ins_ml=4.907, word_reposition=0.362, kpe=0.785, ppl=138.97, wps=5271, ups=0.36, wpb=14749.9, bsz=1024, num_updates=21000, lr=0.000243975, gnorm=1.687, clip=0, loss_scale=32, train_wall=248, wall=60654
2022-08-06 21:30:52 | INFO | train_inner | epoch 006:   2557 / 3715 loss=7.107, nll_loss=3.431, mask_ins=1.062, word_ins_ml=4.912, word_reposition=0.352, kpe=0.781, ppl=137.82, wps=5213.3, ups=0.36, wpb=14596, bsz=1024, num_updates=21100, lr=0.000243396, gnorm=1.437, clip=0, loss_scale=32, train_wall=249, wall=60934
2022-08-06 21:35:32 | INFO | train_inner | epoch 006:   2657 / 3715 loss=7.103, nll_loss=3.424, mask_ins=1.059, word_ins_ml=4.905, word_reposition=0.357, kpe=0.783, ppl=137.5, wps=5244.4, ups=0.36, wpb=14657.2, bsz=1024, num_updates=21200, lr=0.000242821, gnorm=1.801, clip=1, loss_scale=32, train_wall=248, wall=61214
2022-08-06 21:40:12 | INFO | train_inner | epoch 006:   2757 / 3715 loss=7.085, nll_loss=3.412, mask_ins=1.063, word_ins_ml=4.895, word_reposition=0.35, kpe=0.778, ppl=135.74, wps=5194.4, ups=0.36, wpb=14573.2, bsz=1024, num_updates=21300, lr=0.000242251, gnorm=1.421, clip=0, loss_scale=32, train_wall=249, wall=61494
2022-08-06 21:44:51 | INFO | train_inner | epoch 006:   2857 / 3715 loss=7.107, nll_loss=3.434, mask_ins=1.059, word_ins_ml=4.915, word_reposition=0.351, kpe=0.782, ppl=137.87, wps=5257.3, ups=0.36, wpb=14650.4, bsz=1024, num_updates=21400, lr=0.000241684, gnorm=1.434, clip=0, loss_scale=40, train_wall=247, wall=61773
2022-08-06 21:49:30 | INFO | train_inner | epoch 006:   2957 / 3715 loss=7.088, nll_loss=3.423, mask_ins=1.057, word_ins_ml=4.904, word_reposition=0.346, kpe=0.781, ppl=136.09, wps=5259.1, ups=0.36, wpb=14674.1, bsz=1024, num_updates=21500, lr=0.000241121, gnorm=1.676, clip=0, loss_scale=64, train_wall=248, wall=62052
2022-08-06 21:54:09 | INFO | train_inner | epoch 006:   3057 / 3715 loss=7.093, nll_loss=3.421, mask_ins=1.061, word_ins_ml=4.903, word_reposition=0.349, kpe=0.781, ppl=136.56, wps=5256.9, ups=0.36, wpb=14678.4, bsz=1024, num_updates=21600, lr=0.000240563, gnorm=1.473, clip=0, loss_scale=64, train_wall=248, wall=62331
2022-08-06 21:58:47 | INFO | train_inner | epoch 006:   3157 / 3715 loss=7.094, nll_loss=3.412, mask_ins=1.058, word_ins_ml=4.895, word_reposition=0.359, kpe=0.781, ppl=136.59, wps=5265.2, ups=0.36, wpb=14645.3, bsz=1024, num_updates=21700, lr=0.000240008, gnorm=1.634, clip=0, loss_scale=64, train_wall=247, wall=62610
2022-08-06 22:03:27 | INFO | train_inner | epoch 006:   3257 / 3715 loss=7.124, nll_loss=3.458, mask_ins=1.058, word_ins_ml=4.935, word_reposition=0.35, kpe=0.78, ppl=139.54, wps=5220.8, ups=0.36, wpb=14601.4, bsz=1024, num_updates=21800, lr=0.000239457, gnorm=1.401, clip=0, loss_scale=64, train_wall=248, wall=62889
2022-08-06 22:08:06 | INFO | train_inner | epoch 006:   3357 / 3715 loss=7.098, nll_loss=3.431, mask_ins=1.062, word_ins_ml=4.911, word_reposition=0.347, kpe=0.778, ppl=137.03, wps=5240.5, ups=0.36, wpb=14620.8, bsz=1024, num_updates=21900, lr=0.000238909, gnorm=1.42, clip=0, loss_scale=72, train_wall=247, wall=63168
2022-08-06 22:12:45 | INFO | train_inner | epoch 006:   3457 / 3715 loss=7.124, nll_loss=3.444, mask_ins=1.064, word_ins_ml=4.922, word_reposition=0.354, kpe=0.784, ppl=139.46, wps=5256.9, ups=0.36, wpb=14670.5, bsz=1024, num_updates=22000, lr=0.000238366, gnorm=1.414, clip=0, loss_scale=128, train_wall=248, wall=63447
2022-08-06 22:17:24 | INFO | train_inner | epoch 006:   3557 / 3715 loss=7.087, nll_loss=3.41, mask_ins=1.058, word_ins_ml=4.893, word_reposition=0.352, kpe=0.784, ppl=135.92, wps=5262, ups=0.36, wpb=14704.3, bsz=1024, num_updates=22100, lr=0.000237826, gnorm=1.439, clip=0, loss_scale=128, train_wall=248, wall=63727
2022-08-06 22:22:03 | INFO | train_inner | epoch 006:   3657 / 3715 loss=7.095, nll_loss=3.419, mask_ins=1.059, word_ins_ml=4.9, word_reposition=0.353, kpe=0.782, ppl=136.68, wps=5262.6, ups=0.36, wpb=14688.9, bsz=1024, num_updates=22200, lr=0.000237289, gnorm=1.412, clip=0, loss_scale=128, train_wall=248, wall=64006
2022-08-06 22:24:44 | INFO | train | epoch 006 | loss 7.121 | nll_loss 3.439 | mask_ins 1.064 | word_ins_ml 4.919 | word_reposition 0.356 | kpe 0.781 | ppl 139.21 | wps 5123.8 | ups 0.35 | wpb 14662.4 | bsz 1023.7 | num_updates 22258 | lr 0.00023698 | gnorm 1.722 | clip 0.2 | loss_scale 45 | train_wall 9210 | wall 64166
2022-08-06 22:28:24 | INFO | valid | epoch 006 | valid on 'valid' subset | loss nan | nll_loss 3.344 | mask_ins 1.044 | word_ins_ml 4.89 | word_reposition 0.356 | kpe nan | ppl nan | wps 12417.9 | wpb 1849.4 | bsz 127.9 | num_updates 22258 | best_loss nan
2022-08-06 22:28:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 6 @ 22258 updates, score nan) (writing took 9.504737995564938 seconds)
2022-08-06 22:30:30 | INFO | train_inner | epoch 007:     42 / 3715 loss=7.052, nll_loss=3.389, mask_ins=1.051, word_ins_ml=4.875, word_reposition=0.356, kpe=0.771, ppl=132.74, wps=2856.1, ups=0.2, wpb=14477.2, bsz=1014.7, num_updates=22300, lr=0.000236757, gnorm=1.446, clip=0, loss_scale=128, train_wall=246, wall=64513
2022-08-06 22:35:11 | INFO | train_inner | epoch 007:    142 / 3715 loss=6.997, nll_loss=3.35, mask_ins=1.049, word_ins_ml=4.84, word_reposition=0.35, kpe=0.757, ppl=127.7, wps=5202, ups=0.36, wpb=14590.6, bsz=1024, num_updates=22400, lr=0.000236228, gnorm=1.433, clip=0, loss_scale=129, train_wall=249, wall=64793
2022-08-06 22:39:51 | INFO | train_inner | epoch 007:    242 / 3715 loss=7.006, nll_loss=3.362, mask_ins=1.051, word_ins_ml=4.85, word_reposition=0.35, kpe=0.755, ppl=128.51, wps=5208.1, ups=0.36, wpb=14579.4, bsz=1024, num_updates=22500, lr=0.000235702, gnorm=1.424, clip=0, loss_scale=256, train_wall=249, wall=65073
2022-08-06 22:44:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-06 22:44:33 | INFO | train_inner | epoch 007:    343 / 3715 loss=6.985, nll_loss=3.33, mask_ins=1.048, word_ins_ml=4.822, word_reposition=0.349, kpe=0.765, ppl=126.67, wps=5236.4, ups=0.35, wpb=14772.1, bsz=1024, num_updates=22600, lr=0.00023518, gnorm=1.989, clip=1, loss_scale=247, train_wall=251, wall=65355
2022-08-06 22:49:11 | INFO | train_inner | epoch 007:    443 / 3715 loss=7.01, nll_loss=3.361, mask_ins=1.048, word_ins_ml=4.849, word_reposition=0.351, kpe=0.762, ppl=128.91, wps=5265.1, ups=0.36, wpb=14653, bsz=1024, num_updates=22700, lr=0.000234662, gnorm=2.358, clip=1, loss_scale=128, train_wall=247, wall=65634
2022-08-06 22:53:50 | INFO | train_inner | epoch 007:    543 / 3715 loss=6.994, nll_loss=3.349, mask_ins=1.047, word_ins_ml=4.839, word_reposition=0.346, kpe=0.761, ppl=127.49, wps=5256.9, ups=0.36, wpb=14654, bsz=1024, num_updates=22800, lr=0.000234146, gnorm=1.43, clip=0, loss_scale=128, train_wall=248, wall=65912
2022-08-06 22:58:29 | INFO | train_inner | epoch 007:    643 / 3715 loss=6.995, nll_loss=3.346, mask_ins=1.044, word_ins_ml=4.836, word_reposition=0.355, kpe=0.759, ppl=127.52, wps=5259.5, ups=0.36, wpb=14679, bsz=1024, num_updates=22900, lr=0.000233635, gnorm=1.425, clip=0, loss_scale=128, train_wall=248, wall=66191
2022-08-06 23:03:08 | INFO | train_inner | epoch 007:    743 / 3715 loss=6.996, nll_loss=3.352, mask_ins=1.046, word_ins_ml=4.841, word_reposition=0.348, kpe=0.76, ppl=127.62, wps=5246.3, ups=0.36, wpb=14638.8, bsz=1024, num_updates=23000, lr=0.000233126, gnorm=1.443, clip=0, loss_scale=128, train_wall=247, wall=66470
2022-08-06 23:07:47 | INFO | train_inner | epoch 007:    843 / 3715 loss=7.008, nll_loss=3.362, mask_ins=1.048, word_ins_ml=4.85, word_reposition=0.349, kpe=0.761, ppl=128.73, wps=5255.8, ups=0.36, wpb=14681.5, bsz=1024, num_updates=23100, lr=0.000232621, gnorm=1.444, clip=0, loss_scale=128, train_wall=248, wall=66750
2022-08-06 23:12:27 | INFO | train_inner | epoch 007:    943 / 3715 loss=7.003, nll_loss=3.351, mask_ins=1.054, word_ins_ml=4.84, word_reposition=0.348, kpe=0.761, ppl=128.27, wps=5248.3, ups=0.36, wpb=14649.8, bsz=1024, num_updates=23200, lr=0.000232119, gnorm=1.454, clip=0, loss_scale=250, train_wall=248, wall=67029
2022-08-06 23:17:06 | INFO | train_inner | epoch 007:   1043 / 3715 loss=7.016, nll_loss=3.366, mask_ins=1.049, word_ins_ml=4.853, word_reposition=0.355, kpe=0.758, ppl=129.44, wps=5247.9, ups=0.36, wpb=14664.2, bsz=1024, num_updates=23300, lr=0.000231621, gnorm=1.435, clip=0, loss_scale=256, train_wall=248, wall=67308
2022-08-06 23:21:46 | INFO | train_inner | epoch 007:   1143 / 3715 loss=7.025, nll_loss=3.368, mask_ins=1.049, word_ins_ml=4.855, word_reposition=0.354, kpe=0.767, ppl=130.28, wps=5304.3, ups=0.36, wpb=14826.5, bsz=1024, num_updates=23400, lr=0.000231125, gnorm=1.417, clip=0, loss_scale=256, train_wall=248, wall=67588
2022-08-06 23:26:24 | INFO | train_inner | epoch 007:   1243 / 3715 loss=7.018, nll_loss=3.367, mask_ins=1.046, word_ins_ml=4.855, word_reposition=0.354, kpe=0.763, ppl=129.58, wps=5276.8, ups=0.36, wpb=14673.4, bsz=1024, num_updates=23500, lr=0.000230633, gnorm=1.431, clip=0, loss_scale=256, train_wall=247, wall=67866
2022-08-06 23:31:02 | INFO | train_inner | epoch 007:   1343 / 3715 loss=7.002, nll_loss=3.355, mask_ins=1.045, word_ins_ml=4.844, word_reposition=0.351, kpe=0.763, ppl=128.19, wps=5278.2, ups=0.36, wpb=14704.6, bsz=1024, num_updates=23600, lr=0.000230144, gnorm=1.436, clip=0, loss_scale=256, train_wall=247, wall=68145
2022-08-06 23:35:42 | INFO | train_inner | epoch 007:   1443 / 3715 loss=7.01, nll_loss=3.353, mask_ins=1.052, word_ins_ml=4.842, word_reposition=0.352, kpe=0.764, ppl=128.9, wps=5276.8, ups=0.36, wpb=14751.9, bsz=1024, num_updates=23700, lr=0.000229658, gnorm=1.495, clip=0, loss_scale=468, train_wall=248, wall=68424
2022-08-06 23:40:20 | INFO | train_inner | epoch 007:   1543 / 3715 loss=6.992, nll_loss=3.348, mask_ins=1.048, word_ins_ml=4.837, word_reposition=0.344, kpe=0.763, ppl=127.33, wps=5257.5, ups=0.36, wpb=14647.7, bsz=1024, num_updates=23800, lr=0.000229175, gnorm=1.456, clip=0, loss_scale=512, train_wall=247, wall=68703
2022-08-06 23:45:00 | INFO | train_inner | epoch 007:   1643 / 3715 loss=7.007, nll_loss=3.362, mask_ins=1.05, word_ins_ml=4.85, word_reposition=0.348, kpe=0.759, ppl=128.63, wps=5246.2, ups=0.36, wpb=14647, bsz=1024, num_updates=23900, lr=0.000228695, gnorm=1.43, clip=0, loss_scale=512, train_wall=248, wall=68982
2022-08-06 23:49:39 | INFO | train_inner | epoch 007:   1743 / 3715 loss=6.986, nll_loss=3.352, mask_ins=1.039, word_ins_ml=4.841, word_reposition=0.347, kpe=0.76, ppl=126.8, wps=5244.6, ups=0.36, wpb=14637.5, bsz=1024, num_updates=24000, lr=0.000228218, gnorm=1.425, clip=0, loss_scale=512, train_wall=248, wall=69261
2022-08-06 23:54:18 | INFO | train_inner | epoch 007:   1843 / 3715 loss=7.014, nll_loss=3.36, mask_ins=1.051, word_ins_ml=4.848, word_reposition=0.352, kpe=0.763, ppl=129.26, wps=5223.6, ups=0.36, wpb=14590.3, bsz=1024, num_updates=24100, lr=0.000227744, gnorm=1.441, clip=0, loss_scale=512, train_wall=248, wall=69540
2022-08-06 23:55:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-06 23:59:01 | INFO | train_inner | epoch 007:   1944 / 3715 loss=7.01, nll_loss=3.369, mask_ins=1.044, word_ins_ml=4.856, word_reposition=0.348, kpe=0.762, ppl=128.91, wps=5160.3, ups=0.35, wpb=14585.6, bsz=1024, num_updates=24200, lr=0.000227273, gnorm=1.416, clip=0, loss_scale=522, train_wall=251, wall=69823
2022-08-07 00:03:39 | INFO | train_inner | epoch 007:   2044 / 3715 loss=6.983, nll_loss=3.336, mask_ins=1.046, word_ins_ml=4.826, word_reposition=0.347, kpe=0.764, ppl=126.5, wps=5269.2, ups=0.36, wpb=14686.9, bsz=1024, num_updates=24300, lr=0.000226805, gnorm=1.423, clip=0, loss_scale=512, train_wall=247, wall=70102
2022-08-07 00:08:18 | INFO | train_inner | epoch 007:   2144 / 3715 loss=6.997, nll_loss=3.356, mask_ins=1.041, word_ins_ml=4.845, word_reposition=0.353, kpe=0.758, ppl=127.7, wps=5232.8, ups=0.36, wpb=14555.8, bsz=1023.8, num_updates=24400, lr=0.000226339, gnorm=1.431, clip=0, loss_scale=512, train_wall=247, wall=70380
2022-08-07 00:12:58 | INFO | train_inner | epoch 007:   2244 / 3715 loss=6.991, nll_loss=3.347, mask_ins=1.046, word_ins_ml=4.836, word_reposition=0.347, kpe=0.762, ppl=127.16, wps=5247.1, ups=0.36, wpb=14699.2, bsz=1024, num_updates=24500, lr=0.000225877, gnorm=1.425, clip=0, loss_scale=512, train_wall=249, wall=70660
2022-08-07 00:17:38 | INFO | train_inner | epoch 007:   2344 / 3715 loss=7.002, nll_loss=3.354, mask_ins=1.041, word_ins_ml=4.842, word_reposition=0.351, kpe=0.768, ppl=128.21, wps=5258, ups=0.36, wpb=14722, bsz=1024, num_updates=24600, lr=0.000225417, gnorm=1.44, clip=0, loss_scale=512, train_wall=249, wall=70940
2022-08-07 00:22:16 | INFO | train_inner | epoch 007:   2444 / 3715 loss=7.024, nll_loss=3.368, mask_ins=1.05, word_ins_ml=4.855, word_reposition=0.354, kpe=0.765, ppl=130.14, wps=5288.3, ups=0.36, wpb=14695.4, bsz=1024, num_updates=24700, lr=0.000224961, gnorm=1.432, clip=0, loss_scale=809, train_wall=247, wall=71218
2022-08-07 00:26:55 | INFO | train_inner | epoch 007:   2544 / 3715 loss=6.991, nll_loss=3.344, mask_ins=1.046, word_ins_ml=4.833, word_reposition=0.347, kpe=0.765, ppl=127.16, wps=5266.9, ups=0.36, wpb=14701.6, bsz=1024, num_updates=24800, lr=0.000224507, gnorm=1.432, clip=0, loss_scale=1024, train_wall=248, wall=71497
2022-08-07 00:31:33 | INFO | train_inner | epoch 007:   2644 / 3715 loss=6.998, nll_loss=3.362, mask_ins=1.044, word_ins_ml=4.849, word_reposition=0.345, kpe=0.76, ppl=127.8, wps=5229.3, ups=0.36, wpb=14578.2, bsz=1024, num_updates=24900, lr=0.000224055, gnorm=1.453, clip=0, loss_scale=1024, train_wall=247, wall=71776
2022-08-07 00:36:13 | INFO | train_inner | epoch 007:   2744 / 3715 loss=6.975, nll_loss=3.334, mask_ins=1.043, word_ins_ml=4.824, word_reposition=0.345, kpe=0.763, ppl=125.84, wps=5256.5, ups=0.36, wpb=14687.9, bsz=1024, num_updates=25000, lr=0.000223607, gnorm=1.441, clip=0, loss_scale=1024, train_wall=248, wall=72055
2022-08-07 00:40:52 | INFO | train_inner | epoch 007:   2844 / 3715 loss=6.97, nll_loss=3.326, mask_ins=1.04, word_ins_ml=4.817, word_reposition=0.348, kpe=0.764, ppl=125.34, wps=5296.8, ups=0.36, wpb=14767.1, bsz=1024, num_updates=25100, lr=0.000223161, gnorm=1.498, clip=0, loss_scale=1024, train_wall=247, wall=72334
2022-08-07 00:44:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-07 00:45:34 | INFO | train_inner | epoch 007:   2945 / 3715 loss=7.02, nll_loss=3.37, mask_ins=1.048, word_ins_ml=4.856, word_reposition=0.351, kpe=0.764, ppl=129.76, wps=5183, ups=0.35, wpb=14655.3, bsz=1024, num_updates=25200, lr=0.000222718, gnorm=1.427, clip=0, loss_scale=1176, train_wall=251, wall=72617
2022-08-07 00:50:13 | INFO | train_inner | epoch 007:   3045 / 3715 loss=6.981, nll_loss=3.341, mask_ins=1.042, word_ins_ml=4.83, word_reposition=0.344, kpe=0.766, ppl=126.33, wps=5249.9, ups=0.36, wpb=14640.7, bsz=1024, num_updates=25300, lr=0.000222277, gnorm=1.704, clip=1, loss_scale=1024, train_wall=248, wall=72896
2022-08-07 00:54:54 | INFO | train_inner | epoch 007:   3145 / 3715 loss=6.988, nll_loss=3.341, mask_ins=1.046, word_ins_ml=4.83, word_reposition=0.352, kpe=0.759, ppl=126.9, wps=5207.4, ups=0.36, wpb=14592.8, bsz=1024, num_updates=25400, lr=0.000221839, gnorm=1.426, clip=0, loss_scale=1024, train_wall=249, wall=73176
2022-08-07 00:59:33 | INFO | train_inner | epoch 007:   3245 / 3715 loss=6.955, nll_loss=3.325, mask_ins=1.034, word_ins_ml=4.816, word_reposition=0.341, kpe=0.763, ppl=124.04, wps=5258.7, ups=0.36, wpb=14692.9, bsz=1024, num_updates=25500, lr=0.000221404, gnorm=1.424, clip=0, loss_scale=1024, train_wall=248, wall=73455
2022-08-07 01:04:12 | INFO | train_inner | epoch 007:   3345 / 3715 loss=6.95, nll_loss=3.318, mask_ins=1.036, word_ins_ml=4.81, word_reposition=0.343, kpe=0.761, ppl=123.62, wps=5242.8, ups=0.36, wpb=14637, bsz=1024, num_updates=25600, lr=0.000220971, gnorm=1.418, clip=0, loss_scale=1024, train_wall=248, wall=73735
2022-08-07 01:08:52 | INFO | train_inner | epoch 007:   3445 / 3715 loss=6.981, nll_loss=3.339, mask_ins=1.04, word_ins_ml=4.829, word_reposition=0.346, kpe=0.766, ppl=126.29, wps=5254.3, ups=0.36, wpb=14707.4, bsz=1024, num_updates=25700, lr=0.000220541, gnorm=1.411, clip=0, loss_scale=1229, train_wall=248, wall=74014
2022-08-07 01:13:32 | INFO | train_inner | epoch 007:   3545 / 3715 loss=6.965, nll_loss=3.328, mask_ins=1.042, word_ins_ml=4.818, word_reposition=0.342, kpe=0.762, ppl=124.96, wps=5243.1, ups=0.36, wpb=14666.4, bsz=1024, num_updates=25800, lr=0.000220113, gnorm=1.447, clip=0, loss_scale=2048, train_wall=248, wall=74294
2022-08-07 01:18:11 | INFO | train_inner | epoch 007:   3645 / 3715 loss=7.008, nll_loss=3.362, mask_ins=1.05, word_ins_ml=4.849, word_reposition=0.345, kpe=0.765, ppl=128.72, wps=5270.7, ups=0.36, wpb=14699.8, bsz=1024, num_updates=25900, lr=0.000219687, gnorm=1.442, clip=0, loss_scale=2048, train_wall=248, wall=74573
2022-08-07 01:21:23 | INFO | train | epoch 007 | loss 6.996 | nll_loss 3.351 | mask_ins 1.046 | word_ins_ml 4.839 | word_reposition 0.349 | kpe 0.762 | ppl 127.64 | wps 5134.6 | ups 0.35 | wpb 14661.8 | bsz 1023.7 | num_updates 25970 | lr 0.000219391 | gnorm 1.484 | clip 0.1 | loss_scale 663 | train_wall 9204 | wall 74766
2022-08-07 01:25:04 | INFO | valid | epoch 007 | valid on 'valid' subset | loss nan | nll_loss 3.294 | mask_ins 1.03 | word_ins_ml 4.836 | word_reposition 0.35 | kpe nan | ppl nan | wps 12402.9 | wpb 1849.4 | bsz 127.9 | num_updates 25970 | best_loss nan
2022-08-07 01:25:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 7 @ 25970 updates, score nan) (writing took 10.819630494341254 seconds)
2022-08-07 01:26:38 | INFO | train_inner | epoch 008:     30 / 3715 loss=6.959, nll_loss=3.328, mask_ins=1.041, word_ins_ml=4.819, word_reposition=0.343, kpe=0.756, ppl=124.41, wps=2849, ups=0.2, wpb=14460, bsz=1014.7, num_updates=26000, lr=0.000219265, gnorm=1.446, clip=0, loss_scale=2048, train_wall=245, wall=75081
2022-08-07 01:31:17 | INFO | train_inner | epoch 008:    130 / 3715 loss=6.903, nll_loss=3.286, mask_ins=1.037, word_ins_ml=4.782, word_reposition=0.341, kpe=0.743, ppl=119.71, wps=5292.7, ups=0.36, wpb=14757.8, bsz=1024, num_updates=26100, lr=0.000218844, gnorm=1.426, clip=0, loss_scale=2048, train_wall=248, wall=75359
2022-08-07 01:34:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-07 01:35:59 | INFO | train_inner | epoch 008:    231 / 3715 loss=6.883, nll_loss=3.272, mask_ins=1.028, word_ins_ml=4.77, word_reposition=0.349, kpe=0.737, ppl=118.06, wps=5224, ups=0.35, wpb=14729, bsz=1024, num_updates=26200, lr=0.000218426, gnorm=1.457, clip=0, loss_scale=1663, train_wall=251, wall=75641
2022-08-07 01:40:38 | INFO | train_inner | epoch 008:    331 / 3715 loss=6.875, nll_loss=3.267, mask_ins=1.031, word_ins_ml=4.765, word_reposition=0.34, kpe=0.739, ppl=117.39, wps=5246.6, ups=0.36, wpb=14648.3, bsz=1024, num_updates=26300, lr=0.00021801, gnorm=1.462, clip=0, loss_scale=1024, train_wall=248, wall=75921
2022-08-07 01:45:17 | INFO | train_inner | epoch 008:    431 / 3715 loss=6.897, nll_loss=3.282, mask_ins=1.034, word_ins_ml=4.779, word_reposition=0.346, kpe=0.739, ppl=119.17, wps=5250.2, ups=0.36, wpb=14659.7, bsz=1024, num_updates=26400, lr=0.000217597, gnorm=1.462, clip=0, loss_scale=1024, train_wall=248, wall=76200
2022-08-07 01:49:56 | INFO | train_inner | epoch 008:    531 / 3715 loss=6.89, nll_loss=3.273, mask_ins=1.031, word_ins_ml=4.771, word_reposition=0.349, kpe=0.74, ppl=118.58, wps=5250.5, ups=0.36, wpb=14648.6, bsz=1024, num_updates=26500, lr=0.000217186, gnorm=1.451, clip=0, loss_scale=1024, train_wall=247, wall=76479
2022-08-07 01:54:36 | INFO | train_inner | epoch 008:    631 / 3715 loss=6.898, nll_loss=3.286, mask_ins=1.037, word_ins_ml=4.782, word_reposition=0.341, kpe=0.738, ppl=119.28, wps=5199.9, ups=0.36, wpb=14554.4, bsz=1024, num_updates=26600, lr=0.000216777, gnorm=1.458, clip=0, loss_scale=1024, train_wall=248, wall=76759
2022-08-07 01:59:15 | INFO | train_inner | epoch 008:    731 / 3715 loss=6.888, nll_loss=3.267, mask_ins=1.036, word_ins_ml=4.765, word_reposition=0.348, kpe=0.74, ppl=118.47, wps=5244.2, ups=0.36, wpb=14637.9, bsz=1024, num_updates=26700, lr=0.000216371, gnorm=1.445, clip=0, loss_scale=1290, train_wall=248, wall=77038
2022-08-07 02:03:54 | INFO | train_inner | epoch 008:    831 / 3715 loss=6.884, nll_loss=3.271, mask_ins=1.029, word_ins_ml=4.769, word_reposition=0.344, kpe=0.742, ppl=118.13, wps=5249.9, ups=0.36, wpb=14646.8, bsz=1024, num_updates=26800, lr=0.000215967, gnorm=1.465, clip=0, loss_scale=2048, train_wall=248, wall=77317
2022-08-07 02:08:33 | INFO | train_inner | epoch 008:    931 / 3715 loss=6.912, nll_loss=3.294, mask_ins=1.038, word_ins_ml=4.789, word_reposition=0.343, kpe=0.743, ppl=120.45, wps=5256.9, ups=0.36, wpb=14645.2, bsz=1024, num_updates=26900, lr=0.000215565, gnorm=1.44, clip=0, loss_scale=2048, train_wall=247, wall=77595
2022-08-07 02:13:13 | INFO | train_inner | epoch 008:   1031 / 3715 loss=6.878, nll_loss=3.271, mask_ins=1.031, word_ins_ml=4.768, word_reposition=0.338, kpe=0.742, ppl=117.63, wps=5226.1, ups=0.36, wpb=14634.9, bsz=1024, num_updates=27000, lr=0.000215166, gnorm=1.443, clip=0, loss_scale=2048, train_wall=249, wall=77875
2022-08-07 02:17:53 | INFO | train_inner | epoch 008:   1131 / 3715 loss=6.881, nll_loss=3.275, mask_ins=1.031, word_ins_ml=4.771, word_reposition=0.341, kpe=0.738, ppl=117.86, wps=5209.8, ups=0.36, wpb=14590.3, bsz=1024, num_updates=27100, lr=0.000214768, gnorm=1.488, clip=0, loss_scale=2048, train_wall=248, wall=78156
2022-08-07 02:22:34 | INFO | train_inner | epoch 008:   1231 / 3715 loss=6.898, nll_loss=3.279, mask_ins=1.031, word_ins_ml=4.775, word_reposition=0.348, kpe=0.744, ppl=119.3, wps=5238.6, ups=0.36, wpb=14693, bsz=1024, num_updates=27200, lr=0.000214373, gnorm=1.424, clip=0, loss_scale=2335, train_wall=249, wall=78436
2022-08-07 02:27:13 | INFO | train_inner | epoch 008:   1331 / 3715 loss=6.894, nll_loss=3.28, mask_ins=1.033, word_ins_ml=4.777, word_reposition=0.342, kpe=0.742, ppl=118.9, wps=5237, ups=0.36, wpb=14621.7, bsz=1024, num_updates=27300, lr=0.00021398, gnorm=1.472, clip=0, loss_scale=4096, train_wall=248, wall=78715
2022-08-07 02:31:53 | INFO | train_inner | epoch 008:   1431 / 3715 loss=6.89, nll_loss=3.271, mask_ins=1.034, word_ins_ml=4.768, word_reposition=0.344, kpe=0.743, ppl=118.62, wps=5250.9, ups=0.36, wpb=14695.4, bsz=1024, num_updates=27400, lr=0.000213589, gnorm=1.445, clip=0, loss_scale=4096, train_wall=248, wall=78995
2022-08-07 02:36:32 | INFO | train_inner | epoch 008:   1531 / 3715 loss=6.884, nll_loss=3.271, mask_ins=1.031, word_ins_ml=4.768, word_reposition=0.342, kpe=0.742, ppl=118.1, wps=5265.8, ups=0.36, wpb=14688.6, bsz=1024, num_updates=27500, lr=0.000213201, gnorm=1.453, clip=0, loss_scale=4096, train_wall=247, wall=79274
2022-08-07 02:41:11 | INFO | train_inner | epoch 008:   1631 / 3715 loss=6.91, nll_loss=3.295, mask_ins=1.033, word_ins_ml=4.789, word_reposition=0.342, kpe=0.746, ppl=120.22, wps=5281.8, ups=0.36, wpb=14739.3, bsz=1024, num_updates=27600, lr=0.000212814, gnorm=1.427, clip=0, loss_scale=4096, train_wall=248, wall=79553
2022-08-07 02:45:50 | INFO | train_inner | epoch 008:   1731 / 3715 loss=6.887, nll_loss=3.27, mask_ins=1.03, word_ins_ml=4.768, word_reposition=0.343, kpe=0.746, ppl=118.39, wps=5280.7, ups=0.36, wpb=14736.6, bsz=1024, num_updates=27700, lr=0.00021243, gnorm=1.432, clip=0, loss_scale=4178, train_wall=248, wall=79832
2022-08-07 02:46:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-07 02:50:33 | INFO | train_inner | epoch 008:   1832 / 3715 loss=6.929, nll_loss=3.31, mask_ins=1.037, word_ins_ml=4.802, word_reposition=0.344, kpe=0.746, ppl=121.83, wps=5155.9, ups=0.35, wpb=14588.3, bsz=1024, num_updates=27800, lr=0.000212047, gnorm=1.446, clip=0, loss_scale=4704, train_wall=251, wall=80115
2022-08-07 02:55:12 | INFO | train_inner | epoch 008:   1932 / 3715 loss=6.896, nll_loss=3.292, mask_ins=1.029, word_ins_ml=4.786, word_reposition=0.337, kpe=0.744, ppl=119.11, wps=5244.6, ups=0.36, wpb=14658.4, bsz=1024, num_updates=27900, lr=0.000211667, gnorm=1.438, clip=0, loss_scale=4096, train_wall=248, wall=80395
2022-08-07 02:58:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 02:59:56 | INFO | train_inner | epoch 008:   2033 / 3715 loss=6.908, nll_loss=3.295, mask_ins=1.035, word_ins_ml=4.789, word_reposition=0.339, kpe=0.746, ppl=120.11, wps=5153.5, ups=0.35, wpb=14621.3, bsz=1024, num_updates=28000, lr=0.000211289, gnorm=1.45, clip=0, loss_scale=3366, train_wall=252, wall=80678
2022-08-07 03:04:35 | INFO | train_inner | epoch 008:   2133 / 3715 loss=6.888, nll_loss=3.276, mask_ins=1.03, word_ins_ml=4.772, word_reposition=0.34, kpe=0.745, ppl=118.43, wps=5248.3, ups=0.36, wpb=14638.8, bsz=1024, num_updates=28100, lr=0.000210912, gnorm=1.432, clip=0, loss_scale=2048, train_wall=247, wall=80957
2022-08-07 03:09:15 | INFO | train_inner | epoch 008:   2233 / 3715 loss=6.898, nll_loss=3.28, mask_ins=1.032, word_ins_ml=4.775, word_reposition=0.346, kpe=0.744, ppl=119.26, wps=5207.2, ups=0.36, wpb=14610, bsz=1024, num_updates=28200, lr=0.000210538, gnorm=1.451, clip=0, loss_scale=2048, train_wall=249, wall=81238
2022-08-07 03:13:54 | INFO | train_inner | epoch 008:   2333 / 3715 loss=6.894, nll_loss=3.279, mask_ins=1.032, word_ins_ml=4.775, word_reposition=0.341, kpe=0.747, ppl=118.92, wps=5276.1, ups=0.36, wpb=14697.6, bsz=1024, num_updates=28300, lr=0.000210166, gnorm=1.444, clip=0, loss_scale=2048, train_wall=247, wall=81516
2022-08-07 03:18:35 | INFO | train_inner | epoch 008:   2433 / 3715 loss=6.884, nll_loss=3.274, mask_ins=1.028, word_ins_ml=4.771, word_reposition=0.342, kpe=0.744, ppl=118.15, wps=5214.6, ups=0.36, wpb=14635.1, bsz=1024, num_updates=28400, lr=0.000209795, gnorm=1.46, clip=0, loss_scale=2048, train_wall=249, wall=81797
2022-08-07 03:20:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-07 03:24:53 | INFO | train_inner | epoch 008:   2534 / 3715 loss=6.868, nll_loss=3.259, mask_ins=1.024, word_ins_ml=4.757, word_reposition=0.342, kpe=0.746, ppl=116.84, wps=3871.8, ups=0.26, wpb=14665.8, bsz=1024, num_updates=28500, lr=0.000209427, gnorm=1.518, clip=0, loss_scale=1399, train_wall=347, wall=82176
2022-08-07 03:31:10 | INFO | train_inner | epoch 008:   2634 / 3715 loss=6.894, nll_loss=3.276, mask_ins=1.025, word_ins_ml=4.772, word_reposition=0.349, kpe=0.748, ppl=118.92, wps=3905.4, ups=0.27, wpb=14715.5, bsz=1024, num_updates=28600, lr=0.000209061, gnorm=1.458, clip=0, loss_scale=1024, train_wall=345, wall=82553
2022-08-07 03:35:49 | INFO | train_inner | epoch 008:   2734 / 3715 loss=6.911, nll_loss=3.284, mask_ins=1.041, word_ins_ml=4.779, word_reposition=0.342, kpe=0.749, ppl=120.37, wps=5274.2, ups=0.36, wpb=14694.5, bsz=1024, num_updates=28700, lr=0.000208696, gnorm=1.431, clip=0, loss_scale=1024, train_wall=247, wall=82831
2022-08-07 03:40:29 | INFO | train_inner | epoch 008:   2834 / 3715 loss=6.887, nll_loss=3.269, mask_ins=1.027, word_ins_ml=4.766, word_reposition=0.344, kpe=0.75, ppl=118.39, wps=5290.9, ups=0.36, wpb=14796.7, bsz=1024, num_updates=28800, lr=0.000208333, gnorm=1.434, clip=0, loss_scale=1024, train_wall=248, wall=83111
2022-08-07 03:45:09 | INFO | train_inner | epoch 008:   2934 / 3715 loss=6.918, nll_loss=3.291, mask_ins=1.036, word_ins_ml=4.786, word_reposition=0.347, kpe=0.749, ppl=120.94, wps=5213, ups=0.36, wpb=14624.8, bsz=1024, num_updates=28900, lr=0.000207973, gnorm=1.445, clip=0, loss_scale=1024, train_wall=249, wall=83391
2022-08-07 03:49:48 | INFO | train_inner | epoch 008:   3034 / 3715 loss=6.896, nll_loss=3.271, mask_ins=1.035, word_ins_ml=4.767, word_reposition=0.345, kpe=0.748, ppl=119.06, wps=5303.7, ups=0.36, wpb=14798.3, bsz=1024, num_updates=29000, lr=0.000207614, gnorm=1.444, clip=0, loss_scale=1556, train_wall=248, wall=83671
2022-08-07 03:53:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-07 03:54:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-07 03:54:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-07 03:54:36 | INFO | train_inner | epoch 008:   3137 / 3715 loss=6.921, nll_loss=3.297, mask_ins=1.036, word_ins_ml=4.79, word_reposition=0.346, kpe=0.748, ppl=121.14, wps=5084.3, ups=0.35, wpb=14615.3, bsz=1024, num_updates=29100, lr=0.000207257, gnorm=1.6, clip=0, loss_scale=1819, train_wall=255, wall=83958
2022-08-07 03:59:15 | INFO | train_inner | epoch 008:   3237 / 3715 loss=6.866, nll_loss=3.252, mask_ins=1.03, word_ins_ml=4.751, word_reposition=0.337, kpe=0.748, ppl=116.64, wps=5236.3, ups=0.36, wpb=14649.8, bsz=1024, num_updates=29200, lr=0.000206901, gnorm=1.452, clip=0, loss_scale=256, train_wall=248, wall=84238
2022-08-07 04:03:53 | INFO | train_inner | epoch 008:   3337 / 3715 loss=6.902, nll_loss=3.29, mask_ins=1.03, word_ins_ml=4.784, word_reposition=0.342, kpe=0.746, ppl=119.55, wps=5260.3, ups=0.36, wpb=14596.4, bsz=1024, num_updates=29300, lr=0.000206548, gnorm=1.423, clip=0, loss_scale=256, train_wall=247, wall=84515
2022-08-07 04:08:33 | INFO | train_inner | epoch 008:   3437 / 3715 loss=6.897, nll_loss=3.284, mask_ins=1.03, word_ins_ml=4.779, word_reposition=0.34, kpe=0.747, ppl=119.2, wps=5237.6, ups=0.36, wpb=14653.8, bsz=1024, num_updates=29400, lr=0.000206197, gnorm=1.451, clip=0, loss_scale=256, train_wall=248, wall=84795
2022-08-07 04:13:12 | INFO | train_inner | epoch 008:   3537 / 3715 loss=6.9, nll_loss=3.293, mask_ins=1.03, word_ins_ml=4.787, word_reposition=0.338, kpe=0.745, ppl=119.41, wps=5262.9, ups=0.36, wpb=14701.5, bsz=1024, num_updates=29500, lr=0.000205847, gnorm=1.455, clip=0, loss_scale=256, train_wall=248, wall=85074
2022-08-07 04:17:51 | INFO | train_inner | epoch 008:   3637 / 3715 loss=6.898, nll_loss=3.282, mask_ins=1.03, word_ins_ml=4.777, word_reposition=0.346, kpe=0.744, ppl=119.24, wps=5239.6, ups=0.36, wpb=14616.5, bsz=1023.8, num_updates=29600, lr=0.000205499, gnorm=1.434, clip=0, loss_scale=256, train_wall=248, wall=85353
2022-08-07 04:21:27 | INFO | train | epoch 008 | loss 6.895 | nll_loss 3.279 | mask_ins 1.032 | word_ins_ml 4.775 | word_reposition 0.343 | kpe 0.744 | ppl 118.98 | wps 5032 | ups 0.34 | wpb 14660.7 | bsz 1023.7 | num_updates 29678 | lr 0.000205229 | gnorm 1.453 | clip 0 | loss_scale 1935 | train_wall 9406 | wall 85569
2022-08-07 04:25:07 | INFO | valid | epoch 008 | valid on 'valid' subset | loss nan | nll_loss 3.252 | mask_ins 1.023 | word_ins_ml 4.808 | word_reposition 0.36 | kpe nan | ppl nan | wps 12407.8 | wpb 1849.4 | bsz 127.9 | num_updates 29678 | best_loss nan
2022-08-07 04:25:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 8 @ 29678 updates, score nan) (writing took 9.713655324652791 seconds)
2022-08-07 04:26:18 | INFO | train_inner | epoch 009:     22 / 3715 loss=6.868, nll_loss=3.258, mask_ins=1.031, word_ins_ml=4.756, word_reposition=0.337, kpe=0.743, ppl=116.81, wps=2881.8, ups=0.2, wpb=14618.7, bsz=1014.7, num_updates=29700, lr=0.000205152, gnorm=1.451, clip=0, loss_scale=507, train_wall=245, wall=85861
2022-08-07 04:30:57 | INFO | train_inner | epoch 009:    122 / 3715 loss=6.787, nll_loss=3.207, mask_ins=1.02, word_ins_ml=4.712, word_reposition=0.336, kpe=0.72, ppl=110.46, wps=5237.3, ups=0.36, wpb=14580.5, bsz=1024, num_updates=29800, lr=0.000204808, gnorm=1.451, clip=0, loss_scale=512, train_wall=247, wall=86139
2022-08-07 04:35:36 | INFO | train_inner | epoch 009:    222 / 3715 loss=6.78, nll_loss=3.193, mask_ins=1.022, word_ins_ml=4.698, word_reposition=0.342, kpe=0.718, ppl=109.9, wps=5200.9, ups=0.36, wpb=14550.1, bsz=1023.8, num_updates=29900, lr=0.000204465, gnorm=1.479, clip=0, loss_scale=512, train_wall=248, wall=86419
2022-08-07 04:40:16 | INFO | train_inner | epoch 009:    322 / 3715 loss=6.791, nll_loss=3.218, mask_ins=1.021, word_ins_ml=4.721, word_reposition=0.327, kpe=0.722, ppl=110.7, wps=5236.8, ups=0.36, wpb=14647.5, bsz=1024, num_updates=30000, lr=0.000204124, gnorm=1.476, clip=0, loss_scale=512, train_wall=248, wall=86698
2022-08-07 04:44:56 | INFO | train_inner | epoch 009:    422 / 3715 loss=6.816, nll_loss=3.219, mask_ins=1.024, word_ins_ml=4.722, word_reposition=0.347, kpe=0.723, ppl=112.64, wps=5261.2, ups=0.36, wpb=14713.4, bsz=1024, num_updates=30100, lr=0.000203785, gnorm=1.501, clip=0, loss_scale=512, train_wall=248, wall=86978
2022-08-07 04:49:35 | INFO | train_inner | epoch 009:    522 / 3715 loss=6.788, nll_loss=3.212, mask_ins=1.02, word_ins_ml=4.715, word_reposition=0.329, kpe=0.723, ppl=110.49, wps=5291.1, ups=0.36, wpb=14800.3, bsz=1024, num_updates=30200, lr=0.000203447, gnorm=1.476, clip=0, loss_scale=952, train_wall=248, wall=87258
2022-08-07 04:54:15 | INFO | train_inner | epoch 009:    622 / 3715 loss=6.806, nll_loss=3.218, mask_ins=1.025, word_ins_ml=4.72, word_reposition=0.336, kpe=0.724, ppl=111.92, wps=5283.2, ups=0.36, wpb=14782.1, bsz=1024, num_updates=30300, lr=0.000203111, gnorm=1.476, clip=0, loss_scale=1024, train_wall=248, wall=87538
2022-08-07 04:58:55 | INFO | train_inner | epoch 009:    722 / 3715 loss=6.793, nll_loss=3.212, mask_ins=1.028, word_ins_ml=4.716, word_reposition=0.33, kpe=0.719, ppl=110.92, wps=5181.3, ups=0.36, wpb=14483.7, bsz=1024, num_updates=30400, lr=0.000202777, gnorm=1.497, clip=0, loss_scale=1024, train_wall=248, wall=87817
2022-08-07 05:03:34 | INFO | train_inner | epoch 009:    822 / 3715 loss=6.801, nll_loss=3.22, mask_ins=1.022, word_ins_ml=4.722, word_reposition=0.338, kpe=0.72, ppl=111.53, wps=5242, ups=0.36, wpb=14621.6, bsz=1024, num_updates=30500, lr=0.000202444, gnorm=1.472, clip=0, loss_scale=1024, train_wall=248, wall=88096
2022-08-07 05:08:13 | INFO | train_inner | epoch 009:    922 / 3715 loss=6.788, nll_loss=3.212, mask_ins=1.024, word_ins_ml=4.715, word_reposition=0.328, kpe=0.721, ppl=110.48, wps=5250.3, ups=0.36, wpb=14640.7, bsz=1024, num_updates=30600, lr=0.000202113, gnorm=1.468, clip=0, loss_scale=1024, train_wall=248, wall=88375
2022-08-07 05:12:53 | INFO | train_inner | epoch 009:   1022 / 3715 loss=6.816, nll_loss=3.224, mask_ins=1.027, word_ins_ml=4.726, word_reposition=0.336, kpe=0.727, ppl=112.67, wps=5274.3, ups=0.36, wpb=14765, bsz=1024, num_updates=30700, lr=0.000201784, gnorm=1.469, clip=0, loss_scale=1782, train_wall=249, wall=88655
2022-08-07 05:17:32 | INFO | train_inner | epoch 009:   1122 / 3715 loss=6.801, nll_loss=3.215, mask_ins=1.022, word_ins_ml=4.718, word_reposition=0.336, kpe=0.725, ppl=111.53, wps=5282.3, ups=0.36, wpb=14748.6, bsz=1024, num_updates=30800, lr=0.000201456, gnorm=1.464, clip=0, loss_scale=2048, train_wall=248, wall=88934
2022-08-07 05:22:11 | INFO | train_inner | epoch 009:   1222 / 3715 loss=6.798, nll_loss=3.21, mask_ins=1.021, word_ins_ml=4.713, word_reposition=0.342, kpe=0.722, ppl=111.25, wps=5224.7, ups=0.36, wpb=14614.2, bsz=1024, num_updates=30900, lr=0.000201129, gnorm=1.464, clip=0, loss_scale=2048, train_wall=248, wall=89214
2022-08-07 05:26:50 | INFO | train_inner | epoch 009:   1322 / 3715 loss=6.813, nll_loss=3.218, mask_ins=1.024, word_ins_ml=4.721, word_reposition=0.339, kpe=0.73, ppl=112.46, wps=5272.8, ups=0.36, wpb=14709.1, bsz=1024, num_updates=31000, lr=0.000200805, gnorm=1.478, clip=0, loss_scale=2048, train_wall=248, wall=89493
2022-08-07 05:31:31 | INFO | train_inner | epoch 009:   1422 / 3715 loss=6.801, nll_loss=3.211, mask_ins=1.022, word_ins_ml=4.715, word_reposition=0.337, kpe=0.727, ppl=111.48, wps=5201.4, ups=0.36, wpb=14587.9, bsz=1024, num_updates=31100, lr=0.000200482, gnorm=1.482, clip=0, loss_scale=2048, train_wall=249, wall=89773
2022-08-07 05:36:10 | INFO | train_inner | epoch 009:   1522 / 3715 loss=6.828, nll_loss=3.236, mask_ins=1.024, word_ins_ml=4.736, word_reposition=0.341, kpe=0.728, ppl=113.61, wps=5258, ups=0.36, wpb=14655.3, bsz=1024, num_updates=31200, lr=0.00020016, gnorm=1.461, clip=0, loss_scale=3318, train_wall=247, wall=90052
2022-08-07 05:40:48 | INFO | train_inner | epoch 009:   1622 / 3715 loss=6.819, nll_loss=3.233, mask_ins=1.024, word_ins_ml=4.734, word_reposition=0.338, kpe=0.724, ppl=112.93, wps=5233.6, ups=0.36, wpb=14566.1, bsz=1024, num_updates=31300, lr=0.00019984, gnorm=1.446, clip=0, loss_scale=4096, train_wall=247, wall=90330
2022-08-07 05:45:28 | INFO | train_inner | epoch 009:   1722 / 3715 loss=6.838, nll_loss=3.245, mask_ins=1.022, word_ins_ml=4.744, word_reposition=0.341, kpe=0.731, ppl=114.38, wps=5268.7, ups=0.36, wpb=14762.7, bsz=1024, num_updates=31400, lr=0.000199522, gnorm=1.466, clip=0, loss_scale=4096, train_wall=249, wall=90611
2022-08-07 05:50:08 | INFO | train_inner | epoch 009:   1822 / 3715 loss=6.801, nll_loss=3.218, mask_ins=1.016, word_ins_ml=4.72, word_reposition=0.335, kpe=0.73, ppl=111.5, wps=5258.9, ups=0.36, wpb=14718.2, bsz=1024, num_updates=31500, lr=0.000199205, gnorm=1.465, clip=0, loss_scale=4096, train_wall=249, wall=90890
2022-08-07 05:54:48 | INFO | train_inner | epoch 009:   1922 / 3715 loss=6.792, nll_loss=3.217, mask_ins=1.017, word_ins_ml=4.719, word_reposition=0.331, kpe=0.725, ppl=110.83, wps=5239, ups=0.36, wpb=14645.8, bsz=1024, num_updates=31600, lr=0.000198889, gnorm=1.483, clip=0, loss_scale=4096, train_wall=248, wall=91170
2022-08-07 05:59:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-07 05:59:29 | INFO | train_inner | epoch 009:   2023 / 3715 loss=6.823, nll_loss=3.235, mask_ins=1.022, word_ins_ml=4.735, word_reposition=0.336, kpe=0.73, ppl=113.22, wps=5194.3, ups=0.36, wpb=14616.1, bsz=1024, num_updates=31700, lr=0.000198575, gnorm=1.467, clip=0, loss_scale=5921, train_wall=250, wall=91451
2022-08-07 06:04:08 | INFO | train_inner | epoch 009:   2123 / 3715 loss=6.817, nll_loss=3.225, mask_ins=1.024, word_ins_ml=4.726, word_reposition=0.336, kpe=0.731, ppl=112.74, wps=5288.4, ups=0.36, wpb=14758.3, bsz=1024, num_updates=31800, lr=0.000198263, gnorm=1.459, clip=0, loss_scale=4096, train_wall=248, wall=91730
2022-08-07 06:08:47 | INFO | train_inner | epoch 009:   2223 / 3715 loss=6.836, nll_loss=3.238, mask_ins=1.023, word_ins_ml=4.738, word_reposition=0.34, kpe=0.735, ppl=114.23, wps=5276.9, ups=0.36, wpb=14730.2, bsz=1024, num_updates=31900, lr=0.000197952, gnorm=1.464, clip=0, loss_scale=4096, train_wall=248, wall=92010
2022-08-07 06:13:27 | INFO | train_inner | epoch 009:   2323 / 3715 loss=6.813, nll_loss=3.222, mask_ins=1.024, word_ins_ml=4.724, word_reposition=0.338, kpe=0.729, ppl=112.47, wps=5240.1, ups=0.36, wpb=14647, bsz=1024, num_updates=32000, lr=0.000197642, gnorm=1.47, clip=0, loss_scale=4096, train_wall=248, wall=92289
2022-08-07 06:18:06 | INFO | train_inner | epoch 009:   2423 / 3715 loss=6.815, nll_loss=3.224, mask_ins=1.022, word_ins_ml=4.725, word_reposition=0.339, kpe=0.728, ppl=112.58, wps=5229.8, ups=0.36, wpb=14630.5, bsz=1024, num_updates=32100, lr=0.000197334, gnorm=1.472, clip=0, loss_scale=4096, train_wall=248, wall=92569
2022-08-07 06:22:46 | INFO | train_inner | epoch 009:   2523 / 3715 loss=6.806, nll_loss=3.219, mask_ins=1.018, word_ins_ml=4.721, word_reposition=0.336, kpe=0.731, ppl=111.88, wps=5248.6, ups=0.36, wpb=14692.9, bsz=1024, num_updates=32200, lr=0.000197028, gnorm=1.472, clip=0, loss_scale=4096, train_wall=248, wall=92849
2022-08-07 06:25:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-07 06:27:29 | INFO | train_inner | epoch 009:   2624 / 3715 loss=6.833, nll_loss=3.232, mask_ins=1.026, word_ins_ml=4.732, word_reposition=0.345, kpe=0.73, ppl=114.03, wps=5181.7, ups=0.35, wpb=14651.7, bsz=1024, num_updates=32300, lr=0.000196722, gnorm=1.492, clip=0, loss_scale=6205, train_wall=251, wall=93132
2022-08-07 06:28:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 06:32:13 | INFO | train_inner | epoch 009:   2725 / 3715 loss=6.818, nll_loss=3.236, mask_ins=1.013, word_ins_ml=4.736, word_reposition=0.336, kpe=0.732, ppl=112.81, wps=5189.6, ups=0.35, wpb=14718.3, bsz=1024, num_updates=32400, lr=0.000196419, gnorm=1.461, clip=0, loss_scale=2312, train_wall=252, wall=93415
2022-08-07 06:36:54 | INFO | train_inner | epoch 009:   2825 / 3715 loss=6.81, nll_loss=3.224, mask_ins=1.021, word_ins_ml=4.725, word_reposition=0.332, kpe=0.732, ppl=112.17, wps=5222.8, ups=0.36, wpb=14704.7, bsz=1024, num_updates=32500, lr=0.000196116, gnorm=1.466, clip=0, loss_scale=2048, train_wall=251, wall=93697
2022-08-07 06:41:36 | INFO | train_inner | epoch 009:   2925 / 3715 loss=6.801, nll_loss=3.215, mask_ins=1.017, word_ins_ml=4.717, word_reposition=0.34, kpe=0.727, ppl=111.51, wps=5208.2, ups=0.35, wpb=14688.5, bsz=1024, num_updates=32600, lr=0.000195815, gnorm=1.502, clip=0, loss_scale=2048, train_wall=251, wall=93979
2022-08-07 06:46:16 | INFO | train_inner | epoch 009:   3025 / 3715 loss=6.815, nll_loss=3.221, mask_ins=1.021, word_ins_ml=4.724, word_reposition=0.336, kpe=0.734, ppl=112.58, wps=5251.2, ups=0.36, wpb=14667.6, bsz=1024, num_updates=32700, lr=0.000195515, gnorm=1.483, clip=0, loss_scale=2048, train_wall=248, wall=94258
2022-08-07 06:50:55 | INFO | train_inner | epoch 009:   3125 / 3715 loss=6.812, nll_loss=3.219, mask_ins=1.023, word_ins_ml=4.721, word_reposition=0.338, kpe=0.731, ppl=112.39, wps=5232.1, ups=0.36, wpb=14632.3, bsz=1024, num_updates=32800, lr=0.000195217, gnorm=1.493, clip=0, loss_scale=2048, train_wall=248, wall=94538
2022-08-07 06:55:35 | INFO | train_inner | epoch 009:   3225 / 3715 loss=6.824, nll_loss=3.235, mask_ins=1.021, word_ins_ml=4.735, word_reposition=0.338, kpe=0.73, ppl=113.28, wps=5236.3, ups=0.36, wpb=14632.8, bsz=1024, num_updates=32900, lr=0.00019492, gnorm=1.489, clip=0, loss_scale=3604, train_wall=248, wall=94817
2022-08-07 06:55:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 07:00:18 | INFO | train_inner | epoch 009:   3326 / 3715 loss=6.808, nll_loss=3.221, mask_ins=1.016, word_ins_ml=4.722, word_reposition=0.337, kpe=0.734, ppl=112.08, wps=5197.8, ups=0.35, wpb=14733.7, bsz=1024, num_updates=33000, lr=0.000194625, gnorm=1.46, clip=0, loss_scale=2068, train_wall=251, wall=95101
2022-08-07 07:04:57 | INFO | train_inner | epoch 009:   3426 / 3715 loss=6.808, nll_loss=3.224, mask_ins=1.02, word_ins_ml=4.725, word_reposition=0.335, kpe=0.728, ppl=112.01, wps=5222.3, ups=0.36, wpb=14578.5, bsz=1024, num_updates=33100, lr=0.000194331, gnorm=1.456, clip=0, loss_scale=2048, train_wall=248, wall=95380
2022-08-07 07:09:38 | INFO | train_inner | epoch 009:   3526 / 3715 loss=6.796, nll_loss=3.212, mask_ins=1.016, word_ins_ml=4.715, word_reposition=0.333, kpe=0.732, ppl=111.11, wps=5246.9, ups=0.36, wpb=14734, bsz=1024, num_updates=33200, lr=0.000194038, gnorm=1.458, clip=0, loss_scale=2048, train_wall=249, wall=95661
2022-08-07 07:14:19 | INFO | train_inner | epoch 009:   3626 / 3715 loss=6.816, nll_loss=3.229, mask_ins=1.023, word_ins_ml=4.729, word_reposition=0.334, kpe=0.73, ppl=112.64, wps=5198.5, ups=0.36, wpb=14619.3, bsz=1024, num_updates=33300, lr=0.000193746, gnorm=1.452, clip=0, loss_scale=2048, train_wall=250, wall=95942
2022-08-07 07:18:26 | INFO | train | epoch 009 | loss 6.808 | nll_loss 3.221 | mask_ins 1.021 | word_ins_ml 4.723 | word_reposition 0.336 | kpe 0.727 | ppl 112.03 | wps 5123.8 | ups 0.35 | wpb 14662.2 | bsz 1023.7 | num_updates 33389 | lr 0.000193488 | gnorm 1.473 | clip 0 | loss_scale 2525 | train_wall 9221 | wall 96188
2022-08-07 07:22:08 | INFO | valid | epoch 009 | valid on 'valid' subset | loss nan | nll_loss 3.236 | mask_ins 1.019 | word_ins_ml 4.794 | word_reposition 0.339 | kpe nan | ppl nan | wps 12352.1 | wpb 1849.4 | bsz 127.9 | num_updates 33389 | best_loss nan
2022-08-07 07:22:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 9 @ 33389 updates, score nan) (writing took 24.55796436406672 seconds)
2022-08-07 07:23:03 | INFO | train_inner | epoch 010:     11 / 3715 loss=6.773, nll_loss=3.193, mask_ins=1.013, word_ins_ml=4.697, word_reposition=0.336, kpe=0.726, ppl=109.4, wps=2763.9, ups=0.19, wpb=14462.3, bsz=1014.7, num_updates=33400, lr=0.000193456, gnorm=1.491, clip=0, loss_scale=2048, train_wall=246, wall=96465
2022-08-07 07:26:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 07:27:46 | INFO | train_inner | epoch 010:    112 / 3715 loss=6.719, nll_loss=3.156, mask_ins=1.009, word_ins_ml=4.665, word_reposition=0.34, kpe=0.705, ppl=105.34, wps=5165.2, ups=0.35, wpb=14633.4, bsz=1024, num_updates=33500, lr=0.000193167, gnorm=1.542, clip=0, loss_scale=3488, train_wall=251, wall=96748
2022-08-07 07:32:27 | INFO | train_inner | epoch 010:    212 / 3715 loss=6.72, nll_loss=3.163, mask_ins=1.015, word_ins_ml=4.671, word_reposition=0.332, kpe=0.702, ppl=105.42, wps=5209.2, ups=0.36, wpb=14614.4, bsz=1024, num_updates=33600, lr=0.000192879, gnorm=1.482, clip=0, loss_scale=2048, train_wall=249, wall=97029
2022-08-07 07:37:06 | INFO | train_inner | epoch 010:    312 / 3715 loss=6.727, nll_loss=3.17, mask_ins=1.01, word_ins_ml=4.677, word_reposition=0.335, kpe=0.704, ppl=105.91, wps=5258.9, ups=0.36, wpb=14702.7, bsz=1023.8, num_updates=33700, lr=0.000192593, gnorm=1.505, clip=0, loss_scale=2048, train_wall=248, wall=97309
2022-08-07 07:41:46 | INFO | train_inner | epoch 010:    412 / 3715 loss=6.706, nll_loss=3.157, mask_ins=1.009, word_ins_ml=4.666, word_reposition=0.331, kpe=0.701, ppl=104.41, wps=5205.5, ups=0.36, wpb=14559, bsz=1024, num_updates=33800, lr=0.000192308, gnorm=1.489, clip=0, loss_scale=2048, train_wall=248, wall=97588
2022-08-07 07:46:26 | INFO | train_inner | epoch 010:    512 / 3715 loss=6.72, nll_loss=3.165, mask_ins=1.012, word_ins_ml=4.673, word_reposition=0.335, kpe=0.7, ppl=105.4, wps=5189.1, ups=0.36, wpb=14554.9, bsz=1024, num_updates=33900, lr=0.000192024, gnorm=1.496, clip=0, loss_scale=2048, train_wall=249, wall=97869
2022-08-07 07:51:07 | INFO | train_inner | epoch 010:    612 / 3715 loss=6.713, nll_loss=3.147, mask_ins=1.008, word_ins_ml=4.657, word_reposition=0.34, kpe=0.708, ppl=104.92, wps=5271.4, ups=0.36, wpb=14781.9, bsz=1024, num_updates=34000, lr=0.000191741, gnorm=1.493, clip=0, loss_scale=2171, train_wall=249, wall=98149
2022-08-07 07:55:47 | INFO | train_inner | epoch 010:    712 / 3715 loss=6.723, nll_loss=3.164, mask_ins=1.01, word_ins_ml=4.672, word_reposition=0.332, kpe=0.709, ppl=105.66, wps=5276.6, ups=0.36, wpb=14795.2, bsz=1024, num_updates=34100, lr=0.00019146, gnorm=1.492, clip=0, loss_scale=4096, train_wall=249, wall=98430
2022-08-07 08:00:28 | INFO | train_inner | epoch 010:    812 / 3715 loss=6.75, nll_loss=3.18, mask_ins=1.016, word_ins_ml=4.687, word_reposition=0.336, kpe=0.711, ppl=107.6, wps=5244.4, ups=0.36, wpb=14704, bsz=1024, num_updates=34200, lr=0.00019118, gnorm=1.507, clip=0, loss_scale=4096, train_wall=249, wall=98710
2022-08-07 08:01:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 08:05:10 | INFO | train_inner | epoch 010:    913 / 3715 loss=6.739, nll_loss=3.172, mask_ins=1.013, word_ins_ml=4.679, word_reposition=0.335, kpe=0.712, ppl=106.78, wps=5237.2, ups=0.35, wpb=14769.4, bsz=1024, num_updates=34300, lr=0.000190901, gnorm=1.476, clip=0, loss_scale=2312, train_wall=250, wall=98992
2022-08-07 08:09:51 | INFO | train_inner | epoch 010:   1013 / 3715 loss=6.729, nll_loss=3.167, mask_ins=1.012, word_ins_ml=4.675, word_reposition=0.333, kpe=0.71, ppl=106.1, wps=5208, ups=0.36, wpb=14638.4, bsz=1024, num_updates=34400, lr=0.000190623, gnorm=1.529, clip=0, loss_scale=2048, train_wall=250, wall=99273
2022-08-07 08:14:31 | INFO | train_inner | epoch 010:   1113 / 3715 loss=6.716, nll_loss=3.155, mask_ins=1.01, word_ins_ml=4.663, word_reposition=0.334, kpe=0.709, ppl=105.16, wps=5270.3, ups=0.36, wpb=14754.1, bsz=1024, num_updates=34500, lr=0.000190347, gnorm=1.481, clip=0, loss_scale=2048, train_wall=249, wall=99553
2022-08-07 08:19:12 | INFO | train_inner | epoch 010:   1213 / 3715 loss=6.719, nll_loss=3.158, mask_ins=1.009, word_ins_ml=4.667, word_reposition=0.337, kpe=0.707, ppl=105.38, wps=5191.2, ups=0.36, wpb=14589.9, bsz=1024, num_updates=34600, lr=0.000190071, gnorm=1.534, clip=0, loss_scale=2048, train_wall=249, wall=99834
2022-08-07 08:23:51 | INFO | train_inner | epoch 010:   1313 / 3715 loss=6.723, nll_loss=3.159, mask_ins=1.008, word_ins_ml=4.668, word_reposition=0.338, kpe=0.71, ppl=105.65, wps=5249.5, ups=0.36, wpb=14655.2, bsz=1024, num_updates=34700, lr=0.000189797, gnorm=1.495, clip=0, loss_scale=2048, train_wall=248, wall=100113
2022-08-07 08:28:29 | INFO | train_inner | epoch 010:   1413 / 3715 loss=6.742, nll_loss=3.181, mask_ins=1.009, word_ins_ml=4.687, word_reposition=0.337, kpe=0.71, ppl=107.07, wps=5244.1, ups=0.36, wpb=14587, bsz=1024, num_updates=34800, lr=0.000189525, gnorm=1.497, clip=0, loss_scale=3604, train_wall=247, wall=100391
2022-08-07 08:29:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 08:33:11 | INFO | train_inner | epoch 010:   1514 / 3715 loss=6.741, nll_loss=3.184, mask_ins=1.011, word_ins_ml=4.69, word_reposition=0.331, kpe=0.708, ppl=106.97, wps=5174.5, ups=0.35, wpb=14589.4, bsz=1024, num_updates=34900, lr=0.000189253, gnorm=1.5, clip=0, loss_scale=2555, train_wall=250, wall=100673
2022-08-07 08:37:51 | INFO | train_inner | epoch 010:   1614 / 3715 loss=6.738, nll_loss=3.173, mask_ins=1.005, word_ins_ml=4.68, word_reposition=0.341, kpe=0.713, ppl=106.76, wps=5255.2, ups=0.36, wpb=14693.3, bsz=1024, num_updates=35000, lr=0.000188982, gnorm=1.504, clip=0, loss_scale=2048, train_wall=248, wall=100953
2022-08-07 08:42:31 | INFO | train_inner | epoch 010:   1714 / 3715 loss=6.725, nll_loss=3.158, mask_ins=1.007, word_ins_ml=4.667, word_reposition=0.337, kpe=0.714, ppl=105.76, wps=5236.7, ups=0.36, wpb=14686.6, bsz=1024, num_updates=35100, lr=0.000188713, gnorm=1.511, clip=0, loss_scale=2048, train_wall=249, wall=101233
2022-08-07 08:47:12 | INFO | train_inner | epoch 010:   1814 / 3715 loss=6.747, nll_loss=3.181, mask_ins=1.013, word_ins_ml=4.686, word_reposition=0.338, kpe=0.71, ppl=107.42, wps=5211.6, ups=0.36, wpb=14637, bsz=1024, num_updates=35200, lr=0.000188445, gnorm=1.519, clip=0, loss_scale=2048, train_wall=250, wall=101514
2022-08-07 08:51:52 | INFO | train_inner | epoch 010:   1914 / 3715 loss=6.744, nll_loss=3.178, mask_ins=1.013, word_ins_ml=4.684, word_reposition=0.335, kpe=0.712, ppl=107.19, wps=5226.6, ups=0.36, wpb=14663.4, bsz=1024, num_updates=35300, lr=0.000188177, gnorm=1.502, clip=0, loss_scale=2048, train_wall=249, wall=101795
2022-08-07 08:56:33 | INFO | train_inner | epoch 010:   2014 / 3715 loss=6.723, nll_loss=3.159, mask_ins=1.005, word_ins_ml=4.668, word_reposition=0.338, kpe=0.712, ppl=105.63, wps=5253, ups=0.36, wpb=14731.2, bsz=1024, num_updates=35400, lr=0.000187912, gnorm=1.495, clip=0, loss_scale=3359, train_wall=249, wall=102075
2022-08-07 09:01:12 | INFO | train_inner | epoch 010:   2114 / 3715 loss=6.749, nll_loss=3.174, mask_ins=1.013, word_ins_ml=4.68, word_reposition=0.338, kpe=0.718, ppl=107.56, wps=5301.2, ups=0.36, wpb=14795.5, bsz=1024, num_updates=35500, lr=0.000187647, gnorm=1.485, clip=0, loss_scale=4096, train_wall=248, wall=102354
2022-08-07 09:05:51 | INFO | train_inner | epoch 010:   2214 / 3715 loss=6.711, nll_loss=3.155, mask_ins=1.009, word_ins_ml=4.664, word_reposition=0.329, kpe=0.709, ppl=104.77, wps=5209, ups=0.36, wpb=14534.9, bsz=1024, num_updates=35600, lr=0.000187383, gnorm=1.49, clip=0, loss_scale=4096, train_wall=248, wall=102633
2022-08-07 09:10:30 | INFO | train_inner | epoch 010:   2314 / 3715 loss=6.731, nll_loss=3.173, mask_ins=1.009, word_ins_ml=4.679, word_reposition=0.331, kpe=0.712, ppl=106.26, wps=5234.9, ups=0.36, wpb=14594.1, bsz=1024, num_updates=35700, lr=0.00018712, gnorm=1.499, clip=0, loss_scale=4096, train_wall=247, wall=102912
2022-08-07 09:15:10 | INFO | train_inner | epoch 010:   2414 / 3715 loss=6.709, nll_loss=3.152, mask_ins=1.011, word_ins_ml=4.662, word_reposition=0.331, kpe=0.706, ppl=104.62, wps=5168.1, ups=0.36, wpb=14481.4, bsz=1024, num_updates=35800, lr=0.000186859, gnorm=1.502, clip=0, loss_scale=4096, train_wall=249, wall=103192
2022-08-07 09:19:49 | INFO | train_inner | epoch 010:   2514 / 3715 loss=6.727, nll_loss=3.171, mask_ins=1.008, word_ins_ml=4.678, word_reposition=0.33, kpe=0.712, ppl=105.95, wps=5260.8, ups=0.36, wpb=14683.4, bsz=1024, num_updates=35900, lr=0.000186598, gnorm=1.49, clip=0, loss_scale=6226, train_wall=248, wall=103472
2022-08-07 09:20:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-07 09:24:31 | INFO | train_inner | epoch 010:   2615 / 3715 loss=6.74, nll_loss=3.172, mask_ins=1.008, word_ins_ml=4.678, word_reposition=0.345, kpe=0.709, ppl=106.86, wps=5180.6, ups=0.35, wpb=14614.9, bsz=1024, num_updates=36000, lr=0.000186339, gnorm=1.495, clip=0, loss_scale=4461, train_wall=250, wall=103754
2022-08-07 09:29:10 | INFO | train_inner | epoch 010:   2715 / 3715 loss=6.745, nll_loss=3.177, mask_ins=1.008, word_ins_ml=4.683, word_reposition=0.336, kpe=0.719, ppl=107.29, wps=5303.5, ups=0.36, wpb=14797.8, bsz=1024, num_updates=36100, lr=0.000186081, gnorm=1.487, clip=0, loss_scale=4096, train_wall=248, wall=104033
2022-08-07 09:33:50 | INFO | train_inner | epoch 010:   2815 / 3715 loss=6.745, nll_loss=3.171, mask_ins=1.007, word_ins_ml=4.677, word_reposition=0.344, kpe=0.717, ppl=107.27, wps=5276.9, ups=0.36, wpb=14776.3, bsz=1024, num_updates=36200, lr=0.000185824, gnorm=1.502, clip=0, loss_scale=4096, train_wall=249, wall=104313
2022-08-07 09:35:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 09:38:32 | INFO | train_inner | epoch 010:   2916 / 3715 loss=6.756, nll_loss=3.195, mask_ins=1.012, word_ins_ml=4.699, word_reposition=0.33, kpe=0.715, ppl=108.1, wps=5244.4, ups=0.35, wpb=14794.4, bsz=1024, num_updates=36300, lr=0.000185567, gnorm=1.484, clip=0, loss_scale=2900, train_wall=251, wall=104595
2022-08-07 09:43:12 | INFO | train_inner | epoch 010:   3016 / 3715 loss=6.721, nll_loss=3.162, mask_ins=1.008, word_ins_ml=4.669, word_reposition=0.327, kpe=0.717, ppl=105.53, wps=5265.4, ups=0.36, wpb=14740.8, bsz=1024, num_updates=36400, lr=0.000185312, gnorm=1.479, clip=0, loss_scale=2048, train_wall=249, wall=104875
2022-08-07 09:47:51 | INFO | train_inner | epoch 010:   3116 / 3715 loss=6.751, nll_loss=3.173, mask_ins=1.013, word_ins_ml=4.68, word_reposition=0.342, kpe=0.716, ppl=107.69, wps=5291.3, ups=0.36, wpb=14762.4, bsz=1024, num_updates=36500, lr=0.000185058, gnorm=1.486, clip=0, loss_scale=2048, train_wall=248, wall=105154
2022-08-07 09:52:30 | INFO | train_inner | epoch 010:   3216 / 3715 loss=6.76, nll_loss=3.197, mask_ins=1.013, word_ins_ml=4.701, word_reposition=0.332, kpe=0.715, ppl=108.41, wps=5242.8, ups=0.36, wpb=14620.8, bsz=1024, num_updates=36600, lr=0.000184805, gnorm=1.501, clip=0, loss_scale=2048, train_wall=248, wall=105433
2022-08-07 09:57:10 | INFO | train_inner | epoch 010:   3316 / 3715 loss=6.738, nll_loss=3.175, mask_ins=1.007, word_ins_ml=4.681, word_reposition=0.334, kpe=0.716, ppl=106.73, wps=5244.2, ups=0.36, wpb=14690.8, bsz=1024, num_updates=36700, lr=0.000184553, gnorm=1.518, clip=0, loss_scale=2048, train_wall=249, wall=105713
2022-08-07 10:00:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 10:01:53 | INFO | train_inner | epoch 010:   3417 / 3715 loss=6.72, nll_loss=3.159, mask_ins=1.004, word_ins_ml=4.667, word_reposition=0.33, kpe=0.718, ppl=105.44, wps=5220.8, ups=0.35, wpb=14757.8, bsz=1024, num_updates=36800, lr=0.000184302, gnorm=1.483, clip=0, loss_scale=2312, train_wall=251, wall=105995
2022-08-07 10:06:33 | INFO | train_inner | epoch 010:   3517 / 3715 loss=6.73, nll_loss=3.175, mask_ins=1.007, word_ins_ml=4.681, word_reposition=0.329, kpe=0.713, ppl=106.12, wps=5207.2, ups=0.36, wpb=14578.6, bsz=1024, num_updates=36900, lr=0.000184053, gnorm=1.499, clip=0, loss_scale=2048, train_wall=248, wall=106275
2022-08-07 10:11:12 | INFO | train_inner | epoch 010:   3617 / 3715 loss=6.736, nll_loss=3.171, mask_ins=1.009, word_ins_ml=4.677, word_reposition=0.337, kpe=0.712, ppl=106.57, wps=5200, ups=0.36, wpb=14501.5, bsz=1024, num_updates=37000, lr=0.000183804, gnorm=1.495, clip=0, loss_scale=2048, train_wall=248, wall=106554
2022-08-07 10:15:44 | INFO | train | epoch 010 | loss 6.731 | nll_loss 3.169 | mask_ins 1.01 | word_ins_ml 4.676 | word_reposition 0.335 | kpe 0.711 | ppl 106.24 | wps 5112.1 | ups 0.35 | wpb 14662.3 | bsz 1023.7 | num_updates 37098 | lr 0.000183561 | gnorm 1.499 | clip 0 | loss_scale 2838 | train_wall 9226 | wall 106826
2022-08-07 10:19:26 | INFO | valid | epoch 010 | valid on 'valid' subset | loss nan | nll_loss 3.221 | mask_ins 1.013 | word_ins_ml 4.774 | word_reposition 0.349 | kpe nan | ppl nan | wps 12366.4 | wpb 1849.4 | bsz 127.9 | num_updates 37098 | best_loss nan
2022-08-07 10:19:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 10 @ 37098 updates, score nan) (writing took 22.448655731976032 seconds)
2022-08-07 10:19:53 | INFO | train_inner | epoch 011:      2 / 3715 loss=6.723, nll_loss=3.164, mask_ins=1.007, word_ins_ml=4.672, word_reposition=0.332, kpe=0.713, ppl=105.67, wps=2769, ups=0.19, wpb=14438.3, bsz=1014.7, num_updates=37100, lr=0.000183556, gnorm=1.513, clip=0, loss_scale=2048, train_wall=247, wall=107076
2022-08-07 10:24:33 | INFO | train_inner | epoch 011:    102 / 3715 loss=6.648, nll_loss=3.11, mask_ins=1.009, word_ins_ml=4.624, word_reposition=0.327, kpe=0.687, ppl=100.3, wps=5243.2, ups=0.36, wpb=14652.3, bsz=1024, num_updates=37200, lr=0.000183309, gnorm=1.503, clip=0, loss_scale=2048, train_wall=248, wall=107355
2022-08-07 10:29:14 | INFO | train_inner | epoch 011:    202 / 3715 loss=6.629, nll_loss=3.098, mask_ins=0.998, word_ins_ml=4.613, word_reposition=0.333, kpe=0.685, ppl=98.96, wps=5233.5, ups=0.36, wpb=14707.7, bsz=1024, num_updates=37300, lr=0.000183063, gnorm=1.519, clip=0, loss_scale=2519, train_wall=249, wall=107636
2022-08-07 10:33:54 | INFO | train_inner | epoch 011:    302 / 3715 loss=6.652, nll_loss=3.115, mask_ins=1.002, word_ins_ml=4.629, word_reposition=0.335, kpe=0.686, ppl=100.56, wps=5226.7, ups=0.36, wpb=14670.4, bsz=1024, num_updates=37400, lr=0.000182818, gnorm=1.519, clip=0, loss_scale=4096, train_wall=249, wall=107917
2022-08-07 10:38:33 | INFO | train_inner | epoch 011:    402 / 3715 loss=6.645, nll_loss=3.118, mask_ins=0.999, word_ins_ml=4.631, word_reposition=0.328, kpe=0.686, ppl=100.06, wps=5256.9, ups=0.36, wpb=14634.6, bsz=1024, num_updates=37500, lr=0.000182574, gnorm=1.525, clip=0, loss_scale=4096, train_wall=247, wall=108195
2022-08-07 10:43:13 | INFO | train_inner | epoch 011:    502 / 3715 loss=6.641, nll_loss=3.104, mask_ins=1.006, word_ins_ml=4.619, word_reposition=0.334, kpe=0.683, ppl=99.81, wps=5184.6, ups=0.36, wpb=14526.6, bsz=1024, num_updates=37600, lr=0.000182331, gnorm=1.525, clip=0, loss_scale=4096, train_wall=249, wall=108475
2022-08-07 10:47:53 | INFO | train_inner | epoch 011:    602 / 3715 loss=6.661, nll_loss=3.12, mask_ins=1.002, word_ins_ml=4.633, word_reposition=0.338, kpe=0.688, ppl=101.19, wps=5244.4, ups=0.36, wpb=14673.4, bsz=1024, num_updates=37700, lr=0.000182089, gnorm=1.521, clip=0, loss_scale=4096, train_wall=248, wall=108755
2022-08-07 10:48:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 10:52:35 | INFO | train_inner | epoch 011:    703 / 3715 loss=6.671, nll_loss=3.129, mask_ins=1.001, word_ins_ml=4.641, word_reposition=0.339, kpe=0.691, ppl=101.93, wps=5207.4, ups=0.35, wpb=14714.2, bsz=1024, num_updates=37800, lr=0.000181848, gnorm=1.55, clip=0, loss_scale=2089, train_wall=251, wall=109038
2022-08-07 10:53:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-07 10:57:18 | INFO | train_inner | epoch 011:    804 / 3715 loss=6.657, nll_loss=3.117, mask_ins=1.002, word_ins_ml=4.63, word_reposition=0.33, kpe=0.695, ppl=100.92, wps=5210.7, ups=0.35, wpb=14739.1, bsz=1024, num_updates=37900, lr=0.000181608, gnorm=1.51, clip=0, loss_scale=1237, train_wall=251, wall=109321
2022-08-07 11:01:58 | INFO | train_inner | epoch 011:    904 / 3715 loss=6.64, nll_loss=3.103, mask_ins=1, word_ins_ml=4.618, word_reposition=0.331, kpe=0.693, ppl=99.76, wps=5253.3, ups=0.36, wpb=14695.8, bsz=1024, num_updates=38000, lr=0.000181369, gnorm=1.565, clip=0, loss_scale=1024, train_wall=248, wall=109600
2022-08-07 11:06:38 | INFO | train_inner | epoch 011:   1004 / 3715 loss=6.678, nll_loss=3.136, mask_ins=1.005, word_ins_ml=4.646, word_reposition=0.334, kpe=0.693, ppl=102.37, wps=5258.2, ups=0.36, wpb=14711.7, bsz=1024, num_updates=38100, lr=0.000181131, gnorm=1.533, clip=0, loss_scale=1024, train_wall=249, wall=109880
2022-08-07 11:11:17 | INFO | train_inner | epoch 011:   1104 / 3715 loss=6.645, nll_loss=3.115, mask_ins=1.001, word_ins_ml=4.628, word_reposition=0.324, kpe=0.692, ppl=100.07, wps=5237.2, ups=0.36, wpb=14629, bsz=1024, num_updates=38200, lr=0.000180894, gnorm=1.522, clip=0, loss_scale=1024, train_wall=248, wall=110160
2022-08-07 11:15:57 | INFO | train_inner | epoch 011:   1204 / 3715 loss=6.65, nll_loss=3.119, mask_ins=1, word_ins_ml=4.632, word_reposition=0.323, kpe=0.696, ppl=100.4, wps=5258.1, ups=0.36, wpb=14698, bsz=1024, num_updates=38300, lr=0.000180657, gnorm=1.524, clip=0, loss_scale=1024, train_wall=248, wall=110439
2022-08-07 11:20:36 | INFO | train_inner | epoch 011:   1304 / 3715 loss=6.676, nll_loss=3.139, mask_ins=1.004, word_ins_ml=4.649, word_reposition=0.333, kpe=0.69, ppl=102.23, wps=5205.4, ups=0.36, wpb=14522, bsz=1024, num_updates=38400, lr=0.000180422, gnorm=1.52, clip=0, loss_scale=1720, train_wall=248, wall=110718
2022-08-07 11:25:16 | INFO | train_inner | epoch 011:   1404 / 3715 loss=6.677, nll_loss=3.126, mask_ins=1.003, word_ins_ml=4.638, word_reposition=0.34, kpe=0.696, ppl=102.31, wps=5286.5, ups=0.36, wpb=14799.5, bsz=1024, num_updates=38500, lr=0.000180187, gnorm=1.514, clip=0, loss_scale=2048, train_wall=249, wall=110998
2022-08-07 11:25:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-07 11:26:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-07 11:26:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-07 11:30:04 | INFO | train_inner | epoch 011:   1507 / 3715 loss=6.687, nll_loss=3.136, mask_ins=1.007, word_ins_ml=4.647, word_reposition=0.324, kpe=0.708, ppl=103, wps=5092.6, ups=0.35, wpb=14693.8, bsz=1024, num_updates=38600, lr=0.000179954, gnorm=1.949, clip=0, loss_scale=559, train_wall=256, wall=111287
2022-08-07 11:34:43 | INFO | train_inner | epoch 011:   1607 / 3715 loss=6.661, nll_loss=3.107, mask_ins=1.005, word_ins_ml=4.62, word_reposition=0.329, kpe=0.706, ppl=101.16, wps=5286, ups=0.36, wpb=14758.7, bsz=1024, num_updates=38700, lr=0.000179721, gnorm=1.778, clip=0, loss_scale=256, train_wall=248, wall=111566
2022-08-07 11:39:23 | INFO | train_inner | epoch 011:   1707 / 3715 loss=6.688, nll_loss=3.134, mask_ins=1.006, word_ins_ml=4.645, word_reposition=0.333, kpe=0.703, ppl=103.08, wps=5255.3, ups=0.36, wpb=14680.5, bsz=1024, num_updates=38800, lr=0.00017949, gnorm=1.797, clip=0, loss_scale=256, train_wall=248, wall=111845
2022-08-07 11:44:02 | INFO | train_inner | epoch 011:   1807 / 3715 loss=6.649, nll_loss=3.116, mask_ins=0.997, word_ins_ml=4.628, word_reposition=0.328, kpe=0.695, ppl=100.33, wps=5239.4, ups=0.36, wpb=14633.6, bsz=1024, num_updates=38900, lr=0.000179259, gnorm=1.758, clip=0, loss_scale=256, train_wall=248, wall=112124
2022-08-07 11:48:43 | INFO | train_inner | epoch 011:   1907 / 3715 loss=6.667, nll_loss=3.133, mask_ins=1.002, word_ins_ml=4.644, word_reposition=0.326, kpe=0.695, ppl=101.64, wps=5202.7, ups=0.36, wpb=14609.6, bsz=1024, num_updates=39000, lr=0.000179029, gnorm=1.552, clip=0, loss_scale=256, train_wall=249, wall=112405
2022-08-07 11:53:23 | INFO | train_inner | epoch 011:   2007 / 3715 loss=6.652, nll_loss=3.12, mask_ins=1.002, word_ins_ml=4.632, word_reposition=0.322, kpe=0.695, ppl=100.54, wps=5233.4, ups=0.36, wpb=14647.9, bsz=1023.8, num_updates=39100, lr=0.0001788, gnorm=1.657, clip=0, loss_scale=428, train_wall=248, wall=112685
2022-08-07 11:58:02 | INFO | train_inner | epoch 011:   2107 / 3715 loss=6.636, nll_loss=3.106, mask_ins=0.998, word_ins_ml=4.62, word_reposition=0.324, kpe=0.694, ppl=99.46, wps=5219.6, ups=0.36, wpb=14570.5, bsz=1024, num_updates=39200, lr=0.000178571, gnorm=1.535, clip=0, loss_scale=512, train_wall=248, wall=112964
2022-08-07 12:02:41 | INFO | train_inner | epoch 011:   2207 / 3715 loss=6.665, nll_loss=3.133, mask_ins=1, word_ins_ml=4.643, word_reposition=0.326, kpe=0.696, ppl=101.49, wps=5250.7, ups=0.36, wpb=14654.6, bsz=1024, num_updates=39300, lr=0.000178344, gnorm=1.516, clip=0, loss_scale=512, train_wall=248, wall=113243
2022-08-07 12:07:20 | INFO | train_inner | epoch 011:   2307 / 3715 loss=6.646, nll_loss=3.114, mask_ins=1, word_ins_ml=4.626, word_reposition=0.323, kpe=0.696, ppl=100.14, wps=5251.5, ups=0.36, wpb=14644.9, bsz=1024, num_updates=39400, lr=0.000178118, gnorm=1.523, clip=0, loss_scale=512, train_wall=248, wall=113522
2022-08-07 12:12:50 | INFO | train_inner | epoch 011:   2407 / 3715 loss=6.662, nll_loss=3.128, mask_ins=0.996, word_ins_ml=4.64, word_reposition=0.329, kpe=0.697, ppl=101.24, wps=4422.4, ups=0.3, wpb=14592.7, bsz=1024, num_updates=39500, lr=0.000177892, gnorm=1.517, clip=0, loss_scale=512, train_wall=299, wall=113852
2022-08-07 12:19:02 | INFO | train_inner | epoch 011:   2507 / 3715 loss=6.65, nll_loss=3.106, mask_ins=1, word_ins_ml=4.619, word_reposition=0.329, kpe=0.702, ppl=100.41, wps=3973.3, ups=0.27, wpb=14777.1, bsz=1024, num_updates=39600, lr=0.000177667, gnorm=1.552, clip=0, loss_scale=794, train_wall=340, wall=114224
2022-08-07 12:24:27 | INFO | train_inner | epoch 011:   2607 / 3715 loss=6.665, nll_loss=3.127, mask_ins=1.001, word_ins_ml=4.638, word_reposition=0.328, kpe=0.698, ppl=101.48, wps=4512.7, ups=0.31, wpb=14666, bsz=1024, num_updates=39700, lr=0.000177443, gnorm=1.556, clip=0, loss_scale=1024, train_wall=294, wall=114549
2022-08-07 12:29:06 | INFO | train_inner | epoch 011:   2707 / 3715 loss=6.673, nll_loss=3.133, mask_ins=0.998, word_ins_ml=4.644, word_reposition=0.333, kpe=0.699, ppl=102.06, wps=5253.4, ups=0.36, wpb=14682.1, bsz=1024, num_updates=39800, lr=0.00017722, gnorm=1.509, clip=0, loss_scale=1024, train_wall=248, wall=114829
2022-08-07 12:33:45 | INFO | train_inner | epoch 011:   2807 / 3715 loss=6.681, nll_loss=3.13, mask_ins=1.002, word_ins_ml=4.641, word_reposition=0.335, kpe=0.703, ppl=102.61, wps=5278, ups=0.36, wpb=14735, bsz=1024, num_updates=39900, lr=0.000176998, gnorm=1.529, clip=0, loss_scale=1024, train_wall=247, wall=115108
2022-08-07 12:38:25 | INFO | train_inner | epoch 011:   2907 / 3715 loss=6.667, nll_loss=3.128, mask_ins=0.996, word_ins_ml=4.639, word_reposition=0.336, kpe=0.697, ppl=101.63, wps=5224.3, ups=0.36, wpb=14623.2, bsz=1024, num_updates=40000, lr=0.000176777, gnorm=1.521, clip=0, loss_scale=1024, train_wall=248, wall=115388
2022-08-07 12:42:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-07 12:43:07 | INFO | train_inner | epoch 011:   3008 / 3715 loss=6.663, nll_loss=3.123, mask_ins=0.999, word_ins_ml=4.635, word_reposition=0.333, kpe=0.696, ppl=101.32, wps=5199.8, ups=0.36, wpb=14639.2, bsz=1024, num_updates=40100, lr=0.000176556, gnorm=1.517, clip=0, loss_scale=1288, train_wall=250, wall=115669
2022-08-07 12:47:47 | INFO | train_inner | epoch 011:   3108 / 3715 loss=6.68, nll_loss=3.135, mask_ins=1.002, word_ins_ml=4.645, word_reposition=0.333, kpe=0.7, ppl=102.53, wps=5236.5, ups=0.36, wpb=14648.8, bsz=1024, num_updates=40200, lr=0.000176336, gnorm=1.52, clip=0, loss_scale=1024, train_wall=248, wall=115949
2022-08-07 12:52:24 | INFO | train_inner | epoch 011:   3208 / 3715 loss=6.683, nll_loss=3.139, mask_ins=1.003, word_ins_ml=4.649, word_reposition=0.33, kpe=0.702, ppl=102.77, wps=5272.4, ups=0.36, wpb=14634, bsz=1024, num_updates=40300, lr=0.000176117, gnorm=1.525, clip=0, loss_scale=1024, train_wall=246, wall=116227
2022-08-07 12:57:04 | INFO | train_inner | epoch 011:   3308 / 3715 loss=6.681, nll_loss=3.14, mask_ins=1.004, word_ins_ml=4.649, word_reposition=0.33, kpe=0.698, ppl=102.61, wps=5262.2, ups=0.36, wpb=14703.9, bsz=1024, num_updates=40400, lr=0.000175899, gnorm=1.5, clip=0, loss_scale=1024, train_wall=248, wall=116506
2022-08-07 13:01:43 | INFO | train_inner | epoch 011:   3408 / 3715 loss=6.668, nll_loss=3.123, mask_ins=0.999, word_ins_ml=4.635, word_reposition=0.335, kpe=0.699, ppl=101.66, wps=5224.8, ups=0.36, wpb=14582.9, bsz=1024, num_updates=40500, lr=0.000175682, gnorm=1.53, clip=0, loss_scale=1024, train_wall=248, wall=116785
2022-08-07 13:04:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-07 13:06:26 | INFO | train_inner | epoch 011:   3509 / 3715 loss=6.68, nll_loss=3.131, mask_ins=1.005, word_ins_ml=4.642, word_reposition=0.33, kpe=0.702, ppl=102.51, wps=5174.9, ups=0.35, wpb=14648.4, bsz=1024, num_updates=40600, lr=0.000175466, gnorm=1.541, clip=0, loss_scale=771, train_wall=251, wall=117068
2022-08-07 13:10:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-07 13:11:08 | INFO | train_inner | epoch 011:   3610 / 3715 loss=6.665, nll_loss=3.134, mask_ins=0.997, word_ins_ml=4.644, word_reposition=0.321, kpe=0.702, ppl=101.47, wps=5197.5, ups=0.35, wpb=14672.7, bsz=1024, num_updates=40700, lr=0.00017525, gnorm=1.511, clip=0, loss_scale=456, train_wall=251, wall=117350
2022-08-07 13:15:48 | INFO | train_inner | epoch 011:   3710 / 3715 loss=6.701, nll_loss=3.147, mask_ins=1.011, word_ins_ml=4.656, word_reposition=0.329, kpe=0.705, ppl=104.03, wps=5257.8, ups=0.36, wpb=14697.5, bsz=1024, num_updates=40800, lr=0.000175035, gnorm=1.578, clip=0, loss_scale=256, train_wall=248, wall=117630
2022-08-07 13:15:59 | INFO | train | epoch 011 | loss 6.663 | nll_loss 3.124 | mask_ins 1.002 | word_ins_ml 4.635 | word_reposition 0.33 | kpe 0.696 | ppl 101.35 | wps 5025.3 | ups 0.34 | wpb 14661.5 | bsz 1023.7 | num_updates 40805 | lr 0.000175024 | gnorm 1.563 | clip 0 | loss_scale 1294 | train_wall 9406 | wall 117642
2022-08-07 13:19:39 | INFO | valid | epoch 011 | valid on 'valid' subset | loss nan | nll_loss 3.199 | mask_ins 1.01 | word_ins_ml 4.751 | word_reposition 0.342 | kpe nan | ppl nan | wps 12454.5 | wpb 1849.4 | bsz 127.9 | num_updates 40805 | best_loss nan
2022-08-07 13:20:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 11 @ 40805 updates, score nan) (writing took 27.19459968432784 seconds)
2022-08-07 13:24:30 | INFO | train_inner | epoch 012:     95 / 3715 loss=6.581, nll_loss=3.072, mask_ins=0.997, word_ins_ml=4.59, word_reposition=0.324, kpe=0.67, ppl=95.72, wps=2770.4, ups=0.19, wpb=14482.7, bsz=1014.7, num_updates=40900, lr=0.000174821, gnorm=1.582, clip=0, loss_scale=256, train_wall=245, wall=118153
2022-08-07 13:29:10 | INFO | train_inner | epoch 012:    195 / 3715 loss=6.572, nll_loss=3.051, mask_ins=0.997, word_ins_ml=4.571, word_reposition=0.331, kpe=0.673, ppl=95.13, wps=5267.5, ups=0.36, wpb=14731.7, bsz=1024, num_updates=41000, lr=0.000174608, gnorm=1.549, clip=0, loss_scale=256, train_wall=248, wall=118432
2022-08-07 13:33:49 | INFO | train_inner | epoch 012:    295 / 3715 loss=6.581, nll_loss=3.069, mask_ins=0.994, word_ins_ml=4.587, word_reposition=0.328, kpe=0.671, ppl=95.73, wps=5250.2, ups=0.36, wpb=14663.1, bsz=1024, num_updates=41100, lr=0.000174395, gnorm=1.543, clip=0, loss_scale=256, train_wall=248, wall=118712
2022-08-07 13:38:30 | INFO | train_inner | epoch 012:    395 / 3715 loss=6.581, nll_loss=3.079, mask_ins=0.986, word_ins_ml=4.596, word_reposition=0.323, kpe=0.676, ppl=95.76, wps=5256.3, ups=0.36, wpb=14728, bsz=1024, num_updates=41200, lr=0.000174183, gnorm=1.548, clip=0, loss_scale=282, train_wall=249, wall=118992
2022-08-07 13:43:08 | INFO | train_inner | epoch 012:    495 / 3715 loss=6.588, nll_loss=3.066, mask_ins=0.99, word_ins_ml=4.584, word_reposition=0.336, kpe=0.678, ppl=96.21, wps=5304.1, ups=0.36, wpb=14786.8, bsz=1024, num_updates=41300, lr=0.000173972, gnorm=1.556, clip=0, loss_scale=512, train_wall=248, wall=119271
2022-08-07 13:47:48 | INFO | train_inner | epoch 012:    595 / 3715 loss=6.589, nll_loss=3.08, mask_ins=0.996, word_ins_ml=4.597, word_reposition=0.322, kpe=0.674, ppl=96.28, wps=5222.5, ups=0.36, wpb=14630.9, bsz=1024, num_updates=41400, lr=0.000173762, gnorm=1.551, clip=0, loss_scale=512, train_wall=249, wall=119551
2022-08-07 13:52:28 | INFO | train_inner | epoch 012:    695 / 3715 loss=6.575, nll_loss=3.065, mask_ins=0.992, word_ins_ml=4.583, word_reposition=0.325, kpe=0.675, ppl=95.37, wps=5262.9, ups=0.36, wpb=14720.4, bsz=1024, num_updates=41500, lr=0.000173553, gnorm=1.558, clip=0, loss_scale=512, train_wall=248, wall=119831
2022-08-07 13:57:10 | INFO | train_inner | epoch 012:    795 / 3715 loss=6.592, nll_loss=3.073, mask_ins=0.996, word_ins_ml=4.59, word_reposition=0.328, kpe=0.678, ppl=96.5, wps=5274.3, ups=0.36, wpb=14838, bsz=1024, num_updates=41600, lr=0.000173344, gnorm=1.562, clip=0, loss_scale=512, train_wall=250, wall=120112
2022-08-07 14:01:48 | INFO | train_inner | epoch 012:    895 / 3715 loss=6.582, nll_loss=3.069, mask_ins=0.993, word_ins_ml=4.587, word_reposition=0.327, kpe=0.675, ppl=95.79, wps=5240.7, ups=0.36, wpb=14611.2, bsz=1024, num_updates=41700, lr=0.000173136, gnorm=1.573, clip=0, loss_scale=512, train_wall=247, wall=120391
2022-08-07 14:06:28 | INFO | train_inner | epoch 012:    995 / 3715 loss=6.598, nll_loss=3.084, mask_ins=0.993, word_ins_ml=4.6, word_reposition=0.33, kpe=0.675, ppl=96.87, wps=5256.6, ups=0.36, wpb=14715.3, bsz=1024, num_updates=41800, lr=0.000172929, gnorm=1.565, clip=0, loss_scale=1014, train_wall=249, wall=120671
2022-08-07 14:11:10 | INFO | train_inner | epoch 012:   1095 / 3715 loss=6.577, nll_loss=3.07, mask_ins=0.988, word_ins_ml=4.587, word_reposition=0.324, kpe=0.678, ppl=95.5, wps=5220.5, ups=0.36, wpb=14694.3, bsz=1024, num_updates=41900, lr=0.000172722, gnorm=1.585, clip=0, loss_scale=1024, train_wall=250, wall=120952
2022-08-07 14:15:49 | INFO | train_inner | epoch 012:   1195 / 3715 loss=6.597, nll_loss=3.077, mask_ins=0.992, word_ins_ml=4.594, word_reposition=0.331, kpe=0.679, ppl=96.78, wps=5245.4, ups=0.36, wpb=14656.7, bsz=1024, num_updates=42000, lr=0.000172516, gnorm=1.582, clip=0, loss_scale=1024, train_wall=248, wall=121232
2022-08-07 14:20:28 | INFO | train_inner | epoch 012:   1295 / 3715 loss=6.622, nll_loss=3.093, mask_ins=1.003, word_ins_ml=4.608, word_reposition=0.331, kpe=0.68, ppl=98.48, wps=5259.1, ups=0.36, wpb=14684, bsz=1024, num_updates=42100, lr=0.000172311, gnorm=1.554, clip=0, loss_scale=1024, train_wall=248, wall=121511
2022-08-07 14:25:06 | INFO | train_inner | epoch 012:   1395 / 3715 loss=6.602, nll_loss=3.089, mask_ins=0.991, word_ins_ml=4.605, word_reposition=0.326, kpe=0.68, ppl=97.11, wps=5271.8, ups=0.36, wpb=14652.6, bsz=1024, num_updates=42200, lr=0.000172107, gnorm=1.555, clip=0, loss_scale=1024, train_wall=247, wall=121789
2022-08-07 14:29:46 | INFO | train_inner | epoch 012:   1495 / 3715 loss=6.6, nll_loss=3.079, mask_ins=0.995, word_ins_ml=4.596, word_reposition=0.329, kpe=0.681, ppl=97.04, wps=5245.2, ups=0.36, wpb=14672.7, bsz=1024, num_updates=42300, lr=0.000171904, gnorm=1.566, clip=0, loss_scale=1905, train_wall=248, wall=122068
2022-08-07 14:34:25 | INFO | train_inner | epoch 012:   1595 / 3715 loss=6.607, nll_loss=3.093, mask_ins=0.995, word_ins_ml=4.608, word_reposition=0.326, kpe=0.678, ppl=97.48, wps=5240.8, ups=0.36, wpb=14625.2, bsz=1024, num_updates=42400, lr=0.000171701, gnorm=1.573, clip=0, loss_scale=2048, train_wall=247, wall=122348
2022-08-07 14:39:05 | INFO | train_inner | epoch 012:   1695 / 3715 loss=6.604, nll_loss=3.087, mask_ins=0.994, word_ins_ml=4.603, word_reposition=0.324, kpe=0.683, ppl=97.25, wps=5275.6, ups=0.36, wpb=14752.8, bsz=1024, num_updates=42500, lr=0.000171499, gnorm=1.561, clip=0, loss_scale=2048, train_wall=248, wall=122627
2022-08-07 14:43:44 | INFO | train_inner | epoch 012:   1795 / 3715 loss=6.58, nll_loss=3.064, mask_ins=0.99, word_ins_ml=4.582, word_reposition=0.329, kpe=0.678, ppl=95.67, wps=5208.4, ups=0.36, wpb=14566.3, bsz=1024, num_updates=42600, lr=0.000171297, gnorm=1.567, clip=0, loss_scale=2048, train_wall=248, wall=122907
2022-08-07 14:48:25 | INFO | train_inner | epoch 012:   1895 / 3715 loss=6.615, nll_loss=3.089, mask_ins=0.994, word_ins_ml=4.604, word_reposition=0.33, kpe=0.687, ppl=98.03, wps=5302.6, ups=0.36, wpb=14865.8, bsz=1024, num_updates=42700, lr=0.000171096, gnorm=1.582, clip=0, loss_scale=2048, train_wall=249, wall=123187
2022-08-07 14:49:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 14:53:08 | INFO | train_inner | epoch 012:   1996 / 3715 loss=6.6, nll_loss=3.08, mask_ins=0.991, word_ins_ml=4.596, word_reposition=0.329, kpe=0.684, ppl=97.01, wps=5212.1, ups=0.35, wpb=14735.2, bsz=1024, num_updates=42800, lr=0.000170896, gnorm=1.546, clip=0, loss_scale=2068, train_wall=251, wall=123470
2022-08-07 14:57:47 | INFO | train_inner | epoch 012:   2096 / 3715 loss=6.595, nll_loss=3.075, mask_ins=0.99, word_ins_ml=4.592, word_reposition=0.329, kpe=0.684, ppl=96.7, wps=5248.9, ups=0.36, wpb=14666.5, bsz=1024, num_updates=42900, lr=0.000170697, gnorm=1.546, clip=0, loss_scale=2048, train_wall=248, wall=123749
2022-08-07 15:02:26 | INFO | train_inner | epoch 012:   2196 / 3715 loss=6.616, nll_loss=3.087, mask_ins=0.999, word_ins_ml=4.603, word_reposition=0.33, kpe=0.685, ppl=98.08, wps=5268.1, ups=0.36, wpb=14719.7, bsz=1024, num_updates=43000, lr=0.000170499, gnorm=1.549, clip=0, loss_scale=2048, train_wall=248, wall=124029
2022-08-07 15:07:06 | INFO | train_inner | epoch 012:   2296 / 3715 loss=6.618, nll_loss=3.103, mask_ins=0.997, word_ins_ml=4.617, word_reposition=0.323, kpe=0.682, ppl=98.21, wps=5211.8, ups=0.36, wpb=14563, bsz=1024, num_updates=43100, lr=0.000170301, gnorm=1.573, clip=0, loss_scale=2048, train_wall=248, wall=124308
2022-08-07 15:11:45 | INFO | train_inner | epoch 012:   2396 / 3715 loss=6.623, nll_loss=3.106, mask_ins=0.997, word_ins_ml=4.62, word_reposition=0.324, kpe=0.683, ppl=98.57, wps=5228.3, ups=0.36, wpb=14580, bsz=1024, num_updates=43200, lr=0.000170103, gnorm=1.557, clip=0, loss_scale=2048, train_wall=247, wall=124587
2022-08-07 15:16:24 | INFO | train_inner | epoch 012:   2496 / 3715 loss=6.614, nll_loss=3.098, mask_ins=0.995, word_ins_ml=4.612, word_reposition=0.323, kpe=0.684, ppl=97.97, wps=5220.8, ups=0.36, wpb=14599.6, bsz=1024, num_updates=43300, lr=0.000169907, gnorm=1.554, clip=0, loss_scale=3318, train_wall=248, wall=124867
2022-08-07 15:21:05 | INFO | train_inner | epoch 012:   2596 / 3715 loss=6.62, nll_loss=3.094, mask_ins=0.993, word_ins_ml=4.609, word_reposition=0.332, kpe=0.686, ppl=98.34, wps=5233.9, ups=0.36, wpb=14673.8, bsz=1024, num_updates=43400, lr=0.000169711, gnorm=1.558, clip=0, loss_scale=4096, train_wall=249, wall=125147
2022-08-07 15:25:46 | INFO | train_inner | epoch 012:   2696 / 3715 loss=6.601, nll_loss=3.086, mask_ins=0.993, word_ins_ml=4.602, word_reposition=0.325, kpe=0.682, ppl=97.1, wps=5208.4, ups=0.36, wpb=14639.3, bsz=1024, num_updates=43500, lr=0.000169516, gnorm=1.559, clip=0, loss_scale=4096, train_wall=250, wall=125428
2022-08-07 15:30:26 | INFO | train_inner | epoch 012:   2796 / 3715 loss=6.619, nll_loss=3.1, mask_ins=0.995, word_ins_ml=4.615, word_reposition=0.327, kpe=0.682, ppl=98.27, wps=5204.7, ups=0.36, wpb=14561.3, bsz=1024, num_updates=43600, lr=0.000169321, gnorm=1.557, clip=0, loss_scale=4096, train_wall=249, wall=125708
2022-08-07 15:32:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 15:35:08 | INFO | train_inner | epoch 012:   2897 / 3715 loss=6.593, nll_loss=3.08, mask_ins=0.993, word_ins_ml=4.596, word_reposition=0.321, kpe=0.683, ppl=96.51, wps=5156.4, ups=0.35, wpb=14568.5, bsz=1024, num_updates=43700, lr=0.000169128, gnorm=1.552, clip=0, loss_scale=2778, train_wall=251, wall=125990
2022-08-07 15:39:49 | INFO | train_inner | epoch 012:   2997 / 3715 loss=6.602, nll_loss=3.086, mask_ins=0.99, word_ins_ml=4.602, word_reposition=0.327, kpe=0.684, ppl=97.15, wps=5206.9, ups=0.36, wpb=14625.8, bsz=1024, num_updates=43800, lr=0.000168934, gnorm=1.546, clip=0, loss_scale=2048, train_wall=249, wall=126271
2022-08-07 15:44:30 | INFO | train_inner | epoch 012:   3097 / 3715 loss=6.599, nll_loss=3.088, mask_ins=0.988, word_ins_ml=4.604, word_reposition=0.323, kpe=0.684, ppl=96.93, wps=5204.5, ups=0.36, wpb=14606.9, bsz=1024, num_updates=43900, lr=0.000168742, gnorm=1.556, clip=0, loss_scale=2048, train_wall=249, wall=126552
2022-08-07 15:49:10 | INFO | train_inner | epoch 012:   3197 / 3715 loss=6.623, nll_loss=3.099, mask_ins=0.992, word_ins_ml=4.613, word_reposition=0.331, kpe=0.686, ppl=98.54, wps=5210.5, ups=0.36, wpb=14610.6, bsz=1024, num_updates=44000, lr=0.00016855, gnorm=1.568, clip=0, loss_scale=2048, train_wall=249, wall=126832
2022-08-07 15:53:49 | INFO | train_inner | epoch 012:   3297 / 3715 loss=6.606, nll_loss=3.082, mask_ins=0.99, word_ins_ml=4.598, word_reposition=0.329, kpe=0.689, ppl=97.42, wps=5278.4, ups=0.36, wpb=14737.4, bsz=1024, num_updates=44100, lr=0.000168359, gnorm=1.541, clip=0, loss_scale=2048, train_wall=247, wall=127112
2022-08-07 15:58:29 | INFO | train_inner | epoch 012:   3397 / 3715 loss=6.615, nll_loss=3.086, mask_ins=0.989, word_ins_ml=4.602, word_reposition=0.34, kpe=0.684, ppl=98.05, wps=5258.6, ups=0.36, wpb=14691.6, bsz=1024, num_updates=44200, lr=0.000168168, gnorm=1.539, clip=0, loss_scale=3133, train_wall=248, wall=127391
2022-08-07 16:03:08 | INFO | train_inner | epoch 012:   3497 / 3715 loss=6.595, nll_loss=3.086, mask_ins=0.992, word_ins_ml=4.601, word_reposition=0.319, kpe=0.683, ppl=96.67, wps=5208.3, ups=0.36, wpb=14560.7, bsz=1024, num_updates=44300, lr=0.000167978, gnorm=1.548, clip=0, loss_scale=4096, train_wall=248, wall=127671
2022-08-07 16:07:48 | INFO | train_inner | epoch 012:   3597 / 3715 loss=6.64, nll_loss=3.117, mask_ins=0.996, word_ins_ml=4.628, word_reposition=0.328, kpe=0.687, ppl=99.73, wps=5251.8, ups=0.36, wpb=14678, bsz=1024, num_updates=44400, lr=0.000167789, gnorm=1.555, clip=0, loss_scale=4096, train_wall=248, wall=127950
2022-08-07 16:12:27 | INFO | train_inner | epoch 012:   3697 / 3715 loss=6.605, nll_loss=3.089, mask_ins=0.989, word_ins_ml=4.604, word_reposition=0.326, kpe=0.686, ppl=97.36, wps=5234.5, ups=0.36, wpb=14620.7, bsz=1023.8, num_updates=44500, lr=0.0001676, gnorm=1.546, clip=0, loss_scale=4096, train_wall=248, wall=128229
2022-08-07 16:13:15 | INFO | train | epoch 012 | loss 6.601 | nll_loss 3.083 | mask_ins 0.993 | word_ins_ml 4.599 | word_reposition 0.327 | kpe 0.681 | ppl 97.05 | wps 5118.6 | ups 0.35 | wpb 14662 | bsz 1023.7 | num_updates 44518 | lr 0.000167566 | gnorm 1.559 | clip 0 | loss_scale 1934 | train_wall 9223 | wall 128277
2022-08-07 16:16:56 | INFO | valid | epoch 012 | valid on 'valid' subset | loss nan | nll_loss 3.181 | mask_ins 1.013 | word_ins_ml 4.734 | word_reposition 0.348 | kpe nan | ppl nan | wps 12402.5 | wpb 1849.4 | bsz 127.9 | num_updates 44518 | best_loss nan
2022-08-07 16:17:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 12 @ 44518 updates, score nan) (writing took 24.72055108472705 seconds)
2022-08-07 16:21:09 | INFO | train_inner | epoch 013:     82 / 3715 loss=6.522, nll_loss=3.029, mask_ins=0.986, word_ins_ml=4.551, word_reposition=0.321, kpe=0.663, ppl=91.91, wps=2795.7, ups=0.19, wpb=14582.3, bsz=1014.7, num_updates=44600, lr=0.000167412, gnorm=1.605, clip=0, loss_scale=4096, train_wall=245, wall=128751
2022-08-07 16:25:49 | INFO | train_inner | epoch 013:    182 / 3715 loss=6.518, nll_loss=3.035, mask_ins=0.988, word_ins_ml=4.557, word_reposition=0.322, kpe=0.652, ppl=91.65, wps=5207.1, ups=0.36, wpb=14578.3, bsz=1024, num_updates=44700, lr=0.000167225, gnorm=1.582, clip=0, loss_scale=5775, train_wall=249, wall=129031
2022-08-07 16:26:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-07 16:27:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 16:30:34 | INFO | train_inner | epoch 013:    284 / 3715 loss=6.496, nll_loss=3.007, mask_ins=0.987, word_ins_ml=4.532, word_reposition=0.322, kpe=0.655, ppl=90.25, wps=5160.3, ups=0.35, wpb=14750.3, bsz=1024, num_updates=44800, lr=0.000167038, gnorm=1.633, clip=0, loss_scale=3494, train_wall=254, wall=129317
2022-08-07 16:33:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-07 16:33:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-07 16:35:20 | INFO | train_inner | epoch 013:    386 / 3715 loss=6.51, nll_loss=3.022, mask_ins=0.982, word_ins_ml=4.545, word_reposition=0.326, kpe=0.657, ppl=91.15, wps=5112.8, ups=0.35, wpb=14605.8, bsz=1024, num_updates=44900, lr=0.000166852, gnorm=1.748, clip=0, loss_scale=1421, train_wall=254, wall=129602
2022-08-07 16:40:01 | INFO | train_inner | epoch 013:    486 / 3715 loss=6.52, nll_loss=3.036, mask_ins=0.983, word_ins_ml=4.557, word_reposition=0.322, kpe=0.658, ppl=91.75, wps=5199.9, ups=0.36, wpb=14618.9, bsz=1024, num_updates=45000, lr=0.000166667, gnorm=1.6, clip=0, loss_scale=512, train_wall=250, wall=129884
2022-08-07 16:44:41 | INFO | train_inner | epoch 013:    586 / 3715 loss=6.518, nll_loss=3.031, mask_ins=0.986, word_ins_ml=4.553, word_reposition=0.322, kpe=0.657, ppl=91.63, wps=5222.5, ups=0.36, wpb=14624.3, bsz=1024, num_updates=45100, lr=0.000166482, gnorm=1.596, clip=0, loss_scale=512, train_wall=249, wall=130164
2022-08-07 16:46:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-07 16:47:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-07 16:49:27 | INFO | train_inner | epoch 013:    688 / 3715 loss=6.618, nll_loss=3.088, mask_ins=0.995, word_ins_ml=4.604, word_reposition=0.335, kpe=0.684, ppl=98.26, wps=5147.8, ups=0.35, wpb=14696.2, bsz=1024, num_updates=45200, lr=0.000166298, gnorm=7.022, clip=8, loss_scale=310, train_wall=253, wall=130449
2022-08-07 16:54:06 | INFO | train_inner | epoch 013:    788 / 3715 loss=6.534, nll_loss=3.037, mask_ins=0.982, word_ins_ml=4.558, word_reposition=0.326, kpe=0.668, ppl=92.67, wps=5248.7, ups=0.36, wpb=14639.1, bsz=1024, num_updates=45300, lr=0.000166114, gnorm=1.64, clip=0, loss_scale=128, train_wall=248, wall=130728
2022-08-07 16:58:44 | INFO | train_inner | epoch 013:    888 / 3715 loss=6.488, nll_loss=3.003, mask_ins=0.976, word_ins_ml=4.528, word_reposition=0.324, kpe=0.66, ppl=89.78, wps=5270.7, ups=0.36, wpb=14693.5, bsz=1024, num_updates=45400, lr=0.000165931, gnorm=1.564, clip=0, loss_scale=128, train_wall=247, wall=131007
2022-08-07 17:03:24 | INFO | train_inner | epoch 013:    988 / 3715 loss=6.521, nll_loss=3.033, mask_ins=0.984, word_ins_ml=4.554, word_reposition=0.322, kpe=0.661, ppl=91.84, wps=5269.7, ups=0.36, wpb=14706.2, bsz=1024, num_updates=45500, lr=0.000165748, gnorm=1.593, clip=0, loss_scale=128, train_wall=248, wall=131286
2022-08-07 17:08:02 | INFO | train_inner | epoch 013:   1088 / 3715 loss=6.539, nll_loss=3.038, mask_ins=0.987, word_ins_ml=4.56, word_reposition=0.33, kpe=0.662, ppl=93, wps=5276.8, ups=0.36, wpb=14720.7, bsz=1024, num_updates=45600, lr=0.000165567, gnorm=1.59, clip=0, loss_scale=128, train_wall=248, wall=131565
2022-08-07 17:12:41 | INFO | train_inner | epoch 013:   1188 / 3715 loss=6.541, nll_loss=3.045, mask_ins=0.989, word_ins_ml=4.565, word_reposition=0.322, kpe=0.664, ppl=93.11, wps=5270.3, ups=0.36, wpb=14696.6, bsz=1024, num_updates=45700, lr=0.000165385, gnorm=1.829, clip=0, loss_scale=175, train_wall=248, wall=131844
2022-08-07 17:17:20 | INFO | train_inner | epoch 013:   1288 / 3715 loss=6.533, nll_loss=3.036, mask_ins=0.98, word_ins_ml=4.557, word_reposition=0.33, kpe=0.666, ppl=92.6, wps=5271.1, ups=0.36, wpb=14704.5, bsz=1024, num_updates=45800, lr=0.000165205, gnorm=1.82, clip=0, loss_scale=256, train_wall=248, wall=132123
2022-08-07 17:22:01 | INFO | train_inner | epoch 013:   1388 / 3715 loss=6.543, nll_loss=3.046, mask_ins=0.985, word_ins_ml=4.566, word_reposition=0.32, kpe=0.672, ppl=93.24, wps=5315.7, ups=0.36, wpb=14921.3, bsz=1024, num_updates=45900, lr=0.000165025, gnorm=1.75, clip=0, loss_scale=256, train_wall=250, wall=132403
2022-08-07 17:26:40 | INFO | train_inner | epoch 013:   1488 / 3715 loss=6.537, nll_loss=3.045, mask_ins=0.982, word_ins_ml=4.565, word_reposition=0.324, kpe=0.665, ppl=92.84, wps=5262, ups=0.36, wpb=14675.9, bsz=1024, num_updates=46000, lr=0.000164845, gnorm=1.732, clip=0, loss_scale=256, train_wall=248, wall=132682
2022-08-07 17:31:19 | INFO | train_inner | epoch 013:   1588 / 3715 loss=6.531, nll_loss=3.039, mask_ins=0.984, word_ins_ml=4.56, word_reposition=0.321, kpe=0.666, ppl=92.46, wps=5251, ups=0.36, wpb=14676.4, bsz=1024, num_updates=46100, lr=0.000164666, gnorm=1.581, clip=0, loss_scale=256, train_wall=248, wall=132962
2022-08-07 17:35:59 | INFO | train_inner | epoch 013:   1688 / 3715 loss=6.547, nll_loss=3.049, mask_ins=0.988, word_ins_ml=4.569, word_reposition=0.327, kpe=0.664, ppl=93.53, wps=5217.5, ups=0.36, wpb=14582.7, bsz=1024, num_updates=46200, lr=0.000164488, gnorm=1.633, clip=0, loss_scale=320, train_wall=248, wall=133241
2022-08-07 17:40:38 | INFO | train_inner | epoch 013:   1788 / 3715 loss=6.55, nll_loss=3.05, mask_ins=0.986, word_ins_ml=4.569, word_reposition=0.329, kpe=0.666, ppl=93.73, wps=5255.1, ups=0.36, wpb=14674.1, bsz=1024, num_updates=46300, lr=0.00016431, gnorm=1.608, clip=0, loss_scale=512, train_wall=248, wall=133521
2022-08-07 17:45:18 | INFO | train_inner | epoch 013:   1888 / 3715 loss=6.527, nll_loss=3.03, mask_ins=0.983, word_ins_ml=4.552, word_reposition=0.326, kpe=0.666, ppl=92.21, wps=5253.3, ups=0.36, wpb=14677.2, bsz=1024, num_updates=46400, lr=0.000164133, gnorm=1.588, clip=0, loss_scale=512, train_wall=248, wall=133800
2022-08-07 17:49:57 | INFO | train_inner | epoch 013:   1988 / 3715 loss=6.548, nll_loss=3.06, mask_ins=0.982, word_ins_ml=4.578, word_reposition=0.319, kpe=0.668, ppl=93.57, wps=5253.3, ups=0.36, wpb=14672.9, bsz=1024, num_updates=46500, lr=0.000163956, gnorm=1.596, clip=0, loss_scale=512, train_wall=248, wall=134079
2022-08-07 17:54:35 | INFO | train_inner | epoch 013:   2088 / 3715 loss=6.556, nll_loss=3.055, mask_ins=0.987, word_ins_ml=4.574, word_reposition=0.329, kpe=0.666, ppl=94.09, wps=5255.7, ups=0.36, wpb=14619.7, bsz=1024, num_updates=46600, lr=0.00016378, gnorm=1.588, clip=0, loss_scale=512, train_wall=247, wall=134357
2022-08-07 17:59:14 | INFO | train_inner | epoch 013:   2188 / 3715 loss=6.56, nll_loss=3.065, mask_ins=0.986, word_ins_ml=4.583, word_reposition=0.32, kpe=0.673, ppl=94.38, wps=5262.4, ups=0.36, wpb=14696.5, bsz=1024, num_updates=46700, lr=0.000163605, gnorm=1.597, clip=0, loss_scale=579, train_wall=248, wall=134637
2022-08-07 18:03:54 | INFO | train_inner | epoch 013:   2288 / 3715 loss=6.549, nll_loss=3.053, mask_ins=0.984, word_ins_ml=4.572, word_reposition=0.322, kpe=0.67, ppl=93.62, wps=5258.2, ups=0.36, wpb=14680.6, bsz=1024, num_updates=46800, lr=0.00016343, gnorm=1.593, clip=0, loss_scale=1024, train_wall=248, wall=134916
2022-08-07 18:08:34 | INFO | train_inner | epoch 013:   2388 / 3715 loss=6.552, nll_loss=3.059, mask_ins=0.984, word_ins_ml=4.578, word_reposition=0.322, kpe=0.669, ppl=93.85, wps=5214.1, ups=0.36, wpb=14626.5, bsz=1024, num_updates=46900, lr=0.000163256, gnorm=1.602, clip=0, loss_scale=1024, train_wall=249, wall=135196
2022-08-07 18:13:14 | INFO | train_inner | epoch 013:   2488 / 3715 loss=6.558, nll_loss=3.056, mask_ins=0.991, word_ins_ml=4.574, word_reposition=0.325, kpe=0.668, ppl=94.23, wps=5195.9, ups=0.36, wpb=14562.1, bsz=1024, num_updates=47000, lr=0.000163082, gnorm=1.596, clip=0, loss_scale=1024, train_wall=249, wall=135477
2022-08-07 18:17:54 | INFO | train_inner | epoch 013:   2588 / 3715 loss=6.546, nll_loss=3.041, mask_ins=0.99, word_ins_ml=4.562, word_reposition=0.323, kpe=0.67, ppl=93.41, wps=5283.5, ups=0.36, wpb=14755.7, bsz=1024, num_updates=47100, lr=0.000162909, gnorm=1.566, clip=0, loss_scale=1024, train_wall=248, wall=135756
2022-08-07 18:22:33 | INFO | train_inner | epoch 013:   2688 / 3715 loss=6.547, nll_loss=3.051, mask_ins=0.988, word_ins_ml=4.57, word_reposition=0.318, kpe=0.67, ppl=93.49, wps=5263.7, ups=0.36, wpb=14702.6, bsz=1024, num_updates=47200, lr=0.000162736, gnorm=1.576, clip=0, loss_scale=1034, train_wall=248, wall=136035
2022-08-07 18:27:13 | INFO | train_inner | epoch 013:   2788 / 3715 loss=6.573, nll_loss=3.069, mask_ins=0.99, word_ins_ml=4.587, word_reposition=0.326, kpe=0.67, ppl=95.2, wps=5210.9, ups=0.36, wpb=14596.4, bsz=1024, num_updates=47300, lr=0.000162564, gnorm=1.598, clip=0, loss_scale=2048, train_wall=249, wall=136315
2022-08-07 18:31:53 | INFO | train_inner | epoch 013:   2888 / 3715 loss=6.536, nll_loss=3.037, mask_ins=0.985, word_ins_ml=4.558, word_reposition=0.325, kpe=0.668, ppl=92.79, wps=5221.9, ups=0.36, wpb=14596.5, bsz=1024, num_updates=47400, lr=0.000162392, gnorm=1.574, clip=0, loss_scale=2048, train_wall=248, wall=136595
2022-08-07 18:36:32 | INFO | train_inner | epoch 013:   2988 / 3715 loss=6.522, nll_loss=3.037, mask_ins=0.982, word_ins_ml=4.558, word_reposition=0.314, kpe=0.668, ppl=91.89, wps=5220.4, ups=0.36, wpb=14578.8, bsz=1024, num_updates=47500, lr=0.000162221, gnorm=1.576, clip=0, loss_scale=2048, train_wall=248, wall=136874
2022-08-07 18:41:11 | INFO | train_inner | epoch 013:   3088 / 3715 loss=6.569, nll_loss=3.062, mask_ins=0.986, word_ins_ml=4.58, word_reposition=0.329, kpe=0.674, ppl=94.96, wps=5239.3, ups=0.36, wpb=14631, bsz=1024, num_updates=47600, lr=0.000162051, gnorm=1.574, clip=0, loss_scale=2048, train_wall=248, wall=137153
2022-08-07 18:45:52 | INFO | train_inner | epoch 013:   3188 / 3715 loss=6.525, nll_loss=3.031, mask_ins=0.976, word_ins_ml=4.553, word_reposition=0.323, kpe=0.673, ppl=92.07, wps=5233.9, ups=0.36, wpb=14685.6, bsz=1024, num_updates=47700, lr=0.000161881, gnorm=1.57, clip=0, loss_scale=2048, train_wall=249, wall=137434
2022-08-07 18:46:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 18:50:33 | INFO | train_inner | epoch 013:   3289 / 3715 loss=6.56, nll_loss=3.056, mask_ins=0.986, word_ins_ml=4.575, word_reposition=0.329, kpe=0.67, ppl=94.37, wps=5213.9, ups=0.36, wpb=14671.2, bsz=1023.8, num_updates=47800, lr=0.000161712, gnorm=1.598, clip=0, loss_scale=2149, train_wall=250, wall=137715
2022-08-07 18:55:12 | INFO | train_inner | epoch 013:   3389 / 3715 loss=6.54, nll_loss=3.049, mask_ins=0.979, word_ins_ml=4.568, word_reposition=0.325, kpe=0.668, ppl=93.04, wps=5214.9, ups=0.36, wpb=14562.3, bsz=1024, num_updates=47900, lr=0.000161543, gnorm=1.562, clip=0, loss_scale=2048, train_wall=248, wall=137995
2022-08-07 18:59:52 | INFO | train_inner | epoch 013:   3489 / 3715 loss=6.562, nll_loss=3.06, mask_ins=0.984, word_ins_ml=4.578, word_reposition=0.33, kpe=0.671, ppl=94.5, wps=5223.7, ups=0.36, wpb=14615.3, bsz=1024, num_updates=48000, lr=0.000161374, gnorm=1.583, clip=0, loss_scale=2048, train_wall=248, wall=138274
2022-08-07 19:04:32 | INFO | train_inner | epoch 013:   3589 / 3715 loss=6.568, nll_loss=3.056, mask_ins=0.988, word_ins_ml=4.575, word_reposition=0.328, kpe=0.677, ppl=94.85, wps=5257.5, ups=0.36, wpb=14720.4, bsz=1024, num_updates=48100, lr=0.000161206, gnorm=1.588, clip=0, loss_scale=2048, train_wall=249, wall=138554
2022-08-07 19:09:11 | INFO | train_inner | epoch 013:   3689 / 3715 loss=6.555, nll_loss=3.05, mask_ins=0.982, word_ins_ml=4.569, word_reposition=0.329, kpe=0.675, ppl=94, wps=5271.3, ups=0.36, wpb=14685.6, bsz=1024, num_updates=48200, lr=0.000161039, gnorm=1.587, clip=0, loss_scale=2048, train_wall=247, wall=138833
2022-08-07 19:10:21 | INFO | train | epoch 013 | loss 6.541 | nll_loss 3.045 | mask_ins 0.985 | word_ins_ml 4.565 | word_reposition 0.325 | kpe 0.667 | ppl 93.12 | wps 5116.4 | ups 0.35 | wpb 14662.2 | bsz 1023.7 | num_updates 48226 | lr 0.000160996 | gnorm 1.763 | clip 0.2 | loss_scale 1249 | train_wall 9220 | wall 138904
2022-08-07 19:14:02 | INFO | valid | epoch 013 | valid on 'valid' subset | loss nan | nll_loss 3.181 | mask_ins 1.004 | word_ins_ml 4.732 | word_reposition 0.342 | kpe nan | ppl nan | wps 12388.5 | wpb 1849.4 | bsz 127.9 | num_updates 48226 | best_loss nan
2022-08-07 19:14:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 13 @ 48226 updates, score nan) (writing took 41.08770324103534 seconds)
2022-08-07 19:18:09 | INFO | train_inner | epoch 014:     74 / 3715 loss=6.494, nll_loss=3.015, mask_ins=0.982, word_ins_ml=4.539, word_reposition=0.322, kpe=0.651, ppl=90.16, wps=2702.3, ups=0.19, wpb=14549, bsz=1014.7, num_updates=48300, lr=0.000160872, gnorm=1.638, clip=0, loss_scale=3543, train_wall=245, wall=139371
2022-08-07 19:22:49 | INFO | train_inner | epoch 014:    174 / 3715 loss=6.478, nll_loss=3.005, mask_ins=0.979, word_ins_ml=4.529, word_reposition=0.324, kpe=0.644, ppl=89.12, wps=5262.7, ups=0.36, wpb=14710.2, bsz=1024, num_updates=48400, lr=0.000160706, gnorm=1.607, clip=0, loss_scale=4096, train_wall=249, wall=139651
2022-08-07 19:27:28 | INFO | train_inner | epoch 014:    274 / 3715 loss=6.456, nll_loss=2.99, mask_ins=0.977, word_ins_ml=4.516, word_reposition=0.324, kpe=0.639, ppl=87.81, wps=5242.8, ups=0.36, wpb=14638.1, bsz=1024, num_updates=48500, lr=0.00016054, gnorm=1.619, clip=0, loss_scale=4096, train_wall=248, wall=139930
2022-08-07 19:32:07 | INFO | train_inner | epoch 014:    374 / 3715 loss=6.461, nll_loss=2.994, mask_ins=0.977, word_ins_ml=4.52, word_reposition=0.321, kpe=0.643, ppl=88.1, wps=5237.3, ups=0.36, wpb=14637.9, bsz=1024, num_updates=48600, lr=0.000160375, gnorm=1.597, clip=0, loss_scale=4096, train_wall=248, wall=140210
2022-08-07 19:36:47 | INFO | train_inner | epoch 014:    474 / 3715 loss=6.46, nll_loss=2.986, mask_ins=0.977, word_ins_ml=4.513, word_reposition=0.325, kpe=0.646, ppl=88.05, wps=5230.3, ups=0.36, wpb=14610, bsz=1024, num_updates=48700, lr=0.00016021, gnorm=1.62, clip=0, loss_scale=4096, train_wall=248, wall=140489
2022-08-07 19:41:26 | INFO | train_inner | epoch 014:    574 / 3715 loss=6.461, nll_loss=2.992, mask_ins=0.977, word_ins_ml=4.519, word_reposition=0.32, kpe=0.646, ppl=88.12, wps=5231, ups=0.36, wpb=14621.9, bsz=1024, num_updates=48800, lr=0.000160046, gnorm=1.624, clip=0, loss_scale=6595, train_wall=248, wall=140769
2022-08-07 19:41:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-07 19:44:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 19:46:11 | INFO | train_inner | epoch 014:    676 / 3715 loss=6.479, nll_loss=3.001, mask_ins=0.981, word_ins_ml=4.527, word_reposition=0.323, kpe=0.647, ppl=89.19, wps=5117, ups=0.35, wpb=14591.2, bsz=1024, num_updates=48900, lr=0.000159882, gnorm=1.653, clip=0, loss_scale=3514, train_wall=253, wall=141054
2022-08-07 19:50:51 | INFO | train_inner | epoch 014:    776 / 3715 loss=6.449, nll_loss=2.98, mask_ins=0.975, word_ins_ml=4.508, word_reposition=0.319, kpe=0.647, ppl=87.38, wps=5220.3, ups=0.36, wpb=14597.8, bsz=1024, num_updates=49000, lr=0.000159719, gnorm=1.626, clip=0, loss_scale=2048, train_wall=248, wall=141333
2022-08-07 19:55:32 | INFO | train_inner | epoch 014:    876 / 3715 loss=6.493, nll_loss=3.02, mask_ins=0.984, word_ins_ml=4.542, word_reposition=0.318, kpe=0.649, ppl=90.07, wps=5230, ups=0.36, wpb=14683.5, bsz=1024, num_updates=49100, lr=0.000159556, gnorm=1.624, clip=0, loss_scale=2048, train_wall=249, wall=141614
2022-08-07 20:00:13 | INFO | train_inner | epoch 014:    976 / 3715 loss=6.49, nll_loss=3.016, mask_ins=0.977, word_ins_ml=4.539, word_reposition=0.323, kpe=0.651, ppl=89.91, wps=5237.6, ups=0.36, wpb=14721.8, bsz=1024, num_updates=49200, lr=0.000159394, gnorm=1.613, clip=0, loss_scale=2048, train_wall=250, wall=141895
2022-08-07 20:04:54 | INFO | train_inner | epoch 014:   1076 / 3715 loss=6.469, nll_loss=2.995, mask_ins=0.975, word_ins_ml=4.521, word_reposition=0.324, kpe=0.65, ppl=88.62, wps=5221.7, ups=0.36, wpb=14656.9, bsz=1024, num_updates=49300, lr=0.000159232, gnorm=1.624, clip=0, loss_scale=2048, train_wall=250, wall=142176
2022-08-07 20:09:34 | INFO | train_inner | epoch 014:   1176 / 3715 loss=6.493, nll_loss=3.011, mask_ins=0.979, word_ins_ml=4.535, word_reposition=0.325, kpe=0.653, ppl=90.04, wps=5249.4, ups=0.36, wpb=14742.2, bsz=1024, num_updates=49400, lr=0.000159071, gnorm=1.622, clip=0, loss_scale=2396, train_wall=249, wall=142457
2022-08-07 20:13:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-07 20:14:15 | INFO | train_inner | epoch 014:   1277 / 3715 loss=6.458, nll_loss=2.988, mask_ins=0.975, word_ins_ml=4.515, word_reposition=0.317, kpe=0.651, ppl=87.9, wps=5235.2, ups=0.36, wpb=14717.6, bsz=1024, num_updates=49500, lr=0.00015891, gnorm=1.634, clip=0, loss_scale=3609, train_wall=250, wall=142738
2022-08-07 20:18:55 | INFO | train_inner | epoch 014:   1377 / 3715 loss=6.486, nll_loss=3.008, mask_ins=0.984, word_ins_ml=4.532, word_reposition=0.322, kpe=0.648, ppl=89.66, wps=5208.1, ups=0.36, wpb=14555.3, bsz=1024, num_updates=49600, lr=0.00015875, gnorm=1.62, clip=0, loss_scale=2048, train_wall=248, wall=143017
2022-08-07 20:23:35 | INFO | train_inner | epoch 014:   1477 / 3715 loss=6.495, nll_loss=3.016, mask_ins=0.981, word_ins_ml=4.539, word_reposition=0.32, kpe=0.655, ppl=90.21, wps=5276.7, ups=0.36, wpb=14769.5, bsz=1024, num_updates=49700, lr=0.00015859, gnorm=1.611, clip=0, loss_scale=2048, train_wall=249, wall=143297
2022-08-07 20:27:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-07 20:28:17 | INFO | train_inner | epoch 014:   1578 / 3715 loss=6.481, nll_loss=3.008, mask_ins=0.981, word_ins_ml=4.532, word_reposition=0.317, kpe=0.651, ppl=89.3, wps=5180.8, ups=0.35, wpb=14633.3, bsz=1024, num_updates=49800, lr=0.000158431, gnorm=1.612, clip=0, loss_scale=1906, train_wall=251, wall=143580
2022-08-07 20:32:58 | INFO | train_inner | epoch 014:   1678 / 3715 loss=6.486, nll_loss=3.02, mask_ins=0.976, word_ins_ml=4.543, word_reposition=0.315, kpe=0.653, ppl=89.65, wps=5208.9, ups=0.36, wpb=14601.3, bsz=1024, num_updates=49900, lr=0.000158272, gnorm=1.629, clip=0, loss_scale=1024, train_wall=249, wall=143860
2022-08-07 20:37:37 | INFO | train_inner | epoch 014:   1778 / 3715 loss=6.483, nll_loss=3.005, mask_ins=0.977, word_ins_ml=4.529, word_reposition=0.317, kpe=0.66, ppl=89.47, wps=5293.3, ups=0.36, wpb=14810.6, bsz=1024, num_updates=50000, lr=0.000158114, gnorm=1.613, clip=0, loss_scale=1024, train_wall=248, wall=144140
2022-08-07 20:42:19 | INFO | train_inner | epoch 014:   1878 / 3715 loss=6.489, nll_loss=3.011, mask_ins=0.977, word_ins_ml=4.535, word_reposition=0.321, kpe=0.656, ppl=89.85, wps=5228, ups=0.36, wpb=14703.8, bsz=1024, num_updates=50100, lr=0.000157956, gnorm=1.621, clip=0, loss_scale=1024, train_wall=250, wall=144421
2022-08-07 20:46:58 | INFO | train_inner | epoch 014:   1978 / 3715 loss=6.492, nll_loss=3.012, mask_ins=0.981, word_ins_ml=4.536, word_reposition=0.321, kpe=0.654, ppl=89.98, wps=5245.3, ups=0.36, wpb=14647.7, bsz=1024, num_updates=50200, lr=0.000157799, gnorm=1.608, clip=0, loss_scale=1024, train_wall=248, wall=144700
2022-08-07 20:51:37 | INFO | train_inner | epoch 014:   2078 / 3715 loss=6.508, nll_loss=3.031, mask_ins=0.984, word_ins_ml=4.552, word_reposition=0.318, kpe=0.654, ppl=91.01, wps=5243.1, ups=0.36, wpb=14620.1, bsz=1024, num_updates=50300, lr=0.000157642, gnorm=1.621, clip=0, loss_scale=1044, train_wall=248, wall=144979
2022-08-07 20:56:16 | INFO | train_inner | epoch 014:   2178 / 3715 loss=6.482, nll_loss=3.006, mask_ins=0.98, word_ins_ml=4.53, word_reposition=0.318, kpe=0.654, ppl=89.4, wps=5286.3, ups=0.36, wpb=14770.5, bsz=1024, num_updates=50400, lr=0.000157485, gnorm=1.611, clip=0, loss_scale=2048, train_wall=248, wall=145259
2022-08-07 21:00:56 | INFO | train_inner | epoch 014:   2278 / 3715 loss=6.486, nll_loss=3.015, mask_ins=0.976, word_ins_ml=4.537, word_reposition=0.319, kpe=0.653, ppl=89.61, wps=5227.3, ups=0.36, wpb=14606.7, bsz=1024, num_updates=50500, lr=0.000157329, gnorm=1.611, clip=0, loss_scale=2048, train_wall=248, wall=145538
2022-08-07 21:07:03 | INFO | train_inner | epoch 014:   2378 / 3715 loss=6.502, nll_loss=3.022, mask_ins=0.977, word_ins_ml=4.545, word_reposition=0.325, kpe=0.656, ppl=90.62, wps=3992, ups=0.27, wpb=14664.9, bsz=1024, num_updates=50600, lr=0.000157174, gnorm=1.611, clip=0, loss_scale=2048, train_wall=336, wall=145905
2022-08-07 21:10:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-07 21:13:21 | INFO | train_inner | epoch 014:   2479 / 3715 loss=6.502, nll_loss=3.013, mask_ins=0.976, word_ins_ml=4.536, word_reposition=0.332, kpe=0.658, ppl=90.61, wps=3886.6, ups=0.26, wpb=14702.5, bsz=1024, num_updates=50700, lr=0.000157019, gnorm=1.756, clip=0, loss_scale=1632, train_wall=346, wall=146284
2022-08-07 21:18:01 | INFO | train_inner | epoch 014:   2579 / 3715 loss=6.492, nll_loss=3.019, mask_ins=0.982, word_ins_ml=4.541, word_reposition=0.315, kpe=0.654, ppl=89.98, wps=5174.7, ups=0.36, wpb=14498.8, bsz=1024, num_updates=50800, lr=0.000156864, gnorm=1.615, clip=0, loss_scale=1024, train_wall=249, wall=146564
2022-08-07 21:22:40 | INFO | train_inner | epoch 014:   2679 / 3715 loss=6.472, nll_loss=2.998, mask_ins=0.973, word_ins_ml=4.523, word_reposition=0.321, kpe=0.655, ppl=88.77, wps=5226.9, ups=0.36, wpb=14559.5, bsz=1024, num_updates=50900, lr=0.00015671, gnorm=1.626, clip=0, loss_scale=1024, train_wall=247, wall=146842
2022-08-07 21:27:20 | INFO | train_inner | epoch 014:   2779 / 3715 loss=6.509, nll_loss=3.032, mask_ins=0.983, word_ins_ml=4.553, word_reposition=0.318, kpe=0.656, ppl=91.06, wps=5237, ups=0.36, wpb=14671.4, bsz=1024, num_updates=51000, lr=0.000156556, gnorm=1.623, clip=0, loss_scale=1024, train_wall=249, wall=147123
2022-08-07 21:32:02 | INFO | train_inner | epoch 014:   2879 / 3715 loss=6.506, nll_loss=3.022, mask_ins=0.977, word_ins_ml=4.544, word_reposition=0.324, kpe=0.661, ppl=90.88, wps=5201.5, ups=0.36, wpb=14635.4, bsz=1024, num_updates=51100, lr=0.000156403, gnorm=1.616, clip=0, loss_scale=1024, train_wall=250, wall=147404
2022-08-07 21:36:41 | INFO | train_inner | epoch 014:   2979 / 3715 loss=6.512, nll_loss=3.03, mask_ins=0.983, word_ins_ml=4.551, word_reposition=0.319, kpe=0.659, ppl=91.25, wps=5252.9, ups=0.36, wpb=14679.6, bsz=1023.8, num_updates=51200, lr=0.00015625, gnorm=1.608, clip=0, loss_scale=1321, train_wall=248, wall=147683
2022-08-07 21:41:20 | INFO | train_inner | epoch 014:   3079 / 3715 loss=6.509, nll_loss=3.023, mask_ins=0.98, word_ins_ml=4.545, word_reposition=0.323, kpe=0.661, ppl=91.1, wps=5277.2, ups=0.36, wpb=14697.9, bsz=1024, num_updates=51300, lr=0.000156098, gnorm=1.607, clip=0, loss_scale=2048, train_wall=247, wall=147962
2022-08-07 21:45:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-07 21:45:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-07 21:46:03 | INFO | train_inner | epoch 014:   3181 / 3715 loss=6.494, nll_loss=3.012, mask_ins=0.975, word_ins_ml=4.535, word_reposition=0.324, kpe=0.659, ppl=90.13, wps=5167.2, ups=0.35, wpb=14627.2, bsz=1024, num_updates=51400, lr=0.000155946, gnorm=1.619, clip=0, loss_scale=1787, train_wall=251, wall=148245
2022-08-07 21:50:41 | INFO | train_inner | epoch 014:   3281 / 3715 loss=6.525, nll_loss=3.041, mask_ins=0.981, word_ins_ml=4.561, word_reposition=0.323, kpe=0.66, ppl=92.09, wps=5261, ups=0.36, wpb=14671.8, bsz=1024, num_updates=51500, lr=0.000155794, gnorm=1.608, clip=0, loss_scale=512, train_wall=248, wall=148524
2022-08-07 21:55:21 | INFO | train_inner | epoch 014:   3381 / 3715 loss=6.486, nll_loss=3.003, mask_ins=0.979, word_ins_ml=4.527, word_reposition=0.323, kpe=0.658, ppl=89.66, wps=5236.5, ups=0.36, wpb=14639.1, bsz=1024, num_updates=51600, lr=0.000155643, gnorm=1.636, clip=0, loss_scale=512, train_wall=248, wall=148803
2022-08-07 22:00:01 | INFO | train_inner | epoch 014:   3481 / 3715 loss=6.509, nll_loss=3.02, mask_ins=0.981, word_ins_ml=4.542, word_reposition=0.322, kpe=0.664, ppl=91.05, wps=5255.6, ups=0.36, wpb=14735.9, bsz=1024, num_updates=51700, lr=0.000155493, gnorm=1.596, clip=0, loss_scale=512, train_wall=249, wall=149084
2022-08-07 22:04:41 | INFO | train_inner | epoch 014:   3581 / 3715 loss=6.502, nll_loss=3.018, mask_ins=0.978, word_ins_ml=4.54, word_reposition=0.32, kpe=0.664, ppl=90.63, wps=5249.6, ups=0.36, wpb=14696.4, bsz=1024, num_updates=51800, lr=0.000155342, gnorm=1.645, clip=0, loss_scale=512, train_wall=249, wall=149364
2022-08-07 22:09:22 | INFO | train_inner | epoch 014:   3681 / 3715 loss=6.504, nll_loss=3.019, mask_ins=0.979, word_ins_ml=4.542, word_reposition=0.318, kpe=0.665, ppl=90.77, wps=5273.5, ups=0.36, wpb=14789.1, bsz=1024, num_updates=51900, lr=0.000155193, gnorm=1.822, clip=0, loss_scale=532, train_wall=249, wall=149644
2022-08-07 22:10:54 | INFO | train | epoch 014 | loss 6.488 | nll_loss 3.011 | mask_ins 0.979 | word_ins_ml 4.534 | word_reposition 0.321 | kpe 0.654 | ppl 89.74 | wps 5018.6 | ups 0.34 | wpb 14662.4 | bsz 1023.7 | num_updates 51934 | lr 0.000155142 | gnorm 1.629 | clip 0 | loss_scale 2018 | train_wall 9407 | wall 149737
2022-08-07 22:14:35 | INFO | valid | epoch 014 | valid on 'valid' subset | loss nan | nll_loss 3.182 | mask_ins 1.004 | word_ins_ml 4.736 | word_reposition 0.342 | kpe nan | ppl nan | wps 12410.9 | wpb 1849.4 | bsz 127.9 | num_updates 51934 | best_loss nan
2022-08-07 22:14:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 14 @ 51934 updates, score nan) (writing took 14.997043542563915 seconds)
2022-08-07 22:17:55 | INFO | train_inner | epoch 015:     66 / 3715 loss=6.444, nll_loss=2.98, mask_ins=0.977, word_ins_ml=4.507, word_reposition=0.318, kpe=0.642, ppl=87.09, wps=2846.4, ups=0.19, wpb=14598, bsz=1014.7, num_updates=52000, lr=0.000155043, gnorm=1.67, clip=0, loss_scale=1024, train_wall=246, wall=150157
2022-08-07 22:22:34 | INFO | train_inner | epoch 015:    166 / 3715 loss=6.415, nll_loss=2.961, mask_ins=0.972, word_ins_ml=4.49, word_reposition=0.317, kpe=0.636, ppl=85.34, wps=5282.9, ups=0.36, wpb=14778.3, bsz=1024, num_updates=52100, lr=0.000154895, gnorm=1.711, clip=0, loss_scale=1024, train_wall=248, wall=150437
2022-08-07 22:27:15 | INFO | train_inner | epoch 015:    266 / 3715 loss=6.415, nll_loss=2.968, mask_ins=0.975, word_ins_ml=4.497, word_reposition=0.314, kpe=0.63, ppl=85.35, wps=5209, ups=0.36, wpb=14608.2, bsz=1024, num_updates=52200, lr=0.000154746, gnorm=1.643, clip=0, loss_scale=1024, train_wall=249, wall=150717
2022-08-07 22:31:54 | INFO | train_inner | epoch 015:    366 / 3715 loss=6.437, nll_loss=2.982, mask_ins=0.972, word_ins_ml=4.508, word_reposition=0.325, kpe=0.632, ppl=86.66, wps=5266.4, ups=0.36, wpb=14689.1, bsz=1024, num_updates=52300, lr=0.000154598, gnorm=1.657, clip=0, loss_scale=1024, train_wall=248, wall=150996
2022-08-07 22:36:34 | INFO | train_inner | epoch 015:    466 / 3715 loss=6.393, nll_loss=2.941, mask_ins=0.967, word_ins_ml=4.473, word_reposition=0.321, kpe=0.633, ppl=84.04, wps=5263.4, ups=0.36, wpb=14754.8, bsz=1024, num_updates=52400, lr=0.000154451, gnorm=1.656, clip=0, loss_scale=1024, train_wall=249, wall=151277
2022-08-07 22:41:14 | INFO | train_inner | epoch 015:    566 / 3715 loss=6.395, nll_loss=2.95, mask_ins=0.971, word_ins_ml=4.481, word_reposition=0.313, kpe=0.631, ppl=84.14, wps=5210.7, ups=0.36, wpb=14582.9, bsz=1024, num_updates=52500, lr=0.000154303, gnorm=1.656, clip=0, loss_scale=1966, train_wall=249, wall=151556
2022-08-07 22:45:53 | INFO | train_inner | epoch 015:    666 / 3715 loss=6.401, nll_loss=2.953, mask_ins=0.972, word_ins_ml=4.483, word_reposition=0.315, kpe=0.632, ppl=84.51, wps=5247.5, ups=0.36, wpb=14650.7, bsz=1024, num_updates=52600, lr=0.000154157, gnorm=1.65, clip=0, loss_scale=2048, train_wall=248, wall=151836
2022-08-07 22:50:34 | INFO | train_inner | epoch 015:    766 / 3715 loss=6.416, nll_loss=2.96, mask_ins=0.965, word_ins_ml=4.489, word_reposition=0.324, kpe=0.637, ppl=85.36, wps=5276.5, ups=0.36, wpb=14788.9, bsz=1024, num_updates=52700, lr=0.00015401, gnorm=1.654, clip=0, loss_scale=2048, train_wall=249, wall=152116
2022-08-07 22:52:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-07 22:54:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-07 22:55:18 | INFO | train_inner | epoch 015:    868 / 3715 loss=6.412, nll_loss=2.952, mask_ins=0.974, word_ins_ml=4.482, word_reposition=0.318, kpe=0.638, ppl=85.15, wps=5156.4, ups=0.35, wpb=14646.4, bsz=1024, num_updates=52800, lr=0.000153864, gnorm=1.683, clip=0, loss_scale=1426, train_wall=252, wall=152400
2022-08-07 22:59:56 | INFO | train_inner | epoch 015:    968 / 3715 loss=6.428, nll_loss=2.973, mask_ins=0.974, word_ins_ml=4.501, word_reposition=0.318, kpe=0.635, ppl=86.11, wps=5273.8, ups=0.36, wpb=14710.4, bsz=1024, num_updates=52900, lr=0.000153719, gnorm=1.673, clip=0, loss_scale=512, train_wall=248, wall=152679
2022-08-07 23:04:36 | INFO | train_inner | epoch 015:   1068 / 3715 loss=6.429, nll_loss=2.978, mask_ins=0.972, word_ins_ml=4.506, word_reposition=0.318, kpe=0.633, ppl=86.15, wps=5196.6, ups=0.36, wpb=14502.8, bsz=1024, num_updates=53000, lr=0.000153574, gnorm=1.671, clip=0, loss_scale=512, train_wall=248, wall=152958
2022-08-07 23:06:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-07 23:09:18 | INFO | train_inner | epoch 015:   1169 / 3715 loss=6.416, nll_loss=2.959, mask_ins=0.97, word_ins_ml=4.488, word_reposition=0.32, kpe=0.639, ppl=85.41, wps=5212.3, ups=0.35, wpb=14713.8, bsz=1023.8, num_updates=53100, lr=0.000153429, gnorm=1.693, clip=0, loss_scale=332, train_wall=251, wall=153240
2022-08-07 23:13:58 | INFO | train_inner | epoch 015:   1269 / 3715 loss=6.43, nll_loss=2.973, mask_ins=0.973, word_ins_ml=4.501, word_reposition=0.318, kpe=0.638, ppl=86.22, wps=5261.7, ups=0.36, wpb=14722.5, bsz=1024, num_updates=53200, lr=0.000153285, gnorm=1.67, clip=0, loss_scale=256, train_wall=248, wall=153520
2022-08-07 23:18:36 | INFO | train_inner | epoch 015:   1369 / 3715 loss=6.43, nll_loss=2.968, mask_ins=0.972, word_ins_ml=4.497, word_reposition=0.319, kpe=0.643, ppl=86.24, wps=5301, ups=0.36, wpb=14765.9, bsz=1024, num_updates=53300, lr=0.000153141, gnorm=1.644, clip=0, loss_scale=256, train_wall=247, wall=153799
2022-08-07 23:19:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-07 23:19:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-08-07 23:23:21 | INFO | train_inner | epoch 015:   1471 / 3715 loss=6.454, nll_loss=2.991, mask_ins=0.979, word_ins_ml=4.517, word_reposition=0.318, kpe=0.64, ppl=87.65, wps=5114.3, ups=0.35, wpb=14587.8, bsz=1024, num_updates=53400, lr=0.000152998, gnorm=1.981, clip=0, loss_scale=84, train_wall=253, wall=154084
2022-08-07 23:28:01 | INFO | train_inner | epoch 015:   1571 / 3715 loss=6.431, nll_loss=2.976, mask_ins=0.972, word_ins_ml=4.503, word_reposition=0.316, kpe=0.64, ppl=86.29, wps=5227.8, ups=0.36, wpb=14605.4, bsz=1024, num_updates=53500, lr=0.000152854, gnorm=1.666, clip=0, loss_scale=64, train_wall=248, wall=154363
2022-08-07 23:32:41 | INFO | train_inner | epoch 015:   1671 / 3715 loss=6.433, nll_loss=2.972, mask_ins=0.97, word_ins_ml=4.5, word_reposition=0.32, kpe=0.643, ppl=86.4, wps=5262.3, ups=0.36, wpb=14722.5, bsz=1024, num_updates=53600, lr=0.000152712, gnorm=1.643, clip=0, loss_scale=64, train_wall=248, wall=154643
2022-08-07 23:37:21 | INFO | train_inner | epoch 015:   1771 / 3715 loss=6.42, nll_loss=2.963, mask_ins=0.971, word_ins_ml=4.492, word_reposition=0.317, kpe=0.64, ppl=85.62, wps=5231.1, ups=0.36, wpb=14641.5, bsz=1024, num_updates=53700, lr=0.00015257, gnorm=1.812, clip=0, loss_scale=64, train_wall=248, wall=154923
2022-08-07 23:42:00 | INFO | train_inner | epoch 015:   1871 / 3715 loss=6.449, nll_loss=2.98, mask_ins=0.976, word_ins_ml=4.507, word_reposition=0.325, kpe=0.641, ppl=87.37, wps=5238.1, ups=0.36, wpb=14623.7, bsz=1024, num_updates=53800, lr=0.000152428, gnorm=1.636, clip=0, loss_scale=64, train_wall=248, wall=155202
2022-08-07 23:46:39 | INFO | train_inner | epoch 015:   1971 / 3715 loss=6.456, nll_loss=3.001, mask_ins=0.976, word_ins_ml=4.525, word_reposition=0.314, kpe=0.641, ppl=87.8, wps=5244.4, ups=0.36, wpb=14635.8, bsz=1024, num_updates=53900, lr=0.000152286, gnorm=1.65, clip=0, loss_scale=114, train_wall=248, wall=155481
2022-08-07 23:51:18 | INFO | train_inner | epoch 015:   2071 / 3715 loss=6.442, nll_loss=2.984, mask_ins=0.968, word_ins_ml=4.511, word_reposition=0.32, kpe=0.642, ppl=86.92, wps=5252.1, ups=0.36, wpb=14651.1, bsz=1024, num_updates=54000, lr=0.000152145, gnorm=1.66, clip=0, loss_scale=128, train_wall=248, wall=155760
2022-08-07 23:55:57 | INFO | train_inner | epoch 015:   2171 / 3715 loss=6.457, nll_loss=2.993, mask_ins=0.977, word_ins_ml=4.518, word_reposition=0.319, kpe=0.643, ppl=87.86, wps=5238.8, ups=0.36, wpb=14630.9, bsz=1024, num_updates=54100, lr=0.000152004, gnorm=1.731, clip=0, loss_scale=128, train_wall=248, wall=156039
2022-08-08 00:00:37 | INFO | train_inner | epoch 015:   2271 / 3715 loss=6.433, nll_loss=2.974, mask_ins=0.971, word_ins_ml=4.501, word_reposition=0.318, kpe=0.642, ppl=86.38, wps=5231, ups=0.36, wpb=14627.3, bsz=1024, num_updates=54200, lr=0.000151864, gnorm=1.685, clip=0, loss_scale=128, train_wall=248, wall=156319
2022-08-08 00:05:16 | INFO | train_inner | epoch 015:   2371 / 3715 loss=6.429, nll_loss=2.964, mask_ins=0.97, word_ins_ml=4.493, word_reposition=0.319, kpe=0.647, ppl=86.14, wps=5279.2, ups=0.36, wpb=14743.4, bsz=1024, num_updates=54300, lr=0.000151724, gnorm=1.636, clip=0, loss_scale=128, train_wall=249, wall=156598
2022-08-08 00:09:56 | INFO | train_inner | epoch 015:   2471 / 3715 loss=6.453, nll_loss=2.994, mask_ins=0.971, word_ins_ml=4.519, word_reposition=0.316, kpe=0.647, ppl=87.62, wps=5266.6, ups=0.36, wpb=14763.5, bsz=1024, num_updates=54400, lr=0.000151585, gnorm=1.655, clip=0, loss_scale=212, train_wall=249, wall=156879
2022-08-08 00:14:35 | INFO | train_inner | epoch 015:   2571 / 3715 loss=6.448, nll_loss=2.99, mask_ins=0.975, word_ins_ml=4.515, word_reposition=0.314, kpe=0.644, ppl=87.32, wps=5246.4, ups=0.36, wpb=14623.5, bsz=1024, num_updates=54500, lr=0.000151446, gnorm=1.644, clip=0, loss_scale=256, train_wall=247, wall=157157
2022-08-08 00:18:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-08 00:19:18 | INFO | train_inner | epoch 015:   2672 / 3715 loss=6.427, nll_loss=2.977, mask_ins=0.97, word_ins_ml=4.504, word_reposition=0.313, kpe=0.639, ppl=86.02, wps=5134.1, ups=0.35, wpb=14526.6, bsz=1024, num_updates=54600, lr=0.000151307, gnorm=1.704, clip=0, loss_scale=237, train_wall=251, wall=157440
2022-08-08 00:23:58 | INFO | train_inner | epoch 015:   2772 / 3715 loss=6.454, nll_loss=2.998, mask_ins=0.973, word_ins_ml=4.522, word_reposition=0.317, kpe=0.642, ppl=87.69, wps=5209.5, ups=0.36, wpb=14598.8, bsz=1024, num_updates=54700, lr=0.000151169, gnorm=1.653, clip=0, loss_scale=128, train_wall=249, wall=157721
2022-08-08 00:28:38 | INFO | train_inner | epoch 015:   2872 / 3715 loss=6.453, nll_loss=2.984, mask_ins=0.972, word_ins_ml=4.51, word_reposition=0.322, kpe=0.647, ppl=87.59, wps=5270.3, ups=0.36, wpb=14755.2, bsz=1024, num_updates=54800, lr=0.000151031, gnorm=1.669, clip=0, loss_scale=128, train_wall=248, wall=158001
2022-08-08 00:33:18 | INFO | train_inner | epoch 015:   2972 / 3715 loss=6.446, nll_loss=2.979, mask_ins=0.975, word_ins_ml=4.506, word_reposition=0.317, kpe=0.648, ppl=87.21, wps=5266.7, ups=0.36, wpb=14729.2, bsz=1024, num_updates=54900, lr=0.000150893, gnorm=1.652, clip=0, loss_scale=128, train_wall=248, wall=158280
2022-08-08 00:37:56 | INFO | train_inner | epoch 015:   3072 / 3715 loss=6.453, nll_loss=2.997, mask_ins=0.968, word_ins_ml=4.521, word_reposition=0.318, kpe=0.645, ppl=87.58, wps=5249.4, ups=0.36, wpb=14624.7, bsz=1024, num_updates=55000, lr=0.000150756, gnorm=1.63, clip=0, loss_scale=128, train_wall=247, wall=158559
2022-08-08 00:42:35 | INFO | train_inner | epoch 015:   3172 / 3715 loss=6.439, nll_loss=2.973, mask_ins=0.97, word_ins_ml=4.501, word_reposition=0.324, kpe=0.643, ppl=86.74, wps=5234, ups=0.36, wpb=14606, bsz=1024, num_updates=55100, lr=0.000150619, gnorm=1.629, clip=0, loss_scale=132, train_wall=248, wall=158838
2022-08-08 00:47:15 | INFO | train_inner | epoch 015:   3272 / 3715 loss=6.455, nll_loss=2.987, mask_ins=0.97, word_ins_ml=4.513, word_reposition=0.324, kpe=0.648, ppl=87.71, wps=5238.1, ups=0.36, wpb=14662.7, bsz=1024, num_updates=55200, lr=0.000150482, gnorm=1.639, clip=0, loss_scale=256, train_wall=249, wall=159118
2022-08-08 00:51:55 | INFO | train_inner | epoch 015:   3372 / 3715 loss=6.428, nll_loss=2.973, mask_ins=0.97, word_ins_ml=4.5, word_reposition=0.314, kpe=0.643, ppl=86.11, wps=5232.9, ups=0.36, wpb=14619.3, bsz=1024, num_updates=55300, lr=0.000150346, gnorm=1.669, clip=0, loss_scale=256, train_wall=248, wall=159397
2022-08-08 00:56:35 | INFO | train_inner | epoch 015:   3472 / 3715 loss=6.445, nll_loss=2.982, mask_ins=0.972, word_ins_ml=4.508, word_reposition=0.318, kpe=0.647, ppl=87.1, wps=5231.4, ups=0.36, wpb=14653.4, bsz=1024, num_updates=55400, lr=0.00015021, gnorm=1.632, clip=0, loss_scale=256, train_wall=249, wall=159677
2022-08-08 01:01:15 | INFO | train_inner | epoch 015:   3572 / 3715 loss=6.447, nll_loss=2.98, mask_ins=0.973, word_ins_ml=4.506, word_reposition=0.322, kpe=0.646, ppl=87.27, wps=5234.8, ups=0.36, wpb=14651, bsz=1024, num_updates=55500, lr=0.000150075, gnorm=1.637, clip=0, loss_scale=256, train_wall=248, wall=159957
2022-08-08 01:05:54 | INFO | train_inner | epoch 015:   3672 / 3715 loss=6.461, nll_loss=2.997, mask_ins=0.975, word_ins_ml=4.521, word_reposition=0.316, kpe=0.648, ppl=88.07, wps=5237.4, ups=0.36, wpb=14648.9, bsz=1024, num_updates=55600, lr=0.00014994, gnorm=1.656, clip=0, loss_scale=256, train_wall=248, wall=160237
2022-08-08 01:07:52 | INFO | train | epoch 015 | loss 6.434 | nll_loss 2.976 | mask_ins 0.972 | word_ins_ml 4.503 | word_reposition 0.318 | kpe 0.641 | ppl 86.48 | wps 5121.9 | ups 0.35 | wpb 14662 | bsz 1023.7 | num_updates 55643 | lr 0.000149882 | gnorm 1.682 | clip 0 | loss_scale 483 | train_wall 9219 | wall 160354
2022-08-08 01:11:33 | INFO | valid | epoch 015 | valid on 'valid' subset | loss nan | nll_loss 3.151 | mask_ins 0.998 | word_ins_ml 4.705 | word_reposition 0.347 | kpe nan | ppl nan | wps 12396.2 | wpb 1849.4 | bsz 127.9 | num_updates 55643 | best_loss nan
2022-08-08 01:11:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 15 @ 55643 updates, score nan) (writing took 9.376195827499032 seconds)
2022-08-08 01:14:21 | INFO | train_inner | epoch 016:     57 / 3715 loss=6.426, nll_loss=2.971, mask_ins=0.975, word_ins_ml=4.499, word_reposition=0.316, kpe=0.637, ppl=85.97, wps=2907.5, ups=0.2, wpb=14722.9, bsz=1014.7, num_updates=55700, lr=0.000149805, gnorm=2.072, clip=0, loss_scale=489, train_wall=245, wall=160743
2022-08-08 01:19:01 | INFO | train_inner | epoch 016:    157 / 3715 loss=6.364, nll_loss=2.92, mask_ins=0.968, word_ins_ml=4.454, word_reposition=0.324, kpe=0.619, ppl=82.39, wps=5259.3, ups=0.36, wpb=14733.9, bsz=1024, num_updates=55800, lr=0.000149671, gnorm=1.669, clip=0, loss_scale=512, train_wall=248, wall=161023
2022-08-08 01:23:40 | INFO | train_inner | epoch 016:    257 / 3715 loss=6.33, nll_loss=2.906, mask_ins=0.96, word_ins_ml=4.442, word_reposition=0.313, kpe=0.616, ppl=80.47, wps=5247.4, ups=0.36, wpb=14639.9, bsz=1024, num_updates=55900, lr=0.000149537, gnorm=1.7, clip=0, loss_scale=512, train_wall=248, wall=161302
2022-08-08 01:28:19 | INFO | train_inner | epoch 016:    357 / 3715 loss=6.35, nll_loss=2.918, mask_ins=0.962, word_ins_ml=4.452, word_reposition=0.315, kpe=0.621, ppl=81.57, wps=5268.5, ups=0.36, wpb=14721.5, bsz=1024, num_updates=56000, lr=0.000149404, gnorm=1.665, clip=0, loss_scale=512, train_wall=248, wall=161582
2022-08-08 01:32:58 | INFO | train_inner | epoch 016:    457 / 3715 loss=6.353, nll_loss=2.926, mask_ins=0.965, word_ins_ml=4.459, word_reposition=0.31, kpe=0.619, ppl=81.74, wps=5263.4, ups=0.36, wpb=14672.1, bsz=1024, num_updates=56100, lr=0.00014927, gnorm=1.681, clip=0, loss_scale=512, train_wall=248, wall=161861
2022-08-08 01:37:37 | INFO | train_inner | epoch 016:    557 / 3715 loss=6.367, nll_loss=2.95, mask_ins=0.965, word_ins_ml=4.481, word_reposition=0.308, kpe=0.614, ppl=82.53, wps=5196.5, ups=0.36, wpb=14475.3, bsz=1024, num_updates=56200, lr=0.000149137, gnorm=1.676, clip=0, loss_scale=916, train_wall=247, wall=162139
2022-08-08 01:42:16 | INFO | train_inner | epoch 016:    657 / 3715 loss=6.377, nll_loss=2.947, mask_ins=0.967, word_ins_ml=4.478, word_reposition=0.312, kpe=0.621, ppl=83.13, wps=5227.3, ups=0.36, wpb=14619.4, bsz=1024, num_updates=56300, lr=0.000149005, gnorm=1.681, clip=0, loss_scale=1024, train_wall=249, wall=162419
2022-08-08 01:46:56 | INFO | train_inner | epoch 016:    757 / 3715 loss=6.378, nll_loss=2.938, mask_ins=0.966, word_ins_ml=4.47, word_reposition=0.318, kpe=0.624, ppl=83.17, wps=5307.1, ups=0.36, wpb=14828.6, bsz=1024, num_updates=56400, lr=0.000148873, gnorm=1.674, clip=0, loss_scale=1024, train_wall=248, wall=162698
2022-08-08 01:51:37 | INFO | train_inner | epoch 016:    857 / 3715 loss=6.344, nll_loss=2.916, mask_ins=0.959, word_ins_ml=4.45, word_reposition=0.315, kpe=0.62, ppl=81.25, wps=5197.7, ups=0.36, wpb=14634, bsz=1024, num_updates=56500, lr=0.000148741, gnorm=1.691, clip=0, loss_scale=1024, train_wall=250, wall=162980
2022-08-08 01:56:16 | INFO | train_inner | epoch 016:    957 / 3715 loss=6.356, nll_loss=2.923, mask_ins=0.964, word_ins_ml=4.456, word_reposition=0.312, kpe=0.623, ppl=81.9, wps=5241.4, ups=0.36, wpb=14625.1, bsz=1024, num_updates=56600, lr=0.00014861, gnorm=1.669, clip=0, loss_scale=1024, train_wall=248, wall=163259
2022-08-08 02:00:56 | INFO | train_inner | epoch 016:   1057 / 3715 loss=6.374, nll_loss=2.932, mask_ins=0.965, word_ins_ml=4.464, word_reposition=0.322, kpe=0.623, ppl=82.91, wps=5216.7, ups=0.36, wpb=14608.4, bsz=1024, num_updates=56700, lr=0.000148478, gnorm=1.689, clip=0, loss_scale=1710, train_wall=249, wall=163539
2022-08-08 02:05:36 | INFO | train_inner | epoch 016:   1157 / 3715 loss=6.392, nll_loss=2.95, mask_ins=0.969, word_ins_ml=4.481, word_reposition=0.316, kpe=0.626, ppl=83.99, wps=5269, ups=0.36, wpb=14737.1, bsz=1024, num_updates=56800, lr=0.000148348, gnorm=1.664, clip=0, loss_scale=2048, train_wall=248, wall=163819
2022-08-08 02:10:17 | INFO | train_inner | epoch 016:   1257 / 3715 loss=6.394, nll_loss=2.953, mask_ins=0.969, word_ins_ml=4.483, word_reposition=0.314, kpe=0.628, ppl=84.08, wps=5229.4, ups=0.36, wpb=14711.4, bsz=1024, num_updates=56900, lr=0.000148217, gnorm=1.679, clip=0, loss_scale=2048, train_wall=250, wall=164100
2022-08-08 02:14:57 | INFO | train_inner | epoch 016:   1357 / 3715 loss=6.376, nll_loss=2.938, mask_ins=0.967, word_ins_ml=4.469, word_reposition=0.316, kpe=0.625, ppl=83.05, wps=5237.5, ups=0.36, wpb=14664.9, bsz=1024, num_updates=57000, lr=0.000148087, gnorm=1.696, clip=0, loss_scale=2048, train_wall=249, wall=164380
2022-08-08 02:19:37 | INFO | train_inner | epoch 016:   1457 / 3715 loss=6.385, nll_loss=2.941, mask_ins=0.964, word_ins_ml=4.472, word_reposition=0.318, kpe=0.63, ppl=83.55, wps=5284, ups=0.36, wpb=14761.6, bsz=1024, num_updates=57100, lr=0.000147957, gnorm=1.666, clip=0, loss_scale=2048, train_wall=248, wall=164659
2022-08-08 02:24:17 | INFO | train_inner | epoch 016:   1557 / 3715 loss=6.364, nll_loss=2.92, mask_ins=0.966, word_ins_ml=4.453, word_reposition=0.316, kpe=0.629, ppl=82.35, wps=5232.6, ups=0.36, wpb=14644.2, bsz=1024, num_updates=57200, lr=0.000147828, gnorm=1.675, clip=0, loss_scale=3174, train_wall=248, wall=164939
2022-08-08 02:28:56 | INFO | train_inner | epoch 016:   1657 / 3715 loss=6.379, nll_loss=2.945, mask_ins=0.961, word_ins_ml=4.476, word_reposition=0.315, kpe=0.628, ppl=83.25, wps=5247.7, ups=0.36, wpb=14652.2, bsz=1024, num_updates=57300, lr=0.000147699, gnorm=1.666, clip=0, loss_scale=4096, train_wall=248, wall=165218
2022-08-08 02:33:35 | INFO | train_inner | epoch 016:   1757 / 3715 loss=6.38, nll_loss=2.94, mask_ins=0.968, word_ins_ml=4.471, word_reposition=0.312, kpe=0.628, ppl=83.27, wps=5249, ups=0.36, wpb=14655.2, bsz=1024, num_updates=57400, lr=0.00014757, gnorm=1.685, clip=0, loss_scale=4096, train_wall=248, wall=165498
2022-08-08 02:37:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 02:38:18 | INFO | train_inner | epoch 016:   1858 / 3715 loss=6.391, nll_loss=2.949, mask_ins=0.966, word_ins_ml=4.479, word_reposition=0.318, kpe=0.628, ppl=83.92, wps=5174.7, ups=0.35, wpb=14637.6, bsz=1024, num_updates=57500, lr=0.000147442, gnorm=1.676, clip=0, loss_scale=3528, train_wall=252, wall=165780
2022-08-08 02:42:58 | INFO | train_inner | epoch 016:   1958 / 3715 loss=6.37, nll_loss=2.935, mask_ins=0.962, word_ins_ml=4.467, word_reposition=0.311, kpe=0.631, ppl=82.72, wps=5255.6, ups=0.36, wpb=14699.9, bsz=1024, num_updates=57600, lr=0.000147314, gnorm=1.686, clip=0, loss_scale=2048, train_wall=248, wall=166060
2022-08-08 02:47:37 | INFO | train_inner | epoch 016:   2058 / 3715 loss=6.391, nll_loss=2.949, mask_ins=0.966, word_ins_ml=4.479, word_reposition=0.314, kpe=0.632, ppl=83.95, wps=5234.6, ups=0.36, wpb=14640.2, bsz=1024, num_updates=57700, lr=0.000147186, gnorm=1.667, clip=0, loss_scale=2048, train_wall=249, wall=166340
2022-08-08 02:52:19 | INFO | train_inner | epoch 016:   2158 / 3715 loss=6.395, nll_loss=2.949, mask_ins=0.966, word_ins_ml=4.48, word_reposition=0.314, kpe=0.635, ppl=84.16, wps=5250.6, ups=0.35, wpb=14802.5, bsz=1024, num_updates=57800, lr=0.000147059, gnorm=1.674, clip=0, loss_scale=2048, train_wall=251, wall=166622
2022-08-08 02:56:58 | INFO | train_inner | epoch 016:   2258 / 3715 loss=6.423, nll_loss=2.975, mask_ins=0.971, word_ins_ml=4.502, word_reposition=0.316, kpe=0.635, ppl=85.82, wps=5255.1, ups=0.36, wpb=14651.3, bsz=1024, num_updates=57900, lr=0.000146932, gnorm=1.678, clip=0, loss_scale=2048, train_wall=247, wall=166901
2022-08-08 03:01:40 | INFO | train_inner | epoch 016:   2358 / 3715 loss=6.408, nll_loss=2.958, mask_ins=0.968, word_ins_ml=4.487, word_reposition=0.321, kpe=0.631, ppl=84.89, wps=5198, ups=0.36, wpb=14632.5, bsz=1024, num_updates=58000, lr=0.000146805, gnorm=1.768, clip=0, loss_scale=2376, train_wall=250, wall=167182
2022-08-08 03:06:20 | INFO | train_inner | epoch 016:   2458 / 3715 loss=6.39, nll_loss=2.946, mask_ins=0.965, word_ins_ml=4.476, word_reposition=0.316, kpe=0.632, ppl=83.87, wps=5247.7, ups=0.36, wpb=14696.8, bsz=1024, num_updates=58100, lr=0.000146679, gnorm=1.676, clip=0, loss_scale=4096, train_wall=249, wall=167462
2022-08-08 03:06:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 03:11:03 | INFO | train_inner | epoch 016:   2559 / 3715 loss=6.402, nll_loss=2.95, mask_ins=0.968, word_ins_ml=4.48, word_reposition=0.322, kpe=0.632, ppl=84.54, wps=5190.3, ups=0.35, wpb=14728.5, bsz=1024, num_updates=58200, lr=0.000146553, gnorm=1.672, clip=0, loss_scale=2170, train_wall=252, wall=167746
2022-08-08 03:15:44 | INFO | train_inner | epoch 016:   2659 / 3715 loss=6.391, nll_loss=2.948, mask_ins=0.967, word_ins_ml=4.478, word_reposition=0.315, kpe=0.631, ppl=83.93, wps=5237.1, ups=0.36, wpb=14680.5, bsz=1024, num_updates=58300, lr=0.000146427, gnorm=1.669, clip=0, loss_scale=2048, train_wall=249, wall=168026
2022-08-08 03:20:23 | INFO | train_inner | epoch 016:   2759 / 3715 loss=6.405, nll_loss=2.957, mask_ins=0.97, word_ins_ml=4.486, word_reposition=0.317, kpe=0.632, ppl=84.72, wps=5245.2, ups=0.36, wpb=14630.2, bsz=1024, num_updates=58400, lr=0.000146301, gnorm=1.695, clip=0, loss_scale=2048, train_wall=248, wall=168305
2022-08-08 03:25:03 | INFO | train_inner | epoch 016:   2859 / 3715 loss=6.406, nll_loss=2.953, mask_ins=0.968, word_ins_ml=4.483, word_reposition=0.322, kpe=0.633, ppl=84.8, wps=5218, ups=0.36, wpb=14615.7, bsz=1023.8, num_updates=58500, lr=0.000146176, gnorm=1.687, clip=0, loss_scale=2048, train_wall=249, wall=168585
2022-08-08 03:29:42 | INFO | train_inner | epoch 016:   2959 / 3715 loss=6.415, nll_loss=2.963, mask_ins=0.967, word_ins_ml=4.491, word_reposition=0.322, kpe=0.636, ppl=85.33, wps=5300.1, ups=0.36, wpb=14787.4, bsz=1024, num_updates=58600, lr=0.000146052, gnorm=1.668, clip=0, loss_scale=2048, train_wall=247, wall=168864
2022-08-08 03:34:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 03:34:23 | INFO | train_inner | epoch 016:   3060 / 3715 loss=6.431, nll_loss=2.986, mask_ins=0.975, word_ins_ml=4.511, word_reposition=0.314, kpe=0.631, ppl=86.26, wps=5160.2, ups=0.36, wpb=14505.8, bsz=1024, num_updates=58700, lr=0.000145927, gnorm=1.696, clip=0, loss_scale=3609, train_wall=249, wall=169145
2022-08-08 03:39:04 | INFO | train_inner | epoch 016:   3160 / 3715 loss=6.401, nll_loss=2.954, mask_ins=0.971, word_ins_ml=4.483, word_reposition=0.314, kpe=0.633, ppl=84.53, wps=5181.5, ups=0.36, wpb=14541.5, bsz=1024, num_updates=58800, lr=0.000145803, gnorm=1.702, clip=0, loss_scale=2048, train_wall=249, wall=169426
2022-08-08 03:43:42 | INFO | train_inner | epoch 016:   3260 / 3715 loss=6.386, nll_loss=2.954, mask_ins=0.961, word_ins_ml=4.482, word_reposition=0.31, kpe=0.633, ppl=83.65, wps=5246.9, ups=0.36, wpb=14630, bsz=1024, num_updates=58900, lr=0.000145679, gnorm=1.672, clip=0, loss_scale=2048, train_wall=248, wall=169705
2022-08-08 03:48:22 | INFO | train_inner | epoch 016:   3360 / 3715 loss=6.407, nll_loss=2.957, mask_ins=0.966, word_ins_ml=4.485, word_reposition=0.319, kpe=0.636, ppl=84.88, wps=5237, ups=0.36, wpb=14649.3, bsz=1024, num_updates=59000, lr=0.000145556, gnorm=1.676, clip=0, loss_scale=2048, train_wall=249, wall=169985
2022-08-08 03:53:01 | INFO | train_inner | epoch 016:   3460 / 3715 loss=6.405, nll_loss=2.952, mask_ins=0.965, word_ins_ml=4.481, word_reposition=0.322, kpe=0.637, ppl=84.73, wps=5275, ups=0.36, wpb=14730.2, bsz=1024, num_updates=59100, lr=0.000145432, gnorm=1.67, clip=0, loss_scale=2048, train_wall=248, wall=170264
2022-08-08 03:57:41 | INFO | train_inner | epoch 016:   3560 / 3715 loss=6.421, nll_loss=2.98, mask_ins=0.968, word_ins_ml=4.506, word_reposition=0.314, kpe=0.634, ppl=85.71, wps=5218.8, ups=0.36, wpb=14611.3, bsz=1024, num_updates=59200, lr=0.00014531, gnorm=1.659, clip=0, loss_scale=2048, train_wall=249, wall=170544
2022-08-08 04:02:21 | INFO | train_inner | epoch 016:   3660 / 3715 loss=6.396, nll_loss=2.947, mask_ins=0.964, word_ins_ml=4.477, word_reposition=0.319, kpe=0.636, ppl=84.22, wps=5231.1, ups=0.36, wpb=14617.1, bsz=1024, num_updates=59300, lr=0.000145187, gnorm=1.682, clip=0, loss_scale=3994, train_wall=249, wall=170823
2022-08-08 04:04:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 04:04:52 | INFO | train | epoch 016 | loss 6.386 | nll_loss 2.945 | mask_ins 0.966 | word_ins_ml 4.476 | word_reposition 0.316 | kpe 0.628 | ppl 83.63 | wps 5123.2 | ups 0.35 | wpb 14661.6 | bsz 1023.7 | num_updates 59354 | lr 0.000145121 | gnorm 1.683 | clip 0 | loss_scale 2073 | train_wall 9228 | wall 170975
2022-08-08 04:08:34 | INFO | valid | epoch 016 | valid on 'valid' subset | loss nan | nll_loss 3.164 | mask_ins 0.995 | word_ins_ml 4.72 | word_reposition 0.34 | kpe nan | ppl nan | wps 12367.2 | wpb 1849.4 | bsz 127.9 | num_updates 59354 | best_loss nan
2022-08-08 04:08:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 16 @ 59354 updates, score nan) (writing took 9.771884560585022 seconds)
2022-08-08 04:10:52 | INFO | train_inner | epoch 017:     46 / 3715 loss=6.36, nll_loss=2.925, mask_ins=0.963, word_ins_ml=4.458, word_reposition=0.316, kpe=0.624, ppl=82.16, wps=2855.9, ups=0.2, wpb=14598.2, bsz=1014.7, num_updates=59400, lr=0.000145065, gnorm=1.729, clip=0, loss_scale=2819, train_wall=248, wall=171334
2022-08-08 04:15:30 | INFO | train_inner | epoch 017:    146 / 3715 loss=6.301, nll_loss=2.891, mask_ins=0.954, word_ins_ml=4.427, word_reposition=0.315, kpe=0.605, ppl=78.87, wps=5305.5, ups=0.36, wpb=14760.6, bsz=1024, num_updates=59500, lr=0.000144943, gnorm=1.694, clip=0, loss_scale=2048, train_wall=247, wall=171613
2022-08-08 04:18:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 04:20:11 | INFO | train_inner | epoch 017:    247 / 3715 loss=6.313, nll_loss=2.893, mask_ins=0.959, word_ins_ml=4.43, word_reposition=0.318, kpe=0.606, ppl=79.49, wps=5214.7, ups=0.36, wpb=14658.4, bsz=1024, num_updates=59600, lr=0.000144821, gnorm=1.689, clip=0, loss_scale=1774, train_wall=249, wall=171894
2022-08-08 04:24:50 | INFO | train_inner | epoch 017:    347 / 3715 loss=6.319, nll_loss=2.904, mask_ins=0.961, word_ins_ml=4.439, word_reposition=0.315, kpe=0.605, ppl=79.86, wps=5255.5, ups=0.36, wpb=14626.9, bsz=1024, num_updates=59700, lr=0.0001447, gnorm=1.717, clip=0, loss_scale=1024, train_wall=247, wall=172172
2022-08-08 04:29:28 | INFO | train_inner | epoch 017:    447 / 3715 loss=6.317, nll_loss=2.9, mask_ins=0.962, word_ins_ml=4.436, word_reposition=0.312, kpe=0.607, ppl=79.71, wps=5256, ups=0.36, wpb=14639.2, bsz=1024, num_updates=59800, lr=0.000144579, gnorm=1.715, clip=0, loss_scale=1024, train_wall=248, wall=172451
2022-08-08 04:34:07 | INFO | train_inner | epoch 017:    547 / 3715 loss=6.316, nll_loss=2.903, mask_ins=0.957, word_ins_ml=4.438, word_reposition=0.313, kpe=0.609, ppl=79.66, wps=5257.5, ups=0.36, wpb=14681, bsz=1024, num_updates=59900, lr=0.000144458, gnorm=1.713, clip=0, loss_scale=1024, train_wall=248, wall=172730
2022-08-08 04:38:46 | INFO | train_inner | epoch 017:    647 / 3715 loss=6.292, nll_loss=2.884, mask_ins=0.953, word_ins_ml=4.421, word_reposition=0.311, kpe=0.607, ppl=78.38, wps=5230.3, ups=0.36, wpb=14571.5, bsz=1024, num_updates=60000, lr=0.000144338, gnorm=1.724, clip=0, loss_scale=1024, train_wall=247, wall=173008
2022-08-08 04:43:26 | INFO | train_inner | epoch 017:    747 / 3715 loss=6.326, nll_loss=2.905, mask_ins=0.964, word_ins_ml=4.441, word_reposition=0.309, kpe=0.612, ppl=80.23, wps=5230.3, ups=0.36, wpb=14649.2, bsz=1024, num_updates=60100, lr=0.000144217, gnorm=1.716, clip=0, loss_scale=1178, train_wall=249, wall=173288
2022-08-08 04:48:05 | INFO | train_inner | epoch 017:    847 / 3715 loss=6.317, nll_loss=2.919, mask_ins=0.956, word_ins_ml=4.452, word_reposition=0.305, kpe=0.604, ppl=79.7, wps=5157, ups=0.36, wpb=14390.3, bsz=1024, num_updates=60200, lr=0.000144098, gnorm=1.733, clip=0, loss_scale=2048, train_wall=248, wall=173567
2022-08-08 04:52:45 | INFO | train_inner | epoch 017:    947 / 3715 loss=6.311, nll_loss=2.889, mask_ins=0.958, word_ins_ml=4.426, word_reposition=0.313, kpe=0.614, ppl=79.41, wps=5266.6, ups=0.36, wpb=14744.7, bsz=1024, num_updates=60300, lr=0.000143978, gnorm=1.707, clip=0, loss_scale=2048, train_wall=248, wall=173847
2022-08-08 04:57:25 | INFO | train_inner | epoch 017:   1047 / 3715 loss=6.332, nll_loss=2.91, mask_ins=0.961, word_ins_ml=4.445, word_reposition=0.313, kpe=0.614, ppl=80.54, wps=5256.4, ups=0.36, wpb=14710.8, bsz=1024, num_updates=60400, lr=0.000143859, gnorm=1.696, clip=0, loss_scale=2048, train_wall=249, wall=174127
2022-08-08 05:02:04 | INFO | train_inner | epoch 017:   1147 / 3715 loss=6.342, nll_loss=2.927, mask_ins=0.962, word_ins_ml=4.459, word_reposition=0.309, kpe=0.611, ppl=81.12, wps=5249.6, ups=0.36, wpb=14659.2, bsz=1024, num_updates=60500, lr=0.00014374, gnorm=1.754, clip=0, loss_scale=2048, train_wall=248, wall=174407
2022-08-08 05:06:44 | INFO | train_inner | epoch 017:   1247 / 3715 loss=6.344, nll_loss=2.923, mask_ins=0.967, word_ins_ml=4.456, word_reposition=0.31, kpe=0.611, ppl=81.21, wps=5225.3, ups=0.36, wpb=14605, bsz=1024, num_updates=60600, lr=0.000143621, gnorm=1.721, clip=0, loss_scale=2109, train_wall=248, wall=174686
2022-08-08 05:11:23 | INFO | train_inner | epoch 017:   1347 / 3715 loss=6.333, nll_loss=2.905, mask_ins=0.959, word_ins_ml=4.44, word_reposition=0.315, kpe=0.619, ppl=80.61, wps=5285.5, ups=0.36, wpb=14737.6, bsz=1024, num_updates=60700, lr=0.000143503, gnorm=1.715, clip=0, loss_scale=4096, train_wall=247, wall=174965
2022-08-08 05:16:02 | INFO | train_inner | epoch 017:   1447 / 3715 loss=6.328, nll_loss=2.905, mask_ins=0.957, word_ins_ml=4.44, word_reposition=0.317, kpe=0.614, ppl=80.34, wps=5219.4, ups=0.36, wpb=14611.7, bsz=1024, num_updates=60800, lr=0.000143385, gnorm=1.711, clip=0, loss_scale=4096, train_wall=249, wall=175245
2022-08-08 05:20:41 | INFO | train_inner | epoch 017:   1547 / 3715 loss=6.359, nll_loss=2.928, mask_ins=0.962, word_ins_ml=4.46, word_reposition=0.32, kpe=0.617, ppl=82.11, wps=5288.1, ups=0.36, wpb=14736.3, bsz=1024, num_updates=60900, lr=0.000143267, gnorm=1.726, clip=0, loss_scale=4096, train_wall=247, wall=175524
2022-08-08 05:24:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 05:25:22 | INFO | train_inner | epoch 017:   1648 / 3715 loss=6.34, nll_loss=2.911, mask_ins=0.962, word_ins_ml=4.445, word_reposition=0.315, kpe=0.618, ppl=81, wps=5207.8, ups=0.36, wpb=14648.5, bsz=1024, num_updates=61000, lr=0.00014315, gnorm=1.723, clip=0, loss_scale=3630, train_wall=250, wall=175805
2022-08-08 05:30:01 | INFO | train_inner | epoch 017:   1748 / 3715 loss=6.357, nll_loss=2.936, mask_ins=0.961, word_ins_ml=4.467, word_reposition=0.315, kpe=0.614, ppl=81.99, wps=5271.8, ups=0.36, wpb=14664.2, bsz=1024, num_updates=61100, lr=0.000143032, gnorm=1.725, clip=0, loss_scale=2048, train_wall=247, wall=176083
2022-08-08 05:34:40 | INFO | train_inner | epoch 017:   1848 / 3715 loss=6.325, nll_loss=2.912, mask_ins=0.953, word_ins_ml=4.446, word_reposition=0.31, kpe=0.616, ppl=80.19, wps=5232.5, ups=0.36, wpb=14606, bsz=1024, num_updates=61200, lr=0.000142915, gnorm=1.708, clip=0, loss_scale=2048, train_wall=248, wall=176362
2022-08-08 05:39:20 | INFO | train_inner | epoch 017:   1948 / 3715 loss=6.355, nll_loss=2.924, mask_ins=0.962, word_ins_ml=4.457, word_reposition=0.318, kpe=0.618, ppl=81.88, wps=5241.5, ups=0.36, wpb=14668.5, bsz=1024, num_updates=61300, lr=0.000142799, gnorm=1.712, clip=0, loss_scale=2048, train_wall=248, wall=176642
2022-08-08 05:43:58 | INFO | train_inner | epoch 017:   2048 / 3715 loss=6.359, nll_loss=2.933, mask_ins=0.959, word_ins_ml=4.464, word_reposition=0.318, kpe=0.618, ppl=82.1, wps=5270.5, ups=0.36, wpb=14684.4, bsz=1024, num_updates=61400, lr=0.000142683, gnorm=1.709, clip=0, loss_scale=2048, train_wall=247, wall=176921
2022-08-08 05:48:38 | INFO | train_inner | epoch 017:   2148 / 3715 loss=6.373, nll_loss=2.94, mask_ins=0.966, word_ins_ml=4.471, word_reposition=0.316, kpe=0.62, ppl=82.85, wps=5270.6, ups=0.36, wpb=14721.3, bsz=1024, num_updates=61500, lr=0.000142566, gnorm=1.717, clip=0, loss_scale=2273, train_wall=248, wall=177200
2022-08-08 05:53:57 | INFO | train_inner | epoch 017:   2248 / 3715 loss=6.333, nll_loss=2.911, mask_ins=0.96, word_ins_ml=4.445, word_reposition=0.312, kpe=0.616, ppl=80.63, wps=4575.9, ups=0.31, wpb=14620.9, bsz=1023.8, num_updates=61600, lr=0.000142451, gnorm=1.72, clip=0, loss_scale=4096, train_wall=288, wall=177519
2022-08-08 05:59:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 06:00:06 | INFO | train_inner | epoch 017:   2349 / 3715 loss=6.379, nll_loss=2.942, mask_ins=0.965, word_ins_ml=4.472, word_reposition=0.317, kpe=0.625, ppl=83.23, wps=4017.9, ups=0.27, wpb=14810.8, bsz=1024, num_updates=61700, lr=0.000142335, gnorm=1.711, clip=0, loss_scale=3711, train_wall=337, wall=177888
2022-08-08 06:05:27 | INFO | train_inner | epoch 017:   2449 / 3715 loss=6.351, nll_loss=2.923, mask_ins=0.957, word_ins_ml=4.455, word_reposition=0.316, kpe=0.623, ppl=81.63, wps=4558.8, ups=0.31, wpb=14643.1, bsz=1024, num_updates=61800, lr=0.00014222, gnorm=1.709, clip=0, loss_scale=2048, train_wall=290, wall=178209
2022-08-08 06:10:07 | INFO | train_inner | epoch 017:   2549 / 3715 loss=6.362, nll_loss=2.931, mask_ins=0.962, word_ins_ml=4.462, word_reposition=0.318, kpe=0.621, ppl=82.27, wps=5242.1, ups=0.36, wpb=14665.7, bsz=1024, num_updates=61900, lr=0.000142105, gnorm=1.704, clip=0, loss_scale=2048, train_wall=248, wall=178489
2022-08-08 06:14:45 | INFO | train_inner | epoch 017:   2649 / 3715 loss=6.345, nll_loss=2.921, mask_ins=0.964, word_ins_ml=4.454, word_reposition=0.307, kpe=0.62, ppl=81.31, wps=5250.1, ups=0.36, wpb=14622.6, bsz=1024, num_updates=62000, lr=0.00014199, gnorm=1.718, clip=0, loss_scale=2048, train_wall=247, wall=178768
2022-08-08 06:19:24 | INFO | train_inner | epoch 017:   2749 / 3715 loss=6.36, nll_loss=2.926, mask_ins=0.962, word_ins_ml=4.459, word_reposition=0.317, kpe=0.622, ppl=82.15, wps=5256.1, ups=0.36, wpb=14665, bsz=1024, num_updates=62100, lr=0.000141876, gnorm=1.702, clip=0, loss_scale=2048, train_wall=248, wall=179047
2022-08-08 06:24:04 | INFO | train_inner | epoch 017:   2849 / 3715 loss=6.36, nll_loss=2.927, mask_ins=0.965, word_ins_ml=4.459, word_reposition=0.312, kpe=0.624, ppl=82.12, wps=5246.4, ups=0.36, wpb=14664.9, bsz=1024, num_updates=62200, lr=0.000141762, gnorm=1.705, clip=0, loss_scale=2191, train_wall=248, wall=179326
2022-08-08 06:26:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 06:28:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 06:28:51 | INFO | train_inner | epoch 017:   2951 / 3715 loss=6.351, nll_loss=2.918, mask_ins=0.961, word_ins_ml=4.451, word_reposition=0.315, kpe=0.624, ppl=81.62, wps=5158.1, ups=0.35, wpb=14798.2, bsz=1024, num_updates=62300, lr=0.000141648, gnorm=1.701, clip=0, loss_scale=2962, train_wall=255, wall=179613
2022-08-08 06:33:33 | INFO | train_inner | epoch 017:   3051 / 3715 loss=6.353, nll_loss=2.925, mask_ins=0.959, word_ins_ml=4.457, word_reposition=0.314, kpe=0.623, ppl=81.75, wps=5187.9, ups=0.35, wpb=14652, bsz=1024, num_updates=62400, lr=0.000141535, gnorm=1.723, clip=0, loss_scale=1024, train_wall=251, wall=179895
2022-08-08 06:38:13 | INFO | train_inner | epoch 017:   3151 / 3715 loss=6.398, nll_loss=2.959, mask_ins=0.967, word_ins_ml=4.487, word_reposition=0.316, kpe=0.628, ppl=84.34, wps=5293.3, ups=0.36, wpb=14795.1, bsz=1024, num_updates=62500, lr=0.000141421, gnorm=1.707, clip=0, loss_scale=1024, train_wall=248, wall=180175
2022-08-08 06:42:52 | INFO | train_inner | epoch 017:   3251 / 3715 loss=6.354, nll_loss=2.923, mask_ins=0.957, word_ins_ml=4.456, word_reposition=0.317, kpe=0.624, ppl=81.78, wps=5258.7, ups=0.36, wpb=14677, bsz=1024, num_updates=62600, lr=0.000141308, gnorm=1.717, clip=0, loss_scale=1024, train_wall=248, wall=180454
2022-08-08 06:47:31 | INFO | train_inner | epoch 017:   3351 / 3715 loss=6.355, nll_loss=2.924, mask_ins=0.956, word_ins_ml=4.456, word_reposition=0.316, kpe=0.627, ppl=81.85, wps=5275, ups=0.36, wpb=14748.1, bsz=1024, num_updates=62700, lr=0.000141196, gnorm=1.705, clip=0, loss_scale=1024, train_wall=248, wall=180734
2022-08-08 06:52:11 | INFO | train_inner | epoch 017:   3451 / 3715 loss=6.361, nll_loss=2.932, mask_ins=0.966, word_ins_ml=4.463, word_reposition=0.311, kpe=0.621, ppl=82.2, wps=5197.3, ups=0.36, wpb=14532.2, bsz=1024, num_updates=62800, lr=0.000141083, gnorm=1.708, clip=0, loss_scale=1024, train_wall=249, wall=181013
2022-08-08 06:56:51 | INFO | train_inner | epoch 017:   3551 / 3715 loss=6.381, nll_loss=2.948, mask_ins=0.961, word_ins_ml=4.478, word_reposition=0.316, kpe=0.627, ppl=83.36, wps=5237.5, ups=0.36, wpb=14661.9, bsz=1024, num_updates=62900, lr=0.000140971, gnorm=1.701, clip=0, loss_scale=1976, train_wall=248, wall=181293
2022-08-08 07:01:30 | INFO | train_inner | epoch 017:   3651 / 3715 loss=6.368, nll_loss=2.936, mask_ins=0.964, word_ins_ml=4.467, word_reposition=0.315, kpe=0.621, ppl=82.58, wps=5218, ups=0.36, wpb=14588.8, bsz=1024, num_updates=63000, lr=0.000140859, gnorm=1.721, clip=0, loss_scale=2048, train_wall=248, wall=181573
2022-08-08 07:04:28 | INFO | train | epoch 017 | loss 6.344 | nll_loss 2.919 | mask_ins 0.961 | word_ins_ml 4.452 | word_reposition 0.314 | kpe 0.617 | ppl 81.22 | wps 5047.9 | ups 0.34 | wpb 14661.9 | bsz 1023.7 | num_updates 63064 | lr 0.000140788 | gnorm 1.714 | clip 0 | loss_scale 2112 | train_wall 9382 | wall 181750
2022-08-08 07:08:10 | INFO | valid | epoch 017 | valid on 'valid' subset | loss nan | nll_loss 3.158 | mask_ins 0.992 | word_ins_ml 4.706 | word_reposition 0.34 | kpe nan | ppl nan | wps 12357.9 | wpb 1849.4 | bsz 127.9 | num_updates 63064 | best_loss nan
2022-08-08 07:08:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 17 @ 63064 updates, score nan) (writing took 28.169768722727895 seconds)
2022-08-08 07:10:18 | INFO | train_inner | epoch 018:     36 / 3715 loss=6.332, nll_loss=2.909, mask_ins=0.959, word_ins_ml=4.443, word_reposition=0.318, kpe=0.612, ppl=80.56, wps=2735.6, ups=0.19, wpb=14437.5, bsz=1014.7, num_updates=63100, lr=0.000140747, gnorm=1.74, clip=0, loss_scale=2048, train_wall=247, wall=182101
2022-08-08 07:14:58 | INFO | train_inner | epoch 018:    136 / 3715 loss=6.269, nll_loss=2.872, mask_ins=0.955, word_ins_ml=4.411, word_reposition=0.308, kpe=0.595, ppl=77.13, wps=5274.1, ups=0.36, wpb=14754.2, bsz=1024, num_updates=63200, lr=0.000140636, gnorm=1.735, clip=0, loss_scale=2048, train_wall=248, wall=182380
2022-08-08 07:19:38 | INFO | train_inner | epoch 018:    236 / 3715 loss=6.259, nll_loss=2.859, mask_ins=0.949, word_ins_ml=4.399, word_reposition=0.314, kpe=0.596, ppl=76.58, wps=5242.7, ups=0.36, wpb=14702.5, bsz=1023.8, num_updates=63300, lr=0.000140525, gnorm=1.737, clip=0, loss_scale=2048, train_wall=249, wall=182661
2022-08-08 07:24:18 | INFO | train_inner | epoch 018:    336 / 3715 loss=6.279, nll_loss=2.882, mask_ins=0.953, word_ins_ml=4.42, word_reposition=0.312, kpe=0.595, ppl=77.68, wps=5244, ups=0.36, wpb=14657.2, bsz=1024, num_updates=63400, lr=0.000140414, gnorm=1.728, clip=0, loss_scale=3707, train_wall=248, wall=182940
2022-08-08 07:25:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 07:25:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 07:29:02 | INFO | train_inner | epoch 018:    438 / 3715 loss=6.265, nll_loss=2.863, mask_ins=0.953, word_ins_ml=4.402, word_reposition=0.312, kpe=0.598, ppl=76.93, wps=5167.1, ups=0.35, wpb=14700.1, bsz=1024, num_updates=63500, lr=0.000140303, gnorm=1.747, clip=0, loss_scale=1677, train_wall=253, wall=183225
2022-08-08 07:33:42 | INFO | train_inner | epoch 018:    538 / 3715 loss=6.261, nll_loss=2.866, mask_ins=0.952, word_ins_ml=4.405, word_reposition=0.309, kpe=0.595, ppl=76.67, wps=5198.7, ups=0.36, wpb=14564.3, bsz=1024, num_updates=63600, lr=0.000140193, gnorm=1.752, clip=0, loss_scale=1024, train_wall=249, wall=183505
2022-08-08 07:38:22 | INFO | train_inner | epoch 018:    638 / 3715 loss=6.262, nll_loss=2.874, mask_ins=0.951, word_ins_ml=4.413, word_reposition=0.306, kpe=0.593, ppl=76.75, wps=5176, ups=0.36, wpb=14441.9, bsz=1024, num_updates=63700, lr=0.000140083, gnorm=1.776, clip=0, loss_scale=1024, train_wall=248, wall=183784
2022-08-08 07:43:00 | INFO | train_inner | epoch 018:    738 / 3715 loss=6.278, nll_loss=2.88, mask_ins=0.954, word_ins_ml=4.418, word_reposition=0.311, kpe=0.595, ppl=77.58, wps=5213, ups=0.36, wpb=14514.1, bsz=1024, num_updates=63800, lr=0.000139973, gnorm=1.761, clip=0, loss_scale=1024, train_wall=247, wall=184062
2022-08-08 07:47:38 | INFO | train_inner | epoch 018:    838 / 3715 loss=6.277, nll_loss=2.876, mask_ins=0.949, word_ins_ml=4.414, word_reposition=0.315, kpe=0.6, ppl=77.57, wps=5267.9, ups=0.36, wpb=14661.5, bsz=1024, num_updates=63900, lr=0.000139864, gnorm=1.748, clip=0, loss_scale=1024, train_wall=247, wall=184341
2022-08-08 07:52:18 | INFO | train_inner | epoch 018:    938 / 3715 loss=6.272, nll_loss=2.864, mask_ins=0.951, word_ins_ml=4.404, word_reposition=0.316, kpe=0.601, ppl=77.28, wps=5264.8, ups=0.36, wpb=14719.4, bsz=1024, num_updates=64000, lr=0.000139754, gnorm=1.768, clip=0, loss_scale=1669, train_wall=248, wall=184620
2022-08-08 07:56:57 | INFO | train_inner | epoch 018:   1038 / 3715 loss=6.283, nll_loss=2.879, mask_ins=0.958, word_ins_ml=4.417, word_reposition=0.31, kpe=0.599, ppl=77.88, wps=5222.2, ups=0.36, wpb=14591.2, bsz=1024, num_updates=64100, lr=0.000139645, gnorm=1.791, clip=0, loss_scale=2048, train_wall=248, wall=184900
2022-08-08 08:01:37 | INFO | train_inner | epoch 018:   1138 / 3715 loss=6.297, nll_loss=2.899, mask_ins=0.952, word_ins_ml=4.435, word_reposition=0.307, kpe=0.604, ppl=78.62, wps=5251.3, ups=0.36, wpb=14677.1, bsz=1024, num_updates=64200, lr=0.000139536, gnorm=1.739, clip=0, loss_scale=2048, train_wall=248, wall=185179
2022-08-08 08:06:17 | INFO | train_inner | epoch 018:   1238 / 3715 loss=6.298, nll_loss=2.889, mask_ins=0.956, word_ins_ml=4.426, word_reposition=0.312, kpe=0.604, ppl=78.7, wps=5285.2, ups=0.36, wpb=14786.7, bsz=1024, num_updates=64300, lr=0.000139428, gnorm=1.743, clip=0, loss_scale=2048, train_wall=248, wall=185459
2022-08-08 08:07:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 08:10:58 | INFO | train_inner | epoch 018:   1339 / 3715 loss=6.28, nll_loss=2.877, mask_ins=0.949, word_ins_ml=4.415, word_reposition=0.312, kpe=0.604, ppl=77.69, wps=5210.5, ups=0.36, wpb=14644.4, bsz=1024, num_updates=64400, lr=0.00013932, gnorm=1.823, clip=0, loss_scale=1359, train_wall=249, wall=185740
2022-08-08 08:15:37 | INFO | train_inner | epoch 018:   1439 / 3715 loss=6.298, nll_loss=2.89, mask_ins=0.953, word_ins_ml=4.427, word_reposition=0.309, kpe=0.609, ppl=78.68, wps=5276.6, ups=0.36, wpb=14753.3, bsz=1024, num_updates=64500, lr=0.000139212, gnorm=1.749, clip=0, loss_scale=1024, train_wall=249, wall=186020
2022-08-08 08:20:17 | INFO | train_inner | epoch 018:   1539 / 3715 loss=6.305, nll_loss=2.891, mask_ins=0.959, word_ins_ml=4.427, word_reposition=0.313, kpe=0.606, ppl=79.07, wps=5256.8, ups=0.36, wpb=14705.9, bsz=1024, num_updates=64600, lr=0.000139104, gnorm=1.755, clip=0, loss_scale=1024, train_wall=249, wall=186299
2022-08-08 08:24:56 | INFO | train_inner | epoch 018:   1639 / 3715 loss=6.285, nll_loss=2.885, mask_ins=0.946, word_ins_ml=4.422, word_reposition=0.312, kpe=0.604, ppl=77.96, wps=5257.6, ups=0.36, wpb=14683.7, bsz=1024, num_updates=64700, lr=0.000138996, gnorm=1.742, clip=0, loss_scale=1024, train_wall=248, wall=186579
2022-08-08 08:29:35 | INFO | train_inner | epoch 018:   1739 / 3715 loss=6.3, nll_loss=2.895, mask_ins=0.955, word_ins_ml=4.43, word_reposition=0.309, kpe=0.605, ppl=78.77, wps=5263.6, ups=0.36, wpb=14692.9, bsz=1024, num_updates=64800, lr=0.000138889, gnorm=1.732, clip=0, loss_scale=1024, train_wall=248, wall=186858
2022-08-08 08:34:15 | INFO | train_inner | epoch 018:   1839 / 3715 loss=6.298, nll_loss=2.895, mask_ins=0.954, word_ins_ml=4.431, word_reposition=0.307, kpe=0.606, ppl=78.69, wps=5230.2, ups=0.36, wpb=14623.3, bsz=1024, num_updates=64900, lr=0.000138782, gnorm=1.746, clip=0, loss_scale=1597, train_wall=248, wall=187137
2022-08-08 08:38:55 | INFO | train_inner | epoch 018:   1939 / 3715 loss=6.293, nll_loss=2.883, mask_ins=0.956, word_ins_ml=4.42, word_reposition=0.31, kpe=0.607, ppl=78.42, wps=5229.6, ups=0.36, wpb=14630.4, bsz=1024, num_updates=65000, lr=0.000138675, gnorm=1.757, clip=0, loss_scale=2048, train_wall=249, wall=187417
2022-08-08 08:43:34 | INFO | train_inner | epoch 018:   2039 / 3715 loss=6.297, nll_loss=2.889, mask_ins=0.955, word_ins_ml=4.425, word_reposition=0.311, kpe=0.606, ppl=78.62, wps=5262.2, ups=0.36, wpb=14686, bsz=1024, num_updates=65100, lr=0.000138568, gnorm=1.739, clip=0, loss_scale=2048, train_wall=248, wall=187696
2022-08-08 08:48:13 | INFO | train_inner | epoch 018:   2139 / 3715 loss=6.316, nll_loss=2.91, mask_ins=0.951, word_ins_ml=4.444, word_reposition=0.31, kpe=0.611, ppl=79.69, wps=5284.5, ups=0.36, wpb=14739, bsz=1024, num_updates=65200, lr=0.000138462, gnorm=1.746, clip=0, loss_scale=2048, train_wall=248, wall=187975
2022-08-08 08:52:53 | INFO | train_inner | epoch 018:   2239 / 3715 loss=6.301, nll_loss=2.887, mask_ins=0.959, word_ins_ml=4.424, word_reposition=0.311, kpe=0.608, ppl=78.85, wps=5264.8, ups=0.36, wpb=14749.4, bsz=1024, num_updates=65300, lr=0.000138356, gnorm=1.74, clip=0, loss_scale=2048, train_wall=249, wall=188255
2022-08-08 08:57:32 | INFO | train_inner | epoch 018:   2339 / 3715 loss=6.299, nll_loss=2.888, mask_ins=0.954, word_ins_ml=4.425, word_reposition=0.31, kpe=0.61, ppl=78.75, wps=5284.6, ups=0.36, wpb=14732.8, bsz=1024, num_updates=65400, lr=0.00013825, gnorm=1.722, clip=0, loss_scale=2949, train_wall=247, wall=188534
2022-08-08 09:02:11 | INFO | train_inner | epoch 018:   2439 / 3715 loss=6.31, nll_loss=2.897, mask_ins=0.959, word_ins_ml=4.432, word_reposition=0.31, kpe=0.609, ppl=79.36, wps=5244.7, ups=0.36, wpb=14646.3, bsz=1024, num_updates=65500, lr=0.000138145, gnorm=1.738, clip=0, loss_scale=4096, train_wall=248, wall=188813
2022-08-08 09:06:50 | INFO | train_inner | epoch 018:   2539 / 3715 loss=6.317, nll_loss=2.901, mask_ins=0.952, word_ins_ml=4.436, word_reposition=0.317, kpe=0.613, ppl=79.75, wps=5278.5, ups=0.36, wpb=14720.5, bsz=1024, num_updates=65600, lr=0.000138039, gnorm=1.712, clip=0, loss_scale=4096, train_wall=248, wall=189092
2022-08-08 09:11:28 | INFO | train_inner | epoch 018:   2639 / 3715 loss=6.339, nll_loss=2.921, mask_ins=0.962, word_ins_ml=4.453, word_reposition=0.312, kpe=0.611, ppl=80.96, wps=5260.9, ups=0.36, wpb=14657, bsz=1024, num_updates=65700, lr=0.000137934, gnorm=1.759, clip=0, loss_scale=4096, train_wall=247, wall=189371
2022-08-08 09:12:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 09:16:11 | INFO | train_inner | epoch 018:   2740 / 3715 loss=6.311, nll_loss=2.898, mask_ins=0.955, word_ins_ml=4.434, word_reposition=0.311, kpe=0.61, ppl=79.38, wps=5180.7, ups=0.35, wpb=14617.6, bsz=1024, num_updates=65800, lr=0.000137829, gnorm=1.753, clip=0, loss_scale=2291, train_wall=251, wall=189653
2022-08-08 09:20:49 | INFO | train_inner | epoch 018:   2840 / 3715 loss=6.335, nll_loss=2.905, mask_ins=0.961, word_ins_ml=4.44, word_reposition=0.321, kpe=0.614, ppl=80.73, wps=5281.7, ups=0.36, wpb=14732, bsz=1024, num_updates=65900, lr=0.000137725, gnorm=1.744, clip=0, loss_scale=2048, train_wall=248, wall=189932
2022-08-08 09:23:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 09:25:32 | INFO | train_inner | epoch 018:   2941 / 3715 loss=6.305, nll_loss=2.892, mask_ins=0.956, word_ins_ml=4.427, word_reposition=0.311, kpe=0.61, ppl=79.05, wps=5176.7, ups=0.35, wpb=14615.1, bsz=1024, num_updates=66000, lr=0.00013762, gnorm=1.727, clip=0, loss_scale=1673, train_wall=251, wall=190214
2022-08-08 09:28:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-08 09:30:14 | INFO | train_inner | epoch 018:   3042 / 3715 loss=6.292, nll_loss=2.88, mask_ins=0.954, word_ins_ml=4.417, word_reposition=0.311, kpe=0.611, ppl=78.38, wps=5193.5, ups=0.35, wpb=14660.8, bsz=1024, num_updates=66100, lr=0.000137516, gnorm=1.814, clip=0, loss_scale=826, train_wall=250, wall=190497
2022-08-08 09:34:55 | INFO | train_inner | epoch 018:   3142 / 3715 loss=6.328, nll_loss=2.912, mask_ins=0.955, word_ins_ml=4.445, word_reposition=0.316, kpe=0.611, ppl=80.31, wps=5224.2, ups=0.36, wpb=14651.7, bsz=1024, num_updates=66200, lr=0.000137412, gnorm=1.743, clip=0, loss_scale=512, train_wall=249, wall=190777
2022-08-08 09:39:34 | INFO | train_inner | epoch 018:   3242 / 3715 loss=6.336, nll_loss=2.924, mask_ins=0.957, word_ins_ml=4.456, word_reposition=0.313, kpe=0.611, ppl=80.8, wps=5236.8, ups=0.36, wpb=14634.2, bsz=1024, num_updates=66300, lr=0.000137309, gnorm=1.735, clip=0, loss_scale=512, train_wall=248, wall=191056
2022-08-08 09:40:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-08 09:44:15 | INFO | train_inner | epoch 018:   3343 / 3715 loss=6.297, nll_loss=2.893, mask_ins=0.953, word_ins_ml=4.429, word_reposition=0.304, kpe=0.611, ppl=78.63, wps=5199.5, ups=0.36, wpb=14596.6, bsz=1024, num_updates=66400, lr=0.000137205, gnorm=1.748, clip=0, loss_scale=312, train_wall=249, wall=191337
2022-08-08 09:48:54 | INFO | train_inner | epoch 018:   3443 / 3715 loss=6.303, nll_loss=2.881, mask_ins=0.953, word_ins_ml=4.418, word_reposition=0.316, kpe=0.616, ppl=78.94, wps=5270.1, ups=0.36, wpb=14724.8, bsz=1024, num_updates=66500, lr=0.000137102, gnorm=1.732, clip=0, loss_scale=256, train_wall=248, wall=191617
2022-08-08 09:53:34 | INFO | train_inner | epoch 018:   3543 / 3715 loss=6.335, nll_loss=2.912, mask_ins=0.961, word_ins_ml=4.445, word_reposition=0.314, kpe=0.615, ppl=80.73, wps=5247.8, ups=0.36, wpb=14692, bsz=1024, num_updates=66600, lr=0.000136999, gnorm=1.73, clip=0, loss_scale=256, train_wall=248, wall=191897
2022-08-08 09:58:14 | INFO | train_inner | epoch 018:   3643 / 3715 loss=6.334, nll_loss=2.902, mask_ins=0.958, word_ins_ml=4.437, word_reposition=0.323, kpe=0.617, ppl=80.69, wps=5261.5, ups=0.36, wpb=14733.6, bsz=1024, num_updates=66700, lr=0.000136896, gnorm=1.736, clip=0, loss_scale=256, train_wall=249, wall=192177
2022-08-08 10:01:33 | INFO | train | epoch 018 | loss 6.297 | nll_loss 2.889 | mask_ins 0.954 | word_ins_ml 4.425 | word_reposition 0.312 | kpe 0.606 | ppl 78.65 | wps 5116.9 | ups 0.35 | wpb 14662.4 | bsz 1023.7 | num_updates 66772 | lr 0.000136823 | gnorm 1.749 | clip 0 | loss_scale 1664 | train_wall 9212 | wall 192376
2022-08-08 10:05:14 | INFO | valid | epoch 018 | valid on 'valid' subset | loss nan | nll_loss 3.154 | mask_ins 0.995 | word_ins_ml 4.709 | word_reposition 0.346 | kpe nan | ppl nan | wps 12408.9 | wpb 1849.4 | bsz 127.9 | num_updates 66772 | best_loss nan
2022-08-08 10:05:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 18 @ 66772 updates, score nan) (writing took 10.7293746676296 seconds)
2022-08-08 10:06:43 | INFO | train_inner | epoch 019:     28 / 3715 loss=6.275, nll_loss=2.872, mask_ins=0.951, word_ins_ml=4.41, word_reposition=0.312, kpe=0.603, ppl=77.45, wps=2851, ups=0.2, wpb=14499.3, bsz=1014.7, num_updates=66800, lr=0.000136794, gnorm=1.784, clip=0, loss_scale=256, train_wall=246, wall=192685
2022-08-08 10:11:22 | INFO | train_inner | epoch 019:    128 / 3715 loss=6.236, nll_loss=2.851, mask_ins=0.951, word_ins_ml=4.391, word_reposition=0.31, kpe=0.583, ppl=75.39, wps=5265.8, ups=0.36, wpb=14702.6, bsz=1023.8, num_updates=66900, lr=0.000136692, gnorm=1.758, clip=0, loss_scale=428, train_wall=248, wall=192964
2022-08-08 10:16:00 | INFO | train_inner | epoch 019:    228 / 3715 loss=6.22, nll_loss=2.839, mask_ins=0.945, word_ins_ml=4.382, word_reposition=0.312, kpe=0.582, ppl=74.56, wps=5246, ups=0.36, wpb=14609.3, bsz=1024, num_updates=67000, lr=0.00013659, gnorm=1.784, clip=0, loss_scale=512, train_wall=247, wall=193243
2022-08-08 10:20:41 | INFO | train_inner | epoch 019:    328 / 3715 loss=6.222, nll_loss=2.841, mask_ins=0.948, word_ins_ml=4.383, word_reposition=0.307, kpe=0.584, ppl=74.65, wps=5209.9, ups=0.36, wpb=14636, bsz=1024, num_updates=67100, lr=0.000136488, gnorm=1.786, clip=0, loss_scale=512, train_wall=250, wall=193524
2022-08-08 10:25:22 | INFO | train_inner | epoch 019:    428 / 3715 loss=6.238, nll_loss=2.856, mask_ins=0.95, word_ins_ml=4.396, word_reposition=0.308, kpe=0.585, ppl=75.49, wps=5247.3, ups=0.36, wpb=14700.6, bsz=1024, num_updates=67200, lr=0.000136386, gnorm=1.768, clip=0, loss_scale=512, train_wall=249, wall=193804
2022-08-08 10:30:00 | INFO | train_inner | epoch 019:    528 / 3715 loss=6.242, nll_loss=2.855, mask_ins=0.947, word_ins_ml=4.395, word_reposition=0.313, kpe=0.586, ppl=75.68, wps=5300.3, ups=0.36, wpb=14765, bsz=1024, num_updates=67300, lr=0.000136285, gnorm=1.81, clip=0, loss_scale=512, train_wall=247, wall=194082
2022-08-08 10:34:40 | INFO | train_inner | epoch 019:    628 / 3715 loss=6.226, nll_loss=2.841, mask_ins=0.947, word_ins_ml=4.383, word_reposition=0.31, kpe=0.586, ppl=74.83, wps=5212.7, ups=0.36, wpb=14610.4, bsz=1024, num_updates=67400, lr=0.000136184, gnorm=1.784, clip=0, loss_scale=794, train_wall=249, wall=194363
2022-08-08 10:39:19 | INFO | train_inner | epoch 019:    728 / 3715 loss=6.254, nll_loss=2.856, mask_ins=0.949, word_ins_ml=4.396, word_reposition=0.314, kpe=0.594, ppl=76.29, wps=5312.4, ups=0.36, wpb=14825.7, bsz=1024, num_updates=67500, lr=0.000136083, gnorm=1.766, clip=0, loss_scale=1024, train_wall=248, wall=194642
2022-08-08 10:43:59 | INFO | train_inner | epoch 019:    828 / 3715 loss=6.253, nll_loss=2.864, mask_ins=0.949, word_ins_ml=4.403, word_reposition=0.31, kpe=0.591, ppl=76.27, wps=5264.7, ups=0.36, wpb=14690.1, bsz=1024, num_updates=67600, lr=0.000135982, gnorm=1.764, clip=0, loss_scale=1024, train_wall=248, wall=194921
2022-08-08 10:48:37 | INFO | train_inner | epoch 019:    928 / 3715 loss=6.25, nll_loss=2.866, mask_ins=0.946, word_ins_ml=4.405, word_reposition=0.311, kpe=0.588, ppl=76.08, wps=5258.8, ups=0.36, wpb=14635, bsz=1024, num_updates=67700, lr=0.000135882, gnorm=1.768, clip=0, loss_scale=1024, train_wall=247, wall=195199
2022-08-08 10:53:17 | INFO | train_inner | epoch 019:   1028 / 3715 loss=6.249, nll_loss=2.856, mask_ins=0.943, word_ins_ml=4.396, word_reposition=0.317, kpe=0.593, ppl=76.06, wps=5252.1, ups=0.36, wpb=14701.1, bsz=1024, num_updates=67800, lr=0.000135781, gnorm=1.773, clip=0, loss_scale=1024, train_wall=249, wall=195479
2022-08-08 10:57:56 | INFO | train_inner | epoch 019:   1128 / 3715 loss=6.23, nll_loss=2.844, mask_ins=0.943, word_ins_ml=4.385, word_reposition=0.311, kpe=0.59, ppl=75.04, wps=5242.7, ups=0.36, wpb=14635.3, bsz=1024, num_updates=67900, lr=0.000135681, gnorm=1.772, clip=0, loss_scale=1464, train_wall=248, wall=195758
2022-08-08 11:02:35 | INFO | train_inner | epoch 019:   1228 / 3715 loss=6.244, nll_loss=2.856, mask_ins=0.945, word_ins_ml=4.396, word_reposition=0.31, kpe=0.594, ppl=75.81, wps=5245.4, ups=0.36, wpb=14661.3, bsz=1024, num_updates=68000, lr=0.000135582, gnorm=1.783, clip=0, loss_scale=2048, train_wall=248, wall=196038
2022-08-08 11:07:15 | INFO | train_inner | epoch 019:   1328 / 3715 loss=6.259, nll_loss=2.865, mask_ins=0.951, word_ins_ml=4.404, word_reposition=0.312, kpe=0.591, ppl=76.57, wps=5246.5, ups=0.36, wpb=14691.4, bsz=1024, num_updates=68100, lr=0.000135482, gnorm=1.791, clip=0, loss_scale=2048, train_wall=249, wall=196318
2022-08-08 11:11:55 | INFO | train_inner | epoch 019:   1428 / 3715 loss=6.246, nll_loss=2.861, mask_ins=0.944, word_ins_ml=4.4, word_reposition=0.312, kpe=0.59, ppl=75.88, wps=5235.1, ups=0.36, wpb=14627.5, bsz=1024, num_updates=68200, lr=0.000135383, gnorm=1.776, clip=0, loss_scale=2048, train_wall=248, wall=196597
2022-08-08 11:16:34 | INFO | train_inner | epoch 019:   1528 / 3715 loss=6.242, nll_loss=2.852, mask_ins=0.947, word_ins_ml=4.393, word_reposition=0.309, kpe=0.593, ppl=75.67, wps=5255.3, ups=0.36, wpb=14676.1, bsz=1024, num_updates=68300, lr=0.000135283, gnorm=1.758, clip=0, loss_scale=2048, train_wall=248, wall=196876
2022-08-08 11:19:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 11:21:15 | INFO | train_inner | epoch 019:   1629 / 3715 loss=6.241, nll_loss=2.848, mask_ins=0.945, word_ins_ml=4.389, word_reposition=0.311, kpe=0.596, ppl=75.63, wps=5260.6, ups=0.36, wpb=14763.5, bsz=1024, num_updates=68400, lr=0.000135185, gnorm=1.759, clip=0, loss_scale=1693, train_wall=249, wall=197157
2022-08-08 11:25:54 | INFO | train_inner | epoch 019:   1729 / 3715 loss=6.262, nll_loss=2.868, mask_ins=0.947, word_ins_ml=4.407, word_reposition=0.311, kpe=0.597, ppl=76.75, wps=5283, ups=0.36, wpb=14742.9, bsz=1024, num_updates=68500, lr=0.000135086, gnorm=1.79, clip=0, loss_scale=1024, train_wall=248, wall=197436
2022-08-08 11:30:32 | INFO | train_inner | epoch 019:   1829 / 3715 loss=6.266, nll_loss=2.872, mask_ins=0.95, word_ins_ml=4.41, word_reposition=0.307, kpe=0.599, ppl=76.96, wps=5283.1, ups=0.36, wpb=14674.1, bsz=1024, num_updates=68600, lr=0.000134987, gnorm=1.769, clip=0, loss_scale=1024, train_wall=246, wall=197714
2022-08-08 11:35:11 | INFO | train_inner | epoch 019:   1929 / 3715 loss=6.244, nll_loss=2.854, mask_ins=0.947, word_ins_ml=4.394, word_reposition=0.309, kpe=0.593, ppl=75.77, wps=5219, ups=0.36, wpb=14605.3, bsz=1024, num_updates=68700, lr=0.000134889, gnorm=1.781, clip=0, loss_scale=1024, train_wall=248, wall=197994
2022-08-08 11:39:52 | INFO | train_inner | epoch 019:   2029 / 3715 loss=6.231, nll_loss=2.848, mask_ins=0.946, word_ins_ml=4.389, word_reposition=0.303, kpe=0.593, ppl=75.11, wps=5197.5, ups=0.36, wpb=14595.5, bsz=1024, num_updates=68800, lr=0.000134791, gnorm=1.77, clip=0, loss_scale=1024, train_wall=249, wall=198275
2022-08-08 11:44:32 | INFO | train_inner | epoch 019:   2129 / 3715 loss=6.259, nll_loss=2.869, mask_ins=0.952, word_ins_ml=4.407, word_reposition=0.303, kpe=0.597, ppl=76.58, wps=5206.8, ups=0.36, wpb=14576.6, bsz=1024, num_updates=68900, lr=0.000134693, gnorm=1.786, clip=0, loss_scale=1260, train_wall=248, wall=198555
2022-08-08 11:49:12 | INFO | train_inner | epoch 019:   2229 / 3715 loss=6.26, nll_loss=2.861, mask_ins=0.947, word_ins_ml=4.4, word_reposition=0.315, kpe=0.597, ppl=76.65, wps=5230.8, ups=0.36, wpb=14632.5, bsz=1024, num_updates=69000, lr=0.000134595, gnorm=1.786, clip=0, loss_scale=2048, train_wall=248, wall=198834
2022-08-08 11:53:53 | INFO | train_inner | epoch 019:   2329 / 3715 loss=6.256, nll_loss=2.867, mask_ins=0.944, word_ins_ml=4.406, word_reposition=0.309, kpe=0.597, ppl=76.44, wps=5201.2, ups=0.36, wpb=14638.6, bsz=1024, num_updates=69100, lr=0.000134498, gnorm=1.761, clip=0, loss_scale=2048, train_wall=250, wall=199116
2022-08-08 11:58:35 | INFO | train_inner | epoch 019:   2429 / 3715 loss=6.251, nll_loss=2.857, mask_ins=0.943, word_ins_ml=4.397, word_reposition=0.312, kpe=0.599, ppl=76.16, wps=5214.5, ups=0.36, wpb=14681.6, bsz=1024, num_updates=69200, lr=0.000134401, gnorm=1.755, clip=0, loss_scale=2048, train_wall=250, wall=199397
2022-08-08 12:03:15 | INFO | train_inner | epoch 019:   2529 / 3715 loss=6.255, nll_loss=2.86, mask_ins=0.953, word_ins_ml=4.4, word_reposition=0.306, kpe=0.596, ppl=76.39, wps=5190.1, ups=0.36, wpb=14559.2, bsz=1024, num_updates=69300, lr=0.000134304, gnorm=1.763, clip=0, loss_scale=2048, train_wall=249, wall=199678
2022-08-08 12:07:56 | INFO | train_inner | epoch 019:   2629 / 3715 loss=6.278, nll_loss=2.879, mask_ins=0.954, word_ins_ml=4.416, word_reposition=0.309, kpe=0.599, ppl=77.6, wps=5200.7, ups=0.36, wpb=14588.1, bsz=1024, num_updates=69400, lr=0.000134207, gnorm=1.77, clip=0, loss_scale=2273, train_wall=249, wall=199958
2022-08-08 12:12:36 | INFO | train_inner | epoch 019:   2729 / 3715 loss=6.298, nll_loss=2.892, mask_ins=0.953, word_ins_ml=4.428, word_reposition=0.311, kpe=0.606, ppl=78.67, wps=5250.6, ups=0.36, wpb=14701.5, bsz=1024, num_updates=69500, lr=0.00013411, gnorm=1.758, clip=0, loss_scale=4096, train_wall=249, wall=200238
2022-08-08 12:17:17 | INFO | train_inner | epoch 019:   2829 / 3715 loss=6.311, nll_loss=2.897, mask_ins=0.956, word_ins_ml=4.432, word_reposition=0.318, kpe=0.605, ppl=79.39, wps=5248.1, ups=0.36, wpb=14739.6, bsz=1024, num_updates=69600, lr=0.000134014, gnorm=1.748, clip=0, loss_scale=4096, train_wall=249, wall=200519
2022-08-08 12:21:58 | INFO | train_inner | epoch 019:   2929 / 3715 loss=6.271, nll_loss=2.876, mask_ins=0.948, word_ins_ml=4.414, word_reposition=0.31, kpe=0.6, ppl=77.21, wps=5204.1, ups=0.36, wpb=14627.8, bsz=1024, num_updates=69700, lr=0.000133918, gnorm=1.763, clip=0, loss_scale=4096, train_wall=249, wall=200800
2022-08-08 12:26:38 | INFO | train_inner | epoch 019:   3029 / 3715 loss=6.299, nll_loss=2.892, mask_ins=0.953, word_ins_ml=4.427, word_reposition=0.314, kpe=0.605, ppl=78.74, wps=5260.9, ups=0.36, wpb=14755.2, bsz=1024, num_updates=69800, lr=0.000133822, gnorm=1.755, clip=0, loss_scale=4096, train_wall=249, wall=201081
2022-08-08 12:31:20 | INFO | train_inner | epoch 019:   3129 / 3715 loss=6.27, nll_loss=2.859, mask_ins=0.95, word_ins_ml=4.398, word_reposition=0.318, kpe=0.603, ppl=77.16, wps=5246.1, ups=0.35, wpb=14785.4, bsz=1024, num_updates=69900, lr=0.000133726, gnorm=1.753, clip=0, loss_scale=4096, train_wall=250, wall=201363
2022-08-08 12:33:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-08 12:36:05 | INFO | train_inner | epoch 019:   3230 / 3715 loss=6.29, nll_loss=2.881, mask_ins=0.955, word_ins_ml=4.418, word_reposition=0.316, kpe=0.601, ppl=78.23, wps=5132.2, ups=0.35, wpb=14624.2, bsz=1024, num_updates=70000, lr=0.000133631, gnorm=1.757, clip=0, loss_scale=6002, train_wall=253, wall=201648
2022-08-08 12:36:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 12:40:48 | INFO | train_inner | epoch 019:   3331 / 3715 loss=6.294, nll_loss=2.9, mask_ins=0.95, word_ins_ml=4.435, word_reposition=0.311, kpe=0.599, ppl=78.49, wps=5165.7, ups=0.35, wpb=14584.9, bsz=1024, num_updates=70100, lr=0.000133535, gnorm=1.777, clip=0, loss_scale=2149, train_wall=250, wall=201930
2022-08-08 12:42:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 12:45:30 | INFO | train_inner | epoch 019:   3432 / 3715 loss=6.279, nll_loss=2.876, mask_ins=0.952, word_ins_ml=4.413, word_reposition=0.31, kpe=0.604, ppl=77.64, wps=5189.7, ups=0.35, wpb=14666.3, bsz=1024, num_updates=70200, lr=0.00013344, gnorm=1.834, clip=0, loss_scale=1277, train_wall=251, wall=202212
2022-08-08 12:50:10 | INFO | train_inner | epoch 019:   3532 / 3715 loss=6.27, nll_loss=2.865, mask_ins=0.949, word_ins_ml=4.404, word_reposition=0.313, kpe=0.604, ppl=77.16, wps=5225.8, ups=0.36, wpb=14640.6, bsz=1024, num_updates=70300, lr=0.000133345, gnorm=1.763, clip=0, loss_scale=1024, train_wall=249, wall=202493
2022-08-08 12:54:50 | INFO | train_inner | epoch 019:   3632 / 3715 loss=6.271, nll_loss=2.876, mask_ins=0.946, word_ins_ml=4.414, word_reposition=0.31, kpe=0.602, ppl=77.23, wps=5207.2, ups=0.36, wpb=14563.2, bsz=1024, num_updates=70400, lr=0.00013325, gnorm=1.773, clip=0, loss_scale=1024, train_wall=248, wall=202772
2022-08-08 12:58:39 | INFO | train | epoch 019 | loss 6.257 | nll_loss 2.864 | mask_ins 0.948 | word_ins_ml 4.403 | word_reposition 0.311 | kpe 0.595 | ppl 76.49 | wps 5120.4 | ups 0.35 | wpb 14661.5 | bsz 1023.7 | num_updates 70483 | lr 0.000133172 | gnorm 1.773 | clip 0 | loss_scale 1818 | train_wall 9227 | wall 203002
2022-08-08 13:02:25 | INFO | valid | epoch 019 | valid on 'valid' subset | loss nan | nll_loss 3.152 | mask_ins 0.994 | word_ins_ml 4.701 | word_reposition 0.345 | kpe nan | ppl nan | wps 12150.7 | wpb 1849.4 | bsz 127.9 | num_updates 70483 | best_loss nan
2022-08-08 13:02:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 19 @ 70483 updates, score nan) (writing took 15.478617701679468 seconds)
2022-08-08 13:03:28 | INFO | train_inner | epoch 020:     17 / 3715 loss=6.253, nll_loss=2.851, mask_ins=0.948, word_ins_ml=4.391, word_reposition=0.313, kpe=0.601, ppl=76.25, wps=2804.8, ups=0.19, wpb=14527.9, bsz=1014.7, num_updates=70500, lr=0.000133156, gnorm=1.809, clip=0, loss_scale=1024, train_wall=246, wall=203290
2022-08-08 13:08:09 | INFO | train_inner | epoch 020:    117 / 3715 loss=6.168, nll_loss=2.802, mask_ins=0.938, word_ins_ml=4.348, word_reposition=0.307, kpe=0.575, ppl=71.92, wps=5253.6, ups=0.36, wpb=14750, bsz=1024, num_updates=70600, lr=0.000133062, gnorm=1.798, clip=0, loss_scale=1024, train_wall=249, wall=203571
2022-08-08 13:12:51 | INFO | train_inner | epoch 020:    217 / 3715 loss=6.179, nll_loss=2.81, mask_ins=0.944, word_ins_ml=4.355, word_reposition=0.308, kpe=0.571, ppl=72.45, wps=5158.1, ups=0.35, wpb=14545.6, bsz=1024, num_updates=70700, lr=0.000132967, gnorm=1.825, clip=0, loss_scale=1679, train_wall=250, wall=203853
2022-08-08 13:17:49 | INFO | train_inner | epoch 020:    317 / 3715 loss=6.192, nll_loss=2.825, mask_ins=0.941, word_ins_ml=4.368, word_reposition=0.305, kpe=0.577, ppl=73.12, wps=4926, ups=0.34, wpb=14699.9, bsz=1024, num_updates=70800, lr=0.000132874, gnorm=1.806, clip=0, loss_scale=2048, train_wall=262, wall=204151
2022-08-08 13:22:42 | INFO | train_inner | epoch 020:    417 / 3715 loss=6.18, nll_loss=2.82, mask_ins=0.939, word_ins_ml=4.364, word_reposition=0.305, kpe=0.572, ppl=72.52, wps=4991.6, ups=0.34, wpb=14635.4, bsz=1024, num_updates=70900, lr=0.00013278, gnorm=1.818, clip=0, loss_scale=2048, train_wall=257, wall=204445
2022-08-08 13:27:42 | INFO | train_inner | epoch 020:    517 / 3715 loss=6.194, nll_loss=2.825, mask_ins=0.943, word_ins_ml=4.368, word_reposition=0.306, kpe=0.577, ppl=73.2, wps=4909.7, ups=0.33, wpb=14709.6, bsz=1024, num_updates=71000, lr=0.000132686, gnorm=1.805, clip=0, loss_scale=2048, train_wall=262, wall=204744
2022-08-08 13:32:41 | INFO | train_inner | epoch 020:    617 / 3715 loss=6.188, nll_loss=2.809, mask_ins=0.942, word_ins_ml=4.355, word_reposition=0.312, kpe=0.579, ppl=72.91, wps=4959.8, ups=0.33, wpb=14817, bsz=1024, num_updates=71100, lr=0.000132593, gnorm=1.793, clip=0, loss_scale=2048, train_wall=261, wall=205043
2022-08-08 13:37:26 | INFO | train_inner | epoch 020:    717 / 3715 loss=6.183, nll_loss=2.821, mask_ins=0.941, word_ins_ml=4.365, word_reposition=0.3, kpe=0.576, ppl=72.66, wps=5149, ups=0.35, wpb=14704.6, bsz=1024, num_updates=71200, lr=0.0001325, gnorm=1.797, clip=0, loss_scale=3113, train_wall=250, wall=205329
2022-08-08 13:41:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 13:42:23 | INFO | train_inner | epoch 020:    818 / 3715 loss=6.192, nll_loss=2.823, mask_ins=0.934, word_ins_ml=4.367, word_reposition=0.314, kpe=0.576, ppl=73.09, wps=4959.3, ups=0.34, wpb=14731.6, bsz=1024, num_updates=71300, lr=0.000132407, gnorm=1.815, clip=0, loss_scale=3569, train_wall=261, wall=205626
2022-08-08 13:45:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 13:47:21 | INFO | train_inner | epoch 020:    919 / 3715 loss=6.2, nll_loss=2.832, mask_ins=0.941, word_ins_ml=4.374, word_reposition=0.304, kpe=0.58, ppl=73.52, wps=4913.4, ups=0.34, wpb=14614, bsz=1024, num_updates=71400, lr=0.000132314, gnorm=1.812, clip=0, loss_scale=1561, train_wall=262, wall=205923
2022-08-08 13:52:12 | INFO | train_inner | epoch 020:   1019 / 3715 loss=6.215, nll_loss=2.83, mask_ins=0.947, word_ins_ml=4.372, word_reposition=0.314, kpe=0.583, ppl=74.31, wps=5029.5, ups=0.34, wpb=14650.5, bsz=1023.8, num_updates=71500, lr=0.000132221, gnorm=1.834, clip=0, loss_scale=1024, train_wall=256, wall=206214
2022-08-08 13:57:13 | INFO | train_inner | epoch 020:   1119 / 3715 loss=6.188, nll_loss=2.804, mask_ins=0.943, word_ins_ml=4.35, word_reposition=0.31, kpe=0.584, ppl=72.89, wps=4897.7, ups=0.33, wpb=14740.5, bsz=1024, num_updates=71600, lr=0.000132129, gnorm=1.829, clip=0, loss_scale=1024, train_wall=263, wall=206515
2022-08-08 14:02:05 | INFO | train_inner | epoch 020:   1219 / 3715 loss=6.211, nll_loss=2.836, mask_ins=0.942, word_ins_ml=4.378, word_reposition=0.306, kpe=0.585, ppl=74.07, wps=5051.8, ups=0.34, wpb=14742.7, bsz=1024, num_updates=71700, lr=0.000132037, gnorm=1.797, clip=0, loss_scale=1024, train_wall=256, wall=206807
2022-08-08 14:06:56 | INFO | train_inner | epoch 020:   1319 / 3715 loss=6.212, nll_loss=2.841, mask_ins=0.943, word_ins_ml=4.383, word_reposition=0.306, kpe=0.58, ppl=74.13, wps=5042.4, ups=0.34, wpb=14686.3, bsz=1024, num_updates=71800, lr=0.000131945, gnorm=1.803, clip=0, loss_scale=1024, train_wall=255, wall=207098
2022-08-08 14:11:50 | INFO | train_inner | epoch 020:   1419 / 3715 loss=6.209, nll_loss=2.834, mask_ins=0.941, word_ins_ml=4.376, word_reposition=0.309, kpe=0.583, ppl=73.97, wps=4974.9, ups=0.34, wpb=14641.2, bsz=1024, num_updates=71900, lr=0.000131853, gnorm=1.813, clip=0, loss_scale=1393, train_wall=259, wall=207393
2022-08-08 14:15:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 14:16:54 | INFO | train_inner | epoch 020:   1520 / 3715 loss=6.222, nll_loss=2.849, mask_ins=0.943, word_ins_ml=4.39, word_reposition=0.307, kpe=0.582, ppl=74.66, wps=4799.2, ups=0.33, wpb=14577.8, bsz=1024, num_updates=72000, lr=0.000131762, gnorm=1.806, clip=0, loss_scale=1795, train_wall=266, wall=207697
2022-08-08 14:21:48 | INFO | train_inner | epoch 020:   1620 / 3715 loss=6.22, nll_loss=2.833, mask_ins=0.94, word_ins_ml=4.376, word_reposition=0.317, kpe=0.588, ppl=74.56, wps=5007.4, ups=0.34, wpb=14694.4, bsz=1024, num_updates=72100, lr=0.00013167, gnorm=1.815, clip=0, loss_scale=1024, train_wall=257, wall=207990
2022-08-08 14:26:41 | INFO | train_inner | epoch 020:   1720 / 3715 loss=6.238, nll_loss=2.847, mask_ins=0.946, word_ins_ml=4.388, word_reposition=0.314, kpe=0.59, ppl=75.47, wps=5029.9, ups=0.34, wpb=14750.3, bsz=1024, num_updates=72200, lr=0.000131579, gnorm=1.806, clip=0, loss_scale=1024, train_wall=258, wall=208283
2022-08-08 14:31:32 | INFO | train_inner | epoch 020:   1820 / 3715 loss=6.227, nll_loss=2.856, mask_ins=0.945, word_ins_ml=4.396, word_reposition=0.305, kpe=0.582, ppl=74.91, wps=4984.8, ups=0.34, wpb=14494.7, bsz=1024, num_updates=72300, lr=0.000131488, gnorm=1.806, clip=0, loss_scale=1024, train_wall=256, wall=208574
2022-08-08 14:36:23 | INFO | train_inner | epoch 020:   1920 / 3715 loss=6.236, nll_loss=2.857, mask_ins=0.943, word_ins_ml=4.397, word_reposition=0.311, kpe=0.586, ppl=75.36, wps=5054.9, ups=0.34, wpb=14721.3, bsz=1024, num_updates=72400, lr=0.000131397, gnorm=1.795, clip=0, loss_scale=1024, train_wall=256, wall=208865
2022-08-08 14:41:14 | INFO | train_inner | epoch 020:   2020 / 3715 loss=6.214, nll_loss=2.835, mask_ins=0.944, word_ins_ml=4.377, word_reposition=0.309, kpe=0.585, ppl=74.24, wps=5009.9, ups=0.34, wpb=14566, bsz=1024, num_updates=72500, lr=0.000131306, gnorm=1.798, clip=0, loss_scale=1157, train_wall=256, wall=209156
2022-08-08 14:46:47 | INFO | train_inner | epoch 020:   2120 / 3715 loss=6.225, nll_loss=2.844, mask_ins=0.948, word_ins_ml=4.385, word_reposition=0.301, kpe=0.591, ppl=74.81, wps=4415.3, ups=0.3, wpb=14733.9, bsz=1024, num_updates=72600, lr=0.000131216, gnorm=1.814, clip=0, loss_scale=2048, train_wall=299, wall=209490
2022-08-08 14:52:22 | INFO | train_inner | epoch 020:   2220 / 3715 loss=6.216, nll_loss=2.84, mask_ins=0.942, word_ins_ml=4.382, word_reposition=0.303, kpe=0.59, ppl=74.35, wps=4375.7, ups=0.3, wpb=14631.9, bsz=1024, num_updates=72700, lr=0.000131126, gnorm=1.8, clip=0, loss_scale=2048, train_wall=297, wall=209824
2022-08-08 14:57:52 | INFO | train_inner | epoch 020:   2320 / 3715 loss=6.262, nll_loss=2.87, mask_ins=0.954, word_ins_ml=4.408, word_reposition=0.313, kpe=0.587, ppl=76.73, wps=4409.6, ups=0.3, wpb=14567.1, bsz=1024, num_updates=72800, lr=0.000131036, gnorm=1.8, clip=0, loss_scale=2048, train_wall=297, wall=210154
2022-08-08 15:02:39 | INFO | train_inner | epoch 020:   2420 / 3715 loss=6.238, nll_loss=2.861, mask_ins=0.941, word_ins_ml=4.4, word_reposition=0.309, kpe=0.588, ppl=75.49, wps=5120.2, ups=0.35, wpb=14695.4, bsz=1024, num_updates=72900, lr=0.000130946, gnorm=1.784, clip=0, loss_scale=2048, train_wall=254, wall=210441
2022-08-08 15:07:24 | INFO | train_inner | epoch 020:   2520 / 3715 loss=6.242, nll_loss=2.857, mask_ins=0.949, word_ins_ml=4.397, word_reposition=0.308, kpe=0.588, ppl=75.71, wps=5132.9, ups=0.35, wpb=14650.3, bsz=1024, num_updates=73000, lr=0.000130856, gnorm=1.797, clip=0, loss_scale=2068, train_wall=252, wall=210727
2022-08-08 15:08:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 15:12:07 | INFO | train_inner | epoch 020:   2621 / 3715 loss=6.23, nll_loss=2.848, mask_ins=0.947, word_ins_ml=4.389, word_reposition=0.309, kpe=0.586, ppl=75.05, wps=5133.2, ups=0.35, wpb=14520.4, bsz=1024, num_updates=73100, lr=0.000130766, gnorm=1.8, clip=0, loss_scale=2595, train_wall=251, wall=211010
2022-08-08 15:16:48 | INFO | train_inner | epoch 020:   2721 / 3715 loss=6.244, nll_loss=2.855, mask_ins=0.947, word_ins_ml=4.395, word_reposition=0.309, kpe=0.593, ppl=75.78, wps=5255, ups=0.36, wpb=14733.6, bsz=1024, num_updates=73200, lr=0.000130677, gnorm=1.784, clip=0, loss_scale=2048, train_wall=249, wall=211290
2022-08-08 15:21:27 | INFO | train_inner | epoch 020:   2821 / 3715 loss=6.238, nll_loss=2.847, mask_ins=0.946, word_ins_ml=4.388, word_reposition=0.308, kpe=0.596, ppl=75.5, wps=5272.7, ups=0.36, wpb=14725.2, bsz=1024, num_updates=73300, lr=0.000130588, gnorm=1.797, clip=0, loss_scale=2048, train_wall=248, wall=211569
2022-08-08 15:26:06 | INFO | train_inner | epoch 020:   2921 / 3715 loss=6.229, nll_loss=2.851, mask_ins=0.944, word_ins_ml=4.392, word_reposition=0.305, kpe=0.588, ppl=75.03, wps=5216.9, ups=0.36, wpb=14540.9, bsz=1024, num_updates=73400, lr=0.000130499, gnorm=1.792, clip=0, loss_scale=2048, train_wall=247, wall=211848
2022-08-08 15:30:46 | INFO | train_inner | epoch 020:   3021 / 3715 loss=6.231, nll_loss=2.847, mask_ins=0.944, word_ins_ml=4.387, word_reposition=0.306, kpe=0.594, ppl=75.09, wps=5255.1, ups=0.36, wpb=14715.7, bsz=1024, num_updates=73500, lr=0.00013041, gnorm=1.789, clip=0, loss_scale=2048, train_wall=249, wall=212128
2022-08-08 15:35:27 | INFO | train_inner | epoch 020:   3121 / 3715 loss=6.26, nll_loss=2.872, mask_ins=0.947, word_ins_ml=4.409, word_reposition=0.309, kpe=0.595, ppl=76.63, wps=5248.6, ups=0.36, wpb=14736.3, bsz=1024, num_updates=73600, lr=0.000130322, gnorm=1.799, clip=0, loss_scale=3318, train_wall=250, wall=212409
2022-08-08 15:39:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 15:40:10 | INFO | train_inner | epoch 020:   3222 / 3715 loss=6.24, nll_loss=2.863, mask_ins=0.945, word_ins_ml=4.401, word_reposition=0.302, kpe=0.592, ppl=75.58, wps=5195.6, ups=0.35, wpb=14721.8, bsz=1024, num_updates=73700, lr=0.000130233, gnorm=1.789, clip=0, loss_scale=3609, train_wall=251, wall=212692
2022-08-08 15:44:50 | INFO | train_inner | epoch 020:   3322 / 3715 loss=6.246, nll_loss=2.856, mask_ins=0.945, word_ins_ml=4.395, word_reposition=0.311, kpe=0.595, ppl=75.89, wps=5253.6, ups=0.36, wpb=14693.5, bsz=1024, num_updates=73800, lr=0.000130145, gnorm=1.787, clip=0, loss_scale=2048, train_wall=249, wall=212972
2022-08-08 15:49:28 | INFO | train_inner | epoch 020:   3422 / 3715 loss=6.235, nll_loss=2.848, mask_ins=0.944, word_ins_ml=4.389, word_reposition=0.31, kpe=0.592, ppl=75.32, wps=5265.7, ups=0.36, wpb=14644, bsz=1024, num_updates=73900, lr=0.000130057, gnorm=1.792, clip=0, loss_scale=2048, train_wall=247, wall=213250
2022-08-08 15:51:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 15:54:09 | INFO | train_inner | epoch 020:   3523 / 3715 loss=6.24, nll_loss=2.858, mask_ins=0.946, word_ins_ml=4.398, word_reposition=0.307, kpe=0.59, ppl=75.58, wps=5178.7, ups=0.35, wpb=14592.9, bsz=1024, num_updates=74000, lr=0.000129969, gnorm=1.789, clip=0, loss_scale=1450, train_wall=250, wall=213532
2022-08-08 15:58:49 | INFO | train_inner | epoch 020:   3623 / 3715 loss=6.255, nll_loss=2.864, mask_ins=0.946, word_ins_ml=4.402, word_reposition=0.312, kpe=0.594, ppl=76.38, wps=5259.5, ups=0.36, wpb=14690.7, bsz=1024, num_updates=74100, lr=0.000129881, gnorm=1.771, clip=0, loss_scale=1024, train_wall=248, wall=213811
2022-08-08 16:03:04 | INFO | train | epoch 020 | loss 6.22 | nll_loss 2.841 | mask_ins 0.944 | word_ins_ml 4.382 | word_reposition 0.308 | kpe 0.585 | ppl 74.53 | wps 4914.8 | ups 0.34 | wpb 14661.9 | bsz 1023.7 | num_updates 74192 | lr 0.000129801 | gnorm 1.802 | clip 0 | loss_scale 1816 | train_wall 9557 | wall 214066
2022-08-08 16:06:46 | INFO | valid | epoch 020 | valid on 'valid' subset | loss nan | nll_loss 3.144 | mask_ins 0.986 | word_ins_ml 4.694 | word_reposition 0.334 | kpe nan | ppl nan | wps 12354.3 | wpb 1849.4 | bsz 127.9 | num_updates 74192 | best_loss nan
2022-08-08 16:07:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 20 @ 74192 updates, score nan) (writing took 30.410609813407063 seconds)
2022-08-08 16:07:38 | INFO | train_inner | epoch 021:      8 / 3715 loss=6.239, nll_loss=2.843, mask_ins=0.947, word_ins_ml=4.384, word_reposition=0.315, kpe=0.592, ppl=75.53, wps=2728.6, ups=0.19, wpb=14441.2, bsz=1014.7, num_updates=74200, lr=0.000129794, gnorm=1.83, clip=0, loss_scale=1024, train_wall=246, wall=214340
2022-08-08 16:12:18 | INFO | train_inner | epoch 021:    108 / 3715 loss=6.143, nll_loss=2.787, mask_ins=0.934, word_ins_ml=4.335, word_reposition=0.308, kpe=0.566, ppl=70.69, wps=5273.6, ups=0.36, wpb=14737, bsz=1024, num_updates=74300, lr=0.000129706, gnorm=1.829, clip=0, loss_scale=1024, train_wall=248, wall=214620
2022-08-08 16:16:57 | INFO | train_inner | epoch 021:    208 / 3715 loss=6.149, nll_loss=2.794, mask_ins=0.937, word_ins_ml=4.341, word_reposition=0.308, kpe=0.563, ppl=70.98, wps=5211.9, ups=0.36, wpb=14559.9, bsz=1024, num_updates=74400, lr=0.000129619, gnorm=1.831, clip=0, loss_scale=1024, train_wall=248, wall=214899
2022-08-08 16:21:36 | INFO | train_inner | epoch 021:    308 / 3715 loss=6.16, nll_loss=2.808, mask_ins=0.939, word_ins_ml=4.353, word_reposition=0.302, kpe=0.565, ppl=71.5, wps=5251.9, ups=0.36, wpb=14668.1, bsz=1024, num_updates=74500, lr=0.000129532, gnorm=1.813, clip=0, loss_scale=1505, train_wall=248, wall=215179
2022-08-08 16:26:17 | INFO | train_inner | epoch 021:    408 / 3715 loss=6.146, nll_loss=2.786, mask_ins=0.937, word_ins_ml=4.334, word_reposition=0.306, kpe=0.569, ppl=70.83, wps=5273.2, ups=0.36, wpb=14784.3, bsz=1024, num_updates=74600, lr=0.000129445, gnorm=1.843, clip=0, loss_scale=2048, train_wall=249, wall=215459
2022-08-08 16:30:58 | INFO | train_inner | epoch 021:    508 / 3715 loss=6.146, nll_loss=2.79, mask_ins=0.939, word_ins_ml=4.337, word_reposition=0.305, kpe=0.566, ppl=70.83, wps=5221.8, ups=0.35, wpb=14721.2, bsz=1024, num_updates=74700, lr=0.000129358, gnorm=1.83, clip=0, loss_scale=2048, train_wall=251, wall=215741
2022-08-08 16:35:37 | INFO | train_inner | epoch 021:    608 / 3715 loss=6.158, nll_loss=2.808, mask_ins=0.936, word_ins_ml=4.354, word_reposition=0.302, kpe=0.567, ppl=71.42, wps=5289, ups=0.36, wpb=14734.7, bsz=1024, num_updates=74800, lr=0.000129272, gnorm=1.837, clip=0, loss_scale=2048, train_wall=248, wall=216019
2022-08-08 16:40:16 | INFO | train_inner | epoch 021:    708 / 3715 loss=6.158, nll_loss=2.798, mask_ins=0.936, word_ins_ml=4.345, word_reposition=0.306, kpe=0.572, ppl=71.43, wps=5257.7, ups=0.36, wpb=14655, bsz=1024, num_updates=74900, lr=0.000129186, gnorm=1.837, clip=0, loss_scale=2048, train_wall=248, wall=216298
2022-08-08 16:44:55 | INFO | train_inner | epoch 021:    808 / 3715 loss=6.154, nll_loss=2.8, mask_ins=0.934, word_ins_ml=4.346, word_reposition=0.305, kpe=0.569, ppl=71.22, wps=5261.3, ups=0.36, wpb=14682.2, bsz=1024, num_updates=75000, lr=0.000129099, gnorm=1.826, clip=0, loss_scale=2765, train_wall=248, wall=216577
2022-08-08 16:49:35 | INFO | train_inner | epoch 021:    908 / 3715 loss=6.169, nll_loss=2.815, mask_ins=0.933, word_ins_ml=4.359, word_reposition=0.307, kpe=0.57, ppl=71.96, wps=5219.8, ups=0.36, wpb=14620.6, bsz=1024, num_updates=75100, lr=0.000129013, gnorm=1.818, clip=0, loss_scale=4096, train_wall=249, wall=216857
2022-08-08 16:54:14 | INFO | train_inner | epoch 021:   1008 / 3715 loss=6.193, nll_loss=2.827, mask_ins=0.94, word_ins_ml=4.37, word_reposition=0.314, kpe=0.57, ppl=73.18, wps=5277.5, ups=0.36, wpb=14712, bsz=1024, num_updates=75200, lr=0.000128928, gnorm=1.829, clip=0, loss_scale=4096, train_wall=247, wall=217136
2022-08-08 16:58:53 | INFO | train_inner | epoch 021:   1108 / 3715 loss=6.159, nll_loss=2.799, mask_ins=0.931, word_ins_ml=4.346, word_reposition=0.309, kpe=0.573, ppl=71.47, wps=5272.6, ups=0.36, wpb=14703.8, bsz=1024, num_updates=75300, lr=0.000128842, gnorm=1.84, clip=0, loss_scale=4096, train_wall=247, wall=217415
2022-08-08 17:03:31 | INFO | train_inner | epoch 021:   1208 / 3715 loss=6.163, nll_loss=2.808, mask_ins=0.938, word_ins_ml=4.353, word_reposition=0.299, kpe=0.572, ppl=71.64, wps=5238.2, ups=0.36, wpb=14593, bsz=1024, num_updates=75400, lr=0.000128757, gnorm=1.844, clip=0, loss_scale=4096, train_wall=247, wall=217694
2022-08-08 17:05:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 17:08:12 | INFO | train_inner | epoch 021:   1309 / 3715 loss=6.172, nll_loss=2.818, mask_ins=0.94, word_ins_ml=4.362, word_reposition=0.302, kpe=0.569, ppl=72.12, wps=5165.7, ups=0.36, wpb=14510.2, bsz=1024, num_updates=75500, lr=0.000128671, gnorm=1.84, clip=0, loss_scale=2839, train_wall=249, wall=217974
2022-08-08 17:12:52 | INFO | train_inner | epoch 021:   1409 / 3715 loss=6.17, nll_loss=2.807, mask_ins=0.937, word_ins_ml=4.352, word_reposition=0.305, kpe=0.576, ppl=72, wps=5259.5, ups=0.36, wpb=14703.9, bsz=1024, num_updates=75600, lr=0.000128586, gnorm=1.831, clip=0, loss_scale=2048, train_wall=248, wall=218254
2022-08-08 17:14:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 17:17:34 | INFO | train_inner | epoch 021:   1510 / 3715 loss=6.167, nll_loss=2.807, mask_ins=0.938, word_ins_ml=4.352, word_reposition=0.304, kpe=0.573, ppl=71.86, wps=5204.6, ups=0.35, wpb=14700, bsz=1024, num_updates=75700, lr=0.000128501, gnorm=1.832, clip=0, loss_scale=1399, train_wall=251, wall=218536
2022-08-08 17:22:12 | INFO | train_inner | epoch 021:   1610 / 3715 loss=6.194, nll_loss=2.829, mask_ins=0.943, word_ins_ml=4.372, word_reposition=0.304, kpe=0.576, ppl=73.22, wps=5282.5, ups=0.36, wpb=14683.8, bsz=1024, num_updates=75800, lr=0.000128416, gnorm=1.858, clip=0, loss_scale=1024, train_wall=247, wall=218814
2022-08-08 17:26:51 | INFO | train_inner | epoch 021:   1710 / 3715 loss=6.17, nll_loss=2.812, mask_ins=0.939, word_ins_ml=4.357, word_reposition=0.299, kpe=0.574, ppl=72, wps=5210, ups=0.36, wpb=14529.3, bsz=1024, num_updates=75900, lr=0.000128332, gnorm=1.873, clip=0, loss_scale=1024, train_wall=247, wall=219093
2022-08-08 17:31:30 | INFO | train_inner | epoch 021:   1810 / 3715 loss=6.216, nll_loss=2.848, mask_ins=0.939, word_ins_ml=4.389, word_reposition=0.309, kpe=0.579, ppl=74.32, wps=5289.7, ups=0.36, wpb=14780.2, bsz=1024, num_updates=76000, lr=0.000128247, gnorm=1.852, clip=0, loss_scale=1024, train_wall=248, wall=219373
2022-08-08 17:36:09 | INFO | train_inner | epoch 021:   1910 / 3715 loss=6.181, nll_loss=2.817, mask_ins=0.94, word_ins_ml=4.361, word_reposition=0.309, kpe=0.572, ppl=72.57, wps=5197.8, ups=0.36, wpb=14490.3, bsz=1024, num_updates=76100, lr=0.000128163, gnorm=1.823, clip=0, loss_scale=1024, train_wall=247, wall=219652
2022-08-08 17:40:48 | INFO | train_inner | epoch 021:   2010 / 3715 loss=6.172, nll_loss=2.812, mask_ins=0.934, word_ins_ml=4.357, word_reposition=0.307, kpe=0.576, ppl=72.13, wps=5245, ups=0.36, wpb=14640.4, bsz=1023.8, num_updates=76200, lr=0.000128079, gnorm=1.833, clip=0, loss_scale=1556, train_wall=248, wall=219931
2022-08-08 17:45:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 17:45:31 | INFO | train_inner | epoch 021:   2111 / 3715 loss=6.199, nll_loss=2.824, mask_ins=0.941, word_ins_ml=4.367, word_reposition=0.311, kpe=0.58, ppl=73.46, wps=5195, ups=0.35, wpb=14668.6, bsz=1024, num_updates=76300, lr=0.000127995, gnorm=1.841, clip=0, loss_scale=2007, train_wall=251, wall=220213
2022-08-08 17:50:10 | INFO | train_inner | epoch 021:   2211 / 3715 loss=6.189, nll_loss=2.827, mask_ins=0.937, word_ins_ml=4.37, word_reposition=0.301, kpe=0.581, ppl=72.94, wps=5272.7, ups=0.36, wpb=14747.8, bsz=1024, num_updates=76400, lr=0.000127911, gnorm=1.811, clip=0, loss_scale=1024, train_wall=248, wall=220493
2022-08-08 17:54:49 | INFO | train_inner | epoch 021:   2311 / 3715 loss=6.198, nll_loss=2.829, mask_ins=0.94, word_ins_ml=4.371, word_reposition=0.309, kpe=0.578, ppl=73.42, wps=5264.8, ups=0.36, wpb=14671.7, bsz=1024, num_updates=76500, lr=0.000127827, gnorm=1.819, clip=0, loss_scale=1024, train_wall=247, wall=220771
2022-08-08 17:59:28 | INFO | train_inner | epoch 021:   2411 / 3715 loss=6.192, nll_loss=2.819, mask_ins=0.943, word_ins_ml=4.363, word_reposition=0.306, kpe=0.58, ppl=73.09, wps=5273.6, ups=0.36, wpb=14695.1, bsz=1024, num_updates=76600, lr=0.000127744, gnorm=1.858, clip=0, loss_scale=1024, train_wall=247, wall=221050
2022-08-08 18:04:06 | INFO | train_inner | epoch 021:   2511 / 3715 loss=6.195, nll_loss=2.821, mask_ins=0.942, word_ins_ml=4.365, word_reposition=0.309, kpe=0.579, ppl=73.27, wps=5253.4, ups=0.36, wpb=14646.2, bsz=1024, num_updates=76700, lr=0.000127661, gnorm=1.831, clip=0, loss_scale=1024, train_wall=247, wall=221329
2022-08-08 18:08:46 | INFO | train_inner | epoch 021:   2611 / 3715 loss=6.185, nll_loss=2.82, mask_ins=0.941, word_ins_ml=4.363, word_reposition=0.3, kpe=0.581, ppl=72.76, wps=5262.7, ups=0.36, wpb=14689, bsz=1024, num_updates=76800, lr=0.000127578, gnorm=1.833, clip=0, loss_scale=1024, train_wall=248, wall=221608
2022-08-08 18:13:25 | INFO | train_inner | epoch 021:   2711 / 3715 loss=6.209, nll_loss=2.844, mask_ins=0.941, word_ins_ml=4.385, word_reposition=0.303, kpe=0.58, ppl=74, wps=5222.9, ups=0.36, wpb=14608, bsz=1024, num_updates=76900, lr=0.000127495, gnorm=1.837, clip=0, loss_scale=1966, train_wall=248, wall=221888
2022-08-08 18:18:06 | INFO | train_inner | epoch 021:   2811 / 3715 loss=6.206, nll_loss=2.829, mask_ins=0.942, word_ins_ml=4.372, word_reposition=0.31, kpe=0.582, ppl=73.82, wps=5228.7, ups=0.36, wpb=14662.2, bsz=1024, num_updates=77000, lr=0.000127412, gnorm=1.828, clip=0, loss_scale=2048, train_wall=249, wall=222168
2022-08-08 18:22:46 | INFO | train_inner | epoch 021:   2911 / 3715 loss=6.184, nll_loss=2.823, mask_ins=0.938, word_ins_ml=4.366, word_reposition=0.301, kpe=0.579, ppl=72.73, wps=5207.4, ups=0.36, wpb=14597.2, bsz=1024, num_updates=77100, lr=0.000127329, gnorm=1.814, clip=0, loss_scale=2048, train_wall=249, wall=222448
2022-08-08 18:27:25 | INFO | train_inner | epoch 021:   3011 / 3715 loss=6.223, nll_loss=2.842, mask_ins=0.947, word_ins_ml=4.384, word_reposition=0.308, kpe=0.584, ppl=74.71, wps=5263.4, ups=0.36, wpb=14678.5, bsz=1024, num_updates=77200, lr=0.000127247, gnorm=1.828, clip=0, loss_scale=2048, train_wall=248, wall=222727
2022-08-08 18:32:04 | INFO | train_inner | epoch 021:   3111 / 3715 loss=6.217, nll_loss=2.841, mask_ins=0.941, word_ins_ml=4.382, word_reposition=0.31, kpe=0.584, ppl=74.37, wps=5261.3, ups=0.36, wpb=14672.5, bsz=1024, num_updates=77300, lr=0.000127164, gnorm=1.83, clip=0, loss_scale=2048, train_wall=247, wall=223006
2022-08-08 18:34:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 18:36:46 | INFO | train_inner | epoch 021:   3212 / 3715 loss=6.225, nll_loss=2.849, mask_ins=0.948, word_ins_ml=4.389, word_reposition=0.304, kpe=0.584, ppl=74.79, wps=5168.1, ups=0.35, wpb=14609.5, bsz=1024, num_updates=77400, lr=0.000127082, gnorm=1.824, clip=0, loss_scale=2778, train_wall=251, wall=223289
2022-08-08 18:41:25 | INFO | train_inner | epoch 021:   3312 / 3715 loss=6.206, nll_loss=2.828, mask_ins=0.944, word_ins_ml=4.371, word_reposition=0.308, kpe=0.583, ppl=73.82, wps=5254.8, ups=0.36, wpb=14633.5, bsz=1024, num_updates=77500, lr=0.000127, gnorm=1.819, clip=0, loss_scale=2048, train_wall=247, wall=223567
2022-08-08 18:46:05 | INFO | train_inner | epoch 021:   3412 / 3715 loss=6.217, nll_loss=2.842, mask_ins=0.939, word_ins_ml=4.383, word_reposition=0.311, kpe=0.584, ppl=74.38, wps=5273.2, ups=0.36, wpb=14751.9, bsz=1024, num_updates=77600, lr=0.000126918, gnorm=1.832, clip=0, loss_scale=2048, train_wall=249, wall=223847
2022-08-08 18:50:45 | INFO | train_inner | epoch 021:   3512 / 3715 loss=6.206, nll_loss=2.834, mask_ins=0.942, word_ins_ml=4.376, word_reposition=0.303, kpe=0.585, ppl=73.8, wps=5239.4, ups=0.36, wpb=14671.1, bsz=1024, num_updates=77700, lr=0.000126837, gnorm=1.812, clip=0, loss_scale=2048, train_wall=249, wall=224127
2022-08-08 18:55:24 | INFO | train_inner | epoch 021:   3612 / 3715 loss=6.184, nll_loss=2.811, mask_ins=0.935, word_ins_ml=4.356, word_reposition=0.308, kpe=0.586, ppl=72.72, wps=5248.9, ups=0.36, wpb=14667.3, bsz=1024, num_updates=77800, lr=0.000126755, gnorm=1.838, clip=0, loss_scale=2048, train_wall=248, wall=224407
2022-08-08 19:00:03 | INFO | train_inner | epoch 021:   3712 / 3715 loss=6.227, nll_loss=2.847, mask_ins=0.941, word_ins_ml=4.388, word_reposition=0.31, kpe=0.588, ppl=74.89, wps=5279.1, ups=0.36, wpb=14711.8, bsz=1024, num_updates=77900, lr=0.000126674, gnorm=1.807, clip=0, loss_scale=2724, train_wall=247, wall=224685
2022-08-08 19:00:09 | INFO | train | epoch 021 | loss 6.184 | nll_loss 2.819 | mask_ins 0.939 | word_ins_ml 4.363 | word_reposition 0.306 | kpe 0.576 | ppl 72.7 | wps 5120.7 | ups 0.35 | wpb 14661.4 | bsz 1023.7 | num_updates 77903 | lr 0.000126671 | gnorm 1.833 | clip 0 | loss_scale 1995 | train_wall 9208 | wall 224692
2022-08-08 19:03:50 | INFO | valid | epoch 021 | valid on 'valid' subset | loss nan | nll_loss 3.147 | mask_ins 0.991 | word_ins_ml 4.701 | word_reposition 0.359 | kpe nan | ppl nan | wps 12410.3 | wpb 1849.4 | bsz 127.9 | num_updates 77903 | best_loss nan
2022-08-08 19:03:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 21 @ 77903 updates, score nan) (writing took 8.279606243595481 seconds)
2022-08-08 19:08:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 19:08:32 | INFO | train_inner | epoch 022:     98 / 3715 loss=6.098, nll_loss=2.759, mask_ins=0.931, word_ins_ml=4.31, word_reposition=0.303, kpe=0.554, ppl=68.49, wps=2845.9, ups=0.2, wpb=14494.8, bsz=1014.7, num_updates=78000, lr=0.000126592, gnorm=1.877, clip=0, loss_scale=3853, train_wall=249, wall=225195
2022-08-08 19:13:11 | INFO | train_inner | epoch 022:    198 / 3715 loss=6.103, nll_loss=2.77, mask_ins=0.932, word_ins_ml=4.319, word_reposition=0.298, kpe=0.554, ppl=68.72, wps=5246.1, ups=0.36, wpb=14627.6, bsz=1024, num_updates=78100, lr=0.000126511, gnorm=1.837, clip=0, loss_scale=2048, train_wall=248, wall=225473
2022-08-08 19:17:51 | INFO | train_inner | epoch 022:    298 / 3715 loss=6.11, nll_loss=2.771, mask_ins=0.931, word_ins_ml=4.32, word_reposition=0.301, kpe=0.558, ppl=69.07, wps=5250.8, ups=0.36, wpb=14711.2, bsz=1024, num_updates=78200, lr=0.00012643, gnorm=1.845, clip=0, loss_scale=2048, train_wall=249, wall=225754
2022-08-08 19:22:30 | INFO | train_inner | epoch 022:    398 / 3715 loss=6.109, nll_loss=2.768, mask_ins=0.935, word_ins_ml=4.318, word_reposition=0.302, kpe=0.555, ppl=69.02, wps=5238.3, ups=0.36, wpb=14625.1, bsz=1024, num_updates=78300, lr=0.00012635, gnorm=1.864, clip=0, loss_scale=2048, train_wall=248, wall=226033
2022-08-08 19:27:11 | INFO | train_inner | epoch 022:    498 / 3715 loss=6.134, nll_loss=2.791, mask_ins=0.933, word_ins_ml=4.338, word_reposition=0.307, kpe=0.556, ppl=70.21, wps=5212.1, ups=0.36, wpb=14599.2, bsz=1024, num_updates=78400, lr=0.000126269, gnorm=1.864, clip=0, loss_scale=2048, train_wall=249, wall=226313
2022-08-08 19:31:50 | INFO | train_inner | epoch 022:    598 / 3715 loss=6.109, nll_loss=2.762, mask_ins=0.937, word_ins_ml=4.312, word_reposition=0.301, kpe=0.558, ppl=69.02, wps=5259, ups=0.36, wpb=14684.5, bsz=1024, num_updates=78500, lr=0.000126189, gnorm=1.865, clip=0, loss_scale=2048, train_wall=248, wall=226592
2022-08-08 19:36:29 | INFO | train_inner | epoch 022:    698 / 3715 loss=6.11, nll_loss=2.768, mask_ins=0.935, word_ins_ml=4.318, word_reposition=0.3, kpe=0.557, ppl=69.06, wps=5240.2, ups=0.36, wpb=14635.4, bsz=1024, num_updates=78600, lr=0.000126108, gnorm=1.851, clip=0, loss_scale=4096, train_wall=249, wall=226871
2022-08-08 19:37:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 19:41:12 | INFO | train_inner | epoch 022:    799 / 3715 loss=6.106, nll_loss=2.77, mask_ins=0.929, word_ins_ml=4.319, word_reposition=0.298, kpe=0.559, ppl=68.88, wps=5205.9, ups=0.35, wpb=14706.9, bsz=1024, num_updates=78700, lr=0.000126028, gnorm=1.841, clip=0, loss_scale=2413, train_wall=251, wall=227154
2022-08-08 19:45:52 | INFO | train_inner | epoch 022:    899 / 3715 loss=6.123, nll_loss=2.778, mask_ins=0.933, word_ins_ml=4.327, word_reposition=0.306, kpe=0.557, ppl=69.68, wps=5186.3, ups=0.36, wpb=14558, bsz=1024, num_updates=78800, lr=0.000125948, gnorm=1.869, clip=0, loss_scale=2048, train_wall=250, wall=227435
2022-08-08 19:50:31 | INFO | train_inner | epoch 022:    999 / 3715 loss=6.143, nll_loss=2.79, mask_ins=0.935, word_ins_ml=4.337, word_reposition=0.308, kpe=0.562, ppl=70.65, wps=5261.7, ups=0.36, wpb=14659, bsz=1024, num_updates=78900, lr=0.000125868, gnorm=1.881, clip=0, loss_scale=2048, train_wall=247, wall=227713
2022-08-08 19:55:10 | INFO | train_inner | epoch 022:   1099 / 3715 loss=6.144, nll_loss=2.796, mask_ins=0.934, word_ins_ml=4.343, word_reposition=0.303, kpe=0.565, ppl=70.73, wps=5264.2, ups=0.36, wpb=14702.3, bsz=1024, num_updates=79000, lr=0.000125789, gnorm=1.865, clip=0, loss_scale=2048, train_wall=248, wall=227993
2022-08-08 19:59:49 | INFO | train_inner | epoch 022:   1199 / 3715 loss=6.145, nll_loss=2.798, mask_ins=0.936, word_ins_ml=4.344, word_reposition=0.305, kpe=0.561, ppl=70.77, wps=5264.6, ups=0.36, wpb=14702.2, bsz=1024, num_updates=79100, lr=0.000125709, gnorm=1.864, clip=0, loss_scale=2048, train_wall=248, wall=228272
2022-08-08 20:04:28 | INFO | train_inner | epoch 022:   1299 / 3715 loss=6.152, nll_loss=2.797, mask_ins=0.934, word_ins_ml=4.343, word_reposition=0.309, kpe=0.566, ppl=71.1, wps=5288.9, ups=0.36, wpb=14733.2, bsz=1024, num_updates=79200, lr=0.00012563, gnorm=1.865, clip=0, loss_scale=3502, train_wall=247, wall=228550
2022-08-08 20:09:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 20:09:11 | INFO | train_inner | epoch 022:   1400 / 3715 loss=6.155, nll_loss=2.805, mask_ins=0.933, word_ins_ml=4.35, word_reposition=0.309, kpe=0.564, ppl=71.28, wps=5198, ups=0.35, wpb=14722.8, bsz=1024, num_updates=79300, lr=0.00012555, gnorm=1.858, clip=0, loss_scale=3995, train_wall=252, wall=228834
2022-08-08 20:13:51 | INFO | train_inner | epoch 022:   1500 / 3715 loss=6.143, nll_loss=2.796, mask_ins=0.932, word_ins_ml=4.342, word_reposition=0.304, kpe=0.565, ppl=70.67, wps=5233.4, ups=0.36, wpb=14639.3, bsz=1024, num_updates=79400, lr=0.000125471, gnorm=1.872, clip=0, loss_scale=2048, train_wall=248, wall=229113
2022-08-08 20:18:31 | INFO | train_inner | epoch 022:   1600 / 3715 loss=6.142, nll_loss=2.795, mask_ins=0.931, word_ins_ml=4.342, word_reposition=0.306, kpe=0.562, ppl=70.6, wps=5184.8, ups=0.36, wpb=14497.9, bsz=1024, num_updates=79500, lr=0.000125392, gnorm=1.876, clip=0, loss_scale=2048, train_wall=248, wall=229393
2022-08-08 20:23:10 | INFO | train_inner | epoch 022:   1700 / 3715 loss=6.155, nll_loss=2.791, mask_ins=0.936, word_ins_ml=4.338, word_reposition=0.312, kpe=0.568, ppl=71.28, wps=5259.8, ups=0.36, wpb=14674.5, bsz=1024, num_updates=79600, lr=0.000125314, gnorm=1.865, clip=0, loss_scale=2048, train_wall=248, wall=229672
2022-08-08 20:27:47 | INFO | train_inner | epoch 022:   1800 / 3715 loss=6.165, nll_loss=2.808, mask_ins=0.939, word_ins_ml=4.352, word_reposition=0.307, kpe=0.567, ppl=71.77, wps=5285.1, ups=0.36, wpb=14684.6, bsz=1024, num_updates=79700, lr=0.000125235, gnorm=1.863, clip=0, loss_scale=2048, train_wall=247, wall=229950
2022-08-08 20:32:26 | INFO | train_inner | epoch 022:   1900 / 3715 loss=6.149, nll_loss=2.798, mask_ins=0.935, word_ins_ml=4.345, word_reposition=0.303, kpe=0.567, ppl=70.94, wps=5223.9, ups=0.36, wpb=14560.3, bsz=1024, num_updates=79800, lr=0.000125157, gnorm=1.863, clip=0, loss_scale=2048, train_wall=248, wall=230229
2022-08-08 20:34:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 20:37:08 | INFO | train_inner | epoch 022:   2001 / 3715 loss=6.166, nll_loss=2.813, mask_ins=0.933, word_ins_ml=4.357, word_reposition=0.307, kpe=0.569, ppl=71.79, wps=5216.1, ups=0.36, wpb=14689.5, bsz=1024, num_updates=79900, lr=0.000125078, gnorm=1.875, clip=0, loss_scale=2636, train_wall=250, wall=230510
2022-08-08 20:41:46 | INFO | train_inner | epoch 022:   2101 / 3715 loss=6.153, nll_loss=2.795, mask_ins=0.938, word_ins_ml=4.342, word_reposition=0.307, kpe=0.567, ppl=71.18, wps=5279, ups=0.36, wpb=14700.1, bsz=1024, num_updates=80000, lr=0.000125, gnorm=1.842, clip=0, loss_scale=2048, train_wall=247, wall=230789
2022-08-08 20:46:25 | INFO | train_inner | epoch 022:   2201 / 3715 loss=6.147, nll_loss=2.794, mask_ins=0.933, word_ins_ml=4.34, word_reposition=0.302, kpe=0.571, ppl=70.84, wps=5271.1, ups=0.36, wpb=14717.8, bsz=1024, num_updates=80100, lr=0.000124922, gnorm=1.852, clip=0, loss_scale=2048, train_wall=248, wall=231068
2022-08-08 20:47:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-08 20:51:07 | INFO | train_inner | epoch 022:   2302 / 3715 loss=6.158, nll_loss=2.812, mask_ins=0.934, word_ins_ml=4.356, word_reposition=0.3, kpe=0.568, ppl=71.42, wps=5202.2, ups=0.36, wpb=14623.1, bsz=1024, num_updates=80200, lr=0.000124844, gnorm=1.854, clip=0, loss_scale=1186, train_wall=250, wall=231349
2022-08-08 20:55:46 | INFO | train_inner | epoch 022:   2402 / 3715 loss=6.161, nll_loss=2.794, mask_ins=0.941, word_ins_ml=4.34, word_reposition=0.306, kpe=0.574, ppl=71.56, wps=5254.4, ups=0.36, wpb=14672.5, bsz=1024, num_updates=80300, lr=0.000124766, gnorm=1.843, clip=0, loss_scale=1024, train_wall=248, wall=231628
2022-08-08 21:00:24 | INFO | train_inner | epoch 022:   2502 / 3715 loss=6.153, nll_loss=2.798, mask_ins=0.934, word_ins_ml=4.344, word_reposition=0.304, kpe=0.571, ppl=71.18, wps=5281.5, ups=0.36, wpb=14716.9, bsz=1024, num_updates=80400, lr=0.000124689, gnorm=1.891, clip=0, loss_scale=1024, train_wall=247, wall=231907
2022-08-08 21:04:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-08 21:05:07 | INFO | train_inner | epoch 022:   2603 / 3715 loss=6.167, nll_loss=2.805, mask_ins=0.936, word_ins_ml=4.35, word_reposition=0.307, kpe=0.573, ppl=71.87, wps=5200.1, ups=0.35, wpb=14685.5, bsz=1024, num_updates=80500, lr=0.000124611, gnorm=1.981, clip=0, loss_scale=994, train_wall=251, wall=232189
2022-08-08 21:09:47 | INFO | train_inner | epoch 022:   2703 / 3715 loss=6.17, nll_loss=2.806, mask_ins=0.94, word_ins_ml=4.351, word_reposition=0.306, kpe=0.573, ppl=71.99, wps=5228.4, ups=0.36, wpb=14628, bsz=1024, num_updates=80600, lr=0.000124534, gnorm=2.001, clip=0, loss_scale=512, train_wall=249, wall=232469
2022-08-08 21:14:25 | INFO | train_inner | epoch 022:   2803 / 3715 loss=6.188, nll_loss=2.825, mask_ins=0.942, word_ins_ml=4.368, word_reposition=0.305, kpe=0.573, ppl=72.92, wps=5251.8, ups=0.36, wpb=14617.8, bsz=1024, num_updates=80700, lr=0.000124457, gnorm=1.862, clip=0, loss_scale=512, train_wall=247, wall=232747
2022-08-08 21:19:05 | INFO | train_inner | epoch 022:   2903 / 3715 loss=6.153, nll_loss=2.794, mask_ins=0.934, word_ins_ml=4.341, word_reposition=0.304, kpe=0.574, ppl=71.14, wps=5240.6, ups=0.36, wpb=14663, bsz=1024, num_updates=80800, lr=0.00012438, gnorm=1.849, clip=0, loss_scale=512, train_wall=249, wall=233027
2022-08-08 21:23:44 | INFO | train_inner | epoch 022:   3003 / 3715 loss=6.152, nll_loss=2.794, mask_ins=0.938, word_ins_ml=4.34, word_reposition=0.302, kpe=0.572, ppl=71.12, wps=5245.2, ups=0.36, wpb=14641.9, bsz=1024, num_updates=80900, lr=0.000124303, gnorm=1.849, clip=0, loss_scale=512, train_wall=248, wall=233306
2022-08-08 21:28:24 | INFO | train_inner | epoch 022:   3103 / 3715 loss=6.192, nll_loss=2.818, mask_ins=0.941, word_ins_ml=4.362, word_reposition=0.306, kpe=0.583, ppl=73.12, wps=5255.9, ups=0.36, wpb=14717.2, bsz=1024, num_updates=81000, lr=0.000124226, gnorm=3.005, clip=0, loss_scale=512, train_wall=249, wall=233586
2022-08-08 21:33:04 | INFO | train_inner | epoch 022:   3203 / 3715 loss=6.184, nll_loss=2.809, mask_ins=0.937, word_ins_ml=4.354, word_reposition=0.31, kpe=0.582, ppl=72.68, wps=5273.3, ups=0.36, wpb=14745.3, bsz=1024, num_updates=81100, lr=0.000124149, gnorm=2.13, clip=0, loss_scale=993, train_wall=248, wall=233866
2022-08-08 21:37:42 | INFO | train_inner | epoch 022:   3303 / 3715 loss=6.145, nll_loss=2.789, mask_ins=0.931, word_ins_ml=4.336, word_reposition=0.302, kpe=0.576, ppl=70.75, wps=5295, ups=0.36, wpb=14745, bsz=1024, num_updates=81200, lr=0.000124073, gnorm=1.873, clip=0, loss_scale=1024, train_wall=247, wall=234144
2022-08-08 21:38:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-08 21:42:25 | INFO | train_inner | epoch 022:   3404 / 3715 loss=6.154, nll_loss=2.791, mask_ins=0.939, word_ins_ml=4.338, word_reposition=0.303, kpe=0.574, ppl=71.19, wps=5163.5, ups=0.35, wpb=14600.1, bsz=1023.8, num_updates=81300, lr=0.000123997, gnorm=1.866, clip=0, loss_scale=542, train_wall=251, wall=234427
2022-08-08 21:47:04 | INFO | train_inner | epoch 022:   3504 / 3715 loss=6.177, nll_loss=2.809, mask_ins=0.938, word_ins_ml=4.353, word_reposition=0.308, kpe=0.578, ppl=72.35, wps=5248.7, ups=0.36, wpb=14635.9, bsz=1024, num_updates=81400, lr=0.00012392, gnorm=1.883, clip=0, loss_scale=512, train_wall=248, wall=234706
2022-08-08 21:51:43 | INFO | train_inner | epoch 022:   3604 / 3715 loss=6.182, nll_loss=2.808, mask_ins=0.94, word_ins_ml=4.353, word_reposition=0.31, kpe=0.579, ppl=72.6, wps=5282.3, ups=0.36, wpb=14774.6, bsz=1024, num_updates=81500, lr=0.000123844, gnorm=1.854, clip=0, loss_scale=512, train_wall=248, wall=234986
2022-08-08 21:56:23 | INFO | train_inner | epoch 022:   3704 / 3715 loss=6.159, nll_loss=2.797, mask_ins=0.937, word_ins_ml=4.343, word_reposition=0.302, kpe=0.576, ppl=71.44, wps=5253.8, ups=0.36, wpb=14714.8, bsz=1024, num_updates=81600, lr=0.000123768, gnorm=1.845, clip=0, loss_scale=512, train_wall=249, wall=235266
2022-08-08 21:56:52 | INFO | train | epoch 022 | loss 6.148 | nll_loss 2.794 | mask_ins 0.935 | word_ins_ml 4.34 | word_reposition 0.305 | kpe 0.567 | ppl 70.89 | wps 5127.6 | ups 0.35 | wpb 14662.2 | bsz 1023.7 | num_updates 81611 | lr 0.00012376 | gnorm 1.906 | clip 0 | loss_scale 1729 | train_wall 9212 | wall 235294
2022-08-08 22:00:33 | INFO | valid | epoch 022 | valid on 'valid' subset | loss nan | nll_loss 3.153 | mask_ins 0.989 | word_ins_ml 4.703 | word_reposition 0.35 | kpe nan | ppl nan | wps 12390.8 | wpb 1849.4 | bsz 127.9 | num_updates 81611 | best_loss nan
2022-08-08 22:00:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 22 @ 81611 updates, score nan) (writing took 8.061392726376653 seconds)
2022-08-08 22:04:50 | INFO | train_inner | epoch 023:     89 / 3715 loss=6.062, nll_loss=2.736, mask_ins=0.924, word_ins_ml=4.29, word_reposition=0.301, kpe=0.547, ppl=66.83, wps=2860.1, ups=0.2, wpb=14487.6, bsz=1014.7, num_updates=81700, lr=0.000123693, gnorm=1.916, clip=0, loss_scale=512, train_wall=246, wall=235772
2022-08-08 22:09:29 | INFO | train_inner | epoch 023:    189 / 3715 loss=6.075, nll_loss=2.749, mask_ins=0.924, word_ins_ml=4.301, word_reposition=0.306, kpe=0.544, ppl=67.42, wps=5239.2, ups=0.36, wpb=14593.3, bsz=1024, num_updates=81800, lr=0.000123617, gnorm=1.871, clip=0, loss_scale=937, train_wall=248, wall=236051
2022-08-08 22:14:09 | INFO | train_inner | epoch 023:    289 / 3715 loss=6.096, nll_loss=2.76, mask_ins=0.933, word_ins_ml=4.311, word_reposition=0.303, kpe=0.549, ppl=68.39, wps=5230.7, ups=0.36, wpb=14653.1, bsz=1024, num_updates=81900, lr=0.000123542, gnorm=1.887, clip=0, loss_scale=1024, train_wall=249, wall=236331
2022-08-08 22:18:48 | INFO | train_inner | epoch 023:    389 / 3715 loss=6.083, nll_loss=2.756, mask_ins=0.922, word_ins_ml=4.307, word_reposition=0.306, kpe=0.547, ppl=67.77, wps=5237.7, ups=0.36, wpb=14624.1, bsz=1024, num_updates=82000, lr=0.000123466, gnorm=1.91, clip=0, loss_scale=1024, train_wall=248, wall=236610
2022-08-08 22:23:28 | INFO | train_inner | epoch 023:    489 / 3715 loss=6.079, nll_loss=2.752, mask_ins=0.929, word_ins_ml=4.303, word_reposition=0.302, kpe=0.544, ppl=67.59, wps=5166.9, ups=0.36, wpb=14469.1, bsz=1024, num_updates=82100, lr=0.000123391, gnorm=1.885, clip=0, loss_scale=1024, train_wall=249, wall=236890
2022-08-08 22:28:07 | INFO | train_inner | epoch 023:    589 / 3715 loss=6.117, nll_loss=2.79, mask_ins=0.932, word_ins_ml=4.337, word_reposition=0.299, kpe=0.548, ppl=69.39, wps=5258.7, ups=0.36, wpb=14650.2, bsz=1024, num_updates=82200, lr=0.000123316, gnorm=1.889, clip=0, loss_scale=1024, train_wall=248, wall=237169
2022-08-08 22:32:46 | INFO | train_inner | epoch 023:    689 / 3715 loss=6.111, nll_loss=2.773, mask_ins=0.931, word_ins_ml=4.322, word_reposition=0.306, kpe=0.552, ppl=69.1, wps=5256.2, ups=0.36, wpb=14687.5, bsz=1024, num_updates=82300, lr=0.000123241, gnorm=1.888, clip=0, loss_scale=1751, train_wall=248, wall=237448
2022-08-08 22:37:24 | INFO | train_inner | epoch 023:    789 / 3715 loss=6.113, nll_loss=2.784, mask_ins=0.93, word_ins_ml=4.332, word_reposition=0.301, kpe=0.549, ppl=69.22, wps=5258.3, ups=0.36, wpb=14622.3, bsz=1024, num_updates=82400, lr=0.000123166, gnorm=1.882, clip=0, loss_scale=2048, train_wall=247, wall=237726
2022-08-08 22:42:03 | INFO | train_inner | epoch 023:    889 / 3715 loss=6.109, nll_loss=2.767, mask_ins=0.93, word_ins_ml=4.317, word_reposition=0.309, kpe=0.554, ppl=69.03, wps=5255.1, ups=0.36, wpb=14654.5, bsz=1024, num_updates=82500, lr=0.000123091, gnorm=1.878, clip=0, loss_scale=2048, train_wall=248, wall=238005
2022-08-08 22:46:43 | INFO | train_inner | epoch 023:    989 / 3715 loss=6.116, nll_loss=2.78, mask_ins=0.93, word_ins_ml=4.328, word_reposition=0.304, kpe=0.554, ppl=69.37, wps=5247.1, ups=0.36, wpb=14690.9, bsz=1024, num_updates=82600, lr=0.000123017, gnorm=1.875, clip=0, loss_scale=2048, train_wall=248, wall=238285
2022-08-08 22:51:23 | INFO | train_inner | epoch 023:   1089 / 3715 loss=6.103, nll_loss=2.761, mask_ins=0.926, word_ins_ml=4.312, word_reposition=0.31, kpe=0.556, ppl=68.74, wps=5263.5, ups=0.36, wpb=14719.2, bsz=1024, num_updates=82700, lr=0.000122943, gnorm=1.875, clip=0, loss_scale=2048, train_wall=248, wall=238565
2022-08-08 22:56:01 | INFO | train_inner | epoch 023:   1189 / 3715 loss=6.103, nll_loss=2.766, mask_ins=0.925, word_ins_ml=4.316, word_reposition=0.306, kpe=0.556, ppl=68.74, wps=5270, ups=0.36, wpb=14672.1, bsz=1024, num_updates=82800, lr=0.000122868, gnorm=1.88, clip=0, loss_scale=3256, train_wall=247, wall=238843
2022-08-08 23:00:40 | INFO | train_inner | epoch 023:   1289 / 3715 loss=6.105, nll_loss=2.764, mask_ins=0.928, word_ins_ml=4.313, word_reposition=0.305, kpe=0.558, ppl=68.81, wps=5315.1, ups=0.36, wpb=14810.1, bsz=1024, num_updates=82900, lr=0.000122794, gnorm=1.878, clip=0, loss_scale=4096, train_wall=247, wall=239122
2022-08-08 23:02:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-08 23:05:22 | INFO | train_inner | epoch 023:   1390 / 3715 loss=6.109, nll_loss=2.768, mask_ins=0.929, word_ins_ml=4.318, word_reposition=0.303, kpe=0.559, ppl=69.03, wps=5252.7, ups=0.35, wpb=14827.6, bsz=1024, num_updates=83000, lr=0.00012272, gnorm=1.873, clip=0, loss_scale=2697, train_wall=250, wall=239404
2022-08-08 23:10:00 | INFO | train_inner | epoch 023:   1490 / 3715 loss=6.127, nll_loss=2.785, mask_ins=0.933, word_ins_ml=4.332, word_reposition=0.304, kpe=0.558, ppl=69.91, wps=5270.2, ups=0.36, wpb=14667.7, bsz=1024, num_updates=83100, lr=0.000122646, gnorm=1.874, clip=0, loss_scale=2048, train_wall=247, wall=239683
2022-08-08 23:14:39 | INFO | train_inner | epoch 023:   1590 / 3715 loss=6.113, nll_loss=2.767, mask_ins=0.93, word_ins_ml=4.317, word_reposition=0.307, kpe=0.559, ppl=69.23, wps=5277.3, ups=0.36, wpb=14733.6, bsz=1024, num_updates=83200, lr=0.000122573, gnorm=1.862, clip=0, loss_scale=2048, train_wall=248, wall=239962
2022-08-08 23:19:19 | INFO | train_inner | epoch 023:   1690 / 3715 loss=6.106, nll_loss=2.761, mask_ins=0.929, word_ins_ml=4.311, word_reposition=0.302, kpe=0.564, ppl=68.87, wps=5310.8, ups=0.36, wpb=14825.1, bsz=1024, num_updates=83300, lr=0.000122499, gnorm=1.877, clip=0, loss_scale=2048, train_wall=248, wall=240241
2022-08-08 23:23:58 | INFO | train_inner | epoch 023:   1790 / 3715 loss=6.123, nll_loss=2.78, mask_ins=0.936, word_ins_ml=4.328, word_reposition=0.301, kpe=0.558, ppl=69.71, wps=5260.7, ups=0.36, wpb=14678.8, bsz=1024, num_updates=83400, lr=0.000122426, gnorm=1.874, clip=0, loss_scale=2048, train_wall=248, wall=240520
2022-08-08 23:28:36 | INFO | train_inner | epoch 023:   1890 / 3715 loss=6.105, nll_loss=2.766, mask_ins=0.924, word_ins_ml=4.315, word_reposition=0.308, kpe=0.558, ppl=68.81, wps=5283.1, ups=0.36, wpb=14690.3, bsz=1024, num_updates=83500, lr=0.000122352, gnorm=1.872, clip=0, loss_scale=3215, train_wall=247, wall=240798
2022-08-08 23:33:54 | INFO | train_inner | epoch 023:   1990 / 3715 loss=6.092, nll_loss=2.754, mask_ins=0.928, word_ins_ml=4.305, word_reposition=0.299, kpe=0.56, ppl=68.24, wps=4626, ups=0.31, wpb=14731.9, bsz=1024, num_updates=83600, lr=0.000122279, gnorm=1.861, clip=0, loss_scale=4096, train_wall=287, wall=241116
2022-08-08 23:38:34 | INFO | train_inner | epoch 023:   2090 / 3715 loss=6.119, nll_loss=2.784, mask_ins=0.933, word_ins_ml=4.332, word_reposition=0.298, kpe=0.557, ppl=69.52, wps=5191.4, ups=0.36, wpb=14520.7, bsz=1024, num_updates=83700, lr=0.000122206, gnorm=1.9, clip=0, loss_scale=4096, train_wall=249, wall=241396
2022-08-08 23:45:28 | INFO | train_inner | epoch 023:   2190 / 3715 loss=6.117, nll_loss=2.785, mask_ins=0.928, word_ins_ml=4.332, word_reposition=0.299, kpe=0.557, ppl=69.4, wps=3522.5, ups=0.24, wpb=14605.7, bsz=1024, num_updates=83800, lr=0.000122133, gnorm=1.899, clip=0, loss_scale=4096, train_wall=383, wall=241811
2022-08-08 23:50:09 | INFO | train_inner | epoch 023:   2290 / 3715 loss=6.114, nll_loss=2.77, mask_ins=0.931, word_ins_ml=4.319, word_reposition=0.302, kpe=0.563, ppl=69.28, wps=5269.4, ups=0.36, wpb=14765.1, bsz=1024, num_updates=83900, lr=0.00012206, gnorm=1.865, clip=0, loss_scale=4096, train_wall=249, wall=242091
2022-08-08 23:54:48 | INFO | train_inner | epoch 023:   2390 / 3715 loss=6.112, nll_loss=2.778, mask_ins=0.926, word_ins_ml=4.326, word_reposition=0.298, kpe=0.562, ppl=69.15, wps=5235, ups=0.36, wpb=14616.5, bsz=1024, num_updates=84000, lr=0.000121988, gnorm=1.863, clip=0, loss_scale=5939, train_wall=248, wall=242370
2022-08-08 23:54:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-08 23:59:30 | INFO | train_inner | epoch 023:   2491 / 3715 loss=6.121, nll_loss=2.773, mask_ins=0.932, word_ins_ml=4.321, word_reposition=0.305, kpe=0.563, ppl=69.62, wps=5211.7, ups=0.35, wpb=14722.1, bsz=1024, num_updates=84100, lr=0.000121915, gnorm=1.874, clip=0, loss_scale=4218, train_wall=251, wall=242653
2022-08-09 00:02:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-09 00:04:14 | INFO | train_inner | epoch 023:   2592 / 3715 loss=6.121, nll_loss=2.779, mask_ins=0.929, word_ins_ml=4.327, word_reposition=0.301, kpe=0.564, ppl=69.58, wps=5182.3, ups=0.35, wpb=14673, bsz=1024, num_updates=84200, lr=0.000121843, gnorm=1.875, clip=0, loss_scale=3224, train_wall=252, wall=242936
2022-08-09 00:08:53 | INFO | train_inner | epoch 023:   2692 / 3715 loss=6.147, nll_loss=2.796, mask_ins=0.93, word_ins_ml=4.342, word_reposition=0.31, kpe=0.565, ppl=70.85, wps=5278.8, ups=0.36, wpb=14735.6, bsz=1024, num_updates=84300, lr=0.00012177, gnorm=1.882, clip=0, loss_scale=2048, train_wall=248, wall=243215
2022-08-09 00:13:33 | INFO | train_inner | epoch 023:   2792 / 3715 loss=6.138, nll_loss=2.784, mask_ins=0.932, word_ins_ml=4.331, word_reposition=0.308, kpe=0.567, ppl=70.44, wps=5269.4, ups=0.36, wpb=14763.7, bsz=1024, num_updates=84400, lr=0.000121698, gnorm=1.863, clip=0, loss_scale=2048, train_wall=249, wall=243495
2022-08-09 00:18:12 | INFO | train_inner | epoch 023:   2892 / 3715 loss=6.142, nll_loss=2.793, mask_ins=0.933, word_ins_ml=4.339, word_reposition=0.304, kpe=0.567, ppl=70.64, wps=5261.2, ups=0.36, wpb=14708.8, bsz=1024, num_updates=84500, lr=0.000121626, gnorm=1.858, clip=0, loss_scale=2048, train_wall=248, wall=243775
2022-08-09 00:22:51 | INFO | train_inner | epoch 023:   2992 / 3715 loss=6.13, nll_loss=2.789, mask_ins=0.934, word_ins_ml=4.337, word_reposition=0.3, kpe=0.56, ppl=70.05, wps=5218.3, ups=0.36, wpb=14510.9, bsz=1024, num_updates=84600, lr=0.000121554, gnorm=1.875, clip=0, loss_scale=2048, train_wall=247, wall=244053
2022-08-09 00:27:29 | INFO | train_inner | epoch 023:   3092 / 3715 loss=6.142, nll_loss=2.795, mask_ins=0.936, word_ins_ml=4.341, word_reposition=0.301, kpe=0.564, ppl=70.64, wps=5255, ups=0.36, wpb=14642.3, bsz=1024, num_updates=84700, lr=0.000121482, gnorm=1.874, clip=0, loss_scale=2683, train_wall=247, wall=244332
2022-08-09 00:32:08 | INFO | train_inner | epoch 023:   3192 / 3715 loss=6.157, nll_loss=2.805, mask_ins=0.933, word_ins_ml=4.35, word_reposition=0.309, kpe=0.565, ppl=71.37, wps=5259.4, ups=0.36, wpb=14649.1, bsz=1024, num_updates=84800, lr=0.000121411, gnorm=1.858, clip=0, loss_scale=4096, train_wall=247, wall=244610
2022-08-09 00:36:47 | INFO | train_inner | epoch 023:   3292 / 3715 loss=6.143, nll_loss=2.797, mask_ins=0.93, word_ins_ml=4.343, word_reposition=0.306, kpe=0.564, ppl=70.66, wps=5200.4, ups=0.36, wpb=14549.3, bsz=1024, num_updates=84900, lr=0.000121339, gnorm=1.884, clip=0, loss_scale=4096, train_wall=248, wall=244890
2022-08-09 00:41:26 | INFO | train_inner | epoch 023:   3392 / 3715 loss=6.13, nll_loss=2.795, mask_ins=0.926, word_ins_ml=4.341, word_reposition=0.298, kpe=0.565, ppl=70.02, wps=5220.7, ups=0.36, wpb=14556.2, bsz=1024, num_updates=85000, lr=0.000121268, gnorm=1.879, clip=0, loss_scale=4096, train_wall=248, wall=245169
2022-08-09 00:46:07 | INFO | train_inner | epoch 023:   3492 / 3715 loss=6.139, nll_loss=2.795, mask_ins=0.93, word_ins_ml=4.341, word_reposition=0.3, kpe=0.568, ppl=70.46, wps=5236.1, ups=0.36, wpb=14681.1, bsz=1023.8, num_updates=85100, lr=0.000121197, gnorm=1.872, clip=0, loss_scale=4096, train_wall=249, wall=245449
2022-08-09 00:50:45 | INFO | train_inner | epoch 023:   3592 / 3715 loss=6.158, nll_loss=2.805, mask_ins=0.938, word_ins_ml=4.35, word_reposition=0.301, kpe=0.569, ppl=71.42, wps=5271.8, ups=0.36, wpb=14675.2, bsz=1024, num_updates=85200, lr=0.000121125, gnorm=1.897, clip=0, loss_scale=4874, train_wall=247, wall=245727
2022-08-09 00:55:26 | INFO | train_inner | epoch 023:   3692 / 3715 loss=6.142, nll_loss=2.794, mask_ins=0.931, word_ins_ml=4.34, word_reposition=0.304, kpe=0.566, ppl=70.6, wps=5209, ups=0.36, wpb=14635.6, bsz=1024, num_updates=85300, lr=0.000121054, gnorm=1.856, clip=0, loss_scale=8192, train_wall=249, wall=246008
2022-08-09 00:56:29 | INFO | train | epoch 023 | loss 6.116 | nll_loss 2.776 | mask_ins 0.93 | word_ins_ml 4.325 | word_reposition 0.303 | kpe 0.558 | ppl 69.38 | wps 5050 | ups 0.34 | wpb 14661.8 | bsz 1023.7 | num_updates 85323 | lr 0.000121038 | gnorm 1.878 | clip 0 | loss_scale 2933 | train_wall 9387 | wall 246072
2022-08-09 01:00:11 | INFO | valid | epoch 023 | valid on 'valid' subset | loss nan | nll_loss 3.155 | mask_ins 0.989 | word_ins_ml 4.704 | word_reposition 0.345 | kpe nan | ppl nan | wps 12382.8 | wpb 1849.4 | bsz 127.9 | num_updates 85323 | best_loss nan
2022-08-09 01:00:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 23 @ 85323 updates, score nan) (writing took 20.767154587432742 seconds)
2022-08-09 01:02:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-09 01:04:08 | INFO | train_inner | epoch 024:     78 / 3715 loss=6.057, nll_loss=2.736, mask_ins=0.923, word_ins_ml=4.289, word_reposition=0.302, kpe=0.543, ppl=66.57, wps=2780, ups=0.19, wpb=14519, bsz=1014.7, num_updates=85400, lr=0.000120983, gnorm=1.902, clip=0, loss_scale=6935, train_wall=249, wall=246531
2022-08-09 01:08:47 | INFO | train_inner | epoch 024:    178 / 3715 loss=6.027, nll_loss=2.713, mask_ins=0.918, word_ins_ml=4.269, word_reposition=0.304, kpe=0.537, ppl=65.22, wps=5241.8, ups=0.36, wpb=14629.9, bsz=1024, num_updates=85500, lr=0.000120913, gnorm=1.893, clip=0, loss_scale=4096, train_wall=248, wall=246810
2022-08-09 01:13:27 | INFO | train_inner | epoch 024:    278 / 3715 loss=6.056, nll_loss=2.74, mask_ins=0.924, word_ins_ml=4.293, word_reposition=0.301, kpe=0.537, ppl=66.52, wps=5225.3, ups=0.36, wpb=14610.5, bsz=1024, num_updates=85600, lr=0.000120842, gnorm=1.898, clip=0, loss_scale=4096, train_wall=248, wall=247089
2022-08-09 01:15:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-09 01:18:10 | INFO | train_inner | epoch 024:    379 / 3715 loss=6.04, nll_loss=2.724, mask_ins=0.922, word_ins_ml=4.278, word_reposition=0.301, kpe=0.539, ppl=65.8, wps=5220.8, ups=0.35, wpb=14763.6, bsz=1024, num_updates=85700, lr=0.000120772, gnorm=1.956, clip=0, loss_scale=2758, train_wall=251, wall=247372
2022-08-09 01:22:49 | INFO | train_inner | epoch 024:    479 / 3715 loss=6.059, nll_loss=2.742, mask_ins=0.921, word_ins_ml=4.295, word_reposition=0.303, kpe=0.54, ppl=66.67, wps=5272, ups=0.36, wpb=14706.4, bsz=1024, num_updates=85800, lr=0.000120701, gnorm=1.894, clip=0, loss_scale=2048, train_wall=247, wall=247651
2022-08-09 01:27:28 | INFO | train_inner | epoch 024:    579 / 3715 loss=6.051, nll_loss=2.727, mask_ins=0.926, word_ins_ml=4.281, word_reposition=0.301, kpe=0.543, ppl=66.29, wps=5286, ups=0.36, wpb=14774, bsz=1024, num_updates=85900, lr=0.000120631, gnorm=1.911, clip=0, loss_scale=2048, train_wall=249, wall=247931
2022-08-09 01:32:08 | INFO | train_inner | epoch 024:    679 / 3715 loss=6.056, nll_loss=2.739, mask_ins=0.919, word_ins_ml=4.292, word_reposition=0.299, kpe=0.545, ppl=66.52, wps=5270.1, ups=0.36, wpb=14733.5, bsz=1024, num_updates=86000, lr=0.000120561, gnorm=1.906, clip=0, loss_scale=2048, train_wall=248, wall=248210
2022-08-09 01:36:47 | INFO | train_inner | epoch 024:    779 / 3715 loss=6.084, nll_loss=2.765, mask_ins=0.925, word_ins_ml=4.315, word_reposition=0.298, kpe=0.547, ppl=67.83, wps=5277.8, ups=0.36, wpb=14739.1, bsz=1024, num_updates=86100, lr=0.000120491, gnorm=1.904, clip=0, loss_scale=2048, train_wall=248, wall=248489
2022-08-09 01:38:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-09 01:41:29 | INFO | train_inner | epoch 024:    880 / 3715 loss=6.061, nll_loss=2.738, mask_ins=0.923, word_ins_ml=4.29, word_reposition=0.305, kpe=0.544, ppl=66.78, wps=5184.6, ups=0.35, wpb=14632.9, bsz=1024, num_updates=86200, lr=0.000120421, gnorm=1.912, clip=0, loss_scale=1480, train_wall=251, wall=248772
2022-08-09 01:46:10 | INFO | train_inner | epoch 024:    980 / 3715 loss=6.065, nll_loss=2.745, mask_ins=0.922, word_ins_ml=4.296, word_reposition=0.302, kpe=0.545, ppl=66.97, wps=5204.1, ups=0.36, wpb=14587.5, bsz=1024, num_updates=86300, lr=0.000120351, gnorm=1.9, clip=0, loss_scale=1024, train_wall=249, wall=249052
2022-08-09 01:50:50 | INFO | train_inner | epoch 024:   1080 / 3715 loss=6.066, nll_loss=2.748, mask_ins=0.926, word_ins_ml=4.299, word_reposition=0.295, kpe=0.546, ppl=66.98, wps=5253.2, ups=0.36, wpb=14702.5, bsz=1024, num_updates=86400, lr=0.000120281, gnorm=1.925, clip=0, loss_scale=1024, train_wall=248, wall=249332
2022-08-09 01:55:30 | INFO | train_inner | epoch 024:   1180 / 3715 loss=6.059, nll_loss=2.732, mask_ins=0.921, word_ins_ml=4.286, word_reposition=0.305, kpe=0.548, ppl=66.69, wps=5219.1, ups=0.36, wpb=14657.6, bsz=1024, num_updates=86500, lr=0.000120212, gnorm=1.924, clip=0, loss_scale=1024, train_wall=250, wall=249613
2022-08-09 02:00:10 | INFO | train_inner | epoch 024:   1280 / 3715 loss=6.07, nll_loss=2.741, mask_ins=0.924, word_ins_ml=4.294, word_reposition=0.304, kpe=0.548, ppl=67.2, wps=5280.8, ups=0.36, wpb=14748.8, bsz=1024, num_updates=86600, lr=0.000120142, gnorm=1.919, clip=0, loss_scale=1024, train_wall=248, wall=249892
2022-08-09 02:04:50 | INFO | train_inner | epoch 024:   1380 / 3715 loss=6.076, nll_loss=2.753, mask_ins=0.924, word_ins_ml=4.304, word_reposition=0.302, kpe=0.546, ppl=67.48, wps=5209.1, ups=0.36, wpb=14606.1, bsz=1024, num_updates=86700, lr=0.000120073, gnorm=1.915, clip=0, loss_scale=1475, train_wall=249, wall=250172
2022-08-09 02:09:30 | INFO | train_inner | epoch 024:   1480 / 3715 loss=6.054, nll_loss=2.733, mask_ins=0.922, word_ins_ml=4.286, word_reposition=0.298, kpe=0.548, ppl=66.42, wps=5240.6, ups=0.36, wpb=14677.9, bsz=1024, num_updates=86800, lr=0.000120004, gnorm=1.896, clip=0, loss_scale=2048, train_wall=248, wall=250453
2022-08-09 02:14:11 | INFO | train_inner | epoch 024:   1580 / 3715 loss=6.075, nll_loss=2.755, mask_ins=0.918, word_ins_ml=4.306, word_reposition=0.3, kpe=0.551, ppl=67.41, wps=5217.9, ups=0.36, wpb=14654.2, bsz=1024, num_updates=86900, lr=0.000119935, gnorm=1.907, clip=0, loss_scale=2048, train_wall=250, wall=250733
2022-08-09 02:18:50 | INFO | train_inner | epoch 024:   1680 / 3715 loss=6.09, nll_loss=2.757, mask_ins=0.925, word_ins_ml=4.308, word_reposition=0.307, kpe=0.551, ppl=68.14, wps=5265.1, ups=0.36, wpb=14698.5, bsz=1024, num_updates=87000, lr=0.000119866, gnorm=1.92, clip=0, loss_scale=2048, train_wall=247, wall=251013
2022-08-09 02:23:30 | INFO | train_inner | epoch 024:   1780 / 3715 loss=6.079, nll_loss=2.752, mask_ins=0.93, word_ins_ml=4.303, word_reposition=0.297, kpe=0.549, ppl=67.61, wps=5213.1, ups=0.36, wpb=14610.8, bsz=1024, num_updates=87100, lr=0.000119797, gnorm=1.925, clip=0, loss_scale=2048, train_wall=249, wall=251293
2022-08-09 02:28:10 | INFO | train_inner | epoch 024:   1880 / 3715 loss=6.1, nll_loss=2.766, mask_ins=0.928, word_ins_ml=4.315, word_reposition=0.302, kpe=0.554, ppl=68.59, wps=5272.5, ups=0.36, wpb=14740.2, bsz=1024, num_updates=87200, lr=0.000119728, gnorm=1.909, clip=0, loss_scale=2703, train_wall=248, wall=251572
2022-08-09 02:32:50 | INFO | train_inner | epoch 024:   1980 / 3715 loss=6.085, nll_loss=2.752, mask_ins=0.926, word_ins_ml=4.303, word_reposition=0.307, kpe=0.55, ppl=67.88, wps=5225.7, ups=0.36, wpb=14618.4, bsz=1024, num_updates=87300, lr=0.00011966, gnorm=1.933, clip=0, loss_scale=4096, train_wall=248, wall=251852
2022-08-09 02:37:29 | INFO | train_inner | epoch 024:   2080 / 3715 loss=6.08, nll_loss=2.754, mask_ins=0.926, word_ins_ml=4.305, word_reposition=0.301, kpe=0.548, ppl=67.63, wps=5213.8, ups=0.36, wpb=14571.4, bsz=1024, num_updates=87400, lr=0.000119591, gnorm=1.926, clip=0, loss_scale=4096, train_wall=248, wall=252132
2022-08-09 02:38:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-09 02:42:11 | INFO | train_inner | epoch 024:   2181 / 3715 loss=6.105, nll_loss=2.774, mask_ins=0.926, word_ins_ml=4.322, word_reposition=0.303, kpe=0.553, ppl=68.82, wps=5244.6, ups=0.36, wpb=14767.8, bsz=1024, num_updates=87500, lr=0.000119523, gnorm=1.888, clip=0, loss_scale=2575, train_wall=250, wall=252413
2022-08-09 02:46:50 | INFO | train_inner | epoch 024:   2281 / 3715 loss=6.094, nll_loss=2.765, mask_ins=0.927, word_ins_ml=4.315, word_reposition=0.3, kpe=0.552, ppl=68.33, wps=5239.6, ups=0.36, wpb=14629.6, bsz=1024, num_updates=87600, lr=0.000119455, gnorm=1.919, clip=0, loss_scale=2048, train_wall=248, wall=252692
2022-08-09 02:51:29 | INFO | train_inner | epoch 024:   2381 / 3715 loss=6.087, nll_loss=2.765, mask_ins=0.923, word_ins_ml=4.315, word_reposition=0.296, kpe=0.553, ppl=67.99, wps=5217.3, ups=0.36, wpb=14544.1, bsz=1024, num_updates=87700, lr=0.000119386, gnorm=1.912, clip=0, loss_scale=2048, train_wall=248, wall=252971
2022-08-09 02:56:08 | INFO | train_inner | epoch 024:   2481 / 3715 loss=6.093, nll_loss=2.763, mask_ins=0.926, word_ins_ml=4.312, word_reposition=0.302, kpe=0.553, ppl=68.25, wps=5227.2, ups=0.36, wpb=14573.8, bsz=1024, num_updates=87800, lr=0.000119318, gnorm=1.931, clip=0, loss_scale=2048, train_wall=247, wall=253250
2022-08-09 02:57:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-09 03:00:50 | INFO | train_inner | epoch 024:   2582 / 3715 loss=6.095, nll_loss=2.76, mask_ins=0.924, word_ins_ml=4.31, word_reposition=0.306, kpe=0.555, ppl=68.36, wps=5185.3, ups=0.35, wpb=14625.5, bsz=1023.8, num_updates=87900, lr=0.000119251, gnorm=1.924, clip=0, loss_scale=1257, train_wall=251, wall=253532
2022-08-09 03:04:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-09 03:05:31 | INFO | train_inner | epoch 024:   2683 / 3715 loss=6.099, nll_loss=2.762, mask_ins=0.927, word_ins_ml=4.312, word_reposition=0.306, kpe=0.554, ppl=68.54, wps=5183.7, ups=0.36, wpb=14602, bsz=1024, num_updates=88000, lr=0.000119183, gnorm=1.936, clip=0, loss_scale=897, train_wall=250, wall=253814
2022-08-09 03:10:11 | INFO | train_inner | epoch 024:   2783 / 3715 loss=6.108, nll_loss=2.771, mask_ins=0.933, word_ins_ml=4.319, word_reposition=0.301, kpe=0.556, ppl=68.99, wps=5253.2, ups=0.36, wpb=14672.1, bsz=1024, num_updates=88100, lr=0.000119115, gnorm=1.907, clip=0, loss_scale=512, train_wall=248, wall=254093
2022-08-09 03:14:50 | INFO | train_inner | epoch 024:   2883 / 3715 loss=6.091, nll_loss=2.759, mask_ins=0.921, word_ins_ml=4.309, word_reposition=0.305, kpe=0.557, ppl=68.19, wps=5275.8, ups=0.36, wpb=14760.4, bsz=1024, num_updates=88200, lr=0.000119048, gnorm=1.903, clip=0, loss_scale=512, train_wall=248, wall=254373
2022-08-09 03:19:29 | INFO | train_inner | epoch 024:   2983 / 3715 loss=6.116, nll_loss=2.775, mask_ins=0.929, word_ins_ml=4.323, word_reposition=0.304, kpe=0.561, ppl=69.38, wps=5307.5, ups=0.36, wpb=14790.9, bsz=1024, num_updates=88300, lr=0.00011898, gnorm=1.929, clip=0, loss_scale=512, train_wall=247, wall=254652
2022-08-09 03:24:08 | INFO | train_inner | epoch 024:   3083 / 3715 loss=6.119, nll_loss=2.782, mask_ins=0.926, word_ins_ml=4.329, word_reposition=0.305, kpe=0.559, ppl=69.52, wps=5264.8, ups=0.36, wpb=14706.9, bsz=1024, num_updates=88400, lr=0.000118913, gnorm=1.908, clip=0, loss_scale=512, train_wall=248, wall=254931
2022-08-09 03:28:49 | INFO | train_inner | epoch 024:   3183 / 3715 loss=6.088, nll_loss=2.753, mask_ins=0.924, word_ins_ml=4.304, word_reposition=0.301, kpe=0.559, ppl=68.04, wps=5256.3, ups=0.36, wpb=14720.2, bsz=1024, num_updates=88500, lr=0.000118846, gnorm=1.895, clip=0, loss_scale=579, train_wall=249, wall=255211
2022-08-09 03:33:27 | INFO | train_inner | epoch 024:   3283 / 3715 loss=6.12, nll_loss=2.784, mask_ins=0.926, word_ins_ml=4.331, word_reposition=0.306, kpe=0.557, ppl=69.53, wps=5253.2, ups=0.36, wpb=14628.4, bsz=1024, num_updates=88600, lr=0.000118779, gnorm=1.909, clip=0, loss_scale=1024, train_wall=247, wall=255489
2022-08-09 03:38:06 | INFO | train_inner | epoch 024:   3383 / 3715 loss=6.108, nll_loss=2.769, mask_ins=0.928, word_ins_ml=4.318, word_reposition=0.304, kpe=0.557, ppl=68.98, wps=5253.4, ups=0.36, wpb=14682.1, bsz=1024, num_updates=88700, lr=0.000118712, gnorm=1.909, clip=0, loss_scale=1024, train_wall=248, wall=255769
2022-08-09 03:42:44 | INFO | train_inner | epoch 024:   3483 / 3715 loss=6.099, nll_loss=2.779, mask_ins=0.923, word_ins_ml=4.327, word_reposition=0.298, kpe=0.552, ppl=68.53, wps=5179, ups=0.36, wpb=14380.4, bsz=1024, num_updates=88800, lr=0.000118645, gnorm=1.895, clip=0, loss_scale=1024, train_wall=247, wall=256047
2022-08-09 03:47:23 | INFO | train_inner | epoch 024:   3583 / 3715 loss=6.123, nll_loss=2.789, mask_ins=0.928, word_ins_ml=4.335, word_reposition=0.301, kpe=0.56, ppl=69.71, wps=5266.1, ups=0.36, wpb=14678.7, bsz=1024, num_updates=88900, lr=0.000118578, gnorm=1.911, clip=0, loss_scale=1024, train_wall=248, wall=256325
2022-08-09 03:52:03 | INFO | train_inner | epoch 024:   3683 / 3715 loss=6.117, nll_loss=2.772, mask_ins=0.93, word_ins_ml=4.321, word_reposition=0.305, kpe=0.561, ppl=69.42, wps=5231.6, ups=0.36, wpb=14658, bsz=1024, num_updates=89000, lr=0.000118511, gnorm=1.92, clip=0, loss_scale=1034, train_wall=249, wall=256605
2022-08-09 03:53:31 | INFO | train | epoch 024 | loss 6.083 | nll_loss 2.755 | mask_ins 0.925 | word_ins_ml 4.306 | word_reposition 0.302 | kpe 0.55 | ppl 67.79 | wps 5119.8 | ups 0.35 | wpb 14661.8 | bsz 1023.7 | num_updates 89032 | lr 0.00011849 | gnorm 1.913 | clip 0 | loss_scale 1878 | train_wall 9216 | wall 256693
2022-08-09 03:57:11 | INFO | valid | epoch 024 | valid on 'valid' subset | loss nan | nll_loss 3.163 | mask_ins 0.988 | word_ins_ml 4.713 | word_reposition 0.354 | kpe nan | ppl nan | wps 12418.4 | wpb 1849.4 | bsz 127.9 | num_updates 89032 | best_loss nan
2022-08-09 03:57:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 24 @ 89032 updates, score nan) (writing took 7.817894887179136 seconds)
2022-08-09 04:00:29 | INFO | train_inner | epoch 025:     68 / 3715 loss=6.044, nll_loss=2.733, mask_ins=0.923, word_ins_ml=4.286, word_reposition=0.295, kpe=0.54, ppl=65.98, wps=2895.3, ups=0.2, wpb=14646.4, bsz=1014.7, num_updates=89100, lr=0.000118445, gnorm=1.929, clip=0, loss_scale=2048, train_wall=246, wall=257111
2022-08-09 04:05:07 | INFO | train_inner | epoch 025:    168 / 3715 loss=6.015, nll_loss=2.714, mask_ins=0.92, word_ins_ml=4.269, word_reposition=0.294, kpe=0.532, ppl=64.69, wps=5261.2, ups=0.36, wpb=14635.2, bsz=1024, num_updates=89200, lr=0.000118378, gnorm=1.921, clip=0, loss_scale=2048, train_wall=247, wall=257390
2022-08-09 04:09:47 | INFO | train_inner | epoch 025:    268 / 3715 loss=6.029, nll_loss=2.719, mask_ins=0.919, word_ins_ml=4.274, word_reposition=0.302, kpe=0.534, ppl=65.31, wps=5277.6, ups=0.36, wpb=14794, bsz=1024, num_updates=89300, lr=0.000118312, gnorm=1.918, clip=0, loss_scale=2048, train_wall=249, wall=257670
2022-08-09 04:14:26 | INFO | train_inner | epoch 025:    368 / 3715 loss=6.02, nll_loss=2.716, mask_ins=0.92, word_ins_ml=4.271, word_reposition=0.297, kpe=0.531, ppl=64.9, wps=5262.9, ups=0.36, wpb=14654.5, bsz=1024, num_updates=89400, lr=0.000118246, gnorm=1.935, clip=0, loss_scale=2048, train_wall=247, wall=257948
2022-08-09 04:19:05 | INFO | train_inner | epoch 025:    468 / 3715 loss=6.024, nll_loss=2.718, mask_ins=0.918, word_ins_ml=4.273, word_reposition=0.302, kpe=0.531, ppl=65.08, wps=5249.1, ups=0.36, wpb=14659, bsz=1024, num_updates=89500, lr=0.00011818, gnorm=1.937, clip=0, loss_scale=2048, train_wall=248, wall=258228
2022-08-09 04:23:44 | INFO | train_inner | epoch 025:    568 / 3715 loss=6.004, nll_loss=2.707, mask_ins=0.913, word_ins_ml=4.263, word_reposition=0.294, kpe=0.533, ppl=64.16, wps=5222.6, ups=0.36, wpb=14540.8, bsz=1024, num_updates=89600, lr=0.000118114, gnorm=1.92, clip=0, loss_scale=3871, train_wall=247, wall=258506
2022-08-09 04:28:23 | INFO | train_inner | epoch 025:    668 / 3715 loss=6.023, nll_loss=2.714, mask_ins=0.922, word_ins_ml=4.27, word_reposition=0.296, kpe=0.535, ppl=65.03, wps=5238, ups=0.36, wpb=14660.9, bsz=1024, num_updates=89700, lr=0.000118048, gnorm=1.962, clip=0, loss_scale=4096, train_wall=249, wall=258786
2022-08-09 04:33:03 | INFO | train_inner | epoch 025:    768 / 3715 loss=6.026, nll_loss=2.719, mask_ins=0.919, word_ins_ml=4.274, word_reposition=0.3, kpe=0.533, ppl=65.16, wps=5219.6, ups=0.36, wpb=14568.6, bsz=1024, num_updates=89800, lr=0.000117982, gnorm=1.941, clip=0, loss_scale=4096, train_wall=248, wall=259065
2022-08-09 04:37:41 | INFO | train_inner | epoch 025:    868 / 3715 loss=6.055, nll_loss=2.744, mask_ins=0.923, word_ins_ml=4.296, word_reposition=0.302, kpe=0.535, ppl=66.49, wps=5237.1, ups=0.36, wpb=14598.5, bsz=1024, num_updates=89900, lr=0.000117917, gnorm=1.923, clip=0, loss_scale=4096, train_wall=248, wall=259344
2022-08-09 04:42:21 | INFO | train_inner | epoch 025:    968 / 3715 loss=6.052, nll_loss=2.736, mask_ins=0.924, word_ins_ml=4.289, word_reposition=0.302, kpe=0.537, ppl=66.37, wps=5257.2, ups=0.36, wpb=14716.4, bsz=1024, num_updates=90000, lr=0.000117851, gnorm=1.93, clip=0, loss_scale=4096, train_wall=249, wall=259624
2022-08-09 04:45:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-09 04:47:04 | INFO | train_inner | epoch 025:   1069 / 3715 loss=6.033, nll_loss=2.729, mask_ins=0.923, word_ins_ml=4.282, word_reposition=0.292, kpe=0.536, ppl=65.49, wps=5153.4, ups=0.35, wpb=14565.1, bsz=1024, num_updates=90100, lr=0.000117786, gnorm=1.932, clip=0, loss_scale=5678, train_wall=250, wall=259906
2022-08-09 04:50:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-09 04:51:46 | INFO | train_inner | epoch 025:   1170 / 3715 loss=6.02, nll_loss=2.707, mask_ins=0.92, word_ins_ml=4.263, word_reposition=0.3, kpe=0.537, ppl=64.9, wps=5205.2, ups=0.35, wpb=14682.1, bsz=1024, num_updates=90200, lr=0.00011772, gnorm=1.938, clip=0, loss_scale=3325, train_wall=250, wall=260188
2022-08-09 04:52:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-09 04:54:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-09 04:56:32 | INFO | train_inner | epoch 025:   1272 / 3715 loss=6.037, nll_loss=2.73, mask_ins=0.921, word_ins_ml=4.283, word_reposition=0.293, kpe=0.54, ppl=65.65, wps=5138.2, ups=0.35, wpb=14710.8, bsz=1024, num_updates=90300, lr=0.000117655, gnorm=1.944, clip=0, loss_scale=1029, train_wall=254, wall=260475
2022-08-09 05:01:12 | INFO | train_inner | epoch 025:   1372 / 3715 loss=6.023, nll_loss=2.723, mask_ins=0.918, word_ins_ml=4.277, word_reposition=0.291, kpe=0.537, ppl=65.03, wps=5221.8, ups=0.36, wpb=14583.6, bsz=1024, num_updates=90400, lr=0.00011759, gnorm=1.927, clip=0, loss_scale=512, train_wall=248, wall=260754
2022-08-09 05:05:51 | INFO | train_inner | epoch 025:   1472 / 3715 loss=6.063, nll_loss=2.743, mask_ins=0.926, word_ins_ml=4.295, word_reposition=0.301, kpe=0.541, ppl=66.87, wps=5251.2, ups=0.36, wpb=14680.1, bsz=1024, num_updates=90500, lr=0.000117525, gnorm=1.936, clip=0, loss_scale=512, train_wall=248, wall=261034
2022-08-09 05:10:31 | INFO | train_inner | epoch 025:   1572 / 3715 loss=6.055, nll_loss=2.732, mask_ins=0.926, word_ins_ml=4.285, word_reposition=0.304, kpe=0.541, ppl=66.48, wps=5232.1, ups=0.36, wpb=14632, bsz=1024, num_updates=90600, lr=0.00011746, gnorm=1.961, clip=0, loss_scale=512, train_wall=248, wall=261313
2022-08-09 05:15:11 | INFO | train_inner | epoch 025:   1672 / 3715 loss=6.044, nll_loss=2.734, mask_ins=0.92, word_ins_ml=4.287, word_reposition=0.299, kpe=0.539, ppl=66, wps=5210.1, ups=0.36, wpb=14577.6, bsz=1024, num_updates=90700, lr=0.000117395, gnorm=1.928, clip=0, loss_scale=512, train_wall=249, wall=261593
2022-08-09 05:19:51 | INFO | train_inner | epoch 025:   1772 / 3715 loss=6.053, nll_loss=2.73, mask_ins=0.923, word_ins_ml=4.283, word_reposition=0.303, kpe=0.544, ppl=66.38, wps=5270.6, ups=0.36, wpb=14771.1, bsz=1024, num_updates=90800, lr=0.000117331, gnorm=1.931, clip=0, loss_scale=650, train_wall=249, wall=261873
2022-08-09 05:23:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-09 05:24:33 | INFO | train_inner | epoch 025:   1873 / 3715 loss=6.068, nll_loss=2.752, mask_ins=0.922, word_ins_ml=4.303, word_reposition=0.299, kpe=0.543, ppl=67.07, wps=5213.8, ups=0.35, wpb=14703.3, bsz=1024, num_updates=90900, lr=0.000117266, gnorm=1.908, clip=0, loss_scale=892, train_wall=251, wall=262155
2022-08-09 05:29:11 | INFO | train_inner | epoch 025:   1973 / 3715 loss=6.066, nll_loss=2.744, mask_ins=0.918, word_ins_ml=4.296, word_reposition=0.305, kpe=0.546, ppl=67, wps=5292.6, ups=0.36, wpb=14732.1, bsz=1024, num_updates=91000, lr=0.000117202, gnorm=1.922, clip=0, loss_scale=512, train_wall=247, wall=262434
2022-08-09 05:33:51 | INFO | train_inner | epoch 025:   2073 / 3715 loss=6.061, nll_loss=2.742, mask_ins=0.919, word_ins_ml=4.294, word_reposition=0.302, kpe=0.546, ppl=66.78, wps=5260.9, ups=0.36, wpb=14726.6, bsz=1024, num_updates=91100, lr=0.000117137, gnorm=1.943, clip=0, loss_scale=512, train_wall=249, wall=262714
2022-08-09 05:38:32 | INFO | train_inner | epoch 025:   2173 / 3715 loss=6.045, nll_loss=2.726, mask_ins=0.919, word_ins_ml=4.279, word_reposition=0.301, kpe=0.546, ppl=66.03, wps=5248.8, ups=0.36, wpb=14730.8, bsz=1024, num_updates=91200, lr=0.000117073, gnorm=1.932, clip=0, loss_scale=512, train_wall=249, wall=262994
2022-08-09 05:43:11 | INFO | train_inner | epoch 025:   2273 / 3715 loss=6.073, nll_loss=2.748, mask_ins=0.923, word_ins_ml=4.299, word_reposition=0.307, kpe=0.545, ppl=67.34, wps=5265.6, ups=0.36, wpb=14703.5, bsz=1023.8, num_updates=91300, lr=0.000117009, gnorm=1.94, clip=0, loss_scale=512, train_wall=248, wall=263273
2022-08-09 05:47:50 | INFO | train_inner | epoch 025:   2373 / 3715 loss=6.082, nll_loss=2.75, mask_ins=0.928, word_ins_ml=4.301, word_reposition=0.306, kpe=0.547, ppl=67.76, wps=5279.1, ups=0.36, wpb=14712, bsz=1024, num_updates=91400, lr=0.000116945, gnorm=1.929, clip=0, loss_scale=584, train_wall=247, wall=263552
2022-08-09 05:52:29 | INFO | train_inner | epoch 025:   2473 / 3715 loss=6.064, nll_loss=2.744, mask_ins=0.922, word_ins_ml=4.295, word_reposition=0.3, kpe=0.546, ppl=66.9, wps=5243.7, ups=0.36, wpb=14631.9, bsz=1024, num_updates=91500, lr=0.000116881, gnorm=1.942, clip=0, loss_scale=1024, train_wall=247, wall=263831
2022-08-09 05:57:09 | INFO | train_inner | epoch 025:   2573 / 3715 loss=6.066, nll_loss=2.74, mask_ins=0.926, word_ins_ml=4.292, word_reposition=0.301, kpe=0.547, ppl=66.98, wps=5238.3, ups=0.36, wpb=14675.9, bsz=1024, num_updates=91600, lr=0.000116817, gnorm=1.922, clip=0, loss_scale=1024, train_wall=249, wall=264111
2022-08-09 06:01:48 | INFO | train_inner | epoch 025:   2673 / 3715 loss=6.062, nll_loss=2.748, mask_ins=0.922, word_ins_ml=4.299, word_reposition=0.298, kpe=0.544, ppl=66.83, wps=5223.1, ups=0.36, wpb=14573.7, bsz=1024, num_updates=91700, lr=0.000116754, gnorm=1.927, clip=0, loss_scale=1024, train_wall=248, wall=264390
2022-08-09 06:06:28 | INFO | train_inner | epoch 025:   2773 / 3715 loss=6.091, nll_loss=2.758, mask_ins=0.927, word_ins_ml=4.308, word_reposition=0.306, kpe=0.55, ppl=68.18, wps=5249.9, ups=0.36, wpb=14708.1, bsz=1024, num_updates=91800, lr=0.00011669, gnorm=1.912, clip=0, loss_scale=1024, train_wall=249, wall=264671
2022-08-09 06:11:09 | INFO | train_inner | epoch 025:   2873 / 3715 loss=6.092, nll_loss=2.768, mask_ins=0.929, word_ins_ml=4.317, word_reposition=0.299, kpe=0.547, ppl=68.22, wps=5226.3, ups=0.36, wpb=14654.8, bsz=1024, num_updates=91900, lr=0.000116627, gnorm=1.913, clip=0, loss_scale=1044, train_wall=249, wall=264951
2022-08-09 06:15:48 | INFO | train_inner | epoch 025:   2973 / 3715 loss=6.078, nll_loss=2.76, mask_ins=0.926, word_ins_ml=4.31, word_reposition=0.298, kpe=0.544, ppl=67.53, wps=5226, ups=0.36, wpb=14602.8, bsz=1024, num_updates=92000, lr=0.000116563, gnorm=1.919, clip=0, loss_scale=2048, train_wall=248, wall=265230
2022-08-09 06:20:27 | INFO | train_inner | epoch 025:   3073 / 3715 loss=6.069, nll_loss=2.744, mask_ins=0.928, word_ins_ml=4.296, word_reposition=0.297, kpe=0.548, ppl=67.14, wps=5230.5, ups=0.36, wpb=14580.9, bsz=1024, num_updates=92100, lr=0.0001165, gnorm=1.931, clip=0, loss_scale=2048, train_wall=247, wall=265509
2022-08-09 06:25:06 | INFO | train_inner | epoch 025:   3173 / 3715 loss=6.075, nll_loss=2.751, mask_ins=0.925, word_ins_ml=4.302, word_reposition=0.294, kpe=0.553, ppl=67.41, wps=5248.1, ups=0.36, wpb=14629.8, bsz=1024, num_updates=92200, lr=0.000116437, gnorm=1.919, clip=0, loss_scale=2048, train_wall=247, wall=265788
2022-08-09 06:29:48 | INFO | train_inner | epoch 025:   3273 / 3715 loss=6.098, nll_loss=2.758, mask_ins=0.928, word_ins_ml=4.307, word_reposition=0.306, kpe=0.556, ppl=68.5, wps=5239.3, ups=0.35, wpb=14780.9, bsz=1024, num_updates=92300, lr=0.000116374, gnorm=1.921, clip=0, loss_scale=2048, train_wall=251, wall=266070
2022-08-09 06:34:28 | INFO | train_inner | epoch 025:   3373 / 3715 loss=6.087, nll_loss=2.755, mask_ins=0.927, word_ins_ml=4.305, word_reposition=0.305, kpe=0.55, ppl=68, wps=5220.7, ups=0.36, wpb=14645, bsz=1024, num_updates=92400, lr=0.000116311, gnorm=1.95, clip=0, loss_scale=2048, train_wall=249, wall=266351
2022-08-09 06:39:07 | INFO | train_inner | epoch 025:   3473 / 3715 loss=6.1, nll_loss=2.774, mask_ins=0.929, word_ins_ml=4.322, word_reposition=0.297, kpe=0.552, ppl=68.6, wps=5268.8, ups=0.36, wpb=14694.1, bsz=1024, num_updates=92500, lr=0.000116248, gnorm=1.932, clip=0, loss_scale=3891, train_wall=248, wall=266629
2022-08-09 06:40:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-09 06:43:49 | INFO | train_inner | epoch 025:   3574 / 3715 loss=6.072, nll_loss=2.743, mask_ins=0.919, word_ins_ml=4.294, word_reposition=0.303, kpe=0.555, ppl=67.25, wps=5255.9, ups=0.35, wpb=14808, bsz=1024, num_updates=92600, lr=0.000116185, gnorm=1.907, clip=0, loss_scale=2616, train_wall=250, wall=266911
2022-08-09 06:48:28 | INFO | train_inner | epoch 025:   3674 / 3715 loss=6.067, nll_loss=2.744, mask_ins=0.922, word_ins_ml=4.295, word_reposition=0.302, kpe=0.547, ppl=67.02, wps=5238.5, ups=0.36, wpb=14634.4, bsz=1024, num_updates=92700, lr=0.000116122, gnorm=1.939, clip=0, loss_scale=2048, train_wall=248, wall=267191
2022-08-09 06:50:21 | INFO | train | epoch 025 | loss 6.055 | nll_loss 2.737 | mask_ins 0.922 | word_ins_ml 4.29 | word_reposition 0.3 | kpe 0.542 | ppl 66.47 | wps 5125.6 | ups 0.35 | wpb 14662.6 | bsz 1023.7 | num_updates 92741 | lr 0.000116096 | gnorm 1.931 | clip 0 | loss_scale 1912 | train_wall 9216 | wall 267303
2022-08-09 06:54:03 | INFO | valid | epoch 025 | valid on 'valid' subset | loss nan | nll_loss 3.177 | mask_ins 0.992 | word_ins_ml 4.724 | word_reposition 0.356 | kpe nan | ppl nan | wps 12346.6 | wpb 1849.4 | bsz 127.9 | num_updates 92741 | best_loss nan
2022-08-09 06:54:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 25 @ 92741 updates, score nan) (writing took 29.83257000334561 seconds)
2022-08-09 06:56:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-09 06:57:21 | INFO | train_inner | epoch 026:     60 / 3715 loss=6.016, nll_loss=2.712, mask_ins=0.917, word_ins_ml=4.268, word_reposition=0.297, kpe=0.534, ppl=64.71, wps=2731.7, ups=0.19, wpb=14550.6, bsz=1014.7, num_updates=92800, lr=0.00011606, gnorm=1.973, clip=0, loss_scale=1916, train_wall=249, wall=267723
2022-08-09 07:02:01 | INFO | train_inner | epoch 026:    160 / 3715 loss=5.972, nll_loss=2.689, mask_ins=0.912, word_ins_ml=4.247, word_reposition=0.293, kpe=0.52, ppl=62.78, wps=5189.7, ups=0.36, wpb=14566, bsz=1024, num_updates=92900, lr=0.000115997, gnorm=1.944, clip=0, loss_scale=1024, train_wall=249, wall=268004
2022-08-09 07:06:40 | INFO | train_inner | epoch 026:    260 / 3715 loss=5.982, nll_loss=2.684, mask_ins=0.916, word_ins_ml=4.242, word_reposition=0.3, kpe=0.524, ppl=63.18, wps=5286.9, ups=0.36, wpb=14720.3, bsz=1024, num_updates=93000, lr=0.000115935, gnorm=1.956, clip=0, loss_scale=1024, train_wall=247, wall=268282
2022-08-09 07:11:20 | INFO | train_inner | epoch 026:    360 / 3715 loss=5.986, nll_loss=2.693, mask_ins=0.913, word_ins_ml=4.251, word_reposition=0.297, kpe=0.524, ppl=63.37, wps=5245, ups=0.36, wpb=14685, bsz=1024, num_updates=93100, lr=0.000115872, gnorm=1.953, clip=0, loss_scale=1024, train_wall=249, wall=268562
2022-08-09 07:15:59 | INFO | train_inner | epoch 026:    460 / 3715 loss=5.992, nll_loss=2.69, mask_ins=0.916, word_ins_ml=4.248, word_reposition=0.299, kpe=0.529, ppl=63.65, wps=5273.1, ups=0.36, wpb=14724.1, bsz=1024, num_updates=93200, lr=0.00011581, gnorm=1.939, clip=0, loss_scale=1024, train_wall=248, wall=268842
2022-08-09 07:20:38 | INFO | train_inner | epoch 026:    560 / 3715 loss=6.001, nll_loss=2.706, mask_ins=0.919, word_ins_ml=4.263, word_reposition=0.292, kpe=0.528, ppl=64.05, wps=5261.5, ups=0.36, wpb=14675, bsz=1024, num_updates=93300, lr=0.000115748, gnorm=1.961, clip=0, loss_scale=1034, train_wall=247, wall=269120
2022-08-09 07:23:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-09 07:25:20 | INFO | train_inner | epoch 026:    661 / 3715 loss=5.997, nll_loss=2.7, mask_ins=0.913, word_ins_ml=4.257, word_reposition=0.296, kpe=0.53, ppl=63.88, wps=5205.7, ups=0.35, wpb=14683.5, bsz=1024, num_updates=93400, lr=0.000115686, gnorm=1.957, clip=0, loss_scale=1612, train_wall=250, wall=269402
2022-08-09 07:29:58 | INFO | train_inner | epoch 026:    761 / 3715 loss=6.015, nll_loss=2.718, mask_ins=0.912, word_ins_ml=4.272, word_reposition=0.305, kpe=0.526, ppl=64.68, wps=5248.3, ups=0.36, wpb=14600.1, bsz=1024, num_updates=93500, lr=0.000115624, gnorm=1.982, clip=0, loss_scale=1024, train_wall=247, wall=269681
2022-08-09 07:34:39 | INFO | train_inner | epoch 026:    861 / 3715 loss=6.019, nll_loss=2.725, mask_ins=0.917, word_ins_ml=4.279, word_reposition=0.297, kpe=0.527, ppl=64.87, wps=5199.7, ups=0.36, wpb=14579, bsz=1024, num_updates=93600, lr=0.000115563, gnorm=1.968, clip=0, loss_scale=1024, train_wall=249, wall=269961
2022-08-09 07:39:18 | INFO | train_inner | epoch 026:    961 / 3715 loss=6.002, nll_loss=2.702, mask_ins=0.916, word_ins_ml=4.259, word_reposition=0.294, kpe=0.533, ppl=64.09, wps=5285, ups=0.36, wpb=14738.8, bsz=1024, num_updates=93700, lr=0.000115501, gnorm=1.967, clip=0, loss_scale=1024, train_wall=247, wall=270240
2022-08-09 07:43:56 | INFO | train_inner | epoch 026:   1061 / 3715 loss=6.031, nll_loss=2.721, mask_ins=0.92, word_ins_ml=4.276, word_reposition=0.305, kpe=0.53, ppl=65.39, wps=5268.4, ups=0.36, wpb=14694.5, bsz=1024, num_updates=93800, lr=0.000115439, gnorm=1.974, clip=0, loss_scale=1024, train_wall=247, wall=270519
2022-08-09 07:48:37 | INFO | train_inner | epoch 026:   1161 / 3715 loss=6.013, nll_loss=2.703, mask_ins=0.917, word_ins_ml=4.259, word_reposition=0.304, kpe=0.533, ppl=64.57, wps=5253.4, ups=0.36, wpb=14731.5, bsz=1024, num_updates=93900, lr=0.000115378, gnorm=1.966, clip=0, loss_scale=1341, train_wall=249, wall=270799
2022-08-09 07:53:16 | INFO | train_inner | epoch 026:   1261 / 3715 loss=6.018, nll_loss=2.719, mask_ins=0.916, word_ins_ml=4.274, word_reposition=0.297, kpe=0.531, ppl=64.79, wps=5254.9, ups=0.36, wpb=14660, bsz=1024, num_updates=94000, lr=0.000115316, gnorm=1.934, clip=0, loss_scale=2048, train_wall=247, wall=271078
2022-08-09 07:57:55 | INFO | train_inner | epoch 026:   1361 / 3715 loss=6.01, nll_loss=2.705, mask_ins=0.924, word_ins_ml=4.261, word_reposition=0.295, kpe=0.53, ppl=64.43, wps=5223.7, ups=0.36, wpb=14588.1, bsz=1024, num_updates=94100, lr=0.000115255, gnorm=1.964, clip=0, loss_scale=2048, train_wall=248, wall=271358
2022-08-09 08:02:35 | INFO | train_inner | epoch 026:   1461 / 3715 loss=5.997, nll_loss=2.697, mask_ins=0.919, word_ins_ml=4.254, word_reposition=0.295, kpe=0.528, ppl=63.85, wps=5218.5, ups=0.36, wpb=14580.4, bsz=1024, num_updates=94200, lr=0.000115194, gnorm=1.945, clip=0, loss_scale=2048, train_wall=248, wall=271637
2022-08-09 08:07:14 | INFO | train_inner | epoch 026:   1561 / 3715 loss=6.02, nll_loss=2.718, mask_ins=0.919, word_ins_ml=4.272, word_reposition=0.295, kpe=0.533, ppl=64.88, wps=5244.5, ups=0.36, wpb=14630.3, bsz=1024, num_updates=94300, lr=0.000115133, gnorm=1.945, clip=0, loss_scale=2048, train_wall=248, wall=271916
2022-08-09 08:11:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-09 08:11:55 | INFO | train_inner | epoch 026:   1662 / 3715 loss=6.02, nll_loss=2.718, mask_ins=0.92, word_ins_ml=4.273, word_reposition=0.292, kpe=0.535, ppl=64.89, wps=5206.7, ups=0.35, wpb=14669.6, bsz=1024, num_updates=94400, lr=0.000115072, gnorm=1.953, clip=0, loss_scale=2068, train_wall=250, wall=272198
2022-08-09 08:16:34 | INFO | train_inner | epoch 026:   1762 / 3715 loss=6.015, nll_loss=2.715, mask_ins=0.913, word_ins_ml=4.269, word_reposition=0.302, kpe=0.532, ppl=64.68, wps=5240.7, ups=0.36, wpb=14582.3, bsz=1024, num_updates=94500, lr=0.000115011, gnorm=1.955, clip=0, loss_scale=2048, train_wall=247, wall=272476
2022-08-09 08:21:52 | INFO | train_inner | epoch 026:   1862 / 3715 loss=6.007, nll_loss=2.707, mask_ins=0.911, word_ins_ml=4.263, word_reposition=0.297, kpe=0.536, ppl=64.29, wps=4619.4, ups=0.31, wpb=14721.8, bsz=1024, num_updates=94600, lr=0.00011495, gnorm=1.936, clip=0, loss_scale=2048, train_wall=248, wall=272795
2022-08-09 08:26:31 | INFO | train_inner | epoch 026:   1962 / 3715 loss=6.033, nll_loss=2.726, mask_ins=0.92, word_ins_ml=4.28, word_reposition=0.298, kpe=0.535, ppl=65.47, wps=5242.1, ups=0.36, wpb=14598.7, bsz=1024, num_updates=94700, lr=0.000114889, gnorm=1.97, clip=0, loss_scale=2048, train_wall=247, wall=273073
2022-08-09 08:33:18 | INFO | train_inner | epoch 026:   2062 / 3715 loss=6.021, nll_loss=2.712, mask_ins=0.916, word_ins_ml=4.267, word_reposition=0.301, kpe=0.537, ppl=64.95, wps=3605.9, ups=0.25, wpb=14704.2, bsz=1024, num_updates=94800, lr=0.000114829, gnorm=1.935, clip=0, loss_scale=2048, train_wall=376, wall=273481
2022-08-09 08:38:00 | INFO | train_inner | epoch 026:   2162 / 3715 loss=6.039, nll_loss=2.73, mask_ins=0.919, word_ins_ml=4.284, word_reposition=0.299, kpe=0.538, ppl=65.78, wps=5208.9, ups=0.36, wpb=14650.7, bsz=1024, num_updates=94900, lr=0.000114768, gnorm=1.969, clip=0, loss_scale=2191, train_wall=250, wall=273762
2022-08-09 08:42:40 | INFO | train_inner | epoch 026:   2262 / 3715 loss=6.023, nll_loss=2.714, mask_ins=0.915, word_ins_ml=4.269, word_reposition=0.3, kpe=0.538, ppl=65.03, wps=5272, ups=0.36, wpb=14763.4, bsz=1024, num_updates=95000, lr=0.000114708, gnorm=1.945, clip=0, loss_scale=4096, train_wall=249, wall=274042
2022-08-09 08:47:20 | INFO | train_inner | epoch 026:   2362 / 3715 loss=6.014, nll_loss=2.704, mask_ins=0.917, word_ins_ml=4.26, word_reposition=0.299, kpe=0.538, ppl=64.63, wps=5235.7, ups=0.36, wpb=14658.5, bsz=1024, num_updates=95100, lr=0.000114648, gnorm=1.97, clip=0, loss_scale=4096, train_wall=248, wall=274322
2022-08-09 08:51:59 | INFO | train_inner | epoch 026:   2462 / 3715 loss=6.046, nll_loss=2.737, mask_ins=0.921, word_ins_ml=4.289, word_reposition=0.297, kpe=0.539, ppl=66.05, wps=5251.6, ups=0.36, wpb=14658.5, bsz=1024, num_updates=95200, lr=0.000114587, gnorm=1.951, clip=0, loss_scale=4096, train_wall=248, wall=274601
2022-08-09 08:56:38 | INFO | train_inner | epoch 026:   2562 / 3715 loss=6.044, nll_loss=2.733, mask_ins=0.922, word_ins_ml=4.286, word_reposition=0.297, kpe=0.539, ppl=65.99, wps=5237.7, ups=0.36, wpb=14634.2, bsz=1024, num_updates=95300, lr=0.000114527, gnorm=1.941, clip=0, loss_scale=4096, train_wall=248, wall=274881
2022-08-09 08:59:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-09 09:01:22 | INFO | train_inner | epoch 026:   2663 / 3715 loss=6.033, nll_loss=2.72, mask_ins=0.916, word_ins_ml=4.274, word_reposition=0.304, kpe=0.538, ppl=65.47, wps=5162.5, ups=0.35, wpb=14622.5, bsz=1024, num_updates=95400, lr=0.000114467, gnorm=1.933, clip=0, loss_scale=3467, train_wall=251, wall=275164
2022-08-09 09:05:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-09 09:06:03 | INFO | train_inner | epoch 026:   2764 / 3715 loss=6.045, nll_loss=2.73, mask_ins=0.921, word_ins_ml=4.283, word_reposition=0.3, kpe=0.542, ppl=66.04, wps=5221.1, ups=0.36, wpb=14685.4, bsz=1024, num_updates=95500, lr=0.000114407, gnorm=1.971, clip=0, loss_scale=1926, train_wall=250, wall=275445
2022-08-09 09:10:42 | INFO | train_inner | epoch 026:   2864 / 3715 loss=6.031, nll_loss=2.724, mask_ins=0.918, word_ins_ml=4.278, word_reposition=0.295, kpe=0.54, ppl=65.41, wps=5217.9, ups=0.36, wpb=14571.6, bsz=1024, num_updates=95600, lr=0.000114347, gnorm=1.947, clip=0, loss_scale=1024, train_wall=248, wall=275724
2022-08-09 09:15:21 | INFO | train_inner | epoch 026:   2964 / 3715 loss=6.037, nll_loss=2.729, mask_ins=0.92, word_ins_ml=4.282, word_reposition=0.294, kpe=0.542, ppl=65.66, wps=5263.4, ups=0.36, wpb=14694.1, bsz=1023.8, num_updates=95700, lr=0.000114288, gnorm=1.941, clip=0, loss_scale=1024, train_wall=248, wall=276004
2022-08-09 09:20:00 | INFO | train_inner | epoch 026:   3064 / 3715 loss=6.046, nll_loss=2.738, mask_ins=0.918, word_ins_ml=4.29, word_reposition=0.296, kpe=0.542, ppl=66.06, wps=5265.2, ups=0.36, wpb=14670.6, bsz=1024, num_updates=95800, lr=0.000114228, gnorm=1.961, clip=0, loss_scale=1024, train_wall=247, wall=276282
2022-08-09 09:24:39 | INFO | train_inner | epoch 026:   3164 / 3715 loss=6.06, nll_loss=2.737, mask_ins=0.918, word_ins_ml=4.29, word_reposition=0.305, kpe=0.546, ppl=66.7, wps=5305.6, ups=0.36, wpb=14830.5, bsz=1024, num_updates=95900, lr=0.000114168, gnorm=1.936, clip=0, loss_scale=1024, train_wall=248, wall=276562
2022-08-09 09:29:20 | INFO | train_inner | epoch 026:   3264 / 3715 loss=6.059, nll_loss=2.743, mask_ins=0.922, word_ins_ml=4.294, word_reposition=0.302, kpe=0.541, ppl=66.68, wps=5232.8, ups=0.36, wpb=14682.2, bsz=1024, num_updates=96000, lr=0.000114109, gnorm=1.928, clip=0, loss_scale=1024, train_wall=249, wall=276842
2022-08-09 09:34:01 | INFO | train_inner | epoch 026:   3364 / 3715 loss=6.05, nll_loss=2.735, mask_ins=0.917, word_ins_ml=4.288, word_reposition=0.3, kpe=0.545, ppl=66.27, wps=5254.9, ups=0.36, wpb=14756.4, bsz=1024, num_updates=96100, lr=0.000114049, gnorm=1.94, clip=0, loss_scale=2048, train_wall=250, wall=277123
2022-08-09 09:37:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-09 09:38:42 | INFO | train_inner | epoch 026:   3465 / 3715 loss=6.034, nll_loss=2.721, mask_ins=0.915, word_ins_ml=4.275, word_reposition=0.299, kpe=0.545, ppl=65.51, wps=5229.4, ups=0.36, wpb=14728.9, bsz=1024, num_updates=96200, lr=0.00011399, gnorm=1.988, clip=0, loss_scale=1703, train_wall=250, wall=277405
2022-08-09 09:42:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-09 09:43:24 | INFO | train_inner | epoch 026:   3566 / 3715 loss=6.049, nll_loss=2.734, mask_ins=0.919, word_ins_ml=4.287, word_reposition=0.301, kpe=0.541, ppl=66.19, wps=5187.5, ups=0.36, wpb=14578, bsz=1024, num_updates=96300, lr=0.000113931, gnorm=2.191, clip=0, loss_scale=918, train_wall=249, wall=277686
2022-08-09 09:43:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-09 09:43:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-09 09:48:08 | INFO | train_inner | epoch 026:   3668 / 3715 loss=6.086, nll_loss=2.756, mask_ins=0.92, word_ins_ml=4.306, word_reposition=0.309, kpe=0.551, ppl=67.92, wps=5129.7, ups=0.35, wpb=14568.8, bsz=1024, num_updates=96400, lr=0.000113872, gnorm=3.022, clip=1, loss_scale=139, train_wall=253, wall=277970
2022-08-09 09:50:16 | INFO | train | epoch 026 | loss 6.023 | nll_loss 2.717 | mask_ins 0.917 | word_ins_ml 4.272 | word_reposition 0.299 | kpe 0.535 | ppl 65.05 | wps 5033.6 | ups 0.34 | wpb 14662.9 | bsz 1023.7 | num_updates 96447 | lr 0.000113844 | gnorm 1.993 | clip 0 | loss_scale 1771 | train_wall 9338 | wall 278099
2022-08-09 09:53:59 | INFO | valid | epoch 026 | valid on 'valid' subset | loss nan | nll_loss 3.179 | mask_ins 0.997 | word_ins_ml 4.726 | word_reposition 0.365 | kpe nan | ppl nan | wps 12336.4 | wpb 1849.4 | bsz 127.9 | num_updates 96447 | best_loss nan
2022-08-09 09:54:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 26 @ 96447 updates, score nan) (writing took 8.35473309084773 seconds)
2022-08-09 09:56:35 | INFO | train_inner | epoch 027:     53 / 3715 loss=6.017, nll_loss=2.708, mask_ins=0.92, word_ins_ml=4.263, word_reposition=0.302, kpe=0.532, ppl=64.76, wps=2862.4, ups=0.2, wpb=14517.8, bsz=1014.7, num_updates=96500, lr=0.000113813, gnorm=2.228, clip=0, loss_scale=128, train_wall=246, wall=278477
2022-08-09 10:01:13 | INFO | train_inner | epoch 027:    153 / 3715 loss=5.964, nll_loss=2.679, mask_ins=0.911, word_ins_ml=4.239, word_reposition=0.299, kpe=0.515, ppl=62.43, wps=5261.7, ups=0.36, wpb=14667.3, bsz=1024, num_updates=96600, lr=0.000113754, gnorm=1.996, clip=0, loss_scale=128, train_wall=248, wall=278756
2022-08-09 10:05:52 | INFO | train_inner | epoch 027:    253 / 3715 loss=5.955, nll_loss=2.675, mask_ins=0.911, word_ins_ml=4.235, word_reposition=0.295, kpe=0.514, ppl=62.05, wps=5234.2, ups=0.36, wpb=14561.8, bsz=1024, num_updates=96700, lr=0.000113695, gnorm=1.997, clip=0, loss_scale=128, train_wall=247, wall=279034
2022-08-09 10:10:31 | INFO | train_inner | epoch 027:    353 / 3715 loss=5.978, nll_loss=2.69, mask_ins=0.915, word_ins_ml=4.248, word_reposition=0.295, kpe=0.52, ppl=63.05, wps=5290.2, ups=0.36, wpb=14769.6, bsz=1024, num_updates=96800, lr=0.000113636, gnorm=1.989, clip=0, loss_scale=128, train_wall=248, wall=279313
2022-08-09 10:15:10 | INFO | train_inner | epoch 027:    453 / 3715 loss=5.952, nll_loss=2.667, mask_ins=0.917, word_ins_ml=4.227, word_reposition=0.291, kpe=0.517, ppl=61.92, wps=5232, ups=0.36, wpb=14614.9, bsz=1024, num_updates=96900, lr=0.000113578, gnorm=1.968, clip=0, loss_scale=237, train_wall=248, wall=279593
2022-08-09 10:19:51 | INFO | train_inner | epoch 027:    553 / 3715 loss=5.96, nll_loss=2.675, mask_ins=0.907, word_ins_ml=4.234, word_reposition=0.299, kpe=0.519, ppl=62.24, wps=5221.7, ups=0.36, wpb=14653.1, bsz=1024, num_updates=97000, lr=0.000113519, gnorm=1.979, clip=0, loss_scale=256, train_wall=249, wall=279873
2022-08-09 10:24:30 | INFO | train_inner | epoch 027:    653 / 3715 loss=5.981, nll_loss=2.685, mask_ins=0.913, word_ins_ml=4.244, word_reposition=0.301, kpe=0.523, ppl=63.18, wps=5277.9, ups=0.36, wpb=14742.8, bsz=1024, num_updates=97100, lr=0.000113461, gnorm=1.995, clip=0, loss_scale=256, train_wall=248, wall=280153
2022-08-09 10:29:10 | INFO | train_inner | epoch 027:    753 / 3715 loss=5.996, nll_loss=2.711, mask_ins=0.91, word_ins_ml=4.267, word_reposition=0.297, kpe=0.523, ppl=63.82, wps=5251.1, ups=0.36, wpb=14711.4, bsz=1024, num_updates=97200, lr=0.000113402, gnorm=1.977, clip=0, loss_scale=256, train_wall=249, wall=280433
2022-08-09 10:33:50 | INFO | train_inner | epoch 027:    853 / 3715 loss=5.975, nll_loss=2.687, mask_ins=0.912, word_ins_ml=4.246, word_reposition=0.298, kpe=0.519, ppl=62.88, wps=5234.1, ups=0.36, wpb=14630.2, bsz=1024, num_updates=97300, lr=0.000113344, gnorm=1.988, clip=0, loss_scale=256, train_wall=248, wall=280712
2022-08-09 10:38:28 | INFO | train_inner | epoch 027:    953 / 3715 loss=5.995, nll_loss=2.701, mask_ins=0.913, word_ins_ml=4.258, word_reposition=0.299, kpe=0.526, ppl=63.78, wps=5302.9, ups=0.36, wpb=14768.7, bsz=1024, num_updates=97400, lr=0.000113286, gnorm=1.963, clip=0, loss_scale=443, train_wall=247, wall=280991
2022-08-09 10:43:08 | INFO | train_inner | epoch 027:   1053 / 3715 loss=5.978, nll_loss=2.692, mask_ins=0.913, word_ins_ml=4.25, word_reposition=0.294, kpe=0.521, ppl=63.01, wps=5219.8, ups=0.36, wpb=14571, bsz=1024, num_updates=97500, lr=0.000113228, gnorm=2.004, clip=0, loss_scale=512, train_wall=247, wall=281270
2022-08-09 10:47:47 | INFO | train_inner | epoch 027:   1153 / 3715 loss=5.977, nll_loss=2.689, mask_ins=0.914, word_ins_ml=4.247, word_reposition=0.292, kpe=0.524, ppl=62.98, wps=5241.9, ups=0.36, wpb=14634.2, bsz=1024, num_updates=97600, lr=0.00011317, gnorm=1.978, clip=0, loss_scale=512, train_wall=248, wall=281549
2022-08-09 10:51:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-09 10:52:28 | INFO | train_inner | epoch 027:   1254 / 3715 loss=5.98, nll_loss=2.685, mask_ins=0.912, word_ins_ml=4.243, word_reposition=0.297, kpe=0.527, ppl=63.11, wps=5225.2, ups=0.35, wpb=14719.9, bsz=1023.8, num_updates=97700, lr=0.000113112, gnorm=1.967, clip=0, loss_scale=454, train_wall=250, wall=281831
2022-08-09 10:57:06 | INFO | train_inner | epoch 027:   1354 / 3715 loss=5.967, nll_loss=2.675, mask_ins=0.909, word_ins_ml=4.234, word_reposition=0.3, kpe=0.524, ppl=62.55, wps=5278.6, ups=0.36, wpb=14667.9, bsz=1024, num_updates=97800, lr=0.000113054, gnorm=1.992, clip=0, loss_scale=256, train_wall=247, wall=282109
2022-08-09 11:01:46 | INFO | train_inner | epoch 027:   1454 / 3715 loss=5.99, nll_loss=2.699, mask_ins=0.911, word_ins_ml=4.255, word_reposition=0.295, kpe=0.529, ppl=63.54, wps=5277.9, ups=0.36, wpb=14765.9, bsz=1024, num_updates=97900, lr=0.000112996, gnorm=1.961, clip=0, loss_scale=256, train_wall=249, wall=282388
2022-08-09 11:06:24 | INFO | train_inner | epoch 027:   1554 / 3715 loss=5.997, nll_loss=2.702, mask_ins=0.914, word_ins_ml=4.258, word_reposition=0.3, kpe=0.525, ppl=63.87, wps=5276, ups=0.36, wpb=14679.2, bsz=1024, num_updates=98000, lr=0.000112938, gnorm=1.969, clip=0, loss_scale=256, train_wall=247, wall=282667
2022-08-09 11:11:03 | INFO | train_inner | epoch 027:   1654 / 3715 loss=5.986, nll_loss=2.7, mask_ins=0.911, word_ins_ml=4.256, word_reposition=0.293, kpe=0.526, ppl=63.39, wps=5245.7, ups=0.36, wpb=14624, bsz=1024, num_updates=98100, lr=0.000112881, gnorm=1.963, clip=0, loss_scale=256, train_wall=247, wall=282945
2022-08-09 11:15:43 | INFO | train_inner | epoch 027:   1754 / 3715 loss=5.991, nll_loss=2.693, mask_ins=0.911, word_ins_ml=4.25, word_reposition=0.302, kpe=0.527, ppl=63.58, wps=5251.7, ups=0.36, wpb=14681.6, bsz=1024, num_updates=98200, lr=0.000112823, gnorm=1.969, clip=0, loss_scale=284, train_wall=248, wall=283225
2022-08-09 11:20:23 | INFO | train_inner | epoch 027:   1854 / 3715 loss=5.992, nll_loss=2.696, mask_ins=0.915, word_ins_ml=4.253, word_reposition=0.295, kpe=0.529, ppl=63.63, wps=5217.8, ups=0.36, wpb=14640, bsz=1024, num_updates=98300, lr=0.000112766, gnorm=2.014, clip=0, loss_scale=512, train_wall=249, wall=283506
2022-08-09 11:25:02 | INFO | train_inner | epoch 027:   1954 / 3715 loss=6.001, nll_loss=2.703, mask_ins=0.916, word_ins_ml=4.259, word_reposition=0.299, kpe=0.527, ppl=64.05, wps=5244.2, ups=0.36, wpb=14632.8, bsz=1024, num_updates=98400, lr=0.000112709, gnorm=1.984, clip=0, loss_scale=512, train_wall=248, wall=283785
2022-08-09 11:29:42 | INFO | train_inner | epoch 027:   2054 / 3715 loss=5.997, nll_loss=2.697, mask_ins=0.91, word_ins_ml=4.254, word_reposition=0.301, kpe=0.531, ppl=63.85, wps=5264.3, ups=0.36, wpb=14718.3, bsz=1024, num_updates=98500, lr=0.000112651, gnorm=2, clip=0, loss_scale=512, train_wall=249, wall=284064
2022-08-09 11:34:21 | INFO | train_inner | epoch 027:   2154 / 3715 loss=5.992, nll_loss=2.7, mask_ins=0.914, word_ins_ml=4.256, word_reposition=0.294, kpe=0.528, ppl=63.66, wps=5211.8, ups=0.36, wpb=14570.7, bsz=1024, num_updates=98600, lr=0.000112594, gnorm=1.977, clip=0, loss_scale=512, train_wall=249, wall=284344
2022-08-09 11:39:01 | INFO | train_inner | epoch 027:   2254 / 3715 loss=6.003, nll_loss=2.698, mask_ins=0.917, word_ins_ml=4.255, word_reposition=0.303, kpe=0.529, ppl=64.13, wps=5233.2, ups=0.36, wpb=14648.8, bsz=1024, num_updates=98700, lr=0.000112537, gnorm=1.968, clip=0, loss_scale=512, train_wall=248, wall=284624
2022-08-09 11:43:41 | INFO | train_inner | epoch 027:   2354 / 3715 loss=6.01, nll_loss=2.714, mask_ins=0.914, word_ins_ml=4.269, word_reposition=0.297, kpe=0.53, ppl=64.45, wps=5244.8, ups=0.36, wpb=14659.6, bsz=1024, num_updates=98800, lr=0.00011248, gnorm=2.013, clip=0, loss_scale=1019, train_wall=248, wall=284903
2022-08-09 11:48:20 | INFO | train_inner | epoch 027:   2454 / 3715 loss=6.017, nll_loss=2.715, mask_ins=0.913, word_ins_ml=4.27, word_reposition=0.302, kpe=0.532, ppl=64.78, wps=5236.8, ups=0.36, wpb=14597.9, bsz=1024, num_updates=98900, lr=0.000112423, gnorm=1.978, clip=0, loss_scale=1024, train_wall=247, wall=285182
2022-08-09 11:53:00 | INFO | train_inner | epoch 027:   2554 / 3715 loss=6.021, nll_loss=2.72, mask_ins=0.921, word_ins_ml=4.273, word_reposition=0.293, kpe=0.534, ppl=64.94, wps=5250.7, ups=0.36, wpb=14705.2, bsz=1024, num_updates=99000, lr=0.000112367, gnorm=1.978, clip=0, loss_scale=1024, train_wall=249, wall=285462
2022-08-09 11:57:40 | INFO | train_inner | epoch 027:   2654 / 3715 loss=6.023, nll_loss=2.718, mask_ins=0.916, word_ins_ml=4.272, word_reposition=0.302, kpe=0.532, ppl=65.02, wps=5224.3, ups=0.36, wpb=14665.7, bsz=1024, num_updates=99100, lr=0.00011231, gnorm=1.973, clip=0, loss_scale=1024, train_wall=249, wall=285743
2022-08-09 12:02:21 | INFO | train_inner | epoch 027:   2754 / 3715 loss=5.995, nll_loss=2.699, mask_ins=0.912, word_ins_ml=4.255, word_reposition=0.298, kpe=0.53, ppl=63.79, wps=5191.1, ups=0.36, wpb=14550.9, bsz=1024, num_updates=99200, lr=0.000112253, gnorm=1.991, clip=0, loss_scale=1024, train_wall=249, wall=286023
2022-08-09 12:04:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-09 12:07:02 | INFO | train_inner | epoch 027:   2855 / 3715 loss=6.006, nll_loss=2.706, mask_ins=0.911, word_ins_ml=4.261, word_reposition=0.298, kpe=0.536, ppl=64.27, wps=5245.2, ups=0.36, wpb=14760.1, bsz=1024, num_updates=99300, lr=0.000112197, gnorm=1.989, clip=0, loss_scale=1338, train_wall=250, wall=286305
2022-08-09 12:11:42 | INFO | train_inner | epoch 027:   2955 / 3715 loss=5.996, nll_loss=2.701, mask_ins=0.912, word_ins_ml=4.257, word_reposition=0.295, kpe=0.532, ppl=63.83, wps=5216.2, ups=0.36, wpb=14601.2, bsz=1024, num_updates=99400, lr=0.00011214, gnorm=1.957, clip=0, loss_scale=1024, train_wall=248, wall=286584
2022-08-09 12:16:22 | INFO | train_inner | epoch 027:   3055 / 3715 loss=5.997, nll_loss=2.696, mask_ins=0.912, word_ins_ml=4.253, word_reposition=0.297, kpe=0.535, ppl=63.85, wps=5233, ups=0.36, wpb=14641.7, bsz=1024, num_updates=99500, lr=0.000112084, gnorm=1.971, clip=0, loss_scale=1024, train_wall=249, wall=286864
2022-08-09 12:21:02 | INFO | train_inner | epoch 027:   3155 / 3715 loss=6, nll_loss=2.701, mask_ins=0.913, word_ins_ml=4.257, word_reposition=0.295, kpe=0.535, ppl=64.01, wps=5246, ups=0.36, wpb=14673.1, bsz=1024, num_updates=99600, lr=0.000112028, gnorm=1.972, clip=0, loss_scale=1024, train_wall=248, wall=287144
2022-08-09 12:25:40 | INFO | train_inner | epoch 027:   3255 / 3715 loss=6.027, nll_loss=2.719, mask_ins=0.92, word_ins_ml=4.273, word_reposition=0.296, kpe=0.538, ppl=65.21, wps=5276.9, ups=0.36, wpb=14717.2, bsz=1024, num_updates=99700, lr=0.000111971, gnorm=1.963, clip=0, loss_scale=1024, train_wall=248, wall=287423
2022-08-09 12:25:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-09 12:30:22 | INFO | train_inner | epoch 027:   3356 / 3715 loss=6.047, nll_loss=2.734, mask_ins=0.921, word_ins_ml=4.286, word_reposition=0.305, kpe=0.534, ppl=66.1, wps=5209.5, ups=0.35, wpb=14676.5, bsz=1024, num_updates=99800, lr=0.000111915, gnorm=2.032, clip=0, loss_scale=522, train_wall=250, wall=287705
2022-08-09 12:35:01 | INFO | train_inner | epoch 027:   3456 / 3715 loss=6.006, nll_loss=2.709, mask_ins=0.912, word_ins_ml=4.265, word_reposition=0.294, kpe=0.536, ppl=64.29, wps=5254.9, ups=0.36, wpb=14641, bsz=1024, num_updates=99900, lr=0.000111859, gnorm=1.963, clip=0, loss_scale=512, train_wall=247, wall=287983
2022-08-09 12:39:41 | INFO | train_inner | epoch 027:   3556 / 3715 loss=6.048, nll_loss=2.736, mask_ins=0.918, word_ins_ml=4.288, word_reposition=0.303, kpe=0.539, ppl=66.18, wps=5262.8, ups=0.36, wpb=14747.5, bsz=1024, num_updates=100000, lr=0.000111803, gnorm=1.998, clip=0, loss_scale=512, train_wall=249, wall=288263
2022-08-09 12:39:41 | INFO | train | epoch 027 | loss 5.994 | nll_loss 2.699 | mask_ins 0.913 | word_ins_ml 4.255 | word_reposition 0.298 | kpe 0.527 | ppl 63.72 | wps 5125.8 | ups 0.35 | wpb 14664.2 | bsz 1024 | num_updates 100000 | lr 0.000111803 | gnorm 1.985 | clip 0 | loss_scale 552 | train_wall 8822 | wall 288263
2022-08-09 12:43:22 | INFO | valid | epoch 027 | valid on 'valid' subset | loss nan | nll_loss 3.167 | mask_ins 0.987 | word_ins_ml 4.716 | word_reposition 0.346 | kpe nan | ppl nan | wps 12381.5 | wpb 1849.4 | bsz 127.9 | num_updates 100000 | best_loss nan
2022-08-09 12:43:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased_Ggw/checkpoint_last.pt (epoch 27 @ 100000 updates, score nan) (writing took 8.099455431103706 seconds)
2022-08-09 12:43:31 | INFO | fairseq_cli.train | done training in 288154.3 seconds
