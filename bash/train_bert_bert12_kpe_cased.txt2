nohup: ignoring input
2022-08-11 15:03:44 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:18274
2022-08-11 15:03:44 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:18274
2022-08-11 15:03:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-08-11 15:03:44 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:18274
2022-08-11 15:03:44 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:18274
2022-08-11 15:03:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-08-11 15:03:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-08-11 15:03:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-08-11 15:03:44 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-11 15:03:44 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-08-11 15:03:44 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-11 15:03:44 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-08-11 15:03:44 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-11 15:03:44 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-08-11 15:03:44 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-11 15:03:44 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-08-11 15:03:48 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=4, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=4, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:18274', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_kpe_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-08-11 15:03:48 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-08-11 15:03:48 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-08-11 15:03:48 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.source
2022-08-11 15:03:48 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.target
2022-08-11 15:03:48 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]388it [00:00, 3872.06it/s]389it [00:00, 3888.25it/s]390it [00:00, 3888.12it/s]389it [00:00, 3878.40it/s]778it [00:00, 3519.64it/s]779it [00:00, 3490.77it/s]776it [00:00, 3388.53it/s]777it [00:00, 3498.26it/s]1145it [00:00, 3583.98it/s]1131it [00:00, 3483.31it/s]1141it [00:00, 3496.84it/s]1143it [00:00, 3566.84it/s]1506it [00:00, 3443.02it/s]1481it [00:00, 3358.08it/s]1494it [00:00, 3362.44it/s]1502it [00:00, 3369.12it/s]1892it [00:00, 3584.77it/s]1867it [00:00, 3528.64it/s]1862it [00:00, 3468.42it/s]1887it [00:00, 3531.95it/s]2253it [00:00, 3502.75it/s]2236it [00:00, 3448.12it/s]2243it [00:00, 3455.03it/s]2236it [00:00, 3405.40it/s]2649it [00:00, 3644.18it/s]2612it [00:00, 3543.96it/s]2621it [00:00, 3556.05it/s]2632it [00:00, 3574.61it/s]3044it [00:00, 3737.16it/s]3006it [00:00, 3656.45it/s]3014it [00:00, 3670.17it/s]3022it [00:00, 3673.29it/s]3419it [00:00, 3599.90it/s]3373it [00:00, 3524.71it/s]3383it [00:00, 3553.24it/s]3392it [00:00, 3498.12it/s]3807it [00:01, 3681.37it/s]3758it [00:01, 3619.18it/s]3771it [00:01, 3648.70it/s]3776it [00:01, 3595.62it/s]4177it [00:01, 3575.53it/s]4122it [00:01, 3467.81it/s]4138it [00:01, 3484.18it/s]4138it [00:01, 3493.40it/s]4555it [00:01, 3634.56it/s]4497it [00:01, 3547.18it/s]4516it [00:01, 3568.97it/s]4495it [00:01, 3513.86it/s]4920it [00:01, 3523.19it/s]4854it [00:01, 3444.61it/s]4875it [00:01, 3469.76it/s]4848it [00:01, 3419.82it/s]5295it [00:01, 3587.18it/s]5216it [00:01, 3494.83it/s]5239it [00:01, 3518.33it/s]5228it [00:01, 3527.44it/s]5583it [00:01, 3543.39it/s]5655it [00:01, 3446.32it/s]5594it [00:01, 3565.19it/s]5596it [00:01, 3392.85it/s]6002it [00:01, 3435.79it/s]5939it [00:01, 3377.51it/s]5951it [00:01, 3435.91it/s]5952it [00:01, 3332.66it/s]6357it [00:01, 3467.78it/s]6293it [00:01, 3421.69it/s]6297it [00:01, 3430.34it/s]6304it [00:01, 3385.36it/s]6705it [00:02, 2091.48it/s]6637it [00:02, 1965.51it/s]6646it [00:02, 1985.22it/s]6642it [00:02, 1946.76it/s]7060it [00:02, 2383.99it/s]6975it [00:02, 2236.47it/s]6934it [00:02, 2157.80it/s]6996it [00:02, 2247.78it/s]7360it [00:02, 2515.67it/s]7329it [00:02, 2516.53it/s]7287it [00:02, 2454.03it/s]7339it [00:02, 2500.25it/s]7700it [00:02, 2728.47it/s]7639it [00:02, 2595.99it/s]7585it [00:02, 2495.79it/s]7648it [00:02, 2561.66it/s]8058it [00:02, 2945.10it/s]7993it [00:02, 2827.91it/s]7938it [00:02, 2749.66it/s]8002it [00:02, 2800.14it/s]8383it [00:02, 2956.43it/s]8310it [00:02, 2862.24it/s]8245it [00:02, 2804.13it/s]8317it [00:02, 2823.07it/s]8736it [00:02, 3111.87it/s]8646it [00:02, 2995.22it/s]8578it [00:02, 2943.84it/s]8669it [00:02, 3007.41it/s]9064it [00:02, 3053.18it/s]9001it [00:02, 3146.53it/s]8931it [00:02, 3106.33it/s]9011it [00:02, 3119.54it/s]9417it [00:02, 3184.11it/s]9330it [00:02, 3093.94it/s]9256it [00:02, 3057.22it/s]9338it [00:02, 3076.68it/s]9773it [00:03, 3289.43it/s]9670it [00:03, 3179.45it/s]9609it [00:03, 3190.11it/s]9694it [00:03, 3212.51it/s]10109it [00:03, 3174.29it/s]9996it [00:03, 3135.65it/s]9936it [00:03, 3089.00it/s]10024it [00:03, 3120.34it/s]10466it [00:03, 3285.02it/s]10351it [00:03, 3252.92it/s]10290it [00:03, 3214.90it/s]10381it [00:03, 3245.75it/s]10799it [00:03, 3210.65it/s]10647it [00:03, 3316.94it/s]10706it [00:03, 3187.85it/s]10711it [00:03, 3182.40it/s]11154it [00:03, 3307.49it/s]11048it [00:03, 3251.70it/s]10983it [00:03, 3222.94it/s]11066it [00:03, 3286.62it/s]11495it [00:03, 3335.79it/s]11401it [00:03, 3330.19it/s]11322it [00:03, 3268.92it/s]11423it [00:03, 3366.57it/s]11831it [00:03, 3238.91it/s]11652it [00:03, 3180.00it/s]11737it [00:03, 3194.40it/s]11763it [00:03, 3258.24it/s]12189it [00:03, 3337.19it/s]12008it [00:03, 3288.99it/s]12094it [00:03, 3300.02it/s]12122it [00:03, 3353.50it/s]12525it [00:03, 3248.44it/s]12355it [00:03, 3341.27it/s]12427it [00:03, 3216.39it/s]12460it [00:03, 3255.36it/s]12868it [00:03, 3299.36it/s]12691it [00:04, 3238.80it/s]12784it [00:04, 3316.13it/s]12818it [00:04, 3347.35it/s]13211it [00:04, 3217.51it/s]13045it [00:04, 3325.57it/s]13138it [00:04, 3379.25it/s]13368it [00:04, 3229.67it/s]
2022-08-11 15:03:52 | INFO | root | success load 13368 data
2022-08-11 15:03:52 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-08-11 15:03:52 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-08-11 15:03:52 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-08-11 15:03:52 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-08-11 15:03:52 | INFO | transformer.tokenization_utils | loading file None
2022-08-11 15:03:52 | INFO | transformer.tokenization_utils | loading file None
2022-08-11 15:03:52 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
13174it [00:04, 3407.99it/s]13368it [00:04, 3175.70it/s]
13368it [00:04, 3169.02it/s]
13368it [00:04, 3148.72it/s]
2022-08-11 15:03:53 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-11 15:03:53 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-11 15:03:53 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-08-11 15:03:57 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-08-11 15:03:57 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-08-11 15:03:57 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-11 15:03:57 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-11 15:03:57 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
2022-08-11 15:03:59 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-08-11 15:03:59 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 668
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 404
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']
2022-08-11 15:03:59 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-08-11 15:03:59 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-08-11 15:03:59 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 380755715)
2022-08-11 15:03:59 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 146472707)
2022-08-11 15:03:59 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 234283008)
2022-08-11 15:03:59 | INFO | fairseq_cli.train | training on 4 GPUs
2022-08-11 15:03:59 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 4
Trained parameters: len 668
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 404
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']Trained parameters: len 668
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 404
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']Trained parameters: len 668
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 404
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']2022-08-11 15:04:03 | INFO | fairseq.trainer | loaded checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_last.pt (epoch 7 @ 15689 updates)
2022-08-11 15:04:03 | INFO | fairseq.trainer | loading train data for epoch 7
2022-08-11 15:04:03 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.source
2022-08-11 15:04:03 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.target
2022-08-11 15:04:03 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]372it [00:00, 3711.99it/s]747it [00:00, 3732.29it/s]1121it [00:00, 3472.39it/s]1490it [00:00, 3554.27it/s]1848it [00:00, 3452.18it/s]2217it [00:00, 3526.81it/s]2571it [00:00, 3362.54it/s]2945it [00:00, 3474.31it/s]3318it [00:00, 3551.08it/s]3675it [00:01, 3401.23it/s]4049it [00:01, 3499.16it/s]4401it [00:01, 3391.06it/s]4772it [00:01, 3481.41it/s]5122it [00:01, 3366.07it/s]5490it [00:01, 3453.50it/s]5864it [00:01, 3535.55it/s]6219it [00:01, 3415.00it/s]6584it [00:01, 3482.17it/s]
start load cached examples train ...
0it [00:00, ?it/s]357it [00:00, 3568.15it/s]
start load cached examples train ...
0it [00:00, ?it/s]6934it [00:02, 1021.95it/s]728it [00:00, 3650.11it/s]366it [00:00, 3656.86it/s]7307it [00:02, 1316.27it/s]745it [00:00, 3729.52it/s]1094it [00:00, 3410.14it/s]7641it [00:03, 1578.19it/s]
start load cached examples train ...
0it [00:00, ?it/s]1467it [00:00, 3528.31it/s]1118it [00:00, 3480.23it/s]8019it [00:03, 1930.58it/s]364it [00:00, 3619.02it/s]1482it [00:00, 3539.98it/s]1822it [00:00, 3421.27it/s]8384it [00:03, 2250.42it/s]741it [00:00, 3702.38it/s]1838it [00:00, 3448.22it/s]2200it [00:00, 3536.62it/s]8720it [00:03, 2461.63it/s]1112it [00:00, 3407.41it/s]2209it [00:00, 3533.50it/s]2566it [00:00, 3573.33it/s]9100it [00:03, 2767.42it/s]1484it [00:00, 3511.49it/s]2584it [00:00, 3601.15it/s]2925it [00:00, 3464.29it/s]9446it [00:03, 2893.48it/s]1838it [00:00, 3386.79it/s]3292it [00:00, 3525.13it/s]2945it [00:00, 3460.71it/s]9826it [00:03, 3126.61it/s]2213it [00:00, 3503.42it/s]3317it [00:00, 3537.22it/s]3646it [00:01, 3417.85it/s]10178it [00:03, 3130.92it/s]2572it [00:00, 3529.76it/s]4004it [00:01, 3462.54it/s]3673it [00:01, 3400.87it/s]10564it [00:03, 3328.36it/s]2927it [00:00, 3413.09it/s]4045it [00:01, 3493.64it/s]4352it [00:01, 3401.20it/s]10938it [00:03, 3441.46it/s]3288it [00:00, 3470.57it/s]4714it [00:01, 3464.92it/s]11298it [00:04, 3397.30it/s]4397it [00:01, 3383.24it/s]3637it [00:01, 3365.42it/s]5086it [00:01, 3539.70it/s]11678it [00:04, 3511.46it/s]4773it [00:01, 3489.88it/s]3993it [00:01, 3421.97it/s]5441it [00:01, 3418.32it/s]5124it [00:01, 3365.69it/s]12038it [00:04, 3390.23it/s]4337it [00:01, 3338.58it/s]5815it [00:01, 3510.88it/s]5500it [00:01, 3477.92it/s]12426it [00:04, 3528.75it/s]4700it [00:01, 3423.15it/s]6168it [00:01, 3403.47it/s]5867it [00:01, 3530.92it/s]12785it [00:04, 3470.71it/s]5066it [00:01, 3491.86it/s]6542it [00:01, 3499.85it/s]13182it [00:04, 3612.40it/s]6222it [00:01, 3435.02it/s]5417it [00:01, 3364.68it/s]6591it [00:01, 3506.85it/s]13547it [00:04, 3499.31it/s]5787it [00:01, 3460.09it/s]13938it [00:04, 3614.33it/s]6135it [00:01, 3346.49it/s]14326it [00:04, 3689.69it/s]6508it [00:01, 3455.75it/s]14697it [00:05, 3573.46it/s]6856it [00:02, 3325.20it/s]15091it [00:05, 3677.07it/s]15461it [00:05, 3499.33it/s]15840it [00:05, 3580.50it/s]6894it [00:02, 1130.88it/s]6943it [00:02, 1147.22it/s]7260it [00:02, 1430.42it/s]7322it [00:02, 1464.29it/s]7637it [00:02, 1768.44it/s]7681it [00:02, 1775.04it/s]7954it [00:03, 1992.10it/s]7996it [00:02, 2011.99it/s]8332it [00:03, 2340.12it/s]8364it [00:03, 2341.98it/s]7191it [00:02, 1073.12it/s]8664it [00:03, 2504.55it/s]8695it [00:03, 2533.15it/s]7562it [00:02, 1378.72it/s]9042it [00:03, 2802.50it/s]9063it [00:03, 2805.46it/s]7866it [00:03, 1614.57it/s]9393it [00:03, 2869.36it/s]16201it [00:06, 1125.03it/s]9403it [00:03, 2915.80it/s]8227it [00:03, 1951.03it/s]9770it [00:03, 3098.90it/s]16593it [00:06, 1444.28it/s]9767it [00:03, 3106.35it/s]8552it [00:03, 2188.20it/s]10135it [00:03, 3246.48it/s]16951it [00:06, 1728.11it/s]10147it [00:03, 3294.79it/s]8914it [00:03, 2496.58it/s]10485it [00:03, 3250.20it/s]17333it [00:06, 2074.54it/s]9287it [00:03, 2785.98it/s]10501it [00:03, 3260.52it/s]10850it [00:03, 3360.27it/s]17721it [00:06, 2417.68it/s]10871it [00:03, 3381.99it/s]9628it [00:03, 2860.45it/s]11200it [00:03, 3325.33it/s]18071it [00:06, 2594.17it/s]10002it [00:03, 3086.36it/s]11223it [00:03, 3313.83it/s]11562it [00:04, 3408.33it/s]18459it [00:06, 2890.02it/s]11600it [00:04, 3442.71it/s]10345it [00:03, 3083.34it/s]11913it [00:04, 3308.50it/s]18814it [00:06, 2988.37it/s]10716it [00:03, 3253.41it/s]11952it [00:04, 3341.45it/s]12282it [00:04, 3416.37it/s]19191it [00:07, 3188.18it/s]12336it [00:04, 3482.13it/s]11073it [00:03, 3195.51it/s]12665it [00:04, 3535.11it/s]19546it [00:07, 3218.07it/s]12708it [00:04, 3550.36it/s]11448it [00:04, 3347.88it/s]13022it [00:04, 3468.03it/s]19910it [00:07, 3330.70it/s]13067it [00:04, 3480.65it/s]11805it [00:04, 3409.18it/s]13408it [00:04, 3580.58it/s]20288it [00:07, 3455.39it/s]13445it [00:04, 3566.77it/s]12154it [00:04, 3340.54it/s]13769it [00:04, 3475.33it/s]20648it [00:07, 3391.25it/s]13804it [00:04, 3479.08it/s]12522it [00:04, 3435.13it/s]14153it [00:04, 3575.56it/s]21027it [00:07, 3502.65it/s]14177it [00:04, 3549.52it/s]12870it [00:04, 3365.97it/s]14513it [00:04, 3456.29it/s]21385it [00:07, 3394.28it/s]14534it [00:04, 3479.60it/s]13245it [00:04, 3474.28it/s]14900it [00:04, 3572.97it/s]21767it [00:07, 3512.79it/s]14906it [00:04, 3547.52it/s]13596it [00:04, 3409.38it/s]15273it [00:05, 3495.00it/s]22124it [00:07, 3432.16it/s]15273it [00:05, 3478.13it/s]13962it [00:04, 3480.34it/s]15632it [00:05, 3521.46it/s]22507it [00:07, 3544.46it/s]15648it [00:05, 3556.53it/s]14345it [00:04, 3580.63it/s]16009it [00:05, 3592.33it/s]22865it [00:08, 3472.12it/s]16026it [00:05, 3619.51it/s]14705it [00:05, 3415.48it/s]23230it [00:08, 3522.01it/s]15087it [00:05, 3529.97it/s]23602it [00:08, 3573.97it/s]15443it [00:05, 3381.90it/s]23961it [00:08, 3463.63it/s]15817it [00:05, 3482.39it/s]24342it [00:08, 3562.58it/s]16168it [00:05, 3329.46it/s]24700it [00:08, 3454.22it/s]25070it [00:08, 3522.99it/s]25424it [00:08, 3400.52it/s]25803it [00:08, 3511.58it/s]26176it [00:08, 3573.22it/s]16370it [00:06, 924.35it/s] 26535it [00:09, 3462.27it/s]16389it [00:06, 929.74it/s] 16757it [00:06, 1210.06it/s]26904it [00:09, 3527.75it/s]16778it [00:06, 1217.78it/s]17086it [00:06, 1463.12it/s]27259it [00:09, 3444.49it/s]17092it [00:06, 1450.97it/s]17456it [00:06, 1794.05it/s]27640it [00:09, 3549.01it/s]17475it [00:06, 1803.76it/s]17842it [00:06, 2153.94it/s]17861it [00:06, 2163.20it/s]18187it [00:06, 2386.28it/s]16504it [00:06, 858.18it/s] 18207it [00:06, 2372.25it/s]18572it [00:06, 2707.40it/s]16882it [00:06, 1132.22it/s]18592it [00:06, 2695.16it/s]18925it [00:07, 2814.12it/s]17187it [00:06, 1358.80it/s]18943it [00:07, 2828.85it/s]19309it [00:07, 3067.97it/s]17565it [00:06, 1708.53it/s]19324it [00:07, 3067.28it/s]19663it [00:07, 3119.04it/s]17882it [00:06, 1955.65it/s]19677it [00:07, 3111.98it/s]20041it [00:07, 3295.82it/s]18260it [00:07, 2312.64it/s]20057it [00:07, 3296.06it/s]20396it [00:07, 3272.96it/s]18624it [00:07, 2602.25it/s]20412it [00:07, 3270.84it/s]20741it [00:07, 3271.17it/s]18968it [00:07, 2733.97it/s]20779it [00:07, 3379.65it/s]21120it [00:07, 3416.04it/s]19344it [00:07, 2987.95it/s]21158it [00:07, 3493.96it/s]21472it [00:07, 3361.49it/s]19691it [00:07, 3026.99it/s]21518it [00:07, 3413.60it/s]21852it [00:07, 3477.79it/s]20049it [00:07, 3172.55it/s]27997it [00:10, 811.90it/s] 21900it [00:07, 3527.42it/s]22206it [00:08, 3401.02it/s]28368it [00:10, 1063.76it/s]20392it [00:07, 3167.82it/s]22259it [00:08, 3439.91it/s]22594it [00:08, 3537.12it/s]28744it [00:10, 1361.39it/s]20764it [00:07, 3319.10it/s]22647it [00:08, 3563.75it/s]22952it [00:08, 3457.08it/s]29061it [00:10, 1606.03it/s]21138it [00:07, 3438.57it/s]23007it [00:08, 3484.02it/s]23311it [00:08, 3485.92it/s]29418it [00:11, 1925.60it/s]21492it [00:08, 3360.88it/s]23379it [00:08, 3551.87it/s]23681it [00:08, 3547.61it/s]29747it [00:11, 2172.17it/s]21843it [00:08, 3402.95it/s]23743it [00:08, 3441.68it/s]24038it [00:08, 3455.93it/s]30119it [00:11, 2498.43it/s]22189it [00:08, 3324.24it/s]24127it [00:08, 3553.37it/s]24416it [00:08, 3547.30it/s]30461it [00:11, 2672.79it/s]22568it [00:08, 3457.49it/s]24504it [00:08, 3614.31it/s]24773it [00:08, 3448.72it/s]30830it [00:11, 2921.72it/s]22918it [00:08, 3377.63it/s]24868it [00:08, 3492.85it/s]25143it [00:08, 3519.23it/s]31207it [00:11, 3140.10it/s]23286it [00:08, 3463.50it/s]25244it [00:08, 3568.91it/s]25497it [00:08, 3399.24it/s]31561it [00:11, 3127.38it/s]23635it [00:08, 3337.26it/s]25603it [00:08, 3468.07it/s]25876it [00:09, 3509.42it/s]31933it [00:11, 3286.39it/s]23971it [00:08, 3277.84it/s]25979it [00:09, 3551.22it/s]26247it [00:09, 3566.68it/s]32283it [00:11, 3279.85it/s]24347it [00:08, 3414.15it/s]26336it [00:09, 3436.68it/s]26605it [00:09, 3426.43it/s]32652it [00:12, 3393.51it/s]24691it [00:08, 3329.27it/s]26702it [00:09, 3499.89it/s]26975it [00:09, 3503.81it/s]33003it [00:12, 3322.51it/s]25060it [00:09, 3432.49it/s]27083it [00:09, 3587.66it/s]27328it [00:09, 3446.77it/s]33374it [00:12, 3430.98it/s]25422it [00:09, 3339.40it/s]27443it [00:09, 3499.79it/s]27706it [00:09, 3540.52it/s]33739it [00:12, 3492.12it/s]25797it [00:09, 3454.13it/s]27821it [00:09, 3578.76it/s]34093it [00:12, 3417.28it/s]26163it [00:09, 3512.42it/s]34468it [00:12, 3511.84it/s]26516it [00:09, 3389.87it/s]34822it [00:12, 3401.85it/s]26869it [00:09, 3427.44it/s]35197it [00:12, 3501.48it/s]27214it [00:09, 3348.20it/s]35550it [00:12, 3393.38it/s]27592it [00:09, 3462.37it/s]35924it [00:12, 3492.42it/s]27942it [00:09, 3369.37it/s]36294it [00:13, 3550.68it/s]36651it [00:13, 3415.69it/s]37022it [00:13, 3500.01it/s]37374it [00:13, 3390.10it/s]28062it [00:10, 872.05it/s] 37740it [00:13, 3467.15it/s]28432it [00:10, 1135.26it/s]28180it [00:10, 862.85it/s] 38089it [00:13, 3379.73it/s]28795it [00:10, 1428.09it/s]28551it [00:10, 1123.23it/s]38453it [00:13, 3452.17it/s]29111it [00:11, 1676.74it/s]28855it [00:10, 1344.64it/s]38825it [00:13, 3528.43it/s]29481it [00:11, 2020.50it/s]29232it [00:11, 1688.15it/s]39179it [00:13, 3406.60it/s]29603it [00:11, 2028.16it/s]29812it [00:11, 2253.15it/s]39557it [00:14, 3512.20it/s]30176it [00:11, 2553.08it/s]29939it [00:11, 2266.07it/s]39910it [00:14, 3419.36it/s]28281it [00:11, 841.37it/s] 30299it [00:11, 2553.16it/s]30533it [00:11, 2715.85it/s]40272it [00:14, 3475.51it/s]28640it [00:11, 1096.23it/s]30639it [00:11, 2712.05it/s]30905it [00:11, 2963.44it/s]40621it [00:14, 3374.45it/s]28944it [00:11, 1323.32it/s]31011it [00:11, 2962.03it/s]31280it [00:11, 3167.46it/s]40994it [00:14, 3474.66it/s]29319it [00:11, 1670.74it/s]31372it [00:11, 3031.94it/s]31633it [00:11, 3154.59it/s]41371it [00:14, 3558.75it/s]29684it [00:11, 2006.42it/s]31743it [00:11, 3210.08it/s]32007it [00:11, 3313.14it/s]41729it [00:14, 3441.74it/s]30013it [00:11, 2220.57it/s]32126it [00:11, 3378.20it/s]32358it [00:12, 3298.61it/s]42104it [00:14, 3529.13it/s]30386it [00:11, 2544.48it/s]32484it [00:12, 3336.81it/s]32727it [00:12, 3407.16it/s]42459it [00:14, 3445.70it/s]30723it [00:11, 2698.38it/s]32851it [00:12, 3429.25it/s]33078it [00:12, 3310.64it/s]42835it [00:14, 3535.34it/s]31090it [00:11, 2939.94it/s]33205it [00:12, 3344.02it/s]33452it [00:12, 3431.23it/s]31432it [00:12, 2972.37it/s]33579it [00:12, 3453.77it/s]33828it [00:12, 3523.12it/s]31804it [00:12, 3170.20it/s]33931it [00:12, 3395.65it/s]34185it [00:12, 3454.65it/s]32178it [00:12, 3326.18it/s]34312it [00:12, 3512.18it/s]34552it [00:12, 3514.21it/s]32531it [00:12, 3271.17it/s]34676it [00:12, 3547.31it/s]34906it [00:12, 3408.41it/s]32883it [00:12, 3339.00it/s]35034it [00:12, 3414.33it/s]35281it [00:12, 3504.58it/s]33227it [00:12, 3262.40it/s]35412it [00:12, 3518.12it/s]35634it [00:12, 3415.02it/s]33594it [00:12, 3377.04it/s]35996it [00:13, 3471.37it/s]35767it [00:12, 3423.21it/s]33938it [00:12, 3313.51it/s]36370it [00:13, 3547.68it/s]36132it [00:13, 3483.04it/s]34302it [00:12, 3406.82it/s]36482it [00:13, 3379.97it/s]36726it [00:13, 3419.87it/s]34666it [00:12, 3472.87it/s]36856it [00:13, 3482.06it/s]37102it [00:13, 3517.13it/s]35016it [00:13, 3353.06it/s]37231it [00:13, 3557.88it/s]37456it [00:13, 3370.95it/s]35386it [00:13, 3450.80it/s]37589it [00:13, 3402.19it/s]37826it [00:13, 3463.27it/s]35734it [00:13, 3258.43it/s]37958it [00:13, 3482.36it/s]43190it [00:16, 723.55it/s] 38175it [00:13, 3404.47it/s]36106it [00:13, 3387.47it/s]43557it [00:16, 954.91it/s]38536it [00:13, 3461.10it/s]38309it [00:13, 3416.94it/s]36448it [00:13, 3299.78it/s]43938it [00:16, 1242.97it/s]38914it [00:13, 3551.34it/s]38663it [00:13, 3450.06it/s]36811it [00:13, 3391.90it/s]44255it [00:16, 1480.88it/s]39010it [00:13, 3330.00it/s]39271it [00:14, 3382.08it/s]37166it [00:13, 3436.60it/s]44631it [00:16, 1826.86it/s]39387it [00:13, 3455.77it/s]39635it [00:14, 3454.46it/s]37512it [00:13, 3341.02it/s]44964it [00:16, 2078.22it/s]39761it [00:14, 3537.96it/s]39983it [00:14, 3383.96it/s]37882it [00:13, 3435.05it/s]45344it [00:16, 2424.91it/s]40117it [00:14, 3439.59it/s]40361it [00:14, 3496.62it/s]38227it [00:14, 3387.41it/s]40463it [00:14, 3414.72it/s]45688it [00:17, 2523.81it/s]40713it [00:14, 3254.90it/s]38567it [00:14, 3307.81it/s]46048it [00:17, 2773.41it/s]40806it [00:14, 3313.50it/s]41074it [00:14, 3352.45it/s]38932it [00:14, 3278.50it/s]46418it [00:17, 3003.18it/s]41179it [00:14, 3432.35it/s]41452it [00:14, 3322.13it/s]39314it [00:14, 3430.30it/s]46764it [00:17, 3047.38it/s]41524it [00:14, 3361.83it/s]41826it [00:14, 3438.12it/s]39685it [00:14, 3510.43it/s]47138it [00:17, 3232.08it/s]41898it [00:14, 3469.42it/s]42202it [00:14, 3529.59it/s]40038it [00:14, 3407.68it/s]42267it [00:14, 3532.53it/s]47486it [00:17, 3221.18it/s]42558it [00:14, 3440.00it/s]40394it [00:14, 3450.97it/s]47857it [00:17, 3357.00it/s]42622it [00:14, 3419.21it/s]42912it [00:15, 3466.21it/s]40741it [00:14, 3347.81it/s]48206it [00:17, 3299.57it/s]42990it [00:15, 3494.26it/s]41108it [00:14, 3439.08it/s]48579it [00:17, 3420.79it/s]41454it [00:14, 3369.74it/s]48956it [00:18, 3518.89it/s]41826it [00:15, 3470.01it/s]49314it [00:18, 3398.18it/s]42188it [00:15, 3512.40it/s]49692it [00:18, 3507.03it/s]42541it [00:15, 3426.71it/s]50047it [00:18, 3414.56it/s]42912it [00:15, 3508.40it/s]50413it [00:18, 3484.11it/s]50764it [00:18, 3369.21it/s]51142it [00:18, 3486.49it/s]51513it [00:18, 3548.85it/s]51870it [00:18, 3421.45it/s]52240it [00:18, 3500.77it/s]52592it [00:19, 3373.34it/s]52966it [00:19, 3476.10it/s]53316it [00:19, 3376.35it/s]53682it [00:19, 3457.25it/s]43261it [00:16, 601.43it/s] 54051it [00:19, 3384.48it/s]43341it [00:16, 620.08it/s] 43626it [00:16, 805.90it/s]43708it [00:16, 830.02it/s]54426it [00:19, 3488.33it/s]44001it [00:17, 1063.18it/s]54791it [00:19, 3534.34it/s]44042it [00:16, 1045.33it/s]44316it [00:17, 1294.56it/s]44422it [00:17, 1354.97it/s]55146it [00:19, 3410.06it/s]44673it [00:17, 1605.89it/s]44798it [00:17, 1687.34it/s]55510it [00:19, 3474.11it/s]44999it [00:17, 1870.68it/s]45133it [00:17, 1947.08it/s]55859it [00:20, 3351.41it/s]45372it [00:17, 2218.50it/s]43264it [00:17, 611.38it/s] 45505it [00:17, 2282.84it/s]56231it [00:20, 3456.10it/s]45722it [00:17, 2420.24it/s]43628it [00:17, 817.27it/s]45848it [00:17, 2456.98it/s]56579it [00:20, 3371.01it/s]46089it [00:17, 2703.30it/s]44007it [00:17, 1080.23it/s]46214it [00:17, 2730.66it/s]56951it [00:20, 3469.91it/s]46444it [00:17, 2910.45it/s]44323it [00:17, 1313.59it/s]57329it [00:20, 3558.25it/s]46562it [00:17, 2841.74it/s]46789it [00:17, 2973.44it/s]44683it [00:17, 1629.46it/s]46935it [00:17, 3068.24it/s]57687it [00:20, 3446.31it/s]47164it [00:17, 3179.24it/s]45011it [00:17, 1895.26it/s]47316it [00:17, 3266.11it/s]58056it [00:20, 3515.42it/s]45379it [00:17, 2233.89it/s]47512it [00:18, 3145.57it/s]47672it [00:17, 3241.98it/s]58409it [00:20, 3398.25it/s]47883it [00:18, 3300.54it/s]45722it [00:17, 2446.88it/s]48029it [00:18, 3331.28it/s]58780it [00:20, 3485.03it/s]48241it [00:18, 3379.23it/s]46073it [00:17, 2690.55it/s]48378it [00:18, 3292.20it/s]59130it [00:21, 3352.94it/s]46441it [00:17, 2933.84it/s]48591it [00:18, 3325.58it/s]48756it [00:18, 3427.69it/s]59496it [00:21, 3440.11it/s]48968it [00:18, 3452.20it/s]46788it [00:18, 2997.80it/s]49107it [00:18, 3368.09it/s]59863it [00:21, 3506.27it/s]47165it [00:18, 3201.80it/s]49320it [00:18, 3374.02it/s]49485it [00:18, 3483.52it/s]60216it [00:21, 3400.52it/s]49697it [00:18, 3485.59it/s]47514it [00:18, 3165.53it/s]49849it [00:18, 3526.15it/s]60588it [00:21, 3491.81it/s]47887it [00:18, 3320.85it/s]50050it [00:18, 3358.28it/s]50205it [00:18, 3411.93it/s]60939it [00:21, 3392.95it/s]50418it [00:18, 3448.74it/s]48242it [00:18, 3282.57it/s]50577it [00:18, 3500.06it/s]61316it [00:21, 3498.68it/s]48597it [00:18, 3355.63it/s]50766it [00:18, 3351.90it/s]50930it [00:18, 3395.28it/s]61668it [00:21, 3380.22it/s]48973it [00:18, 3470.51it/s]51146it [00:19, 3478.17it/s]51306it [00:19, 3498.08it/s]51498it [00:19, 3488.15it/s]49327it [00:18, 3373.95it/s]51658it [00:19, 3354.31it/s]49700it [00:18, 3475.16it/s]51849it [00:19, 3381.36it/s]52028it [00:19, 3451.72it/s]52221it [00:19, 3478.25it/s]50052it [00:19, 3382.57it/s]52403it [00:19, 3536.98it/s]50418it [00:19, 3459.22it/s]52571it [00:19, 3385.64it/s]52759it [00:19, 3425.03it/s]52943it [00:19, 3479.48it/s]50767it [00:19, 3313.77it/s]53134it [00:19, 3516.77it/s]51145it [00:19, 3445.29it/s]53293it [00:19, 3321.78it/s]53488it [00:19, 3345.66it/s]51513it [00:19, 3511.48it/s]53660it [00:19, 3419.84it/s]53855it [00:19, 3435.44it/s]54036it [00:19, 3516.36it/s]51867it [00:19, 3390.06it/s]54201it [00:19, 3354.01it/s]52232it [00:19, 3464.42it/s]54390it [00:20, 3413.72it/s]54574it [00:19, 3460.66it/s]54753it [00:20, 3474.57it/s]52581it [00:19, 3357.14it/s]54943it [00:20, 3525.66it/s]52946it [00:19, 3440.49it/s]55102it [00:20, 3354.36it/s]55298it [00:20, 3341.62it/s]55462it [00:20, 3424.60it/s]53292it [00:20, 3331.72it/s]55662it [00:20, 3421.33it/s]53656it [00:20, 3417.87it/s]55806it [00:20, 3312.04it/s]56007it [00:20, 3320.24it/s]54027it [00:20, 3502.55it/s]56180it [00:20, 3432.39it/s]56380it [00:20, 3435.87it/s]56549it [00:20, 3505.61it/s]54379it [00:20, 3400.93it/s]56726it [00:20, 3333.05it/s]54743it [00:20, 3468.28it/s]56901it [00:20, 3385.91it/s]57084it [00:20, 3402.94it/s]62008it [00:23, 576.16it/s] 57275it [00:20, 3486.78it/s]55092it [00:20, 3327.02it/s]57457it [00:20, 3497.44it/s]62380it [00:23, 780.47it/s]55452it [00:20, 3404.29it/s]57626it [00:20, 3376.77it/s]57809it [00:20, 3388.95it/s]62691it [00:23, 978.46it/s]57985it [00:21, 3437.36it/s]55803it [00:20, 3301.87it/s]58178it [00:21, 3475.44it/s]63061it [00:23, 1272.89it/s]58331it [00:21, 3334.70it/s]56179it [00:20, 3430.20it/s]63375it [00:23, 1522.22it/s]58527it [00:21, 3368.97it/s]58704it [00:21, 3447.14it/s]56549it [00:20, 3506.94it/s]63729it [00:24, 1845.48it/s]58881it [00:21, 3416.24it/s]59065it [00:21, 3492.04it/s]56902it [00:21, 3394.43it/s]64096it [00:24, 2185.16it/s]59224it [00:21, 3317.40it/s]57273it [00:21, 3477.56it/s]59416it [00:21, 3350.02it/s]64434it [00:24, 2390.31it/s]59583it [00:21, 3393.66it/s]59753it [00:21, 3218.60it/s]57623it [00:21, 3227.99it/s]64764it [00:24, 2566.87it/s]59924it [00:21, 3198.96it/s]57950it [00:21, 3012.40it/s]60077it [00:21, 2874.55it/s]65089it [00:24, 2487.53it/s]60247it [00:21, 2772.45it/s]58257it [00:21, 2999.25it/s]60375it [00:21, 2901.52it/s]65386it [00:24, 2550.97it/s]60559it [00:21, 2860.06it/s]60671it [00:21, 2854.39it/s]58561it [00:21, 2832.90it/s]65680it [00:24, 2647.10it/s]60854it [00:21, 2665.98it/s]58876it [00:21, 2916.96it/s]60960it [00:22, 2756.21it/s]65971it [00:24, 2520.19it/s]61166it [00:22, 2783.86it/s]61278it [00:22, 2870.90it/s]59171it [00:21, 2910.81it/s]66329it [00:24, 2793.35it/s]61521it [00:22, 2991.80it/s]61628it [00:22, 3047.85it/s]59526it [00:21, 3090.48it/s]66691it [00:25, 3016.06it/s]59886it [00:22, 3236.58it/s]67008it [00:25, 3010.83it/s]60213it [00:22, 3177.82it/s]67373it [00:25, 3190.11it/s]60575it [00:22, 3305.66it/s]67701it [00:25, 3153.31it/s]60908it [00:22, 3195.65it/s]68061it [00:25, 3279.80it/s]61271it [00:22, 3320.03it/s]68400it [00:25, 3220.03it/s]61631it [00:22, 3400.63it/s]68765it [00:25, 3343.11it/s]69127it [00:25, 3421.06it/s]69472it [00:25, 3306.94it/s]69836it [00:25, 3400.57it/s]70179it [00:26, 3280.89it/s]70543it [00:26, 3382.94it/s]70906it [00:26, 3453.58it/s]71253it [00:26, 3327.71it/s]71618it [00:26, 3418.85it/s]71962it [00:26, 3317.44it/s]72322it [00:26, 3397.64it/s]72664it [00:26, 3322.12it/s]73041it [00:26, 3449.89it/s]61936it [00:24, 470.77it/s] 73410it [00:27, 3519.68it/s]62310it [00:24, 665.11it/s]73764it [00:27, 3385.07it/s]62623it [00:24, 858.03it/s]74139it [00:27, 3488.63it/s]61828it [00:24, 420.48it/s] 62993it [00:24, 1143.78it/s]74490it [00:27, 3377.51it/s]62192it [00:24, 590.59it/s]63363it [00:24, 1464.53it/s]74857it [00:27, 3459.42it/s]62567it [00:24, 812.39it/s]63689it [00:24, 1728.25it/s]75205it [00:27, 3351.53it/s]62870it [00:24, 1014.02it/s]61973it [00:24, 520.67it/s] 64061it [00:24, 2081.30it/s]75569it [00:27, 3433.42it/s]63244it [00:24, 1327.67it/s]62332it [00:24, 705.53it/s]64398it [00:25, 2293.44it/s]75934it [00:27, 3495.19it/s]63565it [00:24, 1582.61it/s]62648it [00:24, 899.89it/s]64768it [00:25, 2603.44it/s]63935it [00:25, 1935.31it/s]76285it [00:27, 3362.77it/s]63015it [00:24, 1181.26it/s]65112it [00:25, 2744.36it/s]76647it [00:27, 3435.08it/s]64270it [00:25, 2171.55it/s]63387it [00:24, 1502.95it/s]65476it [00:25, 2968.16it/s]64639it [00:25, 2494.09it/s]76993it [00:28, 3281.97it/s]63718it [00:25, 1760.65it/s]65847it [00:25, 3163.42it/s]64994it [00:25, 2739.10it/s]77356it [00:28, 3379.63it/s]64083it [00:25, 2096.66it/s]66198it [00:25, 3152.38it/s]65337it [00:25, 2830.09it/s]77697it [00:28, 3288.07it/s]64420it [00:25, 2317.44it/s]66568it [00:25, 3301.52it/s]65699it [00:25, 3032.14it/s]78060it [00:28, 3385.59it/s]64788it [00:25, 2619.50it/s]66917it [00:25, 3257.19it/s]78424it [00:28, 3457.37it/s]66040it [00:25, 3056.47it/s]65129it [00:25, 2728.35it/s]67270it [00:25, 3333.18it/s]66404it [00:25, 3214.32it/s]78772it [00:28, 3351.44it/s]65494it [00:25, 2958.81it/s]67632it [00:25, 3275.79it/s]66772it [00:25, 3342.65it/s]79131it [00:28, 3419.08it/s]65860it [00:25, 3141.26it/s]68002it [00:26, 3394.98it/s]67122it [00:26, 3271.81it/s]79475it [00:28, 3318.27it/s]66208it [00:25, 3139.49it/s]68369it [00:26, 3472.67it/s]67489it [00:26, 3382.50it/s]79842it [00:28, 3417.29it/s]66575it [00:25, 3284.18it/s]68721it [00:26, 3372.95it/s]80186it [00:29, 3313.79it/s]67836it [00:26, 3147.29it/s]66922it [00:25, 3237.66it/s]69087it [00:26, 3454.51it/s]80543it [00:29, 3387.17it/s]68199it [00:26, 3279.34it/s]67290it [00:26, 3360.06it/s]69436it [00:26, 3356.41it/s]80909it [00:29, 3464.96it/s]68535it [00:26, 3226.58it/s]67636it [00:26, 3265.95it/s]69802it [00:26, 3440.69it/s]81257it [00:29, 3343.07it/s]68900it [00:26, 3345.23it/s]68006it [00:26, 3388.41it/s]70152it [00:26, 3363.63it/s]81620it [00:29, 3424.31it/s]69267it [00:26, 3436.58it/s]68373it [00:26, 3468.64it/s]70510it [00:26, 3425.31it/s]81964it [00:29, 3327.42it/s]69615it [00:26, 3331.77it/s]68725it [00:26, 3371.65it/s]70877it [00:26, 3495.24it/s]82327it [00:29, 3413.78it/s]69984it [00:26, 3434.30it/s]69090it [00:26, 3450.48it/s]71228it [00:27, 3379.86it/s]82681it [00:29, 3320.46it/s]70330it [00:26, 3338.77it/s]69438it [00:26, 3349.87it/s]71595it [00:27, 3463.08it/s]83045it [00:29, 3410.59it/s]70699it [00:27, 3437.54it/s]69802it [00:26, 3430.72it/s]71943it [00:27, 3367.64it/s]83411it [00:29, 3482.46it/s]71045it [00:27, 3317.36it/s]70151it [00:26, 3357.18it/s]72307it [00:27, 3445.00it/s]83761it [00:30, 3330.39it/s]71411it [00:27, 3405.68it/s]70504it [00:27, 3405.91it/s]72672it [00:27, 3352.66it/s]84123it [00:30, 3413.02it/s]71779it [00:27, 3484.34it/s]70870it [00:27, 3477.28it/s]73049it [00:27, 3470.05it/s]84467it [00:30, 3321.72it/s]72129it [00:27, 3336.93it/s]71219it [00:27, 3359.49it/s]73404it [00:27, 3492.29it/s]84829it [00:30, 3406.22it/s]72493it [00:27, 3423.37it/s]71585it [00:27, 3443.56it/s]73755it [00:27, 3403.05it/s]85194it [00:30, 3475.59it/s]72838it [00:27, 3334.12it/s]71931it [00:27, 3357.18it/s]74133it [00:27, 3511.19it/s]73213it [00:27, 3451.40it/s]72296it [00:27, 3440.58it/s]74486it [00:27, 3401.94it/s]72666it [00:27, 3514.16it/s]73560it [00:27, 3353.77it/s]74853it [00:28, 3477.14it/s]73936it [00:28, 3469.41it/s]73019it [00:27, 3420.67it/s]75203it [00:28, 3362.36it/s]74305it [00:28, 3531.94it/s]73367it [00:27, 3436.21it/s]75568it [00:28, 3443.79it/s]74660it [00:28, 3404.87it/s]73712it [00:27, 3365.11it/s]75920it [00:28, 3465.32it/s]75016it [00:28, 3447.16it/s]74087it [00:28, 3474.20it/s]76268it [00:28, 3344.88it/s]75363it [00:28, 3326.53it/s]74436it [00:28, 3357.79it/s]76634it [00:28, 3433.40it/s]75732it [00:28, 3428.20it/s]74804it [00:28, 3449.88it/s]76979it [00:28, 3326.46it/s]75168it [00:28, 3503.54it/s]76077it [00:28, 3311.74it/s]77349it [00:28, 3431.45it/s]76442it [00:28, 3407.51it/s]75520it [00:28, 3380.19it/s]77712it [00:28, 3324.11it/s]76805it [00:28, 3470.22it/s]75888it [00:28, 3465.21it/s]78079it [00:29, 3421.38it/s]77154it [00:28, 3336.32it/s]76236it [00:28, 3291.27it/s]78447it [00:29, 3495.79it/s]77521it [00:29, 3399.94it/s]76599it [00:28, 3385.92it/s]78799it [00:29, 3340.48it/s]77863it [00:29, 3304.82it/s]76940it [00:28, 3299.85it/s]79166it [00:29, 3433.22it/s]78231it [00:29, 3411.91it/s]77304it [00:29, 3396.96it/s]79512it [00:29, 3334.36it/s]77671it [00:29, 3473.88it/s]78574it [00:29, 3314.10it/s]79881it [00:29, 3435.06it/s]78939it [00:29, 3408.56it/s]78020it [00:29, 3360.10it/s]80232it [00:29, 3336.18it/s]79306it [00:29, 3482.13it/s]78390it [00:29, 3455.26it/s]80607it [00:29, 3451.93it/s]79656it [00:29, 3359.21it/s]78738it [00:29, 3352.82it/s]80975it [00:29, 3517.12it/s]80025it [00:29, 3452.19it/s]79098it [00:29, 3388.98it/s]81329it [00:29, 3390.66it/s]80372it [00:29, 3345.01it/s]79439it [00:29, 3300.94it/s]81693it [00:30, 3461.04it/s]80730it [00:30, 3409.85it/s]79808it [00:29, 3410.82it/s]82041it [00:30, 3325.20it/s]80169it [00:29, 3466.99it/s]81073it [00:30, 3308.59it/s]82405it [00:30, 3412.42it/s]85543it [00:32, 434.63it/s] 81437it [00:30, 3401.68it/s]80517it [00:29, 3379.56it/s]85902it [00:33, 591.42it/s]82752it [00:30, 3335.32it/s]81800it [00:30, 3467.66it/s]80884it [00:30, 3463.49it/s]86221it [00:33, 763.60it/s]83120it [00:30, 3430.30it/s]82148it [00:30, 3362.17it/s]81232it [00:30, 3350.34it/s]86579it [00:33, 1007.01it/s]83488it [00:30, 3501.08it/s]82512it [00:30, 3440.73it/s]81596it [00:30, 3431.70it/s]86942it [00:33, 1295.13it/s]83840it [00:30, 3393.05it/s]82858it [00:30, 3334.38it/s]81941it [00:30, 3336.75it/s]87269it [00:33, 1549.31it/s]84206it [00:30, 3468.79it/s]83220it [00:30, 3413.87it/s]82288it [00:30, 3373.71it/s]87637it [00:33, 1892.18it/s]84555it [00:30, 3361.51it/s]83569it [00:30, 3434.00it/s]82654it [00:30, 3455.02it/s]87972it [00:33, 2138.57it/s]84922it [00:31, 3442.62it/s]83914it [00:30, 3331.10it/s]83001it [00:30, 3353.08it/s]88340it [00:33, 2459.80it/s]85272it [00:31, 3301.50it/s]84280it [00:31, 3425.91it/s]83366it [00:30, 3438.66it/s]88681it [00:33, 2635.92it/s]83712it [00:30, 3338.57it/s]84624it [00:31, 3317.72it/s]89046it [00:34, 2883.07it/s]84079it [00:31, 3431.59it/s]84991it [00:31, 3412.00it/s]89408it [00:34, 3072.52it/s]85334it [00:31, 3317.81it/s]84432it [00:31, 3340.77it/s]89757it [00:34, 3073.77it/s]84799it [00:31, 3434.09it/s]90121it [00:34, 3225.65it/s]85165it [00:31, 3498.27it/s]90466it [00:34, 3160.92it/s]90820it [00:34, 3264.69it/s]91159it [00:34, 3178.67it/s]91513it [00:34, 3279.71it/s]91866it [00:34, 3349.06it/s]92206it [00:34, 3230.97it/s]92560it [00:35, 3318.68it/s]92896it [00:35, 3196.53it/s]93250it [00:35, 3291.14it/s]93606it [00:35, 3367.46it/s]93945it [00:35, 3252.56it/s]94300it [00:35, 3334.54it/s]94636it [00:35, 3230.52it/s]94990it [00:35, 3316.35it/s]95345it [00:35, 3382.24it/s]95685it [00:36, 3247.99it/s]96041it [00:36, 3334.81it/s]96377it [00:36, 3190.62it/s]96732it [00:36, 3290.95it/s]85605it [00:33, 414.86it/s] 97064it [00:36, 3188.11it/s]85959it [00:33, 565.93it/s]97420it [00:36, 3291.59it/s]86261it [00:33, 724.46it/s]97775it [00:36, 3365.59it/s]86631it [00:34, 975.67it/s]98114it [00:36, 3251.18it/s]86996it [00:34, 1263.48it/s]85668it [00:34, 401.12it/s] 98468it [00:36, 3331.62it/s]87321it [00:34, 1520.92it/s]86035it [00:34, 555.41it/s]98803it [00:36, 3230.94it/s]87690it [00:34, 1866.39it/s]85516it [00:33, 415.57it/s] 86339it [00:34, 712.29it/s]99156it [00:37, 3315.53it/s]88026it [00:34, 2117.47it/s]85880it [00:34, 568.22it/s]86704it [00:34, 955.24it/s]99493it [00:37, 3329.71it/s]88397it [00:34, 2447.44it/s]86193it [00:34, 730.48it/s]87020it [00:34, 1181.74it/s]99828it [00:37, 3235.57it/s]88739it [00:34, 2603.64it/s]86562it [00:34, 975.99it/s]87389it [00:34, 1509.04it/s]100181it [00:37, 3311.37it/s]89113it [00:34, 2876.37it/s]86925it [00:34, 1257.08it/s]87742it [00:34, 1826.52it/s]100514it [00:37, 3214.49it/s]89472it [00:34, 3058.76it/s]87255it [00:34, 1513.81it/s]88076it [00:34, 2074.26it/s]100865it [00:37, 3298.91it/s]89821it [00:34, 3092.83it/s]87623it [00:34, 1854.01it/s]88447it [00:34, 2408.63it/s]101217it [00:37, 3361.11it/s]90181it [00:35, 3229.81it/s]87961it [00:34, 2091.87it/s]88787it [00:34, 2575.66it/s]101555it [00:37, 3233.93it/s]90527it [00:35, 3176.33it/s]88329it [00:34, 2415.84it/s]89148it [00:35, 2823.55it/s]101908it [00:37, 3316.91it/s]90883it [00:35, 3282.95it/s]89512it [00:35, 3031.31it/s]88701it [00:34, 2615.24it/s]102242it [00:38, 3208.43it/s]91224it [00:35, 3177.42it/s]89064it [00:35, 2855.22it/s]89859it [00:35, 3050.37it/s]102596it [00:38, 3302.03it/s]91580it [00:35, 3283.63it/s]89429it [00:35, 3055.38it/s]90222it [00:35, 3205.16it/s]102928it [00:38, 3164.66it/s]91934it [00:35, 3355.95it/s]89779it [00:35, 3083.61it/s]90566it [00:35, 3141.42it/s]103282it [00:38, 3269.34it/s]92275it [00:35, 3255.91it/s]90142it [00:35, 3230.36it/s]90905it [00:35, 3208.08it/s]103632it [00:38, 3335.23it/s]92625it [00:35, 3323.50it/s]90489it [00:35, 3164.73it/s]91238it [00:35, 3123.49it/s]103968it [00:38, 3219.03it/s]92961it [00:35, 3239.00it/s]90832it [00:35, 3237.51it/s]91592it [00:35, 3238.71it/s]104321it [00:38, 3307.42it/s]93319it [00:35, 3336.69it/s]91186it [00:35, 3322.94it/s]91945it [00:35, 3320.53it/s]104654it [00:38, 3212.49it/s]93673it [00:36, 3394.91it/s]91528it [00:35, 3230.08it/s]92283it [00:36, 3215.02it/s]105005it [00:38, 3296.18it/s]94015it [00:36, 3259.70it/s]91881it [00:35, 3312.93it/s]92635it [00:36, 3301.49it/s]105355it [00:38, 3355.07it/s]94371it [00:36, 3344.28it/s]92218it [00:35, 3219.19it/s]92969it [00:36, 3203.80it/s]105692it [00:39, 3241.60it/s]94708it [00:36, 3246.67it/s]92572it [00:36, 3308.11it/s]93323it [00:36, 3298.08it/s]106043it [00:39, 3317.36it/s]95062it [00:36, 3329.91it/s]93677it [00:36, 3365.77it/s]92907it [00:36, 3215.34it/s]106377it [00:39, 3209.10it/s]95415it [00:36, 3385.49it/s]93260it [00:36, 3304.88it/s]94016it [00:36, 3206.64it/s]106717it [00:39, 3262.88it/s]95755it [00:36, 3265.91it/s]93605it [00:36, 3346.90it/s]94371it [00:36, 3297.67it/s]107069it [00:39, 3336.53it/s]96110it [00:36, 3347.06it/s]93942it [00:36, 3251.09it/s]94704it [00:36, 3204.53it/s]107404it [00:39, 3231.21it/s]96447it [00:36, 3249.15it/s]94299it [00:36, 3340.50it/s]95058it [00:36, 3299.40it/s]107760it [00:39, 3324.80it/s]96791it [00:37, 3301.42it/s]94635it [00:36, 3233.62it/s]95408it [00:36, 3355.74it/s]108094it [00:39, 3219.27it/s]97123it [00:37, 3200.32it/s]94986it [00:36, 3312.04it/s]95746it [00:37, 3229.14it/s]108447it [00:39, 3307.84it/s]97480it [00:37, 3304.19it/s]95340it [00:36, 3377.55it/s]96100it [00:37, 3317.72it/s]108791it [00:40, 3222.13it/s]97836it [00:37, 3376.42it/s]95679it [00:37, 3259.14it/s]96434it [00:37, 3209.29it/s]109145it [00:40, 3312.84it/s]98175it [00:37, 3261.21it/s]96017it [00:37, 3293.86it/s]96788it [00:37, 3301.63it/s]109496it [00:40, 3369.51it/s]98526it [00:37, 3332.56it/s]96348it [00:37, 3196.53it/s]97120it [00:37, 3096.76it/s]109835it [00:40, 3236.57it/s]98861it [00:37, 3237.42it/s]96702it [00:37, 3293.44it/s]97474it [00:37, 3219.99it/s]110188it [00:40, 3318.77it/s]99217it [00:37, 3329.05it/s]97056it [00:37, 3362.67it/s]97826it [00:37, 3304.40it/s]110522it [00:40, 3220.44it/s]99562it [00:37, 3362.80it/s]97394it [00:37, 3256.26it/s]98160it [00:37, 3202.53it/s]110877it [00:40, 3313.73it/s]99900it [00:37, 3264.12it/s]97747it [00:37, 3334.96it/s]98512it [00:37, 3290.94it/s]111232it [00:40, 3380.50it/s]100252it [00:38, 3328.34it/s]98082it [00:37, 3230.62it/s]98844it [00:38, 3200.99it/s]111572it [00:40, 3265.98it/s]100586it [00:38, 3241.51it/s]98435it [00:37, 3316.45it/s]99198it [00:38, 3297.12it/s]111925it [00:40, 3340.74it/s]100941it [00:38, 3328.16it/s]98779it [00:37, 3351.72it/s]99551it [00:38, 3357.29it/s]112261it [00:41, 3239.31it/s]101297it [00:38, 3393.14it/s]99116it [00:38, 3245.30it/s]99889it [00:38, 3215.12it/s]112615it [00:41, 3323.82it/s]101638it [00:38, 3268.31it/s]99472it [00:38, 3333.83it/s]100240it [00:38, 3296.64it/s]112956it [00:41, 3347.28it/s]101977it [00:38, 3303.10it/s]99807it [00:38, 3231.42it/s]100572it [00:38, 3200.78it/s]113292it [00:41, 3246.14it/s]102309it [00:38, 3213.89it/s]100160it [00:38, 3315.52it/s]100924it [00:38, 3289.83it/s]113646it [00:41, 3330.12it/s]102662it [00:38, 3304.42it/s]100493it [00:38, 3215.91it/s]101276it [00:38, 3354.79it/s]113981it [00:41, 3099.67it/s]102994it [00:38, 3212.27it/s]100847it [00:38, 3306.89it/s]101613it [00:38, 3225.22it/s]114338it [00:41, 3229.01it/s]103348it [00:39, 3304.87it/s]101201it [00:38, 3372.43it/s]101964it [00:38, 3304.90it/s]103701it [00:39, 3368.23it/s]114671it [00:41, 3164.59it/s]101540it [00:38, 3229.59it/s]102297it [00:39, 3205.11it/s]104039it [00:39, 3260.51it/s]101891it [00:38, 3309.14it/s]102636it [00:39, 3257.58it/s]104388it [00:39, 3326.59it/s]102224it [00:39, 3198.72it/s]102981it [00:39, 3165.08it/s]104722it [00:39, 3205.75it/s]102576it [00:39, 3289.42it/s]103336it [00:39, 3274.08it/s]105077it [00:39, 3301.71it/s]102931it [00:39, 3362.26it/s]103689it [00:39, 3347.70it/s]105431it [00:39, 3370.32it/s]103269it [00:39, 3248.81it/s]104026it [00:39, 3236.77it/s]105770it [00:39, 3254.17it/s]103618it [00:39, 3317.50it/s]104377it [00:39, 3313.33it/s]106124it [00:39, 3335.75it/s]103952it [00:39, 3195.24it/s]104710it [00:39, 3207.88it/s]106460it [00:39, 3239.23it/s]104306it [00:39, 3292.00it/s]105064it [00:39, 3301.97it/s]106818it [00:40, 3334.98it/s]104660it [00:39, 3362.19it/s]105406it [00:40, 3336.07it/s]107171it [00:40, 3389.49it/s]104998it [00:39, 3237.71it/s]105741it [00:40, 3215.80it/s]107512it [00:40, 3247.96it/s]105350it [00:39, 3318.19it/s]106094it [00:40, 3304.69it/s]107870it [00:40, 3341.45it/s]105684it [00:40, 3219.41it/s]106426it [00:40, 3201.73it/s]108206it [00:40, 3244.23it/s]106036it [00:40, 3303.96it/s]106782it [00:40, 3302.46it/s]108563it [00:40, 3334.90it/s]106368it [00:40, 3199.37it/s]107134it [00:40, 3363.68it/s]108899it [00:40, 3234.69it/s]106711it [00:40, 3265.29it/s]107472it [00:40, 3243.17it/s]109256it [00:40, 3328.41it/s]107065it [00:40, 3342.65it/s]107818it [00:40, 3303.42it/s]109613it [00:40, 3395.91it/s]107401it [00:40, 3233.22it/s]108150it [00:40, 3213.27it/s]109954it [00:41, 3252.91it/s]107756it [00:40, 3323.44it/s]108504it [00:40, 3306.73it/s]110307it [00:41, 3331.18it/s]108090it [00:40, 3224.28it/s]108861it [00:41, 3212.00it/s]110642it [00:41, 3239.56it/s]108442it [00:40, 3307.72it/s]109216it [00:41, 3307.33it/s]110996it [00:41, 3324.60it/s]108799it [00:40, 3382.89it/s]109572it [00:41, 3377.42it/s]111352it [00:41, 3391.65it/s]109139it [00:41, 3271.69it/s]109912it [00:41, 3249.47it/s]111693it [00:41, 3266.55it/s]109479it [00:41, 3308.47it/s]110266it [00:41, 3331.12it/s]112048it [00:41, 3345.96it/s]109811it [00:41, 3207.16it/s]110601it [00:41, 3200.83it/s]112385it [00:41, 3242.88it/s]110167it [00:41, 3306.47it/s]110959it [00:41, 3306.33it/s]112730it [00:41, 3299.97it/s]110521it [00:41, 3373.76it/s]111312it [00:41, 3369.91it/s]113062it [00:41, 3210.59it/s]110860it [00:41, 3248.42it/s]111651it [00:41, 3238.99it/s]113416it [00:42, 3304.04it/s]111212it [00:41, 3325.74it/s]112006it [00:42, 3326.03it/s]113772it [00:42, 3376.43it/s]111547it [00:41, 3222.90it/s]112341it [00:42, 3221.62it/s]114111it [00:42, 3283.62it/s]111901it [00:41, 3313.50it/s]112696it [00:42, 3314.32it/s]114991it [00:45, 322.11it/s] 114466it [00:42, 3360.10it/s]112234it [00:42, 3189.06it/s]113049it [00:42, 3375.72it/s]115331it [00:45, 443.54it/s]114804it [00:42, 3265.63it/s]112590it [00:42, 3293.70it/s]115626it [00:45, 577.28it/s]113388it [00:42, 3226.12it/s]112944it [00:42, 3363.46it/s]115982it [00:45, 788.50it/s]113742it [00:42, 3314.45it/s]113282it [00:42, 3256.41it/s]116339it [00:45, 1043.91it/s]114076it [00:42, 3214.41it/s]113638it [00:42, 3342.08it/s]116654it [00:45, 1280.94it/s]114435it [00:42, 3319.28it/s]113974it [00:42, 3240.23it/s]117012it [00:45, 1606.05it/s]114769it [00:42, 3215.28it/s]114331it [00:42, 3333.92it/s]117337it [00:45, 1854.04it/s]114677it [00:42, 3368.09it/s]117692it [00:45, 2177.37it/s]118049it [00:45, 2475.38it/s]118386it [00:46, 2609.29it/s]118729it [00:46, 2809.46it/s]119060it [00:46, 2853.45it/s]119414it [00:46, 3033.40it/s]119767it [00:46, 3168.80it/s]120104it [00:46, 3106.38it/s]120461it [00:46, 3233.88it/s]120796it [00:46, 3154.92it/s]121150it [00:46, 3262.84it/s]121483it [00:47, 3168.35it/s]121837it [00:47, 3272.68it/s]122175it [00:47, 3302.57it/s]122509it [00:47, 3211.19it/s]122861it [00:47, 3293.32it/s]123193it [00:47, 3189.66it/s]123549it [00:47, 3295.29it/s]123901it [00:47, 3358.15it/s]124239it [00:47, 3217.83it/s]124592it [00:47, 3306.10it/s]124925it [00:48, 3202.96it/s]125262it [00:48, 3249.35it/s]125613it [00:48, 3323.73it/s]125947it [00:48, 3221.13it/s]126303it [00:48, 3317.43it/s]126637it [00:48, 3214.59it/s]115132it [00:45, 306.81it/s] 126988it [00:48, 3299.27it/s]115471it [00:46, 421.54it/s]127341it [00:48, 3195.46it/s]115764it [00:46, 548.12it/s]127694it [00:48, 3288.36it/s]116120it [00:46, 750.59it/s]128047it [00:49, 3356.70it/s]116480it [00:46, 1000.20it/s]128385it [00:49, 3249.93it/s]116796it [00:46, 1233.14it/s]128741it [00:49, 3329.35it/s]117154it [00:46, 1552.53it/s]115015it [00:46, 307.89it/s] 129076it [00:49, 3200.50it/s]115093it [00:46, 287.86it/s] 117479it [00:46, 1805.54it/s]115366it [00:46, 425.90it/s]129428it [00:49, 3290.14it/s]115445it [00:46, 401.58it/s]117837it [00:46, 2137.07it/s]115658it [00:46, 550.64it/s]129780it [00:49, 3356.12it/s]115738it [00:46, 522.55it/s]118170it [00:46, 2335.48it/s]116017it [00:46, 753.20it/s]116096it [00:46, 718.11it/s]130118it [00:49, 3248.80it/s]118514it [00:46, 2586.17it/s]116373it [00:46, 997.22it/s]116452it [00:46, 955.94it/s]130470it [00:49, 3325.41it/s]118868it [00:47, 2818.74it/s]116692it [00:46, 1231.17it/s]116768it [00:47, 1183.33it/s]130804it [00:49, 3222.00it/s]119204it [00:47, 2870.37it/s]117034it [00:46, 1527.73it/s]117120it [00:47, 1490.66it/s]131158it [00:49, 3312.42it/s]119555it [00:47, 3038.55it/s]117356it [00:46, 1779.42it/s]117443it [00:47, 1741.86it/s]131511it [00:50, 3374.21it/s]119888it [00:47, 3030.77it/s]117711it [00:47, 2108.86it/s]117789it [00:47, 2054.51it/s]131850it [00:50, 3255.89it/s]120243it [00:47, 3171.47it/s]118068it [00:47, 2414.59it/s]118143it [00:47, 2360.42it/s]132202it [00:50, 3330.70it/s]120601it [00:47, 3284.30it/s]118404it [00:47, 2564.51it/s]118476it [00:47, 2514.61it/s]132537it [00:50, 3194.93it/s]120941it [00:47, 3201.04it/s]118759it [00:47, 2803.66it/s]118829it [00:47, 2757.49it/s]132890it [00:50, 3290.00it/s]121297it [00:47, 3300.90it/s]119094it [00:47, 2853.29it/s]119161it [00:47, 2809.29it/s]133221it [00:50, 3192.08it/s]121634it [00:47, 3172.05it/s]119435it [00:47, 2988.53it/s]119513it [00:47, 2994.21it/s]133576it [00:50, 3292.08it/s]121990it [00:48, 3279.71it/s]119786it [00:47, 3129.82it/s]119850it [00:48, 2984.43it/s]133931it [00:50, 3365.96it/s]122348it [00:48, 3363.54it/s]120121it [00:47, 3088.06it/s]120200it [00:48, 3121.73it/s]134269it [00:50, 3255.63it/s]122688it [00:48, 3254.15it/s]120477it [00:47, 3219.08it/s]120546it [00:48, 3214.62it/s]134621it [00:51, 3328.86it/s]123044it [00:48, 3339.21it/s]120811it [00:48, 3153.01it/s]120880it [00:48, 3135.64it/s]134956it [00:51, 3236.40it/s]123381it [00:48, 3242.50it/s]121165it [00:48, 3260.58it/s]121230it [00:48, 3236.97it/s]135313it [00:51, 3329.84it/s]123734it [00:48, 3323.71it/s]121519it [00:48, 3339.34it/s]121561it [00:48, 3151.67it/s]135658it [00:51, 3362.33it/s]124069it [00:48, 3230.47it/s]121858it [00:48, 3235.53it/s]121913it [00:48, 3255.80it/s]135996it [00:51, 3259.51it/s]124423it [00:48, 3317.62it/s]122199it [00:48, 3282.88it/s]122266it [00:48, 3334.68it/s]136350it [00:51, 3339.94it/s]124762it [00:48, 3337.33it/s]122531it [00:48, 3198.83it/s]122603it [00:48, 3225.75it/s]136686it [00:51, 3246.03it/s]125097it [00:48, 3242.43it/s]122887it [00:48, 3301.32it/s]122959it [00:48, 3319.68it/s]137043it [00:51, 3337.43it/s]125451it [00:49, 3321.50it/s]123220it [00:48, 3204.96it/s]137398it [00:51, 3397.15it/s]123294it [00:49, 3184.46it/s]125785it [00:49, 3235.18it/s]123572it [00:48, 3293.76it/s]123646it [00:49, 3278.31it/s]137739it [00:51, 3277.52it/s]126142it [00:49, 3329.65it/s]123926it [00:48, 3364.26it/s]123998it [00:49, 3346.28it/s]138096it [00:52, 3359.48it/s]126493it [00:49, 3381.62it/s]124264it [00:49, 3243.05it/s]124335it [00:49, 3219.14it/s]138434it [00:52, 3243.92it/s]126833it [00:49, 3260.35it/s]124618it [00:49, 3327.05it/s]124688it [00:49, 3305.92it/s]138790it [00:52, 3332.25it/s]127187it [00:49, 3339.31it/s]124953it [00:49, 3196.31it/s]125021it [00:49, 3205.27it/s]139125it [00:52, 3234.99it/s]127523it [00:49, 3187.97it/s]125306it [00:49, 3289.05it/s]125372it [00:49, 3291.09it/s]139469it [00:52, 3292.58it/s]127877it [00:49, 3287.11it/s]125660it [00:49, 3356.76it/s]125713it [00:49, 3323.18it/s]139825it [00:52, 3369.74it/s]128230it [00:49, 3355.68it/s]125998it [00:49, 3256.02it/s]126047it [00:49, 3218.87it/s]140164it [00:52, 3265.43it/s]128568it [00:50, 3253.44it/s]126351it [00:49, 3333.91it/s]126401it [00:50, 3308.82it/s]140519it [00:52, 3346.14it/s]128924it [00:50, 3340.83it/s]126686it [00:49, 3224.80it/s]126734it [00:50, 3201.79it/s]140855it [00:52, 3241.67it/s]129260it [00:50, 3242.33it/s]127037it [00:49, 3306.17it/s]127085it [00:50, 3289.01it/s]141211it [00:53, 3331.32it/s]129613it [00:50, 3324.13it/s]127393it [00:50, 3379.67it/s]141566it [00:53, 3394.49it/s]127416it [00:50, 3194.86it/s]129947it [00:50, 3226.47it/s]127733it [00:50, 3263.81it/s]127767it [00:50, 3285.25it/s]141907it [00:53, 3272.16it/s]130288it [00:50, 3277.19it/s]128073it [00:50, 3301.43it/s]128120it [00:50, 3354.77it/s]142263it [00:53, 3354.19it/s]130644it [00:50, 3356.64it/s]128405it [00:50, 3202.18it/s]128457it [00:50, 3204.41it/s]142600it [00:53, 3225.66it/s]130981it [00:50, 3245.95it/s]128761it [00:50, 3303.15it/s]128810it [00:50, 3295.39it/s]142953it [00:53, 3310.31it/s]131336it [00:50, 3333.06it/s]129093it [00:50, 3210.61it/s]129142it [00:50, 3197.42it/s]143301it [00:53, 3218.22it/s]131671it [00:50, 3228.36it/s]129446it [00:50, 3301.48it/s]129497it [00:50, 3296.35it/s]143656it [00:53, 3310.18it/s]132023it [00:51, 3309.54it/s]129800it [00:50, 3368.72it/s]129851it [00:51, 3366.05it/s]144012it [00:53, 3380.06it/s]132378it [00:51, 3376.83it/s]130138it [00:50, 3261.81it/s]130189it [00:51, 3236.81it/s]144352it [00:53, 3267.47it/s]132717it [00:51, 3253.48it/s]130487it [00:50, 3304.21it/s]130543it [00:51, 3322.03it/s]144711it [00:54, 3358.97it/s]133071it [00:51, 3334.22it/s]130819it [00:51, 3204.94it/s]130877it [00:51, 3208.37it/s]145049it [00:54, 3266.49it/s]133406it [00:51, 3191.40it/s]131175it [00:51, 3304.47it/s]131217it [00:51, 3262.83it/s]145402it [00:54, 3341.08it/s]133763it [00:51, 3297.25it/s]131528it [00:51, 3369.60it/s]131571it [00:51, 3342.81it/s]145751it [00:54, 3378.06it/s]134120it [00:51, 3373.51it/s]131867it [00:51, 3253.30it/s]131907it [00:51, 3221.66it/s]146090it [00:54, 3277.21it/s]134460it [00:51, 3258.12it/s]132220it [00:51, 3331.41it/s]132260it [00:51, 3309.03it/s]146443it [00:54, 3348.61it/s]134816it [00:51, 3344.24it/s]132555it [00:51, 3226.16it/s]132593it [00:51, 3194.63it/s]146779it [00:54, 3243.73it/s]135153it [00:52, 3241.16it/s]132909it [00:51, 3314.87it/s]132947it [00:52, 3291.28it/s]147135it [00:54, 3332.79it/s]135512it [00:52, 3339.74it/s]133249it [00:51, 3337.81it/s]147492it [00:54, 3400.91it/s]133290it [00:52, 3181.03it/s]135848it [00:52, 3234.44it/s]133584it [00:51, 3235.39it/s]147834it [00:55, 3285.21it/s]133630it [00:52, 3242.38it/s]136206it [00:52, 3331.15it/s]133940it [00:52, 3326.95it/s]148189it [00:55, 3361.03it/s]133986it [00:52, 3332.94it/s]136547it [00:52, 3352.21it/s]134274it [00:52, 3227.84it/s]148527it [00:55, 3246.95it/s]134321it [00:52, 3213.48it/s]136884it [00:52, 3242.27it/s]134630it [00:52, 3323.30it/s]148866it [00:55, 3286.67it/s]134676it [00:52, 3307.95it/s]137242it [00:52, 3337.49it/s]134971it [00:52, 3230.32it/s]149196it [00:55, 3197.32it/s]135009it [00:52, 3197.98it/s]137578it [00:52, 3238.29it/s]135330it [00:52, 3331.20it/s]149554it [00:55, 3306.11it/s]135370it [00:52, 3313.80it/s]137937it [00:52, 3338.62it/s]135689it [00:52, 3403.81it/s]149907it [00:55, 3370.60it/s]135725it [00:52, 3381.74it/s]138294it [00:52, 3405.39it/s]136031it [00:52, 3241.57it/s]150246it [00:55, 3254.54it/s]136065it [00:52, 3259.67it/s]138636it [00:53, 3282.15it/s]136386it [00:52, 3327.58it/s]150603it [00:55, 3345.05it/s]136420it [00:53, 3340.69it/s]138994it [00:53, 3366.06it/s]136721it [00:52, 3237.06it/s]136756it [00:53, 3186.98it/s]139333it [00:53, 3219.36it/s]137076it [00:52, 3324.00it/s]137111it [00:53, 3289.04it/s]139689it [00:53, 3313.71it/s]137432it [00:53, 3392.26it/s]137467it [00:53, 3366.87it/s]140023it [00:53, 3224.07it/s]137773it [00:53, 3276.35it/s]137806it [00:53, 3251.26it/s]140379it [00:53, 3318.75it/s]138131it [00:53, 3362.46it/s]138164it [00:53, 3345.26it/s]140737it [00:53, 3392.86it/s]138469it [00:53, 3256.38it/s]138501it [00:53, 3229.36it/s]141078it [00:53, 3275.72it/s]138821it [00:53, 3331.37it/s]138859it [00:53, 3328.84it/s]141437it [00:53, 3365.88it/s]139165it [00:53, 3360.86it/s]139194it [00:53, 3228.35it/s]141776it [00:54, 3259.76it/s]139503it [00:53, 3261.95it/s]139554it [00:54, 3334.08it/s]142136it [00:54, 3356.80it/s]139860it [00:53, 3350.19it/s]139895it [00:54, 3355.70it/s]142479it [00:54, 3377.75it/s]140197it [00:53, 3242.99it/s]140232it [00:54, 3239.52it/s]142818it [00:54, 3265.77it/s]140553it [00:54, 3332.54it/s]140586it [00:54, 3325.73it/s]143177it [00:54, 3356.65it/s]140888it [00:54, 3224.09it/s]140921it [00:54, 3211.39it/s]143515it [00:54, 3250.59it/s]141245it [00:54, 3322.59it/s]141281it [00:54, 3320.98it/s]143875it [00:54, 3348.99it/s]141599it [00:54, 3385.27it/s]141638it [00:54, 3392.59it/s]144212it [00:54, 3242.14it/s]141939it [00:54, 3248.52it/s]141979it [00:54, 3260.83it/s]144574it [00:54, 3349.69it/s]142280it [00:54, 3294.41it/s]142336it [00:54, 3349.01it/s]144934it [00:54, 3378.46it/s]142611it [00:54, 3190.48it/s]142673it [00:54, 3237.18it/s]145273it [00:55, 3275.85it/s]142966it [00:54, 3291.55it/s]143016it [00:55, 3291.15it/s]145635it [00:55, 3373.08it/s]143320it [00:54, 3362.68it/s]145974it [00:55, 3272.09it/s]143370it [00:55, 3194.84it/s]143658it [00:54, 3233.00it/s]146331it [00:55, 3356.77it/s]143729it [00:55, 3304.42it/s]144013it [00:55, 3321.37it/s]146688it [00:55, 3418.31it/s]144085it [00:55, 3377.24it/s]144347it [00:55, 3218.10it/s]147031it [00:55, 3304.38it/s]144425it [00:55, 3266.21it/s]144690it [00:55, 3277.72it/s]147392it [00:55, 3392.01it/s]144790it [00:55, 3374.45it/s]145049it [00:55, 3365.92it/s]147733it [00:55, 3284.05it/s]145130it [00:55, 3263.24it/s]145387it [00:55, 3253.76it/s]148074it [00:55, 3318.01it/s]145491it [00:55, 3361.23it/s]145747it [00:55, 3353.07it/s]145852it [00:55, 3432.10it/s]148411it [00:56, 3223.76it/s]146084it [00:55, 3247.28it/s]148769it [00:56, 3325.64it/s]146197it [00:56, 3261.82it/s]146440it [00:55, 3334.88it/s]149130it [00:56, 3406.14it/s]146554it [00:56, 3348.84it/s]146775it [00:55, 3229.34it/s]149472it [00:56, 3288.36it/s]146892it [00:56, 3245.90it/s]147133it [00:56, 3327.56it/s]149833it [00:56, 3379.79it/s]147250it [00:56, 3340.68it/s]147494it [00:56, 3407.73it/s]150173it [00:56, 3265.40it/s]147586it [00:56, 3236.06it/s]147837it [00:56, 3234.66it/s]150532it [00:56, 3357.35it/s]147943it [00:56, 3330.56it/s]148194it [00:56, 3328.06it/s]150893it [00:56, 3429.36it/s]148302it [00:56, 3404.39it/s]148530it [00:56, 3225.34it/s]148644it [00:56, 3267.40it/s]148885it [00:56, 3317.73it/s]149002it [00:56, 3356.68it/s]149243it [00:56, 3392.61it/s]149340it [00:56, 3200.15it/s]149584it [00:56, 3282.13it/s]149701it [00:57, 3313.68it/s]149941it [00:56, 3363.87it/s]150059it [00:57, 3388.98it/s]150279it [00:56, 3260.22it/s]150939it [01:00, 258.04it/s] 150400it [00:57, 3269.25it/s]150638it [00:57, 3354.12it/s]151299it [01:00, 361.85it/s]150760it [00:57, 3362.93it/s]150975it [00:57, 3116.63it/s]151658it [01:00, 499.04it/s]151961it [01:00, 643.80it/s]152313it [01:00, 861.16it/s]152629it [01:00, 1078.19it/s]152991it [01:00, 1386.66it/s]153353it [01:00, 1716.40it/s]153690it [01:00, 1966.01it/s]154056it [01:00, 2297.59it/s]154394it [01:01, 2475.11it/s]154746it [01:01, 2718.19it/s]155109it [01:01, 2945.94it/s]155453it [01:01, 2966.45it/s]155811it [01:01, 3128.17it/s]156150it [01:01, 3097.88it/s]156512it [01:01, 3241.34it/s]156851it [01:01, 3172.66it/s]157214it [01:01, 3300.23it/s]157569it [01:02, 3370.14it/s]157912it [01:02, 3260.46it/s]158272it [01:02, 3356.48it/s]158612it [01:02, 3245.34it/s]158978it [01:02, 3362.97it/s]159330it [01:02, 3264.21it/s]159696it [01:02, 3374.76it/s]160036it [01:02, 3263.96it/s]160365it [01:02, 3196.07it/s]160729it [01:03, 3320.42it/s]161063it [01:03, 3224.22it/s]161428it [01:03, 3345.35it/s]161794it [01:03, 3435.33it/s]162140it [01:03, 3335.77it/s]162507it [01:03, 3430.94it/s]162852it [01:03, 3325.97it/s]163221it [01:03, 3429.92it/s]151238it [01:01, 258.52it/s] 151600it [01:01, 361.15it/s]163566it [01:03, 3327.89it/s]151899it [01:01, 471.00it/s]163935it [01:03, 3431.25it/s]152250it [01:01, 642.22it/s]164297it [01:04, 3485.06it/s]152610it [01:01, 862.14it/s]164647it [01:04, 3359.00it/s]152931it [01:01, 1082.52it/s]165023it [01:04, 3472.61it/s]153293it [01:01, 1387.47it/s]165372it [01:04, 3362.47it/s]153623it [01:01, 1647.47it/s]165746it [01:04, 3470.48it/s]151291it [01:01, 247.73it/s] 153987it [01:01, 1988.85it/s]166095it [01:04, 3380.59it/s]151650it [01:01, 349.68it/s]154351it [01:01, 2312.50it/s]166470it [01:04, 3479.86it/s]151952it [01:01, 461.15it/s]154695it [01:02, 2498.87it/s]166839it [01:04, 3539.53it/s]152312it [01:01, 637.81it/s]155042it [01:02, 2725.01it/s]167195it [01:04, 3415.70it/s]152671it [01:01, 857.42it/s]151099it [01:02, 233.09it/s] 155380it [01:02, 2822.00it/s]167569it [01:04, 3507.73it/s]151458it [01:02, 326.58it/s]152989it [01:01, 1071.01it/s]155743it [01:02, 3029.27it/s]167922it [01:05, 3401.90it/s]151817it [01:02, 451.43it/s]153349it [01:02, 1373.82it/s]156082it [01:02, 3041.63it/s]168296it [01:05, 3497.20it/s]152118it [01:02, 582.14it/s]153674it [01:02, 1629.50it/s]156446it [01:02, 3203.24it/s]168648it [01:05, 3399.61it/s]152455it [01:02, 773.69it/s]154037it [01:02, 1971.38it/s]156808it [01:02, 3318.99it/s]169017it [01:05, 3482.45it/s]152763it [01:02, 975.76it/s]154369it [01:02, 2200.11it/s]157155it [01:02, 3246.87it/s]169386it [01:05, 3540.95it/s]153125it [01:02, 1273.86it/s]154732it [01:02, 2508.95it/s]157506it [01:02, 3318.94it/s]169742it [01:05, 3385.76it/s]153485it [01:02, 1596.34it/s]155093it [01:02, 2768.10it/s]157846it [01:03, 3239.02it/s]170111it [01:05, 3470.84it/s]153818it [01:02, 1852.43it/s]155437it [01:02, 2854.31it/s]158209it [01:03, 3350.10it/s]170460it [01:05, 3367.35it/s]154179it [01:03, 2183.54it/s]155783it [01:02, 3009.86it/s]158561it [01:03, 3264.58it/s]170833it [01:05, 3468.84it/s]154514it [01:03, 2378.87it/s]156120it [01:02, 3023.36it/s]158926it [01:03, 3373.63it/s]171182it [01:06, 3357.71it/s]154874it [01:03, 2657.43it/s]156482it [01:03, 3184.38it/s]159296it [01:03, 3465.08it/s]171552it [01:06, 3453.09it/s]155210it [01:03, 2755.58it/s]156842it [01:03, 3300.73it/s]159646it [01:03, 3351.64it/s]171921it [01:06, 3520.21it/s]155555it [01:03, 2931.43it/s]157187it [01:03, 3236.38it/s]160011it [01:03, 3437.03it/s]172275it [01:06, 3364.49it/s]155915it [01:03, 3108.25it/s]157552it [01:03, 3351.68it/s]160357it [01:03, 3289.19it/s]172646it [01:06, 3462.53it/s]156255it [01:03, 3089.90it/s]157895it [01:03, 3269.64it/s]160721it [01:03, 3387.05it/s]172995it [01:06, 3372.73it/s]156615it [01:03, 3228.48it/s]158257it [01:03, 3368.33it/s]161081it [01:03, 3285.71it/s]173362it [01:06, 3456.86it/s]156954it [01:03, 3169.50it/s]158599it [01:03, 3240.52it/s]161447it [01:04, 3390.56it/s]173710it [01:06, 3366.47it/s]157316it [01:04, 3295.71it/s]158965it [01:03, 3358.80it/s]161813it [01:04, 3467.66it/s]174081it [01:06, 3462.58it/s]157679it [01:04, 3391.36it/s]159330it [01:03, 3440.17it/s]162162it [01:04, 3366.51it/s]174450it [01:07, 3377.64it/s]158025it [01:04, 3282.70it/s]159677it [01:03, 3341.84it/s]162530it [01:04, 3453.96it/s]174804it [01:07, 3422.55it/s]158387it [01:04, 3377.80it/s]160037it [01:04, 3415.41it/s]162878it [01:04, 3349.33it/s]175177it [01:07, 3509.60it/s]158729it [01:04, 3283.27it/s]160381it [01:04, 3318.08it/s]163248it [01:04, 3448.46it/s]175530it [01:07, 3393.79it/s]159097it [01:04, 3394.49it/s]160744it [01:04, 3407.23it/s]163601it [01:04, 3307.63it/s]175902it [01:07, 3487.27it/s]159440it [01:04, 3292.78it/s]161087it [01:04, 3311.08it/s]163969it [01:04, 3411.77it/s]176253it [01:07, 3378.43it/s]159805it [01:04, 3394.75it/s]161452it [01:04, 3405.77it/s]164341it [01:04, 3499.46it/s]176619it [01:07, 3458.18it/s]160168it [01:04, 3462.77it/s]161807it [01:04, 3444.11it/s]164693it [01:05, 3384.95it/s]176970it [01:07, 3366.86it/s]160517it [01:04, 3311.41it/s]162153it [01:04, 3348.24it/s]165070it [01:05, 3493.45it/s]177326it [01:07, 3421.27it/s]160878it [01:05, 3394.79it/s]162521it [01:04, 3442.91it/s]165422it [01:05, 3383.01it/s]177692it [01:07, 3487.87it/s]161220it [01:05, 3302.67it/s]162867it [01:04, 3344.43it/s]165795it [01:05, 3482.26it/s]178042it [01:08, 3397.78it/s]161581it [01:05, 3389.94it/s]163235it [01:05, 3440.29it/s]166145it [01:05, 3393.81it/s]178415it [01:08, 3491.70it/s]161922it [01:05, 3302.76it/s]163600it [01:05, 3349.73it/s]166503it [01:05, 3447.00it/s]178766it [01:08, 3397.82it/s]162289it [01:05, 3408.04it/s]163966it [01:05, 3437.44it/s]166882it [01:05, 3544.75it/s]179135it [01:08, 3480.70it/s]162658it [01:05, 3487.76it/s]164322it [01:05, 3472.79it/s]167238it [01:05, 3425.55it/s]179490it [01:08, 3361.04it/s]163009it [01:05, 3341.50it/s]164671it [01:05, 3366.09it/s]167609it [01:05, 3507.20it/s]179867it [01:08, 3477.84it/s]163377it [01:05, 3438.10it/s]165048it [01:05, 3480.90it/s]167962it [01:05, 3400.50it/s]180228it [01:08, 3514.13it/s]163723it [01:05, 3333.11it/s]165398it [01:05, 3377.29it/s]168337it [01:06, 3500.57it/s]180581it [01:08, 3402.26it/s]164090it [01:06, 3424.09it/s]165770it [01:05, 3473.01it/s]168689it [01:06, 3402.40it/s]180959it [01:08, 3509.39it/s]164440it [01:06, 3322.17it/s]166120it [01:05, 3384.91it/s]169059it [01:06, 3487.57it/s]181312it [01:09, 3387.07it/s]164810it [01:06, 3427.38it/s]166493it [01:05, 3483.49it/s]169424it [01:06, 3534.72it/s]181682it [01:09, 3476.04it/s]165185it [01:06, 3520.24it/s]166870it [01:06, 3566.79it/s]169779it [01:06, 3373.74it/s]182032it [01:09, 3392.16it/s]167228it [01:06, 3404.98it/s]165539it [01:06, 3230.08it/s]170151it [01:06, 3472.06it/s]182405it [01:09, 3488.55it/s]167596it [01:06, 3483.29it/s]165907it [01:06, 3354.43it/s]170501it [01:06, 3371.45it/s]182772it [01:09, 3539.23it/s]167947it [01:06, 3387.18it/s]166248it [01:06, 3290.73it/s]170874it [01:06, 3473.38it/s]183128it [01:09, 3424.66it/s]168321it [01:06, 3486.47it/s]166618it [01:06, 3404.97it/s]171224it [01:06, 3373.78it/s]183499it [01:09, 3506.77it/s]168672it [01:06, 3391.17it/s]166962it [01:06, 3325.44it/s]171590it [01:07, 3455.45it/s]183851it [01:09, 3404.81it/s]169041it [01:06, 3476.68it/s]167335it [01:06, 3438.73it/s]171961it [01:07, 3528.39it/s]184226it [01:09, 3501.84it/s]169408it [01:06, 3531.71it/s]167707it [01:07, 3519.14it/s]172316it [01:07, 3406.63it/s]184578it [01:09, 3409.26it/s]169763it [01:06, 3403.40it/s]168061it [01:07, 3401.50it/s]172675it [01:07, 3457.93it/s]184951it [01:10, 3501.44it/s]170121it [01:07, 3452.00it/s]168432it [01:07, 3489.26it/s]173023it [01:07, 3374.81it/s]185303it [01:10, 3503.98it/s]170468it [01:07, 3346.54it/s]168783it [01:07, 3374.81it/s]173394it [01:07, 3468.88it/s]185655it [01:10, 3387.62it/s]170839it [01:07, 3449.27it/s]169149it [01:07, 3455.14it/s]173743it [01:07, 3377.58it/s]186035it [01:10, 3503.85it/s]171186it [01:07, 3337.59it/s]169497it [01:07, 3337.83it/s]174113it [01:07, 3469.05it/s]186387it [01:10, 3414.47it/s]171556it [01:07, 3439.35it/s]169852it [01:07, 3396.97it/s]174491it [01:07, 3557.11it/s]186777it [01:10, 3552.13it/s]171926it [01:07, 3514.41it/s]170225it [01:07, 3492.84it/s]174848it [01:07, 3427.47it/s]187134it [01:10, 3428.13it/s]172279it [01:07, 3388.77it/s]170576it [01:07, 3365.52it/s]175221it [01:08, 3511.09it/s]187509it [01:10, 3518.19it/s]172636it [01:07, 3440.16it/s]170945it [01:08, 3457.32it/s]175574it [01:08, 3351.69it/s]187889it [01:10, 3597.79it/s]172982it [01:07, 3348.34it/s]171293it [01:08, 3343.57it/s]175947it [01:08, 3458.29it/s]188251it [01:10, 3458.44it/s]173353it [01:07, 3452.31it/s]171657it [01:08, 3426.20it/s]176295it [01:08, 3357.80it/s]188612it [01:11, 3500.02it/s]173700it [01:08, 3337.67it/s]172002it [01:08, 3322.63it/s]176667it [01:08, 3461.03it/s]188964it [01:11, 3380.77it/s]174068it [01:08, 3433.99it/s]172357it [01:08, 3387.57it/s]177035it [01:08, 3521.90it/s]189336it [01:11, 3475.52it/s]174446it [01:08, 3532.19it/s]172729it [01:08, 3481.84it/s]177389it [01:08, 3399.65it/s]189686it [01:11, 3365.48it/s]174801it [01:08, 3404.47it/s]173079it [01:08, 3381.43it/s]177754it [01:08, 3469.30it/s]190064it [01:11, 3482.52it/s]175173it [01:08, 3492.45it/s]173448it [01:08, 3468.67it/s]178103it [01:08, 3394.07it/s]190414it [01:11, 3376.81it/s]175524it [01:08, 3333.89it/s]173797it [01:08, 3356.24it/s]178459it [01:09, 3439.96it/s]190788it [01:11, 3479.58it/s]175896it [01:08, 3442.75it/s]174169it [01:08, 3453.63it/s]178805it [01:09, 3364.98it/s]191159it [01:11, 3546.37it/s]174519it [01:09, 3361.25it/s]176243it [01:08, 3326.36it/s]179177it [01:09, 3464.98it/s]191515it [01:11, 3381.11it/s]174865it [01:09, 3388.61it/s]176607it [01:08, 3414.80it/s]179545it [01:09, 3525.74it/s]191892it [01:12, 3491.72it/s]175237it [01:09, 3483.80it/s]176977it [01:09, 3494.97it/s]179899it [01:09, 3410.50it/s]192244it [01:12, 3395.15it/s]177329it [01:09, 3385.15it/s]175587it [01:09, 3355.84it/s]180277it [01:09, 3516.26it/s]192624it [01:12, 3510.15it/s]177693it [01:09, 3457.74it/s]175958it [01:09, 3456.27it/s]180630it [01:09, 3395.97it/s]192977it [01:12, 3418.30it/s]178041it [01:09, 3371.52it/s]176306it [01:09, 3337.24it/s]180994it [01:09, 3463.05it/s]193349it [01:12, 3503.70it/s]178399it [01:09, 3431.39it/s]176679it [01:09, 3447.26it/s]181342it [01:09, 3360.83it/s]193716it [01:12, 3551.25it/s]178744it [01:09, 3346.17it/s]177039it [01:09, 3325.48it/s]181712it [01:09, 3457.77it/s]194073it [01:12, 3452.69it/s]179112it [01:09, 3440.85it/s]177407it [01:09, 3425.22it/s]182081it [01:10, 3372.95it/s]194435it [01:12, 3499.53it/s]179476it [01:09, 3497.67it/s]177773it [01:10, 3491.08it/s]182456it [01:10, 3478.82it/s]194787it [01:12, 3400.72it/s]179827it [01:09, 3403.92it/s]178124it [01:10, 3352.93it/s]182836it [01:10, 3571.42it/s]195176it [01:12, 3540.55it/s]180199it [01:09, 3494.03it/s]178490it [01:10, 3440.31it/s]183195it [01:10, 3444.14it/s]195532it [01:13, 3428.04it/s]180550it [01:10, 3383.63it/s]178836it [01:10, 3349.50it/s]183565it [01:10, 3515.66it/s]195902it [01:13, 3503.72it/s]180913it [01:10, 3453.08it/s]179201it [01:10, 3435.30it/s]196271it [01:13, 3555.57it/s]183919it [01:10, 3383.18it/s]181260it [01:10, 3353.74it/s]179559it [01:10, 3332.71it/s]184297it [01:10, 3495.92it/s]181629it [01:10, 3448.92it/s]179925it [01:10, 3425.34it/s]184649it [01:10, 3408.20it/s]182004it [01:10, 3534.56it/s]180307it [01:10, 3537.06it/s]185025it [01:10, 3507.87it/s]182359it [01:10, 3421.14it/s]180663it [01:10, 3393.99it/s]185389it [01:11, 3543.64it/s]182736it [01:10, 3520.88it/s]181023it [01:10, 3452.21it/s]185745it [01:11, 3426.40it/s]183090it [01:10, 3407.19it/s]181370it [01:11, 3336.26it/s]186129it [01:11, 3543.55it/s]183465it [01:10, 3503.87it/s]181736it [01:11, 3426.13it/s]186485it [01:11, 3396.49it/s]183817it [01:11, 3368.25it/s]182081it [01:11, 3335.78it/s]186875it [01:11, 3539.42it/s]184187it [01:11, 3461.20it/s]182456it [01:11, 3452.54it/s]187232it [01:11, 3429.28it/s]184561it [01:11, 3541.71it/s]182835it [01:11, 3550.03it/s]187607it [01:11, 3519.65it/s]184917it [01:11, 3432.96it/s]183192it [01:11, 3421.12it/s]187961it [01:11, 3420.70it/s]185288it [01:11, 3510.27it/s]183561it [01:11, 3497.16it/s]188334it [01:11, 3507.49it/s]185641it [01:11, 3384.67it/s]183913it [01:11, 3342.58it/s]188714it [01:11, 3591.94it/s]186014it [01:11, 3482.89it/s]184289it [01:11, 3460.75it/s]189075it [01:12, 3415.66it/s]186364it [01:11, 3399.46it/s]184638it [01:12, 3363.29it/s]189450it [01:12, 3508.16it/s]186746it [01:11, 3518.31it/s]185018it [01:12, 3486.23it/s]189804it [01:12, 3406.51it/s]187118it [01:11, 3575.45it/s]185378it [01:12, 3518.44it/s]190185it [01:12, 3520.40it/s]187477it [01:12, 3450.82it/s]185732it [01:12, 3390.34it/s]190539it [01:12, 3401.15it/s]187854it [01:12, 3542.17it/s]186112it [01:12, 3505.69it/s]190909it [01:12, 3485.17it/s]188210it [01:12, 3430.50it/s]186465it [01:12, 3388.83it/s]191281it [01:12, 3551.93it/s]188588it [01:12, 3530.71it/s]186855it [01:12, 3533.53it/s]191638it [01:12, 3396.12it/s]188943it [01:12, 3401.61it/s]187211it [01:12, 3359.68it/s]192021it [01:12, 3510.45it/s]189302it [01:12, 3453.55it/s]187582it [01:12, 3457.76it/s]192375it [01:13, 3399.16it/s]189649it [01:12, 3346.97it/s]187960it [01:13, 3377.03it/s]192760it [01:13, 3527.34it/s]190026it [01:12, 3467.03it/s]188332it [01:13, 3471.52it/s]193115it [01:13, 3422.07it/s]190403it [01:12, 3551.89it/s]188711it [01:13, 3560.96it/s]193484it [01:13, 3496.79it/s]190760it [01:13, 3422.11it/s]189070it [01:13, 3410.05it/s]193841it [01:13, 3401.67it/s]191127it [01:13, 3490.88it/s]189442it [01:13, 3495.32it/s]194222it [01:13, 3517.56it/s]191478it [01:13, 3388.72it/s]189794it [01:13, 3379.51it/s]194586it [01:13, 3552.41it/s]191857it [01:13, 3501.27it/s]190155it [01:13, 3444.27it/s]194943it [01:13, 3431.44it/s]192209it [01:13, 3305.95it/s]190502it [01:13, 3342.25it/s]195330it [01:13, 3555.36it/s]192584it [01:13, 3429.32it/s]190871it [01:13, 3441.32it/s]195688it [01:13, 3447.29it/s]192964it [01:13, 3533.41it/s]191242it [01:13, 3518.82it/s]196054it [01:14, 3506.56it/s]193320it [01:13, 3426.05it/s]191596it [01:14, 3401.70it/s]196407it [01:14, 3403.54it/s]193688it [01:13, 3498.32it/s]191975it [01:14, 3511.38it/s]194040it [01:13, 3414.72it/s]192328it [01:14, 3384.59it/s]194411it [01:14, 3498.91it/s]192711it [01:14, 3510.64it/s]194763it [01:14, 3370.76it/s]193064it [01:14, 3353.33it/s]195149it [01:14, 3509.68it/s]193431it [01:14, 3440.41it/s]195518it [01:14, 3561.70it/s]193806it [01:14, 3529.21it/s]195876it [01:14, 3430.39it/s]194161it [01:14, 3420.88it/s]196243it [01:14, 3496.94it/s]194539it [01:14, 3521.31it/s]194893it [01:15, 3389.93it/s]195282it [01:15, 3531.15it/s]195638it [01:15, 3407.49it/s]195986it [01:15, 3427.51it/s]196359it [01:15, 3514.07it/s]196628it [01:18, 221.06it/s] 197010it [01:18, 312.33it/s]197325it [01:18, 411.53it/s]197684it [01:18, 562.68it/s]198039it [01:18, 748.52it/s]198414it [01:19, 997.30it/s]198792it [01:19, 1291.75it/s]199141it [01:19, 1560.00it/s]199519it [01:19, 1906.48it/s]199870it [01:19, 2126.97it/s]200238it [01:19, 2438.58it/s]200583it [01:19, 2585.52it/s]200954it [01:19, 2849.65it/s]201326it [01:19, 3068.81it/s]201678it [01:20, 3098.50it/s]202054it [01:20, 3275.69it/s]202406it [01:20, 3265.91it/s]202791it [01:20, 3426.89it/s]203147it [01:20, 3270.58it/s]203517it [01:20, 3389.60it/s]203888it [01:20, 3478.46it/s]204243it [01:20, 3361.90it/s]204617it [01:20, 3466.59it/s]204968it [01:20, 3361.44it/s]205342it [01:21, 3467.43it/s]205692it [01:21, 3333.87it/s]206071it [01:21, 3461.76it/s]206440it [01:21, 3374.58it/s]206811it [01:21, 3469.21it/s]207189it [01:21, 3557.55it/s]207547it [01:21, 3419.53it/s]207923it [01:21, 3514.62it/s]208277it [01:21, 3400.55it/s]208655it [01:22, 3508.43it/s]209008it [01:22, 3415.97it/s]209381it [01:22, 3505.90it/s]209748it [01:22, 3552.90it/s]210105it [01:22, 3418.89it/s]196749it [01:19, 202.10it/s] 210478it [01:22, 3505.69it/s]197129it [01:19, 287.37it/s]210831it [01:22, 3387.31it/s]197451it [01:20, 383.43it/s]211193it [01:22, 3452.11it/s]197831it [01:20, 536.15it/s]211540it [01:22, 3362.94it/s]198154it [01:20, 697.78it/s]211913it [01:22, 3466.22it/s]198532it [01:20, 941.09it/s]212287it [01:23, 3544.08it/s]198909it [01:20, 1227.56it/s]212643it [01:23, 3424.44it/s]199258it [01:20, 1493.73it/s]213017it [01:23, 3512.73it/s]196595it [01:20, 205.55it/s] 199644it [01:20, 1851.15it/s]196982it [01:20, 292.67it/s]213370it [01:23, 3394.49it/s]199997it [01:20, 2099.70it/s]197300it [01:20, 387.50it/s]213729it [01:23, 3448.36it/s]200364it [01:20, 2411.23it/s]197670it [01:20, 535.96it/s]214076it [01:23, 3369.10it/s]200713it [01:20, 2593.12it/s]198057it [01:20, 736.49it/s]214443it [01:23, 3454.57it/s]201084it [01:21, 2855.75it/s]198397it [01:20, 939.99it/s]214812it [01:23, 3520.50it/s]201453it [01:21, 3066.05it/s]198772it [01:20, 1223.45it/s]215166it [01:23, 3415.63it/s]201808it [01:21, 3091.09it/s]199116it [01:20, 1488.08it/s]215538it [01:24, 3501.11it/s]202186it [01:21, 3274.88it/s]199495it [01:21, 1835.90it/s]196712it [01:21, 196.81it/s] 215890it [01:24, 3396.52it/s]202540it [01:21, 3290.86it/s]197074it [01:21, 274.90it/s]199845it [01:21, 2091.17it/s]216256it [01:24, 3470.96it/s]202926it [01:21, 3447.73it/s]197374it [01:21, 360.91it/s]200209it [01:21, 2398.52it/s]216605it [01:24, 3383.65it/s]203285it [01:21, 3386.16it/s]197747it [01:21, 506.24it/s]200578it [01:21, 2684.56it/s]216988it [01:24, 3512.13it/s]203654it [01:21, 3469.55it/s]198108it [01:21, 680.96it/s]200932it [01:21, 2804.34it/s]217360it [01:24, 3409.69it/s]204009it [01:21, 3373.79it/s]198482it [01:21, 913.43it/s]201293it [01:21, 3005.16it/s]217749it [01:24, 3544.85it/s]204382it [01:22, 3474.99it/s]198861it [01:21, 1194.92it/s]201641it [01:21, 3056.89it/s]218130it [01:24, 3616.75it/s]204746it [01:22, 3521.15it/s]199207it [01:22, 1454.24it/s]202014it [01:21, 3236.47it/s]218494it [01:24, 3500.81it/s]205102it [01:22, 3417.39it/s]199591it [01:22, 1805.77it/s]202363it [01:21, 3241.55it/s]218882it [01:24, 3608.60it/s]205474it [01:22, 3502.04it/s]202745it [01:21, 3401.60it/s]199942it [01:22, 2053.09it/s]219245it [01:25, 3487.35it/s]205827it [01:22, 3406.09it/s]203127it [01:22, 3520.05it/s]200310it [01:22, 2370.40it/s]219624it [01:25, 3571.63it/s]206207it [01:22, 3518.94it/s]203490it [01:22, 3420.73it/s]200657it [01:22, 2531.10it/s]219983it [01:25, 3493.95it/s]206561it [01:22, 3426.42it/s]203847it [01:22, 3460.99it/s]201025it [01:22, 2796.39it/s]220371it [01:25, 3603.42it/s]206930it [01:22, 3499.98it/s]201394it [01:22, 3017.28it/s]204199it [01:22, 3368.31it/s]220733it [01:25, 3505.37it/s]207299it [01:22, 3553.40it/s]204569it [01:22, 3460.50it/s]201746it [01:22, 3057.67it/s]221116it [01:25, 3595.31it/s]207656it [01:22, 3427.76it/s]202120it [01:22, 3239.59it/s]204919it [01:22, 3366.18it/s]221502it [01:25, 3672.07it/s]208038it [01:23, 3538.58it/s]205294it [01:22, 3475.92it/s]202471it [01:22, 3244.06it/s]221871it [01:25, 3537.83it/s]208394it [01:23, 3458.54it/s]205665it [01:22, 3542.13it/s]202856it [01:23, 3410.31it/s]222241it [01:25, 3583.85it/s]208768it [01:23, 3538.76it/s]206022it [01:22, 3436.56it/s]203212it [01:23, 3316.94it/s]222601it [01:26, 3464.63it/s]209124it [01:23, 3448.59it/s]206402it [01:23, 3540.09it/s]203580it [01:23, 3418.71it/s]222969it [01:26, 3526.26it/s]209491it [01:23, 3512.46it/s]203947it [01:23, 3490.42it/s]206758it [01:23, 3396.90it/s]223323it [01:26, 3412.40it/s]209851it [01:23, 3536.08it/s]207133it [01:23, 3496.34it/s]204302it [01:23, 3376.91it/s]223694it [01:26, 3495.72it/s]210206it [01:23, 3405.78it/s]204675it [01:23, 3476.72it/s]207485it [01:23, 3401.01it/s]224070it [01:26, 3570.25it/s]210577it [01:23, 3490.60it/s]207858it [01:23, 3493.81it/s]205027it [01:23, 3371.15it/s]224429it [01:26, 3419.14it/s]210928it [01:23, 3389.59it/s]205398it [01:23, 3467.50it/s]208210it [01:23, 3420.70it/s]224796it [01:26, 3489.66it/s]211301it [01:24, 3486.30it/s]208590it [01:23, 3529.17it/s]205748it [01:23, 3333.87it/s]225147it [01:26, 3382.63it/s]211651it [01:24, 3380.10it/s]208964it [01:23, 3588.00it/s]206125it [01:24, 3456.78it/s]225517it [01:26, 3470.87it/s]212025it [01:24, 3481.69it/s]206501it [01:24, 3542.86it/s]209324it [01:23, 3466.78it/s]225866it [01:26, 3355.04it/s]212390it [01:24, 3380.66it/s]209679it [01:23, 3489.58it/s]206858it [01:24, 3417.07it/s]226230it [01:27, 3435.00it/s]212753it [01:24, 3451.01it/s]207233it [01:24, 3510.15it/s]210030it [01:24, 3381.42it/s]226597it [01:27, 3500.79it/s]213127it [01:24, 3533.10it/s]210400it [01:24, 3470.35it/s]207586it [01:24, 3388.02it/s]226949it [01:27, 3333.09it/s]213482it [01:24, 3415.58it/s]210749it [01:24, 3368.65it/s]207964it [01:24, 3499.66it/s]227310it [01:27, 3411.78it/s]213857it [01:24, 3509.40it/s]211120it [01:24, 3466.17it/s]208316it [01:24, 3393.98it/s]227654it [01:27, 3311.98it/s]214210it [01:24, 3392.69it/s]211487it [01:24, 3524.64it/s]208686it [01:24, 3480.35it/s]228020it [01:27, 3409.47it/s]214578it [01:24, 3474.63it/s]211841it [01:24, 3424.76it/s]209036it [01:24, 3377.83it/s]228363it [01:27, 3322.01it/s]214928it [01:25, 3370.25it/s]212210it [01:24, 3500.87it/s]209410it [01:24, 3479.98it/s]228730it [01:27, 3420.27it/s]215296it [01:25, 3456.26it/s]209777it [01:25, 3534.32it/s]212562it [01:24, 3366.01it/s]229096it [01:27, 3488.71it/s]215667it [01:25, 3527.91it/s]212937it [01:24, 3474.30it/s]210132it [01:25, 3395.13it/s]229447it [01:28, 3396.84it/s]216022it [01:25, 3419.70it/s]213287it [01:25, 3371.00it/s]210499it [01:25, 3472.84it/s]229807it [01:28, 3454.12it/s]216398it [01:25, 3515.64it/s]213651it [01:25, 3447.16it/s]210848it [01:25, 3356.17it/s]230154it [01:28, 3343.34it/s]216751it [01:25, 3415.66it/s]214027it [01:25, 3535.33it/s]211203it [01:25, 3411.49it/s]230519it [01:28, 3429.02it/s]217137it [01:25, 3541.48it/s]214382it [01:25, 3409.56it/s]211549it [01:25, 3309.53it/s]230864it [01:28, 3330.22it/s]217493it [01:25, 3436.57it/s]214747it [01:25, 3478.31it/s]211921it [01:25, 3426.21it/s]231238it [01:28, 3446.56it/s]217872it [01:25, 3536.56it/s]212290it [01:25, 3501.57it/s]215097it [01:25, 3346.39it/s]231604it [01:28, 3508.23it/s]218257it [01:25, 3625.48it/s]215470it [01:25, 3447.75it/s]212642it [01:25, 3375.81it/s]231956it [01:28, 3384.96it/s]218621it [01:26, 3511.30it/s]215817it [01:25, 3365.18it/s]213013it [01:26, 3470.15it/s]232311it [01:28, 3431.78it/s]219004it [01:26, 3602.88it/s]216190it [01:25, 3469.15it/s]213362it [01:26, 3357.87it/s]232656it [01:28, 3333.05it/s]219366it [01:26, 3529.83it/s]216565it [01:25, 3548.66it/s]213715it [01:26, 3405.72it/s]233025it [01:29, 3434.92it/s]219748it [01:26, 3611.96it/s]216922it [01:26, 3451.92it/s]214069it [01:26, 3316.07it/s]233370it [01:29, 3345.34it/s]220111it [01:26, 3512.77it/s]217294it [01:26, 3527.13it/s]214437it [01:26, 3417.85it/s]233740it [01:29, 3439.16it/s]220508it [01:26, 3644.44it/s]217648it [01:26, 3448.80it/s]214804it [01:26, 3490.65it/s]234110it [01:29, 3513.30it/s]220874it [01:26, 3486.60it/s]218022it [01:26, 3532.35it/s]215155it [01:26, 3365.72it/s]234463it [01:29, 3389.14it/s]221254it [01:26, 3574.01it/s]218377it [01:26, 3436.10it/s]215525it [01:26, 3460.33it/s]234818it [01:29, 3433.17it/s]221630it [01:26, 3517.16it/s]218767it [01:26, 3569.71it/s]215873it [01:26, 3351.46it/s]235163it [01:29, 3329.92it/s]222011it [01:27, 3598.16it/s]219126it [01:26, 3462.13it/s]216234it [01:26, 3424.64it/s]235524it [01:29, 3410.22it/s]222380it [01:27, 3623.28it/s]219513it [01:26, 3577.89it/s]216589it [01:27, 3338.23it/s]235867it [01:29, 3314.60it/s]222744it [01:27, 3502.82it/s]219896it [01:26, 3648.78it/s]216969it [01:27, 3468.34it/s]236233it [01:30, 3413.49it/s]223113it [01:27, 3555.10it/s]220263it [01:27, 3551.70it/s]217342it [01:27, 3543.54it/s]236593it [01:30, 3466.59it/s]223470it [01:27, 3399.31it/s]220652it [01:27, 3648.67it/s]217698it [01:27, 3451.35it/s]236941it [01:30, 3356.61it/s]223844it [01:27, 3495.38it/s]221019it [01:27, 3495.37it/s]218083it [01:27, 3565.60it/s]237305it [01:30, 3438.34it/s]224196it [01:27, 3399.64it/s]221403it [01:27, 3591.73it/s]218441it [01:27, 3431.16it/s]237651it [01:30, 3304.73it/s]224567it [01:27, 3487.76it/s]221765it [01:27, 3524.17it/s]218821it [01:27, 3535.90it/s]238012it [01:30, 3391.13it/s]224935it [01:27, 3543.05it/s]222134it [01:27, 3571.36it/s]219177it [01:27, 3430.50it/s]238360it [01:30, 3297.81it/s]225291it [01:28, 3422.06it/s]222493it [01:27, 3450.46it/s]219561it [01:27, 3546.45it/s]238723it [01:30, 3392.38it/s]225663it [01:28, 3505.10it/s]222868it [01:27, 3533.79it/s]219945it [01:28, 3631.38it/s]239086it [01:30, 3461.14it/s]226015it [01:28, 3351.80it/s]223240it [01:27, 3577.61it/s]220310it [01:28, 3514.78it/s]239434it [01:30, 3342.33it/s]226381it [01:28, 3436.88it/s]223599it [01:27, 3416.03it/s]220701it [01:28, 3627.02it/s]239803it [01:31, 3441.20it/s]226727it [01:28, 3341.94it/s]223974it [01:28, 3508.91it/s]221066it [01:28, 3500.42it/s]240149it [01:31, 3306.95it/s]227087it [01:28, 3413.48it/s]224327it [01:28, 3402.47it/s]221438it [01:28, 3562.59it/s]240513it [01:31, 3399.45it/s]227448it [01:28, 3468.50it/s]224696it [01:28, 3483.98it/s]221796it [01:28, 3488.22it/s]240878it [01:31, 3471.14it/s]227797it [01:28, 3358.34it/s]225047it [01:28, 3380.15it/s]222164it [01:28, 3543.00it/s]241227it [01:31, 3367.27it/s]228163it [01:28, 3444.64it/s]225412it [01:28, 3456.50it/s]222520it [01:28, 3413.87it/s]241591it [01:31, 3444.87it/s]228509it [01:28, 3318.64it/s]225781it [01:28, 3523.66it/s]222889it [01:28, 3485.23it/s]241937it [01:31, 3352.73it/s]228876it [01:29, 3417.37it/s]226135it [01:28, 3396.05it/s]223261it [01:28, 3551.74it/s]242302it [01:31, 3438.18it/s]229220it [01:29, 3322.98it/s]226491it [01:28, 3440.93it/s]223618it [01:29, 3411.27it/s]242648it [01:31, 3313.48it/s]229600it [01:29, 3458.36it/s]226837it [01:28, 3314.18it/s]223983it [01:29, 3478.70it/s]243014it [01:32, 3410.70it/s]229964it [01:29, 3509.98it/s]227202it [01:29, 3408.46it/s]224333it [01:29, 3375.30it/s]243384it [01:32, 3494.22it/s]230317it [01:29, 3391.94it/s]227545it [01:29, 3307.39it/s]224702it [01:29, 3464.28it/s]243735it [01:32, 3381.55it/s]230683it [01:29, 3468.66it/s]227911it [01:29, 3406.16it/s]225050it [01:29, 3349.45it/s]244100it [01:32, 3450.81it/s]231032it [01:29, 3383.19it/s]228276it [01:29, 3476.48it/s]225415it [01:29, 3434.68it/s]244447it [01:32, 3350.41it/s]231392it [01:29, 3445.13it/s]228625it [01:29, 3370.77it/s]225785it [01:29, 3508.92it/s]244809it [01:32, 3427.14it/s]231738it [01:29, 3348.86it/s]228988it [01:29, 3444.10it/s]226138it [01:29, 3360.56it/s]245153it [01:32, 3300.34it/s]232106it [01:30, 3442.50it/s]229334it [01:29, 3321.57it/s]226495it [01:29, 3420.17it/s]245520it [01:32, 3404.57it/s]232477it [01:30, 3519.95it/s]229708it [01:29, 3440.01it/s]226839it [01:30, 3286.28it/s]245884it [01:32, 3472.14it/s]232831it [01:30, 3384.88it/s]230054it [01:29, 3340.22it/s]227203it [01:30, 3384.99it/s]246233it [01:32, 3362.47it/s]233207it [01:30, 3491.42it/s]230418it [01:29, 3425.55it/s]227544it [01:30, 3274.29it/s]246598it [01:33, 3445.16it/s]233558it [01:30, 3389.83it/s]230787it [01:30, 3500.68it/s]227909it [01:30, 3380.25it/s]246944it [01:33, 3344.74it/s]233917it [01:30, 3447.24it/s]231139it [01:30, 3401.10it/s]228274it [01:30, 3456.15it/s]247312it [01:33, 3440.69it/s]234264it [01:30, 3361.74it/s]231504it [01:30, 3472.27it/s]228622it [01:30, 3333.65it/s]247658it [01:33, 3338.38it/s]234627it [01:30, 3438.71it/s]231853it [01:30, 3337.90it/s]228977it [01:30, 3394.80it/s]248015it [01:33, 3402.65it/s]234997it [01:30, 3514.78it/s]232219it [01:30, 3428.46it/s]248375it [01:33, 3458.38it/s]229319it [01:30, 3275.51it/s]235350it [01:30, 3368.80it/s]232564it [01:30, 3338.87it/s]229689it [01:30, 3395.53it/s]248722it [01:33, 3352.51it/s]235713it [01:31, 3443.10it/s]232931it [01:30, 3433.48it/s]249090it [01:33, 3445.36it/s]230031it [01:31, 3291.21it/s]236059it [01:31, 3340.18it/s]233305it [01:30, 3520.70it/s]230393it [01:31, 3384.28it/s]249436it [01:33, 3345.37it/s]236414it [01:31, 3399.33it/s]233659it [01:30, 3400.26it/s]230754it [01:31, 3449.15it/s]249798it [01:34, 3423.87it/s]236756it [01:31, 3307.86it/s]234026it [01:31, 3477.67it/s]231101it [01:31, 3358.38it/s]250142it [01:34, 3336.58it/s]237121it [01:31, 3405.96it/s]234376it [01:31, 3374.67it/s]231458it [01:31, 3419.48it/s]250504it [01:34, 3417.19it/s]237489it [01:31, 3483.85it/s]234729it [01:31, 3418.16it/s]250874it [01:34, 3497.75it/s]231802it [01:31, 3304.83it/s]237839it [01:31, 3354.42it/s]235073it [01:31, 3321.89it/s]232167it [01:31, 3402.50it/s]251225it [01:34, 3380.89it/s]238206it [01:31, 3444.27it/s]235438it [01:31, 3415.55it/s]232534it [01:31, 3477.77it/s]251590it [01:34, 3448.92it/s]238552it [01:31, 3332.90it/s]235801it [01:31, 3477.80it/s]232884it [01:31, 3346.99it/s]251937it [01:34, 3342.37it/s]238917it [01:32, 3421.38it/s]236150it [01:31, 3363.85it/s]233254it [01:31, 3446.49it/s]252306it [01:34, 3440.61it/s]239270it [01:32, 3290.08it/s]236514it [01:31, 3442.53it/s]233601it [01:32, 3338.04it/s]252652it [01:34, 3329.48it/s]239635it [01:32, 3391.24it/s]236860it [01:31, 3336.56it/s]233968it [01:32, 3431.62it/s]253008it [01:34, 3393.71it/s]240004it [01:32, 3475.88it/s]237226it [01:31, 3427.03it/s]253374it [01:35, 3469.75it/s]234313it [01:32, 3308.97it/s]240354it [01:32, 3361.37it/s]237581it [01:32, 3462.07it/s]234675it [01:32, 3395.85it/s]240721it [01:32, 3449.25it/s]237929it [01:32, 3341.14it/s]235041it [01:32, 3472.02it/s]241068it [01:32, 3344.35it/s]238290it [01:32, 3415.87it/s]235390it [01:32, 3344.29it/s]241439it [01:32, 3448.03it/s]238633it [01:32, 3319.92it/s]235753it [01:32, 3424.22it/s]241790it [01:32, 3320.69it/s]238995it [01:32, 3405.21it/s]236098it [01:32, 3306.85it/s]242158it [01:32, 3421.48it/s]239337it [01:32, 3307.11it/s]236463it [01:32, 3404.14it/s]242528it [01:33, 3500.15it/s]239703it [01:32, 3406.48it/s]236806it [01:33, 3268.24it/s]242880it [01:33, 3373.45it/s]240067it [01:32, 3448.66it/s]237170it [01:33, 3371.47it/s]243251it [01:33, 3467.28it/s]240413it [01:32, 3330.83it/s]237535it [01:33, 3450.62it/s]243600it [01:33, 3355.96it/s]240780it [01:33, 3427.77it/s]237882it [01:33, 3317.24it/s]243967it [01:33, 3444.45it/s]241125it [01:33, 3331.71it/s]238246it [01:33, 3409.11it/s]244314it [01:33, 3303.77it/s]241494it [01:33, 3433.87it/s]238589it [01:33, 3291.80it/s]244680it [01:33, 3403.31it/s]241839it [01:33, 3343.04it/s]238952it [01:33, 3387.22it/s]245047it [01:33, 3479.24it/s]242206it [01:33, 3436.09it/s]239293it [01:33, 3257.09it/s]245397it [01:33, 3360.90it/s]242571it [01:33, 3496.92it/s]239658it [01:33, 3368.26it/s]245757it [01:34, 3428.45it/s]242922it [01:33, 3322.98it/s]240016it [01:33, 3427.49it/s]243281it [01:33, 3397.79it/s]246102it [01:34, 3219.17it/s]240361it [01:34, 3295.72it/s]246468it [01:34, 3341.80it/s]243623it [01:33, 3307.95it/s]240727it [01:34, 3398.18it/s]246825it [01:34, 3406.50it/s]243988it [01:33, 3405.38it/s]241069it [01:34, 3291.14it/s]247169it [01:34, 3306.08it/s]244331it [01:34, 3300.21it/s]241407it [01:34, 3257.28it/s]247502it [01:34, 3278.75it/s]244663it [01:34, 3245.59it/s]241765it [01:34, 3349.13it/s]247832it [01:34, 3225.58it/s]245030it [01:34, 3366.81it/s]242102it [01:34, 3263.82it/s]248200it [01:34, 3354.24it/s]245369it [01:34, 3277.25it/s]242469it [01:34, 3377.99it/s]248537it [01:34, 3273.74it/s]245726it [01:34, 3360.59it/s]242809it [01:34, 3282.02it/s]248909it [01:34, 3401.95it/s]246064it [01:34, 3276.17it/s]243178it [01:34, 3397.01it/s]249280it [01:35, 3481.53it/s]246431it [01:34, 3387.15it/s]243520it [01:35, 3294.90it/s]246798it [01:34, 3468.66it/s]249630it [01:35, 3338.85it/s]243889it [01:35, 3400.83it/s]250002it [01:35, 3447.09it/s]247146it [01:34, 3360.43it/s]244245it [01:35, 3445.34it/s]247513it [01:35, 3449.13it/s]250349it [01:35, 3352.73it/s]244591it [01:35, 3322.32it/s]250725it [01:35, 3468.96it/s]247860it [01:35, 3342.03it/s]244957it [01:35, 3417.41it/s]248220it [01:35, 3415.97it/s]251074it [01:35, 3356.98it/s]245301it [01:35, 3290.53it/s]251439it [01:35, 3438.99it/s]248563it [01:35, 3277.65it/s]245666it [01:35, 3391.51it/s]251800it [01:35, 3485.70it/s]248930it [01:35, 3385.02it/s]246007it [01:35, 3277.40it/s]249295it [01:35, 3459.49it/s]252150it [01:35, 3338.64it/s]246369it [01:35, 3372.88it/s]252513it [01:36, 3420.25it/s]249643it [01:35, 3338.27it/s]246723it [01:35, 3419.12it/s]250010it [01:35, 3431.86it/s]252857it [01:36, 3308.12it/s]247067it [01:36, 3305.60it/s]253223it [01:36, 3407.03it/s]250355it [01:35, 3323.19it/s]247432it [01:36, 3402.89it/s]250727it [01:35, 3434.53it/s]253566it [01:36, 3310.69it/s]247774it [01:36, 3289.18it/s]251073it [01:36, 3296.93it/s]248136it [01:36, 3382.87it/s]251437it [01:36, 3393.52it/s]248499it [01:36, 3452.96it/s]251800it [01:36, 3451.85it/s]248846it [01:36, 3330.47it/s]252147it [01:36, 3358.12it/s]249203it [01:36, 3398.02it/s]252508it [01:36, 3430.55it/s]249545it [01:36, 3284.81it/s]252853it [01:36, 3316.87it/s]249909it [01:36, 3381.72it/s]253220it [01:36, 3417.88it/s]250249it [01:37, 3285.58it/s]253564it [01:36, 3311.48it/s]250616it [01:37, 3395.51it/s]250985it [01:37, 3480.01it/s]251335it [01:37, 3345.99it/s]251682it [01:37, 3379.11it/s]252022it [01:37, 3243.52it/s]252385it [01:37, 3351.21it/s]252722it [01:37, 3242.55it/s]253091it [01:37, 3368.21it/s]253460it [01:37, 3459.43it/s]253723it [01:42, 163.86it/s] 254090it [01:42, 231.70it/s]254388it [01:42, 305.42it/s]254752it [01:42, 428.74it/s]255115it [01:42, 589.32it/s]255440it [01:42, 762.33it/s]255796it [01:42, 1004.70it/s]256124it [01:42, 1241.98it/s]256489it [01:42, 1566.40it/s]256854it [01:42, 1902.47it/s]257197it [01:43, 2134.37it/s]257559it [01:43, 2440.95it/s]257899it [01:43, 2577.39it/s]258260it [01:43, 2811.42it/s]258596it [01:43, 2869.06it/s]258966it [01:43, 3084.77it/s]259330it [01:43, 3234.24it/s]259676it [01:43, 3182.06it/s]260043it [01:43, 3317.41it/s]260387it [01:44, 3230.40it/s]260751it [01:44, 3344.40it/s]261101it [01:44, 3387.12it/s]261445it [01:44, 3279.21it/s]261777it [01:44, 3216.05it/s]262102it [01:44, 3031.92it/s]262409it [01:44, 3034.50it/s]262715it [01:44, 3013.08it/s]263019it [01:44, 2822.97it/s]263349it [01:45, 2953.91it/s]263648it [01:45, 2949.69it/s]264014it [01:45, 3152.51it/s]264386it [01:45, 3316.86it/s]264720it [01:45, 3232.42it/s]265089it [01:45, 3364.66it/s]265428it [01:45, 3291.56it/s]265799it [01:45, 3412.30it/s]266148it [01:45, 3301.49it/s]266521it [01:45, 3422.89it/s]266896it [01:46, 3515.77it/s]267250it [01:46, 3398.85it/s]267620it [01:46, 3484.19it/s]267970it [01:46, 3381.65it/s]268344it [01:46, 3484.38it/s]268694it [01:46, 3331.58it/s]253899it [01:43, 148.02it/s] 269064it [01:46, 3434.34it/s]254262it [01:44, 210.47it/s]269433it [01:46, 3507.55it/s]254566it [01:44, 281.14it/s]254932it [01:44, 397.54it/s]269786it [01:46, 3395.17it/s]255297it [01:44, 550.07it/s]270160it [01:47, 3494.09it/s]255622it [01:44, 715.64it/s]270512it [01:47, 3383.62it/s]255987it [01:44, 955.88it/s]270882it [01:47, 3472.32it/s]256318it [01:44, 1193.69it/s]271231it [01:47, 3335.49it/s]253897it [01:44, 148.85it/s] 256686it [01:44, 1515.04it/s]271601it [01:47, 3437.67it/s]254233it [01:44, 206.75it/s]257024it [01:44, 1779.26it/s]271972it [01:47, 3514.40it/s]254529it [01:44, 275.85it/s]257360it [01:44, 2063.74it/s]272326it [01:47, 3389.91it/s]254882it [01:44, 388.52it/s]257727it [01:45, 2391.79it/s]272693it [01:47, 3468.11it/s]255221it [01:44, 529.98it/s]258069it [01:45, 2567.09it/s]273042it [01:47, 3345.02it/s]255531it [01:44, 689.83it/s]258420it [01:45, 2792.34it/s]273405it [01:47, 3425.88it/s]255875it [01:44, 916.34it/s]258758it [01:45, 2877.00it/s]273750it [01:48, 3332.96it/s]256192it [01:45, 1147.02it/s]259130it [01:45, 3097.25it/s]274108it [01:48, 3402.20it/s]256534it [01:45, 1442.33it/s]259497it [01:45, 3251.55it/s]274474it [01:48, 3476.75it/s]256860it [01:45, 1727.06it/s]259847it [01:45, 3218.48it/s]274823it [01:48, 3371.91it/s]257183it [01:45, 1963.10it/s]260215it [01:45, 3346.33it/s]275191it [01:48, 3459.23it/s]257536it [01:45, 2282.07it/s]260563it [01:45, 3265.88it/s]275539it [01:48, 3363.90it/s]257861it [01:45, 2453.61it/s]253808it [01:45, 145.48it/s] 260934it [01:45, 3391.12it/s]275907it [01:48, 3454.29it/s]258196it [01:45, 2668.42it/s]254172it [01:45, 205.67it/s]261281it [01:46, 3286.51it/s]276254it [01:48, 3353.31it/s]258552it [01:45, 2894.61it/s]254470it [01:46, 272.30it/s]261650it [01:46, 3399.12it/s]276601it [01:48, 3384.64it/s]258885it [01:45, 2920.80it/s]254833it [01:46, 383.66it/s]276968it [01:49, 3465.46it/s]262019it [01:46, 3312.67it/s]259244it [01:46, 3100.37it/s]255198it [01:46, 531.43it/s]262385it [01:46, 3409.36it/s]277316it [01:49, 3365.49it/s]259578it [01:46, 3072.99it/s]255524it [01:46, 693.33it/s]262755it [01:46, 3491.57it/s]277684it [01:49, 3455.07it/s]259946it [01:46, 3240.66it/s]255890it [01:46, 928.89it/s]263107it [01:46, 3376.04it/s]278031it [01:49, 3354.48it/s]260296it [01:46, 3314.22it/s]256223it [01:46, 1158.79it/s]263470it [01:46, 3448.28it/s]278396it [01:49, 3437.13it/s]260637it [01:46, 3252.94it/s]256592it [01:46, 1476.51it/s]263817it [01:46, 3319.74it/s]278749it [01:49, 3347.51it/s]261009it [01:46, 3386.63it/s]256960it [01:46, 1811.55it/s]264187it [01:46, 3427.71it/s]279117it [01:49, 3440.68it/s]261353it [01:46, 3260.75it/s]257306it [01:46, 2056.41it/s]279486it [01:49, 3511.87it/s]264539it [01:47, 3331.71it/s]261722it [01:46, 3381.11it/s]257669it [01:47, 2370.08it/s]264906it [01:47, 3425.77it/s]279839it [01:49, 3356.89it/s]262064it [01:46, 3308.92it/s]258012it [01:47, 2545.92it/s]265277it [01:47, 3505.88it/s]280203it [01:49, 3436.22it/s]262398it [01:46, 3249.81it/s]258378it [01:47, 2808.74it/s]265630it [01:47, 3396.38it/s]280549it [01:50, 3342.73it/s]262768it [01:47, 3377.81it/s]258721it [01:47, 2870.45it/s]266001it [01:47, 3484.36it/s]280919it [01:50, 3440.43it/s]263108it [01:47, 3306.08it/s]259092it [01:47, 3086.86it/s]266351it [01:47, 3376.85it/s]281269it [01:50, 3330.96it/s]263473it [01:47, 3404.23it/s]259456it [01:47, 3235.57it/s]266709it [01:47, 3429.14it/s]281635it [01:50, 3424.37it/s]263815it [01:47, 3318.87it/s]259806it [01:47, 3200.12it/s]282002it [01:50, 3493.38it/s]267059it [01:47, 3335.47it/s]264186it [01:47, 3430.41it/s]260176it [01:47, 3337.84it/s]267432it [01:47, 3446.44it/s]282353it [01:50, 3351.97it/s]264539it [01:47, 3296.78it/s]260524it [01:47, 3262.24it/s]267803it [01:47, 3520.56it/s]282717it [01:50, 3433.01it/s]264906it [01:47, 3401.81it/s]260894it [01:47, 3384.00it/s]268157it [01:48, 3403.69it/s]283063it [01:50, 3323.62it/s]265261it [01:47, 3443.82it/s]261240it [01:48, 3279.10it/s]268528it [01:48, 3490.91it/s]283427it [01:50, 3412.65it/s]265607it [01:47, 3357.93it/s]261605it [01:48, 3382.69it/s]268879it [01:48, 3358.84it/s]283789it [01:51, 3304.88it/s]265975it [01:47, 3450.51it/s]261977it [01:48, 3477.02it/s]269233it [01:48, 3408.58it/s]284155it [01:51, 3404.19it/s]266322it [01:48, 3318.78it/s]262329it [01:48, 3355.96it/s]269579it [01:48, 3317.90it/s]284518it [01:51, 3468.88it/s]266693it [01:48, 3430.61it/s]262698it [01:48, 3449.98it/s]269953it [01:48, 3436.22it/s]284867it [01:51, 3330.87it/s]267059it [01:48, 3340.99it/s]263046it [01:48, 3344.07it/s]270321it [01:48, 3506.00it/s]285231it [01:51, 3417.35it/s]267413it [01:48, 3396.22it/s]263409it [01:48, 3424.37it/s]270673it [01:48, 3377.18it/s]285575it [01:51, 3321.99it/s]267783it [01:48, 3483.52it/s]263754it [01:48, 3296.29it/s]271013it [01:48, 3284.50it/s]285941it [01:51, 3417.70it/s]268133it [01:48, 3389.10it/s]264123it [01:48, 3408.23it/s]286307it [01:51, 3486.59it/s]271343it [01:49, 3222.22it/s]268502it [01:48, 3434.86it/s]264496it [01:49, 3500.20it/s]271697it [01:49, 3312.55it/s]286658it [01:51, 3381.00it/s]268847it [01:48, 3341.37it/s]264848it [01:49, 3370.99it/s]272066it [01:49, 3420.57it/s]287027it [01:51, 3468.26it/s]287112it [01:51, 2563.67it/s]
2022-08-11 15:05:55 | INFO | root | success load 287112 data
2022-08-11 15:05:55 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-08-11 15:05:55 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-08-11 15:05:55 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-08-11 15:05:55 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-08-11 15:05:55 | INFO | transformer.tokenization_utils | loading file None
2022-08-11 15:05:55 | INFO | transformer.tokenization_utils | loading file None
2022-08-11 15:05:55 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
269214it [01:48, 3434.64it/s]265220it [01:49, 3470.02it/s]272410it [01:49, 3314.42it/s]269579it [01:49, 3338.97it/s]265569it [01:49, 3368.49it/s]272776it [01:49, 3412.33it/s]269937it [01:49, 3407.28it/s]265937it [01:49, 3451.33it/s]273119it [01:49, 3313.21it/s]270306it [01:49, 3486.41it/s]266284it [01:49, 3324.08it/s]273486it [01:49, 3414.36it/s]270656it [01:49, 3385.03it/s]266654it [01:49, 3430.33it/s]273829it [01:49, 3316.87it/s]271009it [01:49, 3424.95it/s]267026it [01:49, 3512.22it/s]274197it [01:49, 3418.96it/s]271353it [01:49, 3338.02it/s]267379it [01:49, 3394.72it/s]274549it [01:49, 3442.09it/s]271721it [01:49, 3435.61it/s]267750it [01:49, 3483.71it/s]274895it [01:50, 3340.33it/s]272074it [01:49, 3461.19it/s]268100it [01:50, 3383.06it/s]275264it [01:50, 3441.19it/s]272421it [01:49, 3346.56it/s]268466it [01:50, 3461.33it/s]275610it [01:50, 3334.91it/s]272786it [01:50, 3434.10it/s]268814it [01:50, 3322.77it/s]275978it [01:50, 3431.55it/s]273131it [01:50, 3309.15it/s]269181it [01:50, 3421.33it/s]276323it [01:50, 3320.66it/s]273499it [01:50, 3407.90it/s]269551it [01:50, 3499.25it/s]276688it [01:50, 3414.47it/s]273842it [01:50, 3335.19it/s]269903it [01:50, 3392.97it/s]277047it [01:50, 3463.99it/s]274196it [01:50, 3392.66it/s]270274it [01:50, 3482.61it/s]277395it [01:50, 3342.49it/s]274559it [01:50, 3459.32it/s]270624it [01:50, 3366.90it/s]277764it [01:50, 3439.93it/s]274906it [01:50, 3317.72it/s]270988it [01:50, 3443.50it/s]278110it [01:51, 3327.11it/s]275278it [01:50, 3430.75it/s]271334it [01:51, 3315.50it/s]278475it [01:51, 3419.51it/s]275623it [01:50, 3305.65it/s]271703it [01:51, 3421.89it/s]278819it [01:51, 3318.34it/s]275991it [01:50, 3411.69it/s]272072it [01:51, 3496.97it/s]279186it [01:51, 3417.31it/s]276335it [01:51, 3222.80it/s]272424it [01:51, 3253.29it/s]279530it [01:51, 3329.47it/s]276661it [01:51, 3219.60it/s]272754it [01:51, 3216.71it/s]279865it [01:51, 3153.10it/s]276999it [01:51, 3264.17it/s]273079it [01:51, 3128.49it/s]280221it [01:51, 3266.56it/s]277328it [01:51, 3198.30it/s]273431it [01:51, 3237.04it/s]280551it [01:51, 3208.59it/s]277650it [01:51, 3140.30it/s]273777it [01:51, 3160.68it/s]280920it [01:51, 3343.61it/s]277979it [01:51, 3064.88it/s]274134it [01:51, 3273.44it/s]281286it [01:52, 3433.06it/s]278331it [01:51, 3192.14it/s]274488it [01:51, 3348.07it/s]281631it [01:52, 3319.70it/s]278699it [01:51, 3332.06it/s]274825it [01:52, 3242.71it/s]281996it [01:52, 3412.23it/s]279034it [01:51, 3237.28it/s]275178it [01:52, 3323.29it/s]282339it [01:52, 3309.62it/s]279399it [01:52, 3353.85it/s]275512it [01:52, 3233.87it/s]282686it [01:52, 3354.02it/s]279736it [01:52, 3255.05it/s]275867it [01:52, 3322.46it/s]283023it [01:52, 3256.46it/s]280100it [01:52, 3363.63it/s]276213it [01:52, 3361.01it/s]283387it [01:52, 3366.28it/s]280455it [01:52, 3416.51it/s]276551it [01:52, 3237.51it/s]283753it [01:52, 3449.93it/s]280798it [01:52, 3324.76it/s]276907it [01:52, 3327.91it/s]284100it [01:52, 3329.95it/s]281151it [01:52, 3381.76it/s]277242it [01:52, 3226.19it/s]284465it [01:52, 3421.46it/s]281491it [01:52, 3298.49it/s]277595it [01:52, 3312.96it/s]284809it [01:53, 3312.74it/s]281843it [01:52, 3361.76it/s]277950it [01:53, 3381.08it/s]285173it [01:53, 3405.67it/s]282181it [01:52, 3283.59it/s]278290it [01:53, 3251.11it/s]285521it [01:53, 3426.72it/s]282533it [01:52, 3351.17it/s]278633it [01:53, 3302.37it/s]285865it [01:53, 3310.31it/s]282896it [01:53, 3431.26it/s]278965it [01:53, 3199.73it/s]286230it [01:53, 3407.26it/s]283240it [01:53, 3294.85it/s]279317it [01:53, 3291.12it/s]286573it [01:53, 3312.55it/s]283603it [01:53, 3390.53it/s]279658it [01:53, 3192.98it/s]286939it [01:53, 3410.43it/s]287112it [01:53, 2524.37it/s]
283944it [01:53, 3266.03it/s]280012it [01:53, 3290.67it/s]284312it [01:53, 3383.18it/s]280364it [01:53, 3356.45it/s]284653it [01:53, 3365.92it/s]280702it [01:53, 3256.01it/s]284991it [01:53, 3277.93it/s]281045it [01:53, 3305.16it/s]285345it [01:53, 3353.18it/s]281377it [01:54, 3205.18it/s]285682it [01:53, 3272.35it/s]281730it [01:54, 3296.97it/s]286033it [01:54, 3340.62it/s]282084it [01:54, 3366.63it/s]286379it [01:54, 3277.58it/s]282422it [01:54, 3239.43it/s]286735it [01:54, 3358.23it/s]282774it [01:54, 3318.73it/s]287106it [01:54, 3459.80it/s]287112it [01:54, 2511.50it/s]
283108it [01:54, 3203.45it/s]283444it [01:54, 3246.31it/s]283794it [01:54, 3318.91it/s]284128it [01:54, 3227.27it/s]284482it [01:55, 3316.11it/s]284815it [01:55, 3203.75it/s]285172it [01:55, 3307.75it/s]285538it [01:55, 3241.36it/s]285904it [01:55, 3359.52it/s]286259it [01:55, 3412.55it/s]286602it [01:55, 3319.53it/s]286961it [01:55, 3395.62it/s]287112it [01:55, 2478.75it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-08-11 15:06:36 | INFO | train_inner | epoch 008:     11 / 2244 loss=6.36, nll_loss=2.201, mask_ins=0.887, word_ins_ml=3.831, word_reposition=0.754, kpe=0.889, ppl=82.16, wps=1864.9, ups=0.18, wpb=10145.9, bsz=126.8, num_updates=15700, lr=0.000282166, gnorm=3.059, clip=0, loss_scale=1024, train_wall=258, wall=0
2022-08-11 15:11:24 | INFO | train_inner | epoch 008:    111 / 2244 loss=nan, nll_loss=2.163, mask_ins=0.903, word_ins_ml=3.797, word_reposition=0.763, kpe=nan, ppl=nan, wps=3569.6, ups=0.35, wpb=10275, bsz=128, num_updates=15800, lr=0.000281272, gnorm=3.079, clip=0, loss_scale=1024, train_wall=252, wall=0
2022-08-11 15:15:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 15:16:09 | INFO | train_inner | epoch 008:    212 / 2244 loss=6.323, nll_loss=2.187, mask_ins=0.891, word_ins_ml=3.818, word_reposition=0.761, kpe=0.853, ppl=80.07, wps=3619.2, ups=0.35, wpb=10314.6, bsz=128, num_updates=15900, lr=0.000280386, gnorm=3.208, clip=0, loss_scale=918, train_wall=251, wall=0
2022-08-11 15:21:26 | INFO | train_inner | epoch 008:    312 / 2244 loss=6.293, nll_loss=2.172, mask_ins=0.894, word_ins_ml=3.805, word_reposition=0.747, kpe=0.847, ppl=78.4, wps=3241.9, ups=0.32, wpb=10277.6, bsz=128, num_updates=16000, lr=0.000279508, gnorm=2.987, clip=0, loss_scale=512, train_wall=283, wall=0
2022-08-11 15:26:08 | INFO | train_inner | epoch 008:    412 / 2244 loss=6.341, nll_loss=2.212, mask_ins=0.901, word_ins_ml=3.84, word_reposition=0.752, kpe=0.848, ppl=81.08, wps=3652.6, ups=0.35, wpb=10320, bsz=128, num_updates=16100, lr=0.000278639, gnorm=3.012, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 15:30:51 | INFO | train_inner | epoch 008:    512 / 2244 loss=6.295, nll_loss=2.166, mask_ins=0.89, word_ins_ml=3.799, word_reposition=0.753, kpe=0.853, ppl=78.52, wps=3647.1, ups=0.35, wpb=10307.1, bsz=128, num_updates=16200, lr=0.000277778, gnorm=2.927, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 15:35:33 | INFO | train_inner | epoch 008:    612 / 2244 loss=6.351, nll_loss=2.202, mask_ins=0.9, word_ins_ml=3.83, word_reposition=0.761, kpe=0.86, ppl=81.65, wps=3623.4, ups=0.35, wpb=10221.4, bsz=128, num_updates=16300, lr=0.000276924, gnorm=2.951, clip=0, loss_scale=512, train_wall=248, wall=0
2022-08-11 15:40:15 | INFO | train_inner | epoch 008:    712 / 2244 loss=6.379, nll_loss=2.213, mask_ins=0.928, word_ins_ml=3.84, word_reposition=0.754, kpe=0.858, ppl=83.25, wps=3657.3, ups=0.35, wpb=10327.7, bsz=128, num_updates=16400, lr=0.000276079, gnorm=2.982, clip=0, loss_scale=558, train_wall=249, wall=0
2022-08-11 15:44:58 | INFO | train_inner | epoch 008:    812 / 2244 loss=6.339, nll_loss=2.213, mask_ins=0.893, word_ins_ml=3.84, word_reposition=0.751, kpe=0.856, ppl=80.97, wps=3636.4, ups=0.35, wpb=10274.3, bsz=128, num_updates=16500, lr=0.000275241, gnorm=2.961, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-08-11 15:50:17 | INFO | train_inner | epoch 008:    912 / 2244 loss=6.313, nll_loss=2.174, mask_ins=0.905, word_ins_ml=3.805, word_reposition=0.749, kpe=0.854, ppl=79.49, wps=3210.3, ups=0.31, wpb=10240.9, bsz=128, num_updates=16600, lr=0.000274411, gnorm=2.926, clip=0, loss_scale=1024, train_wall=285, wall=0
2022-08-11 15:55:00 | INFO | train_inner | epoch 008:   1012 / 2244 loss=6.346, nll_loss=2.211, mask_ins=0.89, word_ins_ml=3.838, word_reposition=0.755, kpe=0.863, ppl=81.33, wps=3630.6, ups=0.35, wpb=10274.6, bsz=128, num_updates=16700, lr=0.000273588, gnorm=2.957, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-08-11 15:59:43 | INFO | train_inner | epoch 008:   1112 / 2244 loss=6.319, nll_loss=2.162, mask_ins=0.911, word_ins_ml=3.795, word_reposition=0.757, kpe=0.857, ppl=79.82, wps=3619.2, ups=0.35, wpb=10237.5, bsz=128, num_updates=16800, lr=0.000272772, gnorm=2.986, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-08-11 16:04:26 | INFO | train_inner | epoch 008:   1212 / 2244 loss=6.292, nll_loss=2.149, mask_ins=0.895, word_ins_ml=3.783, word_reposition=0.754, kpe=0.862, ppl=78.38, wps=3637.5, ups=0.35, wpb=10300.5, bsz=128, num_updates=16900, lr=0.000271964, gnorm=2.939, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-08-11 16:09:09 | INFO | train_inner | epoch 008:   1312 / 2244 loss=6.349, nll_loss=2.239, mask_ins=0.877, word_ins_ml=3.862, word_reposition=0.752, kpe=0.858, ppl=81.51, wps=3637.9, ups=0.35, wpb=10282.3, bsz=128, num_updates=17000, lr=0.000271163, gnorm=2.849, clip=0, loss_scale=2017, train_wall=249, wall=0
2022-08-11 16:13:51 | INFO | train_inner | epoch 008:   1412 / 2244 loss=6.33, nll_loss=2.182, mask_ins=0.904, word_ins_ml=3.812, word_reposition=0.753, kpe=0.86, ppl=80.45, wps=3625.7, ups=0.35, wpb=10241.2, bsz=128, num_updates=17100, lr=0.000270369, gnorm=2.916, clip=0, loss_scale=2048, train_wall=248, wall=0
2022-08-11 16:19:19 | INFO | train_inner | epoch 008:   1512 / 2244 loss=nan, nll_loss=2.184, mask_ins=0.893, word_ins_ml=3.813, word_reposition=0.748, kpe=nan, ppl=nan, wps=3138.8, ups=0.3, wpb=10302.5, bsz=128, num_updates=17200, lr=0.000269582, gnorm=2.812, clip=0, loss_scale=2048, train_wall=294, wall=0
2022-08-11 16:24:01 | INFO | train_inner | epoch 008:   1612 / 2244 loss=6.307, nll_loss=2.2, mask_ins=0.867, word_ins_ml=3.827, word_reposition=0.749, kpe=0.865, ppl=79.17, wps=3636.7, ups=0.35, wpb=10251.1, bsz=128, num_updates=17300, lr=0.000268802, gnorm=2.841, clip=0, loss_scale=2048, train_wall=248, wall=0
2022-08-11 16:28:44 | INFO | train_inner | epoch 008:   1712 / 2244 loss=6.298, nll_loss=2.188, mask_ins=0.871, word_ins_ml=3.816, word_reposition=0.751, kpe=0.86, ppl=78.69, wps=3631, ups=0.35, wpb=10255.8, bsz=128, num_updates=17400, lr=0.000268028, gnorm=2.902, clip=0, loss_scale=2048, train_wall=248, wall=0
2022-08-11 16:31:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-11 16:33:29 | INFO | train_inner | epoch 008:   1813 / 2244 loss=6.306, nll_loss=2.168, mask_ins=0.9, word_ins_ml=3.798, word_reposition=0.748, kpe=0.86, ppl=79.14, wps=3600.4, ups=0.35, wpb=10273.4, bsz=128, num_updates=17500, lr=0.000267261, gnorm=2.879, clip=0, loss_scale=2879, train_wall=251, wall=0
2022-08-11 16:37:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-11 16:38:14 | INFO | train_inner | epoch 008:   1914 / 2244 loss=6.294, nll_loss=2.175, mask_ins=0.884, word_ins_ml=3.804, word_reposition=0.748, kpe=0.858, ppl=78.48, wps=3603, ups=0.35, wpb=10267.8, bsz=128, num_updates=17600, lr=0.000266501, gnorm=2.887, clip=0, loss_scale=1845, train_wall=251, wall=0
2022-08-11 16:42:56 | INFO | train_inner | epoch 008:   2014 / 2244 loss=6.267, nll_loss=2.173, mask_ins=0.87, word_ins_ml=3.803, word_reposition=0.731, kpe=0.864, ppl=76.99, wps=3614, ups=0.35, wpb=10193.7, bsz=128, num_updates=17700, lr=0.000265747, gnorm=2.775, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-08-11 16:48:15 | INFO | train_inner | epoch 008:   2114 / 2244 loss=6.256, nll_loss=2.139, mask_ins=0.884, word_ins_ml=3.772, word_reposition=0.739, kpe=0.861, ppl=76.43, wps=3212.4, ups=0.31, wpb=10242.4, bsz=128, num_updates=17800, lr=0.000264999, gnorm=2.861, clip=0, loss_scale=1024, train_wall=285, wall=0
2022-08-11 16:52:57 | INFO | train_inner | epoch 008:   2214 / 2244 loss=6.327, nll_loss=2.204, mask_ins=0.884, word_ins_ml=3.829, word_reposition=0.744, kpe=0.87, ppl=80.28, wps=3621.9, ups=0.35, wpb=10217.5, bsz=128, num_updates=17900, lr=0.000264258, gnorm=2.975, clip=0, loss_scale=1024, train_wall=248, wall=0
2022-08-11 16:54:19 | INFO | train | epoch 008 | loss nan | nll_loss 2.186 | mask_ins 0.892 | word_ins_ml 3.815 | word_reposition 0.751 | kpe nan | ppl nan | wps 3407.8 | ups 0.33 | wpb 10264.4 | bsz 127.9 | num_updates 17930 | lr 0.000264037 | gnorm 2.942 | clip 0 | loss_scale 1254 | train_wall 5741 | wall 0
2022-08-11 16:56:24 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 11.521 | nll_loss 5.826 | mask_ins 1.573 | word_ins_ml 7.175 | word_reposition 1.332 | kpe 1.442 | ppl 2938.15 | wps 7927 | wpb 1183.8 | bsz 16 | num_updates 17930 | best_loss 11.292
2022-08-11 16:56:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_last.pt (epoch 8 @ 17930 updates, score 11.521) (writing took 15.294994994997978 seconds)
2022-08-11 16:59:57 | INFO | train_inner | epoch 009:     70 / 2244 loss=6.248, nll_loss=2.153, mask_ins=0.881, word_ins_ml=3.785, word_reposition=0.754, kpe=0.827, ppl=75.99, wps=2431.7, ups=0.24, wpb=10210.4, bsz=126.8, num_updates=18000, lr=0.000263523, gnorm=3.08, clip=0, loss_scale=1024, train_wall=247, wall=0
2022-08-11 17:04:39 | INFO | train_inner | epoch 009:    170 / 2244 loss=nan, nll_loss=2.111, mask_ins=0.877, word_ins_ml=3.747, word_reposition=0.738, kpe=nan, ppl=nan, wps=3627.2, ups=0.35, wpb=10244.7, bsz=128, num_updates=18100, lr=0.000262794, gnorm=2.899, clip=0, loss_scale=1106, train_wall=249, wall=0
2022-08-11 17:06:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-11 17:08:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 17:09:28 | INFO | train_inner | epoch 009:    272 / 2244 loss=6.197, nll_loss=2.121, mask_ins=0.89, word_ins_ml=3.756, word_reposition=0.745, kpe=0.807, ppl=73.38, wps=3578.6, ups=0.35, wpb=10315.1, bsz=128, num_updates=18200, lr=0.000262071, gnorm=3.017, clip=0, loss_scale=1300, train_wall=254, wall=0
2022-08-11 17:14:10 | INFO | train_inner | epoch 009:    372 / 2244 loss=6.182, nll_loss=2.13, mask_ins=0.868, word_ins_ml=3.764, word_reposition=0.738, kpe=0.812, ppl=72.62, wps=3640.7, ups=0.35, wpb=10290.7, bsz=128, num_updates=18300, lr=0.000261354, gnorm=2.863, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 17:19:28 | INFO | train_inner | epoch 009:    472 / 2244 loss=6.175, nll_loss=2.117, mask_ins=0.878, word_ins_ml=3.753, word_reposition=0.734, kpe=0.81, ppl=72.27, wps=3220, ups=0.31, wpb=10247.4, bsz=128, num_updates=18400, lr=0.000260643, gnorm=2.996, clip=0, loss_scale=512, train_wall=284, wall=0
2022-08-11 17:24:11 | INFO | train_inner | epoch 009:    572 / 2244 loss=6.186, nll_loss=2.127, mask_ins=0.875, word_ins_ml=3.761, word_reposition=0.736, kpe=0.813, ppl=72.8, wps=3625.7, ups=0.35, wpb=10239.2, bsz=128, num_updates=18500, lr=0.000259938, gnorm=2.945, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 17:28:52 | INFO | train_inner | epoch 009:    672 / 2244 loss=6.166, nll_loss=2.1, mask_ins=0.879, word_ins_ml=3.737, word_reposition=0.737, kpe=0.814, ppl=71.81, wps=3666.8, ups=0.36, wpb=10326.9, bsz=128, num_updates=18600, lr=0.000259238, gnorm=2.932, clip=0, loss_scale=512, train_wall=248, wall=0
2022-08-11 17:33:34 | INFO | train_inner | epoch 009:    772 / 2244 loss=6.19, nll_loss=2.13, mask_ins=0.871, word_ins_ml=3.764, word_reposition=0.741, kpe=0.814, ppl=73, wps=3652.1, ups=0.35, wpb=10293.5, bsz=128, num_updates=18700, lr=0.000258544, gnorm=2.903, clip=0, loss_scale=538, train_wall=248, wall=0
2022-08-11 17:37:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 17:38:19 | INFO | train_inner | epoch 009:    873 / 2244 loss=6.153, nll_loss=2.082, mask_ins=0.879, word_ins_ml=3.721, word_reposition=0.739, kpe=0.814, ppl=71.17, wps=3602.6, ups=0.35, wpb=10248.2, bsz=128, num_updates=18800, lr=0.000257855, gnorm=2.905, clip=0, loss_scale=882, train_wall=250, wall=0
2022-08-11 17:43:01 | INFO | train_inner | epoch 009:    973 / 2244 loss=6.226, nll_loss=2.169, mask_ins=0.868, word_ins_ml=3.797, word_reposition=0.74, kpe=0.821, ppl=74.84, wps=3620.8, ups=0.35, wpb=10225.9, bsz=128, num_updates=18900, lr=0.000257172, gnorm=2.971, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 17:48:33 | INFO | train_inner | epoch 009:   1073 / 2244 loss=6.176, nll_loss=2.103, mask_ins=0.879, word_ins_ml=3.739, word_reposition=0.735, kpe=0.823, ppl=72.32, wps=3108.5, ups=0.3, wpb=10312.4, bsz=128, num_updates=19000, lr=0.000256495, gnorm=2.946, clip=0, loss_scale=512, train_wall=298, wall=0
2022-08-11 17:53:16 | INFO | train_inner | epoch 009:   1173 / 2244 loss=6.23, nll_loss=2.165, mask_ins=0.876, word_ins_ml=3.793, word_reposition=0.741, kpe=0.82, ppl=75.06, wps=3624.9, ups=0.35, wpb=10253, bsz=128, num_updates=19100, lr=0.000255822, gnorm=2.961, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 17:57:58 | INFO | train_inner | epoch 009:   1273 / 2244 loss=6.128, nll_loss=2.061, mask_ins=0.869, word_ins_ml=3.702, word_reposition=0.737, kpe=0.82, ppl=69.95, wps=3660.9, ups=0.35, wpb=10341.9, bsz=128, num_updates=19200, lr=0.000255155, gnorm=2.886, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 18:02:40 | INFO | train_inner | epoch 009:   1373 / 2244 loss=6.135, nll_loss=2.079, mask_ins=0.857, word_ins_ml=3.717, word_reposition=0.741, kpe=0.82, ppl=70.28, wps=3619.5, ups=0.35, wpb=10211.8, bsz=128, num_updates=19300, lr=0.000254493, gnorm=2.971, clip=0, loss_scale=594, train_wall=248, wall=0
2022-08-11 18:07:24 | INFO | train_inner | epoch 009:   1473 / 2244 loss=6.163, nll_loss=2.096, mask_ins=0.869, word_ins_ml=3.732, word_reposition=0.74, kpe=0.822, ppl=71.65, wps=3632, ups=0.35, wpb=10307.8, bsz=128, num_updates=19400, lr=0.000253837, gnorm=2.891, clip=0, loss_scale=1024, train_wall=250, wall=0
2022-08-11 18:12:07 | INFO | train_inner | epoch 009:   1573 / 2244 loss=6.174, nll_loss=2.106, mask_ins=0.873, word_ins_ml=3.741, word_reposition=0.74, kpe=0.821, ppl=72.23, wps=3621, ups=0.35, wpb=10253.8, bsz=128, num_updates=19500, lr=0.000253185, gnorm=2.91, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-08-11 18:17:40 | INFO | train_inner | epoch 009:   1673 / 2244 loss=nan, nll_loss=2.069, mask_ins=0.873, word_ins_ml=3.709, word_reposition=0.737, kpe=nan, ppl=nan, wps=3065, ups=0.3, wpb=10184.1, bsz=128, num_updates=19600, lr=0.000252538, gnorm=2.947, clip=0, loss_scale=1024, train_wall=298, wall=0
2022-08-11 18:22:23 | INFO | train_inner | epoch 009:   1773 / 2244 loss=6.216, nll_loss=2.166, mask_ins=0.86, word_ins_ml=3.794, word_reposition=0.736, kpe=0.826, ppl=74.33, wps=3610.2, ups=0.35, wpb=10228.6, bsz=128, num_updates=19700, lr=0.000251896, gnorm=2.904, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-08-11 18:27:06 | INFO | train_inner | epoch 009:   1873 / 2244 loss=6.19, nll_loss=2.127, mask_ins=0.873, word_ins_ml=3.759, word_reposition=0.729, kpe=0.83, ppl=73.03, wps=3634.4, ups=0.35, wpb=10268.1, bsz=128, num_updates=19800, lr=0.000251259, gnorm=2.842, clip=0, loss_scale=1065, train_wall=249, wall=0
2022-08-11 18:31:48 | INFO | train_inner | epoch 009:   1973 / 2244 loss=6.185, nll_loss=2.142, mask_ins=0.857, word_ins_ml=3.772, word_reposition=0.731, kpe=0.826, ppl=72.78, wps=3654.5, ups=0.35, wpb=10318.2, bsz=128, num_updates=19900, lr=0.000250627, gnorm=2.928, clip=0, loss_scale=2048, train_wall=249, wall=0
2022-08-11 18:36:31 | INFO | train_inner | epoch 009:   2073 / 2244 loss=6.175, nll_loss=2.123, mask_ins=0.856, word_ins_ml=3.755, word_reposition=0.736, kpe=0.829, ppl=72.25, wps=3610.8, ups=0.35, wpb=10216.4, bsz=128, num_updates=20000, lr=0.00025, gnorm=2.942, clip=0, loss_scale=2048, train_wall=249, wall=0
2022-08-11 18:41:13 | INFO | train_inner | epoch 009:   2173 / 2244 loss=6.167, nll_loss=2.112, mask_ins=0.861, word_ins_ml=3.745, word_reposition=0.731, kpe=0.831, ppl=71.86, wps=3627, ups=0.35, wpb=10248.6, bsz=128, num_updates=20100, lr=0.000249377, gnorm=2.9, clip=0, loss_scale=2048, train_wall=249, wall=0
2022-08-11 18:44:33 | INFO | train | epoch 009 | loss nan | nll_loss 2.116 | mask_ins 0.872 | word_ins_ml 3.75 | word_reposition 0.738 | kpe nan | ppl nan | wps 3478.1 | ups 0.34 | wpb 10263.7 | bsz 127.9 | num_updates 20171 | lr 0.000248938 | gnorm 2.929 | clip 0 | loss_scale 982 | train_wall 5714 | wall 0
2022-08-11 18:46:38 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 11.201 | nll_loss 5.697 | mask_ins 1.525 | word_ins_ml 7.043 | word_reposition 1.193 | kpe 1.44 | ppl 2354.77 | wps 7909.3 | wpb 1183.8 | bsz 16 | num_updates 20171 | best_loss 11.201
2022-08-11 18:46:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_best.pt (epoch 9 @ 20171 updates, score 11.201) (writing took 12.177387088537216 seconds)
2022-08-11 18:48:35 | INFO | train_inner | epoch 010:     29 / 2244 loss=6.159, nll_loss=2.081, mask_ins=0.887, word_ins_ml=3.718, word_reposition=0.738, kpe=0.816, ppl=71.45, wps=2304.1, ups=0.23, wpb=10184.4, bsz=126.8, num_updates=20200, lr=0.000248759, gnorm=2.955, clip=0, loss_scale=2048, train_wall=271, wall=0
2022-08-11 18:52:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-11 18:53:34 | INFO | train_inner | epoch 010:    130 / 2244 loss=6.049, nll_loss=2.046, mask_ins=0.866, word_ins_ml=3.688, word_reposition=0.73, kpe=0.765, ppl=66.23, wps=3432.3, ups=0.34, wpb=10238.6, bsz=128, num_updates=20300, lr=0.000248146, gnorm=2.926, clip=0, loss_scale=1906, train_wall=264, wall=0
2022-08-11 18:58:16 | INFO | train_inner | epoch 010:    230 / 2244 loss=5.987, nll_loss=1.992, mask_ins=0.852, word_ins_ml=3.64, word_reposition=0.728, kpe=0.767, ppl=63.41, wps=3633, ups=0.35, wpb=10258.6, bsz=128, num_updates=20400, lr=0.000247537, gnorm=3.097, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-08-11 19:02:59 | INFO | train_inner | epoch 010:    330 / 2244 loss=6.031, nll_loss=2.04, mask_ins=0.861, word_ins_ml=3.681, word_reposition=0.725, kpe=0.763, ppl=65.37, wps=3633.1, ups=0.35, wpb=10271.2, bsz=128, num_updates=20500, lr=0.000246932, gnorm=2.853, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-08-11 19:07:42 | INFO | train_inner | epoch 010:    430 / 2244 loss=6.02, nll_loss=2.046, mask_ins=0.85, word_ins_ml=3.687, word_reposition=0.721, kpe=0.762, ppl=64.89, wps=3633.1, ups=0.35, wpb=10282.6, bsz=128, num_updates=20600, lr=0.000246332, gnorm=2.932, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-08-11 19:12:24 | INFO | train_inner | epoch 010:    530 / 2244 loss=6.059, nll_loss=2.076, mask_ins=0.852, word_ins_ml=3.713, word_reposition=0.723, kpe=0.771, ppl=66.66, wps=3647.7, ups=0.35, wpb=10286.9, bsz=128, num_updates=20700, lr=0.000245737, gnorm=3.003, clip=0, loss_scale=1024, train_wall=248, wall=0
2022-08-11 19:17:07 | INFO | train_inner | epoch 010:    630 / 2244 loss=6.034, nll_loss=2.03, mask_ins=0.86, word_ins_ml=3.672, word_reposition=0.735, kpe=0.767, ppl=65.54, wps=3644.2, ups=0.35, wpb=10304.8, bsz=128, num_updates=20800, lr=0.000245145, gnorm=3.016, clip=0, loss_scale=1044, train_wall=249, wall=0
2022-08-11 19:22:37 | INFO | train_inner | epoch 010:    730 / 2244 loss=6.095, nll_loss=2.105, mask_ins=0.853, word_ins_ml=3.739, word_reposition=0.726, kpe=0.777, ppl=68.35, wps=3106.2, ups=0.3, wpb=10257.3, bsz=128, num_updates=20900, lr=0.000244558, gnorm=3.033, clip=0, loss_scale=2048, train_wall=296, wall=0
2022-08-11 19:24:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-11 19:27:21 | INFO | train_inner | epoch 010:    831 / 2244 loss=6.041, nll_loss=2.049, mask_ins=0.853, word_ins_ml=3.688, word_reposition=0.72, kpe=0.78, ppl=65.84, wps=3593.2, ups=0.35, wpb=10225, bsz=128, num_updates=21000, lr=0.000243975, gnorm=3, clip=0, loss_scale=1470, train_wall=250, wall=0
2022-08-11 19:28:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 19:32:06 | INFO | train_inner | epoch 010:    932 / 2244 loss=6.062, nll_loss=2.05, mask_ins=0.862, word_ins_ml=3.69, word_reposition=0.724, kpe=0.785, ppl=66.81, wps=3608.2, ups=0.35, wpb=10282.6, bsz=128, num_updates=21100, lr=0.000243396, gnorm=3.065, clip=0, loss_scale=649, train_wall=251, wall=0
2022-08-11 19:36:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 19:36:51 | INFO | train_inner | epoch 010:   1033 / 2244 loss=6.026, nll_loss=2.031, mask_ins=0.851, word_ins_ml=3.673, word_reposition=0.724, kpe=0.777, ppl=65.15, wps=3584.9, ups=0.35, wpb=10209.8, bsz=128, num_updates=21200, lr=0.000242821, gnorm=3.025, clip=0, loss_scale=497, train_wall=251, wall=0
2022-08-11 19:41:34 | INFO | train_inner | epoch 010:   1133 / 2244 loss=6.063, nll_loss=2.06, mask_ins=0.861, word_ins_ml=3.698, word_reposition=0.722, kpe=0.782, ppl=66.87, wps=3603.3, ups=0.35, wpb=10196.8, bsz=128, num_updates=21300, lr=0.000242251, gnorm=3.002, clip=0, loss_scale=256, train_wall=249, wall=0
2022-08-11 19:46:17 | INFO | train_inner | epoch 010:   1233 / 2244 loss=6.029, nll_loss=2.042, mask_ins=0.84, word_ins_ml=3.683, word_reposition=0.725, kpe=0.781, ppl=65.29, wps=3648.9, ups=0.35, wpb=10317.3, bsz=128, num_updates=21400, lr=0.000241684, gnorm=3.183, clip=0, loss_scale=256, train_wall=249, wall=0
2022-08-11 19:51:44 | INFO | train_inner | epoch 010:   1333 / 2244 loss=6.058, nll_loss=2.076, mask_ins=0.842, word_ins_ml=3.712, word_reposition=0.721, kpe=0.783, ppl=66.63, wps=3133.8, ups=0.31, wpb=10237.3, bsz=128, num_updates=21500, lr=0.000241121, gnorm=2.991, clip=0, loss_scale=256, train_wall=293, wall=0
2022-08-11 19:56:26 | INFO | train_inner | epoch 010:   1433 / 2244 loss=6.087, nll_loss=2.09, mask_ins=0.848, word_ins_ml=3.724, word_reposition=0.731, kpe=0.784, ppl=67.98, wps=3668.8, ups=0.35, wpb=10363.7, bsz=128, num_updates=21600, lr=0.000240563, gnorm=3.228, clip=0, loss_scale=256, train_wall=249, wall=0
2022-08-11 20:01:08 | INFO | train_inner | epoch 010:   1533 / 2244 loss=6.019, nll_loss=2.031, mask_ins=0.837, word_ins_ml=3.672, word_reposition=0.728, kpe=0.782, ppl=64.84, wps=3659.3, ups=0.36, wpb=10301.2, bsz=128, num_updates=21700, lr=0.000240008, gnorm=2.961, clip=0, loss_scale=256, train_wall=248, wall=0
2022-08-11 20:05:50 | INFO | train_inner | epoch 010:   1633 / 2244 loss=5.992, nll_loss=2.018, mask_ins=0.829, word_ins_ml=3.66, word_reposition=0.721, kpe=0.782, ppl=63.63, wps=3646.4, ups=0.35, wpb=10297.1, bsz=128, num_updates=21800, lr=0.000239457, gnorm=2.872, clip=0, loss_scale=497, train_wall=248, wall=0
2022-08-11 20:10:33 | INFO | train_inner | epoch 010:   1733 / 2244 loss=nan, nll_loss=2.027, mask_ins=0.858, word_ins_ml=3.669, word_reposition=0.725, kpe=nan, ppl=nan, wps=3640.4, ups=0.35, wpb=10298.7, bsz=128, num_updates=21900, lr=0.000238909, gnorm=2.889, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 20:15:15 | INFO | train_inner | epoch 010:   1833 / 2244 loss=6.026, nll_loss=2.037, mask_ins=0.846, word_ins_ml=3.677, word_reposition=0.719, kpe=0.783, ppl=65.15, wps=3644.9, ups=0.35, wpb=10296.6, bsz=128, num_updates=22000, lr=0.000238366, gnorm=3.05, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 20:18:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 20:20:49 | INFO | train_inner | epoch 010:   1934 / 2244 loss=6.062, nll_loss=2.06, mask_ins=0.854, word_ins_ml=3.698, word_reposition=0.725, kpe=0.785, ppl=66.8, wps=3068.8, ups=0.3, wpb=10239.5, bsz=128, num_updates=22100, lr=0.000237826, gnorm=2.957, clip=0, loss_scale=400, train_wall=299, wall=0
2022-08-11 20:25:32 | INFO | train_inner | epoch 010:   2034 / 2244 loss=6.039, nll_loss=2.043, mask_ins=0.848, word_ins_ml=3.682, word_reposition=0.722, kpe=0.787, ppl=65.73, wps=3628.4, ups=0.35, wpb=10249.4, bsz=128, num_updates=22200, lr=0.000237289, gnorm=2.884, clip=0, loss_scale=256, train_wall=249, wall=0
2022-08-11 20:30:13 | INFO | train_inner | epoch 010:   2134 / 2244 loss=6.057, nll_loss=2.062, mask_ins=0.842, word_ins_ml=3.699, word_reposition=0.728, kpe=0.788, ppl=66.56, wps=3635.3, ups=0.35, wpb=10243.1, bsz=128, num_updates=22300, lr=0.000236757, gnorm=2.93, clip=0, loss_scale=256, train_wall=248, wall=0
2022-08-11 20:34:55 | INFO | train_inner | epoch 010:   2234 / 2244 loss=6.052, nll_loss=2.024, mask_ins=0.876, word_ins_ml=3.665, word_reposition=0.725, kpe=0.786, ppl=66.37, wps=3639.5, ups=0.35, wpb=10263.2, bsz=128, num_updates=22400, lr=0.000236228, gnorm=2.883, clip=0, loss_scale=256, train_wall=248, wall=0
2022-08-11 20:35:21 | INFO | train | epoch 010 | loss nan | nll_loss 2.048 | mask_ins 0.852 | word_ins_ml 3.687 | word_reposition 0.725 | kpe nan | ppl nan | wps 3456.5 | ups 0.34 | wpb 10264.4 | bsz 127.9 | num_updates 22410 | lr 0.000236175 | gnorm 2.992 | clip 0 | loss_scale 729 | train_wall 5753 | wall 0
2022-08-11 20:37:26 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 11.093 | nll_loss 5.539 | mask_ins 1.528 | word_ins_ml 6.901 | word_reposition 1.183 | kpe 1.481 | ppl 2184.92 | wps 7946.8 | wpb 1183.8 | bsz 16 | num_updates 22410 | best_loss 11.093
2022-08-11 20:37:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_best.pt (epoch 10 @ 22410 updates, score 11.093) (writing took 23.885159522294998 seconds)
2022-08-11 20:42:04 | INFO | train_inner | epoch 011:     90 / 2244 loss=5.969, nll_loss=2.04, mask_ins=0.839, word_ins_ml=3.68, word_reposition=0.726, kpe=0.724, ppl=62.63, wps=2372.5, ups=0.23, wpb=10157.5, bsz=126.8, num_updates=22500, lr=0.000235702, gnorm=3.068, clip=0, loss_scale=256, train_wall=246, wall=0
2022-08-11 20:46:46 | INFO | train_inner | epoch 011:    190 / 2244 loss=5.95, nll_loss=2.022, mask_ins=0.845, word_ins_ml=3.664, word_reposition=0.724, kpe=0.718, ppl=61.82, wps=3662.7, ups=0.35, wpb=10353.5, bsz=128, num_updates=22600, lr=0.00023518, gnorm=3.119, clip=0, loss_scale=338, train_wall=249, wall=0
2022-08-11 20:52:15 | INFO | train_inner | epoch 011:    290 / 2244 loss=5.958, nll_loss=2.043, mask_ins=0.835, word_ins_ml=3.682, word_reposition=0.718, kpe=0.723, ppl=62.17, wps=3103.6, ups=0.3, wpb=10206.6, bsz=128, num_updates=22700, lr=0.000234662, gnorm=2.934, clip=0, loss_scale=512, train_wall=295, wall=0
2022-08-11 20:55:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 20:57:00 | INFO | train_inner | epoch 011:    391 / 2244 loss=5.954, nll_loss=2.015, mask_ins=0.85, word_ins_ml=3.657, word_reposition=0.713, kpe=0.733, ppl=61.98, wps=3594.1, ups=0.35, wpb=10248.8, bsz=128, num_updates=22800, lr=0.000234146, gnorm=3.406, clip=0, loss_scale=408, train_wall=251, wall=0
2022-08-11 21:01:42 | INFO | train_inner | epoch 011:    491 / 2244 loss=nan, nll_loss=2.066, mask_ins=0.838, word_ins_ml=3.702, word_reposition=0.727, kpe=nan, ppl=nan, wps=3645.4, ups=0.35, wpb=10268.7, bsz=128, num_updates=22900, lr=0.000233635, gnorm=3.145, clip=0, loss_scale=256, train_wall=248, wall=0
2022-08-11 21:06:24 | INFO | train_inner | epoch 011:    591 / 2244 loss=5.857, nll_loss=1.949, mask_ins=0.823, word_ins_ml=3.599, word_reposition=0.712, kpe=0.723, ppl=57.95, wps=3632.9, ups=0.35, wpb=10244.7, bsz=128, num_updates=23000, lr=0.000233126, gnorm=3.02, clip=0, loss_scale=256, train_wall=248, wall=0
2022-08-11 21:11:06 | INFO | train_inner | epoch 011:    691 / 2244 loss=5.954, nll_loss=2.013, mask_ins=0.847, word_ins_ml=3.655, word_reposition=0.722, kpe=0.729, ppl=61.98, wps=3635.1, ups=0.35, wpb=10251.3, bsz=128, num_updates=23100, lr=0.000232621, gnorm=2.887, clip=0, loss_scale=256, train_wall=248, wall=0
2022-08-11 21:15:48 | INFO | train_inner | epoch 011:    791 / 2244 loss=5.861, nll_loss=1.933, mask_ins=0.835, word_ins_ml=3.585, word_reposition=0.714, kpe=0.728, ppl=58.13, wps=3637.3, ups=0.35, wpb=10261.9, bsz=128, num_updates=23200, lr=0.000232119, gnorm=2.944, clip=0, loss_scale=256, train_wall=248, wall=0
2022-08-11 21:21:16 | INFO | train_inner | epoch 011:    891 / 2244 loss=5.962, nll_loss=2.013, mask_ins=0.844, word_ins_ml=3.655, word_reposition=0.723, kpe=0.74, ppl=62.34, wps=3129.2, ups=0.3, wpb=10260, bsz=128, num_updates=23300, lr=0.000231621, gnorm=3.523, clip=1, loss_scale=330, train_wall=294, wall=0
2022-08-11 21:25:59 | INFO | train_inner | epoch 011:    991 / 2244 loss=5.924, nll_loss=2.001, mask_ins=0.831, word_ins_ml=3.644, word_reposition=0.718, kpe=0.731, ppl=60.7, wps=3629.7, ups=0.35, wpb=10278.3, bsz=128, num_updates=23400, lr=0.000231125, gnorm=3.114, clip=0, loss_scale=512, train_wall=250, wall=0
2022-08-11 21:30:42 | INFO | train_inner | epoch 011:   1091 / 2244 loss=5.93, nll_loss=1.992, mask_ins=0.834, word_ins_ml=3.636, word_reposition=0.725, kpe=0.734, ppl=60.97, wps=3626.9, ups=0.35, wpb=10245.8, bsz=128, num_updates=23500, lr=0.000230633, gnorm=3.197, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 21:35:23 | INFO | train_inner | epoch 011:   1191 / 2244 loss=5.912, nll_loss=1.98, mask_ins=0.832, word_ins_ml=3.626, word_reposition=0.716, kpe=0.738, ppl=60.19, wps=3644.7, ups=0.36, wpb=10263.6, bsz=128, num_updates=23600, lr=0.000230144, gnorm=3.045, clip=0, loss_scale=512, train_wall=248, wall=0
2022-08-11 21:40:06 | INFO | train_inner | epoch 011:   1291 / 2244 loss=5.977, nll_loss=2.033, mask_ins=0.846, word_ins_ml=3.672, word_reposition=0.719, kpe=0.739, ppl=62.98, wps=3636.3, ups=0.35, wpb=10284.8, bsz=128, num_updates=23700, lr=0.000229658, gnorm=2.959, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 21:44:48 | INFO | train_inner | epoch 011:   1391 / 2244 loss=5.934, nll_loss=1.999, mask_ins=0.836, word_ins_ml=3.642, word_reposition=0.719, kpe=0.738, ppl=61.14, wps=3642.6, ups=0.36, wpb=10251.6, bsz=128, num_updates=23800, lr=0.000229175, gnorm=2.982, clip=0, loss_scale=599, train_wall=248, wall=0
2022-08-11 21:50:16 | INFO | train_inner | epoch 011:   1491 / 2244 loss=5.964, nll_loss=2.024, mask_ins=0.837, word_ins_ml=3.664, word_reposition=0.722, kpe=0.741, ppl=62.4, wps=3132.9, ups=0.3, wpb=10289.9, bsz=128, num_updates=23900, lr=0.000228695, gnorm=2.963, clip=0, loss_scale=1024, train_wall=295, wall=0
2022-08-11 21:55:00 | INFO | train_inner | epoch 011:   1591 / 2244 loss=5.958, nll_loss=2.02, mask_ins=0.831, word_ins_ml=3.66, word_reposition=0.727, kpe=0.739, ppl=62.14, wps=3616.3, ups=0.35, wpb=10256.9, bsz=128, num_updates=24000, lr=0.000228218, gnorm=2.924, clip=0, loss_scale=1024, train_wall=250, wall=0
2022-08-11 21:59:42 | INFO | train_inner | epoch 011:   1691 / 2244 loss=5.987, nll_loss=2.051, mask_ins=0.826, word_ins_ml=3.688, word_reposition=0.732, kpe=0.742, ppl=63.44, wps=3633.9, ups=0.35, wpb=10257.4, bsz=128, num_updates=24100, lr=0.000227744, gnorm=3.059, clip=0, loss_scale=1024, train_wall=248, wall=0
2022-08-11 22:04:24 | INFO | train_inner | epoch 011:   1791 / 2244 loss=nan, nll_loss=1.975, mask_ins=0.831, word_ins_ml=3.621, word_reposition=0.717, kpe=nan, ppl=nan, wps=3666, ups=0.35, wpb=10336.7, bsz=128, num_updates=24200, lr=0.000227273, gnorm=2.942, clip=0, loss_scale=1024, train_wall=248, wall=0
2022-08-11 22:05:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 22:09:09 | INFO | train_inner | epoch 011:   1892 / 2244 loss=5.898, nll_loss=1.976, mask_ins=0.814, word_ins_ml=3.621, word_reposition=0.712, kpe=0.75, ppl=59.61, wps=3592.4, ups=0.35, wpb=10258, bsz=128, num_updates=24300, lr=0.000226805, gnorm=3.009, clip=0, loss_scale=573, train_wall=252, wall=0
2022-08-11 22:13:51 | INFO | train_inner | epoch 011:   1992 / 2244 loss=5.903, nll_loss=1.981, mask_ins=0.824, word_ins_ml=3.625, word_reposition=0.708, kpe=0.746, ppl=59.83, wps=3641.9, ups=0.35, wpb=10265.5, bsz=128, num_updates=24400, lr=0.000226339, gnorm=2.998, clip=0, loss_scale=512, train_wall=248, wall=0
2022-08-11 22:19:07 | INFO | train_inner | epoch 011:   2092 / 2244 loss=5.892, nll_loss=1.953, mask_ins=0.825, word_ins_ml=3.6, word_reposition=0.719, kpe=0.747, ppl=59.37, wps=3275.8, ups=0.32, wpb=10343.8, bsz=128, num_updates=24500, lr=0.000225877, gnorm=2.948, clip=0, loss_scale=512, train_wall=282, wall=0
2022-08-11 22:24:03 | INFO | train_inner | epoch 011:   2192 / 2244 loss=5.93, nll_loss=1.98, mask_ins=0.839, word_ins_ml=3.625, word_reposition=0.717, kpe=0.749, ppl=60.96, wps=3465.9, ups=0.34, wpb=10257.3, bsz=128, num_updates=24600, lr=0.000225417, gnorm=2.923, clip=0, loss_scale=512, train_wall=262, wall=0
2022-08-11 22:26:28 | INFO | train | epoch 011 | loss nan | nll_loss 2.002 | mask_ins 0.834 | word_ins_ml 3.645 | word_reposition 0.72 | kpe nan | ppl nan | wps 3451.9 | ups 0.34 | wpb 10263.6 | bsz 127.9 | num_updates 24652 | lr 0.00022518 | gnorm 3.057 | clip 0 | loss_scale 533 | train_wall 5759 | wall 0
2022-08-11 22:28:33 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 11.216 | nll_loss 5.698 | mask_ins 1.515 | word_ins_ml 7.049 | word_reposition 1.193 | kpe 1.459 | ppl 2379.35 | wps 7911.5 | wpb 1183.8 | bsz 16 | num_updates 24652 | best_loss 11.093
2022-08-11 22:28:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_last.pt (epoch 11 @ 24652 updates, score 11.216) (writing took 15.299604780972004 seconds)
2022-08-11 22:31:04 | INFO | train_inner | epoch 012:     48 / 2244 loss=5.85, nll_loss=1.962, mask_ins=0.812, word_ins_ml=3.608, word_reposition=0.712, kpe=0.717, ppl=57.67, wps=2408, ups=0.24, wpb=10127.5, bsz=126.8, num_updates=24700, lr=0.000224961, gnorm=3.205, clip=0, loss_scale=512, train_wall=247, wall=0
2022-08-11 22:35:46 | INFO | train_inner | epoch 012:    148 / 2244 loss=5.748, nll_loss=1.918, mask_ins=0.804, word_ins_ml=3.57, word_reposition=0.705, kpe=0.669, ppl=53.75, wps=3632.1, ups=0.35, wpb=10254.4, bsz=128, num_updates=24800, lr=0.000224507, gnorm=2.936, clip=0, loss_scale=906, train_wall=249, wall=0
2022-08-11 22:40:28 | INFO | train_inner | epoch 012:    248 / 2244 loss=5.796, nll_loss=1.935, mask_ins=0.826, word_ins_ml=3.584, word_reposition=0.706, kpe=0.681, ppl=55.57, wps=3624.7, ups=0.35, wpb=10230.3, bsz=128, num_updates=24900, lr=0.000224055, gnorm=3.003, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-08-11 22:45:09 | INFO | train_inner | epoch 012:    348 / 2244 loss=5.807, nll_loss=1.945, mask_ins=0.821, word_ins_ml=3.593, word_reposition=0.713, kpe=0.681, ppl=55.98, wps=3653.7, ups=0.36, wpb=10274.6, bsz=128, num_updates=25000, lr=0.000223607, gnorm=3.097, clip=0, loss_scale=1024, train_wall=248, wall=0
2022-08-11 22:49:51 | INFO | train_inner | epoch 012:    448 / 2244 loss=5.805, nll_loss=1.942, mask_ins=0.82, word_ins_ml=3.59, word_reposition=0.711, kpe=0.684, ppl=55.9, wps=3658.7, ups=0.35, wpb=10307.7, bsz=128, num_updates=25100, lr=0.000223161, gnorm=2.972, clip=0, loss_scale=1024, train_wall=248, wall=0
2022-08-11 22:55:19 | INFO | train_inner | epoch 012:    548 / 2244 loss=5.866, nll_loss=1.981, mask_ins=0.836, word_ins_ml=3.625, word_reposition=0.721, kpe=0.684, ppl=58.32, wps=3134.9, ups=0.3, wpb=10283.5, bsz=128, num_updates=25200, lr=0.000222718, gnorm=3.152, clip=0, loss_scale=1024, train_wall=294, wall=0
2022-08-11 23:00:02 | INFO | train_inner | epoch 012:    648 / 2244 loss=5.747, nll_loss=1.911, mask_ins=0.804, word_ins_ml=3.563, word_reposition=0.695, kpe=0.685, ppl=53.7, wps=3613.8, ups=0.35, wpb=10208, bsz=128, num_updates=25300, lr=0.000222277, gnorm=3.083, clip=0, loss_scale=1690, train_wall=249, wall=0
2022-08-11 23:04:43 | INFO | train_inner | epoch 012:    748 / 2244 loss=5.84, nll_loss=1.951, mask_ins=0.836, word_ins_ml=3.598, word_reposition=0.715, kpe=0.69, ppl=57.28, wps=3675.8, ups=0.35, wpb=10357.1, bsz=128, num_updates=25400, lr=0.000221839, gnorm=2.965, clip=0, loss_scale=2048, train_wall=248, wall=0
2022-08-11 23:09:26 | INFO | train_inner | epoch 012:    848 / 2244 loss=5.827, nll_loss=1.968, mask_ins=0.812, word_ins_ml=3.613, word_reposition=0.712, kpe=0.69, ppl=56.76, wps=3623.4, ups=0.35, wpb=10240, bsz=128, num_updates=25500, lr=0.000221404, gnorm=2.959, clip=0, loss_scale=2048, train_wall=249, wall=0
2022-08-11 23:14:08 | INFO | train_inner | epoch 012:    948 / 2244 loss=5.781, nll_loss=1.916, mask_ins=0.823, word_ins_ml=3.567, word_reposition=0.704, kpe=0.687, ppl=54.99, wps=3631.6, ups=0.35, wpb=10229.9, bsz=128, num_updates=25600, lr=0.000220971, gnorm=3.049, clip=0, loss_scale=2048, train_wall=248, wall=0
2022-08-11 23:18:50 | INFO | train_inner | epoch 012:   1048 / 2244 loss=5.835, nll_loss=1.964, mask_ins=0.822, word_ins_ml=3.609, word_reposition=0.707, kpe=0.697, ppl=57.08, wps=3629.1, ups=0.35, wpb=10251.4, bsz=128, num_updates=25700, lr=0.000220541, gnorm=3.009, clip=0, loss_scale=2048, train_wall=249, wall=0
2022-08-11 23:22:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-11 23:24:25 | INFO | train_inner | epoch 012:   1149 / 2244 loss=5.784, nll_loss=1.913, mask_ins=0.819, word_ins_ml=3.564, word_reposition=0.707, kpe=0.694, ppl=55.12, wps=3072.9, ups=0.3, wpb=10274.4, bsz=128, num_updates=25800, lr=0.000220113, gnorm=2.998, clip=0, loss_scale=2474, train_wall=300, wall=0
2022-08-11 23:25:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-11 23:29:10 | INFO | train_inner | epoch 012:   1250 / 2244 loss=5.846, nll_loss=1.974, mask_ins=0.814, word_ins_ml=3.618, word_reposition=0.716, kpe=0.698, ppl=57.51, wps=3573.6, ups=0.35, wpb=10205.6, bsz=128, num_updates=25900, lr=0.000219687, gnorm=3.091, clip=0, loss_scale=1196, train_wall=251, wall=0
2022-08-11 23:33:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 23:33:55 | INFO | train_inner | epoch 012:   1351 / 2244 loss=5.77, nll_loss=1.915, mask_ins=0.796, word_ins_ml=3.566, word_reposition=0.709, kpe=0.699, ppl=54.58, wps=3629, ups=0.35, wpb=10338, bsz=128, num_updates=26000, lr=0.000219265, gnorm=2.942, clip=0, loss_scale=989, train_wall=251, wall=0
2022-08-11 23:38:37 | INFO | train_inner | epoch 012:   1451 / 2244 loss=5.84, nll_loss=1.952, mask_ins=0.822, word_ins_ml=3.599, word_reposition=0.719, kpe=0.7, ppl=57.28, wps=3631.6, ups=0.35, wpb=10258.6, bsz=128, num_updates=26100, lr=0.000218844, gnorm=2.991, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 23:43:19 | INFO | train_inner | epoch 012:   1551 / 2244 loss=5.835, nll_loss=1.96, mask_ins=0.814, word_ins_ml=3.605, word_reposition=0.714, kpe=0.702, ppl=57.07, wps=3637.4, ups=0.35, wpb=10254.1, bsz=128, num_updates=26200, lr=0.000218426, gnorm=3.025, clip=0, loss_scale=512, train_wall=248, wall=0
2022-08-11 23:48:02 | INFO | train_inner | epoch 012:   1651 / 2244 loss=nan, nll_loss=1.939, mask_ins=0.806, word_ins_ml=3.586, word_reposition=0.703, kpe=nan, ppl=nan, wps=3637.9, ups=0.35, wpb=10274.4, bsz=128, num_updates=26300, lr=0.00021801, gnorm=2.959, clip=0, loss_scale=512, train_wall=249, wall=0
2022-08-11 23:53:23 | INFO | train_inner | epoch 012:   1751 / 2244 loss=5.804, nll_loss=1.943, mask_ins=0.802, word_ins_ml=3.59, word_reposition=0.709, kpe=0.703, ppl=55.88, wps=3203, ups=0.31, wpb=10296.4, bsz=128, num_updates=26400, lr=0.000217597, gnorm=2.913, clip=0, loss_scale=512, train_wall=287, wall=0
2022-08-11 23:58:05 | INFO | train_inner | epoch 012:   1851 / 2244 loss=5.791, nll_loss=1.911, mask_ins=0.815, word_ins_ml=3.562, word_reposition=0.704, kpe=0.709, ppl=55.35, wps=3639.7, ups=0.35, wpb=10269.8, bsz=128, num_updates=26500, lr=0.000217186, gnorm=3.076, clip=0, loss_scale=512, train_wall=248, wall=0
2022-08-12 00:02:47 | INFO | train_inner | epoch 012:   1951 / 2244 loss=5.849, nll_loss=1.949, mask_ins=0.839, word_ins_ml=3.595, word_reposition=0.713, kpe=0.703, ppl=57.65, wps=3629.8, ups=0.35, wpb=10235.2, bsz=128, num_updates=26600, lr=0.000216777, gnorm=3.044, clip=0, loss_scale=998, train_wall=248, wall=0
2022-08-12 00:07:30 | INFO | train_inner | epoch 012:   2051 / 2244 loss=5.866, nll_loss=1.969, mask_ins=0.835, word_ins_ml=3.613, word_reposition=0.709, kpe=0.709, ppl=58.32, wps=3626.8, ups=0.35, wpb=10254.1, bsz=128, num_updates=26700, lr=0.000216371, gnorm=3.037, clip=0, loss_scale=1024, train_wall=249, wall=0
2022-08-12 00:08:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-12 00:10:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-12 00:10:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-12 00:10:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-08-12 00:10:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-08-12 00:10:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2022-08-12 00:10:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0
2022-08-12 00:10:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0
2022-08-12 00:10:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2.0
2022-08-12 00:10:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1.0
2022-08-12 00:11:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.5
2022-08-12 00:11:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.25
2022-08-12 00:11:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.125
2022-08-12 00:11:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0625
2022-08-12 00:11:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.03125
2022-08-12 00:11:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.015625
2022-08-12 00:11:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0078125
2022-08-12 00:11:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.00390625
2022-08-12 00:11:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.001953125
2022-08-12 00:11:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0009765625
2022-08-12 00:11:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.00048828125
2022-08-12 00:11:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.000244140625
2022-08-12 00:11:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0001220703125
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:1000: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not take as input a "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:1000: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not take as input a "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:1000: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not take as input a "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:1000: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not take as input a "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:1015: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:1015: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:1015: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:1015: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
2022-08-12 00:11:38 | WARNING | fairseq.nan_detector | Inf detected in output of module.encoder.kpe, shape: torch.Size([4, 248]), forward input max: 5.07421875, input min: -4.13671875
2022-08-12 00:11:38 | WARNING | fairseq.nan_detector | Inf detected in output of module.encoder.kpe, shape: torch.Size([4, 270]), forward input max: 5.24609375, input min: -3.798828125
2022-08-12 00:11:38 | WARNING | fairseq.nan_detector | Inf detected in output of module.encoder.kpe, shape: torch.Size([4, 259]), forward input max: 4.984375, input min: -4.01171875
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:990: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:990: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:990: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
2022-08-12 00:11:38 | WARNING | fairseq.nan_detector | Inf detected in output of module.encoder.kpe, shape: torch.Size([4, 263]), forward input max: 5.03125, input min: -3.86328125
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py:990: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/trainer.py", line 369, in train_step
    grad_norm = self.optimizer.clip_grad_norm(self.args.clip_norm)
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/fp16_optimizer.py", line 170, in clip_grad_norm
    raise FloatingPointError((
FloatingPointError: Minimum loss scale reached (0.0001). Your loss is probably exploding. Try lowering the learning rate, using gradient clipping or increasing the batch size.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 341, in distributed_main
    main(args, init_distributed=True)
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 151, in main
    train(args, trainer, task, epoch_itr)
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 231, in train
    log_output = trainer.train_step(samples)
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/trainer.py", line 398, in train_step
    self.task.train_step(
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/tasks/translation_lev.py", line 230, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/criterions/nat_loss.py", line 194, in forward
    outputs = model(src_tokens, src_lengths, prev_output_tokens, tgt_tokens, cached_features)
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1120, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/legacy_distributed_data_parallel.py", line 86, in forward
    return self.module(*inputs, **kwargs)
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1135, in _call_impl
    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))
StopIteration

