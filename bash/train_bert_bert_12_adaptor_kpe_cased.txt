nohup: ignoring input
2022-07-07 15:19:27 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:14018
2022-07-07 15:19:28 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:14018
2022-07-07 15:19:28 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:14018
2022-07-07 15:19:28 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-07 15:19:28 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:14018
2022-07-07 15:19:28 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-07 15:19:28 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-07 15:19:28 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-07 15:19:28 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-07 15:19:28 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-07 15:19:28 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-07 15:19:28 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-07 15:19:28 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-07 15:19:28 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-07 15:19:28 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-07 15:19:28 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-07 15:19:32 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:14018', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='/data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-07 15:19:32 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-07 15:19:32 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-07-07 15:19:32 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.source
2022-07-07 15:19:32 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.target
2022-07-07 15:19:32 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...

0it [00:00, ?it/s]start load cached examples valid ...

0it [00:00, ?it/s]start load cached examples valid ...

0it [00:00, ?it/s]start load cached examples valid ...

0it [00:00, ?it/s]
396it [00:00, 3955.16it/s]
392it [00:00, 3913.44it/s]
389it [00:00, 3886.34it/s]
388it [00:00, 3856.82it/s]
792it [00:00, 3571.82it/s]
778it [00:00, 3515.30it/s]
784it [00:00, 3522.47it/s]
774it [00:00, 3414.16it/s]
1160it [00:00, 3615.55it/s]
1132it [00:00, 3514.68it/s]
1139it [00:00, 3514.16it/s]
1141it [00:00, 3520.47it/s]
1524it [00:00, 3458.74it/s]
1485it [00:00, 3393.55it/s]
1492it [00:00, 3388.73it/s]
1496it [00:00, 3388.88it/s]
1916it [00:00, 3614.22it/s]
1861it [00:00, 3516.77it/s]
1882it [00:00, 3563.78it/s]
1885it [00:00, 3557.37it/s]
2280it [00:00, 3528.12it/s]
2234it [00:00, 3454.62it/s]
2240it [00:00, 3425.10it/s]
2243it [00:00, 3414.97it/s]
2676it [00:00, 3661.03it/s]
2631it [00:00, 3614.63it/s]
2639it [00:00, 3597.40it/s]
2643it [00:00, 3593.10it/s]
3011it [00:00, 3670.84it/s]
3033it [00:00, 3700.86it/s]
3059it [00:00, 3577.59it/s]
3037it [00:00, 3698.05it/s]
3447it [00:00, 3666.00it/s]
3380it [00:00, 3566.97it/s]
3405it [00:00, 3586.08it/s]
3409it [00:00, 3572.99it/s]
3834it [00:01, 3726.64it/s]
3755it [00:01, 3620.19it/s]
3777it [00:01, 3623.84it/s]
3778it [00:01, 3607.10it/s]
4119it [00:01, 3528.66it/s]
4208it [00:01, 3606.58it/s]
4141it [00:01, 3529.17it/s]
4141it [00:01, 3508.26it/s]
4587it [00:01, 3659.98it/s]
4494it [00:01, 3591.68it/s]
4521it [00:01, 3607.90it/s]
4520it [00:01, 3588.64it/s]
4955it [00:01, 3528.72it/s]
4883it [00:01, 3497.34it/s]
4855it [00:01, 3442.54it/s]
4881it [00:01, 3472.36it/s]
5332it [00:01, 3597.56it/s]
5245it [00:01, 3528.00it/s]
5236it [00:01, 3545.44it/s]
5244it [00:01, 3508.22it/s]
5694it [00:01, 3465.36it/s]
5599it [00:01, 3411.22it/s]
5594it [00:01, 3386.42it/s]
5596it [00:01, 3392.72it/s]
5955it [00:01, 3453.21it/s]
6043it [00:01, 3451.01it/s]
5950it [00:01, 3433.71it/s]
5952it [00:01, 3439.26it/s]
6311it [00:01, 3483.36it/s]
6398it [00:01, 3479.29it/s]
6305it [00:01, 3465.36it/s]
6298it [00:01, 3425.72it/s]
6661it [00:02, 2095.70it/s]
6747it [00:02, 2092.39it/s]
6642it [00:02, 1973.91it/s]
6653it [00:02, 1966.22it/s]
7008it [00:02, 2372.12it/s]
7086it [00:02, 2351.77it/s]
6978it [00:02, 2242.19it/s]
7006it [00:02, 2264.26it/s]
7343it [00:02, 2590.05it/s]
7381it [00:02, 2481.97it/s]
7331it [00:02, 2519.76it/s]
7344it [00:02, 2433.77it/s]
7735it [00:02, 2735.95it/s]
7652it [00:02, 2674.70it/s]
7640it [00:02, 2617.19it/s]
7687it [00:02, 2660.69it/s]
8073it [00:02, 2899.18it/s]
8003it [00:02, 2885.60it/s]
7992it [00:02, 2842.03it/s]
8040it [00:02, 2873.82it/s]
8393it [00:02, 2918.01it/s]
8321it [00:02, 2872.80it/s]
8310it [00:02, 2834.29it/s]
8364it [00:02, 2869.84it/s]
8732it [00:02, 3045.23it/s]
8672it [00:02, 3043.60it/s]
8663it [00:02, 3019.58it/s]
8718it [00:02, 3045.39it/s]
9053it [00:02, 3034.19it/s]
9025it [00:02, 3018.51it/s]
9017it [00:02, 3160.76it/s]
9043it [00:02, 3025.89it/s]
9405it [00:02, 3169.60it/s]
9376it [00:02, 3152.64it/s]
9348it [00:02, 3056.61it/s]
9389it [00:02, 3144.77it/s]
9745it [00:03, 3233.28it/s]
9717it [00:03, 3224.08it/s]
9701it [00:03, 3186.95it/s]
9744it [00:03, 3259.09it/s]
10075it [00:03, 3157.20it/s]
10047it [00:03, 3143.45it/s]
10029it [00:03, 3084.72it/s]
10079it [00:03, 3177.21it/s]
10419it [00:03, 3221.41it/s]
10399it [00:03, 3248.93it/s]
10381it [00:03, 3206.22it/s]
10432it [00:03, 3274.98it/s]
10745it [00:03, 3153.43it/s]
10729it [00:03, 3156.77it/s]
10765it [00:03, 3194.46it/s]
10707it [00:03, 3086.93it/s]
11094it [00:03, 3249.85it/s]
11064it [00:03, 3211.79it/s]
11105it [00:03, 3250.68it/s]
11058it [00:03, 3204.75it/s]
11433it [00:03, 3288.25it/s]
11416it [00:03, 3299.72it/s]
11459it [00:03, 3332.07it/s]
11411it [00:03, 3297.07it/s]
11764it [00:03, 3189.67it/s]
11749it [00:03, 3196.15it/s]
11795it [00:03, 3223.59it/s]
11744it [00:03, 3196.37it/s]
12118it [00:03, 3288.72it/s]
12101it [00:03, 3288.73it/s]
12150it [00:03, 3316.74it/s]
12081it [00:03, 3245.01it/s]
12449it [00:03, 3139.12it/s]
12432it [00:03, 3141.93it/s]
12484it [00:03, 3217.58it/s]
12408it [00:03, 3155.33it/s]
12800it [00:03, 3243.17it/s]
12783it [00:04, 3246.01it/s]
12826it [00:04, 3274.37it/s]
12759it [00:04, 3256.10it/s]
13138it [00:04, 3274.82it/s]
13133it [00:04, 3317.23it/s]
13175it [00:04, 3336.97it/s]
13109it [00:04, 3325.42it/s]
13368it [00:04, 3206.69it/s]
2022-07-07 15:19:36 | INFO | root | success load 13368 data
2022-07-07 15:19:36 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-07 15:19:36 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-07 15:19:36 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-07 15:19:36 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-07 15:19:36 | INFO | transformer.tokenization_utils | loading file None
2022-07-07 15:19:36 | INFO | transformer.tokenization_utils | loading file None
2022-07-07 15:19:36 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json

13368it [00:04, 3191.19it/s]

13368it [00:04, 3175.10it/s]

13368it [00:04, 3155.99it/s]
2022-07-07 15:19:37 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-07 15:19:37 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-07 15:19:37 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-07 15:19:41 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-07 15:19:41 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-07 15:19:41 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-07 15:19:41 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-07 15:19:41 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-07 15:19:43 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-07 15:19:43 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-07-07 15:19:43 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-07-07 15:19:43 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-07 15:19:43 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 142456835)
2022-07-07 15:19:43 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-07-07 15:19:43 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)

Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...

0it [00:00, ?it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...

0it [00:00, ?it/s]
360it [00:00, 3597.21it/s]2022-07-07 15:19:43 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-07 15:19:43 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-07 15:19:43 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_last.pt
2022-07-07 15:19:43 | INFO | fairseq.trainer | loading train data for epoch 1

350it [00:00, 3493.97it/s]
720it [00:00, 3395.42it/s]
700it [00:00, 3325.67it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...

0it [00:00, ?it/s]2022-07-07 15:19:43 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.source

1098it [00:00, 3561.43it/s]2022-07-07 15:19:43 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.target
2022-07-07 15:19:43 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...

0it [00:00, ?it/s]
1058it [00:00, 3436.73it/s]
373it [00:00, 3722.03it/s]
1456it [00:00, 3564.22it/s]
377it [00:00, 3760.00it/s]
1410it [00:00, 3467.84it/s]
746it [00:00, 3519.74it/s]
1813it [00:00, 3451.00it/s]
753it [00:00, 3513.92it/s]
1758it [00:00, 3381.77it/s]
1122it [00:00, 3622.70it/s]
2192it [00:00, 3562.15it/s]
1123it [00:00, 3593.93it/s]
2119it [00:00, 3454.85it/s]
1485it [00:00, 3439.62it/s]
2550it [00:00, 3441.18it/s]
1484it [00:00, 3413.14it/s]
2466it [00:00, 3361.58it/s]
1862it [00:00, 3553.09it/s]
2910it [00:00, 3489.55it/s]
1874it [00:00, 3577.17it/s]
2842it [00:00, 3482.51it/s]
2242it [00:00, 3632.21it/s]
3260it [00:00, 3393.58it/s]
2253it [00:00, 3646.77it/s]
3192it [00:00, 3355.44it/s]
2607it [00:00, 3480.92it/s]
3636it [00:01, 3502.07it/s]
2620it [00:00, 3502.49it/s]
3557it [00:01, 3442.24it/s]
2984it [00:00, 3566.81it/s]
4000it [00:01, 3387.17it/s]
2994it [00:00, 3571.81it/s]
3913it [00:01, 3475.36it/s]
3343it [00:00, 3460.07it/s]
4363it [00:01, 3455.32it/s]
3353it [00:00, 3473.91it/s]
4262it [00:01, 3363.45it/s]
3708it [00:01, 3515.76it/s]
4738it [00:01, 3539.21it/s]
3730it [00:01, 3558.69it/s]
4627it [00:01, 3445.97it/s]
4061it [00:01, 3416.26it/s]
5094it [00:01, 3413.24it/s]
4088it [00:01, 3463.73it/s]
4973it [00:01, 3312.61it/s]
4436it [00:01, 3512.93it/s]
5470it [00:01, 3505.07it/s]
4472it [00:01, 3572.98it/s]
5330it [00:01, 3384.11it/s]
4808it [00:01, 3571.22it/s]
5823it [00:01, 3391.74it/s]
4839it [00:01, 3435.61it/s]
5680it [00:01, 3229.76it/s]
5167it [00:01, 3451.34it/s]
6178it [00:01, 3436.01it/s]
5215it [00:01, 3527.16it/s]
6040it [00:01, 3333.72it/s]
5535it [00:01, 3515.18it/s]
6523it [00:01, 3345.20it/s]
5595it [00:01, 3605.31it/s]
6412it [00:01, 3442.10it/s]
5888it [00:01, 3420.27it/s]
6892it [00:01, 3444.13it/s]
5958it [00:01, 3495.16it/s]
6759it [00:01, 3338.27it/s]
6264it [00:01, 3515.72it/s]
7267it [00:02, 3532.87it/s]
6335it [00:01, 3574.10it/s]
7114it [00:02, 3397.15it/s]
6617it [00:01, 3426.55it/s]
6694it [00:01, 3457.13it/s]
6993it [00:01, 3520.73it/s]
7071it [00:02, 3545.60it/s]
7359it [00:02, 3560.92it/s]
7622it [00:02, 1221.68it/s]
7995it [00:02, 1538.70it/s]
8291it [00:03, 1755.40it/s]
7456it [00:02, 1035.78it/s]
7428it [00:02, 3453.31it/s]
7717it [00:02, 1307.39it/s]
8664it [00:03, 2108.63it/s]
7775it [00:02, 1256.10it/s]
7817it [00:03, 1324.30it/s]
8102it [00:02, 1646.50it/s]
9039it [00:03, 2439.83it/s]
8140it [00:02, 1566.58it/s]
8191it [00:03, 1657.18it/s]
8422it [00:02, 1894.58it/s]
9374it [00:03, 2611.96it/s]
8459it [00:03, 1820.12it/s]
8506it [00:03, 1902.22it/s]
8803it [00:03, 2250.40it/s]
9746it [00:03, 2878.71it/s]
8836it [00:03, 2176.16it/s]
8873it [00:03, 2238.21it/s]
9132it [00:03, 2435.73it/s]
9158it [00:03, 2388.53it/s]
9202it [00:03, 2441.22it/s]
10089it [00:03, 2887.44it/s]
9518it [00:03, 2760.52it/s]
9539it [00:03, 2712.01it/s]
9575it [00:03, 2739.81it/s]
10467it [00:03, 3117.11it/s]
9901it [00:03, 3022.85it/s]
9925it [00:03, 2992.83it/s]
10809it [00:03, 3118.42it/s]
9949it [00:03, 2878.69it/s]
10255it [00:03, 3090.32it/s]
10278it [00:03, 3029.88it/s]
11187it [00:03, 3297.42it/s]
10319it [00:03, 3088.25it/s]
10643it [00:03, 3299.35it/s]
10660it [00:03, 3237.66it/s]
11546it [00:03, 3377.17it/s]
10699it [00:03, 3277.79it/s]
11002it [00:03, 3247.53it/s]
11014it [00:03, 3236.21it/s]
11896it [00:04, 3316.09it/s]
11055it [00:04, 3242.16it/s]
11391it [00:03, 3422.48it/s]
11402it [00:03, 3412.00it/s]
12276it [00:04, 3452.60it/s]
11436it [00:04, 3398.27it/s]
11749it [00:03, 3367.27it/s]
11760it [00:03, 3352.94it/s]
12629it [00:04, 3383.71it/s]
11791it [00:04, 3318.99it/s]
12138it [00:04, 3512.76it/s]
12145it [00:04, 3491.28it/s]
13000it [00:04, 3476.77it/s]
12174it [00:04, 3460.27it/s]
12498it [00:04, 3466.97it/s]
12503it [00:04, 3403.55it/s]
13352it [00:04, 3422.40it/s]
12529it [00:04, 3389.20it/s]
12870it [00:04, 3538.81it/s]
12889it [00:04, 3531.63it/s]
13736it [00:04, 3541.23it/s]
12912it [00:04, 3513.49it/s]
13267it [00:04, 3662.93it/s]
13290it [00:04, 3669.19it/s]
14121it [00:04, 3630.93it/s]
13300it [00:04, 3616.79it/s]
13637it [00:04, 3576.98it/s]
13662it [00:04, 3568.75it/s]
14486it [00:04, 3519.70it/s]
13666it [00:04, 3519.10it/s]
14028it [00:04, 3673.12it/s]
14056it [00:04, 3673.97it/s]
14869it [00:04, 3606.96it/s]
14048it [00:04, 3603.72it/s]
14398it [00:04, 3597.64it/s]
14427it [00:04, 3587.81it/s]
15232it [00:05, 3486.57it/s]
14411it [00:04, 3529.23it/s]
14787it [00:04, 3681.60it/s]
14798it [00:04, 3622.79it/s]
15607it [00:05, 3559.80it/s]
14788it [00:05, 3597.39it/s]
15157it [00:04, 3561.24it/s]
15162it [00:04, 3535.66it/s]
15965it [00:05, 3443.03it/s]
15150it [00:05, 3524.34it/s]
15537it [00:04, 3629.34it/s]
15544it [00:04, 3616.74it/s]
16338it [00:05, 3524.09it/s]
15521it [00:05, 3575.58it/s]
15902it [00:05, 3521.52it/s]
15908it [00:05, 3506.59it/s]
15880it [00:05, 3478.19it/s]
16692it [00:05, 3403.04it/s]
16277it [00:05, 3584.99it/s]
16285it [00:05, 3580.48it/s]
16243it [00:05, 3521.41it/s]
16669it [00:05, 3529.44it/s]
16635it [00:05, 3635.66it/s]
16669it [00:05, 3511.26it/s]
17024it [00:06, 1053.17it/s]
17034it [00:06, 908.51it/s] 
17022it [00:06, 1041.20it/s]
17000it [00:06, 1043.75it/s]
17409it [00:06, 1355.34it/s]
17411it [00:06, 1189.01it/s]
17407it [00:06, 1343.66it/s]
17374it [00:06, 1333.48it/s]
17739it [00:06, 1615.25it/s]
17707it [00:06, 1406.39it/s]
17736it [00:06, 1603.03it/s]
17703it [00:06, 1593.97it/s]
18118it [00:06, 1964.09it/s]
18087it [00:06, 1763.44it/s]
18114it [00:06, 1950.95it/s]
18080it [00:06, 1941.77it/s]
18447it [00:06, 2208.66it/s]
18419it [00:06, 2026.97it/s]
18442it [00:06, 2197.51it/s]
18419it [00:06, 2200.23it/s]
18829it [00:06, 2546.97it/s]
18795it [00:07, 2372.55it/s]
18825it [00:06, 2538.92it/s]
18790it [00:07, 2515.13it/s]
19207it [00:06, 2830.99it/s]
19174it [00:07, 2686.49it/s]
19204it [00:06, 2826.42it/s]
19174it [00:07, 2818.81it/s]
19562it [00:06, 2951.10it/s]
19523it [00:07, 2809.44it/s]
19559it [00:06, 2946.50it/s]
19529it [00:07, 2917.30it/s]
19929it [00:07, 3135.61it/s]
19895it [00:07, 3036.69it/s]
19927it [00:07, 3134.91it/s]
19908it [00:07, 3138.65it/s]
20282it [00:07, 3165.02it/s]
20244it [00:07, 3082.67it/s]
20280it [00:07, 3164.06it/s]
20262it [00:07, 3152.74it/s]
20659it [00:07, 3326.56it/s]
20608it [00:07, 3231.31it/s]
20658it [00:07, 3325.31it/s]
20642it [00:07, 3325.53it/s]
21013it [00:07, 3307.62it/s]
20955it [00:07, 3210.54it/s]
21012it [00:07, 3303.87it/s]
21395it [00:07, 3450.52it/s]
20997it [00:07, 3283.72it/s]
21334it [00:07, 3370.32it/s]
21393it [00:07, 3443.46it/s]
21768it [00:07, 3528.58it/s]
21377it [00:07, 3425.98it/s]
21710it [00:07, 3480.83it/s]
21766it [00:07, 3523.25it/s]
21750it [00:07, 3511.16it/s]
22129it [00:07, 3393.40it/s]
22068it [00:07, 3361.52it/s]
22127it [00:07, 3437.28it/s]
22110it [00:07, 3389.88it/s]
22519it [00:07, 3536.17it/s]
22450it [00:08, 3491.04it/s]
22512it [00:07, 3553.18it/s]
22483it [00:08, 3486.13it/s]
22879it [00:07, 3466.12it/s]
22806it [00:08, 3416.14it/s]
22873it [00:07, 3475.04it/s]
22837it [00:08, 3434.58it/s]
23256it [00:07, 3551.37it/s]
23180it [00:08, 3506.29it/s]
23249it [00:08, 3554.86it/s]
23207it [00:08, 3510.86it/s]
23615it [00:08, 3423.40it/s]
23535it [00:08, 3363.35it/s]
23608it [00:08, 3361.60it/s]
23562it [00:08, 3416.20it/s]
23997it [00:08, 3534.49it/s]
23909it [00:08, 3469.17it/s]
23987it [00:08, 3479.59it/s]
23930it [00:08, 3491.80it/s]
24354it [00:08, 3448.66it/s]
24287it [00:08, 3556.40it/s]
24339it [00:08, 3410.77it/s]
24299it [00:08, 3422.43it/s]
24729it [00:08, 3533.60it/s]
24646it [00:08, 3412.16it/s]
24716it [00:08, 3513.35it/s]
24665it [00:08, 3489.59it/s]
25107it [00:08, 3603.12it/s]
25014it [00:08, 3486.30it/s]
25090it [00:08, 3577.19it/s]
25042it [00:08, 3570.79it/s]
25469it [00:08, 3459.98it/s]
25365it [00:08, 3381.97it/s]
25450it [00:08, 3438.76it/s]
25401it [00:08, 3421.66it/s]
25849it [00:08, 3554.87it/s]
25742it [00:09, 3491.89it/s]
25831it [00:08, 3543.32it/s]
25782it [00:09, 3531.49it/s]
26207it [00:08, 3446.73it/s]
26094it [00:09, 3355.27it/s]
26188it [00:08, 3428.65it/s]
26138it [00:09, 3428.39it/s]
26582it [00:08, 3532.78it/s]
26463it [00:09, 3448.91it/s]
26567it [00:08, 3529.88it/s]
26498it [00:09, 3476.13it/s]
26937it [00:09, 3421.92it/s]
26819it [00:09, 3353.87it/s]
26922it [00:09, 3423.50it/s]
26848it [00:09, 3396.62it/s]
27310it [00:09, 3507.77it/s]
27189it [00:09, 3450.23it/s]
27296it [00:09, 3512.14it/s]
27233it [00:09, 3524.96it/s]
27663it [00:09, 3437.92it/s]
27569it [00:09, 3550.47it/s]
27658it [00:09, 3437.44it/s]
27602it [00:09, 3572.71it/s]
28047it [00:09, 3552.36it/s]
27926it [00:09, 3443.24it/s]
28041it [00:09, 3548.50it/s]
27961it [00:09, 3481.61it/s]
28423it [00:09, 3612.24it/s]
28297it [00:09, 3519.27it/s]
28415it [00:09, 3603.18it/s]
28339it [00:09, 3567.47it/s]
28651it [00:10, 834.72it/s] 
28786it [00:10, 794.92it/s] 
29013it [00:11, 1084.21it/s]
29162it [00:10, 1045.45it/s]
28777it [00:10, 779.44it/s] 
29387it [00:11, 1385.69it/s]
29475it [00:10, 1269.77it/s]
29150it [00:10, 1024.22it/s]
29701it [00:11, 1625.86it/s]
28697it [00:11, 702.44it/s] 
29851it [00:11, 1601.65it/s]
29463it [00:11, 1247.14it/s]
30066it [00:11, 1961.25it/s]
29070it [00:11, 932.66it/s]
30228it [00:11, 1948.38it/s]
29839it [00:11, 1577.05it/s]
30394it [00:11, 2192.47it/s]
29409it [00:11, 1166.29it/s]
30568it [00:11, 2179.02it/s]
30217it [00:11, 1925.04it/s]
30767it [00:11, 2519.96it/s]
29789it [00:11, 1487.57it/s]
30942it [00:11, 2501.74it/s]
30557it [00:11, 2169.79it/s]
31105it [00:11, 2671.71it/s]
30168it [00:11, 1829.17it/s]
31286it [00:11, 2667.36it/s]
30931it [00:11, 2493.34it/s]
31467it [00:11, 2905.08it/s]
30509it [00:11, 2083.60it/s]
31661it [00:11, 2929.83it/s]
31277it [00:11, 2663.25it/s]
31844it [00:11, 3129.17it/s]
30881it [00:11, 2406.85it/s]
32009it [00:11, 3009.72it/s]
31652it [00:11, 2926.05it/s]
32195it [00:11, 3148.34it/s]
31227it [00:11, 2598.71it/s]
32375it [00:11, 3180.91it/s]
32001it [00:11, 3004.70it/s]
32563it [00:12, 3292.71it/s]
31604it [00:12, 2874.62it/s]
32744it [00:11, 3319.21it/s]
32378it [00:11, 3205.14it/s]
32913it [00:12, 3202.53it/s]
31955it [00:12, 2944.46it/s]
33098it [00:11, 3275.11it/s]
32744it [00:11, 3327.86it/s]
33283it [00:12, 3339.96it/s]
32336it [00:12, 3167.81it/s]
33468it [00:12, 3389.14it/s]
33100it [00:12, 3282.72it/s]
33629it [00:12, 3278.08it/s]
32704it [00:12, 3304.61it/s]
33819it [00:12, 3333.70it/s]
33469it [00:12, 3396.43it/s]
33995it [00:12, 3384.84it/s]
33061it [00:12, 3273.21it/s]
34191it [00:12, 3441.12it/s]
33821it [00:12, 3341.60it/s]
34371it [00:12, 3491.45it/s]
33426it [00:12, 3375.43it/s]
34542it [00:12, 3365.82it/s]
34191it [00:12, 3441.64it/s]
34725it [00:12, 3383.27it/s]
33777it [00:12, 3328.87it/s]
34912it [00:12, 3460.14it/s]
34542it [00:12, 3364.83it/s]
35091it [00:12, 3461.23it/s]
34165it [00:12, 3484.26it/s]
35287it [00:12, 3544.00it/s]
34913it [00:12, 3463.53it/s]
35441it [00:12, 3369.54it/s]
34521it [00:12, 3393.96it/s]
35645it [00:12, 3428.33it/s]
35288it [00:12, 3381.11it/s]
35797it [00:13, 3423.48it/s]
34877it [00:13, 3439.71it/s]
36009it [00:12, 3487.86it/s]
35664it [00:12, 3487.14it/s]
36142it [00:13, 3329.65it/s]
35255it [00:13, 3537.33it/s]
36360it [00:12, 3389.99it/s]
36041it [00:12, 3567.15it/s]
36508it [00:13, 3422.43it/s]
35612it [00:13, 3443.70it/s]
36729it [00:13, 3476.15it/s]
36401it [00:13, 3405.86it/s]
36879it [00:13, 3505.89it/s]
35990it [00:13, 3533.66it/s]
37079it [00:13, 3391.13it/s]
36768it [00:13, 3480.61it/s]
37231it [00:13, 3361.68it/s]
36346it [00:13, 3431.54it/s]
37449it [00:13, 3479.07it/s]
37119it [00:13, 3394.63it/s]
37598it [00:13, 3449.41it/s]
36692it [00:13, 3423.02it/s]
37803it [00:13, 3495.97it/s]
37490it [00:13, 3483.54it/s]
37945it [00:13, 3346.90it/s]
37036it [00:13, 3357.28it/s]
38154it [00:13, 3409.56it/s]
37841it [00:13, 3382.33it/s]
38320it [00:13, 3460.23it/s]
37407it [00:13, 3458.57it/s]
38513it [00:13, 3461.92it/s]
38227it [00:13, 3516.74it/s]
38668it [00:13, 3277.78it/s]
37779it [00:13, 3532.78it/s]
38861it [00:13, 3385.97it/s]
38581it [00:13, 3476.70it/s]
39045it [00:13, 3414.42it/s]
38134it [00:13, 3440.05it/s]
39242it [00:13, 3506.99it/s]
38930it [00:13, 3399.15it/s]
39421it [00:14, 3511.14it/s]
38501it [00:14, 3505.53it/s]
39594it [00:13, 3411.69it/s]
39312it [00:13, 3519.52it/s]
39775it [00:14, 3395.19it/s]
38853it [00:14, 3347.89it/s]
39959it [00:13, 3480.46it/s]
39666it [00:13, 3410.11it/s]
40133it [00:14, 3446.41it/s]
39226it [00:14, 3456.23it/s]
40329it [00:14, 3398.51it/s]
40042it [00:14, 3508.46it/s]
40480it [00:14, 3353.51it/s]
39574it [00:14, 3387.37it/s]
40699it [00:14, 3482.22it/s]
40395it [00:14, 3417.59it/s]
40850it [00:14, 3452.08it/s]
39953it [00:14, 3502.75it/s]
41077it [00:14, 3566.97it/s]
40748it [00:14, 3443.10it/s]
41197it [00:14, 3360.72it/s]
40330it [00:14, 3421.16it/s]
41435it [00:14, 3466.93it/s]
41126it [00:14, 3538.82it/s]
41547it [00:14, 3400.29it/s]
40692it [00:14, 3476.50it/s]
41809it [00:14, 3543.79it/s]
41481it [00:14, 3443.96it/s]
41917it [00:14, 3487.43it/s]
41071it [00:14, 3564.98it/s]
42165it [00:14, 3447.12it/s]
41857it [00:14, 3533.52it/s]
42267it [00:14, 3387.50it/s]
41429it [00:14, 3466.90it/s]
42548it [00:14, 3556.09it/s]
42212it [00:14, 3440.80it/s]
42640it [00:15, 3485.91it/s]
41804it [00:15, 3546.83it/s]
42905it [00:14, 3445.14it/s]
42590it [00:14, 3537.31it/s]
42990it [00:15, 3369.49it/s]
42160it [00:15, 3412.90it/s]
43273it [00:14, 3511.85it/s]
42945it [00:14, 3428.42it/s]
43348it [00:15, 3429.75it/s]
42544it [00:15, 3533.52it/s]
43635it [00:15, 3542.52it/s]
43301it [00:15, 3465.19it/s]
43693it [00:15, 3326.46it/s]
42900it [00:15, 3429.48it/s]
43673it [00:15, 3537.86it/s]
43270it [00:15, 3505.84it/s]
43634it [00:15, 3543.75it/s]
43991it [00:16, 653.96it/s] 
44375it [00:16, 882.53it/s]
44028it [00:16, 637.37it/s] 
44027it [00:17, 586.88it/s] 
44691it [00:16, 1094.24it/s]
43990it [00:17, 664.94it/s] 
44405it [00:16, 857.25it/s]
44389it [00:17, 790.90it/s]
45074it [00:16, 1414.72it/s]
44359it [00:17, 884.95it/s]
44720it [00:16, 1066.07it/s]
44701it [00:17, 995.12it/s]
45438it [00:17, 1700.33it/s]
44677it [00:17, 1100.91it/s]
45099it [00:17, 1379.56it/s]
45080it [00:17, 1306.41it/s]
45804it [00:17, 2028.01it/s]
45060it [00:17, 1423.98it/s]
45437it [00:17, 1646.05it/s]
45439it [00:17, 1588.05it/s]
46176it [00:17, 2353.85it/s]
45437it [00:17, 1763.22it/s]
45811it [00:17, 1994.58it/s]
45812it [00:17, 1932.80it/s]
46523it [00:17, 2544.38it/s]
46182it [00:17, 2323.59it/s]
45777it [00:17, 2010.99it/s]
46164it [00:17, 2228.90it/s]
46894it [00:17, 2814.48it/s]
46148it [00:17, 2339.87it/s]
46529it [00:17, 2518.62it/s]
46502it [00:17, 2422.90it/s]
47242it [00:17, 2929.66it/s]
46900it [00:17, 2792.97it/s]
46491it [00:17, 2532.01it/s]
46870it [00:17, 2708.44it/s]
47616it [00:17, 3138.63it/s]
46864it [00:17, 2811.26it/s]
47248it [00:17, 2892.19it/s]
47211it [00:18, 2828.75it/s]
47967it [00:17, 3156.19it/s]
47620it [00:17, 3103.50it/s]
47211it [00:18, 2905.46it/s]
47583it [00:18, 3056.17it/s]
48347it [00:17, 3330.93it/s]
47588it [00:18, 3128.77it/s]
47969it [00:17, 3127.72it/s]
47938it [00:18, 3187.61it/s]
48728it [00:17, 3455.99it/s]
48348it [00:17, 3305.76it/s]
47960it [00:18, 3154.23it/s]
48286it [00:18, 3175.35it/s]
49089it [00:18, 3396.26it/s]
48726it [00:18, 3438.00it/s]
48339it [00:18, 3323.83it/s]
48660it [00:18, 3329.33it/s]
49454it [00:18, 3466.41it/s]
48710it [00:18, 3429.15it/s]
49085it [00:18, 3370.47it/s]
49009it [00:18, 3293.14it/s]
49809it [00:18, 3397.48it/s]
49458it [00:18, 3470.97it/s]
49068it [00:18, 3378.70it/s]
49377it [00:18, 3400.93it/s]
50184it [00:18, 3497.95it/s]
49441it [00:18, 3476.05it/s]
49814it [00:18, 3393.83it/s]
49726it [00:18, 3292.13it/s]
50539it [00:18, 3398.99it/s]
50188it [00:18, 3490.62it/s]
49797it [00:18, 3399.16it/s]
50098it [00:18, 3410.95it/s]
50914it [00:18, 3497.20it/s]
50162it [00:18, 3468.97it/s]
50542it [00:18, 3392.09it/s]
50469it [00:18, 3494.93it/s]
51295it [00:18, 3586.11it/s]
50905it [00:18, 3457.73it/s]
50514it [00:18, 3365.68it/s]
50823it [00:19, 3381.41it/s]
51656it [00:18, 3466.73it/s]
51283it [00:18, 3549.38it/s]
50891it [00:19, 3479.92it/s]
51179it [00:19, 3431.36it/s]
52030it [00:18, 3543.13it/s]
51255it [00:19, 3525.56it/s]
51641it [00:18, 3435.43it/s]
51525it [00:19, 3344.94it/s]
52387it [00:19, 3426.66it/s]
52014it [00:19, 3520.00it/s]
51610it [00:19, 3424.26it/s]
51895it [00:19, 3444.43it/s]
52762it [00:19, 3517.32it/s]
51986it [00:19, 3520.69it/s]
52368it [00:19, 3411.90it/s]
52242it [00:19, 3347.17it/s]
53116it [00:19, 3397.33it/s]
52739it [00:19, 3496.25it/s]
52340it [00:19, 3421.05it/s]
52614it [00:19, 3451.49it/s]
53488it [00:19, 3482.36it/s]
52700it [00:19, 3472.03it/s]
53091it [00:19, 3401.35it/s]
52969it [00:19, 3478.66it/s]
53838it [00:19, 3390.44it/s]
53463it [00:19, 3492.12it/s]
53049it [00:19, 3394.12it/s]
53319it [00:19, 3369.16it/s]
54221it [00:19, 3514.66it/s]
53834it [00:19, 3553.72it/s]
53421it [00:19, 3485.60it/s]
53686it [00:19, 3454.27it/s]
54597it [00:19, 3585.45it/s]
53794it [00:19, 3555.96it/s]
54191it [00:19, 3451.16it/s]
54033it [00:20, 3362.06it/s]
54957it [00:19, 3460.08it/s]
54555it [00:19, 3502.94it/s]
54151it [00:20, 3425.78it/s]
54408it [00:20, 3472.63it/s]
55321it [00:19, 3511.06it/s]
54530it [00:20, 3528.45it/s]
54907it [00:19, 3407.80it/s]
54757it [00:20, 3319.54it/s]
55674it [00:19, 3393.07it/s]
55274it [00:19, 3482.58it/s]
54885it [00:20, 3427.52it/s]
55125it [00:20, 3421.97it/s]
56051it [00:20, 3500.78it/s]
55252it [00:20, 3495.90it/s]
55624it [00:20, 3372.70it/s]
55488it [00:20, 3479.83it/s]
56403it [00:20, 3410.10it/s]
55995it [00:20, 3468.78it/s]
55603it [00:20, 3360.50it/s]
55838it [00:20, 3361.38it/s]
56770it [00:20, 3483.23it/s]
55972it [00:20, 3454.03it/s]
56357it [00:20, 3382.75it/s]
56212it [00:20, 3469.77it/s]
57146it [00:20, 3561.37it/s]
56351it [00:20, 3550.61it/s]
56730it [00:20, 3481.11it/s]
56561it [00:20, 3322.65it/s]
57504it [00:20, 3456.82it/s]
57108it [00:20, 3567.17it/s]
56708it [00:20, 3443.34it/s]
56931it [00:20, 3427.26it/s]
57878it [00:20, 3536.64it/s]
57073it [00:20, 3501.70it/s]
57467it [00:20, 3455.47it/s]
57276it [00:20, 3350.01it/s]
58233it [00:20, 3422.57it/s]
57839it [00:20, 3528.97it/s]
57425it [00:20, 3412.62it/s]
57648it [00:21, 3454.46it/s]
58601it [00:20, 3495.61it/s]
57801it [00:21, 3510.92it/s]
58194it [00:20, 3397.26it/s]
58019it [00:21, 3526.52it/s]
58952it [00:20, 3387.15it/s]
58154it [00:21, 3413.28it/s]
58562it [00:20, 3477.18it/s]
58373it [00:21, 3345.68it/s]
59323it [00:21, 3478.87it/s]
58512it [00:21, 3461.24it/s]
58912it [00:21, 3371.18it/s]
58739it [00:21, 3434.42it/s]
59692it [00:21, 3538.93it/s]
59283it [00:21, 3466.65it/s]
58880it [00:21, 3361.17it/s]
59085it [00:21, 3329.94it/s]
60048it [00:21, 3422.60it/s]
59647it [00:21, 3512.18it/s]
59253it [00:21, 3465.85it/s]
59448it [00:21, 3413.80it/s]
60410it [00:21, 3477.73it/s]
59620it [00:21, 3522.34it/s]
60000it [00:21, 3407.12it/s]
59792it [00:21, 3312.27it/s]
60760it [00:21, 3391.09it/s]
60370it [00:21, 3489.68it/s]
59974it [00:21, 3390.69it/s]
60143it [00:21, 3367.39it/s]
61133it [00:21, 3487.71it/s]
60344it [00:21, 3477.65it/s]
60721it [00:21, 3394.81it/s]
60510it [00:21, 3454.90it/s]
61483it [00:21, 3386.95it/s]
61097it [00:21, 3497.46it/s]
60694it [00:21, 3388.20it/s]
60857it [00:22, 3350.17it/s]
61858it [00:21, 3489.67it/s]
61066it [00:22, 3480.82it/s]
61449it [00:21, 3393.10it/s]
61231it [00:22, 3461.41it/s]
62235it [00:21, 3569.25it/s]
61811it [00:21, 3456.15it/s]
61416it [00:22, 3356.42it/s]
61579it [00:22, 3346.35it/s]
62184it [00:21, 3535.60it/s]
61788it [00:22, 3459.14it/s]
61935it [00:22, 3407.10it/s]
62165it [00:22, 3548.58it/s]
62278it [00:22, 3319.94it/s]
62594it [00:23, 570.63it/s] 
62967it [00:23, 768.84it/s]
62539it [00:23, 561.75it/s] 
63283it [00:23, 965.45it/s]
62866it [00:23, 730.61it/s]
63649it [00:24, 1248.54it/s]
63176it [00:24, 924.36it/s]
63988it [00:24, 1513.16it/s]
63544it [00:24, 1212.71it/s]
64358it [00:24, 1853.66it/s]
63917it [00:24, 1539.84it/s]
64729it [00:24, 2191.61it/s]
64247it [00:24, 1799.02it/s]
65074it [00:24, 2405.00it/s]
62612it [00:24, 451.51it/s] 
64618it [00:24, 2143.74it/s]
65443it [00:24, 2692.00it/s]
62522it [00:24, 454.05it/s] 
62981it [00:24, 622.58it/s]
64956it [00:24, 2365.55it/s]
62895it [00:24, 620.12it/s]
65789it [00:24, 2813.85it/s]
63289it [00:24, 795.02it/s]
65323it [00:24, 2657.62it/s]
63213it [00:24, 792.92it/s]
66149it [00:24, 3011.69it/s]
63643it [00:25, 1045.04it/s]
65668it [00:24, 2777.19it/s]
63580it [00:25, 1045.95it/s]
66508it [00:24, 3050.96it/s]
63989it [00:25, 1307.32it/s]
66040it [00:24, 3014.65it/s]
63955it [00:25, 1347.51it/s]
66881it [00:24, 3232.65it/s]
64356it [00:25, 1637.50it/s]
66407it [00:25, 3186.40it/s]
64291it [00:25, 1608.55it/s]
67255it [00:25, 3371.51it/s]
64724it [00:25, 1977.36it/s]
66758it [00:25, 3182.63it/s]
64648it [00:25, 1927.53it/s]
65063it [00:25, 2207.90it/s]
67610it [00:25, 3312.47it/s]
67131it [00:25, 3333.60it/s]
64986it [00:25, 2172.77it/s]
67983it [00:25, 3428.59it/s]
65393it [00:25, 2401.93it/s]
65356it [00:25, 2492.77it/s]
67482it [00:25, 3242.80it/s]
68336it [00:25, 3344.75it/s]
65716it [00:25, 2558.86it/s]
67856it [00:25, 3380.92it/s]
65699it [00:25, 2648.67it/s]
68709it [00:25, 3452.80it/s]
66087it [00:25, 2839.12it/s]
66074it [00:25, 2917.14it/s]
68204it [00:25, 3304.43it/s]
66447it [00:25, 3035.85it/s]
69060it [00:25, 3358.54it/s]
66425it [00:25, 3068.69it/s]
68574it [00:25, 3415.53it/s]
66789it [00:26, 3067.15it/s]
69432it [00:25, 3460.08it/s]
68947it [00:25, 3504.29it/s]
66773it [00:26, 3101.78it/s]
67158it [00:26, 3236.00it/s]
69792it [00:25, 3499.49it/s]
67145it [00:26, 3268.77it/s]
69302it [00:25, 3399.20it/s]
67502it [00:26, 3193.28it/s]
70145it [00:25, 3394.87it/s]
69667it [00:25, 3470.29it/s]
67494it [00:26, 3234.31it/s]
67876it [00:26, 3345.44it/s]
70516it [00:26, 3483.17it/s]
67872it [00:26, 3385.17it/s]
70017it [00:26, 3381.11it/s]
68222it [00:26, 3263.78it/s]
70867it [00:26, 3381.08it/s]
70389it [00:26, 3477.16it/s]
68223it [00:26, 3279.85it/s]
68570it [00:26, 3323.83it/s]
71236it [00:26, 3468.99it/s]
70739it [00:26, 3380.74it/s]
68595it [00:26, 3402.10it/s]
68940it [00:26, 3431.37it/s]
71585it [00:26, 3369.65it/s]
71088it [00:26, 3410.82it/s]
68966it [00:26, 3487.86it/s]
69288it [00:26, 3326.53it/s]
71960it [00:26, 3477.09it/s]
71458it [00:26, 3494.95it/s]
69320it [00:26, 3391.56it/s]
69653it [00:26, 3417.75it/s]
72323it [00:26, 3520.99it/s]
71809it [00:26, 3391.24it/s]
69688it [00:26, 3471.80it/s]
69998it [00:26, 3327.27it/s]
72677it [00:26, 3418.57it/s]
72176it [00:26, 3471.34it/s]
70039it [00:26, 3386.33it/s]
70365it [00:27, 3424.95it/s]
73058it [00:26, 3531.60it/s]
72525it [00:26, 3368.15it/s]
70414it [00:27, 3488.15it/s]
70710it [00:27, 3297.76it/s]
73413it [00:26, 3365.26it/s]
72904it [00:26, 3489.29it/s]
70765it [00:27, 3388.72it/s]
71070it [00:27, 3383.84it/s]
73791it [00:26, 3482.76it/s]
71129it [00:27, 3457.39it/s]
73255it [00:27, 3376.83it/s]
71438it [00:27, 3467.01it/s]
74142it [00:27, 3403.69it/s]
71502it [00:27, 3535.73it/s]
73633it [00:27, 3491.76it/s]
71787it [00:27, 3356.28it/s]
74514it [00:27, 3493.55it/s]
74010it [00:27, 3571.95it/s]
71857it [00:27, 3425.16it/s]
72137it [00:27, 3395.62it/s]
74885it [00:27, 3555.22it/s]
72214it [00:27, 3465.65it/s]
74369it [00:27, 3448.03it/s]
72478it [00:27, 3295.31it/s]
75242it [00:27, 3429.50it/s]
74723it [00:27, 3472.52it/s]
72562it [00:27, 3366.05it/s]
72848it [00:27, 3410.03it/s]
75609it [00:27, 3496.46it/s]
72949it [00:27, 3502.38it/s]
75072it [00:27, 3364.39it/s]
73222it [00:27, 3504.61it/s]
75961it [00:27, 3383.11it/s]
75438it [00:27, 3447.75it/s]
73301it [00:27, 3393.43it/s]
73574it [00:27, 3395.58it/s]
76324it [00:27, 3453.11it/s]
73681it [00:28, 3508.70it/s]
75785it [00:27, 3351.66it/s]
73950it [00:28, 3498.60it/s]
76671it [00:27, 3357.72it/s]
74064it [00:28, 3600.64it/s]
76149it [00:27, 3433.11it/s]
77021it [00:27, 3397.13it/s]
74302it [00:28, 3339.63it/s]
76512it [00:27, 3490.00it/s]
74426it [00:28, 3462.48it/s]
77394it [00:28, 3493.03it/s]
74671it [00:28, 3437.25it/s]
76863it [00:28, 3372.89it/s]
74796it [00:28, 3529.93it/s]
77745it [00:28, 3382.51it/s]
75017it [00:28, 3324.00it/s]
77228it [00:28, 3451.44it/s]
75151it [00:28, 3380.67it/s]
78116it [00:28, 3475.90it/s]
75380it [00:28, 3409.62it/s]
77575it [00:28, 3355.89it/s]
75520it [00:28, 3466.90it/s]
75745it [00:28, 3478.49it/s]
78465it [00:28, 3380.20it/s]
77941it [00:28, 3441.77it/s]
75869it [00:28, 3371.44it/s]
78829it [00:28, 3453.91it/s]
76095it [00:28, 3304.56it/s]
78287it [00:28, 3352.47it/s]
76233it [00:28, 3446.12it/s]
79176it [00:28, 3361.79it/s]
76454it [00:28, 3383.28it/s]
78640it [00:28, 3402.69it/s]
76589it [00:28, 3344.20it/s]
79540it [00:28, 3440.96it/s]
76795it [00:28, 3288.53it/s]
79006it [00:28, 3477.04it/s]
76955it [00:28, 3433.89it/s]
79915it [00:28, 3528.43it/s]
77155it [00:29, 3375.29it/s]
79355it [00:28, 3366.02it/s]
77325it [00:29, 3510.68it/s]
80269it [00:28, 3408.45it/s]
77495it [00:29, 3291.38it/s]
79723it [00:28, 3455.58it/s]
77678it [00:29, 3395.84it/s]
80633it [00:28, 3474.25it/s]
77858it [00:29, 3387.54it/s]
80070it [00:29, 3357.33it/s]
78030it [00:29, 3428.89it/s]
78207it [00:29, 3416.08it/s]
80982it [00:29, 3368.59it/s]
80440it [00:29, 3455.77it/s]
78375it [00:29, 3346.55it/s]
81348it [00:29, 3446.96it/s]
78550it [00:29, 3320.37it/s]
80787it [00:29, 3365.63it/s]
78744it [00:29, 3443.93it/s]
78913it [00:29, 3408.95it/s]
81694it [00:29, 3349.63it/s]
81149it [00:29, 3437.84it/s]
79109it [00:29, 3343.70it/s]
82062it [00:29, 3444.43it/s]
79256it [00:29, 3298.73it/s]
81514it [00:29, 3497.18it/s]
79479it [00:29, 3443.03it/s]
82432it [00:29, 3517.51it/s]
79621it [00:29, 3398.80it/s]
81865it [00:29, 3384.94it/s]
79852it [00:29, 3525.63it/s]
82785it [00:29, 3407.09it/s]
79963it [00:29, 3268.58it/s]
82234it [00:29, 3472.32it/s]
80207it [00:29, 3405.62it/s]
83152it [00:29, 3482.11it/s]
80330it [00:30, 3381.92it/s]
82583it [00:29, 3328.99it/s]
80579it [00:30, 3493.10it/s]
83502it [00:29, 3375.08it/s]
80700it [00:30, 3471.39it/s]
82953it [00:29, 3433.73it/s]
80930it [00:30, 3388.05it/s]
83868it [00:29, 3455.13it/s]
81049it [00:30, 3348.81it/s]
83307it [00:29, 3333.11it/s]
81271it [00:30, 3374.70it/s]
84215it [00:30, 3339.31it/s]
81412it [00:30, 3427.66it/s]
83675it [00:30, 3429.90it/s]
81629it [00:30, 3292.44it/s]
84583it [00:30, 3435.68it/s]
81757it [00:30, 3313.79it/s]
84043it [00:30, 3500.34it/s]
81999it [00:30, 3408.03it/s]
84954it [00:30, 3514.34it/s]
82107it [00:30, 3366.79it/s]
84395it [00:30, 3387.55it/s]
82366it [00:30, 3483.17it/s]
85307it [00:30, 3392.54it/s]
82469it [00:30, 3276.96it/s]
84763it [00:30, 3470.56it/s]
82716it [00:30, 3381.60it/s]
85679it [00:30, 3484.11it/s]
82835it [00:30, 3384.35it/s]
85112it [00:30, 3363.11it/s]
83085it [00:30, 3469.83it/s]
83198it [00:30, 3453.50it/s]
85478it [00:30, 3446.94it/s]
83434it [00:30, 3364.47it/s]
83545it [00:30, 3339.43it/s]
85827it [00:30, 3342.75it/s]
83802it [00:30, 3452.45it/s]
83894it [00:31, 3382.43it/s]
84149it [00:31, 3345.58it/s]
84234it [00:31, 3292.17it/s]
84520it [00:31, 3449.07it/s]
84599it [00:31, 3393.83it/s]
84867it [00:31, 3437.77it/s]
84965it [00:31, 3470.00it/s]
85212it [00:31, 3329.08it/s]
85314it [00:31, 3342.22it/s]
85581it [00:31, 3432.10it/s]
85681it [00:31, 3435.69it/s]
86029it [00:33, 416.26it/s] 
86388it [00:33, 566.17it/s]
86738it [00:33, 746.23it/s]
86163it [00:33, 402.60it/s] 
87105it [00:33, 986.64it/s]
86529it [00:33, 555.25it/s]
87480it [00:33, 1277.00it/s]
86832it [00:33, 710.54it/s]
87812it [00:33, 1532.49it/s]
87201it [00:33, 955.48it/s]
88183it [00:33, 1872.16it/s]
87576it [00:33, 1249.22it/s]
88523it [00:33, 2122.93it/s]
87906it [00:33, 1506.93it/s]
88895it [00:33, 2449.66it/s]
85926it [00:34, 403.23it/s] 
88262it [00:33, 1827.15it/s]
89258it [00:34, 2628.53it/s]
86303it [00:34, 559.83it/s]
88596it [00:34, 2078.01it/s]
89624it [00:34, 2873.85it/s]
86671it [00:34, 754.20it/s]
88966it [00:34, 2409.93it/s]
86027it [00:34, 375.64it/s] 
89983it [00:34, 3054.66it/s]
86981it [00:34, 944.83it/s]
86396it [00:34, 519.41it/s]
89307it [00:34, 2582.40it/s]
90333it [00:34, 3031.98it/s]
87353it [00:34, 1234.71it/s]
89677it [00:34, 2850.53it/s]
86739it [00:34, 684.87it/s]
90688it [00:34, 3169.64it/s]
87681it [00:34, 1490.05it/s]
90040it [00:34, 3048.30it/s]
87102it [00:34, 910.13it/s]
91029it [00:34, 3130.50it/s]
88054it [00:34, 1838.28it/s]
87472it [00:34, 1184.91it/s]
90389it [00:34, 3058.02it/s]
91386it [00:34, 3250.28it/s]
88410it [00:34, 2151.15it/s]
87802it [00:35, 1433.84it/s]
90745it [00:34, 3191.63it/s]
91743it [00:34, 3339.73it/s]
88754it [00:34, 2369.41it/s]
88152it [00:35, 1740.95it/s]
91088it [00:34, 3141.72it/s]
92087it [00:34, 3246.00it/s]
89126it [00:35, 2671.38it/s]
88484it [00:35, 1996.17it/s]
91444it [00:34, 3256.52it/s]
92445it [00:34, 3338.80it/s]
89472it [00:35, 2779.38it/s]
88850it [00:35, 2325.59it/s]
91782it [00:35, 3086.74it/s]
92785it [00:35, 3246.25it/s]
89844it [00:35, 3014.58it/s]
89216it [00:35, 2619.13it/s]
92137it [00:35, 3213.64it/s]
93144it [00:35, 3343.56it/s]
90190it [00:35, 3032.98it/s]
89562it [00:35, 2739.52it/s]
92493it [00:35, 3310.77it/s]
93482it [00:35, 3225.49it/s]
90546it [00:35, 3173.41it/s]
89910it [00:35, 2923.24it/s]
92831it [00:35, 3219.78it/s]
93840it [00:35, 3325.28it/s]
90903it [00:35, 3280.98it/s]
90249it [00:35, 2954.30it/s]
93188it [00:35, 3318.10it/s]
94197it [00:35, 3393.49it/s]
91249it [00:35, 3195.23it/s]
90600it [00:35, 3102.50it/s]
93524it [00:35, 3234.62it/s]
94539it [00:35, 3286.07it/s]
91588it [00:35, 3247.40it/s]
90939it [00:35, 3063.52it/s]
93878it [00:35, 3317.97it/s]
94895it [00:35, 3363.76it/s]
91922it [00:35, 3164.27it/s]
91292it [00:36, 3190.20it/s]
94234it [00:35, 3387.90it/s]
95234it [00:35, 3255.55it/s]
92279it [00:36, 3278.68it/s]
91645it [00:36, 3284.46it/s]
94575it [00:35, 3283.39it/s]
95590it [00:35, 3341.22it/s]
92619it [00:36, 3188.16it/s]
91984it [00:36, 3153.56it/s]
94928it [00:35, 3350.16it/s]
95946it [00:36, 3402.02it/s]
92977it [00:36, 3296.61it/s]
92337it [00:36, 3257.71it/s]
95265it [00:36, 3226.05it/s]
96288it [00:36, 3284.78it/s]
93335it [00:36, 3376.81it/s]
92669it [00:36, 3163.82it/s]
95619it [00:36, 3313.72it/s]
96644it [00:36, 3363.42it/s]
93676it [00:36, 3258.00it/s]
93024it [00:36, 3271.09it/s]
95974it [00:36, 3380.65it/s]
96982it [00:36, 3230.08it/s]
94031it [00:36, 3341.41it/s]
93378it [00:36, 3346.87it/s]
96314it [00:36, 3267.06it/s]
97340it [00:36, 3329.06it/s]
94368it [00:36, 3209.22it/s]
93716it [00:36, 3190.38it/s]
96669it [00:36, 3347.50it/s]
97675it [00:36, 3232.48it/s]
94723it [00:36, 3303.70it/s]
94068it [00:36, 3281.85it/s]
97006it [00:36, 3242.91it/s]
98032it [00:36, 3328.65it/s]
95079it [00:36, 3377.44it/s]
94400it [00:37, 3186.70it/s]
97362it [00:36, 3332.59it/s]
98388it [00:36, 3394.68it/s]
95419it [00:36, 3263.29it/s]
94753it [00:37, 3282.45it/s]
97697it [00:36, 3230.95it/s]
98729it [00:36, 3277.24it/s]
95771it [00:37, 3336.22it/s]
95105it [00:37, 3348.51it/s]
98053it [00:36, 3324.07it/s]
99086it [00:36, 3359.38it/s]
96107it [00:37, 3232.57it/s]
95442it [00:37, 3222.08it/s]
98408it [00:37, 3389.00it/s]
99424it [00:37, 3261.05it/s]
96464it [00:37, 3327.35it/s]
95777it [00:37, 3257.76it/s]
98749it [00:37, 3246.91it/s]
99780it [00:37, 3346.54it/s]
96820it [00:37, 3219.72it/s]
96105it [00:37, 3167.39it/s]
99105it [00:37, 3334.93it/s]
100135it [00:37, 3405.29it/s]
97166it [00:37, 3286.22it/s]
96458it [00:37, 3270.23it/s]
99441it [00:37, 3240.17it/s]
100477it [00:37, 3262.29it/s]
97521it [00:37, 3360.80it/s]
96810it [00:37, 3341.20it/s]
99767it [00:37, 3214.34it/s]
100834it [00:37, 3348.19it/s]
97859it [00:37, 3250.61it/s]
97146it [00:37, 3216.78it/s]
100118it [00:37, 3297.85it/s]
101171it [00:37, 3246.17it/s]
98215it [00:37, 3336.64it/s]
97500it [00:37, 3308.99it/s]
100449it [00:37, 3211.96it/s]
101525it [00:37, 3328.55it/s]
98551it [00:37, 3223.32it/s]
97833it [00:38, 3159.55it/s]
100804it [00:37, 3308.24it/s]
101860it [00:37, 3226.37it/s]
98908it [00:38, 3320.04it/s]
98184it [00:38, 3258.29it/s]
101136it [00:37, 3209.48it/s]
102217it [00:37, 3323.14it/s]
99266it [00:38, 3392.97it/s]
98512it [00:38, 3156.90it/s]
101487it [00:37, 3295.13it/s]
102573it [00:38, 3391.24it/s]
99607it [00:38, 3264.29it/s]
98865it [00:38, 3262.07it/s]
101840it [00:38, 3363.41it/s]
102914it [00:38, 3275.35it/s]
99953it [00:38, 3319.08it/s]
99218it [00:38, 3338.66it/s]
102178it [00:38, 3224.86it/s]
103271it [00:38, 3358.28it/s]
100287it [00:38, 3207.12it/s]
102533it [00:38, 3317.92it/s]
99554it [00:38, 3181.53it/s]
103609it [00:38, 3223.22it/s]
100645it [00:38, 3312.94it/s]
99907it [00:38, 3278.54it/s]
102867it [00:38, 3211.45it/s]
103965it [00:38, 3316.88it/s]
101001it [00:38, 3382.84it/s]
103222it [00:38, 3307.31it/s]
100238it [00:38, 3177.46it/s]
104321it [00:38, 3385.10it/s]
101341it [00:38, 3259.26it/s]
100592it [00:38, 3279.80it/s]
103555it [00:38, 3208.02it/s]
104662it [00:38, 3273.85it/s]
101695it [00:38, 3339.32it/s]
100944it [00:39, 3347.56it/s]
103910it [00:38, 3305.59it/s]
105017it [00:38, 3351.99it/s]
102031it [00:39, 3224.21it/s]
104265it [00:38, 3376.32it/s]
101281it [00:39, 3222.32it/s]
105354it [00:38, 3253.09it/s]
102378it [00:39, 3285.12it/s]
101613it [00:39, 3248.54it/s]
104604it [00:38, 3258.99it/s]
105708it [00:38, 3328.37it/s]
102708it [00:39, 3182.33it/s]
104958it [00:39, 3337.29it/s]
101940it [00:39, 3152.39it/s]
106058it [00:39, 3230.74it/s]
103065it [00:39, 3292.45it/s]
102292it [00:39, 3257.54it/s]
105294it [00:39, 3224.44it/s]
106414it [00:39, 3323.35it/s]
103421it [00:39, 3367.88it/s]
102643it [00:39, 3328.53it/s]
105638it [00:39, 3284.62it/s]
106773it [00:39, 3399.27it/s]
103760it [00:39, 3237.24it/s]
105987it [00:39, 3343.29it/s]
102978it [00:39, 3217.03it/s]
107115it [00:39, 3256.87it/s]
104117it [00:39, 3331.88it/s]
103311it [00:39, 3247.82it/s]
106323it [00:39, 3226.53it/s]
107473it [00:39, 3347.94it/s]
104452it [00:39, 3215.55it/s]
106661it [00:39, 3269.96it/s]
103637it [00:39, 3146.01it/s]
107810it [00:39, 3253.43it/s]
104809it [00:39, 3316.35it/s]
103990it [00:39, 3254.49it/s]
106990it [00:39, 3174.05it/s]
108168it [00:39, 3346.47it/s]
105153it [00:39, 3350.35it/s]
104341it [00:40, 3328.13it/s]
107343it [00:39, 3274.95it/s]
108526it [00:39, 3411.64it/s]
105490it [00:40, 3236.74it/s]
107695it [00:39, 3344.14it/s]
104676it [00:40, 3210.41it/s]
108869it [00:39, 3301.26it/s]
105844it [00:40, 3321.93it/s]
105025it [00:40, 3290.68it/s]
108031it [00:39, 3235.99it/s]
109227it [00:40, 3380.19it/s]
106178it [00:40, 3211.75it/s]
108386it [00:40, 3326.28it/s]
105356it [00:40, 3144.38it/s]
109567it [00:40, 3273.75it/s]
106534it [00:40, 3310.28it/s]
105708it [00:40, 3248.61it/s]
108720it [00:40, 3230.26it/s]
109924it [00:40, 3357.25it/s]
106890it [00:40, 3382.04it/s]
109067it [00:40, 3298.46it/s]
106059it [00:40, 3322.58it/s]
110262it [00:40, 3255.17it/s]
107230it [00:40, 3257.36it/s]
106393it [00:40, 3205.93it/s]
109417it [00:40, 3211.51it/s]
110609it [00:40, 3316.39it/s]
107587it [00:40, 3345.26it/s]
106747it [00:40, 3299.27it/s]
109767it [00:40, 3292.46it/s]
110965it [00:40, 3385.91it/s]
107924it [00:40, 3196.08it/s]
110121it [00:40, 3363.10it/s]
107079it [00:40, 3148.82it/s]
111305it [00:40, 3282.77it/s]
108281it [00:40, 3300.40it/s]
110459it [00:40, 3257.11it/s]
107430it [00:41, 3249.57it/s]
111658it [00:40, 3352.79it/s]
108614it [00:41, 3203.29it/s]
110815it [00:40, 3343.51it/s]
107758it [00:41, 3158.70it/s]
111995it [00:40, 3259.60it/s]
108973it [00:41, 3312.58it/s]
111151it [00:40, 3239.30it/s]
108112it [00:41, 3265.55it/s]
112353it [00:40, 3351.81it/s]
109328it [00:41, 3381.17it/s]
111507it [00:41, 3328.75it/s]
108464it [00:41, 3337.60it/s]
112708it [00:41, 3408.34it/s]
109668it [00:41, 3256.88it/s]
111859it [00:41, 3382.79it/s]
108800it [00:41, 3227.15it/s]
113050it [00:41, 3296.16it/s]
110024it [00:41, 3342.83it/s]
112199it [00:41, 3269.99it/s]
109140it [00:41, 3266.71it/s]
113408it [00:41, 3369.49it/s]
110360it [00:41, 3192.24it/s]
112545it [00:41, 3322.58it/s]
109468it [00:41, 3174.67it/s]
113747it [00:41, 3269.26it/s]
110718it [00:41, 3300.29it/s]
112879it [00:41, 3224.81it/s]
109818it [00:41, 3266.34it/s]
114098it [00:41, 3336.04it/s]
111072it [00:41, 3367.14it/s]
113236it [00:41, 3322.26it/s]
110169it [00:41, 3336.32it/s]
114458it [00:41, 3249.43it/s]
111411it [00:41, 3248.10it/s]
113591it [00:41, 3388.19it/s]
110504it [00:41, 3212.49it/s]
114814it [00:41, 3337.02it/s]
111766it [00:41, 3333.94it/s]
113931it [00:41, 3270.08it/s]
110856it [00:42, 3299.65it/s]
115173it [00:41, 3408.05it/s]
112102it [00:42, 3229.14it/s]
114287it [00:41, 3352.61it/s]
111188it [00:42, 3152.82it/s]
112461it [00:42, 3329.80it/s]
114624it [00:41, 3246.42it/s]
111539it [00:42, 3253.11it/s]
112796it [00:42, 3213.48it/s]
114979it [00:42, 3332.27it/s]
111891it [00:42, 3328.89it/s]
113137it [00:42, 3268.51it/s]
115314it [00:42, 3219.24it/s]
112226it [00:42, 3212.28it/s]
113493it [00:42, 3352.49it/s]
112579it [00:42, 3301.01it/s]
113830it [00:42, 3219.34it/s]
112911it [00:42, 3158.82it/s]
114189it [00:42, 3324.32it/s]
113265it [00:42, 3265.01it/s]
114524it [00:42, 3217.16it/s]
113617it [00:42, 3337.00it/s]
114879it [00:42, 3308.51it/s]
113953it [00:43, 3221.08it/s]
115237it [00:43, 3386.13it/s]
114308it [00:43, 3312.92it/s]
114642it [00:43, 3203.68it/s]
114978it [00:43, 3248.20it/s]
115305it [00:43, 3161.19it/s]
115516it [00:45, 317.09it/s] 
115877it [00:45, 439.67it/s]
116208it [00:45, 581.85it/s]
116571it [00:45, 785.97it/s]
116933it [00:45, 1033.26it/s]
115638it [00:45, 299.82it/s] 
117258it [00:45, 1263.64it/s]
115990it [00:45, 417.65it/s]
117616it [00:45, 1576.76it/s]
116278it [00:45, 539.91it/s]
117944it [00:45, 1828.82it/s]
116632it [00:45, 738.59it/s]
118301it [00:46, 2152.17it/s]
116987it [00:46, 981.48it/s]
118658it [00:46, 2445.73it/s]
117301it [00:46, 1209.99it/s]
118997it [00:46, 2593.10it/s]
117654it [00:46, 1522.44it/s]
119354it [00:46, 2828.42it/s]
117976it [00:46, 1772.15it/s]
119691it [00:46, 2880.05it/s]
118330it [00:46, 2099.26it/s]
120048it [00:46, 3059.91it/s]
115578it [00:46, 284.77it/s] 
118671it [00:46, 2372.88it/s]
120406it [00:46, 3199.52it/s]
115934it [00:46, 395.77it/s]
119002it [00:46, 2522.08it/s]
116228it [00:47, 513.21it/s]
120748it [00:46, 3128.24it/s]
119352it [00:46, 2758.14it/s]
116589it [00:47, 704.76it/s]
121105it [00:46, 3248.94it/s]
119681it [00:46, 2807.72it/s]
115623it [00:47, 269.72it/s] 
116935it [00:47, 928.40it/s]
121442it [00:47, 3179.15it/s]
120033it [00:47, 2993.08it/s]
115966it [00:47, 376.29it/s]
117252it [00:47, 1154.15it/s]
121799it [00:47, 3287.53it/s]
120387it [00:47, 3140.26it/s]
116260it [00:47, 494.10it/s]
117609it [00:47, 1463.53it/s]
122135it [00:47, 3202.37it/s]
120723it [00:47, 3091.35it/s]
116620it [00:47, 685.35it/s]
117934it [00:47, 1719.22it/s]
122496it [00:47, 3316.44it/s]
121075it [00:47, 3209.74it/s]
116979it [00:47, 919.50it/s]
118291it [00:47, 2051.31it/s]
122854it [00:47, 3391.71it/s]
121408it [00:47, 3136.08it/s]
117294it [00:47, 1143.57it/s]
118649it [00:47, 2363.19it/s]
123197it [00:47, 3279.17it/s]
121758it [00:47, 3218.17it/s]
117645it [00:47, 1446.76it/s]
118987it [00:47, 2518.22it/s]
123554it [00:47, 3361.69it/s]
122087it [00:47, 3137.38it/s]
117967it [00:48, 1698.75it/s]
119342it [00:47, 2764.15it/s]
123893it [00:47, 3225.92it/s]
122441it [00:47, 3249.00it/s]
118322it [00:48, 2030.27it/s]
119677it [00:48, 2796.83it/s]
124249it [00:47, 3319.33it/s]
122793it [00:47, 3326.86it/s]
118678it [00:48, 2341.84it/s]
120033it [00:48, 2993.45it/s]
124605it [00:47, 3388.02it/s]
123129it [00:47, 3220.79it/s]
119014it [00:48, 2498.72it/s]
120390it [00:48, 3148.50it/s]
124946it [00:48, 3274.93it/s]
123480it [00:48, 3303.53it/s]
119359it [00:48, 2724.15it/s]
120729it [00:48, 3102.63it/s]
125301it [00:48, 3353.70it/s]
123813it [00:48, 3191.56it/s]
119690it [00:48, 2794.52it/s]
121085it [00:48, 3227.87it/s]
125639it [00:48, 3250.65it/s]
124165it [00:48, 3283.86it/s]
120047it [00:48, 2996.20it/s]
121421it [00:48, 3151.37it/s]
125998it [00:48, 3346.58it/s]
124517it [00:48, 3351.46it/s]
120405it [00:48, 3152.91it/s]
121777it [00:48, 3265.07it/s]
126335it [00:48, 3246.95it/s]
124854it [00:48, 3233.30it/s]
120745it [00:48, 3108.11it/s]
122111it [00:48, 3145.97it/s]
126690it [00:48, 3333.19it/s]
125194it [00:48, 3279.22it/s]
121092it [00:48, 3208.51it/s]
122471it [00:48, 3271.58it/s]
127044it [00:48, 3390.94it/s]
125524it [00:48, 3173.42it/s]
121426it [00:49, 3141.96it/s]
122829it [00:49, 3357.92it/s]
127385it [00:48, 3253.97it/s]
125876it [00:48, 3272.10it/s]
121779it [00:49, 3249.13it/s]
123169it [00:49, 3238.70it/s]
127741it [00:48, 3341.37it/s]
126227it [00:48, 3339.38it/s]
122111it [00:49, 3160.90it/s]
123525it [00:49, 3328.64it/s]
128077it [00:49, 3240.80it/s]
126563it [00:49, 3227.55it/s]
122470it [00:49, 3280.91it/s]
123861it [00:49, 3216.92it/s]
128436it [00:49, 3338.74it/s]
126914it [00:49, 3306.80it/s]
122827it [00:49, 3362.39it/s]
124218it [00:49, 3315.40it/s]
128796it [00:49, 3413.62it/s]
127247it [00:49, 3201.16it/s]
123167it [00:49, 3223.82it/s]
124576it [00:49, 3389.71it/s]
129139it [00:49, 3300.40it/s]
127602it [00:49, 3300.31it/s]
123524it [00:49, 3319.79it/s]
129497it [00:49, 3379.63it/s]
124917it [00:49, 3237.13it/s]
127956it [00:49, 3369.70it/s]
123859it [00:49, 3210.72it/s]
125273it [00:49, 3327.52it/s]
129837it [00:49, 3270.43it/s]
128295it [00:49, 3139.07it/s]
124215it [00:49, 3308.89it/s]
130196it [00:49, 3359.62it/s]
125608it [00:49, 3220.00it/s]
128644it [00:49, 3235.52it/s]
124570it [00:50, 3378.42it/s]
125969it [00:49, 3330.68it/s]
130534it [00:49, 3254.23it/s]
128971it [00:49, 3165.45it/s]
124910it [00:50, 3108.79it/s]
130881it [00:49, 3315.50it/s]
126305it [00:50, 3222.45it/s]
129330it [00:49, 3285.44it/s]
125262it [00:50, 3220.60it/s]
131239it [00:49, 3390.76it/s]
126661it [00:50, 3316.72it/s]
129661it [00:49, 3194.53it/s]
125589it [00:50, 3138.62it/s]
127016it [00:50, 3383.37it/s]
131580it [00:50, 3275.98it/s]
130021it [00:50, 3308.65it/s]
125947it [00:50, 3261.63it/s]
131935it [00:50, 3354.68it/s]
127356it [00:50, 3264.62it/s]
130381it [00:50, 3392.34it/s]
126289it [00:50, 3180.94it/s]
127702it [00:50, 3319.30it/s]
132272it [00:50, 3250.20it/s]
130722it [00:50, 3276.86it/s]
126643it [00:50, 3281.60it/s]
132631it [00:50, 3346.70it/s]
128036it [00:50, 3222.14it/s]
131081it [00:50, 3365.90it/s]
126997it [00:50, 3354.83it/s]
132990it [00:50, 3415.78it/s]
128391it [00:50, 3314.88it/s]
131420it [00:50, 3252.69it/s]
127335it [00:50, 3249.52it/s]
128751it [00:50, 3395.70it/s]
133333it [00:50, 3294.45it/s]
131777it [00:50, 3342.07it/s]
127689it [00:50, 3328.72it/s]
133692it [00:50, 3377.81it/s]
129092it [00:50, 3273.96it/s]
132123it [00:50, 3376.12it/s]
128024it [00:51, 3224.05it/s]
129449it [00:51, 3358.61it/s]
134032it [00:50, 3270.67it/s]
132462it [00:50, 3262.82it/s]
128359it [00:51, 3259.86it/s]
134382it [00:50, 3334.35it/s]
129787it [00:51, 3250.28it/s]
132821it [00:50, 3355.76it/s]
128715it [00:51, 3345.50it/s]
130134it [00:51, 3311.83it/s]
134717it [00:51, 3234.66it/s]
133159it [00:51, 3248.93it/s]
129051it [00:51, 3232.44it/s]
135078it [00:51, 3341.84it/s]
130489it [00:51, 3214.03it/s]
133519it [00:51, 3347.04it/s]
129407it [00:51, 3325.38it/s]
135442it [00:51, 3426.55it/s]
130846it [00:51, 3314.48it/s]
133856it [00:51, 3246.08it/s]
129741it [00:51, 3217.38it/s]
131205it [00:51, 3392.78it/s]
135786it [00:51, 3308.50it/s]
134216it [00:51, 3345.85it/s]
130099it [00:51, 3318.72it/s]
136148it [00:51, 3395.50it/s]
131546it [00:51, 3267.95it/s]
134574it [00:51, 3412.46it/s]
130440it [00:51, 3343.07it/s]
131901it [00:51, 3348.17it/s]
136490it [00:51, 3280.09it/s]
134917it [00:51, 3289.45it/s]
130776it [00:51, 3224.30it/s]
136851it [00:51, 3372.95it/s]
132238it [00:51, 3243.15it/s]
135277it [00:51, 3377.50it/s]
131131it [00:52, 3316.83it/s]
132594it [00:51, 3331.86it/s]
137208it [00:51, 3265.67it/s]
135617it [00:51, 3248.91it/s]
131465it [00:52, 3212.66it/s]
132941it [00:52, 3370.27it/s]
137569it [00:51, 3363.02it/s]
135978it [00:51, 3349.65it/s]
131819it [00:52, 3300.49it/s]
137920it [00:51, 3403.92it/s]
133280it [00:52, 3258.94it/s]
136337it [00:51, 3419.11it/s]
132169it [00:52, 3169.00it/s]
133639it [00:52, 3345.62it/s]
138262it [00:52, 3292.77it/s]
136681it [00:52, 3299.22it/s]
132525it [00:52, 3277.57it/s]
138620it [00:52, 3373.10it/s]
133975it [00:52, 3248.78it/s]
137041it [00:52, 3385.39it/s]
132882it [00:52, 3359.65it/s]
134335it [00:52, 3347.13it/s]
138959it [00:52, 3268.89it/s]
137382it [00:52, 3272.28it/s]
133220it [00:52, 3242.09it/s]
139323it [00:52, 3372.77it/s]
134689it [00:52, 3235.46it/s]
137741it [00:52, 3362.62it/s]
133578it [00:52, 3336.76it/s]
139684it [00:52, 3439.99it/s]
135049it [00:52, 3338.14it/s]
138079it [00:52, 3261.59it/s]
133914it [00:52, 3238.12it/s]
135411it [00:52, 3418.92it/s]
140030it [00:52, 3318.48it/s]
138439it [00:52, 3357.33it/s]
134269it [00:52, 3325.45it/s]
140389it [00:52, 3395.58it/s]
135755it [00:52, 3265.62it/s]
138799it [00:52, 3425.79it/s]
134612it [00:53, 3355.66it/s]
140731it [00:52, 3282.54it/s]
136117it [00:53, 3363.94it/s]
139143it [00:52, 3275.27it/s]
134949it [00:53, 3244.52it/s]
141092it [00:52, 3375.13it/s]
136456it [00:53, 3252.02it/s]
139504it [00:52, 3369.68it/s]
135308it [00:53, 3342.87it/s]
141432it [00:53, 3244.96it/s]
136818it [00:53, 3354.40it/s]
139843it [00:53, 3264.78it/s]
135644it [00:53, 3240.36it/s]
141792it [00:53, 3345.63it/s]
137179it [00:53, 3426.24it/s]
140203it [00:53, 3359.49it/s]
136003it [00:53, 3339.01it/s]
142154it [00:53, 3424.24it/s]
137524it [00:53, 3302.80it/s]
140562it [00:53, 3425.49it/s]
136360it [00:53, 3403.72it/s]
142499it [00:53, 3304.03it/s]
137885it [00:53, 3388.25it/s]
140906it [00:53, 3303.24it/s]
136702it [00:53, 3237.32it/s]
142857it [00:53, 3381.64it/s]
138226it [00:53, 3250.61it/s]
141267it [00:53, 3382.62it/s]
137060it [00:53, 3333.07it/s]
143197it [00:53, 3272.00it/s]
138586it [00:53, 3347.99it/s]
141607it [00:53, 3279.09it/s]
137396it [00:53, 3227.68it/s]
143559it [00:53, 3369.05it/s]
138923it [00:53, 3244.58it/s]
141967it [00:53, 3363.73it/s]
137753it [00:54, 3324.50it/s]
143919it [00:53, 3435.26it/s]
139284it [00:54, 3348.05it/s]
142305it [00:53, 3264.58it/s]
138088it [00:54, 3227.41it/s]
144264it [00:53, 3307.51it/s]
139645it [00:54, 3422.48it/s]
142653it [00:53, 3325.59it/s]
138445it [00:54, 3325.07it/s]
144628it [00:53, 3397.60it/s]
139989it [00:54, 3300.30it/s]
143010it [00:53, 3394.57it/s]
138800it [00:54, 3388.77it/s]
144970it [00:54, 3275.31it/s]
140349it [00:54, 3385.06it/s]
143351it [00:54, 3281.85it/s]
139141it [00:54, 3224.07it/s]
145329it [00:54, 3362.95it/s]
140690it [00:54, 3267.94it/s]
143712it [00:54, 3374.46it/s]
139501it [00:54, 3328.86it/s]
145667it [00:54, 3280.45it/s]
141040it [00:54, 3333.53it/s]
144051it [00:54, 3268.95it/s]
139837it [00:54, 3228.64it/s]
146030it [00:54, 3379.53it/s]
141402it [00:54, 3414.19it/s]
144411it [00:54, 3361.46it/s]
140195it [00:54, 3327.32it/s]
146391it [00:54, 3443.96it/s]
141745it [00:54, 3289.21it/s]
144767it [00:54, 3273.85it/s]
140553it [00:54, 3398.64it/s]
146737it [00:54, 3321.86it/s]
142106it [00:54, 3380.49it/s]
145129it [00:54, 3370.59it/s]
140895it [00:54, 3276.02it/s]
147100it [00:54, 3408.94it/s]
142446it [00:54, 3267.67it/s]
145492it [00:54, 3444.90it/s]
141238it [00:55, 3319.97it/s]
147443it [00:54, 3301.89it/s]
142802it [00:55, 3350.39it/s]
145838it [00:54, 3330.22it/s]
141572it [00:55, 3220.28it/s]
147806it [00:54, 3395.20it/s]
143139it [00:55, 3244.20it/s]
146189it [00:54, 3379.63it/s]
141930it [00:55, 3322.30it/s]
148148it [00:55, 3284.25it/s]
143500it [00:55, 3347.81it/s]
146529it [00:55, 3271.02it/s]
142264it [00:55, 3221.52it/s]
148500it [00:55, 3351.38it/s]
143850it [00:55, 3390.47it/s]
146884it [00:55, 3349.10it/s]
142621it [00:55, 3319.66it/s]
148861it [00:55, 3425.19it/s]
144191it [00:55, 3271.39it/s]
147246it [00:55, 3425.21it/s]
142978it [00:55, 3392.02it/s]
149205it [00:55, 3314.07it/s]
144554it [00:55, 3372.69it/s]
147590it [00:55, 3313.42it/s]
143319it [00:55, 3267.32it/s]
149569it [00:55, 3405.74it/s]
144893it [00:55, 3265.59it/s]
147951it [00:55, 3398.37it/s]
143661it [00:55, 3308.91it/s]
149911it [00:55, 3297.76it/s]
145255it [00:55, 3366.30it/s]
148293it [00:55, 3291.36it/s]
143994it [00:55, 3217.48it/s]
150274it [00:55, 3391.62it/s]
145609it [00:55, 3265.92it/s]
148652it [00:55, 3376.35it/s]
144349it [00:56, 3312.16it/s]
150636it [00:55, 3457.25it/s]
145974it [00:56, 3372.60it/s]
148992it [00:55, 3275.72it/s]
144713it [00:56, 3405.15it/s]
150983it [00:55, 3330.34it/s]
146324it [00:56, 3406.93it/s]
149352it [00:55, 3368.43it/s]
145055it [00:56, 3283.71it/s]
151343it [00:55, 3406.08it/s]
146667it [00:56, 3285.99it/s]
149704it [00:55, 3411.60it/s]
145414it [00:56, 3370.46it/s]
151686it [00:56, 3296.16it/s]
147031it [00:56, 3385.96it/s]
150047it [00:56, 3298.10it/s]
145753it [00:56, 3225.86it/s]
152040it [00:56, 3365.38it/s]
147372it [00:56, 3279.84it/s]
150408it [00:56, 3387.56it/s]
146115it [00:56, 3336.72it/s]
152378it [00:56, 3270.58it/s]
147735it [00:56, 3379.39it/s]
150749it [00:56, 3283.80it/s]
146451it [00:56, 3228.38it/s]
148098it [00:56, 3449.42it/s]
151109it [00:56, 3372.07it/s]
146812it [00:56, 3334.67it/s]
148445it [00:56, 3316.01it/s]
151469it [00:56, 3436.03it/s]
147169it [00:56, 3401.87it/s]
148804it [00:56, 3393.68it/s]
151814it [00:56, 3313.84it/s]
147511it [00:56, 3285.90it/s]
152177it [00:56, 3402.40it/s]
149146it [00:56, 3248.45it/s]
147870it [00:57, 3373.10it/s]
149508it [00:57, 3353.47it/s]
148209it [00:57, 3216.36it/s]
149846it [00:57, 3251.88it/s]
148567it [00:57, 3318.79it/s]
150209it [00:57, 3357.97it/s]
148924it [00:57, 3391.05it/s]
150571it [00:57, 3433.19it/s]
149265it [00:57, 3273.00it/s]
150916it [00:57, 3297.25it/s]
149625it [00:57, 3365.82it/s]
151277it [00:57, 3384.84it/s]
149964it [00:57, 3254.83it/s]
151618it [00:57, 3237.19it/s]
150306it [00:57, 3299.66it/s]
151979it [00:57, 3339.76it/s]
150649it [00:57, 3211.40it/s]
152329it [00:57, 3236.77it/s]
151006it [00:58, 3312.26it/s]
151363it [00:58, 3386.45it/s]
151704it [00:58, 3267.13it/s]
152065it [00:58, 3364.50it/s]
152707it [01:00, 245.47it/s] 
153072it [01:00, 346.94it/s]
153374it [01:00, 456.10it/s]
153737it [01:01, 631.02it/s]
154077it [01:01, 826.99it/s]
152519it [01:01, 251.46it/s] 
154442it [01:01, 1091.71it/s]
152882it [01:01, 352.02it/s]
154807it [01:01, 1393.73it/s]
153236it [01:01, 478.93it/s]
155144it [01:01, 1654.23it/s]
153585it [01:01, 643.65it/s]
155508it [01:01, 1989.61it/s]
153947it [01:01, 859.36it/s]
155846it [01:01, 2215.84it/s]
154269it [01:01, 1075.69it/s]
156211it [01:01, 2521.36it/s]
154633it [01:01, 1377.41it/s]
156562it [01:01, 2752.24it/s]
154965it [01:01, 1635.51it/s]
156905it [01:01, 2836.83it/s]
155331it [01:01, 1976.84it/s]
157272it [01:02, 3050.80it/s]
155692it [01:02, 2293.46it/s]
157615it [01:02, 3066.80it/s]
156036it [01:02, 2482.09it/s]
157980it [01:02, 3224.97it/s]
156399it [01:02, 2748.15it/s]
158323it [01:02, 3179.45it/s]
156741it [01:02, 2837.46it/s]
158691it [01:02, 3317.27it/s]
157107it [01:02, 3048.08it/s]
152655it [01:02, 224.59it/s] 
159061it [01:02, 3425.53it/s]
157449it [01:02, 3064.05it/s]
153017it [01:02, 316.81it/s]
159412it [01:02, 3331.70it/s]
157799it [01:02, 3182.02it/s]
153312it [01:02, 415.13it/s]
152404it [01:03, 234.18it/s] 
159781it [01:02, 3431.85it/s]
158164it [01:02, 3311.13it/s]
153675it [01:03, 577.91it/s]
152765it [01:03, 328.77it/s]
160129it [01:02, 3287.73it/s]
158510it [01:02, 3251.29it/s]
154041it [01:03, 785.83it/s]
153127it [01:03, 455.41it/s]
160496it [01:02, 3394.26it/s]
158877it [01:02, 3368.68it/s]
154363it [01:03, 993.84it/s]
153429it [01:03, 588.71it/s]
160840it [01:03, 3303.97it/s]
159222it [01:03, 3298.91it/s]
154717it [01:03, 1276.21it/s]
153780it [01:03, 790.90it/s]
161210it [01:03, 3414.77it/s]
159588it [01:03, 3401.48it/s]
155044it [01:03, 1533.24it/s]
154096it [01:03, 999.52it/s]
161573it [01:03, 3475.05it/s]
159955it [01:03, 3476.94it/s]
155407it [01:03, 1871.29it/s]
154460it [01:03, 1298.84it/s]
161923it [01:03, 3376.84it/s]
160307it [01:03, 3352.14it/s]
154823it [01:03, 1623.72it/s]
155757it [01:03, 2120.60it/s]
162293it [01:03, 3469.82it/s]
160674it [01:03, 3442.96it/s]
156120it [01:03, 2433.09it/s]
155161it [01:03, 1880.74it/s]
162642it [01:03, 3375.86it/s]
161021it [01:03, 3336.36it/s]
156484it [01:03, 2708.67it/s]
155512it [01:03, 2188.50it/s]
163012it [01:03, 3467.96it/s]
161390it [01:03, 3435.67it/s]
155847it [01:04, 2382.52it/s]
156829it [01:04, 2801.26it/s]
163361it [01:03, 3370.91it/s]
161736it [01:03, 3297.69it/s]
156210it [01:04, 2666.97it/s]
157195it [01:04, 3018.04it/s]
163734it [01:03, 3472.00it/s]
162108it [01:03, 3417.28it/s]
156572it [01:04, 2901.78it/s]
157538it [01:04, 3012.49it/s]
164095it [01:04, 3511.59it/s]
162476it [01:04, 3491.62it/s]
156916it [01:04, 2947.77it/s]
157903it [01:04, 3181.43it/s]
164448it [01:04, 3397.46it/s]
162828it [01:04, 3378.52it/s]
157281it [01:04, 3132.03it/s]
158269it [01:04, 3312.00it/s]
164821it [01:04, 3491.19it/s]
163199it [01:04, 3471.65it/s]
157624it [01:04, 3090.32it/s]
158617it [01:04, 3236.44it/s]
165172it [01:04, 3401.72it/s]
163548it [01:04, 3368.98it/s]
157988it [01:04, 3239.98it/s]
158982it [01:04, 3351.34it/s]
165548it [01:04, 3503.14it/s]
163918it [01:04, 3463.82it/s]
158328it [01:04, 3182.18it/s]
159327it [01:04, 3273.32it/s]
165900it [01:04, 3408.45it/s]
164266it [01:04, 3372.51it/s]
158693it [01:04, 3311.04it/s]
159693it [01:04, 3382.70it/s]
166275it [01:04, 3504.57it/s]
164635it [01:04, 3461.47it/s]
159060it [01:05, 3412.55it/s]
160037it [01:04, 3254.94it/s]
166641it [01:04, 3548.05it/s]
164997it [01:04, 3361.52it/s]
159408it [01:05, 3278.77it/s]
160402it [01:05, 3364.56it/s]
166997it [01:04, 3439.05it/s]
165368it [01:04, 3458.72it/s]
159775it [01:05, 3387.95it/s]
160770it [01:05, 3453.07it/s]
167371it [01:04, 3525.59it/s]
165726it [01:04, 3493.05it/s]
160119it [01:05, 3282.33it/s]
161119it [01:05, 3337.33it/s]
167725it [01:05, 3421.37it/s]
166077it [01:05, 3402.54it/s]
160484it [01:05, 3385.04it/s]
161486it [01:05, 3431.67it/s]
168100it [01:05, 3515.34it/s]
166453it [01:05, 3504.21it/s]
160826it [01:05, 3286.98it/s]
161832it [01:05, 3333.63it/s]
168453it [01:05, 3419.41it/s]
166805it [01:05, 3423.68it/s]
161182it [01:05, 3363.37it/s]
162198it [01:05, 3426.41it/s]
168827it [01:05, 3510.37it/s]
167173it [01:05, 3495.32it/s]
161546it [01:05, 3441.69it/s]
162543it [01:05, 3335.50it/s]
169189it [01:05, 3540.32it/s]
167524it [01:05, 3400.50it/s]
162895it [01:05, 3387.91it/s]
161892it [01:05, 3334.29it/s]
169544it [01:05, 3420.12it/s]
167899it [01:05, 3498.94it/s]
163263it [01:05, 3471.72it/s]
162260it [01:05, 3431.92it/s]
169912it [01:05, 3493.65it/s]
168276it [01:05, 3577.95it/s]
163612it [01:06, 3359.70it/s]
162605it [01:06, 3328.16it/s]
170263it [01:05, 3400.22it/s]
168635it [01:05, 3458.54it/s]
163981it [01:06, 3453.74it/s]
162961it [01:06, 3392.91it/s]
170634it [01:05, 3488.63it/s]
169003it [01:05, 3520.65it/s]
164328it [01:06, 3353.88it/s]
163319it [01:06, 3300.29it/s]
170985it [01:06, 3388.44it/s]
169357it [01:06, 3406.57it/s]
164698it [01:06, 3451.48it/s]
163688it [01:06, 3410.77it/s]
171356it [01:06, 3479.17it/s]
169725it [01:06, 3483.63it/s]
164056it [01:06, 3487.13it/s]
165045it [01:06, 3364.73it/s]
171707it [01:06, 3487.18it/s]
170075it [01:06, 3380.22it/s]
165417it [01:06, 3466.68it/s]
164407it [01:06, 3366.37it/s]
172057it [01:06, 3389.38it/s]
170448it [01:06, 3480.18it/s]
165790it [01:06, 3543.16it/s]
164776it [01:06, 3457.58it/s]
172428it [01:06, 3481.54it/s]
170819it [01:06, 3544.88it/s]
166146it [01:06, 3378.86it/s]
165124it [01:06, 3321.36it/s]
172778it [01:06, 3383.74it/s]
171175it [01:06, 3417.85it/s]
166523it [01:06, 3488.76it/s]
165492it [01:06, 3421.58it/s]
173159it [01:06, 3505.81it/s]
171548it [01:06, 3505.89it/s]
166874it [01:06, 3392.19it/s]
165839it [01:07, 3338.18it/s]
173511it [01:06, 3394.15it/s]
171901it [01:06, 3399.70it/s]
167243it [01:07, 3475.40it/s]
166216it [01:07, 3459.42it/s]
173884it [01:06, 3487.97it/s]
172260it [01:06, 3452.28it/s]
166588it [01:07, 3534.15it/s]
167593it [01:07, 3372.50it/s]
174238it [01:06, 3388.16it/s]
172607it [01:06, 3370.46it/s]
167968it [01:07, 3478.75it/s]
166943it [01:07, 3373.91it/s]
174594it [01:07, 3436.83it/s]
172980it [01:07, 3473.47it/s]
168345it [01:07, 3562.75it/s]
167313it [01:07, 3466.62it/s]
174963it [01:07, 3508.67it/s]
173353it [01:07, 3545.79it/s]
168703it [01:07, 3434.23it/s]
175315it [01:07, 3408.93it/s]
167662it [01:07, 3362.45it/s]
173709it [01:07, 3430.20it/s]
169071it [01:07, 3504.75it/s]
175690it [01:07, 3506.07it/s]
168034it [01:07, 3461.94it/s]
174082it [01:07, 3515.65it/s]
169424it [01:07, 3337.38it/s]
168382it [01:07, 3367.35it/s]
176042it [01:07, 3394.08it/s]
174435it [01:07, 3420.48it/s]
169794it [01:07, 3438.82it/s]
168753it [01:07, 3464.55it/s]
176408it [01:07, 3469.07it/s]
174796it [01:07, 3472.80it/s]
169106it [01:08, 3483.09it/s]
170141it [01:07, 3343.00it/s]
176758it [01:07, 3383.93it/s]
175145it [01:07, 3380.16it/s]
170512it [01:08, 3446.21it/s]
177126it [01:07, 3468.17it/s]
169456it [01:08, 3365.70it/s]
175512it [01:07, 3462.57it/s]
169823it [01:08, 3453.09it/s]
177474it [01:07, 3421.99it/s]
170878it [01:08, 3351.35it/s]
175886it [01:07, 3542.01it/s]
170170it [01:08, 3347.47it/s]
177818it [01:08, 3338.42it/s]
171248it [01:08, 3449.70it/s]
176242it [01:08, 3414.47it/s]
170537it [01:08, 3439.56it/s]
178199it [01:08, 3472.63it/s]
171619it [01:08, 3522.26it/s]
176610it [01:08, 3490.44it/s]
170883it [01:08, 3341.26it/s]
178548it [01:08, 3368.89it/s]
171973it [01:08, 3399.23it/s]
176961it [01:08, 3397.32it/s]
171250it [01:08, 3434.85it/s]
178927it [01:08, 3488.72it/s]
172342it [01:08, 3480.05it/s]
177303it [01:08, 3366.45it/s]
171601it [01:08, 3454.18it/s]
179278it [01:08, 3372.09it/s]
172692it [01:08, 3337.71it/s]
177641it [01:08, 3284.53it/s]
171948it [01:08, 3345.88it/s]
179653it [01:08, 3479.34it/s]
173065it [01:08, 3448.53it/s]
178018it [01:08, 3421.51it/s]
172315it [01:08, 3439.30it/s]
180021it [01:08, 3535.77it/s]
173412it [01:08, 3348.67it/s]
178393it [01:08, 3515.12it/s]
172661it [01:09, 3347.70it/s]
180376it [01:08, 3452.61it/s]
173785it [01:08, 3457.07it/s]
178746it [01:08, 3415.79it/s]
173034it [01:09, 3457.10it/s]
180730it [01:08, 3475.38it/s]
174160it [01:09, 3541.49it/s]
179118it [01:08, 3501.35it/s]
181079it [01:08, 3398.24it/s]
173399it [01:09, 3351.48it/s]
174516it [01:09, 3435.36it/s]
179470it [01:08, 3380.75it/s]
181448it [01:09, 3472.89it/s]
173753it [01:09, 3404.64it/s]
174877it [01:09, 3484.02it/s]
179845it [01:09, 3484.02it/s]
174125it [01:09, 3494.64it/s]
181798it [01:09, 3384.68it/s]
175227it [01:09, 3385.82it/s]
180195it [01:09, 3397.55it/s]
182179it [01:09, 3504.99it/s]
174476it [01:09, 3392.19it/s]
175595it [01:09, 3469.51it/s]
180537it [01:09, 3368.38it/s]
182553it [01:09, 3572.23it/s]
174843it [01:09, 3470.95it/s]
175944it [01:09, 3316.57it/s]
180914it [01:09, 3482.70it/s]
182912it [01:09, 3467.09it/s]
175192it [01:09, 3369.73it/s]
176313it [01:09, 3421.00it/s]
181264it [01:09, 3376.30it/s]
183272it [01:09, 3504.82it/s]
175557it [01:09, 3448.62it/s]
176687it [01:09, 3512.61it/s]
181633it [01:09, 3465.23it/s]
183624it [01:09, 3395.24it/s]
175919it [01:10, 3347.52it/s]
177040it [01:09, 3371.04it/s]
181981it [01:09, 3367.29it/s]
183999it [01:09, 3495.16it/s]
176269it [01:10, 3386.94it/s]
177412it [01:10, 3468.85it/s]
182357it [01:09, 3472.70it/s]
184350it [01:09, 3410.51it/s]
176640it [01:10, 3477.85it/s]
177761it [01:10, 3349.29it/s]
182706it [01:09, 3406.01it/s]
184728it [01:10, 3515.05it/s]
176990it [01:10, 3362.46it/s]
178143it [01:10, 3481.38it/s]
183076it [01:10, 3488.39it/s]
185103it [01:10, 3582.49it/s]
177361it [01:10, 3460.09it/s]
178494it [01:10, 3363.28it/s]
183453it [01:10, 3569.01it/s]
185463it [01:10, 3433.71it/s]
177709it [01:10, 3337.21it/s]
178858it [01:10, 3432.94it/s]
183811it [01:10, 3411.94it/s]
185825it [01:10, 3486.53it/s]
178087it [01:10, 3462.94it/s]
179227it [01:10, 3505.25it/s]
184182it [01:10, 3496.19it/s]
186176it [01:10, 3409.26it/s]
178439it [01:10, 3307.90it/s]
179580it [01:10, 3389.14it/s]
184534it [01:10, 3401.52it/s]
186553it [01:10, 3510.95it/s]
178813it [01:10, 3427.80it/s]
179948it [01:10, 3471.12it/s]
184913it [01:10, 3511.05it/s]
186906it [01:10, 3456.61it/s]
179183it [01:10, 3505.73it/s]
180297it [01:10, 3384.26it/s]
185266it [01:10, 3407.67it/s]
187278it [01:10, 3531.62it/s]
180665it [01:10, 3466.98it/s]
179536it [01:11, 3382.11it/s]
185633it [01:10, 3481.87it/s]
187656it [01:10, 3602.88it/s]
179905it [01:11, 3469.06it/s]
181013it [01:11, 3371.16it/s]
185997it [01:10, 3355.48it/s]
188018it [01:10, 3478.80it/s]
181380it [01:11, 3454.99it/s]
180254it [01:11, 3378.18it/s]
186378it [01:10, 3482.18it/s]
188378it [01:11, 3512.27it/s]
181749it [01:11, 3521.88it/s]
180618it [01:11, 3451.94it/s]
186770it [01:11, 3605.81it/s]
188731it [01:11, 3417.19it/s]
182103it [01:11, 3407.34it/s]
180965it [01:11, 3323.77it/s]
187133it [01:11, 3469.41it/s]
189099it [01:11, 3492.94it/s]
182479it [01:11, 3506.44it/s]
181331it [01:11, 3419.40it/s]
187509it [01:11, 3550.39it/s]
189450it [01:11, 3396.67it/s]
181701it [01:11, 3498.60it/s]
182831it [01:11, 3407.77it/s]
187866it [01:11, 3451.74it/s]
189825it [01:11, 3496.69it/s]
183200it [01:11, 3488.63it/s]
182053it [01:11, 3388.68it/s]
188238it [01:11, 3528.41it/s]
190198it [01:11, 3419.87it/s]
182427it [01:11, 3487.43it/s]
183551it [01:11, 3378.16it/s]
188593it [01:11, 3435.12it/s]
190568it [01:11, 3498.94it/s]
183926it [01:11, 3482.63it/s]
182778it [01:12, 3402.59it/s]
188948it [01:11, 3467.20it/s]
190930it [01:11, 3533.40it/s]
184292it [01:12, 3531.78it/s]
183129it [01:12, 3424.39it/s]
189322it [01:11, 3544.79it/s]
191285it [01:11, 3415.38it/s]
184647it [01:12, 3417.54it/s]
183479it [01:12, 3337.93it/s]
189678it [01:11, 3421.68it/s]
191662it [01:12, 3516.91it/s]
185024it [01:12, 3517.68it/s]
183851it [01:12, 3446.44it/s]
190057it [01:12, 3521.71it/s]
192016it [01:12, 3439.25it/s]
184224it [01:12, 3526.94it/s]
185378it [01:12, 3375.19it/s]
190411it [01:12, 3422.93it/s]
192389it [01:12, 3521.61it/s]
184578it [01:12, 3415.55it/s]
185752it [01:12, 3477.27it/s]
190784it [01:12, 3510.34it/s]
192743it [01:12, 3442.92it/s]
184950it [01:12, 3503.33it/s]
186102it [01:12, 3389.02it/s]
191137it [01:12, 3401.48it/s]
193120it [01:12, 3535.98it/s]
186468it [01:12, 3465.03it/s]
185302it [01:12, 3339.14it/s]
191510it [01:12, 3495.50it/s]
193482it [01:12, 3558.84it/s]
185670it [01:12, 3434.33it/s]
186838it [01:12, 3414.18it/s]
191875it [01:12, 3538.41it/s]
193839it [01:12, 3446.53it/s]
187212it [01:12, 3505.07it/s]
186016it [01:12, 3348.60it/s]
192230it [01:12, 3428.23it/s]
194222it [01:12, 3557.06it/s]
187587it [01:12, 3575.28it/s]
186395it [01:13, 3472.39it/s]
192611it [01:12, 3537.77it/s]
194579it [01:12, 3456.19it/s]
186784it [01:13, 3592.28it/s]
187946it [01:13, 3457.70it/s]
192967it [01:12, 3443.90it/s]
194952it [01:12, 3534.43it/s]
188319it [01:13, 3534.56it/s]
187145it [01:13, 3450.26it/s]
193341it [01:12, 3528.06it/s]
195307it [01:13, 3471.92it/s]
187520it [01:13, 3533.48it/s]
188674it [01:13, 3376.78it/s]
193696it [01:13, 3411.28it/s]
195682it [01:13, 3551.26it/s]
189044it [01:13, 3467.14it/s]
187876it [01:13, 3389.54it/s]
194077it [01:13, 3524.42it/s]
196039it [01:13, 3556.38it/s]
188248it [01:13, 3482.40it/s]
189393it [01:13, 3372.60it/s]
194431it [01:13, 3425.81it/s]
196396it [01:13, 3442.50it/s]
189767it [01:13, 3477.44it/s]
188599it [01:13, 3390.25it/s]
194785it [01:13, 3457.63it/s]
196782it [01:13, 3563.11it/s]
190144it [01:13, 3560.68it/s]
188969it [01:13, 3478.71it/s]
195177it [01:13, 3590.33it/s]
197140it [01:13, 3454.18it/s]
189340it [01:13, 3543.72it/s]
190502it [01:13, 3429.04it/s]
195538it [01:13, 3464.11it/s]
197524it [01:13, 3565.25it/s]
190874it [01:13, 3512.09it/s]
189696it [01:14, 3404.81it/s]
195909it [01:13, 3534.19it/s]
197882it [01:13, 3470.65it/s]
190058it [01:14, 3464.58it/s]
191227it [01:14, 3396.07it/s]
196264it [01:13, 3411.46it/s]
198261it [01:13, 3561.44it/s]
191569it [01:14, 3386.85it/s]
190407it [01:14, 3369.11it/s]
196646it [01:13, 3528.18it/s]
198619it [01:13, 3424.33it/s]
190776it [01:14, 3460.11it/s]
191909it [01:14, 3324.03it/s]
197001it [01:14, 3448.09it/s]
192286it [01:14, 3451.67it/s]
191124it [01:14, 3350.76it/s]
197371it [01:14, 3518.64it/s]
192667it [01:14, 3553.73it/s]
191497it [01:14, 3457.82it/s]
197744it [01:14, 3578.90it/s]
191876it [01:14, 3552.83it/s]
193024it [01:14, 3434.50it/s]
198103it [01:14, 3488.01it/s]
193397it [01:14, 3518.53it/s]
192233it [01:14, 3426.71it/s]
198476it [01:14, 3556.24it/s]
192592it [01:14, 3472.88it/s]
193751it [01:14, 3402.48it/s]
194132it [01:14, 3517.50it/s]
192941it [01:14, 3388.92it/s]
193317it [01:15, 3494.29it/s]
194486it [01:15, 3413.06it/s]
194831it [01:15, 3423.46it/s]
193668it [01:15, 3371.81it/s]
195226it [01:15, 3575.45it/s]
194049it [01:15, 3495.73it/s]
195585it [01:15, 3443.93it/s]
194401it [01:15, 3396.52it/s]
195951it [01:15, 3504.20it/s]
194756it [01:15, 3439.90it/s]
195151it [01:15, 3587.67it/s]
196303it [01:15, 3383.21it/s]
196691it [01:15, 3523.37it/s]
195512it [01:15, 3443.38it/s]
195883it [01:15, 3519.56it/s]
197046it [01:15, 3414.74it/s]
197414it [01:15, 3488.77it/s]
196237it [01:15, 3391.13it/s]
196618it [01:16, 3509.89it/s]
197765it [01:15, 3375.18it/s]
198153it [01:16, 3517.69it/s]
196971it [01:16, 3426.82it/s]
198529it [01:16, 3587.35it/s]
197334it [01:16, 3483.15it/s]
197703it [01:16, 3543.14it/s]
198059it [01:16, 3459.12it/s]
198435it [01:16, 3544.70it/s]
198964it [01:19, 208.16it/s] 
199348it [01:19, 295.89it/s]
199657it [01:19, 389.07it/s]
200024it [01:19, 538.25it/s]
200347it [01:19, 700.74it/s]
200723it [01:20, 945.23it/s]
198833it [01:20, 208.50it/s] 
201097it [01:20, 1231.62it/s]
199207it [01:20, 292.95it/s]
201442it [01:20, 1496.16it/s]
199522it [01:20, 387.35it/s]
201820it [01:20, 1844.77it/s]
199887it [01:20, 534.34it/s]
202168it [01:20, 2093.50it/s]
200253it [01:20, 723.40it/s]
202561it [01:20, 2461.26it/s]
200585it [01:20, 921.41it/s]
202915it [01:20, 2650.36it/s]
200954it [01:20, 1201.34it/s]
203291it [01:20, 2913.69it/s]
201290it [01:20, 1456.53it/s]
203665it [01:20, 3122.87it/s]
201667it [01:20, 1806.24it/s]
204026it [01:20, 3135.08it/s]
202026it [01:20, 2072.87it/s]
204403it [01:21, 3304.65it/s]
202414it [01:21, 2430.56it/s]
204760it [01:21, 3236.57it/s]
202780it [01:21, 2701.29it/s]
205139it [01:21, 3388.18it/s]
203135it [01:21, 2838.94it/s]
205492it [01:21, 3331.35it/s]
203504it [01:21, 3052.20it/s]
205871it [01:21, 3459.21it/s]
203857it [01:21, 3074.19it/s]
206227it [01:21, 3388.14it/s]
204231it [01:21, 3250.63it/s]
206607it [01:21, 3503.53it/s]
204582it [01:21, 3222.20it/s]
206982it [01:21, 3574.00it/s]
204956it [01:21, 3365.05it/s]
198890it [01:22, 199.42it/s] 
207343it [01:21, 3440.79it/s]
205316it [01:21, 3425.05it/s]
199271it [01:22, 281.29it/s]
207714it [01:22, 3515.61it/s]
199589it [01:22, 372.22it/s]
205669it [01:22, 3335.82it/s]
208069it [01:22, 3436.89it/s]
199920it [01:22, 498.22it/s]
206049it [01:22, 3465.77it/s]
208456it [01:22, 3560.69it/s]
200277it [01:22, 675.27it/s]
206402it [01:22, 3335.23it/s]
208815it [01:22, 3460.94it/s]
198791it [01:22, 190.94it/s] 
200600it [01:22, 867.71it/s]
206778it [01:22, 3453.87it/s]
209195it [01:22, 3557.32it/s]
199161it [01:22, 268.20it/s]
200970it [01:22, 1144.80it/s]
207128it [01:22, 3383.53it/s]
209564it [01:22, 3595.33it/s]
199508it [01:22, 364.77it/s]
201305it [01:22, 1403.51it/s]
207502it [01:22, 3483.88it/s]
209925it [01:22, 3435.86it/s]
201679it [01:22, 1749.21it/s]
199872it [01:22, 500.26it/s]
207871it [01:22, 3543.12it/s]
210293it [01:22, 3504.92it/s]
200239it [01:23, 677.81it/s]
202027it [01:22, 2023.12it/s]
208228it [01:22, 3465.60it/s]
202408it [01:23, 2366.29it/s]
210646it [01:22, 3402.86it/s]
200567it [01:23, 859.97it/s]
208606it [01:22, 3555.38it/s]
202793it [01:23, 2691.14it/s]
211017it [01:22, 3489.23it/s]
200934it [01:23, 1125.91it/s]
208963it [01:22, 3462.63it/s]
211368it [01:23, 3394.32it/s]
203152it [01:23, 2828.11it/s]
201264it [01:23, 1374.10it/s]
209343it [01:23, 3557.84it/s]
211741it [01:23, 3488.16it/s]
203525it [01:23, 3050.81it/s]
201641it [01:23, 1719.58it/s]
209701it [01:23, 3432.63it/s]
202018it [01:23, 2069.32it/s]
203880it [01:23, 3078.03it/s]
212108it [01:23, 3395.69it/s]
210062it [01:23, 3482.27it/s]
204253it [01:23, 3250.82it/s]
212472it [01:23, 3462.99it/s]
202369it [01:23, 2293.85it/s]
210426it [01:23, 3385.27it/s]
212847it [01:23, 3543.95it/s]
202753it [01:23, 2624.66it/s]
204605it [01:23, 3226.99it/s]
210798it [01:23, 3479.66it/s]
204966it [01:23, 3332.19it/s]
213203it [01:23, 3429.57it/s]
203105it [01:23, 2777.91it/s]
211171it [01:23, 3549.97it/s]
205339it [01:23, 3444.50it/s]
213573it [01:23, 3507.19it/s]
203478it [01:24, 3012.21it/s]
211528it [01:23, 3437.43it/s]
205694it [01:24, 3356.47it/s]
213926it [01:23, 3410.49it/s]
203831it [01:24, 3047.45it/s]
211902it [01:23, 3523.77it/s]
206074it [01:24, 3482.05it/s]
214295it [01:23, 3489.84it/s]
204200it [01:24, 3217.36it/s]
212256it [01:23, 3414.29it/s]
206429it [01:24, 3403.93it/s]
214646it [01:24, 3382.57it/s]
204550it [01:24, 3197.71it/s]
212622it [01:24, 3482.98it/s]
206799it [01:24, 3487.54it/s]
215008it [01:24, 3449.08it/s]
204905it [01:24, 3294.64it/s]
212972it [01:24, 3387.45it/s]
215385it [01:24, 3542.04it/s]
205283it [01:24, 3429.38it/s]
207152it [01:24, 3358.17it/s]
213346it [01:24, 3486.74it/s]
215741it [01:24, 3426.78it/s]
207526it [01:24, 3465.37it/s]
205637it [01:24, 3344.71it/s]
213714it [01:24, 3542.26it/s]
216118it [01:24, 3525.29it/s]
207905it [01:24, 3557.24it/s]
206016it [01:24, 3470.47it/s]
214070it [01:24, 3432.46it/s]
216472it [01:24, 3426.09it/s]
208264it [01:24, 3468.46it/s]
206370it [01:24, 3381.35it/s]
214440it [01:24, 3507.87it/s]
216855it [01:24, 3539.73it/s]
208640it [01:24, 3551.83it/s]
206743it [01:24, 3480.80it/s]
214793it [01:24, 3396.22it/s]
208997it [01:24, 3457.84it/s]
217211it [01:24, 3422.27it/s]
207095it [01:25, 3344.53it/s]
215155it [01:24, 3459.52it/s]
209373it [01:25, 3544.84it/s]
217592it [01:24, 3531.31it/s]
207469it [01:25, 3454.19it/s]
215503it [01:24, 3348.02it/s]
217980it [01:24, 3631.57it/s]
207844it [01:25, 3537.63it/s]
209729it [01:25, 3381.93it/s]
215879it [01:24, 3465.98it/s]
218345it [01:25, 3504.70it/s]
210101it [01:25, 3475.79it/s]
208201it [01:25, 3443.06it/s]
216253it [01:25, 3543.64it/s]
218737it [01:25, 3623.41it/s]
208581it [01:25, 3544.23it/s]
210451it [01:25, 3376.91it/s]
216609it [01:25, 3440.00it/s]
219102it [01:25, 3515.33it/s]
210823it [01:25, 3472.73it/s]
208938it [01:25, 3437.58it/s]
216995it [01:25, 3559.71it/s]
219486it [01:25, 3607.63it/s]
211195it [01:25, 3543.71it/s]
209313it [01:25, 3524.71it/s]
217353it [01:25, 3422.37it/s]
219849it [01:25, 3495.04it/s]
211551it [01:25, 3423.42it/s]
209668it [01:25, 3352.23it/s]
217741it [01:25, 3551.36it/s]
220239it [01:25, 3610.43it/s]
211916it [01:25, 3486.90it/s]
210038it [01:25, 3448.87it/s]
218099it [01:25, 3472.82it/s]
220602it [01:25, 3534.90it/s]
210409it [01:26, 3522.70it/s]
212267it [01:25, 3388.39it/s]
218477it [01:25, 3558.92it/s]
220985it [01:25, 3619.10it/s]
212643it [01:26, 3494.19it/s]
210764it [01:26, 3397.10it/s]
218835it [01:25, 3492.61it/s]
221349it [01:25, 3507.28it/s]
212994it [01:26, 3393.43it/s]
211130it [01:26, 3471.56it/s]
219221it [01:25, 3598.81it/s]
221746it [01:26, 3638.92it/s]
213368it [01:26, 3482.70it/s]
219604it [01:25, 3665.79it/s]
211479it [01:26, 3363.80it/s]
222119it [01:26, 3664.15it/s]
213739it [01:26, 3547.24it/s]
211838it [01:26, 3426.61it/s]
219972it [01:26, 3533.23it/s]
222487it [01:26, 3487.69it/s]
214095it [01:26, 3401.02it/s]
212183it [01:26, 3327.85it/s]
220367it [01:26, 3647.95it/s]
222863it [01:26, 3564.88it/s]
214465it [01:26, 3485.53it/s]
212554it [01:26, 3437.10it/s]
220734it [01:26, 3549.17it/s]
212928it [01:26, 3523.10it/s]
223222it [01:26, 3439.61it/s]
214816it [01:26, 3376.74it/s]
221119it [01:26, 3634.41it/s]
223590it [01:26, 3506.11it/s]
215190it [01:26, 3480.25it/s]
213282it [01:26, 3395.43it/s]
221484it [01:26, 3543.46it/s]
213648it [01:26, 3467.92it/s]
223943it [01:26, 3405.23it/s]
215540it [01:26, 3377.13it/s]
221873it [01:26, 3643.10it/s]
224321it [01:26, 3511.38it/s]
215917it [01:26, 3488.11it/s]
213997it [01:27, 3333.51it/s]
222239it [01:26, 3502.70it/s]
224693it [01:26, 3571.42it/s]
216297it [01:27, 3576.72it/s]
214364it [01:27, 3426.95it/s]
222607it [01:26, 3544.07it/s]
225052it [01:26, 3415.17it/s]
216657it [01:27, 3429.28it/s]
222982it [01:26, 3602.31it/s]
214709it [01:27, 3317.12it/s]
225420it [01:27, 3489.90it/s]
217042it [01:27, 3549.16it/s]
215080it [01:27, 3427.80it/s]
223344it [01:27, 3471.88it/s]
215456it [01:27, 3521.52it/s]
225771it [01:27, 3374.35it/s]
223717it [01:27, 3545.35it/s]
217399it [01:27, 3363.96it/s]
226134it [01:27, 3447.08it/s]
217787it [01:27, 3508.14it/s]
215810it [01:27, 3396.50it/s]
224074it [01:27, 3441.87it/s]
226481it [01:27, 3342.56it/s]
216182it [01:27, 3488.81it/s]
218141it [01:27, 3434.00it/s]
224444it [01:27, 3515.72it/s]
226838it [01:27, 3407.49it/s]
218522it [01:27, 3540.88it/s]
216533it [01:27, 3347.19it/s]
224797it [01:27, 3406.01it/s]
227207it [01:27, 3487.05it/s]
216912it [01:27, 3472.53it/s]
218879it [01:27, 3438.20it/s]
225163it [01:27, 3477.59it/s]
227557it [01:27, 3346.79it/s]
219266it [01:27, 3560.16it/s]
225535it [01:27, 3546.10it/s]
217262it [01:28, 3378.14it/s]
227926it [01:27, 3442.86it/s]
219648it [01:28, 3635.29it/s]
217641it [01:28, 3495.35it/s]
225891it [01:27, 3415.83it/s]
220014it [01:28, 3547.42it/s]
228273it [01:27, 3343.15it/s]
226254it [01:27, 3476.26it/s]
217993it [01:28, 3422.76it/s]
220409it [01:28, 3662.08it/s]
228639it [01:28, 3432.36it/s]
218372it [01:28, 3526.67it/s]
226604it [01:28, 3371.48it/s]
218746it [01:28, 3586.85it/s]
228984it [01:28, 3340.29it/s]
220777it [01:28, 3549.21it/s]
226964it [01:28, 3436.58it/s]
229356it [01:28, 3447.84it/s]
221160it [01:28, 3627.25it/s]
219106it [01:28, 3471.90it/s]
227309it [01:28, 3310.15it/s]
229731it [01:28, 3535.09it/s]
219488it [01:28, 3571.02it/s]
221525it [01:28, 3498.83it/s]
227676it [01:28, 3412.65it/s]
221911it [01:28, 3600.87it/s]
230086it [01:28, 3389.00it/s]
219847it [01:28, 3474.65it/s]
228044it [01:28, 3488.52it/s]
230449it [01:28, 3455.96it/s]
220235it [01:28, 3590.24it/s]
222273it [01:28, 3475.53it/s]
228395it [01:28, 3373.85it/s]
230797it [01:28, 3363.39it/s]
222649it [01:28, 3555.40it/s]
220596it [01:28, 3502.74it/s]
228766it [01:28, 3469.68it/s]
231171it [01:28, 3470.43it/s]
223024it [01:28, 3609.03it/s]
220978it [01:29, 3591.72it/s]
229115it [01:28, 3365.33it/s]
221342it [01:29, 3604.07it/s]
231520it [01:28, 3370.11it/s]
223387it [01:29, 3467.97it/s]
229490it [01:28, 3473.42it/s]
231892it [01:28, 3469.93it/s]
221704it [01:29, 3512.06it/s]
223749it [01:29, 3509.39it/s]
229839it [01:28, 3356.54it/s]
232261it [01:29, 3532.80it/s]
222080it [01:29, 3583.86it/s]
224102it [01:29, 3418.47it/s]
230206it [01:29, 3444.67it/s]
232616it [01:29, 3389.22it/s]
224474it [01:29, 3504.81it/s]
222440it [01:29, 3437.91it/s]
230573it [01:29, 3508.20it/s]
232984it [01:29, 3471.19it/s]
222818it [01:29, 3528.06it/s]
224826it [01:29, 3396.02it/s]
230926it [01:29, 3403.40it/s]
233333it [01:29, 3387.32it/s]
225199it [01:29, 3489.82it/s]
231302it [01:29, 3505.05it/s]
223173it [01:29, 3402.55it/s]
233705it [01:29, 3481.79it/s]
223528it [01:29, 3442.84it/s]
225550it [01:29, 3382.93it/s]
231654it [01:29, 3391.94it/s]
234055it [01:29, 3386.58it/s]
225902it [01:29, 3421.65it/s]
223874it [01:29, 3352.62it/s]
232024it [01:29, 3479.38it/s]
234424it [01:29, 3471.74it/s]
226268it [01:29, 3490.78it/s]
224249it [01:30, 3465.57it/s]
232374it [01:29, 3354.10it/s]
234785it [01:29, 3501.66it/s]
224617it [01:30, 3527.29it/s]
226619it [01:30, 3372.14it/s]
232742it [01:29, 3446.67it/s]
235137it [01:29, 3389.66it/s]
226976it [01:30, 3427.42it/s]
224971it [01:30, 3396.87it/s]
233107it [01:29, 3357.68it/s]
235501it [01:30, 3459.29it/s]
225339it [01:30, 3475.64it/s]
227320it [01:30, 3323.85it/s]
233483it [01:30, 3470.07it/s]
235849it [01:30, 3353.30it/s]
227686it [01:30, 3419.65it/s]
225689it [01:30, 3350.67it/s]
233856it [01:30, 3543.82it/s]
236219it [01:30, 3451.71it/s]
228033it [01:30, 3433.82it/s]
226052it [01:30, 3428.35it/s]
234212it [01:30, 3432.57it/s]
236566it [01:30, 3344.66it/s]
228378it [01:30, 3309.91it/s]
226397it [01:30, 3315.18it/s]
234570it [01:30, 3472.95it/s]
236936it [01:30, 3446.02it/s]
228742it [01:30, 3403.04it/s]
226762it [01:30, 3410.60it/s]
234919it [01:30, 3357.87it/s]
237288it [01:30, 3465.57it/s]
227119it [01:30, 3455.92it/s]
229084it [01:30, 3292.88it/s]
235280it [01:30, 3428.35it/s]
237636it [01:30, 3347.13it/s]
229457it [01:30, 3416.65it/s]
227466it [01:30, 3299.89it/s]
235627it [01:30, 3317.32it/s]
238005it [01:30, 3443.34it/s]
227831it [01:31, 3397.80it/s]
229801it [01:30, 3314.34it/s]
235986it [01:30, 3394.19it/s]
238351it [01:30, 3333.63it/s]
230161it [01:31, 3394.73it/s]
236335it [01:30, 3421.41it/s]
228173it [01:31, 3292.70it/s]
238714it [01:30, 3416.62it/s]
230507it [01:31, 3411.46it/s]
228539it [01:31, 3397.44it/s]
236679it [01:30, 3309.19it/s]
239058it [01:31, 3314.04it/s]
228906it [01:31, 3473.93it/s]
230850it [01:31, 3298.85it/s]
237040it [01:31, 3394.00it/s]
239423it [01:31, 3410.05it/s]
231211it [01:31, 3387.43it/s]
229255it [01:31, 3326.16it/s]
237381it [01:31, 3292.71it/s]
239783it [01:31, 3464.29it/s]
231552it [01:31, 3289.98it/s]
229630it [01:31, 3445.72it/s]
237746it [01:31, 3394.28it/s]
240131it [01:31, 3350.05it/s]
231918it [01:31, 3385.15it/s]
238112it [01:31, 3471.21it/s]
229977it [01:31, 3332.54it/s]
240498it [01:31, 3439.94it/s]
232268it [01:31, 3287.73it/s]
230340it [01:31, 3416.48it/s]
238461it [01:31, 3351.35it/s]
240844it [01:31, 3339.71it/s]
232635it [01:31, 3395.04it/s]
238827it [01:31, 3439.18it/s]
230684it [01:31, 3307.49it/s]
241216it [01:31, 3447.57it/s]
232993it [01:31, 3446.54it/s]
231060it [01:32, 3435.31it/s]
239173it [01:31, 3333.80it/s]
241563it [01:31, 3355.34it/s]
231420it [01:32, 3480.44it/s]
233339it [01:32, 3305.58it/s]
239528it [01:31, 3394.72it/s]
241935it [01:31, 3458.21it/s]
233707it [01:32, 3410.51it/s]
231770it [01:32, 3362.29it/s]
239869it [01:31, 3308.83it/s]
242296it [01:32, 3500.77it/s]
234050it [01:32, 3321.19it/s]
232133it [01:32, 3438.14it/s]
240235it [01:32, 3409.70it/s]
242648it [01:32, 3382.70it/s]
234420it [01:32, 3429.56it/s]
240599it [01:32, 3474.92it/s]
232479it [01:32, 3335.02it/s]
243016it [01:32, 3468.22it/s]
234786it [01:32, 3495.31it/s]
232843it [01:32, 3421.27it/s]
240948it [01:32, 3364.69it/s]
243365it [01:32, 3374.04it/s]
235137it [01:32, 3364.34it/s]
241322it [01:32, 3472.86it/s]
233187it [01:32, 3184.56it/s]
243736it [01:32, 3469.88it/s]
235499it [01:32, 3435.44it/s]
241671it [01:32, 3342.74it/s]
233555it [01:32, 3322.31it/s]
244085it [01:32, 3354.74it/s]
235845it [01:32, 3319.10it/s]
242041it [01:32, 3442.94it/s]
233925it [01:32, 3428.41it/s]
244448it [01:32, 3425.11it/s]
236212it [01:32, 3417.37it/s]
242387it [01:32, 3343.07it/s]
234272it [01:32, 3334.64it/s]
244806it [01:32, 3468.95it/s]
236556it [01:32, 3305.75it/s]
242754it [01:32, 3436.70it/s]
234633it [01:33, 3413.11it/s]
245154it [01:32, 3354.35it/s]
236920it [01:33, 3400.02it/s]
243118it [01:32, 3495.55it/s]
234977it [01:33, 3306.82it/s]
245521it [01:32, 3444.31it/s]
237283it [01:33, 3466.53it/s]
243469it [01:32, 3382.04it/s]
235344it [01:33, 3408.94it/s]
245867it [01:33, 3336.59it/s]
243836it [01:33, 3464.95it/s]
237632it [01:33, 3322.76it/s]
235687it [01:33, 3292.14it/s]
246234it [01:33, 3431.76it/s]
237996it [01:33, 3411.72it/s]
244184it [01:33, 3330.63it/s]
236050it [01:33, 3386.58it/s]
246579it [01:33, 3330.08it/s]
244545it [01:33, 3410.33it/s]
238340it [01:33, 3300.36it/s]
236398it [01:33, 3405.90it/s]
246948it [01:33, 3432.22it/s]
238702it [01:33, 3389.33it/s]
244888it [01:33, 3309.27it/s]
236740it [01:33, 3307.46it/s]
247301it [01:33, 3460.08it/s]
245251it [01:33, 3400.67it/s]
239043it [01:33, 3278.92it/s]
237103it [01:33, 3398.82it/s]
247649it [01:33, 3351.15it/s]
245621it [01:33, 3486.38it/s]
239406it [01:33, 3377.89it/s]
237445it [01:33, 3302.34it/s]
248017it [01:33, 3443.30it/s]
239761it [01:33, 3426.22it/s]
245971it [01:33, 3360.90it/s]
237806it [01:34, 3389.02it/s]
248363it [01:33, 3334.28it/s]
246337it [01:33, 3437.89it/s]
240105it [01:34, 3319.16it/s]
238148it [01:34, 3288.25it/s]
248728it [01:33, 3423.81it/s]
240466it [01:34, 3402.16it/s]
246683it [01:33, 3322.78it/s]
238509it [01:34, 3379.05it/s]
249072it [01:34, 3332.79it/s]
240808it [01:34, 3298.33it/s]
247047it [01:34, 3412.16it/s]
238858it [01:34, 3409.61it/s]
249437it [01:34, 3423.84it/s]
241173it [01:34, 3398.17it/s]
247390it [01:34, 3312.98it/s]
239200it [01:34, 3295.77it/s]
249792it [01:34, 3457.85it/s]
241515it [01:34, 3304.17it/s]
247755it [01:34, 3407.91it/s]
239563it [01:34, 3391.16it/s]
250139it [01:34, 3356.95it/s]
241876it [01:34, 3391.19it/s]
248120it [01:34, 3476.81it/s]
239904it [01:34, 3292.89it/s]
250514it [01:34, 3468.32it/s]
242242it [01:34, 3468.57it/s]
248469it [01:34, 3362.32it/s]
240268it [01:34, 3391.44it/s]
250863it [01:34, 3373.46it/s]
242591it [01:34, 3359.19it/s]
248839it [01:34, 3457.66it/s]
240617it [01:34, 3419.22it/s]
251232it [01:34, 3464.51it/s]
242958it [01:34, 3446.25it/s]
249187it [01:34, 3334.80it/s]
240960it [01:34, 3308.60it/s]
251588it [01:34, 3345.56it/s]
243304it [01:34, 3346.23it/s]
249555it [01:34, 3431.02it/s]
241332it [01:35, 3426.63it/s]
251948it [01:34, 3416.57it/s]
243666it [01:35, 3422.96it/s]
241677it [01:35, 3319.17it/s]
249907it [01:34, 3313.19it/s]
252313it [01:34, 3483.30it/s]
244028it [01:35, 3320.09it/s]
242046it [01:35, 3424.83it/s]
250277it [01:34, 3421.68it/s]
252663it [01:35, 3358.46it/s]
244382it [01:35, 3380.94it/s]
250650it [01:35, 3510.64it/s]
242390it [01:35, 3320.57it/s]
253034it [01:35, 3458.82it/s]
244748it [01:35, 3461.30it/s]
242755it [01:35, 3413.31it/s]
251003it [01:35, 3402.69it/s]
253382it [01:35, 3348.21it/s]
245096it [01:35, 3346.03it/s]
243105it [01:35, 3438.11it/s]
251361it [01:35, 3451.98it/s]
253749it [01:35, 3439.64it/s]
245458it [01:35, 3423.56it/s]
243450it [01:35, 3328.67it/s]
251708it [01:35, 3337.84it/s]
254108it [01:35, 3326.99it/s]
245802it [01:35, 3313.57it/s]
243818it [01:35, 3420.41it/s]
252077it [01:35, 3433.68it/s]
254465it [01:35, 3394.26it/s]
246165it [01:35, 3401.64it/s]
244162it [01:35, 3318.72it/s]
252427it [01:35, 3322.93it/s]
254829it [01:35, 3464.31it/s]
246517it [01:35, 3435.61it/s]
244522it [01:36, 3398.72it/s]
252793it [01:35, 3418.79it/s]
255177it [01:35, 3351.21it/s]
253160it [01:35, 3489.33it/s]
246862it [01:36, 3277.00it/s]
244868it [01:36, 3291.18it/s]
255545it [01:35, 3444.28it/s]
247229it [01:36, 3387.28it/s]
245232it [01:36, 3390.29it/s]
253511it [01:35, 3364.34it/s]
245582it [01:36, 3419.97it/s]
247570it [01:36, 3285.64it/s]
253854it [01:36, 3381.78it/s]
247937it [01:36, 3394.64it/s]
245926it [01:36, 3306.17it/s]
254194it [01:36, 3264.35it/s]
246292it [01:36, 3406.67it/s]
248279it [01:36, 3292.59it/s]
254558it [01:36, 3370.78it/s]
248633it [01:36, 3363.20it/s]
254911it [01:36, 3416.24it/s]
246635it [01:36, 3300.11it/s]
248998it [01:36, 3445.98it/s]
247002it [01:36, 3405.31it/s]
255254it [01:36, 3311.29it/s]
247368it [01:36, 3477.98it/s]
249344it [01:36, 3338.78it/s]
255627it [01:36, 3431.50it/s]
249706it [01:36, 3418.75it/s]
247718it [01:36, 3313.78it/s]
250050it [01:37, 3307.00it/s]
248083it [01:37, 3408.61it/s]
250422it [01:37, 3423.83it/s]
248426it [01:37, 3299.88it/s]
250766it [01:37, 3310.98it/s]
248790it [01:37, 3395.88it/s]
251123it [01:37, 3382.76it/s]
249132it [01:37, 3307.04it/s]
251488it [01:37, 3458.08it/s]
249495it [01:37, 3398.02it/s]
249858it [01:37, 3463.92it/s]
251836it [01:37, 3328.36it/s]
252209it [01:37, 3442.25it/s]
250206it [01:37, 3311.10it/s]
252555it [01:37, 3320.80it/s]
250578it [01:37, 3426.68it/s]
252921it [01:37, 3416.31it/s]
250923it [01:37, 3329.96it/s]
251290it [01:38, 3427.05it/s]
253268it [01:37, 3277.82it/s]
253636it [01:38, 3391.04it/s]
251635it [01:38, 3310.76it/s]
254001it [01:38, 3465.28it/s]
251999it [01:38, 3404.34it/s]
252365it [01:38, 3476.37it/s]
254350it [01:38, 3332.21it/s]
254719it [01:38, 3432.85it/s]
252715it [01:38, 3305.37it/s]
255065it [01:38, 3323.53it/s]
253082it [01:38, 3407.13it/s]
255435it [01:38, 3429.10it/s]
253426it [01:38, 3305.42it/s]
253794it [01:38, 3409.98it/s]
255788it [01:38, 3310.54it/s]
254137it [01:38, 3300.11it/s]
254504it [01:38, 3404.36it/s]
254852it [01:39, 3424.21it/s]
255196it [01:39, 3313.25it/s]
255563it [01:39, 3415.69it/s]
255891it [01:43, 157.10it/s] 
256266it [01:43, 223.80it/s]
256638it [01:43, 314.12it/s]
256948it [01:43, 413.27it/s]
257307it [01:43, 566.91it/s]
257632it [01:43, 737.67it/s]
258009it [01:43, 993.04it/s]
258377it [01:43, 1259.09it/s]
255972it [01:43, 153.51it/s] 
258752it [01:44, 1584.97it/s]
256345it [01:44, 218.83it/s]
259128it [01:44, 1927.04it/s]
256695it [01:44, 301.35it/s]
259481it [01:44, 2171.97it/s]
257068it [01:44, 421.19it/s]
259842it [01:44, 2465.11it/s]
257432it [01:44, 574.24it/s]
260190it [01:44, 2637.43it/s]
257762it [01:44, 743.11it/s]
260556it [01:44, 2881.13it/s]
258136it [01:44, 991.28it/s]
260903it [01:44, 2952.68it/s]
258473it [01:44, 1231.90it/s]
261280it [01:44, 3165.74it/s]
258848it [01:44, 1559.56it/s]
261655it [01:44, 3322.42it/s]
259215it [01:44, 1842.82it/s]
262012it [01:44, 3250.73it/s]
259585it [01:45, 2175.23it/s]
262379it [01:45, 3365.62it/s]
259958it [01:45, 2492.02it/s]
262729it [01:45, 3303.31it/s]
260312it [01:45, 2635.28it/s]
263102it [01:45, 3421.90it/s]
260683it [01:45, 2889.86it/s]
263452it [01:45, 3334.46it/s]
261032it [01:45, 2965.62it/s]
263827it [01:45, 3452.18it/s]
261404it [01:45, 3160.30it/s]
264199it [01:45, 3529.08it/s]
261753it [01:45, 3163.66it/s]
264556it [01:45, 3389.51it/s]
262126it [01:45, 3317.02it/s]
264929it [01:45, 3484.58it/s]
262486it [01:45, 3395.14it/s]
265281it [01:45, 3388.16it/s]
262839it [01:45, 3325.02it/s]
265657it [01:46, 3490.61it/s]
256122it [01:46, 149.61it/s] 
263209it [01:46, 3429.29it/s]
256490it [01:46, 212.93it/s]
266009it [01:46, 3392.92it/s]
263559it [01:46, 3340.02it/s]
256799it [01:46, 284.82it/s]
266384it [01:46, 3493.53it/s]
263934it [01:46, 3455.99it/s]
257168it [01:46, 402.18it/s]
266760it [01:46, 3569.11it/s]
264284it [01:46, 3372.85it/s]
257531it [01:46, 553.64it/s]
255907it [01:46, 152.63it/s] 
267119it [01:46, 3424.31it/s]
264657it [01:46, 3474.01it/s]
257857it [01:46, 719.41it/s]
256277it [01:46, 217.20it/s]
267492it [01:46, 3510.14it/s]
265018it [01:46, 3513.00it/s]
258225it [01:46, 961.78it/s]
256632it [01:46, 301.69it/s]
267845it [01:46, 3413.60it/s]
265372it [01:46, 3408.50it/s]
256936it [01:47, 397.37it/s]
258558it [01:46, 1199.27it/s]
268223it [01:46, 3516.52it/s]
265745it [01:46, 3499.23it/s]
257302it [01:47, 552.47it/s]
258910it [01:47, 1498.59it/s]
268577it [01:46, 3417.11it/s]
266097it [01:46, 3397.60it/s]
257624it [01:47, 719.26it/s]
259243it [01:47, 1762.17it/s]
268947it [01:46, 3493.72it/s]
266471it [01:47, 3493.65it/s]
257995it [01:47, 967.36it/s]
259610it [01:47, 2105.81it/s]
269298it [01:47, 3387.18it/s]
266822it [01:47, 3398.53it/s]
258364it [01:47, 1255.82it/s]
259970it [01:47, 2410.45it/s]
269662it [01:47, 3457.07it/s]
267196it [01:47, 3489.25it/s]
258708it [01:47, 1513.79it/s]
260315it [01:47, 2577.49it/s]
270037it [01:47, 3540.59it/s]
267560it [01:47, 3531.31it/s]
259079it [01:47, 1854.89it/s]
260678it [01:47, 2828.23it/s]
270393it [01:47, 3433.91it/s]
267915it [01:47, 3359.48it/s]
259422it [01:47, 2100.43it/s]
261021it [01:47, 2897.72it/s]
270765it [01:47, 3516.28it/s]
268290it [01:47, 3468.41it/s]
259791it [01:47, 2422.17it/s]
261386it [01:47, 3089.99it/s]
271118it [01:47, 3391.89it/s]
268639it [01:47, 3377.06it/s]
261728it [01:47, 3179.43it/s]
260135it [01:47, 2588.85it/s]
271489it [01:47, 3482.35it/s]
269009it [01:47, 3468.87it/s]
260487it [01:48, 2810.81it/s]
262070it [01:48, 3166.85it/s]
271839it [01:47, 3374.66it/s]
269358it [01:47, 3366.70it/s]
260858it [01:48, 3039.34it/s]
262417it [01:48, 3249.60it/s]
272203it [01:47, 3449.04it/s]
269716it [01:47, 3425.71it/s]
261207it [01:48, 3068.68it/s]
262755it [01:48, 3190.59it/s]
272571it [01:48, 3515.52it/s]
270093it [01:48, 3524.63it/s]
261578it [01:48, 3239.76it/s]
263128it [01:48, 3342.71it/s]
272924it [01:48, 3397.86it/s]
270447it [01:48, 3408.39it/s]
261926it [01:48, 3209.96it/s]
263470it [01:48, 3264.96it/s]
273288it [01:48, 3466.66it/s]
270817it [01:48, 3491.31it/s]
262294it [01:48, 3340.00it/s]
263843it [01:48, 3397.64it/s]
273636it [01:48, 3375.90it/s]
271168it [01:48, 3376.54it/s]
264218it [01:48, 3498.29it/s]
262641it [01:48, 3234.48it/s]
274005it [01:48, 3464.00it/s]
271536it [01:48, 3457.72it/s]
263010it [01:48, 3359.75it/s]
264572it [01:48, 3358.78it/s]
274353it [01:48, 3364.02it/s]
271884it [01:48, 3369.98it/s]
263378it [01:48, 3450.99it/s]
264944it [01:48, 3460.61it/s]
274713it [01:48, 3431.74it/s]
272237it [01:48, 3415.85it/s]
263729it [01:49, 3349.51it/s]
265293it [01:48, 3363.47it/s]
275084it [01:48, 3512.37it/s]
272603it [01:48, 3485.96it/s]
264098it [01:49, 3446.40it/s]
265667it [01:49, 3469.93it/s]
275437it [01:48, 3399.24it/s]
272953it [01:48, 3371.49it/s]
266017it [01:49, 3367.60it/s]
264447it [01:49, 3319.38it/s]
275806it [01:48, 3482.40it/s]
273319it [01:49, 3453.65it/s]
266393it [01:49, 3477.75it/s]
264814it [01:49, 3418.66it/s]
276156it [01:49, 3378.88it/s]
273666it [01:49, 3358.24it/s]
266769it [01:49, 3557.47it/s]
265159it [01:49, 3326.70it/s]
276522it [01:49, 3458.40it/s]
274035it [01:49, 3452.70it/s]
265530it [01:49, 3435.25it/s]
267127it [01:49, 3400.39it/s]
276870it [01:49, 3360.56it/s]
274382it [01:49, 3353.31it/s]
265900it [01:49, 3509.59it/s]
267503it [01:49, 3501.65it/s]
277232it [01:49, 3434.75it/s]
274738it [01:49, 3412.12it/s]
266253it [01:49, 3385.59it/s]
267856it [01:49, 3394.66it/s]
277600it [01:49, 3504.35it/s]
275094it [01:49, 3453.99it/s]
266611it [01:49, 3441.21it/s]
268235it [01:49, 3505.73it/s]
277952it [01:49, 3394.05it/s]
275441it [01:49, 3347.74it/s]
268588it [01:49, 3396.56it/s]
266957it [01:50, 3309.97it/s]
278318it [01:49, 3470.71it/s]
275806it [01:49, 3429.51it/s]
268957it [01:50, 3478.83it/s]
267330it [01:50, 3428.06it/s]
278667it [01:49, 3367.88it/s]
276151it [01:49, 3336.73it/s]
267675it [01:50, 3330.78it/s]
269307it [01:50, 3337.77it/s]
279035it [01:49, 3457.20it/s]
276513it [01:49, 3416.00it/s]
268048it [01:50, 3444.33it/s]
269681it [01:50, 3449.42it/s]
279383it [01:50, 3328.59it/s]
276856it [01:50, 3318.30it/s]
268410it [01:50, 3493.79it/s]
270056it [01:50, 3533.82it/s]
279754it [01:50, 3437.11it/s]
277214it [01:50, 3391.30it/s]
268761it [01:50, 3376.63it/s]
270412it [01:50, 3417.23it/s]
280120it [01:50, 3500.10it/s]
277578it [01:50, 3462.86it/s]
269129it [01:50, 3461.93it/s]
270781it [01:50, 3492.98it/s]
280472it [01:50, 3387.64it/s]
277926it [01:50, 3350.97it/s]
269477it [01:50, 3346.53it/s]
271132it [01:50, 3370.64it/s]
280844it [01:50, 3481.41it/s]
278286it [01:50, 3422.68it/s]
269851it [01:50, 3456.99it/s]
271489it [01:50, 3426.86it/s]
281194it [01:50, 3366.54it/s]
278630it [01:50, 3315.65it/s]
270199it [01:50, 3326.36it/s]
271834it [01:50, 3324.17it/s]
281560it [01:50, 3448.93it/s]
278996it [01:50, 3413.47it/s]
270565it [01:51, 3419.99it/s]
272208it [01:50, 3442.57it/s]
281907it [01:50, 3321.98it/s]
279349it [01:50, 3447.21it/s]
270934it [01:51, 3495.85it/s]
272575it [01:51, 3508.24it/s]
282276it [01:50, 3426.13it/s]
279695it [01:50, 3346.64it/s]
271286it [01:51, 3371.19it/s]
272928it [01:51, 3383.31it/s]
282644it [01:50, 3497.90it/s]
280061it [01:50, 3435.21it/s]
271654it [01:51, 3458.68it/s]
273292it [01:51, 3454.75it/s]
282996it [01:51, 3377.89it/s]
280406it [01:51, 3330.32it/s]
272002it [01:51, 3352.00it/s]
273639it [01:51, 3351.11it/s]
283363it [01:51, 3459.25it/s]
280776it [01:51, 3430.17it/s]
272355it [01:51, 3402.92it/s]
273999it [01:51, 3421.98it/s]
283711it [01:51, 3356.20it/s]
281121it [01:51, 3338.96it/s]
272697it [01:51, 3298.98it/s]
274343it [01:51, 3320.01it/s]
284075it [01:51, 3436.72it/s]
281485it [01:51, 3425.22it/s]
273064it [01:51, 3404.95it/s]
274714it [01:51, 3430.44it/s]
284421it [01:51, 3313.69it/s]
281838it [01:51, 3453.95it/s]
273427it [01:51, 3464.80it/s]
275085it [01:51, 3511.54it/s]
284790it [01:51, 3420.99it/s]
282185it [01:51, 3344.87it/s]
273775it [01:51, 3367.07it/s]
275438it [01:51, 3384.17it/s]
285159it [01:51, 3497.59it/s]
282550it [01:51, 3430.94it/s]
274129it [01:52, 3415.69it/s]
275806it [01:52, 3460.16it/s]
285511it [01:51, 3384.97it/s]
282895it [01:51, 3326.05it/s]
274472it [01:52, 3314.24it/s]
276154it [01:52, 3328.57it/s]
285881it [01:51, 3473.83it/s]
283259it [01:51, 3415.60it/s]
274833it [01:52, 3398.01it/s]
276519it [01:52, 3419.90it/s]
286230it [01:52, 3366.87it/s]
283602it [01:52, 3312.65it/s]
275177it [01:52, 3302.99it/s]
276863it [01:52, 3319.68it/s]
286599it [01:52, 3458.92it/s]
283969it [01:52, 3414.92it/s]
275547it [01:52, 3415.70it/s]
277237it [01:52, 3437.82it/s]
286947it [01:52, 3343.90it/s]
284327it [01:52, 3460.79it/s]
287112it [01:52, 2556.71it/s]

275914it [01:52, 3488.52it/s]
277606it [01:52, 3508.43it/s]
284675it [01:52, 3351.62it/s]
276265it [01:52, 3335.32it/s]
277959it [01:52, 3387.92it/s]
285043it [01:52, 3443.66it/s]
276631it [01:52, 3426.48it/s]
278313it [01:52, 3430.47it/s]
285389it [01:52, 3342.22it/s]
276976it [01:52, 3319.48it/s]
278658it [01:52, 3335.28it/s]
285753it [01:52, 3426.56it/s]
277343it [01:53, 3418.47it/s]
279028it [01:52, 3438.44it/s]
286097it [01:52, 3319.15it/s]
277697it [01:53, 3307.45it/s]
279377it [01:53, 3331.13it/s]
286457it [01:52, 3392.13it/s]
279748it [01:53, 3439.25it/s]
278053it [01:53, 3377.42it/s]
286829it [01:52, 3486.80it/s]
280115it [01:53, 3504.67it/s]
278418it [01:53, 3454.76it/s]
287112it [01:53, 2539.03it/s]
2022-07-07 15:21:36 | INFO | root | success load 287112 data
2022-07-07 15:21:36 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-07 15:21:36 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-07 15:21:36 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-07 15:21:36 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-07 15:21:36 | INFO | transformer.tokenization_utils | loading file None
2022-07-07 15:21:36 | INFO | transformer.tokenization_utils | loading file None
2022-07-07 15:21:36 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json

278765it [01:53, 3343.68it/s]
280467it [01:53, 3383.36it/s]
279133it [01:53, 3438.85it/s]
280830it [01:53, 3452.05it/s]
279479it [01:53, 3330.16it/s]
281177it [01:53, 3346.93it/s]
279844it [01:53, 3419.86it/s]
281547it [01:53, 3444.40it/s]
280194it [01:53, 3442.63it/s]
281897it [01:53, 3339.86it/s]
280540it [01:54, 3330.50it/s]
282268it [01:53, 3444.76it/s]
280909it [01:54, 3433.79it/s]
282639it [01:54, 3519.96it/s]
281254it [01:54, 3317.13it/s]
282993it [01:54, 3359.77it/s]
281618it [01:54, 3408.04it/s]
283362it [01:54, 3452.37it/s]
281961it [01:54, 3279.47it/s]
283710it [01:54, 3345.75it/s]
282324it [01:54, 3376.86it/s]
284078it [01:54, 3440.20it/s]
282687it [01:54, 3449.79it/s]
284424it [01:54, 3322.83it/s]
283034it [01:54, 3321.00it/s]
284766it [01:54, 3349.39it/s]
283394it [01:54, 3400.65it/s]
285121it [01:54, 3405.44it/s]
283736it [01:54, 3290.27it/s]
285463it [01:54, 3289.48it/s]
284089it [01:55, 3357.82it/s]
285831it [01:54, 3400.09it/s]
284427it [01:55, 3265.50it/s]
286173it [01:55, 3313.83it/s]
284792it [01:55, 3373.82it/s]
286549it [01:55, 3440.01it/s]
285158it [01:55, 3454.14it/s]
286925it [01:55, 3533.04it/s]
287112it [01:55, 2488.76it/s]

285505it [01:55, 3337.89it/s]
285859it [01:55, 3393.83it/s]
286200it [01:55, 3294.18it/s]
286569it [01:55, 3405.49it/s]
286937it [01:55, 3313.32it/s]
287112it [01:55, 2476.08it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-07 15:26:40 | INFO | train_inner | epoch 001:    100 / 1122 loss=nan, nll_loss=12.239, mask_ins=7.591, word_ins_ml=12.71, word_reposition=5.805, kpe=nan, ppl=nan, wps=6943.1, ups=0.34, wpb=20527, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=24.118, clip=21, loss_scale=128, train_wall=260, wall=417
2022-07-07 15:31:35 | INFO | train_inner | epoch 001:    200 / 1122 loss=22.647, nll_loss=11.252, mask_ins=4.691, word_ins_ml=11.83, word_reposition=4.718, kpe=1.408, ppl=6.56712e+06, wps=6969.3, ups=0.34, wpb=20583.2, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=19.405, clip=0, loss_scale=128, train_wall=253, wall=712
2022-07-07 15:36:30 | INFO | train_inner | epoch 001:    300 / 1122 loss=17.762, nll_loss=11.322, mask_ins=2.305, word_ins_ml=11.881, word_reposition=2.318, kpe=1.258, ppl=222244, wps=6980.7, ups=0.34, wpb=20561.3, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=5.735, clip=0, loss_scale=128, train_wall=253, wall=1007
2022-07-07 15:41:25 | INFO | train_inner | epoch 001:    400 / 1122 loss=16.244, nll_loss=10.973, mask_ins=1.922, word_ins_ml=11.573, word_reposition=1.557, kpe=1.192, ppl=77619.7, wps=6980.5, ups=0.34, wpb=20576.5, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=3.207, clip=0, loss_scale=128, train_wall=253, wall=1301
2022-07-07 15:46:19 | INFO | train_inner | epoch 001:    500 / 1122 loss=15.836, nll_loss=10.787, mask_ins=1.85, word_ins_ml=11.414, word_reposition=1.423, kpe=1.149, ppl=58493.4, wps=6961.8, ups=0.34, wpb=20523.5, bsz=256, num_updates=500, lr=5.009e-05, gnorm=2.966, clip=0, loss_scale=128, train_wall=253, wall=1596
2022-07-07 15:51:28 | INFO | train_inner | epoch 001:    600 / 1122 loss=15.595, nll_loss=10.529, mask_ins=1.847, word_ins_ml=11.195, word_reposition=1.434, kpe=1.119, ppl=49490.3, wps=6636.3, ups=0.32, wpb=20491.4, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=2.798, clip=0, loss_scale=242, train_wall=266, wall=1905
2022-07-07 15:56:24 | INFO | train_inner | epoch 001:    700 / 1122 loss=15.313, nll_loss=10.207, mask_ins=1.842, word_ins_ml=10.921, word_reposition=1.451, kpe=1.098, ppl=40698.6, wps=6944.2, ups=0.34, wpb=20542.5, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=2.86, clip=0, loss_scale=256, train_wall=254, wall=2201
2022-07-07 16:01:19 | INFO | train_inner | epoch 001:    800 / 1122 loss=15.016, nll_loss=9.852, mask_ins=1.833, word_ins_ml=10.62, word_reposition=1.479, kpe=1.084, ppl=33126.6, wps=6982.8, ups=0.34, wpb=20579, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=2.787, clip=0, loss_scale=256, train_wall=253, wall=2495
2022-07-07 16:06:14 | INFO | train_inner | epoch 001:    900 / 1122 loss=14.719, nll_loss=9.568, mask_ins=1.792, word_ins_ml=10.378, word_reposition=1.484, kpe=1.064, ppl=26962.6, wps=6934.2, ups=0.34, wpb=20464, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=2.716, clip=0, loss_scale=256, train_wall=253, wall=2791
2022-07-07 16:11:08 | INFO | train_inner | epoch 001:   1000 / 1122 loss=14.446, nll_loss=9.354, mask_ins=1.721, word_ins_ml=10.193, word_reposition=1.475, kpe=1.056, ppl=22324.1, wps=7008, ups=0.34, wpb=20597.8, bsz=256, num_updates=1000, lr=0.00010008, gnorm=2.495, clip=0, loss_scale=256, train_wall=252, wall=3085
2022-07-07 16:16:03 | INFO | train_inner | epoch 001:   1100 / 1122 loss=14.242, nll_loss=9.167, mask_ins=1.706, word_ins_ml=10.032, word_reposition=1.461, kpe=1.043, ppl=19376.5, wps=6929.3, ups=0.34, wpb=20473.7, bsz=256, num_updates=1100, lr=0.000110078, gnorm=2.43, clip=0, loss_scale=453, train_wall=253, wall=3380
2022-07-07 16:17:07 | INFO | train | epoch 001 | loss nan | nll_loss 10.451 | mask_ins 2.627 | word_ins_ml 11.136 | word_reposition 2.222 | kpe nan | ppl nan | wps 6928.7 | ups 0.34 | wpb 20520.3 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 6.425 | clip 1.9 | loss_scale 220 | train_wall 2858 | wall 3444
2022-07-07 16:18:30 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.892 | nll_loss 9.171 | mask_ins 1.927 | word_ins_ml 10.108 | word_reposition 1.472 | kpe 1.386 | ppl 30407.7 | wps 11967.3 | wpb 2367.6 | bsz 32 | num_updates 1122
2022-07-07 16:18:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_best.pt (epoch 1 @ 1122 updates, score 14.892) (writing took 5.45296888332814 seconds)
2022-07-07 16:22:25 | INFO | train_inner | epoch 002:     78 / 1122 loss=14.082, nll_loss=9.005, mask_ins=1.696, word_ins_ml=9.893, word_reposition=1.458, kpe=1.035, ppl=17343.2, wps=5324.7, ups=0.26, wpb=20333.3, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=2.357, clip=0, loss_scale=512, train_wall=252, wall=3762
2022-07-07 16:27:24 | INFO | train_inner | epoch 002:    178 / 1122 loss=13.864, nll_loss=8.807, mask_ins=1.679, word_ins_ml=9.72, word_reposition=1.441, kpe=1.023, ppl=14905.1, wps=6892.2, ups=0.33, wpb=20587.3, bsz=256, num_updates=1300, lr=0.000130074, gnorm=2.246, clip=0, loss_scale=512, train_wall=256, wall=4061
2022-07-07 16:32:19 | INFO | train_inner | epoch 002:    278 / 1122 loss=13.706, nll_loss=8.62, mask_ins=1.673, word_ins_ml=9.557, word_reposition=1.46, kpe=1.017, ppl=13361.3, wps=6968.3, ups=0.34, wpb=20599.8, bsz=256, num_updates=1400, lr=0.000140072, gnorm=2.323, clip=0, loss_scale=512, train_wall=254, wall=4356
2022-07-07 16:37:15 | INFO | train_inner | epoch 002:    378 / 1122 loss=13.456, nll_loss=8.375, mask_ins=1.666, word_ins_ml=9.341, word_reposition=1.435, kpe=1.014, ppl=11234.5, wps=6894.8, ups=0.34, wpb=20347.3, bsz=256, num_updates=1500, lr=0.00015007, gnorm=2.284, clip=0, loss_scale=512, train_wall=253, wall=4651
2022-07-07 16:42:10 | INFO | train_inner | epoch 002:    478 / 1122 loss=13.184, nll_loss=8.067, mask_ins=1.656, word_ins_ml=9.072, word_reposition=1.443, kpe=1.013, ppl=9308.56, wps=6967.2, ups=0.34, wpb=20567.7, bsz=256, num_updates=1600, lr=0.000160068, gnorm=2.463, clip=0, loss_scale=845, train_wall=254, wall=4947
2022-07-07 16:47:05 | INFO | train_inner | epoch 002:    578 / 1122 loss=12.865, nll_loss=7.671, mask_ins=1.662, word_ins_ml=8.725, word_reposition=1.458, kpe=1.02, ppl=7461.58, wps=6963, ups=0.34, wpb=20536.9, bsz=256, num_updates=1700, lr=0.000170066, gnorm=2.659, clip=0, loss_scale=1024, train_wall=253, wall=5241
2022-07-07 16:51:59 | INFO | train_inner | epoch 002:    678 / 1122 loss=12.459, nll_loss=7.253, mask_ins=1.658, word_ins_ml=8.362, word_reposition=1.416, kpe=1.023, ppl=5632.07, wps=6956.5, ups=0.34, wpb=20477.4, bsz=256, num_updates=1800, lr=0.000180064, gnorm=2.824, clip=0, loss_scale=1024, train_wall=253, wall=5536
2022-07-07 16:57:20 | INFO | train_inner | epoch 002:    778 / 1122 loss=nan, nll_loss=6.852, mask_ins=1.638, word_ins_ml=8.014, word_reposition=1.39, kpe=nan, ppl=nan, wps=6415, ups=0.31, wpb=20576, bsz=256, num_updates=1900, lr=0.000190062, gnorm=3.002, clip=0, loss_scale=1024, train_wall=279, wall=5857
2022-07-07 17:02:16 | INFO | train_inner | epoch 002:    878 / 1122 loss=11.736, nll_loss=6.523, mask_ins=1.629, word_ins_ml=7.731, word_reposition=1.354, kpe=1.022, ppl=3410.68, wps=6894.2, ups=0.34, wpb=20447.7, bsz=256, num_updates=2000, lr=0.00020006, gnorm=2.97, clip=0, loss_scale=1024, train_wall=255, wall=6153
2022-07-07 17:07:11 | INFO | train_inner | epoch 002:    978 / 1122 loss=11.458, nll_loss=6.261, mask_ins=1.623, word_ins_ml=7.505, word_reposition=1.314, kpe=1.017, ppl=2813.92, wps=6953.2, ups=0.34, wpb=20513.5, bsz=256, num_updates=2100, lr=0.000210058, gnorm=2.917, clip=0, loss_scale=1567, train_wall=253, wall=6448
2022-07-07 17:12:07 | INFO | train_inner | epoch 002:   1078 / 1122 loss=nan, nll_loss=6.029, mask_ins=1.636, word_ins_ml=7.304, word_reposition=1.316, kpe=nan, ppl=nan, wps=7006.6, ups=0.34, wpb=20708.1, bsz=256, num_updates=2200, lr=0.000220056, gnorm=2.812, clip=0, loss_scale=2048, train_wall=253, wall=6744
2022-07-07 17:14:16 | INFO | train | epoch 002 | loss nan | nll_loss 7.492 | mask_ins 1.655 | word_ins_ml 8.574 | word_reposition 1.402 | kpe nan | ppl nan | wps 6715.3 | ups 0.33 | wpb 20521 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 2.64 | clip 0 | loss_scale 1015 | train_wall 2870 | wall 6873
2022-07-07 17:15:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.57 | nll_loss 7.742 | mask_ins 1.744 | word_ins_ml 8.901 | word_reposition 1.423 | kpe 1.504 | ppl 12165.1 | wps 11951.6 | wpb 2367.6 | bsz 32 | num_updates 2244 | best_loss 13.57
2022-07-07 17:15:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_best.pt (epoch 2 @ 2244 updates, score 13.57) (writing took 7.620778433047235 seconds)
2022-07-07 17:18:31 | INFO | train_inner | epoch 003:     56 / 1122 loss=nan, nll_loss=5.841, mask_ins=1.633, word_ins_ml=7.142, word_reposition=1.289, kpe=nan, ppl=nan, wps=5305.7, ups=0.26, wpb=20387.7, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=2.83, clip=0, loss_scale=2048, train_wall=252, wall=7128
2022-07-07 17:23:26 | INFO | train_inner | epoch 003:    156 / 1122 loss=10.822, nll_loss=5.612, mask_ins=1.615, word_ins_ml=6.943, word_reposition=1.259, kpe=1.004, ppl=1809.72, wps=6953.8, ups=0.34, wpb=20466.9, bsz=256, num_updates=2400, lr=0.000240052, gnorm=2.642, clip=0, loss_scale=2048, train_wall=253, wall=7422
2022-07-07 17:28:21 | INFO | train_inner | epoch 003:    256 / 1122 loss=10.664, nll_loss=5.46, mask_ins=1.599, word_ins_ml=6.812, word_reposition=1.246, kpe=1.006, ppl=1622.06, wps=6962.5, ups=0.34, wpb=20590.4, bsz=256, num_updates=2500, lr=0.00025005, gnorm=2.607, clip=0, loss_scale=2048, train_wall=254, wall=7718
2022-07-07 17:33:19 | INFO | train_inner | epoch 003:    356 / 1122 loss=10.521, nll_loss=5.321, mask_ins=1.605, word_ins_ml=6.691, word_reposition=1.225, kpe=1, ppl=1469.09, wps=6898.5, ups=0.34, wpb=20552.9, bsz=256, num_updates=2600, lr=0.000260048, gnorm=2.516, clip=0, loss_scale=2888, train_wall=255, wall=8016
2022-07-07 17:38:14 | INFO | train_inner | epoch 003:    456 / 1122 loss=10.364, nll_loss=5.192, mask_ins=1.586, word_ins_ml=6.577, word_reposition=1.197, kpe=1.004, ppl=1318.17, wps=6913.9, ups=0.34, wpb=20384, bsz=256, num_updates=2700, lr=0.000270046, gnorm=2.477, clip=0, loss_scale=4096, train_wall=252, wall=8311
2022-07-07 17:43:09 | INFO | train_inner | epoch 003:    556 / 1122 loss=10.171, nll_loss=5.011, mask_ins=1.575, word_ins_ml=6.42, word_reposition=1.177, kpe=1, ppl=1153.09, wps=6954.4, ups=0.34, wpb=20480.9, bsz=256, num_updates=2800, lr=0.000280044, gnorm=2.444, clip=0, loss_scale=4096, train_wall=253, wall=8605
2022-07-07 17:48:03 | INFO | train_inner | epoch 003:    656 / 1122 loss=9.816, nll_loss=4.716, mask_ins=1.511, word_ins_ml=6.158, word_reposition=1.143, kpe=1.004, ppl=901.49, wps=6997.2, ups=0.34, wpb=20612.3, bsz=256, num_updates=2900, lr=0.000290042, gnorm=2.524, clip=0, loss_scale=4096, train_wall=253, wall=8900
2022-07-07 17:52:58 | INFO | train_inner | epoch 003:    756 / 1122 loss=9.239, nll_loss=4.23, mask_ins=1.421, word_ins_ml=5.731, word_reposition=1.083, kpe=1.004, ppl=604.25, wps=6986.8, ups=0.34, wpb=20597.8, bsz=256, num_updates=3000, lr=0.00030004, gnorm=2.547, clip=0, loss_scale=4096, train_wall=253, wall=9195
2022-07-07 17:57:52 | INFO | train_inner | epoch 003:    856 / 1122 loss=8.945, nll_loss=4.001, mask_ins=1.359, word_ins_ml=5.528, word_reposition=1.056, kpe=1.002, ppl=492.92, wps=7011.2, ups=0.34, wpb=20609.8, bsz=256, num_updates=3100, lr=0.000310038, gnorm=2.505, clip=0, loss_scale=5284, train_wall=252, wall=9489
2022-07-07 18:03:12 | INFO | train_inner | epoch 003:    956 / 1122 loss=nan, nll_loss=3.891, mask_ins=1.305, word_ins_ml=5.429, word_reposition=1.023, kpe=nan, ppl=nan, wps=6421.9, ups=0.31, wpb=20572.9, bsz=256, num_updates=3200, lr=0.000320036, gnorm=2.514, clip=0, loss_scale=8192, train_wall=279, wall=9809
2022-07-07 18:07:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-07 18:08:12 | INFO | train_inner | epoch 003:   1057 / 1122 loss=8.664, nll_loss=3.818, mask_ins=1.278, word_ins_ml=5.363, word_reposition=1.017, kpe=1.006, ppl=405.69, wps=6849.4, ups=0.33, wpb=20515.2, bsz=256, num_updates=3300, lr=0.000330034, gnorm=2.419, clip=0, loss_scale=7746, train_wall=257, wall=10109
2022-07-07 18:11:23 | INFO | train | epoch 003 | loss nan | nll_loss 4.724 | mask_ins 1.478 | word_ins_ml 6.164 | word_reposition 1.141 | kpe nan | ppl nan | wps 6712.4 | ups 0.33 | wpb 20521.1 | bsz 255.8 | num_updates 3365 | lr 0.000336533 | gnorm 2.529 | clip 0 | loss_scale 4321 | train_wall 2866 | wall 10300
2022-07-07 18:12:46 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.284 | nll_loss 6.619 | mask_ins 1.558 | word_ins_ml 7.988 | word_reposition 1.394 | kpe 1.345 | ppl 4988.73 | wps 11965.2 | wpb 2367.6 | bsz 32 | num_updates 3365 | best_loss 12.284
2022-07-07 18:12:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_best.pt (epoch 3 @ 3365 updates, score 12.284) (writing took 7.645150105468929 seconds)
2022-07-07 18:14:36 | INFO | train_inner | epoch 004:     35 / 1122 loss=8.537, nll_loss=3.757, mask_ins=1.237, word_ins_ml=5.308, word_reposition=0.99, kpe=1.002, ppl=371.56, wps=5293, ups=0.26, wpb=20332.9, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=2.502, clip=0, loss_scale=4096, train_wall=252, wall=10493
2022-07-07 18:19:32 | INFO | train_inner | epoch 004:    135 / 1122 loss=8.4, nll_loss=3.633, mask_ins=1.225, word_ins_ml=5.197, word_reposition=0.982, kpe=0.996, ppl=337.73, wps=6918.4, ups=0.34, wpb=20505.9, bsz=256, num_updates=3500, lr=0.00035003, gnorm=2.383, clip=0, loss_scale=4096, train_wall=254, wall=10789
2022-07-07 18:24:27 | INFO | train_inner | epoch 004:    235 / 1122 loss=8.316, nll_loss=3.585, mask_ins=1.19, word_ins_ml=5.153, word_reposition=0.975, kpe=0.998, ppl=318.74, wps=6996.7, ups=0.34, wpb=20607.3, bsz=256, num_updates=3600, lr=0.000360028, gnorm=2.279, clip=0, loss_scale=4096, train_wall=253, wall=11084
2022-07-07 18:29:22 | INFO | train_inner | epoch 004:    335 / 1122 loss=8.258, nll_loss=3.544, mask_ins=1.186, word_ins_ml=5.115, word_reposition=0.957, kpe=1, ppl=306.15, wps=6935.9, ups=0.34, wpb=20432.2, bsz=256, num_updates=3700, lr=0.000370026, gnorm=2.319, clip=0, loss_scale=4096, train_wall=253, wall=11378
2022-07-07 18:34:19 | INFO | train_inner | epoch 004:    435 / 1122 loss=8.154, nll_loss=3.483, mask_ins=1.157, word_ins_ml=5.06, word_reposition=0.946, kpe=0.992, ppl=284.87, wps=6941.7, ups=0.34, wpb=20656.5, bsz=256, num_updates=3800, lr=0.000380024, gnorm=2.245, clip=0, loss_scale=4096, train_wall=255, wall=11676
2022-07-07 18:34:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-07 18:39:17 | INFO | train_inner | epoch 004:    536 / 1122 loss=8.11, nll_loss=3.462, mask_ins=1.131, word_ins_ml=5.04, word_reposition=0.942, kpe=0.997, ppl=276.27, wps=6870.2, ups=0.34, wpb=20484.6, bsz=256, num_updates=3900, lr=0.000390022, gnorm=2.167, clip=0, loss_scale=4339, train_wall=256, wall=11974
2022-07-07 18:44:12 | INFO | train_inner | epoch 004:    636 / 1122 loss=nan, nll_loss=3.443, mask_ins=1.148, word_ins_ml=5.022, word_reposition=0.931, kpe=nan, ppl=nan, wps=6950.4, ups=0.34, wpb=20511, bsz=256, num_updates=4000, lr=0.00040002, gnorm=2.197, clip=0, loss_scale=4096, train_wall=254, wall=12269
2022-07-07 18:46:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 18:49:10 | INFO | train_inner | epoch 004:    737 / 1122 loss=8.053, nll_loss=3.426, mask_ins=1.111, word_ins_ml=5.005, word_reposition=0.935, kpe=1.001, ppl=265.56, wps=6929.6, ups=0.34, wpb=20629.8, bsz=256, num_updates=4100, lr=0.000410018, gnorm=2.089, clip=0, loss_scale=2798, train_wall=255, wall=12567
2022-07-07 18:54:05 | INFO | train_inner | epoch 004:    837 / 1122 loss=nan, nll_loss=3.356, mask_ins=1.107, word_ins_ml=4.943, word_reposition=0.922, kpe=nan, ppl=nan, wps=6939.7, ups=0.34, wpb=20444.7, bsz=256, num_updates=4200, lr=0.000420016, gnorm=2.18, clip=0, loss_scale=2048, train_wall=253, wall=12862
2022-07-07 18:59:00 | INFO | train_inner | epoch 004:    937 / 1122 loss=7.998, nll_loss=3.383, mask_ins=1.104, word_ins_ml=4.965, word_reposition=0.93, kpe=0.999, ppl=255.67, wps=6997, ups=0.34, wpb=20636.4, bsz=256, num_updates=4300, lr=0.000430014, gnorm=2.112, clip=0, loss_scale=2048, train_wall=253, wall=13156
2022-07-07 19:03:54 | INFO | train_inner | epoch 004:   1037 / 1122 loss=7.948, nll_loss=3.372, mask_ins=1.08, word_ins_ml=4.954, word_reposition=0.913, kpe=1.001, ppl=246.96, wps=6940.8, ups=0.34, wpb=20459.3, bsz=256, num_updates=4400, lr=0.000440012, gnorm=2.098, clip=0, loss_scale=2048, train_wall=252, wall=13451
2022-07-07 19:08:28 | INFO | train | epoch 004 | loss nan | nll_loss 3.466 | mask_ins 1.142 | word_ins_ml 5.043 | word_reposition 0.943 | kpe nan | ppl nan | wps 6709 | ups 0.33 | wpb 20519.5 | bsz 255.8 | num_updates 4485 | lr 0.00044851 | gnorm 2.208 | clip 0 | loss_scale 3298 | train_wall 2863 | wall 13725
2022-07-07 19:09:52 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 13.196 | nll_loss 6.952 | mask_ins 1.72 | word_ins_ml 8.291 | word_reposition 1.653 | kpe 1.531 | ppl 9380.99 | wps 11913 | wpb 2367.6 | bsz 32 | num_updates 4485 | best_loss 12.284
2022-07-07 19:09:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_last.pt (epoch 4 @ 4485 updates, score 13.196) (writing took 4.3206991776824 seconds)
2022-07-07 19:10:40 | INFO | train_inner | epoch 005:     15 / 1122 loss=7.912, nll_loss=3.316, mask_ins=1.085, word_ins_ml=4.904, word_reposition=0.916, kpe=1.006, ppl=240.81, wps=5018.8, ups=0.25, wpb=20363.9, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=2.169, clip=0, loss_scale=2048, train_wall=277, wall=13857
2022-07-07 19:15:35 | INFO | train_inner | epoch 005:    115 / 1122 loss=7.874, nll_loss=3.286, mask_ins=1.083, word_ins_ml=4.877, word_reposition=0.917, kpe=0.996, ppl=234.55, wps=7001.9, ups=0.34, wpb=20646.2, bsz=256, num_updates=4600, lr=0.000460008, gnorm=2.113, clip=0, loss_scale=3113, train_wall=253, wall=14152
2022-07-07 19:16:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 19:20:33 | INFO | train_inner | epoch 005:    216 / 1122 loss=7.795, nll_loss=3.24, mask_ins=1.066, word_ins_ml=4.835, word_reposition=0.901, kpe=0.994, ppl=222.09, wps=6881.8, ups=0.34, wpb=20506, bsz=256, num_updates=4700, lr=0.000470006, gnorm=2.025, clip=0, loss_scale=2210, train_wall=256, wall=14450
2022-07-07 19:24:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 19:25:32 | INFO | train_inner | epoch 005:    317 / 1122 loss=7.839, nll_loss=3.286, mask_ins=1.073, word_ins_ml=4.874, word_reposition=0.896, kpe=0.996, ppl=228.9, wps=6895, ups=0.33, wpb=20586.5, bsz=256, num_updates=4800, lr=0.000480004, gnorm=2.037, clip=0, loss_scale=1855, train_wall=256, wall=14748
2022-07-07 19:30:28 | INFO | train_inner | epoch 005:    417 / 1122 loss=nan, nll_loss=3.245, mask_ins=1.059, word_ins_ml=4.837, word_reposition=0.905, kpe=nan, ppl=nan, wps=6916.8, ups=0.34, wpb=20468.4, bsz=256, num_updates=4900, lr=0.000490002, gnorm=1.973, clip=0, loss_scale=1024, train_wall=253, wall=15044
2022-07-07 19:35:22 | INFO | train_inner | epoch 005:    517 / 1122 loss=7.747, nll_loss=3.217, mask_ins=1.052, word_ins_ml=4.81, word_reposition=0.892, kpe=0.992, ppl=214.75, wps=6927.3, ups=0.34, wpb=20383.8, bsz=256, num_updates=5000, lr=0.0005, gnorm=1.97, clip=0, loss_scale=1024, train_wall=253, wall=15339
2022-07-07 19:40:19 | INFO | train_inner | epoch 005:    617 / 1122 loss=7.711, nll_loss=3.214, mask_ins=1.021, word_ins_ml=4.807, word_reposition=0.886, kpe=0.996, ppl=209.46, wps=6898.7, ups=0.34, wpb=20491.5, bsz=256, num_updates=5100, lr=0.000495074, gnorm=1.91, clip=0, loss_scale=1024, train_wall=254, wall=15636
2022-07-07 19:45:13 | INFO | train_inner | epoch 005:    717 / 1122 loss=7.687, nll_loss=3.165, mask_ins=1.041, word_ins_ml=4.764, word_reposition=0.888, kpe=0.994, ppl=206.07, wps=6989.3, ups=0.34, wpb=20532.4, bsz=256, num_updates=5200, lr=0.00049029, gnorm=1.89, clip=0, loss_scale=1024, train_wall=252, wall=15929
2022-07-07 19:50:08 | INFO | train_inner | epoch 005:    817 / 1122 loss=7.667, nll_loss=3.166, mask_ins=1.026, word_ins_ml=4.762, word_reposition=0.884, kpe=0.995, ppl=203.25, wps=6998.2, ups=0.34, wpb=20650.7, bsz=256, num_updates=5300, lr=0.000485643, gnorm=1.865, clip=0, loss_scale=1096, train_wall=253, wall=16225
2022-07-07 19:55:02 | INFO | train_inner | epoch 005:    917 / 1122 loss=7.634, nll_loss=3.129, mask_ins=1.033, word_ins_ml=4.729, word_reposition=0.879, kpe=0.993, ppl=198.62, wps=6994.6, ups=0.34, wpb=20558.5, bsz=256, num_updates=5400, lr=0.000481125, gnorm=1.862, clip=0, loss_scale=2048, train_wall=253, wall=16518
2022-07-07 19:57:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 19:59:59 | INFO | train_inner | epoch 005:   1018 / 1122 loss=nan, nll_loss=3.089, mask_ins=1.013, word_ins_ml=4.692, word_reposition=0.87, kpe=nan, ppl=nan, wps=6882.4, ups=0.34, wpb=20459.1, bsz=256, num_updates=5500, lr=0.000476731, gnorm=1.825, clip=0, loss_scale=1440, train_wall=256, wall=16816
2022-07-07 20:04:54 | INFO | train_inner | epoch 005:   1118 / 1122 loss=7.57, nll_loss=3.082, mask_ins=1.023, word_ins_ml=4.685, word_reposition=0.875, kpe=0.988, ppl=190.03, wps=6967.3, ups=0.34, wpb=20588.5, bsz=256, num_updates=5600, lr=0.000472456, gnorm=1.885, clip=0, loss_scale=1024, train_wall=253, wall=17111
2022-07-07 20:05:05 | INFO | train | epoch 005 | loss nan | nll_loss 3.195 | mask_ins 1.045 | word_ins_ml 4.791 | word_reposition 0.891 | kpe nan | ppl nan | wps 6760.1 | ups 0.33 | wpb 20519.8 | bsz 255.8 | num_updates 5604 | lr 0.000472287 | gnorm 1.953 | clip 0 | loss_scale 1541 | train_wall 2840 | wall 17122
2022-07-07 20:06:28 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.481 | nll_loss 5.98 | mask_ins 1.516 | word_ins_ml 7.356 | word_reposition 1.207 | kpe 1.402 | ppl 2858.28 | wps 11962.1 | wpb 2367.6 | bsz 32 | num_updates 5604 | best_loss 11.481
2022-07-07 20:06:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_best.pt (epoch 5 @ 5604 updates, score 11.481) (writing took 7.808206115849316 seconds)
2022-07-07 20:11:18 | INFO | train_inner | epoch 006:     96 / 1122 loss=7.566, nll_loss=3.106, mask_ins=1.015, word_ins_ml=4.705, word_reposition=0.867, kpe=0.979, ppl=189.52, wps=5295.9, ups=0.26, wpb=20336, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=1.871, clip=0, loss_scale=1024, train_wall=252, wall=17495
2022-07-07 20:16:39 | INFO | train_inner | epoch 006:    196 / 1122 loss=7.494, nll_loss=3.039, mask_ins=1.003, word_ins_ml=4.645, word_reposition=0.861, kpe=0.985, ppl=180.32, wps=6419.5, ups=0.31, wpb=20603.5, bsz=256, num_updates=5800, lr=0.000464238, gnorm=1.852, clip=0, loss_scale=1024, train_wall=278, wall=17816
2022-07-07 20:21:34 | INFO | train_inner | epoch 006:    296 / 1122 loss=nan, nll_loss=3.019, mask_ins=1.001, word_ins_ml=4.626, word_reposition=0.863, kpe=nan, ppl=nan, wps=6986.1, ups=0.34, wpb=20594.7, bsz=256, num_updates=5900, lr=0.000460287, gnorm=1.792, clip=0, loss_scale=1024, train_wall=253, wall=18111
2022-07-07 20:26:29 | INFO | train_inner | epoch 006:    396 / 1122 loss=7.422, nll_loss=2.979, mask_ins=0.997, word_ins_ml=4.591, word_reposition=0.853, kpe=0.981, ppl=171.46, wps=6984.6, ups=0.34, wpb=20576.6, bsz=256, num_updates=6000, lr=0.000456435, gnorm=1.831, clip=0, loss_scale=1516, train_wall=253, wall=18406
2022-07-07 20:31:24 | INFO | train_inner | epoch 006:    496 / 1122 loss=7.422, nll_loss=2.985, mask_ins=0.995, word_ins_ml=4.595, word_reposition=0.857, kpe=0.976, ppl=171.51, wps=6954.3, ups=0.34, wpb=20499.7, bsz=256, num_updates=6100, lr=0.000452679, gnorm=1.796, clip=0, loss_scale=2048, train_wall=253, wall=18700
2022-07-07 20:36:19 | INFO | train_inner | epoch 006:    596 / 1122 loss=7.416, nll_loss=3.006, mask_ins=0.981, word_ins_ml=4.612, word_reposition=0.849, kpe=0.975, ppl=170.77, wps=6893.2, ups=0.34, wpb=20363.4, bsz=256, num_updates=6200, lr=0.000449013, gnorm=1.759, clip=0, loss_scale=2048, train_wall=254, wall=18996
2022-07-07 20:41:17 | INFO | train_inner | epoch 006:    696 / 1122 loss=7.409, nll_loss=3.001, mask_ins=0.976, word_ins_ml=4.608, word_reposition=0.85, kpe=0.975, ppl=169.93, wps=6917.3, ups=0.34, wpb=20637, bsz=256, num_updates=6300, lr=0.000445435, gnorm=1.734, clip=0, loss_scale=2048, train_wall=255, wall=19294
2022-07-07 20:46:11 | INFO | train_inner | epoch 006:    796 / 1122 loss=nan, nll_loss=2.907, mask_ins=0.958, word_ins_ml=4.524, word_reposition=0.844, kpe=nan, ppl=nan, wps=6976.3, ups=0.34, wpb=20487.4, bsz=256, num_updates=6400, lr=0.000441942, gnorm=1.722, clip=0, loss_scale=2048, train_wall=252, wall=19588
2022-07-07 20:47:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 20:51:08 | INFO | train_inner | epoch 006:    897 / 1122 loss=7.375, nll_loss=2.97, mask_ins=0.972, word_ins_ml=4.579, word_reposition=0.848, kpe=0.977, ppl=166.03, wps=6948.1, ups=0.34, wpb=20637.4, bsz=256, num_updates=6500, lr=0.000438529, gnorm=1.808, clip=0, loss_scale=1196, train_wall=256, wall=19885
2022-07-07 20:56:02 | INFO | train_inner | epoch 006:    997 / 1122 loss=7.328, nll_loss=2.93, mask_ins=0.967, word_ins_ml=4.542, word_reposition=0.845, kpe=0.974, ppl=160.65, wps=6973.6, ups=0.34, wpb=20483.4, bsz=256, num_updates=6600, lr=0.000435194, gnorm=1.711, clip=0, loss_scale=1024, train_wall=252, wall=20179
2022-07-07 21:00:57 | INFO | train_inner | epoch 006:   1097 / 1122 loss=7.31, nll_loss=2.924, mask_ins=0.96, word_ins_ml=4.537, word_reposition=0.837, kpe=0.976, ppl=158.65, wps=6944.5, ups=0.34, wpb=20512.8, bsz=256, num_updates=6700, lr=0.000431934, gnorm=1.785, clip=0, loss_scale=1024, train_wall=253, wall=20474
2022-07-07 21:02:10 | INFO | train | epoch 006 | loss nan | nll_loss 2.986 | mask_ins 0.984 | word_ins_ml 4.594 | word_reposition 0.852 | kpe nan | ppl nan | wps 6716.3 | ups 0.33 | wpb 20519.5 | bsz 255.8 | num_updates 6725 | lr 0.000431131 | gnorm 1.784 | clip 0 | loss_scale 1448 | train_wall 2863 | wall 20547
2022-07-07 21:03:33 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.891 | nll_loss 6.196 | mask_ins 1.564 | word_ins_ml 7.574 | word_reposition 1.361 | kpe 1.392 | ppl 3797.45 | wps 11980.4 | wpb 2367.6 | bsz 32 | num_updates 6725 | best_loss 11.481
2022-07-07 21:03:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_last.pt (epoch 6 @ 6725 updates, score 11.891) (writing took 4.585735322907567 seconds)
2022-07-07 21:07:18 | INFO | train_inner | epoch 007:     75 / 1122 loss=7.236, nll_loss=2.859, mask_ins=0.955, word_ins_ml=4.479, word_reposition=0.838, kpe=0.964, ppl=150.71, wps=5331.9, ups=0.26, wpb=20312.5, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=1.752, clip=0, loss_scale=1024, train_wall=252, wall=20855
2022-07-07 21:12:14 | INFO | train_inner | epoch 007:    175 / 1122 loss=7.215, nll_loss=2.857, mask_ins=0.942, word_ins_ml=4.476, word_reposition=0.835, kpe=0.963, ppl=148.59, wps=6969.9, ups=0.34, wpb=20588.8, bsz=256, num_updates=6900, lr=0.000425628, gnorm=1.685, clip=0, loss_scale=1024, train_wall=254, wall=21150
2022-07-07 21:12:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-07 21:17:12 | INFO | train_inner | epoch 007:    276 / 1122 loss=7.27, nll_loss=2.911, mask_ins=0.94, word_ins_ml=4.523, word_reposition=0.843, kpe=0.965, ppl=154.38, wps=6919.8, ups=0.34, wpb=20632.8, bsz=256, num_updates=7000, lr=0.000422577, gnorm=1.724, clip=0, loss_scale=537, train_wall=256, wall=21449
2022-07-07 21:22:31 | INFO | train_inner | epoch 007:    376 / 1122 loss=7.181, nll_loss=2.845, mask_ins=0.935, word_ins_ml=4.464, word_reposition=0.826, kpe=0.955, ppl=145.06, wps=6429.4, ups=0.31, wpb=20531.4, bsz=256, num_updates=7100, lr=0.000419591, gnorm=1.658, clip=0, loss_scale=512, train_wall=277, wall=21768
2022-07-07 21:27:26 | INFO | train_inner | epoch 007:    476 / 1122 loss=nan, nll_loss=2.823, mask_ins=0.926, word_ins_ml=4.443, word_reposition=0.825, kpe=nan, ppl=nan, wps=6968.8, ups=0.34, wpb=20532.2, bsz=256, num_updates=7200, lr=0.000416667, gnorm=1.726, clip=0, loss_scale=512, train_wall=253, wall=22063
2022-07-07 21:32:21 | INFO | train_inner | epoch 007:    576 / 1122 loss=7.208, nll_loss=2.874, mask_ins=0.932, word_ins_ml=4.488, word_reposition=0.827, kpe=0.961, ppl=147.87, wps=6929.2, ups=0.34, wpb=20434.1, bsz=256, num_updates=7300, lr=0.000413803, gnorm=1.67, clip=0, loss_scale=512, train_wall=253, wall=22357
2022-07-07 21:37:16 | INFO | train_inner | epoch 007:    676 / 1122 loss=7.181, nll_loss=2.848, mask_ins=0.936, word_ins_ml=4.465, word_reposition=0.822, kpe=0.959, ppl=145.15, wps=6961.4, ups=0.34, wpb=20544, bsz=256, num_updates=7400, lr=0.000410997, gnorm=1.678, clip=0, loss_scale=512, train_wall=253, wall=22653
2022-07-07 21:42:13 | INFO | train_inner | epoch 007:    776 / 1122 loss=7.165, nll_loss=2.832, mask_ins=0.925, word_ins_ml=4.45, word_reposition=0.83, kpe=0.96, ppl=143.46, wps=6910.9, ups=0.34, wpb=20543.5, bsz=256, num_updates=7500, lr=0.000408248, gnorm=1.813, clip=0, loss_scale=942, train_wall=255, wall=22950
2022-07-07 21:47:08 | INFO | train_inner | epoch 007:    876 / 1122 loss=7.131, nll_loss=2.814, mask_ins=0.916, word_ins_ml=4.434, word_reposition=0.819, kpe=0.962, ppl=140.2, wps=6973.8, ups=0.34, wpb=20546.2, bsz=256, num_updates=7600, lr=0.000405554, gnorm=1.659, clip=0, loss_scale=1024, train_wall=253, wall=23244
2022-07-07 21:52:03 | INFO | train_inner | epoch 007:    976 / 1122 loss=7.167, nll_loss=2.838, mask_ins=0.931, word_ins_ml=4.455, word_reposition=0.821, kpe=0.961, ppl=143.74, wps=6976.2, ups=0.34, wpb=20570.9, bsz=256, num_updates=7700, lr=0.000402911, gnorm=1.961, clip=0, loss_scale=1024, train_wall=253, wall=23539
2022-07-07 21:56:57 | INFO | train_inner | epoch 007:   1076 / 1122 loss=7.16, nll_loss=2.829, mask_ins=0.921, word_ins_ml=4.446, word_reposition=0.82, kpe=0.972, ppl=142.98, wps=6976.8, ups=0.34, wpb=20527.8, bsz=256, num_updates=7800, lr=0.00040032, gnorm=1.795, clip=0, loss_scale=1024, train_wall=253, wall=23834
2022-07-07 21:59:11 | INFO | train | epoch 007 | loss nan | nll_loss 2.844 | mask_ins 0.931 | word_ins_ml 4.462 | word_reposition 0.827 | kpe nan | ppl nan | wps 6724.4 | ups 0.33 | wpb 20519.7 | bsz 255.8 | num_updates 7846 | lr 0.000399145 | gnorm 1.738 | clip 0 | loss_scale 790 | train_wall 2863 | wall 23968
2022-07-07 22:00:33 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.466 | nll_loss 5.903 | mask_ins 1.484 | word_ins_ml 7.308 | word_reposition 1.31 | kpe 1.364 | ppl 2829.28 | wps 12085.3 | wpb 2367.6 | bsz 32 | num_updates 7846 | best_loss 11.466
2022-07-07 22:00:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_best.pt (epoch 7 @ 7846 updates, score 11.466) (writing took 8.034811781719327 seconds)
2022-07-07 22:03:21 | INFO | train_inner | epoch 008:     54 / 1122 loss=7.086, nll_loss=2.762, mask_ins=0.919, word_ins_ml=4.388, word_reposition=0.824, kpe=0.955, ppl=135.88, wps=5314.9, ups=0.26, wpb=20398.9, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=1.742, clip=0, loss_scale=1024, train_wall=252, wall=24217
2022-07-07 22:08:15 | INFO | train_inner | epoch 008:    154 / 1122 loss=nan, nll_loss=2.733, mask_ins=0.909, word_ins_ml=4.361, word_reposition=0.817, kpe=nan, ppl=nan, wps=6975.1, ups=0.34, wpb=20517.5, bsz=256, num_updates=8000, lr=0.000395285, gnorm=1.581, clip=0, loss_scale=1761, train_wall=252, wall=24512
2022-07-07 22:09:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 22:13:12 | INFO | train_inner | epoch 008:    255 / 1122 loss=7.04, nll_loss=2.753, mask_ins=0.907, word_ins_ml=4.378, word_reposition=0.805, kpe=0.95, ppl=131.64, wps=6888.3, ups=0.34, wpb=20501.5, bsz=256, num_updates=8100, lr=0.000392837, gnorm=1.68, clip=0, loss_scale=1348, train_wall=256, wall=24809
2022-07-07 22:18:07 | INFO | train_inner | epoch 008:    355 / 1122 loss=7.06, nll_loss=2.773, mask_ins=0.905, word_ins_ml=4.395, word_reposition=0.806, kpe=0.953, ppl=133.41, wps=6963.6, ups=0.34, wpb=20550.8, bsz=256, num_updates=8200, lr=0.000390434, gnorm=1.586, clip=0, loss_scale=1024, train_wall=253, wall=25104
2022-07-07 22:23:02 | INFO | train_inner | epoch 008:    455 / 1122 loss=7.017, nll_loss=2.729, mask_ins=0.895, word_ins_ml=4.356, word_reposition=0.818, kpe=0.947, ppl=129.49, wps=6971.9, ups=0.34, wpb=20539.7, bsz=256, num_updates=8300, lr=0.000388075, gnorm=1.669, clip=0, loss_scale=1024, train_wall=253, wall=25399
2022-07-07 22:28:24 | INFO | train_inner | epoch 008:    555 / 1122 loss=7.016, nll_loss=2.732, mask_ins=0.907, word_ins_ml=4.358, word_reposition=0.801, kpe=0.949, ppl=129.41, wps=6396.7, ups=0.31, wpb=20568.3, bsz=256, num_updates=8400, lr=0.000385758, gnorm=1.543, clip=0, loss_scale=1024, train_wall=279, wall=25720
2022-07-07 22:33:19 | INFO | train_inner | epoch 008:    655 / 1122 loss=7.046, nll_loss=2.77, mask_ins=0.9, word_ins_ml=4.391, word_reposition=0.808, kpe=0.947, ppl=132.11, wps=6982.4, ups=0.34, wpb=20618.5, bsz=256, num_updates=8500, lr=0.000383482, gnorm=1.554, clip=0, loss_scale=1024, train_wall=253, wall=26016
2022-07-07 22:38:15 | INFO | train_inner | epoch 008:    755 / 1122 loss=6.999, nll_loss=2.721, mask_ins=0.902, word_ins_ml=4.348, word_reposition=0.804, kpe=0.945, ppl=127.91, wps=6932, ups=0.34, wpb=20488.6, bsz=256, num_updates=8600, lr=0.000381246, gnorm=1.556, clip=0, loss_scale=1608, train_wall=254, wall=26311
2022-07-07 22:43:09 | INFO | train_inner | epoch 008:    855 / 1122 loss=nan, nll_loss=2.733, mask_ins=0.894, word_ins_ml=4.357, word_reposition=0.804, kpe=nan, ppl=nan, wps=6981.6, ups=0.34, wpb=20560.7, bsz=256, num_updates=8700, lr=0.000379049, gnorm=1.553, clip=0, loss_scale=2048, train_wall=253, wall=26606
2022-07-07 22:46:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 22:48:09 | INFO | train_inner | epoch 008:    956 / 1122 loss=6.966, nll_loss=2.689, mask_ins=0.897, word_ins_ml=4.318, word_reposition=0.805, kpe=0.945, ppl=124.98, wps=6853.4, ups=0.33, wpb=20576.7, bsz=256, num_updates=8800, lr=0.000376889, gnorm=1.6, clip=0, loss_scale=1744, train_wall=258, wall=26906
2022-07-07 22:53:04 | INFO | train_inner | epoch 008:   1056 / 1122 loss=6.995, nll_loss=2.728, mask_ins=0.894, word_ins_ml=4.352, word_reposition=0.799, kpe=0.949, ppl=127.54, wps=6899.9, ups=0.34, wpb=20344.8, bsz=256, num_updates=8900, lr=0.000374766, gnorm=1.578, clip=0, loss_scale=1024, train_wall=253, wall=27201
2022-07-07 22:56:18 | INFO | train | epoch 008 | loss nan | nll_loss 2.738 | mask_ins 0.902 | word_ins_ml 4.363 | word_reposition 0.808 | kpe nan | ppl nan | wps 6705.7 | ups 0.33 | wpb 20521.5 | bsz 255.8 | num_updates 8966 | lr 0.000373384 | gnorm 1.598 | clip 0 | loss_scale 1327 | train_wall 2866 | wall 27395
2022-07-07 22:57:40 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 11.374 | nll_loss 5.806 | mask_ins 1.489 | word_ins_ml 7.218 | word_reposition 1.261 | kpe 1.406 | ppl 2654.3 | wps 12047.6 | wpb 2367.6 | bsz 32 | num_updates 8966 | best_loss 11.374
2022-07-07 22:57:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_best.pt (epoch 8 @ 8966 updates, score 11.374) (writing took 8.374239862896502 seconds)
2022-07-07 22:59:29 | INFO | train_inner | epoch 009:     34 / 1122 loss=7.037, nll_loss=2.753, mask_ins=0.909, word_ins_ml=4.375, word_reposition=0.812, kpe=0.941, ppl=131.33, wps=5281.3, ups=0.26, wpb=20309, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=1.647, clip=0, loss_scale=1024, train_wall=252, wall=27585
2022-07-07 23:04:24 | INFO | train_inner | epoch 009:    134 / 1122 loss=6.947, nll_loss=2.694, mask_ins=0.884, word_ins_ml=4.322, word_reposition=0.801, kpe=0.94, ppl=123.35, wps=6994.7, ups=0.34, wpb=20654.5, bsz=256, num_updates=9100, lr=0.000370625, gnorm=1.565, clip=0, loss_scale=1024, train_wall=253, wall=27881
2022-07-07 23:08:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-07 23:09:22 | INFO | train_inner | epoch 009:    235 / 1122 loss=6.935, nll_loss=2.677, mask_ins=0.889, word_ins_ml=4.306, word_reposition=0.802, kpe=0.937, ppl=122.35, wps=6864.5, ups=0.34, wpb=20468.1, bsz=256, num_updates=9200, lr=0.000368605, gnorm=1.578, clip=0, loss_scale=923, train_wall=256, wall=28179
2022-07-07 23:14:17 | INFO | train_inner | epoch 009:    335 / 1122 loss=6.916, nll_loss=2.67, mask_ins=0.89, word_ins_ml=4.3, word_reposition=0.791, kpe=0.935, ppl=120.78, wps=6943.5, ups=0.34, wpb=20452.7, bsz=256, num_updates=9300, lr=0.000366618, gnorm=1.58, clip=0, loss_scale=512, train_wall=253, wall=28474
2022-07-07 23:19:11 | INFO | train_inner | epoch 009:    435 / 1122 loss=nan, nll_loss=2.689, mask_ins=0.885, word_ins_ml=4.317, word_reposition=0.792, kpe=nan, ppl=nan, wps=6948.3, ups=0.34, wpb=20462.9, bsz=256, num_updates=9400, lr=0.000364662, gnorm=1.544, clip=0, loss_scale=512, train_wall=253, wall=28768
2022-07-07 23:24:06 | INFO | train_inner | epoch 009:    535 / 1122 loss=6.918, nll_loss=2.666, mask_ins=0.886, word_ins_ml=4.296, word_reposition=0.801, kpe=0.935, ppl=120.93, wps=6976.6, ups=0.34, wpb=20555.1, bsz=256, num_updates=9500, lr=0.000362738, gnorm=1.543, clip=0, loss_scale=512, train_wall=253, wall=29063
2022-07-07 23:29:12 | INFO | train_inner | epoch 009:    635 / 1122 loss=6.881, nll_loss=2.64, mask_ins=0.883, word_ins_ml=4.273, word_reposition=0.793, kpe=0.932, ppl=117.85, wps=6703.6, ups=0.33, wpb=20546.7, bsz=256, num_updates=9600, lr=0.000360844, gnorm=1.51, clip=0, loss_scale=512, train_wall=265, wall=29369
2022-07-07 23:34:21 | INFO | train_inner | epoch 009:    735 / 1122 loss=6.887, nll_loss=2.659, mask_ins=0.88, word_ins_ml=4.289, word_reposition=0.783, kpe=0.935, ppl=118.35, wps=6668.1, ups=0.32, wpb=20558.9, bsz=256, num_updates=9700, lr=0.000358979, gnorm=1.546, clip=0, loss_scale=553, train_wall=266, wall=29677
2022-07-07 23:39:16 | INFO | train_inner | epoch 009:    835 / 1122 loss=6.934, nll_loss=2.685, mask_ins=0.884, word_ins_ml=4.312, word_reposition=0.802, kpe=0.936, ppl=122.26, wps=6951.4, ups=0.34, wpb=20498, bsz=256, num_updates=9800, lr=0.000357143, gnorm=1.61, clip=0, loss_scale=1024, train_wall=253, wall=29972
2022-07-07 23:44:11 | INFO | train_inner | epoch 009:    935 / 1122 loss=6.836, nll_loss=2.612, mask_ins=0.866, word_ins_ml=4.248, word_reposition=0.786, kpe=0.937, ppl=114.26, wps=6964.7, ups=0.34, wpb=20583.1, bsz=256, num_updates=9900, lr=0.000355335, gnorm=1.721, clip=0, loss_scale=1024, train_wall=253, wall=30268
2022-07-07 23:49:08 | INFO | train_inner | epoch 009:   1035 / 1122 loss=nan, nll_loss=2.661, mask_ins=0.886, word_ins_ml=4.291, word_reposition=0.789, kpe=nan, ppl=nan, wps=6908.8, ups=0.34, wpb=20519.6, bsz=256, num_updates=10000, lr=0.000353553, gnorm=1.531, clip=0, loss_scale=1024, train_wall=255, wall=30565
2022-07-07 23:53:23 | INFO | train | epoch 009 | loss nan | nll_loss 2.668 | mask_ins 0.883 | word_ins_ml 4.298 | word_reposition 0.795 | kpe nan | ppl nan | wps 6716.3 | ups 0.33 | wpb 20520 | bsz 255.8 | num_updates 10087 | lr 0.000352025 | gnorm 1.579 | clip 0 | loss_scale 790 | train_wall 2864 | wall 30820
2022-07-07 23:54:46 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 11.21 | nll_loss 5.775 | mask_ins 1.455 | word_ins_ml 7.191 | word_reposition 1.2 | kpe 1.365 | ppl 2369.51 | wps 11985.4 | wpb 2367.6 | bsz 32 | num_updates 10087 | best_loss 11.21
2022-07-07 23:54:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_best.pt (epoch 9 @ 10087 updates, score 11.21) (writing took 7.964737609028816 seconds)
2022-07-07 23:55:32 | INFO | train_inner | epoch 010:     13 / 1122 loss=6.89, nll_loss=2.665, mask_ins=0.876, word_ins_ml=4.293, word_reposition=0.792, kpe=0.929, ppl=118.63, wps=5327.1, ups=0.26, wpb=20466, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=1.627, clip=0, loss_scale=1024, train_wall=251, wall=30949
2022-07-08 00:00:27 | INFO | train_inner | epoch 010:    113 / 1122 loss=6.861, nll_loss=2.641, mask_ins=0.87, word_ins_ml=4.273, word_reposition=0.792, kpe=0.926, ppl=116.26, wps=6981.6, ups=0.34, wpb=20579.3, bsz=256, num_updates=10200, lr=0.00035007, gnorm=1.533, clip=0, loss_scale=1024, train_wall=253, wall=31244
2022-07-08 00:05:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-08 00:05:25 | INFO | train_inner | epoch 010:    214 / 1122 loss=6.845, nll_loss=2.625, mask_ins=0.871, word_ins_ml=4.258, word_reposition=0.79, kpe=0.926, ppl=114.99, wps=6896.2, ups=0.34, wpb=20531, bsz=256, num_updates=10300, lr=0.000348367, gnorm=1.611, clip=0, loss_scale=1947, train_wall=256, wall=31542
2022-07-08 00:10:20 | INFO | train_inner | epoch 010:    314 / 1122 loss=6.821, nll_loss=2.599, mask_ins=0.87, word_ins_ml=4.234, word_reposition=0.79, kpe=0.926, ppl=113.03, wps=6990.5, ups=0.34, wpb=20607.2, bsz=256, num_updates=10400, lr=0.000346688, gnorm=1.506, clip=0, loss_scale=1024, train_wall=253, wall=31836
2022-07-08 00:15:14 | INFO | train_inner | epoch 010:    414 / 1122 loss=6.852, nll_loss=2.637, mask_ins=0.873, word_ins_ml=4.268, word_reposition=0.783, kpe=0.929, ppl=115.51, wps=6988.2, ups=0.34, wpb=20562.6, bsz=256, num_updates=10500, lr=0.000345033, gnorm=1.514, clip=0, loss_scale=1024, train_wall=252, wall=32131
2022-07-08 00:20:08 | INFO | train_inner | epoch 010:    514 / 1122 loss=6.783, nll_loss=2.573, mask_ins=0.869, word_ins_ml=4.211, word_reposition=0.781, kpe=0.922, ppl=110.12, wps=6981.8, ups=0.34, wpb=20513.3, bsz=256, num_updates=10600, lr=0.000343401, gnorm=1.532, clip=0, loss_scale=1024, train_wall=252, wall=32424
2022-07-08 00:25:01 | INFO | train_inner | epoch 010:    614 / 1122 loss=nan, nll_loss=2.664, mask_ins=0.876, word_ins_ml=4.291, word_reposition=0.796, kpe=nan, ppl=nan, wps=7047.3, ups=0.34, wpb=20699.5, bsz=256, num_updates=10700, lr=0.000341793, gnorm=1.487, clip=0, loss_scale=1024, train_wall=252, wall=32718
2022-07-08 00:29:57 | INFO | train_inner | epoch 010:    714 / 1122 loss=6.782, nll_loss=2.589, mask_ins=0.86, word_ins_ml=4.224, word_reposition=0.773, kpe=0.925, ppl=110.05, wps=6896.1, ups=0.34, wpb=20397, bsz=256, num_updates=10800, lr=0.000340207, gnorm=1.442, clip=0, loss_scale=1024, train_wall=254, wall=33014
2022-07-08 00:35:05 | INFO | train_inner | epoch 010:    814 / 1122 loss=nan, nll_loss=2.611, mask_ins=0.861, word_ins_ml=4.244, word_reposition=0.791, kpe=nan, ppl=nan, wps=6667, ups=0.32, wpb=20530, bsz=256, num_updates=10900, lr=0.000338643, gnorm=1.46, clip=0, loss_scale=1987, train_wall=266, wall=33322
2022-07-08 00:38:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-08 00:40:20 | INFO | train_inner | epoch 010:    915 / 1122 loss=6.754, nll_loss=2.556, mask_ins=0.857, word_ins_ml=4.195, word_reposition=0.776, kpe=0.926, ppl=107.94, wps=6530.8, ups=0.32, wpb=20534.5, bsz=256, num_updates=11000, lr=0.0003371, gnorm=1.524, clip=0, loss_scale=1663, train_wall=273, wall=33636
2022-07-08 00:45:14 | INFO | train_inner | epoch 010:   1015 / 1122 loss=6.804, nll_loss=2.592, mask_ins=0.864, word_ins_ml=4.227, word_reposition=0.783, kpe=0.93, ppl=111.76, wps=6939.7, ups=0.34, wpb=20464, bsz=256, num_updates=11100, lr=0.000335578, gnorm=1.691, clip=0, loss_scale=1024, train_wall=253, wall=33931
2022-07-08 00:50:12 | INFO | train_inner | epoch 010:   1115 / 1122 loss=6.772, nll_loss=2.578, mask_ins=0.856, word_ins_ml=4.214, word_reposition=0.779, kpe=0.924, ppl=109.28, wps=6890.5, ups=0.34, wpb=20477.9, bsz=256, num_updates=11200, lr=0.000334077, gnorm=1.54, clip=0, loss_scale=1024, train_wall=254, wall=34228
2022-07-08 00:50:32 | INFO | train | epoch 010 | loss nan | nll_loss 2.606 | mask_ins 0.866 | word_ins_ml 4.24 | word_reposition 0.785 | kpe nan | ppl nan | wps 6702.9 | ups 0.33 | wpb 20521 | bsz 255.8 | num_updates 11207 | lr 0.000333972 | gnorm 1.534 | clip 0 | loss_scale 1250 | train_wall 2869 | wall 34249
2022-07-08 00:51:55 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 11.242 | nll_loss 5.757 | mask_ins 1.441 | word_ins_ml 7.175 | word_reposition 1.23 | kpe 1.396 | ppl 2422.28 | wps 11936.4 | wpb 2367.6 | bsz 32 | num_updates 11207 | best_loss 11.21
2022-07-08 00:51:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_last.pt (epoch 10 @ 11207 updates, score 11.242) (writing took 4.368292845785618 seconds)
2022-07-08 00:56:33 | INFO | train_inner | epoch 011:     93 / 1122 loss=6.786, nll_loss=2.589, mask_ins=0.861, word_ins_ml=4.224, word_reposition=0.789, kpe=0.912, ppl=110.36, wps=5344.3, ups=0.26, wpb=20402, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=1.546, clip=0, loss_scale=1024, train_wall=253, wall=34610
2022-07-08 01:01:28 | INFO | train_inner | epoch 011:    193 / 1122 loss=nan, nll_loss=2.515, mask_ins=0.85, word_ins_ml=4.158, word_reposition=0.775, kpe=nan, ppl=nan, wps=6953.9, ups=0.34, wpb=20469.8, bsz=256, num_updates=11400, lr=0.000331133, gnorm=1.593, clip=0, loss_scale=1024, train_wall=253, wall=34905
2022-07-08 01:06:22 | INFO | train_inner | epoch 011:    293 / 1122 loss=nan, nll_loss=2.554, mask_ins=0.853, word_ins_ml=4.193, word_reposition=0.777, kpe=nan, ppl=nan, wps=6985, ups=0.34, wpb=20531, bsz=256, num_updates=11500, lr=0.00032969, gnorm=1.73, clip=0, loss_scale=1290, train_wall=253, wall=35199
2022-07-08 01:11:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-08 01:11:19 | INFO | train_inner | epoch 011:    394 / 1122 loss=6.757, nll_loss=2.56, mask_ins=0.856, word_ins_ml=4.198, word_reposition=0.785, kpe=0.918, ppl=108.14, wps=6908, ups=0.34, wpb=20560.3, bsz=256, num_updates=11600, lr=0.000328266, gnorm=1.61, clip=0, loss_scale=1997, train_wall=255, wall=35496
2022-07-08 01:16:15 | INFO | train_inner | epoch 011:    494 / 1122 loss=6.746, nll_loss=2.564, mask_ins=0.847, word_ins_ml=4.201, word_reposition=0.776, kpe=0.921, ppl=107.36, wps=6976.7, ups=0.34, wpb=20651.1, bsz=256, num_updates=11700, lr=0.00032686, gnorm=1.61, clip=0, loss_scale=1024, train_wall=254, wall=35792
2022-07-08 01:20:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-08 01:21:12 | INFO | train_inner | epoch 011:    595 / 1122 loss=6.743, nll_loss=2.571, mask_ins=0.848, word_ins_ml=4.207, word_reposition=0.773, kpe=0.915, ppl=107.11, wps=6917.7, ups=0.34, wpb=20543, bsz=256, num_updates=11800, lr=0.000325472, gnorm=1.592, clip=0, loss_scale=983, train_wall=255, wall=36089
2022-07-08 01:26:07 | INFO | train_inner | epoch 011:    695 / 1122 loss=6.699, nll_loss=2.526, mask_ins=0.84, word_ins_ml=4.167, word_reposition=0.772, kpe=0.92, ppl=103.92, wps=6965, ups=0.34, wpb=20542.2, bsz=256, num_updates=11900, lr=0.000324102, gnorm=1.561, clip=0, loss_scale=512, train_wall=253, wall=36384
2022-07-08 01:31:02 | INFO | train_inner | epoch 011:    795 / 1122 loss=6.75, nll_loss=2.561, mask_ins=0.855, word_ins_ml=4.198, word_reposition=0.775, kpe=0.922, ppl=107.66, wps=6889.1, ups=0.34, wpb=20333, bsz=256, num_updates=12000, lr=0.000322749, gnorm=1.565, clip=0, loss_scale=512, train_wall=253, wall=36679
2022-07-08 01:35:56 | INFO | train_inner | epoch 011:    895 / 1122 loss=6.752, nll_loss=2.574, mask_ins=0.837, word_ins_ml=4.209, word_reposition=0.784, kpe=0.922, ppl=107.78, wps=7023.5, ups=0.34, wpb=20614.3, bsz=256, num_updates=12100, lr=0.000321412, gnorm=1.722, clip=0, loss_scale=512, train_wall=252, wall=36973
2022-07-08 01:41:03 | INFO | train_inner | epoch 011:    995 / 1122 loss=6.746, nll_loss=2.564, mask_ins=0.851, word_ins_ml=4.2, word_reposition=0.776, kpe=0.919, ppl=107.33, wps=6663.3, ups=0.33, wpb=20487.8, bsz=256, num_updates=12200, lr=0.000320092, gnorm=1.499, clip=0, loss_scale=512, train_wall=266, wall=37280
2022-07-08 01:46:14 | INFO | train_inner | epoch 011:   1095 / 1122 loss=6.735, nll_loss=2.571, mask_ins=0.841, word_ins_ml=4.206, word_reposition=0.772, kpe=0.916, ppl=106.55, wps=6630.8, ups=0.32, wpb=20565.7, bsz=256, num_updates=12300, lr=0.000318788, gnorm=1.507, clip=0, loss_scale=512, train_wall=268, wall=37590
2022-07-08 01:47:32 | INFO | train | epoch 011 | loss nan | nll_loss 2.557 | mask_ins 0.849 | word_ins_ml 4.195 | word_reposition 0.778 | kpe nan | ppl nan | wps 6721.2 | ups 0.33 | wpb 20522.1 | bsz 255.8 | num_updates 12327 | lr 0.000318439 | gnorm 1.591 | clip 0 | loss_scale 902 | train_wall 2864 | wall 37669
2022-07-08 01:48:54 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 11.245 | nll_loss 5.726 | mask_ins 1.429 | word_ins_ml 7.151 | word_reposition 1.288 | kpe 1.377 | ppl 2427.21 | wps 11993.5 | wpb 2367.6 | bsz 32 | num_updates 12327 | best_loss 11.21
2022-07-08 01:48:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_last.pt (epoch 11 @ 12327 updates, score 11.245) (writing took 4.58565648086369 seconds)
2022-07-08 01:52:35 | INFO | train_inner | epoch 012:     73 / 1122 loss=6.728, nll_loss=2.555, mask_ins=0.849, word_ins_ml=4.193, word_reposition=0.777, kpe=0.909, ppl=106, wps=5363.2, ups=0.26, wpb=20456.1, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=1.502, clip=0, loss_scale=1004, train_wall=253, wall=37972
2022-07-08 01:55:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-08 01:57:35 | INFO | train_inner | epoch 012:    174 / 1122 loss=6.642, nll_loss=2.492, mask_ins=0.83, word_ins_ml=4.136, word_reposition=0.769, kpe=0.907, ppl=99.84, wps=6831.5, ups=0.33, wpb=20478.8, bsz=256, num_updates=12500, lr=0.000316228, gnorm=1.47, clip=0, loss_scale=801, train_wall=257, wall=38272
2022-07-08 02:02:29 | INFO | train_inner | epoch 012:    274 / 1122 loss=6.711, nll_loss=2.552, mask_ins=0.842, word_ins_ml=4.189, word_reposition=0.775, kpe=0.905, ppl=104.74, wps=6963.8, ups=0.34, wpb=20509.2, bsz=256, num_updates=12600, lr=0.00031497, gnorm=1.445, clip=0, loss_scale=512, train_wall=253, wall=38566
2022-07-08 02:07:23 | INFO | train_inner | epoch 012:    374 / 1122 loss=6.685, nll_loss=2.518, mask_ins=0.838, word_ins_ml=4.158, word_reposition=0.78, kpe=0.91, ppl=102.92, wps=7014.2, ups=0.34, wpb=20620, bsz=256, num_updates=12700, lr=0.000313728, gnorm=1.484, clip=0, loss_scale=512, train_wall=253, wall=38860
2022-07-08 02:12:18 | INFO | train_inner | epoch 012:    474 / 1122 loss=6.681, nll_loss=2.517, mask_ins=0.836, word_ins_ml=4.157, word_reposition=0.774, kpe=0.913, ppl=102.63, wps=6954.2, ups=0.34, wpb=20488.3, bsz=256, num_updates=12800, lr=0.0003125, gnorm=1.536, clip=0, loss_scale=512, train_wall=253, wall=39155
2022-07-08 02:17:12 | INFO | train_inner | epoch 012:    574 / 1122 loss=6.619, nll_loss=2.469, mask_ins=0.832, word_ins_ml=4.115, word_reposition=0.764, kpe=0.908, ppl=98.27, wps=7003.8, ups=0.34, wpb=20598.6, bsz=256, num_updates=12900, lr=0.000311286, gnorm=1.504, clip=0, loss_scale=512, train_wall=253, wall=39449
2022-07-08 02:22:06 | INFO | train_inner | epoch 012:    674 / 1122 loss=6.597, nll_loss=2.46, mask_ins=0.829, word_ins_ml=4.107, word_reposition=0.753, kpe=0.908, ppl=96.81, wps=6928.1, ups=0.34, wpb=20387.5, bsz=256, num_updates=13000, lr=0.000310087, gnorm=1.482, clip=0, loss_scale=676, train_wall=252, wall=39743
2022-07-08 02:27:00 | INFO | train_inner | epoch 012:    774 / 1122 loss=6.654, nll_loss=2.5, mask_ins=0.833, word_ins_ml=4.142, word_reposition=0.769, kpe=0.909, ppl=100.69, wps=7005.3, ups=0.34, wpb=20601.2, bsz=256, num_updates=13100, lr=0.000308901, gnorm=1.671, clip=0, loss_scale=1024, train_wall=252, wall=40037
2022-07-08 02:31:55 | INFO | train_inner | epoch 012:    874 / 1122 loss=nan, nll_loss=2.49, mask_ins=0.844, word_ins_ml=4.133, word_reposition=0.769, kpe=nan, ppl=nan, wps=6979, ups=0.34, wpb=20564.8, bsz=256, num_updates=13200, lr=0.000307729, gnorm=1.57, clip=0, loss_scale=1024, train_wall=252, wall=40332
2022-07-08 02:36:50 | INFO | train_inner | epoch 012:    974 / 1122 loss=6.648, nll_loss=2.488, mask_ins=0.831, word_ins_ml=4.131, word_reposition=0.773, kpe=0.913, ppl=100.26, wps=6947.2, ups=0.34, wpb=20506.7, bsz=256, num_updates=13300, lr=0.00030657, gnorm=1.849, clip=0, loss_scale=1024, train_wall=253, wall=40627
2022-07-08 02:41:44 | INFO | train_inner | epoch 012:   1074 / 1122 loss=nan, nll_loss=2.5, mask_ins=0.835, word_ins_ml=4.142, word_reposition=0.769, kpe=nan, ppl=nan, wps=6978.4, ups=0.34, wpb=20498.5, bsz=256, num_updates=13400, lr=0.000305424, gnorm=1.539, clip=0, loss_scale=1024, train_wall=252, wall=40921
2022-07-08 02:44:04 | INFO | train | epoch 012 | loss nan | nll_loss 2.504 | mask_ins 0.836 | word_ins_ml 4.146 | word_reposition 0.77 | kpe nan | ppl nan | wps 6781.1 | ups 0.33 | wpb 20519.9 | bsz 255.8 | num_updates 13448 | lr 0.000304878 | gnorm 1.55 | clip 0 | loss_scale 790 | train_wall 2836 | wall 41061
2022-07-08 02:45:27 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.352 | nll_loss 5.768 | mask_ins 1.468 | word_ins_ml 7.191 | word_reposition 1.277 | kpe 1.415 | ppl 2613.71 | wps 11986.2 | wpb 2367.6 | bsz 32 | num_updates 13448 | best_loss 11.21
2022-07-08 02:45:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_last.pt (epoch 12 @ 13448 updates, score 11.352) (writing took 4.556168244220316 seconds)
2022-07-08 02:48:16 | INFO | train_inner | epoch 013:     52 / 1122 loss=6.639, nll_loss=2.481, mask_ins=0.84, word_ins_ml=4.125, word_reposition=0.771, kpe=0.904, ppl=99.7, wps=5195.4, ups=0.25, wpb=20381.7, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=1.596, clip=0, loss_scale=1229, train_wall=264, wall=41313
2022-07-08 02:50:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-08 02:53:41 | INFO | train_inner | epoch 013:    153 / 1122 loss=6.659, nll_loss=2.511, mask_ins=0.834, word_ins_ml=4.151, word_reposition=0.767, kpe=0.907, ppl=101.04, wps=6334.3, ups=0.31, wpb=20577.8, bsz=256, num_updates=13600, lr=0.00030317, gnorm=1.763, clip=0, loss_scale=1359, train_wall=283, wall=41638
2022-07-08 02:58:38 | INFO | train_inner | epoch 013:    253 / 1122 loss=6.657, nll_loss=2.503, mask_ins=0.839, word_ins_ml=4.144, word_reposition=0.766, kpe=0.908, ppl=100.88, wps=6898.6, ups=0.34, wpb=20450.1, bsz=256, num_updates=13700, lr=0.000302061, gnorm=1.857, clip=0, loss_scale=1024, train_wall=254, wall=41934
2022-07-08 03:03:27 | INFO | train_inner | epoch 013:    353 / 1122 loss=nan, nll_loss=2.504, mask_ins=0.827, word_ins_ml=4.145, word_reposition=0.761, kpe=nan, ppl=nan, wps=7104.6, ups=0.35, wpb=20550.4, bsz=256, num_updates=13800, lr=0.000300965, gnorm=1.974, clip=0, loss_scale=1024, train_wall=252, wall=42224
2022-07-08 03:08:14 | INFO | train_inner | epoch 013:    453 / 1122 loss=6.618, nll_loss=2.473, mask_ins=0.828, word_ins_ml=4.118, word_reposition=0.764, kpe=0.908, ppl=98.2, wps=7160.5, ups=0.35, wpb=20578.3, bsz=256, num_updates=13900, lr=0.00029988, gnorm=1.779, clip=0, loss_scale=1024, train_wall=250, wall=42511
2022-07-08 03:13:02 | INFO | train_inner | epoch 013:    553 / 1122 loss=6.657, nll_loss=2.512, mask_ins=0.828, word_ins_ml=4.152, word_reposition=0.77, kpe=0.907, ppl=100.89, wps=7148.4, ups=0.35, wpb=20537.5, bsz=256, num_updates=14000, lr=0.000298807, gnorm=1.928, clip=0, loss_scale=1024, train_wall=250, wall=42798
2022-07-08 03:17:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-08 03:17:54 | INFO | train_inner | epoch 013:    654 / 1122 loss=6.633, nll_loss=2.481, mask_ins=0.83, word_ins_ml=4.125, word_reposition=0.768, kpe=0.91, ppl=99.26, wps=7007.3, ups=0.34, wpb=20476.7, bsz=256, num_updates=14100, lr=0.000297746, gnorm=1.785, clip=0, loss_scale=1551, train_wall=256, wall=43091
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     2022-07-08 15:57:07 | INFO | train_inner | epoch 027:    167 / 1122 loss=6.127, nll_loss=2.153, mask_ins=0.762, word_ins_ml=3.824, word_reposition=0.721, kpe=0.82, ppl=69.9, wps=7239.7, ups=0.35, wpb=20671.4, bsz=256, num_updates=29300, lr=0.000206548, gnorm=1.49, clip=0, loss_scale=1024, train_wall=248, wall=88644
2022-07-08 16:01:56 | INFO | train_inner | epoch 027:    267 / 1122 loss=6.161, nll_loss=2.191, mask_ins=0.758, word_ins_ml=3.858, word_reposition=0.723, kpe=0.821, ppl=71.54, wps=7124.1, ups=0.35, wpb=20620.9, bsz=256, num_updates=29400, lr=0.000206197, gnorm=1.581, clip=0, loss_scale=1024, train_wall=251, wall=88933
2022-07-08 16:06:55 | INFO | train_inner | epoch 027:    367 / 1122 loss=nan, nll_loss=2.181, mask_ins=0.761, word_ins_ml=3.849, word_reposition=0.719, kpe=nan, ppl=nan, wps=6822.6, ups=0.34, wpb=20350.4, bsz=256, num_updates=29500, lr=0.000205847, gnorm=1.539, clip=0, loss_scale=1024, train_wall=254, wall=89231
2022-07-08 16:11:55 | INFO | train_inner | epoch 027:    467 / 1122 loss=6.15, nll_loss=2.18, mask_ins=0.757, word_ins_ml=3.848, word_reposition=0.72, kpe=0.824, ppl=71.01, wps=6814.6, ups=0.33, wpb=20438.5, bsz=256, num_updates=29600, lr=0.000205499, gnorm=1.543, clip=0, loss_scale=1024, train_wall=254, wall=89531
2022-07-08 16:16:55 | INFO | train_inner | epoch 027:    567 / 1122 loss=6.126, nll_loss=2.168, mask_ins=0.75, word_ins_ml=3.838, word_reposition=0.715, kpe=0.823, ppl=69.84, wps=6841.8, ups=0.33, wpb=20529.3, bsz=256, num_updates=29700, lr=0.000205152, gnorm=1.562, clip=0, loss_scale=1516, train_wall=254, wall=89831
2022-07-08 16:18:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-08 16:21:57 | INFO | train_inner | epoch 027:    668 / 1122 loss=6.18, nll_loss=2.211, mask_ins=0.756, word_ins_ml=3.876, word_reposition=0.723, kpe=0.825, ppl=72.49, wps=6816.5, ups=0.33, wpb=20576.4, bsz=256, num_updates=29800, lr=0.000204808, gnorm=1.513, clip=0, loss_scale=1277, train_wall=257, wall=90133
2022-07-08 16:26:55 | INFO | train_inner | epoch 027:    768 / 1122 loss=6.149, nll_loss=2.18, mask_ins=0.765, word_ins_ml=3.849, word_reposition=0.714, kpe=0.821, ppl=70.99, wps=6885.3, ups=0.33, wpb=20554.9, bsz=256, num_updates=29900, lr=0.000204465, gnorm=1.534, clip=0, loss_scale=1024, train_wall=254, wall=90432
2022-07-08 16:29:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-08 16:31:53 | INFO | train_inner | epoch 027:    869 / 1122 loss=nan, nll_loss=2.155, mask_ins=0.755, word_ins_ml=3.826, word_reposition=0.711, kpe=nan, ppl=nan, wps=6896.4, ups=0.34, wpb=20529.2, bsz=256, num_updates=30000, lr=0.000204124, gnorm=1.526, clip=0, loss_scale=796, train_wall=256, wall=90730
2022-07-08 16:36:48 | INFO | train_inner | epoch 027:    969 / 1122 loss=6.149, nll_loss=2.186, mask_ins=0.752, word_ins_ml=3.853, word_reposition=0.718, kpe=0.825, ppl=70.97, wps=6966, ups=0.34, wpb=20589.5, bsz=256, num_updates=30100, lr=0.000203785, gnorm=1.547, clip=0, loss_scale=512, train_wall=253, wall=91025
2022-07-08 16:41:46 | INFO | train_inner | epoch 027:   1069 / 1122 loss=6.159, nll_loss=2.186, mask_ins=0.762, word_ins_ml=3.853, word_reposition=0.717, kpe=0.826, ppl=71.48, wps=6904.1, ups=0.34, wpb=20522, bsz=256, num_updates=30200, lr=0.000203447, gnorm=1.562, clip=0, loss_scale=512, train_wall=254, wall=91322
2022-07-08 16:44:36 | INFO | train | epoch 027 | loss nan | nll_loss 2.181 | mask_ins 0.758 | word_ins_ml 3.849 | word_reposition 0.718 | kpe nan | ppl nan | wps 6744.7 | ups 0.33 | wpb 20519.6 | bsz 255.8 | num_updates 30253 | lr 0.000203269 | gnorm 1.542 | clip 0 | loss_scale 951 | train_wall 2845 | wall 91493
2022-07-08 16:45:59 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 10.679 | nll_loss 5.28 | mask_ins 1.384 | word_ins_ml 6.73 | word_reposition 1.179 | kpe 1.385 | ppl 1639.28 | wps 11894.8 | wpb 2367.6 | bsz 32 | num_updates 30253 | best_loss 10.679
2022-07-08 16:46:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_best.pt (epoch 27 @ 30253 updates, score 10.679) (writing took 6.564304698258638 seconds)
2022-07-08 16:48:24 | INFO | train_inner | epoch 028:     47 / 1122 loss=6.105, nll_loss=2.156, mask_ins=0.748, word_ins_ml=3.827, word_reposition=0.712, kpe=0.818, ppl=68.84, wps=5093.5, ups=0.25, wpb=20316, bsz=253.8, num_updates=30300, lr=0.000203111, gnorm=1.518, clip=0, loss_scale=512, train_wall=264, wall=91721
2022-07-08 16:54:05 | INFO | train_inner | epoch 028:    147 / 1122 loss=6.153, nll_loss=2.212, mask_ins=0.748, word_ins_ml=3.877, word_reposition=0.714, kpe=0.813, ppl=71.16, wps=6035.5, ups=0.29, wpb=20530.9, bsz=256, num_updates=30400, lr=0.000202777, gnorm=1.548, clip=0, loss_scale=512, train_wall=293, wall=92061
2022-07-08 16:59:04 | INFO | train_inner | epoch 028:    247 / 1122 loss=6.122, nll_loss=2.163, mask_ins=0.762, word_ins_ml=3.832, word_reposition=0.716, kpe=0.811, ppl=69.63, wps=6843.2, ups=0.33, wpb=20521.8, bsz=256, num_updates=30500, lr=0.000202444, gnorm=1.598, clip=0, loss_scale=681, train_wall=256, wall=92361
2022-07-08 17:04:02 | INFO | train_inner | epoch 028:    347 / 1122 loss=6.133, nll_loss=2.18, mask_ins=0.757, word_ins_ml=3.848, word_reposition=0.711, kpe=0.817, ppl=70.2, wps=6885.5, ups=0.34, wpb=20504.6, bsz=256, num_updates=30600, lr=0.000202113, gnorm=1.524, clip=0, loss_scale=1024, train_wall=254, wall=92659
2022-07-08 17:09:02 | INFO | train_inner | epoch 028:    447 / 1122 loss=6.115, nll_loss=2.144, mask_ins=0.76, word_ins_ml=3.816, word_reposition=0.726, kpe=0.812, ppl=69.31, wps=6900.5, ups=0.33, wpb=20679.8, bsz=256, num_updates=30700, lr=0.000201784, gnorm=1.507, clip=0, loss_scale=1024, train_wall=253, wall=92959
2022-07-08 17:14:00 | INFO | train_inner | epoch 028:    547 / 1122 loss=nan, nll_loss=2.128, mask_ins=0.754, word_ins_ml=3.802, word_reposition=0.707, kpe=nan, ppl=nan, wps=6888.5, ups=0.34, wpb=20522.7, bsz=256, num_updates=30800, lr=0.000201456, gnorm=1.491, clip=0, loss_scale=1024, train_wall=254, wall=93257
2022-07-08 17:18:58 | INFO | train_inner | epoch 028:    647 / 1122 loss=nan, nll_loss=2.179, mask_ins=0.758, word_ins_ml=3.847, word_reposition=0.712, kpe=nan, ppl=nan, wps=6857.2, ups=0.34, wpb=20460.7, bsz=256, num_updates=30900, lr=0.000201129, gnorm=1.512, clip=0, loss_scale=1024, train_wall=254, wall=93555
2022-07-08 17:23:56 | INFO | train_inner | epoch 028:    747 / 1122 loss=6.109, nll_loss=2.151, mask_ins=0.758, word_ins_ml=3.822, word_reposition=0.712, kpe=0.817, ppl=69.01, wps=6876.9, ups=0.34, wpb=20452, bsz=256, num_updates=31000, lr=0.000200805, gnorm=1.486, clip=0, loss_scale=1239, train_wall=255, wall=93853
2022-07-08 17:28:54 | INFO | train_inner | epoch 028:    847 / 1122 loss=6.066, nll_loss=2.126, mask_ins=0.741, word_ins_ml=3.8, word_reposition=0.708, kpe=0.817, ppl=66.98, wps=6855.9, ups=0.33, wpb=20475.6, bsz=256, num_updates=31100, lr=0.000200482, gnorm=1.509, clip=0, loss_scale=2048, train_wall=254, wall=94151
2022-07-08 17:29:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-08 17:32:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-08 17:34:04 | INFO | train_inner | epoch 028:    949 / 1122 loss=6.139, nll_loss=2.176, mask_ins=0.759, word_ins_ml=3.844, word_reposition=0.719, kpe=0.817, ppl=70.49, wps=6627.2, ups=0.32, wpb=20508.3, bsz=256, num_updates=31200, lr=0.00020016, gnorm=1.536, clip=0, loss_scale=1034, train_wall=258, wall=94461
2022-07-08 17:39:05 | INFO | train_inner | epoch 028:   1049 / 1122 loss=6.151, nll_loss=2.187, mask_ins=0.756, word_ins_ml=3.854, word_reposition=0.72, kpe=0.82, ppl=71.06, wps=6834.1, ups=0.33, wpb=20571.5, bsz=256, num_updates=31300, lr=0.00019984, gnorm=1.535, clip=0, loss_scale=512, train_wall=253, wall=94762
2022-07-08 17:42:44 | INFO | train | epoch 028 | loss nan | nll_loss 2.162 | mask_ins 0.755 | word_ins_ml 3.832 | word_reposition 0.715 | kpe nan | ppl nan | wps 6589.4 | ups 0.32 | wpb 20520.6 | bsz 255.8 | num_updates 31373 | lr 0.000199608 | gnorm 1.525 | clip 0 | loss_scale 959 | train_wall 2888 | wall 94981
2022-07-08 17:44:07 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 10.762 | nll_loss 5.351 | mask_ins 1.4 | word_ins_ml 6.799 | word_reposition 1.155 | kpe 1.408 | ppl 1736.62 | wps 11868.6 | wpb 2367.6 | bsz 32 | num_updates 31373 | best_loss 10.679
2022-07-08 17:44:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_last.pt (epoch 28 @ 31373 updates, score 10.762) (writing took 3.8476375620812178 seconds)
2022-07-08 17:45:33 | INFO | train_inner | epoch 029:     27 / 1122 loss=6.092, nll_loss=2.123, mask_ins=0.76, word_ins_ml=3.797, word_reposition=0.719, kpe=0.816, ppl=68.23, wps=5274, ups=0.26, wpb=20477.9, bsz=253.8, num_updates=31400, lr=0.000199522, gnorm=1.537, clip=0, loss_scale=512, train_wall=253, wall=95150
2022-07-08 17:50:47 | INFO | train_inner | epoch 029:    127 / 1122 loss=6.118, nll_loss=2.178, mask_ins=0.753, word_ins_ml=3.846, word_reposition=0.714, kpe=0.805, ppl=69.46, wps=6554.5, ups=0.32, wpb=20577.6, bsz=256, num_updates=31500, lr=0.000199205, gnorm=1.504, clip=0, loss_scale=512, train_wall=266, wall=95464
2022-07-08 17:56:12 | INFO | train_inner | epoch 029:    227 / 1122 loss=6.067, nll_loss=2.13, mask_ins=0.747, word_ins_ml=3.804, word_reposition=0.711, kpe=0.805, ppl=67.04, wps=6310.5, ups=0.31, wpb=20524.7, bsz=256, num_updates=31600, lr=0.000198889, gnorm=1.528, clip=0, loss_scale=512, train_wall=279, wall=95789
2022-07-08 18:01:25 | INFO | train_inner | epoch 029:    327 / 1122 loss=6.124, nll_loss=2.172, mask_ins=0.758, word_ins_ml=3.841, word_reposition=0.719, kpe=0.807, ppl=69.75, wps=6573.2, ups=0.32, wpb=20533.5, bsz=256, num_updates=31700, lr=0.000198575, gnorm=1.517, clip=0, loss_scale=614, train_wall=270, wall=96102
2022-07-08 18:06:25 | INFO | train_inner | epoch 029:    427 / 1122 loss=nan, nll_loss=2.162, mask_ins=0.757, word_ins_ml=3.832, word_reposition=0.717, kpe=nan, ppl=nan, wps=6821.4, ups=0.33, wpb=20502, bsz=256, num_updates=31800, lr=0.000198263, gnorm=1.571, clip=0, loss_scale=1024, train_wall=253, wall=96402
2022-07-08 18:11:23 | INFO | train_inner | epoch 029:    527 / 1122 loss=6.078, nll_loss=2.148, mask_ins=0.747, word_ins_ml=3.819, word_reposition=0.706, kpe=0.806, ppl=67.54, wps=6882.4, ups=0.34, wpb=20501.8, bsz=256, num_updates=31900, lr=0.000197952, gnorm=1.482, clip=0, loss_scale=1024, train_wall=254, wall=96700
2022-07-08 18:16:21 | INFO | train_inner | epoch 029:    627 / 1122 loss=nan, nll_loss=2.155, mask_ins=0.749, word_ins_ml=3.825, word_reposition=0.71, kpe=nan, ppl=nan, wps=6907.8, ups=0.34, wpb=20583.7, bsz=256, num_updates=32000, lr=0.000197642, gnorm=1.516, clip=0, loss_scale=1024, train_wall=254, wall=96998
2022-07-08 18:21:21 | INFO | train_inner | epoch 029:    727 / 1122 loss=6.096, nll_loss=2.151, mask_ins=0.751, word_ins_ml=3.822, word_reposition=0.709, kpe=0.814, ppl=68.4, wps=6832.1, ups=0.33, wpb=20511.2, bsz=256, num_updates=32100, lr=0.000197334, gnorm=1.724, clip=0, loss_scale=1024, train_wall=254, wall=97298
2022-07-08 18:26:21 | INFO | train_inner | epoch 029:    827 / 1122 loss=6.117, nll_loss=2.169, mask_ins=0.751, word_ins_ml=3.838, word_reposition=0.717, kpe=0.811, ppl=69.41, wps=6845.2, ups=0.33, wpb=20515.7, bsz=256, num_updates=32200, lr=0.000197028, gnorm=1.629, clip=0, loss_scale=1106, train_wall=253, wall=97598
2022-07-08 18:26:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-08 18:30:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-08 18:31:25 | INFO | train_inner | epoch 029:    929 / 1122 loss=6.104, nll_loss=2.153, mask_ins=0.751, word_ins_ml=3.823, word_reposition=0.717, kpe=0.813, ppl=68.79, wps=6798.5, ups=0.33, wpb=20626.3, bsz=256, num_updates=32300, lr=0.000196722, gnorm=1.747, clip=0, loss_scale=989, train_wall=260, wall=97901
2022-07-08 18:36:23 | INFO | train_inner | epoch 029:   1029 / 1122 loss=6.139, nll_loss=2.171, mask_ins=0.761, word_ins_ml=3.84, word_reposition=0.723, kpe=0.815, ppl=70.46, wps=6863.9, ups=0.34, wpb=20467.3, bsz=256, num_updates=32400, lr=0.000196419, gnorm=1.788, clip=0, loss_scale=512, train_wall=253, wall=98200
2022-07-08 18:40:57 | INFO | train | epoch 029 | loss nan | nll_loss 2.157 | mask_ins 0.753 | word_ins_ml 3.827 | word_reposition 0.714 | kpe nan | ppl nan | wps 6579.3 | ups 0.32 | wpb 20519.5 | bsz 255.8 | num_updates 32493 | lr 0.000196137 | gnorm 1.6 | clip 0 | loss_scale 800 | train_wall 2899 | wall 98474
2022-07-08 18:42:20 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 10.836 | nll_loss 5.343 | mask_ins 1.408 | word_ins_ml 6.792 | word_reposition 1.203 | kpe 1.433 | ppl 1827.56 | wps 11900.8 | wpb 2367.6 | bsz 32 | num_updates 32493 | best_loss 10.679
2022-07-08 18:42:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_last.pt (epoch 29 @ 32493 updates, score 10.836) (writing took 3.543042140081525 seconds)
2022-07-08 18:42:44 | INFO | train_inner | epoch 030:      7 / 1122 loss=6.093, nll_loss=2.156, mask_ins=0.75, word_ins_ml=3.826, word_reposition=0.71, kpe=0.807, ppl=68.26, wps=5337.2, ups=0.26, wpb=20348.7, bsz=253.8, num_updates=32500, lr=0.000196116, gnorm=1.745, clip=0, loss_scale=512, train_wall=253, wall=98581
2022-07-08 18:47:40 | INFO | train_inner | epoch 030:    107 / 1122 loss=6.107, nll_loss=2.165, mask_ins=0.751, word_ins_ml=3.835, word_reposition=0.719, kpe=0.802, ppl=68.91, wps=6920.7, ups=0.34, wpb=20514.5, bsz=256, num_updates=32600, lr=0.000195815, gnorm=1.566, clip=0, loss_scale=512, train_wall=253, wall=98877
2022-07-08 18:52:38 | INFO | train_inner | epoch 030:    207 / 1122 loss=6.094, nll_loss=2.159, mask_ins=0.752, word_ins_ml=3.829, word_reposition=0.714, kpe=0.8, ppl=68.33, wps=6886.8, ups=0.34, wpb=20511.3, bsz=256, num_updates=32700, lr=0.000195515, gnorm=1.57, clip=0, loss_scale=512, train_wall=254, wall=99175
2022-07-08 18:57:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-08 18:57:55 | INFO | train_inner | epoch 030:    308 / 1122 loss=6.041, nll_loss=2.117, mask_ins=0.746, word_ins_ml=3.792, word_reposition=0.705, kpe=0.799, ppl=65.85, wps=6482.9, ups=0.32, wpb=20527.2, bsz=256, num_updates=32800, lr=0.000195217, gnorm=1.643, clip=0, loss_scale=497, train_wall=271, wall=99492
2022-07-08 19:03:35 | INFO | train_inner | epoch 030:    408 / 1122 loss=6.086, nll_loss=2.149, mask_ins=0.749, word_ins_ml=3.82, word_reposition=0.716, kpe=0.8, ppl=67.91, wps=5998.2, ups=0.29, wpb=20412.4, bsz=256, num_updates=32900, lr=0.00019492, gnorm=1.776, clip=0, loss_scale=256, train_wall=293, wall=99832
2022-07-08 19:08:39 | INFO | train_inner | epoch 030:    508 / 1122 loss=nan, nll_loss=2.132, mask_ins=0.743, word_ins_ml=3.805, word_reposition=0.71, kpe=nan, ppl=nan, wps=6820.7, ups=0.33, wpb=20695.8, bsz=256, num_updates=33000, lr=0.000194625, gnorm=1.58, clip=0, loss_scale=256, train_wall=255, wall=100135
2022-07-08 19:13:38 | INFO | train_inner | epoch 030:    608 / 1122 loss=6.09, nll_loss=2.147, mask_ins=0.75, word_ins_ml=3.818, word_reposition=0.713, kpe=0.81, ppl=68.14, wps=6845.3, ups=0.33, wpb=20481.9, bsz=256, num_updates=33100, lr=0.000194331, gnorm=1.825, clip=0, loss_scale=256, train_wall=253, wall=100435
2022-07-08 19:18:39 | INFO | train_inner | epoch 030:    708 / 1122 loss=6.1, nll_loss=2.156, mask_ins=0.755, word_ins_ml=3.826, word_reposition=0.714, kpe=0.805, ppl=68.57, wps=6772.1, ups=0.33, wpb=20393.5, bsz=256, num_updates=33200, lr=0.000194038, gnorm=1.588, clip=0, loss_scale=256, train_wall=253, wall=100736
2022-07-08 19:23:37 | INFO | train_inner | epoch 030:    808 / 1122 loss=6.077, nll_loss=2.145, mask_ins=0.747, word_ins_ml=3.817, word_reposition=0.71, kpe=0.804, ppl=67.51, wps=6914, ups=0.34, wpb=20580.9, bsz=256, num_updates=33300, lr=0.000193746, gnorm=1.656, clip=0, loss_scale=256, train_wall=253, wall=101033
2022-07-08 19:28:34 | INFO | train_inner | epoch 030:    908 / 1122 loss=6.047, nll_loss=2.104, mask_ins=0.753, word_ins_ml=3.78, word_reposition=0.707, kpe=0.807, ppl=66.13, wps=6917.4, ups=0.34, wpb=20586.2, bsz=256, num_updates=33400, lr=0.000193456, gnorm=1.657, clip=0, loss_scale=497, train_wall=253, wall=101331
2022-07-08 19:33:34 | INFO | train_inner | epoch 030:   1008 / 1122 loss=6.057, nll_loss=2.125, mask_ins=0.748, word_ins_ml=3.798, word_reposition=0.708, kpe=0.802, ppl=66.56, wps=6893.8, ups=0.33, wpb=20639.5, bsz=256, num_updates=33500, lr=0.000193167, gnorm=1.59, clip=0, loss_scale=512, train_wall=254, wall=101630
2022-07-08 19:38:32 | INFO | train_inner | epoch 030:   1108 / 1122 loss=nan, nll_loss=2.138, mask_ins=0.751, word_ins_ml=3.81, word_reposition=0.715, kpe=nan, ppl=nan, wps=6884.3, ups=0.33, wpb=20570, bsz=256, num_updates=33600, lr=0.000192879, gnorm=1.564, clip=0, loss_scale=512, train_wall=252, wall=101929
2022-07-08 19:39:13 | INFO | train | epoch 030 | loss nan | nll_loss 2.139 | mask_ins 0.75 | word_ins_ml 3.811 | word_reposition 0.711 | kpe nan | ppl nan | wps 6579.8 | ups 0.32 | wpb 20521.6 | bsz 255.8 | num_updates 33614 | lr 0.000192839 | gnorm 1.651 | clip 0 | loss_scale 395 | train_wall 2897 | wall 101970
2022-07-08 19:40:36 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 10.869 | nll_loss 5.358 | mask_ins 1.423 | word_ins_ml 6.806 | word_reposition 1.193 | kpe 1.446 | ppl 1869.76 | wps 11931 | wpb 2367.6 | bsz 32 | num_updates 33614 | best_loss 10.679
2022-07-08 19:40:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_last.pt (epoch 30 @ 33614 updates, score 10.869) (writing took 3.80834929458797 seconds)
2022-07-08 19:44:57 | INFO | train_inner | epoch 031:     86 / 1122 loss=nan, nll_loss=2.089, mask_ins=0.736, word_ins_ml=3.766, word_reposition=0.705, kpe=nan, ppl=nan, wps=5338.4, ups=0.26, wpb=20501.2, bsz=253.8, num_updates=33700, lr=0.000192593, gnorm=1.68, clip=0, loss_scale=512, train_wall=253, wall=102313
2022-07-08 19:49:56 | INFO | train_inner | epoch 031:    186 / 1122 loss=6.049, nll_loss=2.136, mask_ins=0.742, word_ins_ml=3.808, word_reposition=0.707, kpe=0.791, ppl=66.2, wps=6894.6, ups=0.33, wpb=20627.7, bsz=256, num_updates=33800, lr=0.000192308, gnorm=1.544, clip=0, loss_scale=512, train_wall=254, wall=102613
2022-07-08 19:54:54 | INFO | train_inner | epoch 031:    286 / 1122 loss=6.042, nll_loss=2.116, mask_ins=0.752, word_ins_ml=3.79, word_reposition=0.705, kpe=0.795, ppl=65.9, wps=6828.3, ups=0.33, wpb=20397.5, bsz=256, num_updates=33900, lr=0.000192024, gnorm=1.638, clip=0, loss_scale=932, train_wall=253, wall=102911
2022-07-08 19:59:55 | INFO | train_inner | epoch 031:    386 / 1122 loss=6.068, nll_loss=2.149, mask_ins=0.748, word_ins_ml=3.82, word_reposition=0.705, kpe=0.796, ppl=67.1, wps=6852.5, ups=0.33, wpb=20603.3, bsz=256, num_updates=34000, lr=0.000191741, gnorm=1.62, clip=0, loss_scale=1024, train_wall=256, wall=103212
2022-07-08 20:05:08 | INFO | train_inner | epoch 031:    486 / 1122 loss=6.069, nll_loss=2.142, mask_ins=0.749, word_ins_ml=3.813, word_reposition=0.713, kpe=0.794, ppl=67.14, wps=6532.3, ups=0.32, wpb=20438.4, bsz=256, num_updates=34100, lr=0.00019146, gnorm=1.526, clip=0, loss_scale=1024, train_wall=266, wall=103525
2022-07-08 20:10:42 | INFO | train_inner | epoch 031:    586 / 1122 loss=6.018, nll_loss=2.095, mask_ins=0.739, word_ins_ml=3.772, word_reposition=0.711, kpe=0.796, ppl=64.82, wps=6175.4, ups=0.3, wpb=20614.5, bsz=256, num_updates=34200, lr=0.00019118, gnorm=1.541, clip=0, loss_scale=1024, train_wall=290, wall=103859
2022-07-08 20:15:47 | INFO | train_inner | epoch 031:    686 / 1122 loss=6.059, nll_loss=2.137, mask_ins=0.741, word_ins_ml=3.809, word_reposition=0.711, kpe=0.798, ppl=66.67, wps=6744.2, ups=0.33, wpb=20553.7, bsz=256, num_updates=34300, lr=0.000190901, gnorm=1.647, clip=0, loss_scale=1024, train_wall=254, wall=104163
2022-07-08 20:19:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-08 20:20:49 | INFO | train_inner | epoch 031:    787 / 1122 loss=6.027, nll_loss=2.107, mask_ins=0.738, word_ins_ml=3.782, word_reposition=0.706, kpe=0.801, ppl=65.23, wps=6802, ups=0.33, wpb=20549.1, bsz=256, num_updates=34400, lr=0.000190623, gnorm=1.649, clip=0, loss_scale=1369, train_wall=257, wall=104466
2022-07-08 20:25:48 | INFO | train_inner | epoch 031:    887 / 1122 loss=6.069, nll_loss=2.141, mask_ins=0.743, word_ins_ml=3.812, word_reposition=0.713, kpe=0.801, ppl=67.13, wps=6839.7, ups=0.33, wpb=20485.6, bsz=256, num_updates=34500, lr=0.000190347, gnorm=1.653, clip=0, loss_scale=1024, train_wall=253, wall=104765
2022-07-08 20:30:44 | INFO | train_inner | epoch 031:    987 / 1122 loss=6.051, nll_loss=2.125, mask_ins=0.746, word_ins_ml=3.798, word_reposition=0.709, kpe=0.799, ppl=66.32, wps=6899.7, ups=0.34, wpb=20410.6, bsz=256, num_updates=34600, lr=0.000190071, gnorm=1.768, clip=0, loss_scale=1024, train_wall=254, wall=105061
2022-07-08 20:35:42 | INFO | train_inner | epoch 031:   1087 / 1122 loss=6.086, nll_loss=2.165, mask_ins=0.738, word_ins_ml=3.834, word_reposition=0.713, kpe=0.801, ppl=67.93, wps=6902.4, ups=0.34, wpb=20553.1, bsz=256, num_updates=34700, lr=0.000189797, gnorm=1.686, clip=0, loss_scale=1024, train_wall=253, wall=105359
2022-07-08 20:36:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-08 20:37:24 | INFO | train | epoch 031 | loss nan | nll_loss 2.13 | mask_ins 0.743 | word_ins_ml 3.802 | word_reposition 0.71 | kpe nan | ppl nan | wps 6583.7 | ups 0.32 | wpb 20520.8 | bsz 255.8 | num_updates 34734 | lr 0.000189704 | gnorm 1.634 | clip 0 | loss_scale 949 | train_wall 2898 | wall 105461
2022-07-08 20:38:47 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 10.942 | nll_loss 5.396 | mask_ins 1.392 | word_ins_ml 6.843 | word_reposition 1.241 | kpe 1.465 | ppl 1967.41 | wps 11953.2 | wpb 2367.6 | bsz 32 | num_updates 34734 | best_loss 10.679
2022-07-08 20:38:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_cased/checkpoint_last.pt (epoch 31 @ 34734 updates, score 10.942) (writing took 3.8882313277572393 seconds)
2022-07-08 20:39:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-08 20:42:10 | INFO | train_inner | epoch 032:     67 / 1122 loss=nan, nll_loss=2.147, mask_ins=0.752, word_ins_ml=3.818, word_reposition=0.716, kpe=nan, ppl=nan, wps=5231.7, ups=0.26, wpb=20327.1, bsz=253.8, num_updates=34800, lr=0.000189525, gnorm=1.707, clip=0, loss_scale=397, train_wall=259, wall=105747
2022-07-08 20:47:11 | INFO | train_inner | epoch 032:    167 / 1122 loss=6.059, nll_loss=2.153, mask_ins=0.747, word_ins_ml=3.823, word_reposition=0.704, kpe=0.786, ppl=66.68, wps=6802.7, ups=0.33, wpb=20454, bsz=256, num_updates=34900, lr=0.000189253, gnorm=1.666, clip=0, loss_scale=256, train_wall=253, wall=106048
2022-07-08 20:52:08 | INFO | train_inner | epoch 032:    267 / 1122 loss=6.052, nll_loss=2.133, mask_ins=0.746, word_ins_ml=3.805, word_reposition=0.712, kpe=0.788, ppl=66.33, wps=6915.4, ups=0.34, wpb=20518.8, bsz=256, num_updates=35000, lr=0.000188982, gnorm=1.664, clip=0, loss_scale=256, train_wall=254, wall=106345
2022-07-08 20:57:10 | INFO | train_inner | epoch 032:    367 / 1122 loss=6.048, nll_loss=2.125, mask_ins=0.753, word_ins_ml=3.798, word_reposition=0.708, kpe=0.789, ppl=66.14, wps=6851.1, ups=0.33, wpb=20693.8, bsz=256, num_updates=35100, lr=0.000188713, gnorm=1.585, clip=0, loss_scale=256, train_wall=255, wall=106647
2022-07-08 21:02:07 | INFO | train_inner | epoch 032:    467 / 1122 loss=6.029, nll_loss=2.112, mask_ins=0.745, word_ins_ml=3.786, word_reposition=0.711, kpe=0.787, ppl=65.32, wps=6963.3, ups=0.34, wpb=20725.3, bsz=256, num_updates=35200, lr=0.000188445, gnorm=1.67, clip=0, loss_scale=256, train_wall=253, wall=106944
2022-07-08 21:07:08 | INFO | train_inner | epoch 032:    567 / 1122 loss=nan, nll_loss=2.093, mask_ins=0.736, word_ins_ml=3.77, word_reposition=0.705, kpe=nan, ppl=nan, wps=6836.4, ups=0.33, wpb=20548.6, bsz=256, num_updates=35300, lr=0.000188177, gnorm=1.576, clip=0, loss_scale=374, train_wall=253, wall=107245
2022-07-08 21:12:17 | INFO | train_inner | epoch 032:    667 / 1122 loss=6.04, nll_loss=2.118, mask_ins=0.745, word_ins_ml=3.792, word_reposition=0.711, kpe=0.792, ppl=65.81, wps=6607.6, ups=0.32, wpb=20438.5, bsz=256, num_updates=35400, lr=0.000187912, gnorm=1.768, clip=0, loss_scale=512, train_wall=266, wall=107554
2022-07-08 21:17:53 | INFO | train_inner | epoch 032:    767 / 1122 loss=6.038, nll_loss=2.125, mask_ins=0.746, word_ins_ml=3.799, word_reposition=0.702, kpe=0.791, ppl=65.71, wps=6071.7, ups=0.3, wpb=20364.2, bsz=256, num_updates=35500, lr=0.000187647, gnorm=1.634, clip=0, loss_scale=512, train_wall=290, wall=107890
2022-07-08 21:22:53 | INFO | train_inner | epoch 032:    867 / 1122 loss=6.042, nll_loss=2.121, mask_ins=0.744, word_ins_ml=3.795, word_reposition=0.709, kpe=0.794, ppl=65.87, wps=6802.1, ups=0.33, wpb=20416, bsz=256, num_updates=35600, lr=0.000187383, gnorm=1.697, clip=0, loss_scale=512, train_wall=254, wall=108190
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
train.sh: line 44: 12,12,12: command not found
