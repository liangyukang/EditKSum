nohup: ignoring input
2022-08-10 15:31:01 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:16127
2022-08-10 15:31:01 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:16127
2022-08-10 15:31:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-08-10 15:31:01 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:16127
2022-08-10 15:31:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-08-10 15:31:02 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:16127
2022-08-10 15:31:02 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-08-10 15:31:02 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-08-10 15:31:02 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-10 15:31:02 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-08-10 15:31:02 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-10 15:31:02 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-08-10 15:31:02 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-10 15:31:02 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-08-10 15:31:02 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-10 15:31:02 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-08-10 15:31:06 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:16127', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_finetune_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, constraint=True, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-08-10 15:31:06 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-08-10 15:31:06 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-08-10 15:31:06 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.source
2022-08-10 15:31:06 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.target
2022-08-10 15:31:06 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]359it [00:00, 3588.57it/s]385it [00:00, 3846.96it/s]381it [00:00, 3806.83it/s]382it [00:00, 3816.35it/s]770it [00:00, 3281.75it/s]762it [00:00, 3225.07it/s]718it [00:00, 2762.90it/s]764it [00:00, 2694.58it/s]1104it [00:00, 3239.86it/s]1006it [00:00, 2550.15it/s]1081it [00:00, 2870.70it/s]1091it [00:00, 2769.40it/s]1431it [00:00, 3049.46it/s]1267it [00:00, 2526.87it/s]1376it [00:00, 2678.41it/s]1397it [00:00, 2825.53it/s]1801it [00:00, 3256.05it/s]1542it [00:00, 2598.10it/s]1676it [00:00, 2770.91it/s]1766it [00:00, 3100.27it/s]2188it [00:00, 3450.65it/s]1906it [00:00, 2925.32it/s]2059it [00:00, 3097.76it/s]2135it [00:00, 3284.63it/s]2537it [00:00, 3423.62it/s]2236it [00:00, 3034.00it/s]2388it [00:00, 3154.94it/s]2471it [00:00, 3253.25it/s]2933it [00:00, 3587.41it/s]2623it [00:00, 3262.35it/s]2785it [00:00, 3357.13it/s]2866it [00:00, 3462.76it/s]3294it [00:00, 3511.39it/s]3017it [00:00, 3466.41it/s]3124it [00:00, 3343.77it/s]3217it [00:01, 3371.66it/s]3683it [00:01, 3622.34it/s]3367it [00:01, 3414.28it/s]3512it [00:01, 3502.68it/s]3606it [00:01, 3521.79it/s]4047it [00:01, 3529.82it/s]3734it [00:01, 3488.04it/s]3898it [00:01, 3608.38it/s]3962it [00:01, 3461.39it/s]4428it [00:01, 3611.47it/s]4085it [00:01, 3432.33it/s]4261it [00:01, 3469.46it/s]4329it [00:01, 3520.70it/s]4791it [00:01, 3499.81it/s]4442it [00:01, 3472.63it/s]4635it [00:01, 3545.40it/s]4686it [00:01, 3533.70it/s]5168it [00:01, 3577.81it/s]4791it [00:01, 3387.07it/s]4992it [00:01, 3443.44it/s]5041it [00:01, 3393.30it/s]5542it [00:01, 3622.52it/s]5151it [00:01, 3447.50it/s]5366it [00:01, 3527.67it/s]5398it [00:01, 3443.86it/s]5526it [00:01, 3529.08it/s]5906it [00:01, 3406.70it/s]5721it [00:01, 3357.21it/s]5744it [00:01, 3230.13it/s]6261it [00:01, 3447.20it/s]5880it [00:01, 3372.06it/s]6076it [00:01, 3411.87it/s]6086it [00:01, 3282.49it/s]6222it [00:01, 3383.87it/s]6431it [00:01, 3450.77it/s]6424it [00:01, 3310.02it/s]6609it [00:02, 2132.49it/s]6965it [00:02, 2421.18it/s]6562it [00:02, 2017.63it/s]6778it [00:02, 2013.01it/s]6758it [00:02, 1878.49it/s]7319it [00:02, 2672.19it/s]6900it [00:02, 2287.14it/s]7131it [00:02, 2308.94it/s]7109it [00:02, 2187.71it/s]7636it [00:02, 2752.15it/s]7251it [00:02, 2556.61it/s]7428it [00:02, 2414.33it/s]7400it [00:02, 2340.72it/s]7973it [00:02, 2909.38it/s]7559it [00:02, 2615.75it/s]7783it [00:02, 2680.25it/s]7754it [00:02, 2620.01it/s]8292it [00:02, 2933.66it/s]7909it [00:02, 2835.98it/s]8128it [00:02, 2871.68it/s]8096it [00:02, 2819.92it/s]8635it [00:02, 3066.62it/s]8224it [00:02, 2752.41it/s]8449it [00:02, 2822.89it/s]8415it [00:02, 2785.54it/s]8979it [00:02, 3171.09it/s]8565it [00:02, 2923.32it/s]8769it [00:02, 2920.89it/s]8754it [00:02, 2943.53it/s]9308it [00:02, 2988.65it/s]8909it [00:02, 3062.67it/s]9079it [00:03, 2920.15it/s]9069it [00:03, 2928.45it/s]9644it [00:03, 3090.10it/s]9229it [00:03, 2906.48it/s]9386it [00:03, 2961.00it/s]9376it [00:03, 2960.63it/s]9961it [00:03, 2940.78it/s]9538it [00:03, 2955.64it/s]9702it [00:03, 3015.00it/s]9690it [00:03, 3010.88it/s]10272it [00:03, 2985.31it/s]9842it [00:03, 2887.86it/s]10010it [00:03, 2812.25it/s]9999it [00:03, 2893.63it/s]10576it [00:03, 3000.25it/s]10137it [00:03, 2848.10it/s]10357it [00:03, 2991.93it/s]10346it [00:03, 3055.13it/s]10880it [00:03, 2991.29it/s]10485it [00:03, 3024.12it/s]10700it [00:03, 3191.78it/s]10706it [00:03, 3001.22it/s]11235it [00:03, 3152.09it/s]10792it [00:03, 2997.54it/s]11058it [00:03, 3115.11it/s]11024it [00:03, 3129.13it/s]11553it [00:03, 3072.50it/s]11145it [00:03, 3150.43it/s]11415it [00:03, 3243.76it/s]11379it [00:03, 3248.41it/s]11910it [00:03, 3215.98it/s]11501it [00:03, 3267.79it/s]11743it [00:03, 3162.25it/s]11707it [00:03, 3172.53it/s]12270it [00:03, 3327.60it/s]11830it [00:03, 3178.15it/s]12091it [00:03, 3251.87it/s]12063it [00:03, 3282.14it/s]12605it [00:04, 3200.65it/s]12188it [00:04, 3292.59it/s]12419it [00:04, 3166.51it/s]12394it [00:04, 3198.82it/s]12959it [00:04, 3297.01it/s]12519it [00:04, 3205.71it/s]12775it [00:04, 3278.47it/s]12747it [00:04, 3292.20it/s]13291it [00:04, 3209.89it/s]13368it [00:04, 3149.60it/s]
2022-08-10 15:31:10 | INFO | root | success load 13368 data
2022-08-10 15:31:10 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-08-10 15:31:10 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-08-10 15:31:10 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-08-10 15:31:10 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-08-10 15:31:10 | INFO | transformer.tokenization_utils | loading file None
2022-08-10 15:31:10 | INFO | transformer.tokenization_utils | loading file None
2022-08-10 15:31:10 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
12874it [00:04, 3303.55it/s]13130it [00:04, 3356.79it/s]13101it [00:04, 3362.96it/s]13368it [00:04, 3060.15it/s]
13227it [00:04, 3199.22it/s]13368it [00:04, 3053.88it/s]
13368it [00:04, 3026.26it/s]
2022-08-10 15:31:11 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-10 15:31:11 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-10 15:31:11 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-08-10 15:31:15 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-08-10 15:31:15 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-08-10 15:31:15 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-10 15:31:15 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-10 15:31:15 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
2022-08-10 15:31:17 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-08-10 15:31:17 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-08-10 15:31:17 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-08-10 15:31:17 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-08-10 15:31:17 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 142456835)
2022-08-10 15:31:17 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-08-10 15:31:17 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-08-10 15:31:17 | INFO | fairseq_cli.train | training on 4 GPUs
2022-08-10 15:31:17 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-08-10 15:31:19 | INFO | fairseq.trainer | loaded checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 31 @ 34734 updates)
2022-08-10 15:31:19 | INFO | fairseq.trainer | loading train data for epoch 31
2022-08-10 15:31:19 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.source
2022-08-10 15:31:19 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.target
2022-08-10 15:31:19 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]350it [00:00, 3490.72it/s]700it [00:00, 3476.09it/s]1048it [00:00, 3240.21it/s]1395it [00:00, 3325.67it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]1755it [00:00, 3418.81it/s]354it [00:00, 3531.97it/s]355it [00:00, 3545.11it/s]2099it [00:00, 3304.99it/s]725it [00:00, 3636.26it/s]723it [00:00, 3623.64it/s]2460it [00:00, 3398.93it/s]1089it [00:00, 3366.47it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]1086it [00:00, 3362.41it/s]2802it [00:00, 3311.39it/s]1461it [00:00, 3497.04it/s]362it [00:00, 3617.24it/s]1454it [00:00, 3452.59it/s]3168it [00:00, 3414.39it/s]725it [00:00, 3623.41it/s]1830it [00:00, 3413.97it/s]1830it [00:00, 3379.47it/s]3511it [00:01, 3311.12it/s]2212it [00:00, 3544.68it/s]1088it [00:00, 3320.04it/s]2200it [00:00, 3481.39it/s]3871it [00:01, 3394.57it/s]2585it [00:00, 3603.20it/s]1460it [00:00, 3466.41it/s]2575it [00:00, 3565.30it/s]4247it [00:01, 3501.62it/s]2947it [00:00, 3442.03it/s]1828it [00:00, 3540.60it/s]2933it [00:00, 3420.30it/s]4599it [00:01, 3336.92it/s]3309it [00:00, 3492.73it/s]2184it [00:00, 3386.36it/s]3298it [00:00, 3487.37it/s]4960it [00:01, 3413.60it/s]2528it [00:00, 3402.20it/s]3660it [00:01, 3288.68it/s]3649it [00:01, 3272.19it/s]5304it [00:01, 3158.41it/s]4016it [00:01, 3365.29it/s]2870it [00:00, 3221.75it/s]4005it [00:01, 3351.96it/s]5652it [00:01, 3246.01it/s]4356it [00:01, 3282.26it/s]3235it [00:00, 3347.00it/s]4350it [00:01, 3223.61it/s]5981it [00:01, 2963.13it/s]4687it [00:01, 3248.45it/s]3573it [00:01, 3088.73it/s]4679it [00:01, 3240.51it/s]5014it [00:01, 3244.74it/s]6284it [00:01, 2852.27it/s]3893it [00:01, 3118.11it/s]5006it [00:01, 3192.52it/s]6594it [00:02, 2906.89it/s]5340it [00:01, 3069.92it/s]4210it [00:01, 3129.75it/s]5327it [00:01, 3048.41it/s]5687it [00:01, 3180.86it/s]6889it [00:02, 2825.54it/s]4526it [00:01, 3081.20it/s]5691it [00:01, 3213.39it/s]6030it [00:01, 3168.64it/s]4883it [00:01, 3222.03it/s]6030it [00:01, 3170.15it/s]6392it [00:01, 3296.54it/s]5208it [00:01, 3179.13it/s]6405it [00:01, 3334.70it/s]6766it [00:02, 3424.13it/s]5572it [00:01, 3310.90it/s]6771it [00:02, 3427.00it/s]5945it [00:01, 3433.78it/s]6290it [00:01, 3342.15it/s]6668it [00:02, 3467.53it/s]7175it [00:03, 876.95it/s] 7551it [00:03, 1191.00it/s]7111it [00:02, 1204.73it/s]7862it [00:03, 1449.18it/s]7485it [00:02, 1528.70it/s]7116it [00:02, 1076.10it/s]8218it [00:03, 1789.22it/s]7779it [00:02, 1733.04it/s]7489it [00:02, 1382.91it/s]8584it [00:03, 2138.47it/s]8150it [00:03, 2087.44it/s]7792it [00:03, 1616.01it/s]8903it [00:03, 2347.37it/s]8521it [00:03, 2418.49it/s]8165it [00:03, 1971.89it/s]9265it [00:03, 2639.96it/s]8851it [00:03, 2580.99it/s]8539it [00:03, 2314.37it/s]9595it [00:03, 2765.55it/s]9220it [00:03, 2847.96it/s]8874it [00:03, 2497.45it/s]9967it [00:03, 3011.04it/s]9558it [00:03, 2905.69it/s]7017it [00:03, 844.38it/s] 9243it [00:03, 2775.67it/s]10306it [00:03, 3050.31it/s]9931it [00:03, 3120.19it/s]7382it [00:03, 1103.53it/s]9583it [00:03, 2881.86it/s]10683it [00:04, 3246.44it/s]10299it [00:03, 3072.57it/s]7743it [00:03, 1395.56it/s]9949it [00:03, 3079.10it/s]11046it [00:04, 3353.62it/s]10658it [00:03, 3209.96it/s]8052it [00:03, 1599.99it/s]10299it [00:03, 3036.67it/s]11397it [00:04, 3224.06it/s]11018it [00:03, 3317.68it/s]8405it [00:03, 1920.18it/s]10659it [00:03, 3187.70it/s]11748it [00:04, 3302.77it/s]11362it [00:04, 3156.07it/s]8718it [00:03, 2090.03it/s]10998it [00:04, 3239.91it/s]12087it [00:04, 3258.36it/s]11711it [00:04, 3246.33it/s]12426it [00:04, 3294.51it/s]11336it [00:04, 3062.36it/s]9019it [00:03, 2126.56it/s]12044it [00:04, 3096.54it/s]12760it [00:04, 3272.39it/s]11654it [00:04, 3083.09it/s]9304it [00:03, 2283.93it/s]12383it [00:04, 3177.50it/s]11971it [00:04, 3103.54it/s]9584it [00:04, 2366.31it/s]13091it [00:04, 3157.54it/s]12717it [00:04, 3222.99it/s]9858it [00:04, 2449.54it/s]13442it [00:04, 3256.72it/s]12288it [00:04, 2956.10it/s]13044it [00:04, 3218.87it/s]10225it [00:04, 2768.55it/s]13771it [00:05, 3245.91it/s]12673it [00:04, 3204.46it/s]13430it [00:04, 3403.80it/s]10535it [00:04, 2857.80it/s]14149it [00:05, 3399.59it/s]12999it [00:04, 3186.65it/s]13773it [00:04, 3367.95it/s]10894it [00:04, 3062.31it/s]14499it [00:05, 3357.14it/s]13394it [00:04, 3405.16it/s]14155it [00:04, 3497.99it/s]11214it [00:04, 3076.33it/s]14882it [00:05, 3494.50it/s]13739it [00:04, 3350.44it/s]14507it [00:04, 3419.32it/s]11583it [00:04, 3250.76it/s]15255it [00:05, 3560.99it/s]14130it [00:04, 3510.99it/s]14886it [00:05, 3524.44it/s]11948it [00:04, 3366.24it/s]15613it [00:05, 3448.88it/s]14500it [00:05, 3413.60it/s]15270it [00:05, 3615.74it/s]12291it [00:04, 3296.43it/s]15972it [00:05, 3487.71it/s]14886it [00:05, 3540.62it/s]15633it [00:05, 3482.26it/s]12667it [00:04, 3428.33it/s]15270it [00:05, 3602.18it/s]16008it [00:05, 3557.79it/s]13014it [00:05, 3365.18it/s]15632it [00:05, 3476.53it/s]13396it [00:05, 3495.90it/s]15999it [00:05, 3529.29it/s]13748it [00:05, 3419.99it/s]14127it [00:05, 3525.05it/s]14499it [00:05, 3418.02it/s]14876it [00:05, 3516.50it/s]15256it [00:05, 3597.25it/s]16322it [00:06, 1053.51it/s]15618it [00:05, 3449.14it/s]16706it [00:06, 1365.37it/s]15982it [00:05, 3502.22it/s]17089it [00:06, 1672.71it/s]16366it [00:06, 970.51it/s] 17460it [00:06, 2001.88it/s]16755it [00:06, 1267.98it/s]17848it [00:06, 2354.05it/s]16354it [00:06, 951.31it/s] 17089it [00:06, 1507.66it/s]18193it [00:07, 2487.71it/s]16728it [00:06, 1231.07it/s]17458it [00:06, 1838.53it/s]18566it [00:07, 2766.22it/s]17088it [00:06, 1481.04it/s]17827it [00:06, 2167.97it/s]18908it [00:07, 2802.48it/s]17456it [00:06, 1806.19it/s]18162it [00:06, 2321.00it/s]19281it [00:07, 3033.76it/s]17798it [00:06, 2086.71it/s]18512it [00:07, 2576.96it/s]19622it [00:07, 2955.92it/s]18121it [00:07, 2254.66it/s]18840it [00:07, 2636.18it/s]19960it [00:07, 3058.47it/s]18448it [00:07, 2473.67it/s]19177it [00:07, 2816.36it/s]20286it [00:07, 3107.00it/s]18768it [00:07, 2555.55it/s]19531it [00:07, 3004.28it/s]20611it [00:07, 3091.49it/s]19135it [00:07, 2829.21it/s]19862it [00:07, 3049.69it/s]20977it [00:07, 3250.56it/s]19505it [00:07, 3054.89it/s]20219it [00:07, 3191.36it/s]21310it [00:08, 3236.06it/s]16334it [00:07, 740.40it/s] 19843it [00:07, 3069.82it/s]20555it [00:07, 3177.39it/s]21672it [00:08, 3345.60it/s]16722it [00:07, 992.47it/s]20228it [00:07, 3283.11it/s]20936it [00:07, 3354.90it/s]22054it [00:08, 3482.76it/s]17088it [00:07, 1252.32it/s]20574it [00:07, 3228.99it/s]21289it [00:07, 3304.23it/s]22406it [00:08, 3424.29it/s]17470it [00:07, 1577.86it/s]20949it [00:07, 3373.93it/s]21670it [00:07, 3446.50it/s]22781it [00:08, 3517.35it/s]17855it [00:07, 1926.24it/s]21297it [00:08, 3295.37it/s]22049it [00:08, 3544.93it/s]23135it [00:08, 3433.68it/s]18200it [00:07, 2163.83it/s]21680it [00:08, 3444.42it/s]22408it [00:08, 3457.81it/s]23497it [00:08, 3486.15it/s]18584it [00:07, 2500.58it/s]22044it [00:08, 3499.04it/s]22782it [00:08, 3538.69it/s]23847it [00:08, 3407.50it/s]22399it [00:08, 3432.32it/s]18935it [00:08, 2638.81it/s]23139it [00:08, 3431.99it/s]24219it [00:08, 3495.50it/s]22784it [00:08, 3552.67it/s]19317it [00:08, 2916.77it/s]23509it [00:08, 3507.34it/s]24595it [00:08, 3572.06it/s]19668it [00:08, 2992.28it/s]23142it [00:08, 3421.37it/s]23862it [00:08, 3409.82it/s]24954it [00:09, 3424.51it/s]20047it [00:08, 3198.04it/s]23513it [00:08, 3503.52it/s]24238it [00:08, 3510.26it/s]25335it [00:09, 3533.06it/s]20425it [00:08, 3353.50it/s]23866it [00:08, 3404.91it/s]24616it [00:08, 3588.06it/s]25691it [00:09, 3416.85it/s]24245it [00:08, 3513.79it/s]20785it [00:08, 3310.84it/s]24977it [00:08, 3461.43it/s]26064it [00:09, 3504.23it/s]24626it [00:08, 3598.81it/s]21165it [00:08, 3445.33it/s]25352it [00:08, 3541.84it/s]26417it [00:09, 3395.76it/s]24988it [00:09, 3478.57it/s]25708it [00:09, 3433.13it/s]21523it [00:08, 3202.32it/s]26795it [00:09, 3503.12it/s]25363it [00:09, 3556.66it/s]26080it [00:09, 3515.10it/s]21904it [00:08, 3366.90it/s]27166it [00:09, 3561.58it/s]25721it [00:09, 3466.63it/s]26433it [00:09, 3382.94it/s]22251it [00:08, 3315.17it/s]27524it [00:09, 3468.55it/s]26087it [00:09, 3519.91it/s]26795it [00:09, 3450.02it/s]22625it [00:09, 3432.47it/s]27896it [00:09, 3540.51it/s]27168it [00:09, 3529.21it/s]26441it [00:09, 3333.89it/s]22974it [00:09, 3279.18it/s]26791it [00:09, 3379.08it/s]23332it [00:09, 3360.88it/s]27523it [00:09, 3330.64it/s]27169it [00:09, 3340.67it/s]23693it [00:09, 3429.21it/s]27901it [00:09, 3455.05it/s]27505it [00:09, 3203.54it/s]24040it [00:09, 3095.36it/s]27828it [00:09, 3209.89it/s]24358it [00:09, 2735.94it/s]24649it [00:09, 2735.14it/s]25010it [00:09, 2962.87it/s]25365it [00:10, 3122.60it/s]25686it [00:10, 3117.84it/s]26045it [00:10, 3249.54it/s]26375it [00:10, 3201.58it/s]26738it [00:10, 3324.19it/s]27117it [00:10, 3457.37it/s]27466it [00:10, 3351.30it/s]28250it [00:11, 789.58it/s] 27851it [00:10, 3493.66it/s]28625it [00:11, 1042.60it/s]28252it [00:11, 630.74it/s] 28935it [00:11, 1266.57it/s]28625it [00:11, 844.97it/s]29316it [00:11, 1608.58it/s]28928it [00:11, 1041.20it/s]28151it [00:11, 671.53it/s] 29679it [00:11, 1933.99it/s]29307it [00:11, 1353.89it/s]28522it [00:11, 909.14it/s]30013it [00:11, 2176.93it/s]29676it [00:11, 1680.46it/s]28896it [00:11, 1193.50it/s]30389it [00:11, 2507.14it/s]30011it [00:12, 1938.52it/s]29200it [00:11, 1406.89it/s]30732it [00:11, 2680.72it/s]30388it [00:12, 2287.41it/s]29572it [00:11, 1754.42it/s]31106it [00:11, 2939.64it/s]30732it [00:12, 2486.22it/s]29891it [00:11, 2003.99it/s]31454it [00:11, 3008.95it/s]31105it [00:12, 2772.36it/s]30260it [00:11, 2342.86it/s]31836it [00:12, 3222.52it/s]31452it [00:12, 2868.40it/s]30598it [00:12, 2529.97it/s]32215it [00:12, 3378.45it/s]31831it [00:12, 3103.41it/s]30959it [00:12, 2786.98it/s]32196it [00:12, 3249.15it/s]32575it [00:12, 3228.50it/s]31316it [00:12, 2984.39it/s]32932it [00:12, 3320.93it/s]32550it [00:12, 3138.97it/s]31659it [00:12, 2957.52it/s]32906it [00:12, 3251.91it/s]33277it [00:12, 3164.26it/s]28203it [00:12, 714.42it/s] 32026it [00:12, 3145.97it/s]33638it [00:12, 3285.53it/s]28553it [00:12, 932.61it/s]33247it [00:13, 3135.97it/s]32365it [00:12, 3079.75it/s]28870it [00:12, 1157.63it/s]33573it [00:13, 3147.69it/s]33975it [00:12, 3153.03it/s]32690it [00:12, 3102.64it/s]34309it [00:12, 3199.90it/s]29163it [00:12, 1363.57it/s]33896it [00:13, 3080.12it/s]33013it [00:12, 3094.93it/s]34642it [00:12, 3234.08it/s]29448it [00:12, 1570.91it/s]34210it [00:13, 2952.43it/s]33331it [00:12, 2993.31it/s]34969it [00:13, 3180.04it/s]29755it [00:12, 1833.51it/s]34559it [00:13, 3100.10it/s]33699it [00:13, 3182.69it/s]35345it [00:13, 3345.90it/s]30056it [00:12, 2068.75it/s]34874it [00:13, 3104.15it/s]34024it [00:13, 3181.54it/s]30430it [00:12, 2439.74it/s]35683it [00:13, 3304.14it/s]35251it [00:13, 3294.17it/s]34400it [00:13, 3346.54it/s]30746it [00:12, 2600.02it/s]36060it [00:13, 3436.72it/s]35630it [00:13, 3436.98it/s]34773it [00:13, 3458.05it/s]31121it [00:13, 2892.28it/s]36433it [00:13, 3522.07it/s]35977it [00:13, 3372.64it/s]35122it [00:13, 3365.45it/s]31454it [00:13, 2957.80it/s]36787it [00:13, 3371.67it/s]36317it [00:13, 3355.01it/s]35498it [00:13, 3470.08it/s]31834it [00:13, 3185.31it/s]37166it [00:13, 3489.69it/s]36654it [00:14, 3299.87it/s]35847it [00:13, 3386.57it/s]32212it [00:13, 3349.60it/s]37517it [00:13, 3390.57it/s]37031it [00:14, 3435.21it/s]36218it [00:13, 3478.64it/s]32565it [00:13, 3264.67it/s]37885it [00:13, 3472.21it/s]37376it [00:14, 3353.47it/s]36568it [00:13, 3337.29it/s]32926it [00:13, 3359.90it/s]38234it [00:13, 3397.35it/s]37739it [00:14, 3433.52it/s]36941it [00:13, 3447.91it/s]33272it [00:13, 3269.48it/s]38591it [00:14, 3446.68it/s]38117it [00:14, 3533.67it/s]37311it [00:14, 3520.51it/s]33645it [00:13, 3398.02it/s]38975it [00:14, 3560.18it/s]38472it [00:14, 3329.70it/s]37665it [00:14, 3392.27it/s]33991it [00:13, 3314.75it/s]39333it [00:14, 3479.80it/s]38849it [00:14, 3454.64it/s]38026it [00:14, 3454.39it/s]34370it [00:14, 3447.87it/s]39709it [00:14, 3559.45it/s]39198it [00:14, 3367.58it/s]38374it [00:14, 3361.19it/s]34728it [00:14, 3479.73it/s]40066it [00:14, 3462.66it/s]39578it [00:14, 3489.16it/s]38735it [00:14, 3432.32it/s]40449it [00:14, 3568.45it/s]35079it [00:14, 3376.31it/s]39929it [00:15, 3408.85it/s]39080it [00:14, 3343.64it/s]35441it [00:14, 3445.92it/s]40807it [00:14, 3443.56it/s]40309it [00:15, 3520.82it/s]39442it [00:14, 3422.41it/s]35788it [00:14, 3354.59it/s]41187it [00:14, 3545.58it/s]40679it [00:15, 3572.54it/s]39814it [00:14, 3506.64it/s]36151it [00:14, 3433.05it/s]41544it [00:14, 3457.52it/s]41038it [00:15, 3423.11it/s]40166it [00:14, 3403.14it/s]41922it [00:14, 3549.10it/s]36496it [00:14, 3257.14it/s]41413it [00:15, 3516.61it/s]40532it [00:15, 3475.01it/s]42294it [00:15, 3598.74it/s]36836it [00:14, 3295.28it/s]41767it [00:15, 3341.03it/s]40881it [00:15, 3286.82it/s]37195it [00:14, 3379.67it/s]42655it [00:15, 3417.50it/s]42120it [00:15, 3392.14it/s]41223it [00:15, 3321.85it/s]43018it [00:15, 3476.13it/s]37535it [00:14, 3200.80it/s]42462it [00:15, 3289.58it/s]41558it [00:15, 3241.97it/s]37858it [00:15, 3170.35it/s]42804it [00:15, 3324.39it/s]41884it [00:15, 3240.05it/s]38177it [00:15, 3045.81it/s]43138it [00:16, 3046.30it/s]42214it [00:15, 3257.26it/s]38491it [00:15, 3070.10it/s]42541it [00:15, 3043.94it/s]38800it [00:15, 3072.23it/s]42875it [00:15, 3126.34it/s]39109it [00:15, 3057.00it/s]43199it [00:15, 3098.92it/s]39488it [00:15, 3263.79it/s]39838it [00:15, 3212.24it/s]40198it [00:15, 3321.37it/s]40570it [00:15, 3435.61it/s]40915it [00:16, 3328.08it/s]41262it [00:16, 3368.52it/s]41600it [00:16, 3270.79it/s]41976it [00:16, 3411.42it/s]42351it [00:16, 3508.39it/s]43368it [00:16, 678.05it/s] 42704it [00:16, 3398.02it/s]43743it [00:16, 906.62it/s]43069it [00:16, 3470.31it/s]44108it [00:17, 1156.09it/s]44491it [00:17, 1475.24it/s]43448it [00:17, 602.58it/s] 44873it [00:17, 1816.31it/s]43823it [00:17, 827.68it/s]44113it [00:17, 1019.65it/s]45215it [00:17, 2068.93it/s]44496it [00:17, 1348.16it/s]45590it [00:17, 2397.42it/s]43511it [00:17, 588.75it/s] 44877it [00:18, 1699.95it/s]45937it [00:17, 2581.94it/s]43890it [00:17, 819.01it/s]45206it [00:18, 1958.53it/s]46309it [00:17, 2848.65it/s]44203it [00:17, 1034.29it/s]45579it [00:18, 2301.01it/s]46658it [00:17, 2935.40it/s]44583it [00:17, 1358.88it/s]45919it [00:18, 2481.99it/s]47020it [00:17, 3111.94it/s]44938it [00:17, 1673.67it/s]46279it [00:18, 2737.81it/s]47390it [00:17, 3269.04it/s]45263it [00:17, 1888.10it/s]46629it [00:18, 2776.84it/s]47743it [00:18, 3167.51it/s]45619it [00:18, 2206.98it/s]46990it [00:18, 2985.87it/s]48102it [00:18, 3282.43it/s]45944it [00:18, 2335.91it/s]47348it [00:18, 3141.02it/s]48445it [00:18, 3220.90it/s]46281it [00:18, 2570.39it/s]47690it [00:18, 3020.20it/s]48778it [00:18, 3113.01it/s]46603it [00:18, 2728.23it/s]48020it [00:18, 3095.34it/s]46922it [00:18, 2719.88it/s]49097it [00:18, 2682.60it/s]48345it [00:19, 3003.05it/s]47264it [00:18, 2901.26it/s]49382it [00:18, 2723.10it/s]48714it [00:19, 3189.67it/s]47580it [00:18, 2953.22it/s]49758it [00:18, 2997.26it/s]49093it [00:19, 3358.04it/s]47952it [00:18, 3166.00it/s]50069it [00:18, 3012.82it/s]49437it [00:19, 3302.91it/s]48307it [00:18, 3167.29it/s]50426it [00:18, 3168.50it/s]43418it [00:18, 529.53it/s] 49816it [00:19, 3441.35it/s]48671it [00:19, 3300.24it/s]50788it [00:19, 3296.55it/s]43721it [00:18, 678.91it/s]50165it [00:19, 3339.41it/s]49052it [00:19, 3444.32it/s]44100it [00:18, 923.47it/s]51123it [00:19, 3222.77it/s]50535it [00:19, 3442.35it/s]49403it [00:19, 3334.19it/s]44416it [00:18, 1149.19it/s]51481it [00:19, 3325.30it/s]50883it [00:19, 3370.37it/s]49779it [00:19, 3453.54it/s]44790it [00:19, 1475.58it/s]51817it [00:19, 3243.98it/s]51249it [00:19, 3453.69it/s]50129it [00:19, 3332.26it/s]45115it [00:19, 1737.35it/s]52180it [00:19, 3354.19it/s]51625it [00:20, 3540.30it/s]50500it [00:19, 3438.97it/s]45487it [00:19, 2089.76it/s]52518it [00:19, 3255.22it/s]51981it [00:20, 3439.22it/s]50847it [00:19, 3341.46it/s]45824it [00:19, 2309.76it/s]52881it [00:19, 3362.10it/s]52359it [00:20, 3534.98it/s]51227it [00:19, 3470.92it/s]46195it [00:19, 2619.04it/s]53243it [00:19, 3435.04it/s]52714it [00:20, 3399.78it/s]51591it [00:19, 3517.84it/s]46563it [00:19, 2873.98it/s]53589it [00:19, 3278.00it/s]53087it [00:20, 3493.46it/s]51945it [00:19, 3403.94it/s]46913it [00:19, 2927.48it/s]53967it [00:20, 3419.70it/s]53439it [00:20, 3401.49it/s]52317it [00:20, 3492.21it/s]47292it [00:19, 3150.55it/s]54312it [00:20, 3328.21it/s]53810it [00:20, 3489.74it/s]52668it [00:20, 3370.38it/s]47641it [00:19, 3146.78it/s]54676it [00:20, 3417.01it/s]54178it [00:20, 3543.66it/s]53034it [00:20, 3451.47it/s]48015it [00:20, 3307.44it/s]55029it [00:20, 3296.53it/s]54534it [00:20, 3439.55it/s]53381it [00:20, 3346.78it/s]48364it [00:20, 3222.95it/s]55384it [00:20, 3366.77it/s]54904it [00:20, 3514.49it/s]53751it [00:20, 3446.54it/s]48740it [00:20, 3370.45it/s]55739it [00:20, 3417.94it/s]55257it [00:21, 3400.56it/s]54127it [00:20, 3535.78it/s]49119it [00:20, 3487.24it/s]56083it [00:20, 3314.05it/s]55612it [00:21, 3441.93it/s]54482it [00:20, 3379.18it/s]49476it [00:20, 3334.29it/s]56433it [00:20, 3367.05it/s]55958it [00:21, 3261.42it/s]54836it [00:20, 3424.37it/s]49839it [00:20, 3415.62it/s]56772it [00:20, 3183.98it/s]56324it [00:21, 3373.46it/s]55181it [00:20, 3225.60it/s]50186it [00:20, 3231.46it/s]57125it [00:21, 3280.26it/s]56686it [00:21, 3443.52it/s]55527it [00:21, 3288.73it/s]50547it [00:20, 3330.31it/s]57479it [00:21, 3344.58it/s]57033it [00:21, 3296.63it/s]55859it [00:21, 3253.27it/s]50885it [00:20, 3149.44it/s]57816it [00:21, 3147.93it/s]57366it [00:21, 3289.94it/s]51207it [00:20, 3168.59it/s]56187it [00:21, 3005.07it/s]58135it [00:21, 2969.79it/s]57697it [00:21, 3113.91it/s]51528it [00:21, 3171.92it/s]56507it [00:21, 3058.40it/s]58436it [00:21, 2843.65it/s]58028it [00:21, 3167.30it/s]51848it [00:21, 3074.43it/s]56817it [00:21, 2978.47it/s]58796it [00:21, 3048.10it/s]58380it [00:22, 3267.40it/s]52204it [00:21, 3210.25it/s]57188it [00:21, 3181.95it/s]59159it [00:21, 3207.87it/s]58709it [00:22, 3225.02it/s]52528it [00:21, 3166.46it/s]57548it [00:21, 3176.03it/s]59484it [00:21, 3146.75it/s]59076it [00:22, 3350.92it/s]52906it [00:21, 3341.28it/s]57921it [00:21, 3329.97it/s]59835it [00:21, 3250.05it/s]59413it [00:22, 3282.42it/s]53277it [00:21, 3447.65it/s]58279it [00:21, 3399.98it/s]60163it [00:21, 3200.05it/s]59767it [00:22, 3356.97it/s]53624it [00:21, 3327.44it/s]58622it [00:22, 3317.28it/s]60517it [00:22, 3296.23it/s]60104it [00:22, 3291.82it/s]53999it [00:21, 3447.07it/s]58990it [00:22, 3419.67it/s]60889it [00:22, 3417.99it/s]60474it [00:22, 3409.99it/s]54346it [00:21, 3372.53it/s]59334it [00:22, 3320.87it/s]61233it [00:22, 3282.85it/s]60845it [00:22, 3496.21it/s]54707it [00:22, 3439.18it/s]59688it [00:22, 3383.45it/s]61601it [00:22, 3396.00it/s]61196it [00:22, 3367.67it/s]60057it [00:22, 3471.13it/s]55053it [00:22, 3248.72it/s]61560it [00:22, 3445.07it/s]55405it [00:22, 3322.73it/s]60406it [00:22, 3344.62it/s]55752it [00:22, 3364.98it/s]60774it [00:22, 3438.52it/s]61120it [00:22, 3308.89it/s]56091it [00:22, 3192.57it/s]61487it [00:22, 3410.22it/s]56418it [00:22, 3212.16it/s]56742it [00:22, 3136.28it/s]57110it [00:22, 3290.32it/s]57476it [00:22, 3395.73it/s]57818it [00:23, 3210.98it/s]58178it [00:23, 3319.66it/s]58513it [00:23, 3229.36it/s]58856it [00:23, 3285.36it/s]59199it [00:23, 3326.74it/s]59534it [00:23, 3109.42it/s]59857it [00:23, 3041.63it/s]60164it [00:23, 2868.66it/s]60485it [00:23, 2959.65it/s]60803it [00:23, 3020.58it/s]61108it [00:24, 2985.45it/s]61462it [00:24, 3142.94it/s]61779it [00:24, 3075.27it/s]61906it [00:25, 431.45it/s] 62285it [00:25, 597.91it/s]62658it [00:25, 797.20it/s]61943it [00:25, 371.31it/s] 63030it [00:25, 1047.07it/s]62304it [00:25, 511.74it/s]63407it [00:25, 1343.39it/s]62658it [00:25, 683.87it/s]63744it [00:25, 1601.19it/s]61830it [00:25, 407.58it/s] 63014it [00:25, 903.67it/s]64118it [00:26, 1943.41it/s]62189it [00:25, 557.20it/s]63388it [00:25, 1182.85it/s]62564it [00:25, 758.12it/s]64462it [00:26, 2186.16it/s]63717it [00:25, 1425.68it/s]62872it [00:25, 949.11it/s]64836it [00:26, 2508.18it/s]64089it [00:25, 1765.55it/s]63248it [00:25, 1245.22it/s]65183it [00:26, 2664.48it/s]64425it [00:26, 2001.79it/s]63576it [00:26, 1497.03it/s]65550it [00:26, 2907.17it/s]64798it [00:26, 2340.39it/s]63928it [00:26, 1812.78it/s]65924it [00:26, 3120.38it/s]65156it [00:26, 2610.84it/s]64296it [00:26, 2152.77it/s]66279it [00:26, 3131.03it/s]65501it [00:26, 2736.89it/s]64638it [00:26, 2369.26it/s]66648it [00:26, 3280.54it/s]65849it [00:26, 2920.65it/s]64991it [00:26, 2629.64it/s]66999it [00:26, 3137.77it/s]66188it [00:26, 2839.73it/s]65330it [00:26, 2680.25it/s]67362it [00:27, 3270.50it/s]66541it [00:26, 3017.52it/s]65669it [00:26, 2854.70it/s]67703it [00:27, 3154.17it/s]66869it [00:26, 3009.59it/s]66017it [00:26, 2880.87it/s]68062it [00:27, 3272.63it/s]67188it [00:26, 3024.26it/s]68398it [00:27, 3218.72it/s]66334it [00:26, 2631.08it/s]67504it [00:26, 3050.18it/s]68726it [00:27, 3078.97it/s]66638it [00:27, 2733.20it/s]67819it [00:27, 2955.89it/s]69039it [00:27, 3076.74it/s]66930it [00:27, 2710.18it/s]68134it [00:27, 3008.62it/s]69378it [00:27, 3052.94it/s]67297it [00:27, 2967.39it/s]68457it [00:27, 3069.53it/s]69741it [00:27, 3213.19it/s]67665it [00:27, 3164.12it/s]68768it [00:27, 3036.98it/s]70113it [00:27, 3358.00it/s]67992it [00:27, 3153.51it/s]62089it [00:27, 339.85it/s] 69138it [00:27, 3227.75it/s]70452it [00:28, 3297.05it/s]68347it [00:27, 3265.80it/s]62461it [00:27, 486.69it/s]69464it [00:27, 3172.27it/s]70820it [00:28, 3407.49it/s]68679it [00:27, 3219.43it/s]62770it [00:27, 638.76it/s]69836it [00:27, 3330.46it/s]69052it [00:27, 3365.06it/s]71163it [00:28, 3328.66it/s]63141it [00:27, 875.24it/s]70202it [00:27, 3425.71it/s]71533it [00:28, 3434.76it/s]69392it [00:27, 3304.79it/s]63497it [00:27, 1126.10it/s]70547it [00:27, 3350.06it/s]69761it [00:27, 3416.06it/s]71898it [00:28, 3360.04it/s]63865it [00:27, 1438.82it/s]70906it [00:28, 3418.04it/s]70140it [00:28, 3523.62it/s]72265it [00:28, 3447.61it/s]64234it [00:27, 1773.91it/s]71249it [00:28, 3330.33it/s]70495it [00:28, 3423.05it/s]72612it [00:28, 3406.29it/s]64573it [00:27, 2024.58it/s]71611it [00:28, 3412.02it/s]70862it [00:28, 3493.32it/s]72954it [00:28, 3361.38it/s]64944it [00:28, 2357.44it/s]71954it [00:28, 3338.36it/s]73322it [00:28, 3453.75it/s]71213it [00:28, 3387.83it/s]72312it [00:28, 3406.39it/s]65287it [00:28, 2516.52it/s]71582it [00:28, 3474.79it/s]73669it [00:28, 3383.48it/s]72682it [00:28, 3491.40it/s]65635it [00:28, 2741.66it/s]74050it [00:29, 3504.64it/s]71931it [00:28, 3380.18it/s]65979it [00:28, 2916.05it/s]73032it [00:28, 3389.48it/s]72294it [00:28, 3451.20it/s]74418it [00:29, 3403.28it/s]73403it [00:28, 3479.99it/s]66317it [00:28, 2822.30it/s]72651it [00:28, 3484.54it/s]74788it [00:29, 3487.62it/s]73753it [00:28, 3374.64it/s]66689it [00:28, 3054.36it/s]75156it [00:29, 3541.35it/s]73001it [00:28, 3376.44it/s]74132it [00:28, 3494.06it/s]67022it [00:28, 3079.71it/s]73367it [00:28, 3458.20it/s]75512it [00:29, 3422.58it/s]74483it [00:29, 3370.72it/s]67395it [00:28, 3258.04it/s]75884it [00:29, 3506.47it/s]73714it [00:29, 3272.08it/s]74851it [00:29, 3458.60it/s]67736it [00:28, 3233.52it/s]74090it [00:29, 3408.19it/s]76237it [00:29, 3290.54it/s]75203it [00:29, 3475.80it/s]68099it [00:28, 3344.59it/s]76591it [00:29, 3357.84it/s]74434it [00:29, 3229.96it/s]68459it [00:29, 3416.95it/s]75552it [00:29, 3279.68it/s]74792it [00:29, 3325.63it/s]76939it [00:29, 3205.13it/s]75896it [00:29, 3324.97it/s]68807it [00:29, 3257.47it/s]75132it [00:29, 3344.71it/s]77298it [00:30, 3310.96it/s]69166it [00:29, 3349.05it/s]76231it [00:29, 3198.09it/s]77652it [00:30, 3374.15it/s]75469it [00:29, 2966.33it/s]76554it [00:29, 3102.25it/s]69506it [00:29, 3187.39it/s]77992it [00:30, 3166.58it/s]75775it [00:29, 2964.54it/s]76876it [00:29, 3132.80it/s]69830it [00:29, 3166.08it/s]78316it [00:30, 3184.91it/s]76090it [00:29, 3014.27it/s]70157it [00:29, 3193.79it/s]77191it [00:29, 2672.83it/s]78638it [00:30, 3044.14it/s]76397it [00:29, 2906.24it/s]70479it [00:29, 3098.64it/s]77540it [00:30, 2882.81it/s]79004it [00:30, 3215.07it/s]76751it [00:30, 3070.20it/s]70830it [00:29, 3215.81it/s]77840it [00:30, 2892.09it/s]79371it [00:30, 3344.12it/s]77062it [00:30, 3051.88it/s]71154it [00:29, 3178.30it/s]78204it [00:30, 3098.35it/s]79709it [00:30, 3244.63it/s]77418it [00:30, 3196.55it/s]71514it [00:30, 3299.46it/s]78560it [00:30, 3227.53it/s]80079it [00:30, 3374.42it/s]77774it [00:30, 3301.48it/s]71876it [00:30, 3392.16it/s]78889it [00:30, 3184.46it/s]80419it [00:30, 3308.80it/s]78107it [00:30, 3205.08it/s]72217it [00:30, 3275.94it/s]79237it [00:30, 3266.99it/s]80789it [00:31, 3419.84it/s]78463it [00:30, 3306.95it/s]72575it [00:30, 3362.12it/s]79567it [00:30, 3204.56it/s]81139it [00:31, 3329.44it/s]78796it [00:30, 3206.36it/s]72913it [00:30, 3306.85it/s]79922it [00:30, 3303.43it/s]81504it [00:31, 3420.81it/s]79147it [00:30, 3292.93it/s]73269it [00:30, 3378.21it/s]80290it [00:30, 3411.47it/s]81874it [00:31, 3501.17it/s]79478it [00:30, 3186.73it/s]73608it [00:30, 3299.62it/s]80633it [00:31, 3273.32it/s]82226it [00:31, 3395.39it/s]79834it [00:31, 3292.62it/s]73978it [00:30, 3413.63it/s]80999it [00:31, 3377.81it/s]82583it [00:31, 3444.08it/s]80183it [00:31, 3348.85it/s]74341it [00:30, 3476.94it/s]81339it [00:31, 3297.09it/s]82929it [00:31, 3357.45it/s]80520it [00:31, 3217.78it/s]74690it [00:30, 3299.43it/s]81693it [00:31, 3365.87it/s]83298it [00:31, 3453.06it/s]80876it [00:31, 3315.30it/s]75047it [00:31, 3376.51it/s]82031it [00:31, 3280.32it/s]83659it [00:31, 3350.11it/s]81210it [00:31, 3199.85it/s]75387it [00:31, 3271.54it/s]82387it [00:31, 3358.74it/s]84031it [00:32, 3454.38it/s]81561it [00:31, 3286.33it/s]75748it [00:31, 3359.05it/s]82756it [00:31, 3453.51it/s]84400it [00:32, 3520.78it/s]81916it [00:31, 3360.87it/s]76098it [00:31, 3253.93it/s]83103it [00:31, 3321.39it/s]84754it [00:32, 3397.06it/s]82254it [00:31, 3244.55it/s]76454it [00:31, 3340.53it/s]83469it [00:31, 3417.46it/s]85113it [00:32, 3449.99it/s]82606it [00:31, 3321.75it/s]76812it [00:31, 3407.75it/s]83813it [00:31, 3191.69it/s]82940it [00:31, 3216.18it/s]77155it [00:31, 3285.88it/s]84179it [00:32, 3322.23it/s]83282it [00:32, 3274.27it/s]77508it [00:31, 3346.20it/s]84515it [00:32, 3152.92it/s]83614it [00:32, 3286.33it/s]77845it [00:31, 3180.87it/s]84867it [00:32, 3252.82it/s]83944it [00:32, 3131.44it/s]78198it [00:32, 3278.82it/s]85219it [00:32, 3328.13it/s]84297it [00:32, 3244.58it/s]78563it [00:32, 3383.06it/s]84624it [00:32, 3130.42it/s]78904it [00:32, 3195.61it/s]84944it [00:32, 3148.06it/s]79227it [00:32, 3118.56it/s]85261it [00:32, 2947.76it/s]79542it [00:32, 2722.81it/s]79869it [00:32, 2863.62it/s]80225it [00:32, 3051.44it/s]80539it [00:32, 3057.50it/s]80912it [00:32, 3246.94it/s]81242it [00:33, 3161.09it/s]81611it [00:33, 3284.44it/s]81978it [00:33, 3206.85it/s]82337it [00:33, 3312.41it/s]82709it [00:33, 3428.14it/s]83055it [00:33, 3269.91it/s]83421it [00:33, 3362.25it/s]83760it [00:33, 3251.01it/s]84129it [00:33, 3343.32it/s]84498it [00:34, 3236.37it/s]84866it [00:34, 3357.76it/s]85235it [00:34, 3451.65it/s]85460it [00:35, 389.83it/s] 85828it [00:35, 537.02it/s]86201it [00:35, 728.74it/s]86511it [00:35, 906.67it/s]86857it [00:35, 1163.20it/s]87170it [00:35, 1377.45it/s]87541it [00:35, 1724.50it/s]87871it [00:35, 2000.43it/s]88194it [00:36, 2161.10it/s]88516it [00:36, 2389.17it/s]88828it [00:36, 2521.89it/s]89196it [00:36, 2808.61it/s]89567it [00:36, 3043.44it/s]89905it [00:36, 3036.14it/s]90271it [00:36, 3205.83it/s]90610it [00:36, 3149.76it/s]85555it [00:36, 274.76it/s] 90969it [00:36, 3271.95it/s]85559it [00:36, 270.16it/s] 85921it [00:36, 385.93it/s]91306it [00:36, 3184.30it/s]85881it [00:36, 373.02it/s]86248it [00:36, 513.67it/s]91662it [00:37, 3287.97it/s]86246it [00:36, 525.53it/s]86617it [00:36, 704.26it/s]92018it [00:37, 3366.06it/s]86614it [00:36, 725.16it/s]86972it [00:36, 928.84it/s]92359it [00:37, 3222.43it/s]86980it [00:36, 968.34it/s]87296it [00:36, 1158.37it/s]92717it [00:37, 3322.28it/s]87297it [00:36, 1198.83it/s]87658it [00:36, 1468.24it/s]93053it [00:37, 3219.40it/s]87669it [00:37, 1529.30it/s]87991it [00:37, 1735.51it/s]93410it [00:37, 3316.99it/s]88001it [00:37, 1787.78it/s]88349it [00:37, 2062.20it/s]93768it [00:37, 3391.06it/s]88371it [00:37, 2134.99it/s]88725it [00:37, 2405.50it/s]94110it [00:37, 3266.05it/s]88735it [00:37, 2444.09it/s]89073it [00:37, 2570.42it/s]94469it [00:37, 3358.28it/s]89081it [00:37, 2608.76it/s]89440it [00:37, 2830.08it/s]94807it [00:38, 3243.43it/s]89428it [00:37, 2814.45it/s]89785it [00:37, 2890.49it/s]95148it [00:38, 3290.16it/s]89767it [00:37, 2896.91it/s]90151it [00:37, 3089.50it/s]95487it [00:38, 3169.49it/s]90132it [00:37, 3093.53it/s]90494it [00:37, 3043.83it/s]95840it [00:38, 3270.03it/s]90473it [00:37, 3042.25it/s]90847it [00:37, 3174.64it/s]96187it [00:38, 3323.68it/s]90815it [00:37, 3143.34it/s]91183it [00:38, 3201.05it/s]96521it [00:38, 3127.93it/s]91146it [00:38, 3180.38it/s]91516it [00:38, 3015.09it/s]96862it [00:38, 3206.83it/s]91476it [00:38, 3052.12it/s]91858it [00:38, 3123.77it/s]85583it [00:37, 300.49it/s] 97186it [00:38, 3108.34it/s]91814it [00:38, 3143.36it/s]92179it [00:38, 3024.75it/s]85913it [00:38, 404.11it/s]97500it [00:38, 3109.63it/s]92136it [00:38, 2967.42it/s]92488it [00:38, 3041.19it/s]86235it [00:38, 536.53it/s]97813it [00:38, 3099.51it/s]92440it [00:38, 2983.63it/s]92797it [00:38, 3037.62it/s]86522it [00:38, 678.42it/s]98125it [00:39, 2923.11it/s]92753it [00:38, 3022.67it/s]93104it [00:38, 2879.77it/s]86800it [00:38, 848.21it/s]98448it [00:39, 3007.52it/s]93059it [00:38, 2937.96it/s]93445it [00:38, 3025.80it/s]87087it [00:38, 1056.99it/s]98802it [00:39, 3157.74it/s]93413it [00:38, 3108.81it/s]93798it [00:38, 3169.05it/s]87448it [00:38, 1387.75it/s]99121it [00:39, 3105.10it/s]93757it [00:38, 3202.70it/s]94119it [00:39, 3077.37it/s]87817it [00:38, 1747.30it/s]99477it [00:39, 3228.99it/s]94080it [00:39, 3148.11it/s]94476it [00:39, 3217.80it/s]88139it [00:38, 1999.43it/s]99802it [00:39, 3153.96it/s]94427it [00:39, 3241.45it/s]88512it [00:38, 2354.32it/s]94801it [00:39, 3099.68it/s]100143it [00:39, 3227.71it/s]94753it [00:39, 3157.18it/s]88847it [00:38, 2533.86it/s]95152it [00:39, 3214.24it/s]100495it [00:39, 3310.64it/s]95092it [00:39, 3222.85it/s]89204it [00:39, 2782.77it/s]95488it [00:39, 3133.30it/s]100828it [00:39, 3197.63it/s]95442it [00:39, 3301.27it/s]89557it [00:39, 2973.09it/s]95825it [00:39, 3198.65it/s]101181it [00:40, 3293.25it/s]95774it [00:39, 3174.51it/s]96178it [00:39, 3292.43it/s]89898it [00:39, 2948.95it/s]101512it [00:40, 3181.99it/s]96123it [00:39, 3264.38it/s]90237it [00:39, 3058.19it/s]96509it [00:39, 3172.68it/s]101857it [00:40, 3258.42it/s]96451it [00:39, 3106.50it/s]96860it [00:39, 3267.15it/s]90566it [00:39, 2949.32it/s]102207it [00:40, 3171.69it/s]96801it [00:39, 3217.83it/s]90905it [00:39, 3066.59it/s]97189it [00:39, 3126.34it/s]102557it [00:40, 3263.73it/s]97142it [00:39, 3271.35it/s]91240it [00:39, 3143.49it/s]97543it [00:40, 3242.04it/s]102902it [00:40, 3316.76it/s]97471it [00:40, 3179.66it/s]97896it [00:40, 3323.71it/s]91564it [00:39, 2961.86it/s]103236it [00:40, 3206.69it/s]97810it [00:40, 3239.78it/s]98231it [00:40, 3161.36it/s]91883it [00:39, 3023.54it/s]103587it [00:40, 3292.79it/s]98136it [00:40, 3164.17it/s]98581it [00:40, 3255.55it/s]92192it [00:40, 2902.92it/s]103918it [00:40, 3186.11it/s]98481it [00:40, 3244.96it/s]98909it [00:40, 3172.48it/s]92519it [00:40, 3003.23it/s]104269it [00:40, 3276.47it/s]98831it [00:40, 3319.00it/s]99235it [00:40, 3196.01it/s]92828it [00:40, 3025.76it/s]104617it [00:41, 3334.83it/s]99164it [00:40, 3189.60it/s]99587it [00:40, 3289.58it/s]93134it [00:40, 2984.80it/s]104952it [00:41, 3217.17it/s]99519it [00:40, 3292.96it/s]99918it [00:40, 3182.69it/s]93475it [00:40, 3105.45it/s]105287it [00:41, 3241.12it/s]99850it [00:40, 3094.17it/s]100248it [00:40, 3210.05it/s]93804it [00:40, 3156.76it/s]105613it [00:41, 3065.90it/s]100191it [00:40, 3181.65it/s]100571it [00:41, 3051.42it/s]94122it [00:40, 2984.15it/s]105953it [00:41, 3157.84it/s]100524it [00:41, 3223.84it/s]100913it [00:41, 3154.70it/s]94457it [00:40, 3086.50it/s]106296it [00:41, 3234.82it/s]100849it [00:41, 3080.47it/s]101257it [00:41, 3234.24it/s]94769it [00:40, 2992.92it/s]106622it [00:41, 3069.53it/s]101160it [00:41, 3071.69it/s]95071it [00:41, 2959.98it/s]101583it [00:41, 2956.69it/s]106932it [00:41, 3077.72it/s]101469it [00:41, 2911.36it/s]95372it [00:41, 2972.08it/s]101893it [00:41, 2994.29it/s]107242it [00:41, 3079.32it/s]101767it [00:41, 2928.50it/s]102207it [00:41, 3032.82it/s]95671it [00:41, 2826.43it/s]102096it [00:41, 3030.45it/s]107552it [00:42, 2738.77it/s]96010it [00:41, 2984.99it/s]102514it [00:41, 2969.73it/s]107890it [00:42, 2910.38it/s]102401it [00:41, 2989.34it/s]102867it [00:41, 3127.62it/s]96327it [00:41, 2969.76it/s]108189it [00:42, 2911.20it/s]102752it [00:41, 3138.92it/s]96675it [00:41, 3115.58it/s]103183it [00:41, 3075.98it/s]108546it [00:42, 3095.67it/s]103068it [00:41, 3070.39it/s]103536it [00:41, 3206.53it/s]97028it [00:41, 3206.87it/s]108905it [00:42, 3236.52it/s]103423it [00:42, 3206.82it/s]97351it [00:41, 3126.56it/s]103888it [00:42, 3297.00it/s]109233it [00:42, 3161.94it/s]103766it [00:42, 3270.21it/s]104220it [00:42, 3188.36it/s]97705it [00:41, 3243.90it/s]109591it [00:42, 3280.69it/s]104095it [00:42, 3197.28it/s]104572it [00:42, 3283.24it/s]98031it [00:41, 3167.29it/s]104447it [00:42, 3281.51it/s]109922it [00:42, 3190.01it/s]98358it [00:42, 3186.53it/s]104902it [00:42, 3182.83it/s]110280it [00:42, 3300.86it/s]104777it [00:42, 3201.95it/s]98712it [00:42, 3287.06it/s]105252it [00:42, 3272.74it/s]105128it [00:42, 3289.59it/s]110613it [00:43, 3172.27it/s]99042it [00:42, 3171.06it/s]105581it [00:42, 3164.95it/s]105481it [00:42, 3359.26it/s]110971it [00:43, 3286.60it/s]99395it [00:42, 3272.82it/s]105923it [00:42, 3237.72it/s]111328it [00:43, 3366.42it/s]105818it [00:42, 3244.00it/s]106277it [00:42, 3323.48it/s]99724it [00:42, 3182.45it/s]106172it [00:42, 3327.36it/s]111667it [00:43, 3242.63it/s]100081it [00:42, 3291.80it/s]106611it [00:42, 3219.55it/s]112024it [00:43, 3334.44it/s]106506it [00:42, 3229.77it/s]100438it [00:42, 3371.37it/s]106968it [00:43, 3318.38it/s]106861it [00:43, 3320.50it/s]112360it [00:43, 3226.76it/s]100777it [00:42, 3256.74it/s]107302it [00:43, 3183.36it/s]107215it [00:43, 3384.12it/s]112716it [00:43, 3321.56it/s]101124it [00:42, 3316.34it/s]107654it [00:43, 3279.27it/s]113065it [00:43, 3368.83it/s]107555it [00:43, 3243.93it/s]108009it [00:43, 3355.36it/s]101457it [00:43, 3186.85it/s]107912it [00:43, 3336.78it/s]113404it [00:43, 3256.57it/s]101796it [00:43, 3243.18it/s]108347it [00:43, 3125.15it/s]113758it [00:43, 3335.71it/s]108248it [00:43, 3214.73it/s]102139it [00:43, 3294.76it/s]108702it [00:43, 3243.51it/s]108603it [00:43, 3309.65it/s]114093it [00:44, 3239.90it/s]102470it [00:43, 3172.94it/s]109030it [00:43, 3163.12it/s]114454it [00:44, 3344.16it/s]108936it [00:43, 3183.54it/s]102802it [00:43, 3212.74it/s]109377it [00:43, 3249.07it/s]114799it [00:44, 3374.73it/s]109281it [00:43, 3259.07it/s]109716it [00:43, 3288.06it/s]103125it [00:43, 3065.16it/s]109615it [00:43, 3280.65it/s]103450it [00:43, 3116.60it/s]110047it [00:43, 3096.58it/s]109945it [00:44, 3108.55it/s]103795it [00:43, 3212.52it/s]110392it [00:44, 3195.79it/s]110281it [00:44, 3179.32it/s]104118it [00:43, 3013.85it/s]110715it [00:44, 3020.22it/s]110602it [00:44, 3163.01it/s]104423it [00:43, 3020.35it/s]111021it [00:44, 2777.24it/s]110920it [00:44, 2953.95it/s]111328it [00:44, 2855.27it/s]111227it [00:44, 2985.15it/s]104728it [00:44, 2377.94it/s]111619it [00:44, 2822.41it/s]111529it [00:44, 2920.44it/s]105063it [00:44, 2613.73it/s]111970it [00:44, 3012.96it/s]111883it [00:44, 3095.92it/s]105384it [00:44, 2766.65it/s]112289it [00:44, 2986.98it/s]112230it [00:44, 3203.13it/s]105679it [00:44, 2814.16it/s]112644it [00:44, 3146.06it/s]112553it [00:44, 3149.33it/s]106033it [00:44, 3014.39it/s]113001it [00:44, 3266.08it/s]112896it [00:44, 3230.64it/s]106390it [00:44, 3170.96it/s]113330it [00:45, 3178.48it/s]113221it [00:45, 3168.29it/s]106716it [00:44, 3116.58it/s]113671it [00:45, 3242.83it/s]113566it [00:45, 3247.92it/s]107074it [00:44, 3249.02it/s]113997it [00:45, 3155.97it/s]113924it [00:45, 3344.28it/s]107404it [00:44, 3144.41it/s]114355it [00:45, 3277.97it/s]114260it [00:45, 3221.15it/s]107744it [00:45, 3215.47it/s]114711it [00:45, 3358.67it/s]114618it [00:45, 3322.97it/s]108088it [00:45, 3161.13it/s]108407it [00:45, 3096.85it/s]108769it [00:45, 3245.49it/s]109096it [00:45, 3174.24it/s]109455it [00:45, 3293.12it/s]109786it [00:45, 3200.73it/s]110147it [00:45, 3316.00it/s]110498it [00:45, 3371.18it/s]110837it [00:46, 3247.70it/s]111185it [00:46, 3314.31it/s]111518it [00:46, 3197.47it/s]111861it [00:46, 3261.78it/s]112189it [00:46, 3150.34it/s]112506it [00:46, 2929.05it/s]112826it [00:46, 3002.59it/s]113130it [00:46, 2666.97it/s]113446it [00:46, 2795.17it/s]113755it [00:47, 2874.38it/s]114049it [00:47, 2704.19it/s]114399it [00:47, 2919.43it/s]114736it [00:47, 3044.95it/s]115138it [00:48, 243.66it/s] 115495it [00:48, 341.28it/s]115786it [00:48, 444.55it/s]116143it [00:49, 614.37it/s]116503it [00:49, 829.40it/s]116823it [00:49, 1042.47it/s]117181it [00:49, 1338.76it/s]117507it [00:49, 1581.90it/s]117866it [00:49, 1916.83it/s]118201it [00:49, 2192.39it/s]118531it [00:49, 2367.79it/s]118887it [00:49, 2642.62it/s]119217it [00:50, 2641.32it/s]119562it [00:50, 2842.98it/s]119904it [00:50, 2992.92it/s]120231it [00:50, 2894.16it/s]120583it [00:50, 3062.28it/s]114952it [00:49, 239.70it/s] 115269it [00:50, 325.10it/s]120905it [00:50, 2287.31it/s]115049it [00:50, 230.23it/s] 115575it [00:50, 434.43it/s]115303it [00:50, 294.74it/s]115849it [00:50, 557.10it/s]121173it [00:50, 2095.32it/s]115634it [00:50, 410.41it/s]116191it [00:50, 761.45it/s]121519it [00:50, 2396.40it/s]115920it [00:50, 537.63it/s]116542it [00:50, 1015.80it/s]121811it [00:51, 2521.96it/s]116261it [00:50, 736.01it/s]116851it [00:50, 1245.40it/s]122169it [00:51, 2791.12it/s]116557it [00:50, 926.45it/s]117207it [00:50, 1572.91it/s]122472it [00:51, 2853.10it/s]116907it [00:50, 1216.53it/s]117525it [00:50, 1820.51it/s]122830it [00:51, 3053.23it/s]117257it [00:50, 1532.84it/s]117884it [00:50, 2159.91it/s]123175it [00:51, 3164.69it/s]117576it [00:51, 1785.30it/s]118230it [00:51, 2438.66it/s]123502it [00:51, 3122.24it/s]117928it [00:51, 2113.32it/s]118562it [00:51, 2576.61it/s]123850it [00:51, 3224.43it/s]118252it [00:51, 2305.54it/s]118919it [00:51, 2819.76it/s]124178it [00:51, 3172.22it/s]118601it [00:51, 2576.55it/s]119251it [00:51, 2859.08it/s]124528it [00:51, 3265.18it/s]118950it [00:51, 2801.26it/s]119583it [00:51, 2981.13it/s]124886it [00:51, 3354.04it/s]119283it [00:51, 2853.58it/s]119916it [00:51, 2975.61it/s]125224it [00:52, 3239.75it/s]119631it [00:51, 3018.67it/s]120271it [00:51, 3132.75it/s]125580it [00:52, 3329.53it/s]119961it [00:51, 3001.58it/s]120629it [00:51, 3256.79it/s]125915it [00:52, 3219.07it/s]120311it [00:51, 3137.21it/s]120966it [00:51, 3166.72it/s]126270it [00:52, 3312.79it/s]120660it [00:51, 3236.11it/s]121306it [00:51, 3231.07it/s]126612it [00:52, 3342.53it/s]120995it [00:52, 3147.29it/s]121636it [00:52, 3140.44it/s]126948it [00:52, 3233.64it/s]121342it [00:52, 3238.67it/s]121990it [00:52, 3253.02it/s]127296it [00:52, 3302.68it/s]121673it [00:52, 3145.66it/s]122346it [00:52, 3340.41it/s]127628it [00:52, 3218.48it/s]122018it [00:52, 3231.64it/s]122684it [00:52, 3230.58it/s]127975it [00:52, 3289.16it/s]122369it [00:52, 3311.47it/s]123027it [00:52, 3285.13it/s]128316it [00:53, 3112.27it/s]122704it [00:52, 3082.65it/s]123358it [00:52, 3101.15it/s]128651it [00:53, 3177.21it/s]123036it [00:52, 3148.24it/s]123696it [00:52, 3175.27it/s]128994it [00:53, 3249.03it/s]123355it [00:52, 3060.16it/s]124045it [00:52, 3264.77it/s]129321it [00:53, 3117.53it/s]115046it [00:52, 196.34it/s] 123664it [00:52, 3045.27it/s]129635it [00:53, 3116.90it/s]124374it [00:52, 2990.47it/s]115360it [00:52, 271.43it/s]123971it [00:53, 3037.61it/s]124679it [00:53, 2994.31it/s]129949it [00:53, 3013.89it/s]115646it [00:52, 362.30it/s]124277it [00:53, 2842.11it/s]124983it [00:53, 2836.80it/s]130252it [00:53, 2886.99it/s]115904it [00:52, 465.68it/s]124607it [00:53, 2966.58it/s]125324it [00:53, 2993.84it/s]130586it [00:53, 3013.48it/s]116261it [00:52, 663.00it/s]124956it [00:53, 3114.63it/s]125677it [00:53, 3144.42it/s]130890it [00:53, 3001.10it/s]116556it [00:53, 852.66it/s]125271it [00:53, 3030.54it/s]131231it [00:53, 3118.23it/s]125996it [00:53, 3094.19it/s]116908it [00:53, 1135.74it/s]125625it [00:53, 3175.81it/s]131586it [00:54, 3243.31it/s]126333it [00:53, 3173.20it/s]117262it [00:53, 1452.26it/s]125945it [00:53, 3085.13it/s]126653it [00:53, 3100.37it/s]131912it [00:54, 3126.64it/s]117582it [00:53, 1695.05it/s]126288it [00:53, 3181.55it/s]127002it [00:53, 3211.93it/s]132269it [00:54, 3253.08it/s]117937it [00:53, 2031.31it/s]126630it [00:53, 3249.00it/s]127358it [00:53, 3312.20it/s]132597it [00:54, 3154.39it/s]118260it [00:53, 2226.40it/s]126957it [00:53, 3166.36it/s]132955it [00:54, 3275.84it/s]127691it [00:53, 3176.97it/s]118609it [00:53, 2508.15it/s]127301it [00:54, 3244.53it/s]133300it [00:54, 3325.12it/s]128044it [00:54, 3276.64it/s]118962it [00:53, 2753.82it/s]127627it [00:54, 3160.20it/s]133634it [00:54, 3237.27it/s]128374it [00:54, 3183.06it/s]119295it [00:53, 2804.52it/s]127984it [00:54, 3276.28it/s]133985it [00:54, 3314.40it/s]128729it [00:54, 3286.00it/s]119640it [00:54, 2972.45it/s]128318it [00:54, 3162.34it/s]129069it [00:54, 3318.93it/s]134318it [00:54, 3222.88it/s]119968it [00:54, 2974.41it/s]128676it [00:54, 3280.81it/s]134667it [00:54, 3297.42it/s]129403it [00:54, 3209.21it/s]120318it [00:54, 3116.61it/s]129030it [00:54, 3354.47it/s]135021it [00:55, 3365.77it/s]129750it [00:54, 3282.83it/s]120666it [00:54, 3216.45it/s]129367it [00:54, 3248.84it/s]135359it [00:55, 3232.23it/s]130080it [00:54, 3180.59it/s]121000it [00:54, 3134.83it/s]129718it [00:54, 3323.96it/s]135717it [00:55, 3330.65it/s]130418it [00:54, 3236.50it/s]121333it [00:54, 3189.39it/s]130052it [00:54, 3218.00it/s]130766it [00:54, 3298.03it/s]136052it [00:55, 3198.34it/s]121659it [00:54, 3111.80it/s]130407it [00:55, 3312.22it/s]131097it [00:55, 3198.74it/s]136408it [00:55, 3298.99it/s]121987it [00:54, 3158.47it/s]130761it [00:55, 3378.16it/s]131448it [00:55, 3287.11it/s]136740it [00:55, 3186.18it/s]122338it [00:54, 3258.43it/s]131101it [00:55, 3258.31it/s]137099it [00:55, 3301.09it/s]131778it [00:55, 3144.14it/s]122667it [00:54, 3171.28it/s]131451it [00:55, 3326.04it/s]137446it [00:55, 3348.65it/s]132127it [00:55, 3240.40it/s]123015it [00:55, 3257.75it/s]131786it [00:55, 3206.21it/s]132479it [00:55, 3318.64it/s]137783it [00:55, 3240.88it/s]123343it [00:55, 2912.73it/s]132132it [00:55, 3277.70it/s]138119it [00:56, 3272.51it/s]132813it [00:55, 3105.52it/s]123685it [00:55, 3048.22it/s]132469it [00:55, 3302.62it/s]133144it [00:55, 3160.37it/s]138448it [00:56, 3082.20it/s]124028it [00:55, 3154.10it/s]132801it [00:55, 3128.15it/s]138797it [00:56, 3196.44it/s]133463it [00:55, 3011.03it/s]124350it [00:55, 3017.18it/s]133117it [00:55, 3101.50it/s]139147it [00:56, 3281.83it/s]133801it [00:55, 3113.27it/s]124675it [00:55, 3081.64it/s]133429it [00:56, 2945.94it/s]134116it [00:56, 3069.59it/s]139478it [00:56, 2908.16it/s]124987it [00:55, 2896.81it/s]133738it [00:56, 2985.86it/s]134425it [00:56, 2907.22it/s]139782it [00:56, 2942.11it/s]125300it [00:55, 2960.66it/s]134047it [00:56, 3013.14it/s]134738it [00:56, 2967.48it/s]140083it [00:56, 2848.65it/s]125613it [00:55, 3008.34it/s]134350it [00:56, 2954.45it/s]135038it [00:56, 2951.49it/s]140429it [00:56, 3016.34it/s]125917it [00:56, 2993.95it/s]134679it [00:56, 3049.25it/s]135386it [00:56, 3098.80it/s]140791it [00:56, 3184.82it/s]126251it [00:56, 3093.60it/s]135037it [00:56, 3203.61it/s]135743it [00:56, 3234.81it/s]126605it [00:56, 3223.03it/s]141114it [00:57, 3114.53it/s]135359it [00:56, 3149.95it/s]136069it [00:56, 3159.24it/s]141474it [00:57, 3251.23it/s]126929it [00:56, 3132.93it/s]135721it [00:56, 3282.71it/s]136423it [00:56, 3267.55it/s]127284it [00:56, 3253.81it/s]141803it [00:57, 3156.18it/s]136051it [00:56, 3208.05it/s]136752it [00:56, 3138.85it/s]142165it [00:57, 3288.50it/s]127611it [00:56, 3143.61it/s]136395it [00:56, 3275.09it/s]137109it [00:56, 3259.88it/s]142517it [00:57, 3353.49it/s]127966it [00:56, 3260.02it/s]136724it [00:57, 3193.91it/s]137463it [00:57, 3338.58it/s]128306it [00:56, 3300.40it/s]142855it [00:57, 3250.68it/s]137071it [00:57, 3273.82it/s]137799it [00:57, 3218.29it/s]143204it [00:57, 3318.34it/s]128638it [00:56, 3162.01it/s]137427it [00:57, 3355.09it/s]138152it [00:57, 3307.13it/s]143538it [00:57, 3225.02it/s]128991it [00:56, 3267.12it/s]137764it [00:57, 3230.82it/s]138485it [00:57, 3165.27it/s]143889it [00:57, 3305.34it/s]129320it [00:57, 3142.98it/s]138121it [00:57, 3326.60it/s]138840it [00:57, 3274.24it/s]144249it [00:57, 3389.76it/s]129673it [00:57, 3250.51it/s]138456it [00:57, 3204.61it/s]139196it [00:57, 3355.54it/s]144590it [00:58, 3264.39it/s]130001it [00:57, 3137.34it/s]138812it [00:57, 3305.84it/s]139534it [00:57, 3223.88it/s]144950it [00:58, 3359.68it/s]130352it [00:57, 3242.68it/s]139157it [00:57, 3346.68it/s]139876it [00:57, 3277.73it/s]145288it [00:58, 3238.57it/s]130696it [00:57, 3297.25it/s]139493it [00:57, 3254.27it/s]140206it [00:57, 3178.98it/s]145650it [00:58, 3345.59it/s]131028it [00:57, 3190.47it/s]139841it [00:57, 3317.83it/s]140558it [00:58, 3276.11it/s]145987it [00:58, 3232.37it/s]131370it [00:57, 3254.11it/s]140174it [00:58, 3222.42it/s]140912it [00:58, 3351.00it/s]146345it [00:58, 3329.76it/s]131697it [00:57, 3152.00it/s]140519it [00:58, 3287.37it/s]141249it [00:58, 3194.18it/s]146690it [00:58, 3363.98it/s]132035it [00:57, 3216.73it/s]140877it [00:58, 3371.65it/s]141602it [00:58, 3288.23it/s]147028it [00:58, 3265.17it/s]132386it [00:58, 3301.57it/s]141216it [00:58, 3236.05it/s]147367it [00:58, 3299.02it/s]141933it [00:58, 3101.68it/s]132718it [00:58, 3067.12it/s]141564it [00:58, 3304.07it/s]142276it [00:58, 3192.55it/s]147699it [00:59, 3142.98it/s]133059it [00:58, 3162.63it/s]141896it [00:58, 3110.62it/s]148037it [00:59, 3210.03it/s]142599it [00:58, 2938.23it/s]133379it [00:58, 3027.47it/s]142248it [00:58, 3190.56it/s]148387it [00:59, 3292.94it/s]142942it [00:58, 3071.33it/s]133695it [00:58, 3062.77it/s]142584it [00:58, 3238.23it/s]143255it [00:58, 3073.66it/s]148718it [00:59, 3046.12it/s]134005it [00:58, 3071.00it/s]142910it [00:58, 3025.20it/s]149035it [00:59, 3078.82it/s]143566it [00:59, 2882.37it/s]143225it [00:59, 3058.14it/s]134314it [00:58, 2680.96it/s]143877it [00:59, 2943.08it/s]149347it [00:59, 2896.52it/s]134635it [00:58, 2820.84it/s]143534it [00:59, 2854.24it/s]144217it [00:59, 3070.38it/s]149697it [00:59, 3060.99it/s]134978it [00:58, 2987.88it/s]143894it [00:59, 3056.80it/s]150048it [00:59, 3185.27it/s]144528it [00:59, 3001.29it/s]135285it [00:59, 2991.90it/s]144242it [00:59, 3175.58it/s]144874it [00:59, 3131.97it/s]150371it [00:59, 3126.88it/s]135635it [00:59, 3134.69it/s]144564it [00:59, 3146.06it/s]150720it [01:00, 3229.29it/s]145190it [00:59, 3076.95it/s]135953it [00:59, 3104.73it/s]144917it [00:59, 3256.39it/s]145546it [00:59, 3215.60it/s]151046it [01:00, 3178.42it/s]136300it [00:59, 3208.19it/s]145246it [00:59, 3205.40it/s]145905it [00:59, 3324.79it/s]136659it [00:59, 3317.58it/s]145602it [00:59, 3308.01it/s]146240it [00:59, 3191.14it/s]136993it [00:59, 3216.30it/s]145958it [00:59, 3196.30it/s]146593it [00:59, 3287.64it/s]137349it [00:59, 3313.40it/s]146312it [01:00, 3291.29it/s]146924it [01:00, 3191.78it/s]137683it [00:59, 3217.76it/s]146671it [01:00, 3375.50it/s]147280it [01:00, 3296.58it/s]138044it [00:59, 3330.33it/s]147011it [01:00, 3265.43it/s]147628it [01:00, 3349.29it/s]138396it [00:59, 3384.64it/s]147375it [01:00, 3372.65it/s]147965it [01:00, 3226.56it/s]138736it [01:00, 3274.44it/s]147715it [01:00, 3261.06it/s]148319it [01:00, 3314.89it/s]139086it [01:00, 3338.33it/s]148076it [01:00, 3358.62it/s]148653it [01:00, 3189.35it/s]139422it [01:00, 3240.80it/s]148426it [01:00, 3396.93it/s]148997it [01:00, 3260.25it/s]139768it [01:00, 3301.48it/s]148768it [01:00, 3285.58it/s]149325it [01:00, 3155.10it/s]140100it [01:00, 3214.06it/s]149119it [01:00, 3349.50it/s]149680it [01:00, 3265.39it/s]140445it [01:00, 3281.89it/s]149456it [01:00, 3259.35it/s]150030it [01:00, 3332.01it/s]140802it [01:00, 3363.28it/s]149807it [01:01, 3328.97it/s]150365it [01:01, 3189.80it/s]141140it [01:00, 3236.15it/s]150158it [01:01, 3238.40it/s]150716it [01:01, 3277.39it/s]141497it [01:00, 3331.37it/s]150510it [01:01, 3316.18it/s]151046it [01:01, 3084.13it/s]141832it [01:01, 3124.70it/s]150859it [01:01, 3364.90it/s]142168it [01:01, 3188.71it/s]142514it [01:01, 3264.70it/s]142843it [01:01, 3120.32it/s]143165it [01:01, 3147.88it/s]143482it [01:01, 2558.97it/s]143792it [01:01, 2692.82it/s]144150it [01:01, 2925.99it/s]144457it [01:01, 2943.47it/s]144824it [01:02, 3144.00it/s]145147it [01:02, 3103.46it/s]145512it [01:02, 3257.20it/s]145866it [01:02, 3338.73it/s]146204it [01:02, 3241.90it/s]146562it [01:02, 3339.07it/s]146899it [01:02, 3227.80it/s]147257it [01:02, 3328.13it/s]147617it [01:02, 3406.85it/s]147960it [01:02, 3281.28it/s]148323it [01:03, 3379.29it/s]148663it [01:03, 3264.41it/s]149026it [01:03, 3367.83it/s]149365it [01:03, 3224.91it/s]149730it [01:03, 3343.20it/s]150079it [01:03, 3383.04it/s]150420it [01:03, 3265.24it/s]150772it [01:03, 3337.78it/s]151366it [01:06, 173.47it/s] 151728it [01:06, 249.32it/s]152026it [01:06, 332.55it/s]152374it [01:06, 463.88it/s]152738it [01:06, 642.52it/s]153056it [01:06, 827.06it/s]153422it [01:06, 1097.86it/s]153750it [01:06, 1347.36it/s]154106it [01:07, 1668.50it/s]154437it [01:07, 1921.30it/s]154803it [01:07, 2259.84it/s]151358it [01:06, 194.15it/s] 155169it [01:07, 2563.84it/s]151717it [01:06, 277.20it/s]155514it [01:07, 2669.74it/s]152015it [01:06, 368.41it/s]155879it [01:07, 2909.08it/s]152376it [01:07, 517.48it/s]152718it [01:07, 696.38it/s]156220it [01:07, 2880.01it/s]153030it [01:07, 881.46it/s]156571it [01:07, 3041.76it/s]153379it [01:07, 1148.58it/s]156910it [01:07, 3134.14it/s]153693it [01:07, 1374.09it/s]157244it [01:07, 3034.80it/s]154029it [01:07, 1674.55it/s]157578it [01:08, 3116.65it/s]154344it [01:07, 1936.59it/s]151197it [01:07, 174.35it/s] 157901it [01:08, 2957.31it/s]154657it [01:07, 2093.51it/s]151485it [01:07, 231.28it/s]158206it [01:08, 2963.49it/s]154970it [01:07, 2318.54it/s]151734it [01:07, 296.84it/s]158532it [01:08, 3044.93it/s]155272it [01:08, 2473.23it/s]152028it [01:08, 402.76it/s]158842it [01:08, 3009.14it/s]155632it [01:08, 2756.98it/s]152375it [01:08, 568.80it/s]159209it [01:08, 3196.75it/s]155991it [01:08, 2976.93it/s]152736it [01:08, 787.13it/s]159532it [01:08, 3129.33it/s]156322it [01:08, 2977.30it/s]153044it [01:08, 994.84it/s]159897it [01:08, 3277.41it/s]156680it [01:08, 3142.04it/s]153404it [01:08, 1298.93it/s]160259it [01:08, 3375.99it/s]153724it [01:08, 1561.66it/s]157012it [01:08, 3112.41it/s]160599it [01:09, 3283.65it/s]154071it [01:08, 1883.64it/s]157371it [01:08, 3244.82it/s]160961it [01:09, 3379.35it/s]157723it [01:08, 3321.71it/s]154428it [01:08, 2145.73it/s]161301it [01:09, 3288.48it/s]154790it [01:08, 2458.14it/s]158063it [01:08, 3238.13it/s]161666it [01:09, 3392.72it/s]155155it [01:08, 2733.86it/s]158425it [01:08, 3347.63it/s]162007it [01:09, 3281.93it/s]158764it [01:09, 3267.18it/s]155498it [01:09, 2793.76it/s]162378it [01:09, 3403.75it/s]159119it [01:09, 3347.32it/s]155860it [01:09, 3004.21it/s]162748it [01:09, 3489.68it/s]159466it [01:09, 3264.07it/s]156199it [01:09, 3023.92it/s]163099it [01:09, 3373.51it/s]159831it [01:09, 3373.42it/s]156556it [01:09, 3169.45it/s]163456it [01:09, 3425.81it/s]160191it [01:09, 3438.48it/s]156922it [01:09, 3303.66it/s]163800it [01:09, 3338.37it/s]160537it [01:09, 3297.01it/s]157268it [01:09, 3241.73it/s]164169it [01:10, 3437.72it/s]160899it [01:09, 3387.84it/s]157635it [01:09, 3362.41it/s]164515it [01:10, 3328.61it/s]161240it [01:09, 3281.73it/s]157980it [01:09, 3270.33it/s]164876it [01:10, 3408.80it/s]161601it [01:09, 3374.56it/s]158342it [01:09, 3369.26it/s]165245it [01:10, 3490.17it/s]161958it [01:10, 3428.70it/s]158684it [01:10, 3278.80it/s]165596it [01:10, 3380.25it/s]159041it [01:10, 3358.95it/s]162303it [01:10, 3283.81it/s]165958it [01:10, 3448.21it/s]159380it [01:10, 3365.91it/s]162656it [01:10, 3352.52it/s]166305it [01:10, 3219.08it/s]159719it [01:10, 3202.93it/s]162994it [01:10, 3139.56it/s]166669it [01:10, 3336.42it/s]160077it [01:10, 3308.00it/s]163356it [01:10, 3272.93it/s]167026it [01:10, 3249.41it/s]163687it [01:10, 3101.84it/s]160411it [01:10, 3024.17it/s]167354it [01:11, 3229.28it/s]164001it [01:10, 3095.50it/s]160731it [01:10, 3071.69it/s]167679it [01:11, 3220.38it/s]151108it [01:10, 168.40it/s] 164314it [01:10, 3100.42it/s]161052it [01:10, 3108.73it/s]168003it [01:11, 3013.44it/s]151427it [01:10, 230.18it/s]161367it [01:10, 3073.23it/s]164626it [01:10, 3047.63it/s]168370it [01:11, 3194.85it/s]151778it [01:10, 323.24it/s]161735it [01:11, 3245.81it/s]164998it [01:10, 3239.09it/s]168706it [01:11, 3172.82it/s]152070it [01:10, 424.41it/s]162063it [01:11, 3211.21it/s]165345it [01:11, 3196.41it/s]169076it [01:11, 3321.04it/s]152428it [01:10, 590.74it/s]162424it [01:11, 3326.36it/s]165705it [01:11, 3311.42it/s]169445it [01:11, 3426.74it/s]152746it [01:10, 767.41it/s]162796it [01:11, 3440.05it/s]166077it [01:11, 3429.20it/s]169790it [01:11, 3294.28it/s]153105it [01:11, 1022.66it/s]163142it [01:11, 3351.70it/s]166422it [01:11, 3339.61it/s]170161it [01:11, 3412.84it/s]153447it [01:11, 1296.97it/s]163513it [01:11, 3453.52it/s]166799it [01:11, 3463.42it/s]170505it [01:12, 3315.92it/s]153775it [01:11, 1555.45it/s]163860it [01:11, 3257.06it/s]167147it [01:11, 3326.80it/s]170876it [01:12, 3423.52it/s]154139it [01:11, 1899.36it/s]164229it [01:11, 3379.31it/s]167515it [01:11, 3426.41it/s]171226it [01:12, 3285.57it/s]154472it [01:11, 2066.98it/s]164570it [01:11, 3306.78it/s]167865it [01:11, 3331.95it/s]171594it [01:12, 3394.76it/s]154834it [01:11, 2386.21it/s]164946it [01:11, 3434.99it/s]168239it [01:11, 3447.24it/s]171964it [01:12, 3481.96it/s]155196it [01:11, 2662.31it/s]165320it [01:12, 3523.05it/s]168598it [01:12, 3486.54it/s]172315it [01:12, 3362.20it/s]155533it [01:11, 2771.61it/s]165675it [01:12, 3419.33it/s]168948it [01:12, 3366.52it/s]172687it [01:12, 3464.41it/s]155893it [01:11, 2982.36it/s]166019it [01:12, 3418.01it/s]169315it [01:12, 3452.69it/s]173036it [01:12, 3341.31it/s]156231it [01:12, 3005.03it/s]166362it [01:12, 3348.08it/s]169662it [01:12, 3343.56it/s]173403it [01:12, 3433.63it/s]156592it [01:12, 3166.51it/s]166743it [01:12, 3480.65it/s]170015it [01:12, 3395.59it/s]173749it [01:12, 3343.17it/s]156946it [01:12, 3139.93it/s]167093it [01:12, 3385.60it/s]170385it [01:12, 3318.77it/s]174122it [01:13, 3451.89it/s]157309it [01:12, 3275.14it/s]167465it [01:12, 3480.66it/s]170752it [01:12, 3418.23it/s]174483it [01:13, 3496.69it/s]157672it [01:12, 3373.91it/s]167839it [01:12, 3555.38it/s]171119it [01:12, 3488.78it/s]174834it [01:13, 3374.15it/s]158018it [01:12, 3281.63it/s]168196it [01:12, 3416.04it/s]171470it [01:12, 3340.35it/s]175207it [01:13, 3474.30it/s]158373it [01:12, 3357.70it/s]168563it [01:12, 3486.89it/s]171823it [01:12, 3391.81it/s]175556it [01:13, 3210.34it/s]158714it [01:12, 3163.61it/s]168914it [01:13, 3290.10it/s]172164it [01:13, 3231.18it/s]175916it [01:13, 3309.79it/s]159068it [01:12, 3267.58it/s]169274it [01:13, 3374.86it/s]172512it [01:13, 3288.24it/s]159410it [01:12, 3310.20it/s]176266it [01:13, 3199.47it/s]169615it [01:13, 3262.95it/s]172880it [01:13, 3400.20it/s]176613it [01:13, 3273.38it/s]159745it [01:13, 3183.86it/s]169944it [01:13, 3256.85it/s]173222it [01:13, 3125.67it/s]160067it [01:13, 3119.27it/s]176944it [01:13, 2880.15it/s]170272it [01:13, 3262.62it/s]173540it [01:13, 3128.27it/s]160382it [01:13, 2941.58it/s]177242it [01:14, 2778.49it/s]170600it [01:13, 3071.60it/s]173857it [01:13, 2975.94it/s]160709it [01:13, 3031.63it/s]177527it [01:14, 2783.71it/s]170941it [01:13, 3164.58it/s]174210it [01:13, 3127.74it/s]161049it [01:13, 3135.65it/s]177888it [01:14, 3007.77it/s]171260it [01:13, 3148.11it/s]174585it [01:13, 3147.88it/s]161366it [01:13, 3099.35it/s]178195it [01:14, 2992.15it/s]171630it [01:13, 3307.16it/s]174945it [01:13, 3273.38it/s]161715it [01:13, 3210.31it/s]178557it [01:14, 3169.83it/s]172002it [01:14, 3426.71it/s]175319it [01:14, 3404.68it/s]162038it [01:13, 3175.87it/s]178878it [01:14, 3117.71it/s]172347it [01:14, 3322.68it/s]175663it [01:14, 3272.03it/s]162392it [01:13, 3279.41it/s]179248it [01:14, 3285.03it/s]172722it [01:14, 3445.88it/s]176025it [01:14, 3370.12it/s]162757it [01:14, 3378.23it/s]179624it [01:14, 3422.33it/s]173069it [01:14, 3373.64it/s]176365it [01:14, 3280.22it/s]163096it [01:14, 3261.27it/s]179969it [01:14, 3312.87it/s]173444it [01:14, 3481.54it/s]176733it [01:14, 3392.68it/s]163456it [01:14, 3357.53it/s]180348it [01:15, 3450.40it/s]173794it [01:14, 3356.65it/s]177083it [01:14, 3422.56it/s]163794it [01:14, 3256.09it/s]180696it [01:15, 3345.58it/s]174168it [01:14, 3463.76it/s]177427it [01:14, 3322.78it/s]164160it [01:14, 3370.98it/s]181075it [01:15, 3471.15it/s]174549it [01:14, 3562.70it/s]177790it [01:14, 3409.69it/s]164507it [01:14, 3267.20it/s]181425it [01:15, 3347.92it/s]174907it [01:14, 3425.10it/s]178133it [01:14, 3335.59it/s]164882it [01:14, 3403.21it/s]181797it [01:15, 3451.14it/s]175274it [01:15, 3494.86it/s]178499it [01:15, 3428.68it/s]165240it [01:14, 3453.73it/s]182147it [01:15, 3375.44it/s]175626it [01:15, 3392.33it/s]178844it [01:15, 3301.68it/s]165587it [01:14, 3364.46it/s]182520it [01:15, 3474.78it/s]175993it [01:15, 3470.59it/s]179206it [01:15, 3390.82it/s]165950it [01:14, 3440.51it/s]182892it [01:15, 3544.54it/s]176342it [01:15, 3337.49it/s]179570it [01:15, 3463.05it/s]166296it [01:15, 3349.58it/s]183248it [01:15, 3436.40it/s]176717it [01:15, 3453.77it/s]179918it [01:15, 3341.47it/s]166658it [01:15, 3426.23it/s]183618it [01:15, 3512.27it/s]177086it [01:15, 3520.76it/s]180298it [01:15, 3471.34it/s]167027it [01:15, 3347.35it/s]183971it [01:16, 3414.01it/s]177440it [01:15, 3412.06it/s]180647it [01:15, 3306.06it/s]167377it [01:15, 3382.40it/s]184341it [01:16, 3496.37it/s]177799it [01:15, 3462.64it/s]181019it [01:15, 3420.54it/s]167744it [01:15, 3462.68it/s]184692it [01:16, 3330.40it/s]178147it [01:15, 3310.02it/s]181364it [01:15, 3218.87it/s]168092it [01:15, 3242.59it/s]185060it [01:16, 3428.15it/s]178504it [01:15, 3383.16it/s]181716it [01:15, 3301.80it/s]168453it [01:15, 3345.39it/s]185411it [01:16, 3449.25it/s]178845it [01:16, 3240.05it/s]182073it [01:16, 3377.29it/s]168791it [01:15, 3212.06it/s]185758it [01:16, 3256.84it/s]179194it [01:16, 3309.51it/s]182414it [01:16, 3174.59it/s]169122it [01:15, 3237.87it/s]186087it [01:16, 3201.72it/s]179527it [01:16, 3302.39it/s]182743it [01:16, 3204.47it/s]169448it [01:16, 3210.78it/s]179859it [01:16, 3121.27it/s]186410it [01:16, 3016.35it/s]183067it [01:16, 3020.79it/s]169771it [01:16, 2876.18it/s]180191it [01:16, 3176.43it/s]186715it [01:17, 2615.68it/s]183373it [01:16, 2973.44it/s]170076it [01:16, 2921.69it/s]180511it [01:16, 3071.26it/s]187080it [01:17, 2877.20it/s]183744it [01:16, 3178.26it/s]170387it [01:16, 2915.00it/s]180883it [01:16, 3254.58it/s]187382it [01:17, 2914.80it/s]184065it [01:16, 3150.30it/s]170746it [01:16, 3102.69it/s]181248it [01:16, 3367.12it/s]187756it [01:17, 3141.66it/s]184432it [01:16, 3298.94it/s]171100it [01:16, 3225.93it/s]181587it [01:16, 3295.04it/s]188079it [01:17, 3140.82it/s]184765it [01:16, 3238.38it/s]171426it [01:16, 3196.05it/s]181942it [01:17, 3368.07it/s]188439it [01:17, 3271.51it/s]185139it [01:17, 3383.02it/s]171779it [01:16, 3291.27it/s]182281it [01:17, 3336.93it/s]188812it [01:17, 3402.14it/s]185500it [01:17, 3447.73it/s]172111it [01:16, 3220.47it/s]182658it [01:17, 3462.84it/s]189156it [01:17, 3289.71it/s]185847it [01:17, 3354.44it/s]172478it [01:16, 3348.63it/s]183006it [01:17, 3366.58it/s]189529it [01:17, 3413.89it/s]186221it [01:17, 3465.27it/s]172852it [01:17, 3460.83it/s]183372it [01:17, 3450.12it/s]189874it [01:17, 3290.42it/s]186569it [01:17, 3356.78it/s]173200it [01:17, 3348.12it/s]183744it [01:17, 3528.56it/s]190252it [01:18, 3429.25it/s]186959it [01:17, 3512.06it/s]173569it [01:17, 3446.45it/s]184098it [01:17, 3421.50it/s]190598it [01:18, 3323.42it/s]187312it [01:17, 3385.75it/s]173916it [01:17, 3327.00it/s]184472it [01:17, 3512.86it/s]190963it [01:18, 3415.70it/s]187683it [01:17, 3477.11it/s]174288it [01:17, 3439.00it/s]184825it [01:17, 3387.56it/s]191323it [01:18, 3466.62it/s]188033it [01:17, 3373.80it/s]174634it [01:17, 3349.00it/s]185199it [01:17, 3487.00it/s]191672it [01:18, 3347.78it/s]188407it [01:18, 3476.75it/s]175001it [01:17, 3439.34it/s]185550it [01:18, 3356.56it/s]192050it [01:18, 3471.32it/s]188768it [01:18, 3513.58it/s]175359it [01:17, 3478.98it/s]185926it [01:18, 3470.38it/s]192399it [01:18, 3329.07it/s]189121it [01:18, 3369.09it/s]175709it [01:17, 3375.92it/s]186291it [01:18, 3521.64it/s]192782it [01:18, 3470.95it/s]189494it [01:18, 3471.10it/s]176064it [01:18, 3424.56it/s]186645it [01:18, 3430.93it/s]193132it [01:18, 3360.46it/s]189843it [01:18, 3351.32it/s]176408it [01:18, 3331.53it/s]187033it [01:18, 3558.59it/s]193499it [01:18, 3448.34it/s]190221it [01:18, 3472.72it/s]176772it [01:18, 3419.47it/s]187391it [01:18, 3422.94it/s]193867it [01:19, 3512.69it/s]190571it [01:18, 3343.36it/s]177116it [01:18, 3275.32it/s]187763it [01:18, 3507.76it/s]194220it [01:19, 3388.31it/s]190929it [01:18, 3410.32it/s]177466it [01:18, 3338.94it/s]188116it [01:18, 3337.51it/s]194587it [01:19, 3468.43it/s]191273it [01:18, 3418.56it/s]177826it [01:18, 3413.02it/s]188478it [01:18, 3415.18it/s]194936it [01:19, 3260.34it/s]191617it [01:18, 3223.61it/s]178169it [01:18, 3234.80it/s]188824it [01:19, 3427.56it/s]195307it [01:19, 3378.14it/s]191991it [01:19, 3368.20it/s]178496it [01:18, 3195.39it/s]189169it [01:19, 3288.68it/s]195648it [01:19, 3118.01it/s]189500it [01:19, 3280.66it/s]178818it [01:18, 2770.57it/s]195966it [01:19, 3128.36it/s]192331it [01:19, 2517.12it/s]189830it [01:19, 3109.32it/s]179138it [01:19, 2881.84it/s]196289it [01:19, 3154.41it/s]192665it [01:19, 2709.30it/s]190144it [01:19, 3065.28it/s]179448it [01:19, 2940.63it/s]193015it [01:19, 2906.18it/s]190498it [01:19, 3198.37it/s]179759it [01:19, 2987.13it/s]193330it [01:19, 2939.14it/s]190820it [01:19, 3167.87it/s]180117it [01:19, 3150.89it/s]193696it [01:19, 3132.97it/s]191187it [01:19, 3311.23it/s]180467it [01:19, 3150.37it/s]194024it [01:19, 3149.01it/s]191520it [01:19, 3262.08it/s]180832it [01:19, 3292.94it/s]194398it [01:19, 3316.10it/s]191887it [01:19, 3380.66it/s]181202it [01:19, 3409.71it/s]194746it [01:20, 3245.50it/s]192228it [01:20, 3305.61it/s]181546it [01:19, 3273.48it/s]195139it [01:20, 3438.97it/s]192606it [01:20, 3441.83it/s]181913it [01:19, 3385.73it/s]195488it [01:20, 3367.37it/s]192985it [01:20, 3542.53it/s]182254it [01:19, 3288.57it/s]195829it [01:20, 3296.60it/s]193341it [01:20, 3399.31it/s]182632it [01:20, 3427.53it/s]196197it [01:20, 3404.72it/s]193711it [01:20, 3484.30it/s]182987it [01:20, 3301.38it/s]194062it [01:20, 3382.54it/s]183362it [01:20, 3427.18it/s]194441it [01:20, 3498.86it/s]183724it [01:20, 3482.24it/s]194793it [01:20, 3371.65it/s]184075it [01:20, 3348.72it/s]195185it [01:20, 3526.47it/s]184452it [01:20, 3468.42it/s]195562it [01:21, 3595.83it/s]184801it [01:20, 3344.04it/s]195924it [01:21, 3450.34it/s]185175it [01:20, 3455.22it/s]196287it [01:21, 3500.02it/s]185523it [01:20, 3301.67it/s]185901it [01:21, 3435.05it/s]186272it [01:21, 3511.68it/s]186626it [01:21, 3408.16it/s]186992it [01:21, 3477.71it/s]187342it [01:21, 3265.99it/s]187705it [01:21, 3365.48it/s]188045it [01:21, 3182.47it/s]188401it [01:21, 3286.15it/s]188733it [01:21, 3295.49it/s]189065it [01:22, 3078.32it/s]189407it [01:22, 3172.92it/s]189728it [01:22, 3117.77it/s]190108it [01:22, 3311.25it/s]190473it [01:22, 3406.90it/s]190817it [01:22, 3309.38it/s]191177it [01:22, 3383.47it/s]191518it [01:22, 3301.26it/s]191884it [01:22, 3403.84it/s]192227it [01:22, 3313.72it/s]192595it [01:23, 3418.04it/s]192973it [01:23, 3522.40it/s]193327it [01:23, 3360.46it/s]193695it [01:23, 3451.56it/s]194043it [01:23, 3332.29it/s]194418it [01:23, 3449.60it/s]194765it [01:23, 3315.93it/s]195164it [01:23, 3505.09it/s]195521it [01:23, 3523.66it/s]195876it [01:24, 3400.44it/s]196233it [01:24, 3447.23it/s]196608it [01:26, 162.94it/s] 196994it [01:26, 239.00it/s]197336it [01:26, 329.03it/s]197713it [01:26, 463.56it/s]198103it [01:26, 645.28it/s]198439it [01:26, 832.47it/s]198807it [01:27, 1091.13it/s]199149it [01:27, 1348.37it/s]199530it [01:27, 1693.01it/s]196540it [01:26, 178.40it/s] 199879it [01:27, 1957.95it/s]196926it [01:26, 256.38it/s]200238it [01:27, 2266.95it/s]197294it [01:26, 356.92it/s]200610it [01:27, 2576.13it/s]197604it [01:27, 467.28it/s]200963it [01:27, 2718.73it/s]197984it [01:27, 648.52it/s]201332it [01:27, 2954.74it/s]198315it [01:27, 836.93it/s]198686it [01:27, 1103.95it/s]201682it [01:27, 2993.84it/s]202050it [01:27, 3173.60it/s]199025it [01:27, 1342.24it/s]202397it [01:28, 3118.24it/s]199392it [01:27, 1669.71it/s]202774it [01:28, 3295.37it/s]199742it [01:27, 1975.66it/s]203129it [01:28, 3340.18it/s]200082it [01:27, 2191.69it/s]200410it [01:27, 2413.97it/s]203475it [01:28, 2932.68it/s]200737it [01:28, 2485.54it/s]201063it [01:28, 2669.51it/s]203785it [01:28, 2410.78it/s]201426it [01:28, 2914.14it/s]204087it [01:28, 2551.44it/s]196639it [01:28, 163.71it/s] 201755it [01:28, 2969.89it/s]204462it [01:28, 2848.11it/s]197024it [01:28, 233.63it/s]202122it [01:28, 3159.15it/s]204835it [01:28, 3077.68it/s]197338it [01:28, 310.29it/s]205161it [01:29, 3102.53it/s]202459it [01:28, 3174.77it/s]197708it [01:28, 433.41it/s]205523it [01:29, 3246.51it/s]202839it [01:28, 3350.45it/s]198082it [01:28, 596.45it/s]205858it [01:29, 3219.62it/s]203215it [01:28, 3306.73it/s]198417it [01:28, 774.01it/s]206231it [01:29, 3364.01it/s]203581it [01:28, 3406.06it/s]198792it [01:28, 1028.67it/s]206576it [01:29, 3307.79it/s]203948it [01:28, 3479.90it/s]199135it [01:29, 1276.78it/s]206939it [01:29, 3398.72it/s]204301it [01:29, 3366.01it/s]199504it [01:29, 1598.62it/s]207316it [01:29, 3505.17it/s]204658it [01:29, 3424.11it/s]199857it [01:29, 1865.51it/s]207670it [01:29, 3402.54it/s]205004it [01:29, 3336.72it/s]200224it [01:29, 2194.98it/s]208050it [01:29, 3515.54it/s]205380it [01:29, 3455.74it/s]200598it [01:29, 2513.58it/s]208404it [01:29, 3430.32it/s]205735it [01:29, 3353.26it/s]208783it [01:30, 3532.58it/s]200951it [01:29, 2650.15it/s]206117it [01:29, 3485.15it/s]201325it [01:29, 2909.75it/s]209138it [01:30, 3449.40it/s]206497it [01:29, 3574.44it/s]209510it [01:30, 3525.30it/s]201674it [01:29, 2983.22it/s]206857it [01:29, 3413.89it/s]209869it [01:30, 3542.71it/s]202052it [01:29, 3191.50it/s]207237it [01:29, 3523.37it/s]210225it [01:30, 3417.29it/s]202403it [01:30, 3183.84it/s]207592it [01:30, 3399.04it/s]210600it [01:30, 3511.42it/s]202787it [01:30, 3362.70it/s]207972it [01:30, 3512.35it/s]203165it [01:30, 3478.62it/s]210953it [01:30, 3397.48it/s]208326it [01:30, 3423.30it/s]211314it [01:30, 3455.68it/s]203526it [01:30, 3320.69it/s]208693it [01:30, 3493.76it/s]203883it [01:30, 3389.16it/s]211661it [01:30, 3269.29it/s]209044it [01:30, 3482.62it/s]212025it [01:31, 3371.71it/s]204230it [01:30, 3207.65it/s]209394it [01:30, 3281.50it/s]212384it [01:31, 3433.20it/s]204589it [01:30, 3313.07it/s]209748it [01:30, 3353.78it/s]212730it [01:31, 3282.79it/s]204927it [01:30, 3194.43it/s]210086it [01:30, 3125.50it/s]213061it [01:31, 3268.88it/s]205251it [01:30, 3043.03it/s]210413it [01:30, 3164.93it/s]205578it [01:31, 3104.04it/s]210734it [01:31, 3177.01it/s]213390it [01:31, 2881.11it/s]213725it [01:31, 3004.40it/s]211055it [01:31, 3045.96it/s]205892it [01:31, 2840.87it/s]214088it [01:31, 3174.47it/s]211421it [01:31, 3217.92it/s]206271it [01:31, 3094.88it/s]214413it [01:31, 3152.16it/s]211746it [01:31, 3149.25it/s]206588it [01:31, 3095.52it/s]214780it [01:31, 3298.31it/s]212116it [01:31, 3305.56it/s]206953it [01:31, 3250.71it/s]215114it [01:32, 3251.51it/s]207332it [01:31, 3404.78it/s]212455it [01:31, 3247.98it/s]215474it [01:32, 3349.13it/s]212829it [01:31, 3386.88it/s]207677it [01:31, 3318.41it/s]215817it [01:32, 3289.56it/s]213201it [01:31, 3481.77it/s]208056it [01:31, 3453.13it/s]216191it [01:32, 3417.22it/s]213551it [01:31, 3359.98it/s]208404it [01:31, 3366.80it/s]216566it [01:32, 3512.67it/s]213911it [01:31, 3428.95it/s]208781it [01:31, 3481.27it/s]216919it [01:32, 3400.83it/s]214256it [01:32, 3326.89it/s]209132it [01:32, 3396.62it/s]217291it [01:32, 3492.79it/s]214621it [01:32, 3419.61it/s]209501it [01:32, 3479.66it/s]217642it [01:32, 3423.94it/s]209860it [01:32, 3510.27it/s]214975it [01:32, 3310.29it/s]196580it [01:31, 146.62it/s] 218020it [01:32, 3526.56it/s]215350it [01:32, 3434.46it/s]210213it [01:32, 3383.96it/s]196966it [01:32, 210.85it/s]218374it [01:32, 3438.26it/s]215720it [01:32, 3510.27it/s]210583it [01:32, 3472.35it/s]197336it [01:32, 293.94it/s]218765it [01:33, 3572.40it/s]216073it [01:32, 3367.18it/s]210932it [01:32, 3340.49it/s]197708it [01:32, 407.63it/s]219144it [01:33, 3635.62it/s]216446it [01:32, 3468.81it/s]211303it [01:32, 3444.69it/s]198094it [01:32, 563.86it/s]219509it [01:33, 3510.11it/s]216795it [01:32, 3373.29it/s]211650it [01:32, 3342.50it/s]198434it [01:32, 732.49it/s]219894it [01:33, 3606.71it/s]217179it [01:32, 3505.11it/s]212025it [01:32, 3456.89it/s]198807it [01:32, 970.59it/s]220257it [01:33, 3525.00it/s]212387it [01:33, 3494.22it/s]217532it [01:33, 3403.18it/s]199151it [01:32, 1211.65it/s]220647it [01:33, 3630.35it/s]217919it [01:33, 3535.87it/s]212738it [01:33, 3384.97it/s]199531it [01:32, 1537.80it/s]221012it [01:33, 3487.46it/s]218295it [01:33, 3599.06it/s]213106it [01:33, 3467.31it/s]199880it [01:32, 1777.18it/s]221384it [01:33, 3553.56it/s]213455it [01:33, 3264.32it/s]218657it [01:33, 3336.90it/s]200235it [01:33, 2085.84it/s]221741it [01:33, 3420.62it/s]213798it [01:33, 3310.25it/s]219024it [01:33, 3429.05it/s]200594it [01:33, 2384.88it/s]222106it [01:33, 3483.39it/s]214137it [01:33, 3222.78it/s]219371it [01:33, 3284.96it/s]200938it [01:33, 2482.79it/s]222456it [01:34, 3466.15it/s]219723it [01:33, 3350.10it/s]214462it [01:33, 3023.47it/s]201273it [01:33, 2682.00it/s]222804it [01:34, 3254.39it/s]214783it [01:33, 3073.90it/s]220061it [01:33, 3175.40it/s]201600it [01:33, 2697.75it/s]223133it [01:34, 3246.38it/s]220382it [01:33, 3115.29it/s]215094it [01:33, 2962.43it/s]201923it [01:33, 2785.64it/s]223460it [01:34, 3069.53it/s]220721it [01:33, 3190.02it/s]215418it [01:34, 3038.83it/s]202274it [01:33, 2974.70it/s]223809it [01:34, 3184.66it/s]215786it [01:34, 3219.56it/s]221043it [01:34, 3135.93it/s]202596it [01:33, 3027.11it/s]224185it [01:34, 3347.11it/s]216111it [01:34, 3204.81it/s]221423it [01:34, 3323.91it/s]202980it [01:33, 3252.21it/s]224523it [01:34, 3282.45it/s]216486it [01:34, 3361.49it/s]221758it [01:34, 3310.02it/s]203319it [01:34, 3210.07it/s]224890it [01:34, 3391.33it/s]222126it [01:34, 3415.57it/s]216824it [01:34, 3293.84it/s]203689it [01:34, 3346.94it/s]225232it [01:34, 3284.16it/s]222496it [01:34, 3498.28it/s]217206it [01:34, 3444.21it/s]204044it [01:34, 3404.30it/s]225600it [01:35, 3397.39it/s]222847it [01:34, 3380.17it/s]217552it [01:34, 3371.66it/s]204390it [01:34, 3312.84it/s]225942it [01:35, 3296.76it/s]223212it [01:34, 3456.57it/s]217939it [01:34, 3513.86it/s]204756it [01:34, 3408.46it/s]226305it [01:35, 3391.40it/s]218309it [01:34, 3566.60it/s]223559it [01:34, 3298.96it/s]205101it [01:34, 3322.33it/s]226655it [01:35, 3421.51it/s]218667it [01:34, 3487.47it/s]223932it [01:34, 3419.42it/s]205460it [01:34, 3399.20it/s]226999it [01:35, 3308.18it/s]219048it [01:35, 3580.60it/s]224277it [01:35, 3334.03it/s]205803it [01:34, 3322.75it/s]227362it [01:35, 3399.93it/s]219408it [01:35, 3498.33it/s]224640it [01:35, 3418.32it/s]206180it [01:34, 3451.43it/s]227704it [01:35, 3263.22it/s]219787it [01:35, 3581.64it/s]225009it [01:35, 3494.35it/s]206557it [01:34, 3542.66it/s]228067it [01:35, 3364.45it/s]220147it [01:35, 3500.49it/s]225360it [01:35, 3363.83it/s]206913it [01:35, 3387.04it/s]220542it [01:35, 3629.79it/s]228417it [01:35, 3291.43it/s]225714it [01:35, 3412.17it/s]207287it [01:35, 3485.45it/s]228785it [01:36, 3400.58it/s]220907it [01:35, 3524.31it/s]226057it [01:35, 3295.82it/s]207638it [01:35, 3370.12it/s]229150it [01:36, 3472.43it/s]221277it [01:35, 3569.31it/s]226417it [01:35, 3381.41it/s]207998it [01:35, 3434.91it/s]221674it [01:35, 3683.41it/s]229499it [01:36, 3353.24it/s]226757it [01:35, 3264.50it/s]208344it [01:35, 3367.49it/s]229865it [01:36, 3440.61it/s]222044it [01:35, 3530.55it/s]227109it [01:35, 3337.28it/s]208715it [01:35, 3464.77it/s]230211it [01:36, 3325.89it/s]222407it [01:35, 3557.71it/s]227465it [01:35, 3401.37it/s]209089it [01:35, 3542.80it/s]230572it [01:36, 3404.63it/s]222765it [01:36, 3424.90it/s]227807it [01:36, 3283.56it/s]209445it [01:35, 3380.72it/s]223129it [01:36, 3486.07it/s]230937it [01:36, 3248.27it/s]228157it [01:36, 3300.17it/s]209797it [01:35, 3418.15it/s]231300it [01:36, 3352.43it/s]223480it [01:36, 3280.54it/s]228489it [01:36, 3132.26it/s]210141it [01:36, 3222.70it/s]231644it [01:36, 3375.22it/s]223846it [01:36, 3386.15it/s]228843it [01:36, 3244.19it/s]210496it [01:36, 3312.04it/s]224202it [01:36, 3434.71it/s]231984it [01:36, 3229.97it/s]229198it [01:36, 3330.12it/s]210830it [01:36, 3201.28it/s]232328it [01:37, 3288.47it/s]224548it [01:36, 3202.65it/s]229533it [01:36, 3188.04it/s]211159it [01:36, 3224.54it/s]232659it [01:37, 3093.95it/s]224873it [01:36, 3199.65it/s]229855it [01:36, 3182.46it/s]211484it [01:36, 3221.72it/s]232984it [01:37, 3137.20it/s]225196it [01:36, 2997.09it/s]230175it [01:36, 2980.80it/s]211808it [01:36, 3020.79it/s]233325it [01:37, 3214.19it/s]225500it [01:36, 3007.00it/s]230506it [01:36, 3071.59it/s]212145it [01:36, 3118.09it/s]233649it [01:37, 3184.40it/s]225863it [01:37, 3181.21it/s]230850it [01:37, 3174.14it/s]212460it [01:36, 3099.04it/s]234017it [01:37, 3322.48it/s]226185it [01:37, 3142.82it/s]231171it [01:37, 3129.79it/s]212831it [01:36, 3273.52it/s]234351it [01:37, 3268.73it/s]226541it [01:37, 3261.49it/s]231537it [01:37, 3280.19it/s]213202it [01:36, 3398.15it/s]234716it [01:37, 3378.11it/s]226870it [01:37, 3201.02it/s]231867it [01:37, 3218.16it/s]213544it [01:37, 3279.54it/s]235083it [01:37, 3418.89it/s]227228it [01:37, 3308.65it/s]232232it [01:37, 3342.21it/s]213918it [01:37, 3410.02it/s]235426it [01:38, 3320.93it/s]232600it [01:37, 3439.28it/s]227578it [01:37, 3246.62it/s]214261it [01:37, 3316.84it/s]235791it [01:38, 3413.70it/s]227931it [01:37, 3324.36it/s]232946it [01:37, 3301.65it/s]214626it [01:37, 3407.72it/s]236134it [01:38, 3321.74it/s]228297it [01:37, 3421.05it/s]233318it [01:37, 3421.09it/s]214976it [01:37, 3276.52it/s]236499it [01:38, 3414.81it/s]228641it [01:37, 3325.21it/s]233662it [01:37, 3324.90it/s]215352it [01:37, 3412.41it/s]236842it [01:38, 3320.27it/s]229009it [01:38, 3426.46it/s]234027it [01:37, 3417.36it/s]215720it [01:37, 3487.76it/s]237208it [01:38, 3417.66it/s]229353it [01:38, 3319.34it/s]234371it [01:38, 3319.81it/s]216071it [01:37, 3353.38it/s]237551it [01:38, 3371.86it/s]229730it [01:38, 3447.92it/s]234733it [01:38, 3405.82it/s]216444it [01:37, 3460.76it/s]237890it [01:38, 3278.97it/s]235102it [01:38, 3487.48it/s]230098it [01:38, 3354.14it/s]216793it [01:38, 3372.06it/s]238255it [01:38, 3384.56it/s]230463it [01:38, 3437.91it/s]235452it [01:38, 3321.30it/s]217171it [01:38, 3487.74it/s]238595it [01:38, 3295.05it/s]230826it [01:38, 3492.73it/s]235814it [01:38, 3406.29it/s]217522it [01:38, 3357.62it/s]238957it [01:39, 3387.77it/s]231177it [01:38, 3407.25it/s]236157it [01:38, 3304.05it/s]217913it [01:38, 3514.10it/s]239321it [01:39, 3460.96it/s]231547it [01:38, 3490.40it/s]236520it [01:38, 3397.18it/s]218290it [01:38, 3586.65it/s]239669it [01:39, 3356.60it/s]231898it [01:38, 3392.21it/s]236862it [01:38, 3296.79it/s]218651it [01:38, 3479.06it/s]240033it [01:39, 3435.71it/s]232258it [01:38, 3450.63it/s]237226it [01:38, 3392.86it/s]219019it [01:38, 3535.28it/s]240378it [01:39, 3247.48it/s]237575it [01:39, 3418.74it/s]232618it [01:39, 3284.48it/s]219374it [01:38, 3442.39it/s]240732it [01:39, 3329.72it/s]232977it [01:39, 3369.25it/s]219740it [01:38, 3504.47it/s]237919it [01:39, 3203.38it/s]241068it [01:39, 3180.89it/s]233342it [01:39, 3448.92it/s]238270it [01:39, 3288.29it/s]220092it [01:38, 3334.98it/s]241426it [01:39, 3292.88it/s]233689it [01:39, 3249.64it/s]220453it [01:39, 3411.39it/s]238602it [01:39, 3126.46it/s]241790it [01:39, 3390.07it/s]234040it [01:39, 3322.66it/s]220809it [01:39, 3451.69it/s]238940it [01:39, 3195.51it/s]242132it [01:40, 3170.64it/s]239263it [01:39, 3146.35it/s]234376it [01:39, 2862.02it/s]221156it [01:39, 2817.69it/s]242454it [01:40, 3177.27it/s]239580it [01:39, 2923.45it/s]234683it [01:39, 2903.09it/s]221474it [01:39, 2907.41it/s]242775it [01:40, 2973.38it/s]239894it [01:39, 2981.61it/s]235016it [01:39, 3016.73it/s]221783it [01:39, 2955.25it/s]243138it [01:40, 3153.84it/s]240196it [01:39, 2967.39it/s]235326it [01:39, 3009.54it/s]222140it [01:39, 3123.13it/s]243505it [01:40, 3297.25it/s]240557it [01:40, 3149.56it/s]235680it [01:40, 3157.25it/s]222475it [01:39, 3185.59it/s]243839it [01:40, 3239.58it/s]240920it [01:40, 3287.02it/s]236001it [01:40, 3106.70it/s]222801it [01:39, 3187.25it/s]244204it [01:40, 3355.57it/s]241251it [01:40, 3217.37it/s]236366it [01:40, 3261.21it/s]223166it [01:39, 3319.54it/s]244543it [01:40, 3268.30it/s]241605it [01:40, 3310.42it/s]236731it [01:40, 3372.62it/s]223502it [01:40, 3244.16it/s]244907it [01:40, 3373.35it/s]241938it [01:40, 3184.25it/s]237071it [01:40, 3292.88it/s]223875it [01:40, 3382.82it/s]245247it [01:41, 3259.31it/s]242303it [01:40, 3317.17it/s]237424it [01:40, 3360.91it/s]224216it [01:40, 3318.29it/s]245614it [01:41, 3376.04it/s]242669it [01:40, 3414.64it/s]237762it [01:40, 3273.57it/s]224585it [01:40, 3425.80it/s]245981it [01:41, 3458.73it/s]243013it [01:40, 3304.10it/s]238128it [01:40, 3383.14it/s]224941it [01:40, 3462.56it/s]246329it [01:41, 3353.96it/s]243384it [01:40, 3418.43it/s]238490it [01:40, 3449.74it/s]225289it [01:40, 3367.64it/s]246693it [01:41, 3434.22it/s]243728it [01:40, 3320.45it/s]238837it [01:41, 3314.86it/s]225658it [01:40, 3459.77it/s]247038it [01:41, 3309.46it/s]244084it [01:41, 3386.98it/s]239198it [01:41, 3396.19it/s]226006it [01:40, 3345.32it/s]247404it [01:41, 3409.10it/s]244425it [01:41, 3281.46it/s]239540it [01:41, 3306.51it/s]226360it [01:40, 3400.62it/s]247747it [01:41, 3317.51it/s]244791it [01:41, 3388.89it/s]239904it [01:41, 3400.57it/s]226722it [01:40, 3464.29it/s]248113it [01:41, 3414.04it/s]245152it [01:41, 3451.02it/s]240246it [01:41, 3286.48it/s]227070it [01:41, 3337.57it/s]248479it [01:41, 3482.78it/s]245499it [01:41, 3332.96it/s]240608it [01:41, 3381.45it/s]227429it [01:41, 3409.31it/s]248829it [01:42, 3371.20it/s]245863it [01:41, 3421.19it/s]240975it [01:41, 3464.04it/s]227772it [01:41, 3290.41it/s]249186it [01:42, 3426.58it/s]246207it [01:41, 3292.65it/s]241323it [01:41, 3371.46it/s]228135it [01:41, 3386.73it/s]249530it [01:42, 3330.98it/s]246571it [01:41, 3391.44it/s]241675it [01:41, 3414.32it/s]228476it [01:41, 3291.75it/s]249893it [01:42, 3416.79it/s]242018it [01:41, 3316.19it/s]246912it [01:41, 3208.28it/s]228836it [01:41, 3373.04it/s]250250it [01:42, 3459.83it/s]242377it [01:42, 3393.31it/s]247266it [01:42, 3301.01it/s]229175it [01:41, 3377.04it/s]250597it [01:42, 3279.40it/s]247616it [01:42, 3357.00it/s]242718it [01:42, 3213.15it/s]229514it [01:41, 3234.23it/s]250942it [01:42, 3327.12it/s]243059it [01:42, 3268.31it/s]247954it [01:42, 3181.14it/s]229876it [01:41, 3343.52it/s]251277it [01:42, 3222.59it/s]243398it [01:42, 3292.56it/s]248281it [01:42, 3205.32it/s]230213it [01:42, 3169.81it/s]251602it [01:42, 3205.61it/s]243729it [01:42, 3095.22it/s]248604it [01:42, 3008.96it/s]230533it [01:42, 3132.65it/s]251924it [01:43, 3113.09it/s]244051it [01:42, 3127.51it/s]248926it [01:42, 3061.99it/s]230849it [01:42, 3120.93it/s]252237it [01:43, 2921.00it/s]249257it [01:42, 3131.61it/s]244378it [01:42, 3032.18it/s]231163it [01:42, 3042.63it/s]252591it [01:43, 3090.16it/s]249573it [01:42, 3098.20it/s]244728it [01:42, 3156.89it/s]231527it [01:42, 3212.04it/s]252904it [01:43, 3054.60it/s]249937it [01:42, 3254.19it/s]245093it [01:42, 3296.40it/s]231850it [01:42, 3165.36it/s]253271it [01:43, 3228.21it/s]250265it [01:42, 3190.32it/s]245425it [01:43, 3234.23it/s]232207it [01:42, 3280.48it/s]253617it [01:43, 3200.28it/s]250628it [01:43, 3317.33it/s]245789it [01:43, 3348.88it/s]232574it [01:42, 3394.24it/s]250998it [01:43, 3427.97it/s]232915it [01:42, 3308.48it/s]246126it [01:43, 3239.09it/s]251343it [01:43, 3319.76it/s]233292it [01:43, 3440.26it/s]246486it [01:43, 3341.50it/s]251706it [01:43, 3407.02it/s]246853it [01:43, 3435.98it/s]233638it [01:43, 3255.28it/s]252049it [01:43, 3302.82it/s]247199it [01:43, 3329.45it/s]234008it [01:43, 3379.68it/s]252414it [01:43, 3402.19it/s]247546it [01:43, 3367.85it/s]234349it [01:43, 3315.71it/s]252769it [01:43, 3443.06it/s]247885it [01:43, 3293.84it/s]234707it [01:43, 3390.12it/s]253115it [01:43, 3327.67it/s]248251it [01:43, 3397.75it/s]235075it [01:43, 3471.79it/s]253476it [01:43, 3404.68it/s]248592it [01:43, 3302.83it/s]235424it [01:43, 3359.21it/s]248965it [01:44, 3424.58it/s]235790it [01:43, 3444.12it/s]249326it [01:44, 3476.18it/s]236136it [01:43, 3324.59it/s]249675it [01:44, 3360.92it/s]236503it [01:43, 3422.27it/s]250044it [01:44, 3454.55it/s]236847it [01:44, 3321.42it/s]250391it [01:44, 3350.73it/s]237213it [01:44, 3418.23it/s]250759it [01:44, 3445.22it/s]237569it [01:44, 3459.22it/s]251105it [01:44, 3337.19it/s]237917it [01:44, 3343.24it/s]251469it [01:44, 3421.73it/s]238278it [01:44, 3418.04it/s]251821it [01:44, 3448.94it/s]238622it [01:44, 3212.83it/s]252167it [01:45, 3259.79it/s]238960it [01:44, 3258.67it/s]252519it [01:45, 3332.88it/s]239313it [01:44, 3335.55it/s]252855it [01:45, 3199.87it/s]239649it [01:44, 3187.99it/s]253182it [01:45, 3219.14it/s]239971it [01:45, 3190.11it/s]253506it [01:45, 3185.73it/s]240292it [01:45, 3027.52it/s]240602it [01:45, 3045.69it/s]240962it [01:45, 3201.77it/s]241285it [01:45, 3172.60it/s]241652it [01:45, 3316.05it/s]241986it [01:45, 3237.68it/s]242351it [01:45, 3356.47it/s]242696it [01:45, 3273.35it/s]243065it [01:45, 3391.56it/s]243427it [01:46, 3457.13it/s]243774it [01:46, 3349.10it/s]244144it [01:46, 3449.52it/s]244491it [01:46, 3337.56it/s]244850it [01:46, 3407.84it/s]245216it [01:46, 3306.84it/s]245587it [01:46, 3419.70it/s]245954it [01:46, 3491.24it/s]246305it [01:46, 3348.69it/s]246674it [01:47, 3445.57it/s]247021it [01:47, 3345.55it/s]247384it [01:47, 3424.76it/s]247736it [01:47, 3323.52it/s]248093it [01:47, 3391.45it/s]248445it [01:47, 3428.03it/s]248789it [01:47, 3194.76it/s]249154it [01:47, 3320.81it/s]249490it [01:47, 3142.90it/s]249809it [01:48, 3082.16it/s]250120it [01:48, 3028.36it/s]250425it [01:48, 2810.08it/s]250790it [01:48, 3036.21it/s]251099it [01:48, 3037.39it/s]251463it [01:48, 3206.84it/s]251828it [01:48, 3334.10it/s]252165it [01:48, 3270.21it/s]252531it [01:48, 3382.45it/s]252872it [01:48, 3267.15it/s]253245it [01:49, 3399.59it/s]253616it [01:49, 3313.76it/s]253939it [01:51, 133.06it/s] 254258it [01:51, 184.07it/s]254535it [01:51, 244.27it/s]254902it [01:52, 354.51it/s]255261it [01:52, 496.71it/s]255574it [01:52, 650.12it/s]255941it [01:52, 884.34it/s]256267it [01:52, 1113.98it/s]256638it [01:52, 1435.95it/s]257012it [01:52, 1782.69it/s]257359it [01:52, 2034.14it/s]253818it [01:52, 135.33it/s] 257717it [01:52, 2339.66it/s]254169it [01:52, 190.18it/s]258059it [01:52, 2522.02it/s]254524it [01:52, 265.44it/s]258430it [01:53, 2800.36it/s]254887it [01:52, 370.69it/s]258774it [01:53, 2884.06it/s]255250it [01:52, 510.09it/s]259146it [01:53, 3098.83it/s]255574it [01:52, 664.69it/s]259503it [01:53, 3224.34it/s]255938it [01:52, 889.91it/s]259852it [01:53, 3193.16it/s]256270it [01:53, 1118.22it/s]260227it [01:53, 3346.50it/s]256627it [01:53, 1415.95it/s]260576it [01:53, 3268.45it/s]256995it [01:53, 1750.59it/s]260951it [01:53, 3403.15it/s]257339it [01:53, 2001.40it/s]261300it [01:53, 3323.56it/s]257698it [01:53, 2311.99it/s]261643it [01:54, 3352.01it/s]258039it [01:53, 2441.20it/s]262004it [01:54, 3425.32it/s]258390it [01:53, 2686.39it/s]262350it [01:54, 3223.72it/s]258724it [01:53, 2742.72it/s]262716it [01:54, 3338.68it/s]259064it [01:53, 2908.08it/s]263054it [01:54, 3131.42it/s]259389it [01:54, 2963.22it/s]253826it [01:54, 124.66it/s] 263375it [01:54, 3151.11it/s]254144it [01:54, 173.13it/s]263694it [01:54, 3132.15it/s]259710it [01:54, 2529.96it/s]254471it [01:54, 241.76it/s]264010it [01:54, 3063.53it/s]260068it [01:54, 2787.25it/s]254772it [01:54, 326.70it/s]264384it [01:54, 3255.38it/s]260404it [01:54, 2872.12it/s]255139it [01:54, 467.08it/s]264712it [01:54, 3197.56it/s]260768it [01:54, 3076.00it/s]255447it [01:54, 614.06it/s]265083it [01:55, 3344.94it/s]261142it [01:54, 3257.45it/s]255819it [01:54, 845.96it/s]261480it [01:54, 3226.30it/s]265446it [01:55, 3271.72it/s]256171it [01:54, 1103.75it/s]261853it [01:54, 3368.53it/s]265811it [01:55, 3378.01it/s]256505it [01:54, 1360.82it/s]266181it [01:55, 3469.92it/s]262197it [01:54, 3296.15it/s]256877it [01:54, 1705.08it/s]262569it [01:55, 3414.76it/s]266530it [01:55, 3333.83it/s]257217it [01:55, 1962.55it/s]266903it [01:55, 3446.17it/s]262924it [01:55, 3333.50it/s]257582it [01:55, 2290.71it/s]263285it [01:55, 3409.94it/s]267250it [01:55, 3346.29it/s]257923it [01:55, 2458.09it/s]263656it [01:55, 3494.89it/s]267618it [01:55, 3441.38it/s]258292it [01:55, 2743.43it/s]264008it [01:55, 3388.29it/s]267966it [01:55, 3322.84it/s]258658it [01:55, 2970.86it/s]264384it [01:55, 3493.69it/s]268339it [01:56, 3438.27it/s]259006it [01:55, 3020.72it/s]268709it [01:56, 3513.05it/s]264736it [01:55, 3390.05it/s]259374it [01:55, 3196.54it/s]265106it [01:55, 3479.02it/s]269062it [01:56, 3386.63it/s]259721it [01:55, 3149.87it/s]269433it [01:56, 3479.24it/s]265456it [01:55, 3364.80it/s]260093it [01:55, 3305.41it/s]265832it [01:55, 3475.44it/s]269783it [01:56, 3349.21it/s]260439it [01:56, 3245.46it/s]266204it [01:56, 3543.77it/s]270158it [01:56, 3461.94it/s]260812it [01:56, 3380.34it/s]266560it [01:56, 3436.36it/s]270507it [01:56, 3346.86it/s]261182it [01:56, 3471.04it/s]266929it [01:56, 3509.44it/s]270876it [01:56, 3444.54it/s]261536it [01:56, 3330.74it/s]267282it [01:56, 3407.73it/s]271242it [01:56, 3505.49it/s]261904it [01:56, 3427.90it/s]267628it [01:56, 3421.07it/s]271595it [01:56, 3234.43it/s]262252it [01:56, 3217.62it/s]267972it [01:56, 3246.04it/s]271951it [01:57, 3323.88it/s]262608it [01:56, 3312.59it/s]268331it [01:56, 3341.12it/s]272288it [01:57, 3142.25it/s]262944it [01:56, 3178.63it/s]268693it [01:56, 3420.14it/s]272642it [01:57, 3250.29it/s]263278it [01:56, 3223.09it/s]269037it [01:56, 3207.06it/s]272971it [01:57, 3224.64it/s]263605it [01:56, 3234.03it/s]269362it [01:57, 3213.41it/s]273297it [01:57, 3013.18it/s]263931it [01:57, 2761.24it/s]269686it [01:57, 3031.07it/s]273629it [01:57, 3096.39it/s]264235it [01:57, 2833.68it/s]269993it [01:57, 3034.20it/s]273943it [01:57, 3075.27it/s]264592it [01:57, 3032.45it/s]270358it [01:57, 3206.77it/s]274308it [01:57, 3237.59it/s]264905it [01:57, 3020.81it/s]270682it [01:57, 3183.03it/s]274675it [01:57, 3361.68it/s]265279it [01:57, 3222.91it/s]271045it [01:57, 3311.99it/s]275014it [01:58, 3279.44it/s]265607it [01:57, 3204.49it/s]271378it [01:57, 3263.72it/s]275375it [01:58, 3372.73it/s]265980it [01:57, 3355.86it/s]271742it [01:57, 3372.17it/s]275714it [01:58, 3281.78it/s]266319it [01:57, 3297.68it/s]272100it [01:57, 3432.84it/s]276081it [01:58, 3393.14it/s]266683it [01:57, 3395.33it/s]272445it [01:57, 3318.60it/s]276422it [01:58, 3298.45it/s]253950it [01:57, 133.49it/s] 267056it [01:58, 3489.28it/s]272812it [01:58, 3419.84it/s]276786it [01:58, 3390.69it/s]254314it [01:57, 189.51it/s]267407it [01:58, 3396.78it/s]273156it [01:58, 3321.18it/s]277147it [01:58, 3451.89it/s]254617it [01:57, 252.92it/s]267781it [01:58, 3494.57it/s]273521it [01:58, 3414.94it/s]277494it [01:58, 3335.98it/s]254971it [01:58, 354.49it/s]268132it [01:58, 3401.66it/s]273864it [01:58, 3334.32it/s]277863it [01:58, 3437.43it/s]255334it [01:58, 492.98it/s]268494it [01:58, 3464.65it/s]274219it [01:58, 3395.11it/s]278209it [01:59, 3326.80it/s]255656it [01:58, 646.36it/s]268842it [01:58, 3365.68it/s]274583it [01:58, 3466.37it/s]278579it [01:59, 3433.02it/s]256020it [01:58, 870.94it/s]269212it [01:58, 3461.15it/s]274931it [01:58, 3349.70it/s]278924it [01:59, 3325.19it/s]256351it [01:58, 1096.52it/s]269582it [01:58, 3530.56it/s]275299it [01:58, 3444.00it/s]279279it [01:59, 3389.08it/s]256717it [01:58, 1405.17it/s]269937it [01:58, 3429.71it/s]275645it [01:58, 3345.15it/s]279644it [01:59, 3463.54it/s]257052it [01:58, 1667.82it/s]270306it [01:59, 3503.24it/s]276011it [01:59, 3435.43it/s]257412it [01:58, 1998.79it/s]279992it [01:59, 3343.17it/s]270658it [01:59, 3364.89it/s]276365it [01:59, 3327.32it/s]257763it [01:58, 2295.57it/s]280353it [01:59, 3419.60it/s]271024it [01:59, 3447.23it/s]276715it [01:59, 3374.64it/s]258104it [01:58, 2479.42it/s]280697it [01:59, 3311.18it/s]271371it [01:59, 3316.34it/s]277080it [01:59, 3453.81it/s]258459it [01:59, 2728.25it/s]281041it [01:59, 3345.68it/s]271728it [01:59, 3387.57it/s]277427it [01:59, 3252.81it/s]281393it [01:59, 3395.62it/s]258797it [01:59, 2740.77it/s]272089it [01:59, 3449.26it/s]277786it [01:59, 3345.47it/s]259157it [01:59, 2957.45it/s]281734it [02:00, 3206.23it/s]272436it [01:59, 3221.32it/s]278124it [01:59, 3240.50it/s]259517it [01:59, 3127.66it/s]282094it [02:00, 3315.77it/s]272787it [01:59, 3299.69it/s]278465it [01:59, 3279.85it/s]282429it [02:00, 3104.07it/s]259857it [01:59, 2964.59it/s]273121it [01:59, 3070.70it/s]260174it [01:59, 3018.03it/s]282744it [02:00, 3024.31it/s]278795it [01:59, 2733.56it/s]273438it [01:59, 3096.17it/s]283054it [02:00, 3043.49it/s]260491it [01:59, 2895.17it/s]279085it [02:00, 2684.33it/s]273752it [02:00, 3097.16it/s]283361it [02:00, 2964.83it/s]260846it [01:59, 3071.46it/s]279429it [02:00, 2880.50it/s]274065it [02:00, 2989.39it/s]283711it [02:00, 3116.04it/s]261215it [01:59, 3242.78it/s]279728it [02:00, 2877.32it/s]274432it [02:00, 3179.34it/s]284025it [02:00, 3070.35it/s]261548it [02:00, 3170.95it/s]280090it [02:00, 3082.17it/s]274753it [02:00, 3153.12it/s]284388it [02:00, 3231.02it/s]261918it [02:00, 3319.98it/s]280456it [02:00, 3245.77it/s]275117it [02:00, 3292.18it/s]284745it [02:01, 3328.52it/s]262255it [02:00, 3241.47it/s]280787it [02:00, 3170.14it/s]275489it [02:00, 3415.52it/s]285080it [02:01, 3252.30it/s]262624it [02:00, 3367.37it/s]281152it [02:00, 3306.19it/s]275833it [02:00, 3312.48it/s]285449it [02:01, 3377.80it/s]262964it [02:00, 3261.47it/s]281487it [02:00, 3231.37it/s]276194it [02:00, 3398.29it/s]285789it [02:01, 3289.45it/s]263331it [02:00, 3375.63it/s]281852it [02:00, 3350.04it/s]276536it [02:00, 3313.62it/s]286158it [02:01, 3404.50it/s]263696it [02:00, 3453.53it/s]282216it [02:00, 3432.32it/s]276907it [02:01, 3426.21it/s]286500it [02:01, 3298.53it/s]264044it [02:00, 3340.39it/s]282562it [02:01, 3315.28it/s]277251it [02:01, 3340.53it/s]286867it [02:01, 3405.23it/s]264407it [02:00, 3422.89it/s]282926it [02:01, 3408.22it/s]277620it [02:01, 3439.53it/s]287112it [02:01, 2358.95it/s]
2022-08-10 15:33:21 | INFO | root | success load 287112 data
2022-08-10 15:33:21 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-08-10 15:33:21 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-08-10 15:33:21 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-08-10 15:33:21 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-08-10 15:33:21 | INFO | transformer.tokenization_utils | loading file None
2022-08-10 15:33:21 | INFO | transformer.tokenization_utils | loading file None
2022-08-10 15:33:21 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
264752it [02:00, 3312.97it/s]283269it [02:01, 3260.61it/s]277978it [02:01, 3478.09it/s]265122it [02:01, 3422.33it/s]283633it [02:01, 3365.91it/s]278327it [02:01, 3367.37it/s]265466it [02:01, 3288.94it/s]283972it [02:01, 3258.34it/s]278699it [02:01, 3467.07it/s]265839it [02:01, 3412.12it/s]284339it [02:01, 3373.74it/s]279047it [02:01, 3371.54it/s]266208it [02:01, 3490.91it/s]284699it [02:01, 3438.38it/s]279415it [02:01, 3460.00it/s]266559it [02:01, 3373.61it/s]285045it [02:01, 3313.93it/s]279763it [02:01, 3339.08it/s]266915it [02:01, 3427.01it/s]285396it [02:01, 3368.28it/s]280130it [02:01, 3431.69it/s]267260it [02:01, 3334.59it/s]285735it [02:02, 3274.36it/s]280496it [02:02, 3497.21it/s]267628it [02:01, 3431.91it/s]286097it [02:02, 3373.06it/s]280847it [02:02, 3394.14it/s]267973it [02:01, 3252.19it/s]281206it [02:02, 3449.39it/s]286445it [02:02, 3209.66it/s]268317it [02:02, 3302.52it/s]286800it [02:02, 3303.76it/s]281553it [02:02, 3225.92it/s]287112it [02:02, 2344.88it/s]
268675it [02:02, 3380.32it/s]281910it [02:02, 3320.77it/s]269015it [02:02, 3209.20it/s]282246it [02:02, 3243.51it/s]269364it [02:02, 3278.69it/s]282584it [02:02, 3282.29it/s]282915it [02:02, 3261.88it/s]269695it [02:02, 3068.68it/s]270016it [02:02, 3107.52it/s]283243it [02:02, 2838.98it/s]270335it [02:02, 3129.96it/s]283579it [02:03, 2975.37it/s]270651it [02:02, 3083.90it/s]283926it [02:03, 3003.26it/s]271000it [02:02, 3198.69it/s]284295it [02:03, 3190.42it/s]271324it [02:03, 3175.99it/s]284661it [02:03, 3321.65it/s]271694it [02:03, 3327.02it/s]284999it [02:03, 3257.97it/s]272065it [02:03, 3438.99it/s]285360it [02:03, 3356.91it/s]272410it [02:03, 3320.76it/s]285699it [02:03, 3282.91it/s]272778it [02:03, 3422.58it/s]286068it [02:03, 3397.85it/s]273122it [02:03, 3331.66it/s]286436it [02:03, 3479.42it/s]273490it [02:03, 3431.57it/s]286786it [02:04, 3378.69it/s]273844it [02:03, 3319.67it/s]287112it [02:04, 2313.48it/s]
274215it [02:03, 3429.29it/s]274586it [02:03, 3509.53it/s]274939it [02:04, 3402.28it/s]275306it [02:04, 3477.45it/s]275656it [02:04, 3386.78it/s]276023it [02:04, 3465.97it/s]276371it [02:04, 3343.23it/s]276742it [02:04, 3445.98it/s]277118it [02:04, 3534.93it/s]277473it [02:04, 3419.72it/s]277830it [02:04, 3462.51it/s]278178it [02:05, 3266.89it/s]278540it [02:05, 3364.78it/s]278884it [02:05, 3247.16it/s]279234it [02:05, 3307.66it/s]279567it [02:05, 3299.73it/s]279899it [02:05, 3114.60it/s]280218it [02:05, 3133.94it/s]280564it [02:05, 3113.53it/s]280937it [02:05, 3286.25it/s]281305it [02:05, 3398.10it/s]281647it [02:06, 3238.75it/s]281981it [02:06, 3265.57it/s]282310it [02:06, 3221.58it/s]282676it [02:06, 3346.30it/s]283044it [02:06, 3443.11it/s]283390it [02:06, 3321.83it/s]283759it [02:06, 3427.75it/s]284104it [02:06, 3333.19it/s]284471it [02:06, 3429.85it/s]284816it [02:07, 3303.58it/s]285184it [02:07, 3405.84it/s]285553it [02:07, 3486.12it/s]285904it [02:07, 3352.01it/s]286274it [02:07, 3450.63it/s]286621it [02:07, 3363.00it/s]286997it [02:07, 3476.23it/s]287112it [02:07, 2248.69it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-08-10 15:37:12 | INFO | train_inner | epoch 032:     66 / 1122 loss=nan, nll_loss=2.96, mask_ins=1.429, word_ins_ml=4.54, word_reposition=1.652, kpe=nan, ppl=nan, wps=3775.3, ups=0.19, wpb=20327.9, bsz=253.8, num_updates=34800, lr=0.000189525, gnorm=2.508, clip=0, loss_scale=542, train_wall=288, wall=0
2022-08-10 15:42:46 | INFO | train_inner | epoch 032:    166 / 1122 loss=9.118, nll_loss=3.255, mask_ins=1.633, word_ins_ml=4.802, word_reposition=1.895, kpe=0.787, ppl=555.55, wps=6127.8, ups=0.3, wpb=20451.1, bsz=256, num_updates=34900, lr=0.000189253, gnorm=2.112, clip=0, loss_scale=512, train_wall=290, wall=0
2022-08-10 15:48:21 | INFO | train_inner | epoch 032:    266 / 1122 loss=9.046, nll_loss=3.24, mask_ins=1.615, word_ins_ml=4.789, word_reposition=1.855, kpe=0.787, ppl=528.48, wps=6131.3, ups=0.3, wpb=20527.4, bsz=256, num_updates=35000, lr=0.000188982, gnorm=2.06, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-10 15:53:55 | INFO | train_inner | epoch 032:    366 / 1122 loss=9.094, nll_loss=3.273, mask_ins=1.622, word_ins_ml=4.818, word_reposition=1.864, kpe=0.79, ppl=546.49, wps=6187.6, ups=0.3, wpb=20690.2, bsz=256, num_updates=35100, lr=0.000188713, gnorm=2.112, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-10 15:59:29 | INFO | train_inner | epoch 032:    466 / 1122 loss=9.069, nll_loss=3.262, mask_ins=1.615, word_ins_ml=4.807, word_reposition=1.86, kpe=0.786, ppl=536.97, wps=6209.1, ups=0.3, wpb=20715.2, bsz=256, num_updates=35200, lr=0.000188445, gnorm=2.112, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-10 16:05:16 | INFO | train_inner | epoch 032:    566 / 1122 loss=nan, nll_loss=3.247, mask_ins=1.615, word_ins_ml=4.795, word_reposition=1.865, kpe=nan, ppl=nan, wps=5927.7, ups=0.29, wpb=20554.8, bsz=256, num_updates=35300, lr=0.000188177, gnorm=2.194, clip=0, loss_scale=794, train_wall=303, wall=0
2022-08-10 16:10:50 | INFO | train_inner | epoch 032:    666 / 1122 loss=9.025, nll_loss=3.215, mask_ins=1.614, word_ins_ml=4.768, word_reposition=1.847, kpe=0.796, ppl=520.9, wps=6104.2, ups=0.3, wpb=20427.5, bsz=256, num_updates=35400, lr=0.000187912, gnorm=2.112, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 16:16:32 | INFO | train_inner | epoch 032:    766 / 1122 loss=9.016, nll_loss=3.225, mask_ins=1.603, word_ins_ml=4.776, word_reposition=1.844, kpe=0.793, ppl=517.79, wps=5965.2, ups=0.29, wpb=20371.9, bsz=256, num_updates=35500, lr=0.000187647, gnorm=2.119, clip=0, loss_scale=1024, train_wall=296, wall=0
2022-08-10 16:22:04 | INFO | train_inner | epoch 032:    866 / 1122 loss=9.031, nll_loss=3.236, mask_ins=1.607, word_ins_ml=4.786, word_reposition=1.844, kpe=0.794, ppl=523, wps=6138.1, ups=0.3, wpb=20420.8, bsz=256, num_updates=35600, lr=0.000187383, gnorm=2.144, clip=0, loss_scale=1024, train_wall=290, wall=0
2022-08-10 16:27:39 | INFO | train_inner | epoch 032:    966 / 1122 loss=8.989, nll_loss=3.21, mask_ins=1.608, word_ins_ml=4.762, word_reposition=1.825, kpe=0.794, ppl=508.06, wps=6166.8, ups=0.3, wpb=20635.7, bsz=256, num_updates=35700, lr=0.00018712, gnorm=2.056, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 16:32:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-10 16:33:16 | INFO | train_inner | epoch 032:   1067 / 1122 loss=nan, nll_loss=3.221, mask_ins=1.606, word_ins_ml=4.772, word_reposition=1.839, kpe=nan, ppl=nan, wps=6104.5, ups=0.3, wpb=20563.4, bsz=256, num_updates=35800, lr=0.000186859, gnorm=2.127, clip=0, loss_scale=1288, train_wall=293, wall=0
2022-08-10 16:36:16 | INFO | train | epoch 032 | loss nan | nll_loss 3.245 | mask_ins 1.623 | word_ins_ml 4.794 | word_reposition 1.868 | kpe nan | ppl nan | wps 5779.8 | ups 0.28 | wpb 20520.9 | bsz 255.8 | num_updates 35855 | lr 0.000186715 | gnorm 2.165 | clip 0 | loss_scale 815 | train_wall 3286 | wall 0
2022-08-10 16:37:44 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 15.712 | nll_loss 7.442 | mask_ins 2.401 | word_ins_ml 8.662 | word_reposition 3.183 | kpe 1.466 | ppl 53694.5 | wps 11303.2 | wpb 2367.6 | bsz 32 | num_updates 35855 | best_loss 10.679
2022-08-10 16:37:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 32 @ 35855 updates, score 15.712) (writing took 11.74929635412991 seconds)
2022-08-10 16:39:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-10 16:40:29 | INFO | train_inner | epoch 033:     46 / 1122 loss=9.002, nll_loss=3.213, mask_ins=1.604, word_ins_ml=4.766, word_reposition=1.841, kpe=0.791, ppl=512.57, wps=4721.3, ups=0.23, wpb=20434.1, bsz=253.8, num_updates=35900, lr=0.000186598, gnorm=2.195, clip=0, loss_scale=943, train_wall=291, wall=0
2022-08-10 16:46:03 | INFO | train_inner | epoch 033:    146 / 1122 loss=8.902, nll_loss=3.162, mask_ins=1.591, word_ins_ml=4.721, word_reposition=1.812, kpe=0.778, ppl=478.23, wps=6191.4, ups=0.3, wpb=20675, bsz=256, num_updates=36000, lr=0.000186339, gnorm=2.171, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-10 16:51:36 | INFO | train_inner | epoch 033:    246 / 1122 loss=8.918, nll_loss=3.157, mask_ins=1.596, word_ins_ml=4.716, word_reposition=1.827, kpe=0.779, ppl=483.82, wps=6157, ups=0.3, wpb=20537, bsz=256, num_updates=36100, lr=0.000186081, gnorm=2.051, clip=0, loss_scale=512, train_wall=290, wall=0
2022-08-10 16:57:10 | INFO | train_inner | epoch 033:    346 / 1122 loss=8.936, nll_loss=3.16, mask_ins=1.592, word_ins_ml=4.719, word_reposition=1.845, kpe=0.78, ppl=489.67, wps=6160.5, ups=0.3, wpb=20584, bsz=256, num_updates=36200, lr=0.000185824, gnorm=2.118, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-10 17:02:45 | INFO | train_inner | epoch 033:    446 / 1122 loss=8.912, nll_loss=3.17, mask_ins=1.594, word_ins_ml=4.728, word_reposition=1.808, kpe=0.782, ppl=481.7, wps=6115.1, ups=0.3, wpb=20438.5, bsz=256, num_updates=36300, lr=0.000185567, gnorm=2.129, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-10 17:08:16 | INFO | train_inner | epoch 033:    546 / 1122 loss=8.899, nll_loss=3.145, mask_ins=1.594, word_ins_ml=4.706, word_reposition=1.816, kpe=0.783, ppl=477.31, wps=6184.8, ups=0.3, wpb=20519.7, bsz=256, num_updates=36400, lr=0.000185312, gnorm=2.102, clip=0, loss_scale=532, train_wall=289, wall=0
2022-08-10 17:14:19 | INFO | train_inner | epoch 033:    646 / 1122 loss=nan, nll_loss=3.126, mask_ins=1.597, word_ins_ml=4.689, word_reposition=1.837, kpe=nan, ppl=nan, wps=5655.7, ups=0.28, wpb=20487.5, bsz=256, num_updates=36500, lr=0.000185058, gnorm=2.116, clip=0, loss_scale=1024, train_wall=318, wall=0
2022-08-10 17:19:53 | INFO | train_inner | epoch 033:    746 / 1122 loss=8.933, nll_loss=3.169, mask_ins=1.593, word_ins_ml=4.727, word_reposition=1.828, kpe=0.786, ppl=488.91, wps=6138.9, ups=0.3, wpb=20502.9, bsz=256, num_updates=36600, lr=0.000184805, gnorm=2.141, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 17:23:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-10 17:25:31 | INFO | train_inner | epoch 033:    847 / 1122 loss=8.92, nll_loss=3.164, mask_ins=1.595, word_ins_ml=4.723, word_reposition=1.813, kpe=0.789, ppl=484.43, wps=6059.8, ups=0.3, wpb=20487.2, bsz=256, num_updates=36700, lr=0.000184553, gnorm=2.176, clip=0, loss_scale=877, train_wall=294, wall=0
2022-08-10 17:31:04 | INFO | train_inner | epoch 033:    947 / 1122 loss=8.919, nll_loss=3.168, mask_ins=1.582, word_ins_ml=4.726, word_reposition=1.824, kpe=0.786, ppl=483.95, wps=6150.4, ups=0.3, wpb=20521.2, bsz=256, num_updates=36800, lr=0.000184302, gnorm=2.166, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-10 17:36:37 | INFO | train_inner | epoch 033:   1047 / 1122 loss=nan, nll_loss=3.153, mask_ins=1.594, word_ins_ml=4.713, word_reposition=1.798, kpe=nan, ppl=nan, wps=6206.7, ups=0.3, wpb=20624.2, bsz=256, num_updates=36900, lr=0.000184053, gnorm=2.108, clip=0, loss_scale=512, train_wall=289, wall=0
2022-08-10 17:40:45 | INFO | train | epoch 033 | loss nan | nll_loss 3.158 | mask_ins 1.593 | word_ins_ml 4.717 | word_reposition 1.823 | kpe nan | ppl nan | wps 5940.2 | ups 0.29 | wpb 20522 | bsz 255.8 | num_updates 36975 | lr 0.000183866 | gnorm 2.133 | clip 0 | loss_scale 652 | train_wall 3286 | wall 0
2022-08-10 17:42:13 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 15.22 | nll_loss 7.382 | mask_ins 2.206 | word_ins_ml 8.601 | word_reposition 3.008 | kpe 1.405 | ppl 38174.8 | wps 11288.9 | wpb 2367.6 | bsz 32 | num_updates 36975 | best_loss 10.679
2022-08-10 17:42:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 33 @ 36975 updates, score 15.22) (writing took 3.6569336149841547 seconds)
2022-08-10 17:43:40 | INFO | train_inner | epoch 034:     25 / 1122 loss=8.903, nll_loss=3.139, mask_ins=1.593, word_ins_ml=4.701, word_reposition=1.825, kpe=0.784, ppl=478.87, wps=4801.6, ups=0.24, wpb=20344.3, bsz=253.8, num_updates=37000, lr=0.000183804, gnorm=2.257, clip=0, loss_scale=512, train_wall=290, wall=0
2022-08-10 17:49:15 | INFO | train_inner | epoch 034:    125 / 1122 loss=8.793, nll_loss=3.079, mask_ins=1.576, word_ins_ml=4.648, word_reposition=1.8, kpe=0.77, ppl=443.56, wps=6120.5, ups=0.3, wpb=20452, bsz=256, num_updates=37100, lr=0.000183556, gnorm=2.161, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-10 17:54:49 | INFO | train_inner | epoch 034:    225 / 1122 loss=8.817, nll_loss=3.115, mask_ins=1.576, word_ins_ml=4.68, word_reposition=1.79, kpe=0.771, ppl=450.87, wps=6123.2, ups=0.3, wpb=20449.2, bsz=256, num_updates=37200, lr=0.000183309, gnorm=2.103, clip=0, loss_scale=599, train_wall=290, wall=0
2022-08-10 18:00:22 | INFO | train_inner | epoch 034:    325 / 1122 loss=8.783, nll_loss=3.074, mask_ins=1.579, word_ins_ml=4.643, word_reposition=1.786, kpe=0.774, ppl=440.44, wps=6158.9, ups=0.3, wpb=20521.7, bsz=256, num_updates=37300, lr=0.000183063, gnorm=2.151, clip=0, loss_scale=1024, train_wall=290, wall=0
2022-08-10 18:05:55 | INFO | train_inner | epoch 034:    425 / 1122 loss=nan, nll_loss=3.096, mask_ins=1.573, word_ins_ml=4.663, word_reposition=1.776, kpe=nan, ppl=nan, wps=6153.9, ups=0.3, wpb=20505.5, bsz=256, num_updates=37400, lr=0.000182818, gnorm=2.157, clip=0, loss_scale=1024, train_wall=290, wall=0
2022-08-10 18:11:31 | INFO | train_inner | epoch 034:    525 / 1122 loss=8.838, nll_loss=3.115, mask_ins=1.581, word_ins_ml=4.68, word_reposition=1.802, kpe=0.775, ppl=457.53, wps=6106.3, ups=0.3, wpb=20523.5, bsz=256, num_updates=37500, lr=0.000182574, gnorm=2.14, clip=0, loss_scale=1024, train_wall=293, wall=0
2022-08-10 18:17:08 | INFO | train_inner | epoch 034:    625 / 1122 loss=8.83, nll_loss=3.108, mask_ins=1.585, word_ins_ml=4.674, word_reposition=1.793, kpe=0.778, ppl=455, wps=6117.8, ups=0.3, wpb=20587.3, bsz=256, num_updates=37600, lr=0.000182331, gnorm=2.177, clip=0, loss_scale=1024, train_wall=293, wall=0
2022-08-10 18:23:09 | INFO | train_inner | epoch 034:    725 / 1122 loss=8.783, nll_loss=3.09, mask_ins=1.581, word_ins_ml=4.657, word_reposition=1.766, kpe=0.779, ppl=440.53, wps=5721.7, ups=0.28, wpb=20695.3, bsz=256, num_updates=37700, lr=0.000182089, gnorm=2.108, clip=0, loss_scale=1075, train_wall=317, wall=0
2022-08-10 18:26:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-10 18:28:47 | INFO | train_inner | epoch 034:    826 / 1122 loss=8.787, nll_loss=3.064, mask_ins=1.582, word_ins_ml=4.634, word_reposition=1.794, kpe=0.777, ppl=441.71, wps=6088.5, ups=0.3, wpb=20563.1, bsz=256, num_updates=37800, lr=0.000181848, gnorm=2.314, clip=0, loss_scale=1673, train_wall=295, wall=0
2022-08-10 18:34:22 | INFO | train_inner | epoch 034:    926 / 1122 loss=8.796, nll_loss=3.087, mask_ins=1.589, word_ins_ml=4.654, word_reposition=1.772, kpe=0.78, ppl=444.48, wps=6133.8, ups=0.3, wpb=20570.9, bsz=256, num_updates=37900, lr=0.000181608, gnorm=2.08, clip=0, loss_scale=1024, train_wall=292, wall=0
2022-08-10 18:39:57 | INFO | train_inner | epoch 034:   1026 / 1122 loss=8.794, nll_loss=3.092, mask_ins=1.581, word_ins_ml=4.66, word_reposition=1.77, kpe=0.784, ppl=443.91, wps=6124, ups=0.3, wpb=20490.9, bsz=256, num_updates=38000, lr=0.000181369, gnorm=2.109, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 18:45:15 | INFO | train | epoch 034 | loss nan | nll_loss 3.089 | mask_ins 1.58 | word_ins_ml 4.657 | word_reposition 1.782 | kpe nan | ppl nan | wps 5945.4 | ups 0.29 | wpb 20522 | bsz 255.8 | num_updates 38096 | lr 0.00018114 | gnorm 2.157 | clip 0 | loss_scale 992 | train_wall 3292 | wall 0
2022-08-10 18:46:43 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 15.449 | nll_loss 7.33 | mask_ins 2.265 | word_ins_ml 8.557 | word_reposition 3.174 | kpe 1.454 | ppl 44731.7 | wps 11229.5 | wpb 2367.6 | bsz 32 | num_updates 38096 | best_loss 10.679
2022-08-10 18:46:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 34 @ 38096 updates, score 15.449) (writing took 12.410144014284015 seconds)
2022-08-10 18:47:08 | INFO | train_inner | epoch 035:      4 / 1122 loss=8.713, nll_loss=3.048, mask_ins=1.569, word_ins_ml=4.62, word_reposition=1.744, kpe=0.779, ppl=419.6, wps=4722.4, ups=0.23, wpb=20364, bsz=253.8, num_updates=38100, lr=0.000181131, gnorm=2.17, clip=0, loss_scale=1024, train_wall=289, wall=0
2022-08-10 18:52:42 | INFO | train_inner | epoch 035:    104 / 1122 loss=nan, nll_loss=3.033, mask_ins=1.568, word_ins_ml=4.607, word_reposition=1.777, kpe=nan, ppl=nan, wps=6145.4, ups=0.3, wpb=20492.3, bsz=256, num_updates=38200, lr=0.000180894, gnorm=2.123, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 18:58:16 | INFO | train_inner | epoch 035:    204 / 1122 loss=8.702, nll_loss=3.041, mask_ins=1.565, word_ins_ml=4.615, word_reposition=1.761, kpe=0.761, ppl=416.57, wps=6130.7, ups=0.3, wpb=20518.5, bsz=256, num_updates=38300, lr=0.000180657, gnorm=2.197, clip=0, loss_scale=1280, train_wall=291, wall=0
2022-08-10 19:03:52 | INFO | train_inner | epoch 035:    304 / 1122 loss=8.736, nll_loss=3.056, mask_ins=1.57, word_ins_ml=4.628, word_reposition=1.771, kpe=0.767, ppl=426.37, wps=6137.6, ups=0.3, wpb=20571, bsz=256, num_updates=38400, lr=0.000180422, gnorm=2.143, clip=0, loss_scale=2048, train_wall=292, wall=0
2022-08-10 19:04:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-10 19:09:30 | INFO | train_inner | epoch 035:    405 / 1122 loss=8.737, nll_loss=3.051, mask_ins=1.574, word_ins_ml=4.623, word_reposition=1.769, kpe=0.771, ppl=426.74, wps=6084, ups=0.3, wpb=20580.6, bsz=256, num_updates=38500, lr=0.000180187, gnorm=2.136, clip=0, loss_scale=1115, train_wall=294, wall=0
2022-08-10 19:15:02 | INFO | train_inner | epoch 035:    505 / 1122 loss=nan, nll_loss=3.038, mask_ins=1.57, word_ins_ml=4.612, word_reposition=1.77, kpe=nan, ppl=nan, wps=6198.1, ups=0.3, wpb=20606.1, bsz=256, num_updates=38600, lr=0.000179954, gnorm=2.329, clip=0, loss_scale=1024, train_wall=290, wall=0
2022-08-10 19:20:36 | INFO | train_inner | epoch 035:    605 / 1122 loss=8.698, nll_loss=3.03, mask_ins=1.564, word_ins_ml=4.604, word_reposition=1.762, kpe=0.768, ppl=415.43, wps=6117.5, ups=0.3, wpb=20418.2, bsz=256, num_updates=38700, lr=0.000179721, gnorm=2.219, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 19:26:36 | INFO | train_inner | epoch 035:    705 / 1122 loss=8.74, nll_loss=3.037, mask_ins=1.575, word_ins_ml=4.611, word_reposition=1.786, kpe=0.769, ppl=427.67, wps=5734.1, ups=0.28, wpb=20613.5, bsz=256, num_updates=38800, lr=0.00017949, gnorm=2.131, clip=0, loss_scale=1024, train_wall=316, wall=0
2022-08-10 19:32:21 | INFO | train_inner | epoch 035:    805 / 1122 loss=8.764, nll_loss=3.058, mask_ins=1.576, word_ins_ml=4.629, word_reposition=1.787, kpe=0.772, ppl=434.75, wps=5924.1, ups=0.29, wpb=20465.1, bsz=256, num_updates=38900, lr=0.000179259, gnorm=2.22, clip=0, loss_scale=1024, train_wall=302, wall=0
2022-08-10 19:37:55 | INFO | train_inner | epoch 035:    905 / 1122 loss=8.751, nll_loss=3.042, mask_ins=1.573, word_ins_ml=4.616, word_reposition=1.79, kpe=0.773, ppl=430.97, wps=6133.9, ups=0.3, wpb=20485.7, bsz=256, num_updates=39000, lr=0.000179029, gnorm=2.184, clip=0, loss_scale=1843, train_wall=291, wall=0
2022-08-10 19:41:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-10 19:43:31 | INFO | train_inner | epoch 035:   1006 / 1122 loss=8.749, nll_loss=3.058, mask_ins=1.578, word_ins_ml=4.63, word_reposition=1.765, kpe=0.777, ppl=430.38, wps=6119, ups=0.3, wpb=20535.6, bsz=256, num_updates=39100, lr=0.0001788, gnorm=2.21, clip=0, loss_scale=1734, train_wall=293, wall=0
2022-08-10 19:49:05 | INFO | train_inner | epoch 035:   1106 / 1122 loss=8.705, nll_loss=3.021, mask_ins=1.571, word_ins_ml=4.596, word_reposition=1.768, kpe=0.77, ppl=417.45, wps=6147.4, ups=0.3, wpb=20570.4, bsz=256, num_updates=39200, lr=0.000178571, gnorm=2.183, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 19:49:57 | INFO | train | epoch 035 | loss nan | nll_loss 3.042 | mask_ins 1.571 | word_ins_ml 4.615 | word_reposition 1.772 | kpe nan | ppl nan | wps 5919.9 | ups 0.29 | wpb 20520.8 | bsz 255.8 | num_updates 39216 | lr 0.000178535 | gnorm 2.195 | clip 0 | loss_scale 1283 | train_wall 3298 | wall 0
2022-08-10 19:51:25 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 15.454 | nll_loss 7.27 | mask_ins 2.323 | word_ins_ml 8.497 | word_reposition 3.179 | kpe 1.454 | ppl 44875.6 | wps 11247.3 | wpb 2367.6 | bsz 32 | num_updates 39216 | best_loss 10.679
2022-08-10 19:51:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 35 @ 39216 updates, score 15.454) (writing took 17.235230887308717 seconds)
2022-08-10 19:56:22 | INFO | train_inner | epoch 036:     84 / 1122 loss=nan, nll_loss=3.007, mask_ins=1.557, word_ins_ml=4.585, word_reposition=1.75, kpe=nan, ppl=nan, wps=4669.7, ups=0.23, wpb=20404.8, bsz=253.8, num_updates=39300, lr=0.000178344, gnorm=2.238, clip=0, loss_scale=1024, train_wall=289, wall=0
2022-08-10 20:01:56 | INFO | train_inner | epoch 036:    184 / 1122 loss=8.696, nll_loss=3.026, mask_ins=1.558, word_ins_ml=4.601, word_reposition=1.78, kpe=0.757, ppl=414.7, wps=6157.2, ups=0.3, wpb=20574.9, bsz=256, num_updates=39400, lr=0.000178118, gnorm=2.268, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 20:07:38 | INFO | train_inner | epoch 036:    284 / 1122 loss=8.708, nll_loss=3.032, mask_ins=1.558, word_ins_ml=4.608, word_reposition=1.781, kpe=0.762, ppl=418.21, wps=6016.1, ups=0.29, wpb=20570.8, bsz=256, num_updates=39500, lr=0.000177892, gnorm=2.304, clip=0, loss_scale=1024, train_wall=297, wall=0
2022-08-10 20:13:12 | INFO | train_inner | epoch 036:    384 / 1122 loss=8.656, nll_loss=3.008, mask_ins=1.555, word_ins_ml=4.586, word_reposition=1.755, kpe=0.76, ppl=403.3, wps=6175.4, ups=0.3, wpb=20583.8, bsz=256, num_updates=39600, lr=0.000177667, gnorm=2.238, clip=0, loss_scale=1219, train_wall=290, wall=0
2022-08-10 20:18:44 | INFO | train_inner | epoch 036:    484 / 1122 loss=8.69, nll_loss=3.012, mask_ins=1.564, word_ins_ml=4.589, word_reposition=1.777, kpe=0.76, ppl=413.01, wps=6153.8, ups=0.3, wpb=20437.5, bsz=256, num_updates=39700, lr=0.000177443, gnorm=2.25, clip=0, loss_scale=2048, train_wall=290, wall=0
2022-08-10 20:22:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-10 20:24:22 | INFO | train_inner | epoch 036:    585 / 1122 loss=8.654, nll_loss=3.007, mask_ins=1.559, word_ins_ml=4.584, word_reposition=1.75, kpe=0.761, ppl=402.75, wps=6062.9, ups=0.3, wpb=20488.5, bsz=256, num_updates=39800, lr=0.00017722, gnorm=2.28, clip=0, loss_scale=1764, train_wall=294, wall=0
2022-08-10 20:29:56 | INFO | train_inner | epoch 036:    685 / 1122 loss=nan, nll_loss=3.015, mask_ins=1.568, word_ins_ml=4.592, word_reposition=1.765, kpe=nan, ppl=nan, wps=6154.8, ups=0.3, wpb=20570.7, bsz=256, num_updates=39900, lr=0.000176998, gnorm=2.295, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 20:36:07 | INFO | train_inner | epoch 036:    785 / 1122 loss=8.628, nll_loss=2.984, mask_ins=1.558, word_ins_ml=4.564, word_reposition=1.742, kpe=0.764, ppl=395.57, wps=5509.4, ups=0.27, wpb=20466.2, bsz=256, num_updates=40000, lr=0.000176777, gnorm=2.212, clip=0, loss_scale=1024, train_wall=328, wall=0
2022-08-10 20:41:41 | INFO | train_inner | epoch 036:    885 / 1122 loss=8.582, nll_loss=2.985, mask_ins=1.551, word_ins_ml=4.566, word_reposition=1.702, kpe=0.764, ppl=383.24, wps=6129.3, ups=0.3, wpb=20451.4, bsz=256, num_updates=40100, lr=0.000176556, gnorm=2.373, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 20:47:14 | INFO | train_inner | epoch 036:    985 / 1122 loss=8.68, nll_loss=3.001, mask_ins=1.572, word_ins_ml=4.579, word_reposition=1.762, kpe=0.767, ppl=410.12, wps=6205, ups=0.3, wpb=20658.4, bsz=256, num_updates=40200, lr=0.000176336, gnorm=2.302, clip=0, loss_scale=1024, train_wall=290, wall=0
2022-08-10 20:52:48 | INFO | train_inner | epoch 036:   1085 / 1122 loss=8.671, nll_loss=2.995, mask_ins=1.563, word_ins_ml=4.573, word_reposition=1.768, kpe=0.767, ppl=407.67, wps=6157.3, ups=0.3, wpb=20549.7, bsz=256, num_updates=40300, lr=0.000176117, gnorm=2.166, clip=0, loss_scale=1188, train_wall=290, wall=0
2022-08-10 20:54:50 | INFO | train | epoch 036 | loss nan | nll_loss 3.006 | mask_ins 1.561 | word_ins_ml 4.584 | word_reposition 1.758 | kpe nan | ppl nan | wps 5909 | ups 0.29 | wpb 20520.6 | bsz 255.8 | num_updates 40337 | lr 0.000176037 | gnorm 2.262 | clip 0 | loss_scale 1248 | train_wall 3302 | wall 0
2022-08-10 20:56:18 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 15.492 | nll_loss 7.343 | mask_ins 2.331 | word_ins_ml 8.578 | word_reposition 3.099 | kpe 1.485 | ppl 46086.6 | wps 11275.6 | wpb 2367.6 | bsz 32 | num_updates 40337 | best_loss 10.679
2022-08-10 20:56:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 36 @ 40337 updates, score 15.492) (writing took 15.32076557353139 seconds)
2022-08-10 21:00:04 | INFO | train_inner | epoch 037:     63 / 1122 loss=8.635, nll_loss=2.985, mask_ins=1.559, word_ins_ml=4.565, word_reposition=1.753, kpe=0.758, ppl=397.57, wps=4651, ups=0.23, wpb=20282.4, bsz=253.8, num_updates=40400, lr=0.000175899, gnorm=2.218, clip=0, loss_scale=2048, train_wall=290, wall=0
2022-08-10 21:05:38 | INFO | train_inner | epoch 037:    163 / 1122 loss=nan, nll_loss=2.953, mask_ins=1.541, word_ins_ml=4.538, word_reposition=1.725, kpe=nan, ppl=nan, wps=6140.3, ups=0.3, wpb=20541.4, bsz=256, num_updates=40500, lr=0.000175682, gnorm=2.27, clip=0, loss_scale=2048, train_wall=291, wall=0
2022-08-10 21:11:13 | INFO | train_inner | epoch 037:    263 / 1122 loss=8.611, nll_loss=2.984, mask_ins=1.551, word_ins_ml=4.565, word_reposition=1.742, kpe=0.753, ppl=390.88, wps=6149.4, ups=0.3, wpb=20576.4, bsz=256, num_updates=40600, lr=0.000175466, gnorm=2.229, clip=0, loss_scale=2048, train_wall=291, wall=0
2022-08-10 21:16:47 | INFO | train_inner | epoch 037:    363 / 1122 loss=8.622, nll_loss=2.972, mask_ins=1.556, word_ins_ml=4.555, word_reposition=1.758, kpe=0.754, ppl=394.02, wps=6173.2, ups=0.3, wpb=20587.9, bsz=256, num_updates=40700, lr=0.00017525, gnorm=2.263, clip=0, loss_scale=2048, train_wall=291, wall=0
2022-08-10 21:17:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-10 21:22:23 | INFO | train_inner | epoch 037:    464 / 1122 loss=8.568, nll_loss=2.938, mask_ins=1.548, word_ins_ml=4.524, word_reposition=1.744, kpe=0.752, ppl=379.59, wps=6090.6, ups=0.3, wpb=20515.2, bsz=256, num_updates=40800, lr=0.000175035, gnorm=2.296, clip=0, loss_scale=1186, train_wall=294, wall=0
2022-08-10 21:27:58 | INFO | train_inner | epoch 037:    564 / 1122 loss=8.622, nll_loss=2.978, mask_ins=1.551, word_ins_ml=4.558, word_reposition=1.755, kpe=0.758, ppl=393.91, wps=6133.2, ups=0.3, wpb=20510, bsz=256, num_updates=40900, lr=0.000174821, gnorm=2.258, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 21:33:31 | INFO | train_inner | epoch 037:    664 / 1122 loss=8.632, nll_loss=3.002, mask_ins=1.556, word_ins_ml=4.58, word_reposition=1.737, kpe=0.759, ppl=396.81, wps=6182.7, ups=0.3, wpb=20615.6, bsz=256, num_updates=41000, lr=0.000174608, gnorm=2.264, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 21:39:06 | INFO | train_inner | epoch 037:    764 / 1122 loss=8.636, nll_loss=2.989, mask_ins=1.559, word_ins_ml=4.568, word_reposition=1.749, kpe=0.759, ppl=397.79, wps=6153.9, ups=0.3, wpb=20585.1, bsz=256, num_updates=41100, lr=0.000174395, gnorm=2.327, clip=0, loss_scale=1024, train_wall=292, wall=0
2022-08-10 21:45:19 | INFO | train_inner | epoch 037:    864 / 1122 loss=8.607, nll_loss=2.935, mask_ins=1.562, word_ins_ml=4.521, word_reposition=1.767, kpe=0.756, ppl=389.92, wps=5503.1, ups=0.27, wpb=20528.5, bsz=256, num_updates=41200, lr=0.000174183, gnorm=2.297, clip=0, loss_scale=1024, train_wall=330, wall=0
2022-08-10 21:50:52 | INFO | train_inner | epoch 037:    964 / 1122 loss=8.57, nll_loss=2.957, mask_ins=1.552, word_ins_ml=4.541, word_reposition=1.72, kpe=0.757, ppl=380.07, wps=6166.9, ups=0.3, wpb=20557.9, bsz=256, num_updates=41300, lr=0.000173972, gnorm=2.267, clip=0, loss_scale=1772, train_wall=290, wall=0
2022-08-10 21:55:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-10 21:56:29 | INFO | train_inner | epoch 037:   1065 / 1122 loss=nan, nll_loss=2.947, mask_ins=1.557, word_ins_ml=4.531, word_reposition=1.753, kpe=nan, ppl=nan, wps=6071.7, ups=0.3, wpb=20457.4, bsz=256, num_updates=41400, lr=0.000173762, gnorm=2.256, clip=0, loss_scale=1774, train_wall=293, wall=0
2022-08-10 21:59:37 | INFO | train | epoch 037 | loss nan | nll_loss 2.967 | mask_ins 1.553 | word_ins_ml 4.549 | word_reposition 1.745 | kpe nan | ppl nan | wps 5912.1 | ups 0.29 | wpb 20520 | bsz 255.8 | num_updates 41457 | lr 0.000173643 | gnorm 2.274 | clip 0 | loss_scale 1504 | train_wall 3301 | wall 0
2022-08-10 22:01:05 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 15.843 | nll_loss 7.35 | mask_ins 2.46 | word_ins_ml 8.58 | word_reposition 3.277 | kpe 1.526 | ppl 58792.4 | wps 11325.9 | wpb 2367.6 | bsz 32 | num_updates 41457 | best_loss 10.679
2022-08-10 22:01:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 37 @ 41457 updates, score 15.843) (writing took 12.915813500061631 seconds)
2022-08-10 22:03:50 | INFO | train_inner | epoch 038:     43 / 1122 loss=8.55, nll_loss=2.952, mask_ins=1.542, word_ins_ml=4.536, word_reposition=1.717, kpe=0.754, ppl=374.74, wps=4609.3, ups=0.23, wpb=20330, bsz=253.8, num_updates=41500, lr=0.000173553, gnorm=2.383, clip=0, loss_scale=1024, train_wall=294, wall=0
2022-08-10 22:09:24 | INFO | train_inner | epoch 038:    143 / 1122 loss=nan, nll_loss=2.978, mask_ins=1.54, word_ins_ml=4.559, word_reposition=1.74, kpe=nan, ppl=nan, wps=6179.4, ups=0.3, wpb=20612.6, bsz=256, num_updates=41600, lr=0.000173344, gnorm=2.394, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-10 22:14:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-10 22:15:03 | INFO | train_inner | epoch 038:    244 / 1122 loss=8.578, nll_loss=2.96, mask_ins=1.553, word_ins_ml=4.543, word_reposition=1.735, kpe=0.747, ppl=382.26, wps=6079.4, ups=0.29, wpb=20627.2, bsz=256, num_updates=41700, lr=0.000173136, gnorm=2.305, clip=0, loss_scale=989, train_wall=295, wall=0
2022-08-10 22:19:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-10 22:20:41 | INFO | train_inner | epoch 038:    345 / 1122 loss=8.505, nll_loss=2.907, mask_ins=1.535, word_ins_ml=4.497, word_reposition=1.73, kpe=0.744, ppl=363.27, wps=6034, ups=0.3, wpb=20395.6, bsz=256, num_updates=41800, lr=0.000172929, gnorm=2.299, clip=0, loss_scale=464, train_wall=294, wall=0
2022-08-10 22:25:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-10 22:26:16 | INFO | train_inner | epoch 038:    446 / 1122 loss=8.572, nll_loss=2.946, mask_ins=1.544, word_ins_ml=4.531, word_reposition=1.748, kpe=0.748, ppl=380.44, wps=6100.6, ups=0.3, wpb=20451.4, bsz=256, num_updates=41900, lr=0.000172722, gnorm=2.276, clip=0, loss_scale=236, train_wall=292, wall=0
2022-08-10 22:31:51 | INFO | train_inner | epoch 038:    546 / 1122 loss=8.528, nll_loss=2.905, mask_ins=1.547, word_ins_ml=4.494, word_reposition=1.737, kpe=0.749, ppl=369.17, wps=6120.9, ups=0.3, wpb=20450.9, bsz=256, num_updates=42000, lr=0.000172516, gnorm=2.249, clip=0, loss_scale=128, train_wall=291, wall=0
2022-08-10 22:37:24 | INFO | train_inner | epoch 038:    646 / 1122 loss=8.549, nll_loss=2.943, mask_ins=1.542, word_ins_ml=4.529, word_reposition=1.726, kpe=0.753, ppl=374.63, wps=6164.6, ups=0.3, wpb=20554.2, bsz=256, num_updates=42100, lr=0.000172311, gnorm=2.388, clip=0, loss_scale=128, train_wall=291, wall=0
2022-08-10 22:42:57 | INFO | train_inner | epoch 038:    746 / 1122 loss=8.552, nll_loss=2.933, mask_ins=1.547, word_ins_ml=4.52, word_reposition=1.736, kpe=0.749, ppl=375.35, wps=6174.4, ups=0.3, wpb=20587.4, bsz=256, num_updates=42200, lr=0.000172107, gnorm=2.268, clip=0, loss_scale=128, train_wall=290, wall=0
2022-08-10 22:48:32 | INFO | train_inner | epoch 038:    846 / 1122 loss=8.478, nll_loss=2.9, mask_ins=1.54, word_ins_ml=4.49, word_reposition=1.699, kpe=0.749, ppl=356.61, wps=6138.5, ups=0.3, wpb=20530.4, bsz=256, num_updates=42300, lr=0.000171904, gnorm=2.363, clip=0, loss_scale=128, train_wall=291, wall=0
2022-08-10 22:54:41 | INFO | train_inner | epoch 038:    946 / 1122 loss=nan, nll_loss=2.953, mask_ins=1.547, word_ins_ml=4.537, word_reposition=1.735, kpe=nan, ppl=nan, wps=5594.4, ups=0.27, wpb=20634.1, bsz=256, num_updates=42400, lr=0.000171701, gnorm=2.244, clip=0, loss_scale=133, train_wall=327, wall=0
2022-08-10 23:00:14 | INFO | train_inner | epoch 038:   1046 / 1122 loss=8.517, nll_loss=2.92, mask_ins=1.539, word_ins_ml=4.508, word_reposition=1.717, kpe=0.753, ppl=366.21, wps=6130, ups=0.3, wpb=20456.7, bsz=256, num_updates=42500, lr=0.000171499, gnorm=2.244, clip=0, loss_scale=256, train_wall=291, wall=0
2022-08-10 23:04:28 | INFO | train | epoch 038 | loss nan | nll_loss 2.936 | mask_ins 1.543 | word_ins_ml 4.522 | word_reposition 1.73 | kpe nan | ppl nan | wps 5902.8 | ups 0.29 | wpb 20522.3 | bsz 255.8 | num_updates 42576 | lr 0.000171345 | gnorm 2.309 | clip 0 | loss_scale 380 | train_wall 3305 | wall 0
2022-08-10 23:05:56 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 15.574 | nll_loss 7.379 | mask_ins 2.333 | word_ins_ml 8.607 | word_reposition 3.122 | kpe 1.512 | ppl 48774.4 | wps 11254.7 | wpb 2367.6 | bsz 32 | num_updates 42576 | best_loss 10.679
2022-08-10 23:06:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 38 @ 42576 updates, score 15.574) (writing took 10.320095207542181 seconds)
2022-08-10 23:07:26 | INFO | train_inner | epoch 039:     24 / 1122 loss=8.564, nll_loss=2.945, mask_ins=1.548, word_ins_ml=4.529, word_reposition=1.737, kpe=0.75, ppl=378.5, wps=4738.6, ups=0.23, wpb=20452.9, bsz=253.8, num_updates=42600, lr=0.000171297, gnorm=2.35, clip=0, loss_scale=256, train_wall=290, wall=0
2022-08-10 23:13:01 | INFO | train_inner | epoch 039:    124 / 1122 loss=8.493, nll_loss=2.917, mask_ins=1.524, word_ins_ml=4.505, word_reposition=1.73, kpe=0.734, ppl=360.22, wps=6105, ups=0.3, wpb=20456.8, bsz=256, num_updates=42700, lr=0.000171096, gnorm=2.31, clip=0, loss_scale=256, train_wall=291, wall=0
2022-08-10 23:18:35 | INFO | train_inner | epoch 039:    224 / 1122 loss=8.456, nll_loss=2.899, mask_ins=1.53, word_ins_ml=4.489, word_reposition=1.697, kpe=0.739, ppl=351.14, wps=6140, ups=0.3, wpb=20500.2, bsz=256, num_updates=42800, lr=0.000170896, gnorm=2.328, clip=0, loss_scale=256, train_wall=291, wall=0
2022-08-10 23:24:07 | INFO | train_inner | epoch 039:    324 / 1122 loss=8.497, nll_loss=2.917, mask_ins=1.537, word_ins_ml=4.505, word_reposition=1.713, kpe=0.741, ppl=361.34, wps=6209.1, ups=0.3, wpb=20586.9, bsz=256, num_updates=42900, lr=0.000170697, gnorm=2.391, clip=0, loss_scale=256, train_wall=290, wall=0
2022-08-10 23:29:42 | INFO | train_inner | epoch 039:    424 / 1122 loss=8.503, nll_loss=2.908, mask_ins=1.538, word_ins_ml=4.497, word_reposition=1.726, kpe=0.741, ppl=362.91, wps=6108.6, ups=0.3, wpb=20499.8, bsz=256, num_updates=43000, lr=0.000170499, gnorm=2.323, clip=0, loss_scale=492, train_wall=292, wall=0
2022-08-10 23:35:16 | INFO | train_inner | epoch 039:    524 / 1122 loss=8.513, nll_loss=2.931, mask_ins=1.538, word_ins_ml=4.518, word_reposition=1.713, kpe=0.744, ppl=365.27, wps=6162.8, ups=0.3, wpb=20567.7, bsz=256, num_updates=43100, lr=0.000170301, gnorm=2.32, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-10 23:40:50 | INFO | train_inner | epoch 039:    624 / 1122 loss=nan, nll_loss=2.93, mask_ins=1.534, word_ins_ml=4.517, word_reposition=1.696, kpe=nan, ppl=nan, wps=6093.9, ups=0.3, wpb=20363.5, bsz=256, num_updates=43200, lr=0.000170103, gnorm=2.33, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-10 23:46:24 | INFO | train_inner | epoch 039:    724 / 1122 loss=8.463, nll_loss=2.9, mask_ins=1.533, word_ins_ml=4.49, word_reposition=1.696, kpe=0.744, ppl=352.82, wps=6169, ups=0.3, wpb=20615.6, bsz=256, num_updates=43300, lr=0.000169907, gnorm=2.259, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-10 23:51:57 | INFO | train_inner | epoch 039:    824 / 1122 loss=nan, nll_loss=2.888, mask_ins=1.536, word_ins_ml=4.48, word_reposition=1.696, kpe=nan, ppl=nan, wps=6172.6, ups=0.3, wpb=20565.1, bsz=256, num_updates=43400, lr=0.000169711, gnorm=2.269, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-10 23:57:31 | INFO | train_inner | epoch 039:    924 / 1122 loss=8.557, nll_loss=2.906, mask_ins=1.551, word_ins_ml=4.496, word_reposition=1.763, kpe=0.747, ppl=376.7, wps=6162.1, ups=0.3, wpb=20567.9, bsz=256, num_updates=43500, lr=0.000169516, gnorm=2.353, clip=0, loss_scale=922, train_wall=290, wall=0
2022-08-10 23:58:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 00:05:07 | INFO | train_inner | epoch 039:   1025 / 1122 loss=8.454, nll_loss=2.886, mask_ins=1.531, word_ins_ml=4.477, word_reposition=1.705, kpe=0.741, ppl=350.57, wps=4534.9, ups=0.22, wpb=20646.4, bsz=256, num_updates=43600, lr=0.000169321, gnorm=2.281, clip=0, loss_scale=618, train_wall=409, wall=0
2022-08-11 00:07:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 00:10:36 | INFO | train | epoch 039 | loss nan | nll_loss 2.906 | mask_ins 1.535 | word_ins_ml 4.496 | word_reposition 1.712 | kpe nan | ppl nan | wps 5792.1 | ups 0.28 | wpb 20520.3 | bsz 255.8 | num_updates 43696 | lr 0.000169135 | gnorm 2.34 | clip 0 | loss_scale 470 | train_wall 3385 | wall 0
2022-08-11 00:12:03 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 15.492 | nll_loss 7.427 | mask_ins 2.282 | word_ins_ml 8.659 | word_reposition 3.094 | kpe 1.457 | ppl 46085.8 | wps 11291.5 | wpb 2367.6 | bsz 32 | num_updates 43696 | best_loss 10.679
2022-08-11 00:12:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 39 @ 43696 updates, score 15.492) (writing took 4.197628064081073 seconds)
2022-08-11 00:12:21 | INFO | train_inner | epoch 040:      4 / 1122 loss=8.461, nll_loss=2.888, mask_ins=1.533, word_ins_ml=4.479, word_reposition=1.703, kpe=0.746, ppl=352.28, wps=4681.1, ups=0.23, wpb=20337.5, bsz=253.8, num_updates=43700, lr=0.000169128, gnorm=2.602, clip=0, loss_scale=370, train_wall=299, wall=0
2022-08-11 00:17:55 | INFO | train_inner | epoch 040:    104 / 1122 loss=8.447, nll_loss=2.88, mask_ins=1.526, word_ins_ml=4.473, word_reposition=1.712, kpe=0.737, ppl=349.09, wps=6172, ups=0.3, wpb=20626.2, bsz=256, num_updates=43800, lr=0.000168934, gnorm=3.363, clip=1, loss_scale=256, train_wall=291, wall=0
2022-08-11 00:23:30 | INFO | train_inner | epoch 040:    204 / 1122 loss=8.482, nll_loss=2.912, mask_ins=1.527, word_ins_ml=4.501, word_reposition=1.716, kpe=0.738, ppl=357.47, wps=6141.1, ups=0.3, wpb=20560.8, bsz=256, num_updates=43900, lr=0.000168742, gnorm=3.261, clip=0, loss_scale=256, train_wall=291, wall=0
2022-08-11 00:29:02 | INFO | train_inner | epoch 040:    304 / 1122 loss=8.439, nll_loss=2.876, mask_ins=1.527, word_ins_ml=4.469, word_reposition=1.707, kpe=0.735, ppl=347.03, wps=6189.5, ups=0.3, wpb=20561.5, bsz=256, num_updates=44000, lr=0.00016855, gnorm=2.583, clip=0, loss_scale=256, train_wall=290, wall=0
2022-08-11 00:34:36 | INFO | train_inner | epoch 040:    404 / 1122 loss=8.38, nll_loss=2.858, mask_ins=1.516, word_ins_ml=4.453, word_reposition=1.675, kpe=0.736, ppl=333.22, wps=6126.7, ups=0.3, wpb=20457.7, bsz=256, num_updates=44100, lr=0.000168359, gnorm=2.601, clip=0, loss_scale=256, train_wall=291, wall=0
2022-08-11 00:38:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 00:40:13 | INFO | train_inner | epoch 040:    505 / 1122 loss=nan, nll_loss=2.881, mask_ins=1.531, word_ins_ml=4.474, word_reposition=1.713, kpe=nan, ppl=nan, wps=6084.9, ups=0.3, wpb=20517.3, bsz=256, num_updates=44200, lr=0.000168168, gnorm=2.6, clip=0, loss_scale=294, train_wall=294, wall=0
2022-08-11 00:45:48 | INFO | train_inner | epoch 040:    605 / 1122 loss=8.446, nll_loss=2.876, mask_ins=1.526, word_ins_ml=4.469, word_reposition=1.714, kpe=0.738, ppl=348.82, wps=6144.5, ups=0.3, wpb=20556.2, bsz=256, num_updates=44300, lr=0.000167978, gnorm=2.602, clip=0, loss_scale=256, train_wall=291, wall=0
2022-08-11 00:51:23 | INFO | train_inner | epoch 040:    705 / 1122 loss=8.416, nll_loss=2.86, mask_ins=1.536, word_ins_ml=4.455, word_reposition=1.683, kpe=0.742, ppl=341.46, wps=6142.2, ups=0.3, wpb=20579.9, bsz=256, num_updates=44400, lr=0.000167789, gnorm=2.484, clip=0, loss_scale=256, train_wall=292, wall=0
2022-08-11 00:56:57 | INFO | train_inner | epoch 040:    805 / 1122 loss=8.527, nll_loss=2.9, mask_ins=1.543, word_ins_ml=4.491, word_reposition=1.748, kpe=0.745, ppl=368.86, wps=6127.5, ups=0.3, wpb=20465.2, bsz=256, num_updates=44500, lr=0.0001676, gnorm=2.703, clip=0, loss_scale=256, train_wall=291, wall=0
2022-08-11 01:02:29 | INFO | train_inner | epoch 040:    905 / 1122 loss=8.462, nll_loss=2.878, mask_ins=1.535, word_ins_ml=4.471, word_reposition=1.718, kpe=0.739, ppl=352.7, wps=6186.4, ups=0.3, wpb=20568.9, bsz=256, num_updates=44600, lr=0.000167412, gnorm=2.621, clip=0, loss_scale=256, train_wall=290, wall=0
2022-08-11 01:08:03 | INFO | train_inner | epoch 040:   1005 / 1122 loss=8.486, nll_loss=2.884, mask_ins=1.538, word_ins_ml=4.476, word_reposition=1.726, kpe=0.746, ppl=358.57, wps=6141.5, ups=0.3, wpb=20469, bsz=256, num_updates=44700, lr=0.000167225, gnorm=3.058, clip=1, loss_scale=302, train_wall=290, wall=0
2022-08-11 01:09:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 01:14:29 | INFO | train_inner | epoch 040:   1106 / 1122 loss=8.483, nll_loss=2.903, mask_ins=1.532, word_ins_ml=4.493, word_reposition=1.716, kpe=0.743, ppl=357.8, wps=5317.7, ups=0.26, wpb=20557.6, bsz=256, num_updates=44800, lr=0.000167038, gnorm=2.516, clip=0, loss_scale=322, train_wall=342, wall=0
2022-08-11 01:15:21 | INFO | train | epoch 040 | loss nan | nll_loss 2.883 | mask_ins 1.531 | word_ins_ml 4.475 | word_reposition 1.712 | kpe nan | ppl nan | wps 5915.4 | ups 0.29 | wpb 20522.3 | bsz 255.8 | num_updates 44816 | lr 0.000167008 | gnorm 2.766 | clip 0.2 | loss_scale 269 | train_wall 3311 | wall 0
2022-08-11 01:16:48 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 15.383 | nll_loss 7.345 | mask_ins 2.265 | word_ins_ml 8.581 | word_reposition 3.085 | kpe 1.452 | ppl 42730.4 | wps 11381.3 | wpb 2367.6 | bsz 32 | num_updates 44816 | best_loss 10.679
2022-08-11 01:16:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 40 @ 44816 updates, score 15.383) (writing took 4.2617360427975655 seconds)
2022-08-11 01:21:33 | INFO | train_inner | epoch 041:     84 / 1122 loss=8.41, nll_loss=2.859, mask_ins=1.523, word_ins_ml=4.454, word_reposition=1.7, kpe=0.733, ppl=340.22, wps=4800.5, ups=0.24, wpb=20347, bsz=253.8, num_updates=44900, lr=0.000166852, gnorm=2.612, clip=0, loss_scale=256, train_wall=290, wall=0
2022-08-11 01:27:07 | INFO | train_inner | epoch 041:    184 / 1122 loss=8.357, nll_loss=2.844, mask_ins=1.512, word_ins_ml=4.441, word_reposition=1.679, kpe=0.725, ppl=327.86, wps=6150.5, ups=0.3, wpb=20517.2, bsz=256, num_updates=45000, lr=0.000166667, gnorm=2.394, clip=0, loss_scale=256, train_wall=290, wall=0
2022-08-11 01:32:38 | INFO | train_inner | epoch 041:    284 / 1122 loss=nan, nll_loss=2.888, mask_ins=1.522, word_ins_ml=4.48, word_reposition=1.674, kpe=nan, ppl=nan, wps=6253.3, ups=0.3, wpb=20709.4, bsz=256, num_updates=45100, lr=0.000166482, gnorm=2.584, clip=0, loss_scale=256, train_wall=289, wall=0
2022-08-11 01:38:12 | INFO | train_inner | epoch 041:    384 / 1122 loss=8.444, nll_loss=2.889, mask_ins=1.521, word_ins_ml=4.48, word_reposition=1.711, kpe=0.732, ppl=348.24, wps=6144.5, ups=0.3, wpb=20496.9, bsz=256, num_updates=45200, lr=0.000166298, gnorm=2.691, clip=0, loss_scale=256, train_wall=291, wall=0
2022-08-11 01:43:45 | INFO | train_inner | epoch 041:    484 / 1122 loss=8.385, nll_loss=2.828, mask_ins=1.524, word_ins_ml=4.426, word_reposition=1.701, kpe=0.734, ppl=334.39, wps=6153.5, ups=0.3, wpb=20546.2, bsz=256, num_updates=45300, lr=0.000166114, gnorm=2.656, clip=0, loss_scale=417, train_wall=291, wall=0
2022-08-11 01:47:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 01:49:23 | INFO | train_inner | epoch 041:    585 / 1122 loss=8.416, nll_loss=2.875, mask_ins=1.519, word_ins_ml=4.468, word_reposition=1.695, kpe=0.735, ppl=341.65, wps=6087.7, ups=0.3, wpb=20551.8, bsz=256, num_updates=45400, lr=0.000165931, gnorm=2.754, clip=0, loss_scale=444, train_wall=294, wall=0
2022-08-11 01:55:02 | INFO | train_inner | epoch 041:    685 / 1122 loss=8.462, nll_loss=2.905, mask_ins=1.526, word_ins_ml=4.495, word_reposition=1.707, kpe=0.734, ppl=352.55, wps=6076.5, ups=0.3, wpb=20577.8, bsz=256, num_updates=45500, lr=0.000165748, gnorm=2.474, clip=0, loss_scale=256, train_wall=295, wall=0
2022-08-11 02:00:36 | INFO | train_inner | epoch 041:    785 / 1122 loss=8.465, nll_loss=2.882, mask_ins=1.524, word_ins_ml=4.475, word_reposition=1.733, kpe=0.734, ppl=353.37, wps=6156.1, ups=0.3, wpb=20549.7, bsz=256, num_updates=45600, lr=0.000165567, gnorm=2.56, clip=0, loss_scale=256, train_wall=291, wall=0
2022-08-11 02:06:08 | INFO | train_inner | epoch 041:    885 / 1122 loss=nan, nll_loss=2.859, mask_ins=1.518, word_ins_ml=4.454, word_reposition=1.717, kpe=nan, ppl=nan, wps=6173.6, ups=0.3, wpb=20497.9, bsz=256, num_updates=45700, lr=0.000165385, gnorm=2.658, clip=0, loss_scale=256, train_wall=290, wall=0
2022-08-11 02:11:41 | INFO | train_inner | epoch 041:    985 / 1122 loss=8.402, nll_loss=2.85, mask_ins=1.525, word_ins_ml=4.445, word_reposition=1.69, kpe=0.741, ppl=338.2, wps=6144.3, ups=0.3, wpb=20482.4, bsz=256, num_updates=45800, lr=0.000165205, gnorm=2.551, clip=0, loss_scale=256, train_wall=291, wall=0
2022-08-11 02:17:15 | INFO | train_inner | epoch 041:   1085 / 1122 loss=8.446, nll_loss=2.858, mask_ins=1.533, word_ins_ml=4.453, word_reposition=1.719, kpe=0.741, ppl=348.65, wps=6129.9, ups=0.3, wpb=20506.7, bsz=256, num_updates=45900, lr=0.000165025, gnorm=2.995, clip=0, loss_scale=294, train_wall=291, wall=0
2022-08-11 02:19:41 | INFO | train | epoch 041 | loss nan | nll_loss 2.866 | mask_ins 1.522 | word_ins_ml 4.46 | word_reposition 1.703 | kpe nan | ppl nan | wps 5959.9 | ups 0.29 | wpb 20519.8 | bsz 255.8 | num_updates 45937 | lr 0.000164958 | gnorm 2.625 | clip 0 | loss_scale 299 | train_wall 3286 | wall 0
2022-08-11 02:21:21 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 15.741 | nll_loss 7.437 | mask_ins 2.343 | word_ins_ml 8.669 | word_reposition 3.236 | kpe 1.494 | ppl 54770.6 | wps 9951 | wpb 2367.6 | bsz 32 | num_updates 45937 | best_loss 10.679
2022-08-11 02:21:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 41 @ 45937 updates, score 15.741) (writing took 15.909812780097127 seconds)
2022-08-11 02:25:19 | INFO | train_inner | epoch 042:     63 / 1122 loss=nan, nll_loss=2.831, mask_ins=1.516, word_ins_ml=4.43, word_reposition=1.701, kpe=nan, ppl=nan, wps=4211.5, ups=0.21, wpb=20379.3, bsz=253.8, num_updates=46000, lr=0.000164845, gnorm=2.705, clip=0, loss_scale=512, train_wall=326, wall=0
2022-08-11 02:30:53 | INFO | train_inner | epoch 042:    163 / 1122 loss=8.307, nll_loss=2.804, mask_ins=1.507, word_ins_ml=4.406, word_reposition=1.668, kpe=0.726, ppl=316.6, wps=6102.6, ups=0.3, wpb=20383.7, bsz=256, num_updates=46100, lr=0.000164666, gnorm=2.981, clip=1, loss_scale=512, train_wall=291, wall=0
2022-08-11 02:36:26 | INFO | train_inner | epoch 042:    263 / 1122 loss=8.343, nll_loss=2.839, mask_ins=1.512, word_ins_ml=4.437, word_reposition=1.668, kpe=0.726, ppl=324.61, wps=6166.7, ups=0.3, wpb=20536.2, bsz=256, num_updates=46200, lr=0.000164488, gnorm=2.663, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-11 02:42:01 | INFO | train_inner | epoch 042:    363 / 1122 loss=nan, nll_loss=2.828, mask_ins=1.511, word_ins_ml=4.426, word_reposition=1.679, kpe=nan, ppl=nan, wps=6139.2, ups=0.3, wpb=20533.6, bsz=256, num_updates=46300, lr=0.00016431, gnorm=2.496, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-11 02:47:35 | INFO | train_inner | epoch 042:    463 / 1122 loss=8.376, nll_loss=2.85, mask_ins=1.51, word_ins_ml=4.446, word_reposition=1.695, kpe=0.725, ppl=332.21, wps=6149.9, ups=0.3, wpb=20576.2, bsz=256, num_updates=46400, lr=0.000164133, gnorm=2.515, clip=0, loss_scale=527, train_wall=292, wall=0
2022-08-11 02:53:10 | INFO | train_inner | epoch 042:    563 / 1122 loss=8.426, nll_loss=2.862, mask_ins=1.516, word_ins_ml=4.457, word_reposition=1.72, kpe=0.732, ppl=343.84, wps=6143.4, ups=0.3, wpb=20559.6, bsz=256, num_updates=46500, lr=0.000163956, gnorm=2.521, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-11 02:58:45 | INFO | train_inner | epoch 042:    663 / 1122 loss=8.355, nll_loss=2.828, mask_ins=1.512, word_ins_ml=4.428, word_reposition=1.687, kpe=0.729, ppl=327.49, wps=6114.4, ups=0.3, wpb=20443.8, bsz=256, num_updates=46600, lr=0.00016378, gnorm=2.536, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-11 03:03:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 03:04:21 | INFO | train_inner | epoch 042:    764 / 1122 loss=8.369, nll_loss=2.821, mask_ins=1.52, word_ins_ml=4.42, word_reposition=1.696, kpe=0.732, ppl=330.55, wps=6114, ups=0.3, wpb=20576.9, bsz=256, num_updates=46700, lr=0.000163605, gnorm=2.57, clip=0, loss_scale=948, train_wall=293, wall=0
2022-08-11 03:09:56 | INFO | train_inner | epoch 042:    864 / 1122 loss=8.428, nll_loss=2.852, mask_ins=1.527, word_ins_ml=4.447, word_reposition=1.719, kpe=0.734, ppl=344.37, wps=6152.1, ups=0.3, wpb=20599.3, bsz=256, num_updates=46800, lr=0.00016343, gnorm=2.513, clip=0, loss_scale=512, train_wall=292, wall=0
2022-08-11 03:15:31 | INFO | train_inner | epoch 042:    964 / 1122 loss=8.394, nll_loss=2.864, mask_ins=1.522, word_ins_ml=4.458, word_reposition=1.681, kpe=0.733, ppl=336.28, wps=6149.2, ups=0.3, wpb=20592.7, bsz=256, num_updates=46900, lr=0.000163256, gnorm=2.446, clip=0, loss_scale=512, train_wall=292, wall=0
2022-08-11 03:21:05 | INFO | train_inner | epoch 042:   1064 / 1122 loss=8.421, nll_loss=2.859, mask_ins=1.527, word_ins_ml=4.454, word_reposition=1.705, kpe=0.735, ppl=342.84, wps=6160.4, ups=0.3, wpb=20594.8, bsz=256, num_updates=47000, lr=0.000163082, gnorm=2.472, clip=0, loss_scale=512, train_wall=291, wall=0
2022-08-11 03:24:17 | INFO | train | epoch 042 | loss nan | nll_loss 2.839 | mask_ins 1.516 | word_ins_ml 4.437 | word_reposition 1.69 | kpe nan | ppl nan | wps 5934.5 | ups 0.29 | wpb 20521.5 | bsz 255.8 | num_updates 47058 | lr 0.000162981 | gnorm 2.586 | clip 0.1 | loss_scale 644 | train_wall 3277 | wall 0
2022-08-11 03:25:46 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 15.645 | nll_loss 7.417 | mask_ins 2.347 | word_ins_ml 8.647 | word_reposition 3.163 | kpe 1.488 | ppl 51232.1 | wps 11207.9 | wpb 2367.6 | bsz 32 | num_updates 47058 | best_loss 10.679
2022-08-11 03:25:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 42 @ 47058 updates, score 15.645) (writing took 4.2360409293323755 seconds)
2022-08-11 03:28:10 | INFO | train_inner | epoch 043:     42 / 1122 loss=8.318, nll_loss=2.833, mask_ins=1.509, word_ins_ml=4.431, word_reposition=1.65, kpe=0.728, ppl=319.12, wps=4766.9, ups=0.24, wpb=20260.1, bsz=253.8, num_updates=47100, lr=0.000162909, gnorm=2.555, clip=0, loss_scale=512, train_wall=290, wall=0
2022-08-11 03:29:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 03:34:34 | INFO | train_inner | epoch 043:    143 / 1122 loss=8.233, nll_loss=2.785, mask_ins=1.495, word_ins_ml=4.389, word_reposition=1.634, kpe=0.714, ppl=300.79, wps=5339.4, ups=0.26, wpb=20499.7, bsz=256, num_updates=47200, lr=0.000162736, gnorm=2.744, clip=0, loss_scale=294, train_wall=341, wall=0
2022-08-11 03:40:08 | INFO | train_inner | epoch 043:    243 / 1122 loss=8.29, nll_loss=2.797, mask_ins=1.503, word_ins_ml=4.4, word_reposition=1.67, kpe=0.717, ppl=313.02, wps=6209.6, ups=0.3, wpb=20724.5, bsz=256, num_updates=47300, lr=0.000162564, gnorm=2.762, clip=0, loss_scale=256, train_wall=291, wall=0
2022-08-11 03:45:42 | INFO | train_inner | epoch 043:    343 / 1122 loss=8.355, nll_loss=2.833, mask_ins=1.507, word_ins_ml=4.431, word_reposition=1.696, kpe=0.721, ppl=327.42, wps=6167.6, ups=0.3, wpb=20619.2, bsz=256, num_updates=47400, lr=0.000162392, gnorm=2.517, clip=0, loss_scale=256, train_wall=291, wall=0
2022-08-11 03:51:24 | INFO | train_inner | epoch 043:    443 / 1122 loss=8.363, nll_loss=2.85, mask_ins=1.51, word_ins_ml=4.446, word_reposition=1.682, kpe=0.724, ppl=329.13, wps=6001.5, ups=0.29, wpb=20539.8, bsz=256, num_updates=47500, lr=0.000162221, gnorm=2.535, clip=0, loss_scale=256, train_wall=297, wall=0
2022-08-11 03:56:45 | INFO | train_inner | epoch 043:    543 / 1122 loss=8.298, nll_loss=2.81, mask_ins=1.496, word_ins_ml=4.411, word_reposition=1.672, kpe=0.719, ppl=314.83, wps=6394.1, ups=0.31, wpb=20480.6, bsz=256, num_updates=47600, lr=0.000162051, gnorm=2.495, clip=0, loss_scale=256, train_wall=282, wall=0
2022-08-11 04:02:04 | INFO | train_inner | epoch 043:    643 / 1122 loss=nan, nll_loss=2.829, mask_ins=1.508, word_ins_ml=4.427, word_reposition=1.701, kpe=nan, ppl=nan, wps=6387.6, ups=0.31, wpb=20417.8, bsz=256, num_updates=47700, lr=0.000161881, gnorm=2.492, clip=0, loss_scale=445, train_wall=282, wall=0
2022-08-11 04:07:25 | INFO | train_inner | epoch 043:    743 / 1122 loss=nan, nll_loss=2.819, mask_ins=1.511, word_ins_ml=4.419, word_reposition=1.692, kpe=nan, ppl=nan, wps=6386.9, ups=0.31, wpb=20465.6, bsz=256, num_updates=47800, lr=0.000161712, gnorm=2.518, clip=0, loss_scale=512, train_wall=283, wall=0
2022-08-11 04:12:45 | INFO | train_inner | epoch 043:    843 / 1122 loss=8.344, nll_loss=2.846, mask_ins=1.51, word_ins_ml=4.442, word_reposition=1.665, kpe=0.726, ppl=324.82, wps=6426.6, ups=0.31, wpb=20597.3, bsz=256, num_updates=47900, lr=0.000161543, gnorm=2.489, clip=0, loss_scale=512, train_wall=283, wall=0
2022-08-11 04:18:06 | INFO | train_inner | epoch 043:    943 / 1122 loss=8.377, nll_loss=2.831, mask_ins=1.52, word_ins_ml=4.43, word_reposition=1.698, kpe=0.729, ppl=332.39, wps=6402.7, ups=0.31, wpb=20502.4, bsz=256, num_updates=48000, lr=0.000161374, gnorm=2.547, clip=0, loss_scale=512, train_wall=283, wall=0
2022-08-11 04:23:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 04:23:30 | INFO | train_inner | epoch 043:   1044 / 1122 loss=8.321, nll_loss=2.787, mask_ins=1.514, word_ins_ml=4.39, word_reposition=1.692, kpe=0.725, ppl=319.74, wps=6342.9, ups=0.31, wpb=20547.2, bsz=256, num_updates=48100, lr=0.000161206, gnorm=2.608, clip=0, loss_scale=502, train_wall=286, wall=0
2022-08-11 04:27:38 | INFO | train | epoch 043 | loss nan | nll_loss 2.82 | mask_ins 1.508 | word_ins_ml 4.419 | word_reposition 1.677 | kpe nan | ppl nan | wps 6047.4 | ups 0.29 | wpb 20521 | bsz 255.8 | num_updates 48178 | lr 0.000161076 | gnorm 2.609 | clip 0.1 | loss_scale 376 | train_wall 3260 | wall 0
2022-08-11 04:29:01 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 15.545 | nll_loss 7.3 | mask_ins 2.353 | word_ins_ml 8.546 | word_reposition 3.155 | kpe 1.491 | ppl 47795.3 | wps 12003.6 | wpb 2367.6 | bsz 32 | num_updates 48178 | best_loss 10.679
2022-08-11 04:29:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 43 @ 48178 updates, score 15.545) (writing took 5.3228728249669075 seconds)
2022-08-11 04:30:16 | INFO | train_inner | epoch 044:     22 / 1122 loss=8.332, nll_loss=2.827, mask_ins=1.515, word_ins_ml=4.426, word_reposition=1.668, kpe=0.724, ppl=322.25, wps=5006.8, ups=0.25, wpb=20363.5, bsz=253.8, num_updates=48200, lr=0.000161039, gnorm=3.261, clip=1, loss_scale=256, train_wall=282, wall=0
2022-08-11 04:35:48 | INFO | train_inner | epoch 044:    122 / 1122 loss=8.283, nll_loss=2.806, mask_ins=1.501, word_ins_ml=4.407, word_reposition=1.659, kpe=0.716, ppl=311.4, wps=6190.3, ups=0.3, wpb=20551.1, bsz=256, num_updates=48300, lr=0.000160872, gnorm=2.553, clip=0, loss_scale=256, train_wall=295, wall=0
2022-08-11 04:41:34 | INFO | train_inner | epoch 044:    222 / 1122 loss=8.258, nll_loss=2.789, mask_ins=1.494, word_ins_ml=4.392, word_reposition=1.655, kpe=0.716, ppl=306.05, wps=5923, ups=0.29, wpb=20451.5, bsz=256, num_updates=48400, lr=0.000160706, gnorm=2.611, clip=0, loss_scale=256, train_wall=307, wall=0
2022-08-11 04:46:54 | INFO | train_inner | epoch 044:    322 / 1122 loss=nan, nll_loss=2.804, mask_ins=1.498, word_ins_ml=4.405, word_reposition=1.666, kpe=nan, ppl=nan, wps=6402.3, ups=0.31, wpb=20486.9, bsz=256, num_updates=48500, lr=0.00016054, gnorm=2.666, clip=0, loss_scale=256, train_wall=282, wall=0
2022-08-11 04:52:14 | INFO | train_inner | epoch 044:    422 / 1122 loss=8.264, nll_loss=2.793, mask_ins=1.498, word_ins_ml=4.395, word_reposition=1.657, kpe=0.714, ppl=307.38, wps=6426, ups=0.31, wpb=20587.1, bsz=256, num_updates=48600, lr=0.000160375, gnorm=2.631, clip=0, loss_scale=256, train_wall=283, wall=0
2022-08-11 04:54:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 04:57:37 | INFO | train_inner | epoch 044:    523 / 1122 loss=8.284, nll_loss=2.806, mask_ins=1.498, word_ins_ml=4.407, word_reposition=1.664, kpe=0.714, ppl=311.66, wps=6368.5, ups=0.31, wpb=20596.3, bsz=256, num_updates=48700, lr=0.00016021, gnorm=2.552, clip=0, loss_scale=345, train_wall=285, wall=0
2022-08-11 05:02:58 | INFO | train_inner | epoch 044:    623 / 1122 loss=8.253, nll_loss=2.779, mask_ins=1.5, word_ins_ml=4.383, word_reposition=1.653, kpe=0.717, ppl=305.16, wps=6392.1, ups=0.31, wpb=20479.6, bsz=256, num_updates=48800, lr=0.000160046, gnorm=2.701, clip=0, loss_scale=256, train_wall=282, wall=0
2022-08-11 05:08:20 | INFO | train_inner | epoch 044:    723 / 1122 loss=8.325, nll_loss=2.804, mask_ins=1.502, word_ins_ml=4.405, word_reposition=1.699, kpe=0.719, ppl=320.63, wps=6356.9, ups=0.31, wpb=20509.3, bsz=256, num_updates=48900, lr=0.000159882, gnorm=2.689, clip=0, loss_scale=256, train_wall=285, wall=0
2022-08-11 05:13:40 | INFO | train_inner | epoch 044:    823 / 1122 loss=8.311, nll_loss=2.808, mask_ins=1.507, word_ins_ml=4.408, word_reposition=1.672, kpe=0.723, ppl=317.54, wps=6410.2, ups=0.31, wpb=20480.3, bsz=256, num_updates=49000, lr=0.000159719, gnorm=2.951, clip=1, loss_scale=256, train_wall=282, wall=0
2022-08-11 05:18:59 | INFO | train_inner | epoch 044:    923 / 1122 loss=8.291, nll_loss=2.792, mask_ins=1.507, word_ins_ml=4.395, word_reposition=1.664, kpe=0.725, ppl=313.12, wps=6417.7, ups=0.31, wpb=20500.1, bsz=256, num_updates=49100, lr=0.000159556, gnorm=2.776, clip=0, loss_scale=256, train_wall=281, wall=0
2022-08-11 05:24:20 | INFO | train_inner | epoch 044:   1023 / 1122 loss=8.332, nll_loss=2.816, mask_ins=1.513, word_ins_ml=4.416, word_reposition=1.681, kpe=0.722, ppl=322.23, wps=6425.3, ups=0.31, wpb=20594, bsz=256, num_updates=49200, lr=0.000159394, gnorm=2.652, clip=0, loss_scale=374, train_wall=283, wall=0
2022-08-11 05:29:36 | INFO | train | epoch 044 | loss nan | nll_loss 2.803 | mask_ins 1.503 | word_ins_ml 4.404 | word_reposition 1.671 | kpe nan | ppl nan | wps 6188.2 | ups 0.3 | wpb 20521.3 | bsz 255.8 | num_updates 49299 | lr 0.000159234 | gnorm 2.696 | clip 0.1 | loss_scale 297 | train_wall 3206 | wall 0
2022-08-11 05:30:58 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 15.725 | nll_loss 7.298 | mask_ins 2.411 | word_ins_ml 8.54 | word_reposition 3.244 | kpe 1.53 | ppl 54143.6 | wps 11966.4 | wpb 2367.6 | bsz 32 | num_updates 49299 | best_loss 10.679
2022-08-11 05:31:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 44 @ 49299 updates, score 15.725) (writing took 11.97509335540235 seconds)
2022-08-11 05:31:13 | INFO | train_inner | epoch 045:      1 / 1122 loss=8.364, nll_loss=2.832, mask_ins=1.512, word_ins_ml=4.43, word_reposition=1.7, kpe=0.722, ppl=329.58, wps=4946.7, ups=0.24, wpb=20445.2, bsz=253.8, num_updates=49300, lr=0.000159232, gnorm=2.698, clip=0, loss_scale=512, train_wall=281, wall=0
2022-08-11 05:36:34 | INFO | train_inner | epoch 045:    101 / 1122 loss=8.207, nll_loss=2.766, mask_ins=1.482, word_ins_ml=4.371, word_reposition=1.648, kpe=0.706, ppl=295.55, wps=6398, ups=0.31, wpb=20538.4, bsz=256, num_updates=49400, lr=0.000159071, gnorm=2.733, clip=0, loss_scale=512, train_wall=283, wall=0
2022-08-11 05:42:32 | INFO | train_inner | epoch 045:    201 / 1122 loss=8.178, nll_loss=2.746, mask_ins=1.489, word_ins_ml=4.353, word_reposition=1.627, kpe=0.709, ppl=289.6, wps=5732.7, ups=0.28, wpb=20524, bsz=256, num_updates=49500, lr=0.00015891, gnorm=2.636, clip=0, loss_scale=512, train_wall=320, wall=0
2022-08-11 05:48:07 | INFO | train_inner | epoch 045:    301 / 1122 loss=8.227, nll_loss=2.776, mask_ins=1.493, word_ins_ml=4.38, word_reposition=1.643, kpe=0.711, ppl=299.72, wps=6137.9, ups=0.3, wpb=20567.2, bsz=256, num_updates=49600, lr=0.00015875, gnorm=2.636, clip=0, loss_scale=512, train_wall=297, wall=0
2022-08-11 05:50:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 05:53:35 | INFO | train_inner | epoch 045:    402 / 1122 loss=8.242, nll_loss=2.773, mask_ins=1.493, word_ins_ml=4.377, word_reposition=1.662, kpe=0.71, ppl=302.76, wps=6219, ups=0.3, wpb=20403.8, bsz=256, num_updates=49700, lr=0.00015859, gnorm=2.619, clip=0, loss_scale=370, train_wall=290, wall=0
2022-08-11 05:58:55 | INFO | train_inner | epoch 045:    502 / 1122 loss=nan, nll_loss=2.781, mask_ins=1.498, word_ins_ml=4.385, word_reposition=1.676, kpe=nan, ppl=nan, wps=6419.3, ups=0.31, wpb=20498.2, bsz=256, num_updates=49800, lr=0.000158431, gnorm=2.784, clip=0, loss_scale=256, train_wall=282, wall=0
2022-08-11 06:04:14 | INFO | train_inner | epoch 045:    602 / 1122 loss=8.24, nll_loss=2.765, mask_ins=1.505, word_ins_ml=4.371, word_reposition=1.648, kpe=0.716, ppl=302.29, wps=6424.2, ups=0.31, wpb=20516.7, bsz=256, num_updates=49900, lr=0.000158272, gnorm=2.635, clip=0, loss_scale=256, train_wall=282, wall=0
2022-08-11 06:09:33 | INFO | train_inner | epoch 045:    702 / 1122 loss=8.283, nll_loss=2.801, mask_ins=1.498, word_ins_ml=4.402, word_reposition=1.665, kpe=0.717, ppl=311.49, wps=6481.2, ups=0.31, wpb=20639.7, bsz=256, num_updates=50000, lr=0.000158114, gnorm=2.79, clip=0, loss_scale=256, train_wall=281, wall=0
2022-08-11 06:14:53 | INFO | train_inner | epoch 045:    802 / 1122 loss=8.275, nll_loss=2.803, mask_ins=1.499, word_ins_ml=4.405, word_reposition=1.652, kpe=0.72, ppl=309.74, wps=6403.4, ups=0.31, wpb=20508.2, bsz=256, num_updates=50100, lr=0.000157956, gnorm=3.215, clip=0, loss_scale=256, train_wall=283, wall=0
2022-08-11 06:20:13 | INFO | train_inner | epoch 045:    902 / 1122 loss=8.292, nll_loss=2.795, mask_ins=1.505, word_ins_ml=4.397, word_reposition=1.671, kpe=0.719, ppl=313.48, wps=6444.9, ups=0.31, wpb=20622.3, bsz=256, num_updates=50200, lr=0.000157799, gnorm=2.773, clip=0, loss_scale=369, train_wall=282, wall=0
2022-08-11 06:25:34 | INFO | train_inner | epoch 045:   1002 / 1122 loss=nan, nll_loss=2.805, mask_ins=1.504, word_ins_ml=4.406, word_reposition=1.636, kpe=nan, ppl=nan, wps=6421.1, ups=0.31, wpb=20599.2, bsz=256, num_updates=50300, lr=0.000157642, gnorm=2.539, clip=0, loss_scale=512, train_wall=283, wall=0
2022-08-11 06:30:53 | INFO | train_inner | epoch 045:   1102 / 1122 loss=8.307, nll_loss=2.815, mask_ins=1.501, word_ins_ml=4.414, word_reposition=1.672, kpe=0.719, ppl=316.62, wps=6401.2, ups=0.31, wpb=20459.2, bsz=256, num_updates=50400, lr=0.000157485, gnorm=2.709, clip=0, loss_scale=512, train_wall=282, wall=0
2022-08-11 06:31:56 | INFO | train | epoch 045 | loss nan | nll_loss 2.783 | mask_ins 1.497 | word_ins_ml 4.387 | word_reposition 1.654 | kpe nan | ppl nan | wps 6149.8 | ups 0.3 | wpb 20519.5 | bsz 255.8 | num_updates 50420 | lr 0.000157454 | gnorm 2.745 | clip 0 | loss_scale 395 | train_wall 3223 | wall 0
2022-08-11 06:33:18 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 15.831 | nll_loss 7.415 | mask_ins 2.452 | word_ins_ml 8.652 | word_reposition 3.216 | kpe 1.512 | ppl 58305.7 | wps 12036.7 | wpb 2367.6 | bsz 32 | num_updates 50420 | best_loss 10.679
2022-08-11 06:33:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 45 @ 50420 updates, score 15.831) (writing took 11.97117111645639 seconds)
2022-08-11 06:37:46 | INFO | train_inner | epoch 046:     80 / 1122 loss=8.21, nll_loss=2.786, mask_ins=1.485, word_ins_ml=4.389, word_reposition=1.629, kpe=0.707, ppl=296.18, wps=4963.6, ups=0.24, wpb=20497.4, bsz=253.8, num_updates=50500, lr=0.000157329, gnorm=2.925, clip=0, loss_scale=512, train_wall=281, wall=0
2022-08-11 06:40:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 06:43:10 | INFO | train_inner | epoch 046:    181 / 1122 loss=8.135, nll_loss=2.724, mask_ins=1.482, word_ins_ml=4.335, word_reposition=1.615, kpe=0.703, ppl=281.06, wps=6323.9, ups=0.31, wpb=20460.2, bsz=256, num_updates=50600, lr=0.000157174, gnorm=2.723, clip=0, loss_scale=385, train_wall=286, wall=0
2022-08-11 06:49:04 | INFO | train_inner | epoch 046:    281 / 1122 loss=8.257, nll_loss=2.785, mask_ins=1.494, word_ins_ml=4.389, word_reposition=1.665, kpe=0.709, ppl=306, wps=5828.6, ups=0.28, wpb=20630, bsz=256, num_updates=50700, lr=0.000157019, gnorm=3.008, clip=1, loss_scale=256, train_wall=317, wall=0
2022-08-11 06:54:34 | INFO | train_inner | epoch 046:    381 / 1122 loss=8.191, nll_loss=2.743, mask_ins=1.487, word_ins_ml=4.352, word_reposition=1.642, kpe=0.709, ppl=292.2, wps=6192.6, ups=0.3, wpb=20475.1, bsz=256, num_updates=50800, lr=0.000156864, gnorm=2.816, clip=0, loss_scale=256, train_wall=293, wall=0
2022-08-11 06:59:54 | INFO | train_inner | epoch 046:    481 / 1122 loss=nan, nll_loss=2.769, mask_ins=1.483, word_ins_ml=4.374, word_reposition=1.636, kpe=nan, ppl=nan, wps=6417.3, ups=0.31, wpb=20513, bsz=256, num_updates=50900, lr=0.00015671, gnorm=2.555, clip=0, loss_scale=256, train_wall=282, wall=0
2022-08-11 07:05:14 | INFO | train_inner | epoch 046:    581 / 1122 loss=8.248, nll_loss=2.79, mask_ins=1.489, word_ins_ml=4.393, word_reposition=1.657, kpe=0.709, ppl=303.99, wps=6432.4, ups=0.31, wpb=20610.9, bsz=256, num_updates=51000, lr=0.000156556, gnorm=2.696, clip=0, loss_scale=256, train_wall=283, wall=0
2022-08-11 07:10:34 | INFO | train_inner | epoch 046:    681 / 1122 loss=nan, nll_loss=2.8, mask_ins=1.491, word_ins_ml=4.402, word_reposition=1.655, kpe=nan, ppl=nan, wps=6439, ups=0.31, wpb=20558.4, bsz=256, num_updates=51100, lr=0.000156403, gnorm=3.08, clip=0, loss_scale=353, train_wall=282, wall=0
2022-08-11 07:15:54 | INFO | train_inner | epoch 046:    781 / 1122 loss=8.239, nll_loss=2.769, mask_ins=1.496, word_ins_ml=4.374, word_reposition=1.657, kpe=0.712, ppl=302.12, wps=6401.2, ups=0.31, wpb=20484, bsz=256, num_updates=51200, lr=0.00015625, gnorm=2.765, clip=0, loss_scale=512, train_wall=283, wall=0
2022-08-11 07:21:13 | INFO | train_inner | epoch 046:    881 / 1122 loss=8.196, nll_loss=2.734, mask_ins=1.492, word_ins_ml=4.344, word_reposition=1.647, kpe=0.713, ppl=293.16, wps=6384.6, ups=0.31, wpb=20375.2, bsz=256, num_updates=51300, lr=0.000156098, gnorm=2.773, clip=0, loss_scale=512, train_wall=282, wall=0
2022-08-11 07:26:32 | INFO | train_inner | epoch 046:    981 / 1122 loss=8.269, nll_loss=2.785, mask_ins=1.5, word_ins_ml=4.388, word_reposition=1.669, kpe=0.711, ppl=308.4, wps=6424.4, ups=0.31, wpb=20532.5, bsz=256, num_updates=51400, lr=0.000155946, gnorm=2.743, clip=0, loss_scale=512, train_wall=282, wall=0
2022-08-11 07:31:52 | INFO | train_inner | epoch 046:   1081 / 1122 loss=8.25, nll_loss=2.772, mask_ins=1.497, word_ins_ml=4.377, word_reposition=1.661, kpe=0.714, ppl=304.37, wps=6438.2, ups=0.31, wpb=20600.2, bsz=256, num_updates=51500, lr=0.000155794, gnorm=2.82, clip=0, loss_scale=512, train_wall=282, wall=0
2022-08-11 07:34:02 | INFO | train | epoch 046 | loss nan | nll_loss 2.771 | mask_ins 1.491 | word_ins_ml 4.376 | word_reposition 1.65 | kpe nan | ppl nan | wps 6172.7 | ups 0.3 | wpb 20520 | bsz 255.8 | num_updates 51541 | lr 0.000155732 | gnorm 2.798 | clip 0.1 | loss_scale 395 | train_wall 3211 | wall 0
2022-08-11 07:35:25 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 15.984 | nll_loss 7.406 | mask_ins 2.484 | word_ins_ml 8.643 | word_reposition 3.324 | kpe 1.533 | ppl 64830 | wps 11979.3 | wpb 2367.6 | bsz 32 | num_updates 51541 | best_loss 10.679
2022-08-11 07:35:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 46 @ 51541 updates, score 15.984) (writing took 15.259633416309953 seconds)
2022-08-11 07:38:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 07:38:52 | INFO | train_inner | epoch 047:     60 / 1122 loss=8.271, nll_loss=2.786, mask_ins=1.491, word_ins_ml=4.39, word_reposition=1.685, kpe=0.705, ppl=308.81, wps=4844.1, ups=0.24, wpb=20333.2, bsz=253.8, num_updates=51600, lr=0.000155643, gnorm=2.97, clip=0, loss_scale=593, train_wall=284, wall=0
2022-08-11 07:44:13 | INFO | train_inner | epoch 047:    160 / 1122 loss=8.162, nll_loss=2.748, mask_ins=1.476, word_ins_ml=4.355, word_reposition=1.628, kpe=0.702, ppl=286.41, wps=6381.5, ups=0.31, wpb=20454.5, bsz=256, num_updates=51700, lr=0.000155493, gnorm=3.201, clip=0, loss_scale=512, train_wall=282, wall=0
2022-08-11 07:49:33 | INFO | train_inner | epoch 047:    260 / 1122 loss=nan, nll_loss=2.789, mask_ins=1.489, word_ins_ml=4.392, word_reposition=1.652, kpe=nan, ppl=nan, wps=6409.9, ups=0.31, wpb=20510, bsz=256, num_updates=51800, lr=0.000155342, gnorm=2.929, clip=0, loss_scale=512, train_wall=283, wall=0
2022-08-11 07:52:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 07:52:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-11 07:52:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-08-11 07:55:48 | INFO | train_inner | epoch 047:    363 / 1122 loss=8.181, nll_loss=2.76, mask_ins=1.484, word_ins_ml=4.366, word_reposition=1.625, kpe=0.706, ppl=290.21, wps=5477.8, ups=0.27, wpb=20543.8, bsz=256, num_updates=51900, lr=0.000155193, gnorm=2.899, clip=0, loss_scale=290, train_wall=336, wall=0
2022-08-11 08:01:08 | INFO | train_inner | epoch 047:    463 / 1122 loss=8.226, nll_loss=2.753, mask_ins=1.487, word_ins_ml=4.36, word_reposition=1.674, kpe=0.705, ppl=299.43, wps=6379.8, ups=0.31, wpb=20454.7, bsz=256, num_updates=52000, lr=0.000155043, gnorm=2.747, clip=0, loss_scale=64, train_wall=283, wall=0
2022-08-11 08:06:29 | INFO | train_inner | epoch 047:    563 / 1122 loss=8.261, nll_loss=2.803, mask_ins=1.492, word_ins_ml=4.405, word_reposition=1.657, kpe=0.708, ppl=306.87, wps=6428.6, ups=0.31, wpb=20590.9, bsz=256, num_updates=52100, lr=0.000154895, gnorm=3.619, clip=1, loss_scale=64, train_wall=283, wall=0
2022-08-11 08:11:48 | INFO | train_inner | epoch 047:    663 / 1122 loss=8.173, nll_loss=2.735, mask_ins=1.486, word_ins_ml=4.343, word_reposition=1.64, kpe=0.705, ppl=288.7, wps=6463.8, ups=0.31, wpb=20630, bsz=256, num_updates=52200, lr=0.000154746, gnorm=2.768, clip=0, loss_scale=64, train_wall=282, wall=0
2022-08-11 08:17:08 | INFO | train_inner | epoch 047:    763 / 1122 loss=8.144, nll_loss=2.732, mask_ins=1.48, word_ins_ml=4.342, word_reposition=1.618, kpe=0.705, ppl=282.95, wps=6406.3, ups=0.31, wpb=20496.4, bsz=256, num_updates=52300, lr=0.000154598, gnorm=2.904, clip=0, loss_scale=64, train_wall=283, wall=0
2022-08-11 08:22:27 | INFO | train_inner | epoch 047:    863 / 1122 loss=8.168, nll_loss=2.737, mask_ins=1.483, word_ins_ml=4.345, word_reposition=1.633, kpe=0.707, ppl=287.67, wps=6418, ups=0.31, wpb=20514.8, bsz=256, num_updates=52400, lr=0.000154451, gnorm=2.839, clip=0, loss_scale=87, train_wall=282, wall=0
2022-08-11 08:27:48 | INFO | train_inner | epoch 047:    963 / 1122 loss=nan, nll_loss=2.778, mask_ins=1.494, word_ins_ml=4.382, word_reposition=1.636, kpe=nan, ppl=nan, wps=6452.2, ups=0.31, wpb=20690.6, bsz=256, num_updates=52500, lr=0.000154303, gnorm=2.715, clip=0, loss_scale=128, train_wall=283, wall=0
2022-08-11 08:33:08 | INFO | train_inner | epoch 047:   1063 / 1122 loss=8.189, nll_loss=2.733, mask_ins=1.488, word_ins_ml=4.341, word_reposition=1.65, kpe=0.71, ppl=291.8, wps=6425.5, ups=0.31, wpb=20529.7, bsz=256, num_updates=52600, lr=0.000154157, gnorm=2.774, clip=0, loss_scale=128, train_wall=282, wall=0
2022-08-11 08:36:16 | INFO | train | epoch 047 | loss nan | nll_loss 2.759 | mask_ins 1.487 | word_ins_ml 4.365 | word_reposition 1.645 | kpe nan | ppl nan | wps 6144.9 | ups 0.3 | wpb 20519.8 | bsz 255.8 | num_updates 52659 | lr 0.00015407 | gnorm 2.958 | clip 0.1 | loss_scale 213 | train_wall 3214 | wall 0
2022-08-11 08:37:38 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 15.918 | nll_loss 7.369 | mask_ins 2.442 | word_ins_ml 8.6 | word_reposition 3.357 | kpe 1.519 | ppl 61915.5 | wps 12059.2 | wpb 2367.6 | bsz 32 | num_updates 52659 | best_loss 10.679
2022-08-11 08:37:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 47 @ 52659 updates, score 15.918) (writing took 11.448499696329236 seconds)
2022-08-11 08:40:01 | INFO | train_inner | epoch 048:     41 / 1122 loss=nan, nll_loss=2.775, mask_ins=1.488, word_ins_ml=4.38, word_reposition=1.634, kpe=nan, ppl=nan, wps=4907.7, ups=0.24, wpb=20309.3, bsz=253.8, num_updates=52700, lr=0.00015401, gnorm=3.044, clip=0, loss_scale=128, train_wall=282, wall=0
2022-08-11 08:45:22 | INFO | train_inner | epoch 048:    141 / 1122 loss=8.072, nll_loss=2.675, mask_ins=1.467, word_ins_ml=4.291, word_reposition=1.62, kpe=0.695, ppl=269.15, wps=6400.5, ups=0.31, wpb=20530, bsz=256, num_updates=52800, lr=0.000153864, gnorm=2.75, clip=0, loss_scale=128, train_wall=282, wall=0
2022-08-11 08:50:43 | INFO | train_inner | epoch 048:    241 / 1122 loss=nan, nll_loss=2.742, mask_ins=1.483, word_ins_ml=4.35, word_reposition=1.654, kpe=nan, ppl=nan, wps=6428.4, ups=0.31, wpb=20628.2, bsz=256, num_updates=52900, lr=0.000153719, gnorm=2.865, clip=0, loss_scale=159, train_wall=282, wall=0
2022-08-11 08:56:06 | INFO | train_inner | epoch 048:    341 / 1122 loss=8.128, nll_loss=2.705, mask_ins=1.482, word_ins_ml=4.318, word_reposition=1.628, kpe=0.701, ppl=279.8, wps=6345.9, ups=0.31, wpb=20519, bsz=256, num_updates=53000, lr=0.000153574, gnorm=3.044, clip=1, loss_scale=256, train_wall=284, wall=0
2022-08-11 09:02:19 | INFO | train_inner | epoch 048:    441 / 1122 loss=8.184, nll_loss=2.763, mask_ins=1.485, word_ins_ml=4.369, word_reposition=1.625, kpe=0.705, ppl=290.82, wps=5573.6, ups=0.27, wpb=20745.9, bsz=256, num_updates=53100, lr=0.000153429, gnorm=3.191, clip=0, loss_scale=256, train_wall=332, wall=0
2022-08-11 09:07:39 | INFO | train_inner | epoch 048:    541 / 1122 loss=8.109, nll_loss=2.702, mask_ins=1.469, word_ins_ml=4.316, word_reposition=1.624, kpe=0.7, ppl=276.17, wps=6382.9, ups=0.31, wpb=20431.1, bsz=256, num_updates=53200, lr=0.000153285, gnorm=2.98, clip=0, loss_scale=256, train_wall=282, wall=0
2022-08-11 09:13:00 | INFO | train_inner | epoch 048:    641 / 1122 loss=8.121, nll_loss=2.729, mask_ins=1.476, word_ins_ml=4.338, word_reposition=1.607, kpe=0.7, ppl=278.38, wps=6364.9, ups=0.31, wpb=20444.6, bsz=256, num_updates=53300, lr=0.000153141, gnorm=2.743, clip=0, loss_scale=256, train_wall=284, wall=0
2022-08-11 09:18:20 | INFO | train_inner | epoch 048:    741 / 1122 loss=8.169, nll_loss=2.749, mask_ins=1.48, word_ins_ml=4.356, word_reposition=1.63, kpe=0.703, ppl=287.73, wps=6398.4, ups=0.31, wpb=20455, bsz=256, num_updates=53400, lr=0.000152998, gnorm=2.685, clip=0, loss_scale=287, train_wall=282, wall=0
2022-08-11 09:23:39 | INFO | train_inner | epoch 048:    841 / 1122 loss=8.168, nll_loss=2.728, mask_ins=1.479, word_ins_ml=4.338, word_reposition=1.648, kpe=0.703, ppl=287.53, wps=6432.4, ups=0.31, wpb=20564.5, bsz=256, num_updates=53500, lr=0.000152854, gnorm=2.946, clip=0, loss_scale=512, train_wall=282, wall=0
2022-08-11 09:25:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 09:29:03 | INFO | train_inner | epoch 048:    942 / 1122 loss=8.22, nll_loss=2.765, mask_ins=1.493, word_ins_ml=4.371, word_reposition=1.649, kpe=0.708, ppl=298.19, wps=6366.2, ups=0.31, wpb=20573.9, bsz=256, num_updates=53600, lr=0.000152712, gnorm=2.9, clip=0, loss_scale=319, train_wall=286, wall=0
2022-08-11 09:34:23 | INFO | train_inner | epoch 048:   1042 / 1122 loss=8.204, nll_loss=2.754, mask_ins=1.485, word_ins_ml=4.36, word_reposition=1.659, kpe=0.7, ppl=294.87, wps=6412, ups=0.31, wpb=20555.1, bsz=256, num_updates=53700, lr=0.00015257, gnorm=2.664, clip=0, loss_scale=256, train_wall=283, wall=0
2022-08-11 09:38:38 | INFO | train | epoch 048 | loss nan | nll_loss 2.733 | mask_ins 1.48 | word_ins_ml 4.343 | word_reposition 1.633 | kpe nan | ppl nan | wps 6147.4 | ups 0.3 | wpb 20521 | bsz 255.8 | num_updates 53780 | lr 0.000152456 | gnorm 2.873 | clip 0.1 | loss_scale 263 | train_wall 3220 | wall 0
2022-08-11 09:40:00 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 15.745 | nll_loss 7.348 | mask_ins 2.403 | word_ins_ml 8.582 | word_reposition 3.253 | kpe 1.507 | ppl 54905.2 | wps 12029.6 | wpb 2367.6 | bsz 32 | num_updates 53780 | best_loss 10.679
2022-08-11 09:40:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 48 @ 53780 updates, score 15.745) (writing took 3.9608965292572975 seconds)
2022-08-11 09:41:07 | INFO | train_inner | epoch 049:     20 / 1122 loss=nan, nll_loss=2.76, mask_ins=1.481, word_ins_ml=4.366, word_reposition=1.641, kpe=nan, ppl=nan, wps=5034, ups=0.25, wpb=20352, bsz=253.8, num_updates=53800, lr=0.000152428, gnorm=2.822, clip=0, loss_scale=256, train_wall=281, wall=0
2022-08-11 09:46:27 | INFO | train_inner | epoch 049:    120 / 1122 loss=8.064, nll_loss=2.696, mask_ins=1.464, word_ins_ml=4.31, word_reposition=1.598, kpe=0.692, ppl=267.61, wps=6400.4, ups=0.31, wpb=20435.2, bsz=256, num_updates=53900, lr=0.000152286, gnorm=2.784, clip=0, loss_scale=256, train_wall=282, wall=0
2022-08-11 09:51:46 | INFO | train_inner | epoch 049:    220 / 1122 loss=8.081, nll_loss=2.699, mask_ins=1.46, word_ins_ml=4.312, word_reposition=1.621, kpe=0.688, ppl=270.79, wps=6463.5, ups=0.31, wpb=20628.6, bsz=256, num_updates=54000, lr=0.000152145, gnorm=2.674, clip=0, loss_scale=256, train_wall=282, wall=0
2022-08-11 09:57:07 | INFO | train_inner | epoch 049:    320 / 1122 loss=8.077, nll_loss=2.71, mask_ins=1.472, word_ins_ml=4.321, word_reposition=1.595, kpe=0.689, ppl=270.01, wps=6431.2, ups=0.31, wpb=20625.3, bsz=256, num_updates=54100, lr=0.000152004, gnorm=2.59, clip=0, loss_scale=420, train_wall=283, wall=0
2022-08-11 10:02:28 | INFO | train_inner | epoch 049:    420 / 1122 loss=8.078, nll_loss=2.698, mask_ins=1.464, word_ins_ml=4.311, word_reposition=1.611, kpe=0.691, ppl=270.21, wps=6396.8, ups=0.31, wpb=20544.9, bsz=256, num_updates=54200, lr=0.000151864, gnorm=2.642, clip=0, loss_scale=512, train_wall=283, wall=0
2022-08-11 10:08:38 | INFO | train_inner | epoch 049:    520 / 1122 loss=8.169, nll_loss=2.722, mask_ins=1.48, word_ins_ml=4.332, word_reposition=1.662, kpe=0.696, ppl=287.88, wps=5531.2, ups=0.27, wpb=20490.5, bsz=256, num_updates=54300, lr=0.000151724, gnorm=2.619, clip=0, loss_scale=512, train_wall=333, wall=0
2022-08-11 10:13:58 | INFO | train_inner | epoch 049:    620 / 1122 loss=8.182, nll_loss=2.773, mask_ins=1.47, word_ins_ml=4.378, word_reposition=1.637, kpe=0.696, ppl=290.34, wps=6434.3, ups=0.31, wpb=20552.9, bsz=256, num_updates=54400, lr=0.000151585, gnorm=2.726, clip=0, loss_scale=512, train_wall=282, wall=0
2022-08-11 10:19:19 | INFO | train_inner | epoch 049:    720 / 1122 loss=8.165, nll_loss=2.746, mask_ins=1.475, word_ins_ml=4.354, word_reposition=1.641, kpe=0.695, ppl=287.03, wps=6403.1, ups=0.31, wpb=20590.1, bsz=256, num_updates=54500, lr=0.000151446, gnorm=2.713, clip=0, loss_scale=512, train_wall=283, wall=0
2022-08-11 10:24:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 10:24:45 | INFO | train_inner | epoch 049:    821 / 1122 loss=nan, nll_loss=2.721, mask_ins=1.479, word_ins_ml=4.332, word_reposition=1.614, kpe=nan, ppl=nan, wps=6304.4, ups=0.31, wpb=20531.7, bsz=256, num_updates=54600, lr=0.000151307, gnorm=2.588, clip=0, loss_scale=735, train_wall=286, wall=0
2022-08-11 10:30:07 | INFO | train_inner | epoch 049:    921 / 1122 loss=8.066, nll_loss=2.68, mask_ins=1.472, word_ins_ml=4.295, word_reposition=1.605, kpe=0.694, ppl=267.98, wps=6359.8, ups=0.31, wpb=20460.8, bsz=256, num_updates=54700, lr=0.000151169, gnorm=2.692, clip=0, loss_scale=512, train_wall=284, wall=0
2022-08-11 10:35:27 | INFO | train_inner | epoch 049:   1021 / 1122 loss=8.119, nll_loss=2.711, mask_ins=1.473, word_ins_ml=4.323, word_reposition=1.628, kpe=0.696, ppl=278.09, wps=6401, ups=0.31, wpb=20478.5, bsz=256, num_updates=54800, lr=0.000151031, gnorm=2.66, clip=0, loss_scale=512, train_wall=282, wall=0
2022-08-11 10:40:46 | INFO | train_inner | epoch 049:   1121 / 1122 loss=8.121, nll_loss=2.711, mask_ins=1.475, word_ins_ml=4.323, word_reposition=1.623, kpe=0.7, ppl=278.37, wps=6417.6, ups=0.31, wpb=20490.5, bsz=256, num_updates=54900, lr=0.000150893, gnorm=2.683, clip=0, loss_scale=512, train_wall=282, wall=0
2022-08-11 10:40:48 | INFO | train | epoch 049 | loss nan | nll_loss 2.716 | mask_ins 1.471 | word_ins_ml 4.327 | word_reposition 1.622 | kpe nan | ppl nan | wps 6166.9 | ups 0.3 | wpb 20520.4 | bsz 255.8 | num_updates 54901 | lr 0.000150892 | gnorm 2.68 | clip 0 | loss_scale 474 | train_wall 3220 | wall 0
2022-08-11 10:42:11 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 16.137 | nll_loss 7.385 | mask_ins 2.519 | word_ins_ml 8.626 | word_reposition 3.426 | kpe 1.566 | ppl 72071.8 | wps 11973 | wpb 2367.6 | bsz 32 | num_updates 54901 | best_loss 10.679
2022-08-11 10:42:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 49 @ 54901 updates, score 16.137) (writing took 10.76498918235302 seconds)
2022-08-11 10:47:41 | INFO | train_inner | epoch 050:     99 / 1122 loss=8.101, nll_loss=2.706, mask_ins=1.463, word_ins_ml=4.318, word_reposition=1.637, kpe=0.683, ppl=274.62, wps=4900.2, ups=0.24, wpb=20356.7, bsz=253.8, num_updates=55000, lr=0.000150756, gnorm=2.668, clip=0, loss_scale=512, train_wall=283, wall=0
2022-08-11 10:53:08 | INFO | train_inner | epoch 050:    199 / 1122 loss=nan, nll_loss=2.687, mask_ins=1.462, word_ins_ml=4.302, word_reposition=1.608, kpe=nan, ppl=nan, wps=6269.1, ups=0.31, wpb=20479.7, bsz=256, num_updates=55100, lr=0.000150619, gnorm=2.673, clip=0, loss_scale=512, train_wall=285, wall=0
2022-08-11 10:58:34 | INFO | train_inner | epoch 050:    299 / 1122 loss=8.069, nll_loss=2.687, mask_ins=1.463, word_ins_ml=4.301, word_reposition=1.619, kpe=0.686, ppl=268.46, wps=6290.9, ups=0.31, wpb=20524, bsz=256, num_updates=55200, lr=0.000150482, gnorm=2.687, clip=0, loss_scale=1009, train_wall=286, wall=0
2022-08-11 11:04:01 | INFO | train_inner | epoch 050:    399 / 1122 loss=8.042, nll_loss=2.702, mask_ins=1.457, word_ins_ml=4.314, word_reposition=1.587, kpe=0.684, ppl=263.48, wps=6297.8, ups=0.31, wpb=20573.4, bsz=256, num_updates=55300, lr=0.000150346, gnorm=2.593, clip=0, loss_scale=1024, train_wall=286, wall=0
2022-08-11 11:09:20 | INFO | train_inner | epoch 050:    499 / 1122 loss=8.074, nll_loss=2.705, mask_ins=1.466, word_ins_ml=4.317, word_reposition=1.606, kpe=0.686, ppl=269.54, wps=6462.6, ups=0.31, wpb=20640.4, bsz=256, num_updates=55400, lr=0.00015021, gnorm=2.697, clip=0, loss_scale=1024, train_wall=282, wall=0
2022-08-11 11:15:27 | INFO | train_inner | epoch 050:    599 / 1122 loss=8.102, nll_loss=2.714, mask_ins=1.462, word_ins_ml=4.325, word_reposition=1.625, kpe=0.691, ppl=274.76, wps=5571, ups=0.27, wpb=20450.6, bsz=256, num_updates=55500, lr=0.000150075, gnorm=2.587, clip=0, loss_scale=1024, train_wall=329, wall=0
2022-08-11 11:20:48 | INFO | train_inner | epoch 050:    699 / 1122 loss=nan, nll_loss=2.691, mask_ins=1.462, word_ins_ml=4.305, word_reposition=1.611, kpe=nan, ppl=nan, wps=6421.1, ups=0.31, wpb=20556.3, bsz=256, num_updates=55600, lr=0.00014994, gnorm=2.682, clip=0, loss_scale=1024, train_wall=283, wall=0
2022-08-11 11:26:07 | INFO | train_inner | epoch 050:    799 / 1122 loss=8.125, nll_loss=2.734, mask_ins=1.478, word_ins_ml=4.343, word_reposition=1.61, kpe=0.695, ppl=279.22, wps=6440.8, ups=0.31, wpb=20593.2, bsz=256, num_updates=55700, lr=0.000149805, gnorm=2.64, clip=0, loss_scale=1894, train_wall=283, wall=0
2022-08-11 11:31:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-11 11:31:30 | INFO | train_inner | epoch 050:    900 / 1122 loss=8.091, nll_loss=2.699, mask_ins=1.466, word_ins_ml=4.312, word_reposition=1.622, kpe=0.69, ppl=272.61, wps=6348.8, ups=0.31, wpb=20501.5, bsz=256, num_updates=55800, lr=0.000149671, gnorm=2.593, clip=0, loss_scale=1957, train_wall=285, wall=0
2022-08-11 11:36:50 | INFO | train_inner | epoch 050:   1000 / 1122 loss=8.081, nll_loss=2.709, mask_ins=1.46, word_ins_ml=4.321, word_reposition=1.608, kpe=0.691, ppl=270.78, wps=6420.1, ups=0.31, wpb=20538.9, bsz=256, num_updates=55900, lr=0.000149537, gnorm=2.658, clip=0, loss_scale=1024, train_wall=282, wall=0
2022-08-11 11:42:11 | INFO | train_inner | epoch 050:   1100 / 1122 loss=8.093, nll_loss=2.721, mask_ins=1.466, word_ins_ml=4.332, word_reposition=1.6, kpe=0.695, ppl=272.97, wps=6411.4, ups=0.31, wpb=20550.9, bsz=256, num_updates=56000, lr=0.000149404, gnorm=2.74, clip=0, loss_scale=1024, train_wall=283, wall=0
2022-08-11 11:43:20 | INFO | train | epoch 050 | loss nan | nll_loss 2.706 | mask_ins 1.464 | word_ins_ml 4.318 | word_reposition 1.613 | kpe nan | ppl nan | wps 6131.3 | ups 0.3 | wpb 20521.9 | bsz 255.8 | num_updates 56022 | lr 0.000149374 | gnorm 2.657 | clip 0 | loss_scale 1093 | train_wall 3225 | wall 0
2022-08-11 11:44:43 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 15.845 | nll_loss 7.284 | mask_ins 2.501 | word_ins_ml 8.519 | word_reposition 3.272 | kpe 1.553 | ppl 58865.2 | wps 11959.1 | wpb 2367.6 | bsz 32 | num_updates 56022 | best_loss 10.679
2022-08-11 11:44:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 50 @ 56022 updates, score 15.845) (writing took 8.158357663080096 seconds)
2022-08-11 11:49:03 | INFO | train_inner | epoch 051:     78 / 1122 loss=8.096, nll_loss=2.729, mask_ins=1.463, word_ins_ml=4.338, word_reposition=1.612, kpe=0.683, ppl=273.63, wps=4946.9, ups=0.24, wpb=20389.7, bsz=253.8, num_updates=56100, lr=0.00014927, gnorm=2.743, clip=0, loss_scale=1024, train_wall=283, wall=0
2022-08-11 11:54:26 | INFO | train_inner | epoch 051:    178 / 1122 loss=8.023, nll_loss=2.671, mask_ins=1.456, word_ins_ml=4.287, word_reposition=1.601, kpe=0.678, ppl=260.05, wps=6353.1, ups=0.31, wpb=20511.6, bsz=256, num_updates=56200, lr=0.000149137, gnorm=2.616, clip=0, loss_scale=1024, train_wall=284, wall=0
2022-08-11 11:59:48 | INFO | train_inner | epoch 051:    278 / 1122 loss=8.029, nll_loss=2.695, mask_ins=1.45, word_ins_ml=4.308, word_reposition=1.592, kpe=0.679, ppl=261.16, wps=6390.8, ups=0.31, wpb=20600.7, bsz=256, num_updates=56300, lr=0.000149005, gnorm=2.688, clip=0, loss_scale=1024, train_wall=282, wall=0
2022-08-11 12:00:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-11 12:05:13 | INFO | train_inner | epoch 051:    379 / 1122 loss=8.005, nll_loss=2.676, mask_ins=1.449, word_ins_ml=4.292, word_reposition=1.584, kpe=0.68, ppl=256.84, wps=6357, ups=0.31, wpb=20646.3, bsz=256, num_updates=56400, lr=0.000148873, gnorm=2.796, clip=0, loss_scale=1166, train_wall=286, wall=0
2022-08-11 12:10:33 | INFO | train_inner | epoch 051:    479 / 1122 loss=nan, nll_loss=2.679, mask_ins=1.462, word_ins_ml=4.294, word_reposition=1.593, kpe=nan, ppl=nan, wps=6432.9, ups=0.31, wpb=20591.3, bsz=256, num_updates=56500, lr=0.000148741, gnorm=2.666, clip=0, loss_scale=1024, train_wall=283, wall=0
2022-08-11 12:16:00 | INFO | train_inner | epoch 051:    579 / 1122 loss=8.04, nll_loss=2.7, mask_ins=1.457, word_ins_ml=4.313, word_reposition=1.587, kpe=0.684, ppl=263.21, wps=6308, ups=0.31, wpb=20600.1, bsz=256, num_updates=56600, lr=0.00014861, gnorm=2.687, clip=0, loss_scale=1024, train_wall=288, wall=0
2022-08-11 12:22:09 | INFO | train_inner | epoch 051:    679 / 1122 loss=nan, nll_loss=2.691, mask_ins=1.468, word_ins_ml=4.305, word_reposition=1.615, kpe=nan, ppl=nan, wps=5571.4, ups=0.27, wpb=20571.8, bsz=256, num_updates=56700, lr=0.000148478, gnorm=2.87, clip=0, loss_scale=1024, train_wall=331, wall=0
2022-08-11 12:27:29 | INFO | train_inner | epoch 051:    779 / 1122 loss=8.079, nll_loss=2.686, mask_ins=1.467, word_ins_ml=4.301, word_reposition=1.623, kpe=0.689, ppl=270.5, wps=6415.8, ups=0.31, wpb=20569.1, bsz=256, num_updates=56800, lr=0.000148348, gnorm=2.903, clip=0, loss_scale=1024, train_wall=283, wall=0
2022-08-11 12:32:52 | INFO | train_inner | epoch 051:    879 / 1122 loss=8.006, nll_loss=2.652, mask_ins=1.455, word_ins_ml=4.27, word_reposition=1.594, kpe=0.686, ppl=257.06, wps=6321.6, ups=0.31, wpb=20419.8, bsz=256, num_updates=56900, lr=0.000148217, gnorm=2.646, clip=0, loss_scale=1761, train_wall=285, wall=0
2022-08-11 12:34:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-11 12:38:15 | INFO | train_inner | epoch 051:    980 / 1122 loss=8.116, nll_loss=2.724, mask_ins=1.463, word_ins_ml=4.334, word_reposition=1.626, kpe=0.692, ppl=277.35, wps=6290.2, ups=0.31, wpb=20293.4, bsz=256, num_updates=57000, lr=0.000148087, gnorm=2.672, clip=0, loss_scale=1308, train_wall=285, wall=0
2022-08-11 12:42:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 12:43:39 | INFO | train_inner | epoch 051:   1081 / 1122 loss=8.051, nll_loss=2.683, mask_ins=1.461, word_ins_ml=4.298, word_reposition=1.603, kpe=0.689, ppl=265.26, wps=6312.1, ups=0.31, wpb=20470.7, bsz=256, num_updates=57100, lr=0.000147957, gnorm=2.726, clip=0, loss_scale=902, train_wall=286, wall=0
2022-08-11 12:45:50 | INFO | train | epoch 051 | loss nan | nll_loss 2.688 | mask_ins 1.459 | word_ins_ml 4.302 | word_reposition 1.604 | kpe nan | ppl nan | wps 6122.5 | ups 0.3 | wpb 20519.5 | bsz 255.8 | num_updates 57141 | lr 0.000147904 | gnorm 2.737 | clip 0 | loss_scale 1098 | train_wall 3228 | wall 0
2022-08-11 12:47:13 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 16.158 | nll_loss 7.431 | mask_ins 2.486 | word_ins_ml 8.672 | word_reposition 3.399 | kpe 1.601 | ppl 73110.7 | wps 11946.9 | wpb 2367.6 | bsz 32 | num_updates 57141 | best_loss 10.679
2022-08-11 12:47:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 51 @ 57141 updates, score 16.158) (writing took 6.065206496044993 seconds)
2022-08-11 12:50:29 | INFO | train_inner | epoch 052:     59 / 1122 loss=8.052, nll_loss=2.687, mask_ins=1.457, word_ins_ml=4.301, word_reposition=1.611, kpe=0.683, ppl=265.31, wps=4988.2, ups=0.24, wpb=20412.8, bsz=253.8, num_updates=57200, lr=0.000147828, gnorm=2.875, clip=0, loss_scale=512, train_wall=282, wall=0
2022-08-11 12:53:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 12:55:53 | INFO | train_inner | epoch 052:    160 / 1122 loss=7.971, nll_loss=2.664, mask_ins=1.441, word_ins_ml=4.281, word_reposition=1.576, kpe=0.673, ppl=250.87, wps=6322.3, ups=0.31, wpb=20501.3, bsz=256, num_updates=57300, lr=0.000147699, gnorm=2.76, clip=0, loss_scale=408, train_wall=285, wall=0
2022-08-11 13:01:18 | INFO | train_inner | epoch 052:    260 / 1122 loss=8.041, nll_loss=2.693, mask_ins=1.453, word_ins_ml=4.307, word_reposition=1.603, kpe=0.678, ppl=263.45, wps=6297.6, ups=0.31, wpb=20479.8, bsz=256, num_updates=57400, lr=0.00014757, gnorm=2.876, clip=0, loss_scale=256, train_wall=284, wall=0
2022-08-11 13:06:51 | INFO | train_inner | epoch 052:    360 / 1122 loss=7.99, nll_loss=2.661, mask_ins=1.451, word_ins_ml=4.278, word_reposition=1.584, kpe=0.677, ppl=254.23, wps=6190.4, ups=0.3, wpb=20593.5, bsz=256, num_updates=57500, lr=0.000147442, gnorm=2.769, clip=0, loss_scale=256, train_wall=288, wall=0
2022-08-11 13:12:20 | INFO | train_inner | epoch 052:    460 / 1122 loss=7.946, nll_loss=2.622, mask_ins=1.437, word_ins_ml=4.244, word_reposition=1.595, kpe=0.67, ppl=246.57, wps=6192.6, ups=0.3, wpb=20380.2, bsz=256, num_updates=57600, lr=0.000147314, gnorm=2.798, clip=0, loss_scale=256, train_wall=287, wall=0
2022-08-11 13:17:48 | INFO | train_inner | epoch 052:    560 / 1122 loss=8.079, nll_loss=2.711, mask_ins=1.457, word_ins_ml=4.323, word_reposition=1.618, kpe=0.681, ppl=270.39, wps=6259.7, ups=0.3, wpb=20536.9, bsz=256, num_updates=57700, lr=0.000147186, gnorm=2.881, clip=0, loss_scale=256, train_wall=284, wall=0
2022-08-11 13:23:17 | INFO | train_inner | epoch 052:    660 / 1122 loss=8.042, nll_loss=2.676, mask_ins=1.465, word_ins_ml=4.291, word_reposition=1.604, kpe=0.681, ppl=263.56, wps=6241, ups=0.3, wpb=20531, bsz=256, num_updates=57800, lr=0.000147059, gnorm=2.816, clip=0, loss_scale=330, train_wall=287, wall=0
2022-08-11 13:29:41 | INFO | train_inner | epoch 052:    760 / 1122 loss=nan, nll_loss=2.699, mask_ins=1.458, word_ins_ml=4.312, word_reposition=1.594, kpe=nan, ppl=nan, wps=5363.1, ups=0.26, wpb=20613.4, bsz=256, num_updates=57900, lr=0.000146932, gnorm=2.742, clip=0, loss_scale=512, train_wall=339, wall=0
2022-08-11 13:35:12 | INFO | train_inner | epoch 052:    860 / 1122 loss=8.008, nll_loss=2.671, mask_ins=1.45, word_ins_ml=4.287, word_reposition=1.591, kpe=0.681, ppl=257.43, wps=6261.7, ups=0.3, wpb=20677.9, bsz=256, num_updates=58000, lr=0.000146805, gnorm=2.807, clip=0, loss_scale=512, train_wall=286, wall=0
2022-08-11 13:40:38 | INFO | train_inner | epoch 052:    960 / 1122 loss=nan, nll_loss=2.674, mask_ins=1.454, word_ins_ml=4.289, word_reposition=1.603, kpe=nan, ppl=nan, wps=6293.9, ups=0.31, wpb=20513.7, bsz=256, num_updates=58100, lr=0.000146679, gnorm=2.653, clip=0, loss_scale=512, train_wall=284, wall=0
2022-08-11 13:46:03 | INFO | train_inner | epoch 052:   1060 / 1122 loss=8.03, nll_loss=2.7, mask_ins=1.451, word_ins_ml=4.313, word_reposition=1.585, kpe=0.682, ppl=261.38, wps=6308.1, ups=0.31, wpb=20532.3, bsz=256, num_updates=58200, lr=0.000146553, gnorm=2.732, clip=0, loss_scale=512, train_wall=286, wall=0
2022-08-11 13:49:21 | INFO | train | epoch 052 | loss nan | nll_loss 2.679 | mask_ins 1.452 | word_ins_ml 4.294 | word_reposition 1.595 | kpe nan | ppl nan | wps 6036.5 | ups 0.29 | wpb 20520.8 | bsz 255.8 | num_updates 58262 | lr 0.000146475 | gnorm 2.79 | clip 0 | loss_scale 395 | train_wall 3249 | wall 0
2022-08-11 13:50:43 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 15.718 | nll_loss 7.334 | mask_ins 2.416 | word_ins_ml 8.576 | word_reposition 3.218 | kpe 1.508 | ppl 53914.8 | wps 12063.9 | wpb 2367.6 | bsz 32 | num_updates 58262 | best_loss 10.679
2022-08-11 13:50:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 52 @ 58262 updates, score 15.718) (writing took 8.48119287379086 seconds)
2022-08-11 13:52:53 | INFO | train_inner | epoch 053:     38 / 1122 loss=8.012, nll_loss=2.662, mask_ins=1.456, word_ins_ml=4.279, word_reposition=1.598, kpe=0.679, ppl=258.1, wps=4944.5, ups=0.24, wpb=20275.3, bsz=253.8, num_updates=58300, lr=0.000146427, gnorm=2.869, clip=0, loss_scale=599, train_wall=282, wall=0
2022-08-11 13:54:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 13:58:23 | INFO | train_inner | epoch 053:    139 / 1122 loss=7.934, nll_loss=2.625, mask_ins=1.438, word_ins_ml=4.246, word_reposition=1.581, kpe=0.668, ppl=244.58, wps=6203.1, ups=0.3, wpb=20475, bsz=256, num_updates=58400, lr=0.000146301, gnorm=2.658, clip=0, loss_scale=639, train_wall=287, wall=0
2022-08-11 14:03:50 | INFO | train_inner | epoch 053:    239 / 1122 loss=7.946, nll_loss=2.661, mask_ins=1.438, word_ins_ml=4.278, word_reposition=1.563, kpe=0.667, ppl=246.51, wps=6320.5, ups=0.31, wpb=20656, bsz=256, num_updates=58500, lr=0.000146176, gnorm=2.739, clip=0, loss_scale=512, train_wall=285, wall=0
2022-08-11 14:09:11 | INFO | train_inner | epoch 053:    339 / 1122 loss=7.984, nll_loss=2.659, mask_ins=1.445, word_ins_ml=4.277, word_reposition=1.587, kpe=0.675, ppl=253.13, wps=6399.3, ups=0.31, wpb=20524.5, bsz=256, num_updates=58600, lr=0.000146052, gnorm=2.904, clip=0, loss_scale=512, train_wall=283, wall=0
2022-08-11 14:14:33 | INFO | train_inner | epoch 053:    439 / 1122 loss=8.017, nll_loss=2.675, mask_ins=1.452, word_ins_ml=4.29, word_reposition=1.598, kpe=0.677, ppl=259.06, wps=6358.7, ups=0.31, wpb=20496.2, bsz=256, num_updates=58700, lr=0.000145927, gnorm=2.778, clip=0, loss_scale=512, train_wall=284, wall=0
2022-08-11 14:19:55 | INFO | train_inner | epoch 053:    539 / 1122 loss=8.017, nll_loss=2.672, mask_ins=1.452, word_ins_ml=4.288, word_reposition=1.599, kpe=0.679, ppl=259.12, wps=6411.2, ups=0.31, wpb=20615.1, bsz=256, num_updates=58800, lr=0.000145803, gnorm=2.968, clip=0, loss_scale=512, train_wall=282, wall=0
2022-08-11 14:25:15 | INFO | train_inner | epoch 053:    639 / 1122 loss=nan, nll_loss=2.677, mask_ins=1.446, word_ins_ml=4.293, word_reposition=1.596, kpe=nan, ppl=nan, wps=6403.9, ups=0.31, wpb=20527.6, bsz=256, num_updates=58900, lr=0.000145679, gnorm=2.69, clip=0, loss_scale=840, train_wall=282, wall=0
2022-08-11 14:30:49 | INFO | train_inner | epoch 053:    739 / 1122 loss=7.996, nll_loss=2.677, mask_ins=1.454, word_ins_ml=4.292, word_reposition=1.571, kpe=0.679, ppl=255.37, wps=6163.6, ups=0.3, wpb=20561.7, bsz=256, num_updates=59000, lr=0.000145556, gnorm=2.822, clip=0, loss_scale=1024, train_wall=296, wall=0
2022-08-11 14:33:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 14:36:51 | INFO | train_inner | epoch 053:    840 / 1122 loss=8.005, nll_loss=2.65, mask_ins=1.456, word_ins_ml=4.269, word_reposition=1.604, kpe=0.677, ppl=256.92, wps=5699, ups=0.28, wpb=20639.2, bsz=256, num_updates=59100, lr=0.000145432, gnorm=2.771, clip=0, loss_scale=705, train_wall=322, wall=0
2022-08-11 14:42:22 | INFO | train_inner | epoch 053:    940 / 1122 loss=7.96, nll_loss=2.647, mask_ins=1.436, word_ins_ml=4.265, word_reposition=1.583, kpe=0.675, ppl=249.02, wps=6156.6, ups=0.3, wpb=20395.8, bsz=256, num_updates=59200, lr=0.00014531, gnorm=2.724, clip=0, loss_scale=512, train_wall=287, wall=0
2022-08-11 14:47:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 14:47:58 | INFO | train_inner | epoch 053:   1041 / 1122 loss=nan, nll_loss=2.659, mask_ins=1.45, word_ins_ml=4.277, word_reposition=1.592, kpe=nan, ppl=nan, wps=6107.5, ups=0.3, wpb=20488, bsz=256, num_updates=59300, lr=0.000145187, gnorm=2.816, clip=0, loss_scale=469, train_wall=289, wall=0
2022-08-11 14:52:18 | INFO | train | epoch 053 | loss nan | nll_loss 2.661 | mask_ins 1.447 | word_ins_ml 4.278 | word_reposition 1.59 | kpe nan | ppl nan | wps 6078.7 | ups 0.3 | wpb 20519 | bsz 255.8 | num_updates 59381 | lr 0.000145088 | gnorm 2.784 | clip 0 | loss_scale 601 | train_wall 3235 | wall 0
2022-08-11 14:53:42 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 15.86 | nll_loss 7.303 | mask_ins 2.465 | word_ins_ml 8.553 | word_reposition 3.298 | kpe 1.545 | ppl 59488.4 | wps 11895.4 | wpb 2367.6 | bsz 32 | num_updates 59381 | best_loss 10.679
2022-08-11 14:53:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_finetune_cased/checkpoint_last.pt (epoch 53 @ 59381 updates, score 15.86) (writing took 9.238745959475636 seconds)
2022-08-11 14:54:52 | INFO | train_inner | epoch 054:     19 / 1122 loss=8.026, nll_loss=2.687, mask_ins=1.452, word_ins_ml=4.301, word_reposition=1.597, kpe=0.676, ppl=260.62, wps=4911.3, ups=0.24, wpb=20332, bsz=253.8, num_updates=59400, lr=0.000145065, gnorm=2.845, clip=0, loss_scale=256, train_wall=284, wall=0
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
