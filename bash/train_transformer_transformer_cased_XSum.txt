nohup: ignoring input
2022-10-12 06:59:36 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:18406
2022-10-12 06:59:36 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:18406
2022-10-12 06:59:36 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:18406
2022-10-12 06:59:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-10-12 06:59:36 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:18406
2022-10-12 06:59:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-10-12 06:59:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-10-12 06:59:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-10-12 06:59:36 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-10-12 06:59:36 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-10-12 06:59:36 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-10-12 06:59:36 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-10-12 06:59:36 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-10-12 06:59:36 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-10-12 06:59:36 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-10-12 06:59:36 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-10-12 06:59:43 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=8100, max_sentences=None, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=8100, max_sentences_valid=None, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:18406', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_transformer_transformer_kpe_cased_XSum', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='6,6,6', layers_num='6,6,6', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-XSum', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510_XSum', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=False, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='transformer', decoder='transformer', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-10-12 06:59:43 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-10-12 06:59:43 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-10-12 06:59:43 | INFO | fairseq.data.data_utils | loaded 11321 examples from: ../data-bin-bert-cased-XSum/valid.source-target.source
2022-10-12 06:59:43 | INFO | fairseq.data.data_utils | loaded 11321 examples from: ../data-bin-bert-cased-XSum/valid.source-target.target
2022-10-12 06:59:43 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-XSum valid source-target 11321 examples
2022-10-12 06:59:43 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-10-12 06:59:43 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
2022-10-12 06:59:45 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): EditorTransformerEncoder(
    (embed_tokens): Embedding(28996, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): EditorTransformerDecoder(
    (embed_tokens): Embedding(28996, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
    (embed_mask_ins): Embedding(256, 1536)
    (layers_reposition): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-10-12 06:59:45 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-10-12 06:59:45 | INFO | fairseq_cli.train | num. model params: 151068672 (num. trained: 151068672)
2022-10-12 06:59:45 | INFO | fairseq_cli.train | num. Encoder model params: 55746816 (Encoder num. trained: 55746816)
2022-10-12 06:59:45 | INFO | fairseq_cli.train | num. Decoder model params: 117590784 (Decoder num. trained: 117590784)
2022-10-12 06:59:48 | INFO | fairseq_cli.train | training on 4 GPUs
2022-10-12 06:59:48 | INFO | fairseq_cli.train | max tokens per GPU = 8100 and max sentences per GPU = None
2022-10-12 06:59:48 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt
2022-10-12 06:59:48 | INFO | fairseq.trainer | loading train data for epoch 1
2022-10-12 06:59:48 | INFO | fairseq.data.data_utils | loaded 203998 examples from: ../data-bin-bert-cased-XSum/train.source-target.source
2022-10-12 06:59:48 | INFO | fairseq.data.data_utils | loaded 203998 examples from: ../data-bin-bert-cased-XSum/train.source-target.target
2022-10-12 06:59:48 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-XSum train source-target 203998 examples
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-10-12 07:01:36 | INFO | train_inner | epoch 001:    100 / 459 loss=23.91, nll_loss=13.957, mask_ins=7.568, word_ins_ml=14.068, word_reposition=2.273, ppl=1.57603e+07, wps=13825.8, ups=0.94, wpb=14706.8, bsz=449.1, num_updates=100, lr=1.0098e-05, gnorm=18.579, clip=0, loss_scale=128, train_wall=106, wall=109
2022-10-12 07:03:21 | INFO | train_inner | epoch 001:    200 / 459 loss=17.937, nll_loss=12.264, mask_ins=4.362, word_ins_ml=12.553, word_reposition=1.023, ppl=251009, wps=14172.1, ups=0.96, wpb=14771.2, bsz=444.7, num_updates=200, lr=2.0096e-05, gnorm=18.891, clip=0, loss_scale=128, train_wall=103, wall=213
2022-10-12 07:05:05 | INFO | train_inner | epoch 001:    300 / 459 loss=14.481, nll_loss=10.886, mask_ins=2.221, word_ins_ml=11.365, word_reposition=0.895, ppl=22868.1, wps=14218.4, ups=0.96, wpb=14847.9, bsz=452.1, num_updates=300, lr=3.0094e-05, gnorm=4.041, clip=0, loss_scale=128, train_wall=103, wall=317
2022-10-12 07:06:48 | INFO | train_inner | epoch 001:    400 / 459 loss=13.723, nll_loss=10.288, mask_ins=1.929, word_ins_ml=10.913, word_reposition=0.881, ppl=13522.5, wps=13931.8, ups=0.97, wpb=14421, bsz=434.5, num_updates=400, lr=4.0092e-05, gnorm=1.803, clip=0, loss_scale=128, train_wall=103, wall=421
2022-10-12 07:07:49 | INFO | train | epoch 001 | loss 17.017 | nll_loss 11.636 | mask_ins 3.749 | word_ins_ml 12.048 | word_reposition 1.22 | ppl 132598 | wps 14013 | ups 0.96 | wpb 14638.6 | bsz 444 | num_updates 459 | lr 4.59908e-05 | gnorm 9.677 | clip 0 | loss_scale 128 | train_wall 476 | wall 482
2022-10-12 07:07:59 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.197 | nll_loss 10.71 | mask_ins 2.127 | word_ins_ml 11.318 | word_reposition 0.753 | ppl 18784.7 | wps 35104.1 | wpb 1628.7 | bsz 55.8 | num_updates 459
2022-10-12 07:08:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 1 @ 459 updates, score 14.197) (writing took 2.7048853500018595 seconds)
2022-10-12 07:08:45 | INFO | train_inner | epoch 002:     41 / 459 loss=13.595, nll_loss=10.157, mask_ins=1.88, word_ins_ml=10.824, word_reposition=0.891, ppl=12370.2, wps=12441.5, ups=0.86, wpb=14441, bsz=438.2, num_updates=500, lr=5.009e-05, gnorm=1.655, clip=0, loss_scale=128, train_wall=103, wall=537
2022-10-12 07:10:37 | INFO | train_inner | epoch 002:    141 / 459 loss=13.484, nll_loss=10.07, mask_ins=1.859, word_ins_ml=10.751, word_reposition=0.874, ppl=11460.9, wps=13156.1, ups=0.89, wpb=14756, bsz=448.8, num_updates=600, lr=6.0088e-05, gnorm=1.607, clip=0, loss_scale=242, train_wall=111, wall=649
2022-10-12 07:12:24 | INFO | train_inner | epoch 002:    241 / 459 loss=13.344, nll_loss=9.953, mask_ins=1.838, word_ins_ml=10.648, word_reposition=0.858, ppl=10400.9, wps=13654.9, ups=0.94, wpb=14599.4, bsz=441.9, num_updates=700, lr=7.0086e-05, gnorm=1.51, clip=0, loss_scale=256, train_wall=106, wall=756
2022-10-12 07:14:07 | INFO | train_inner | epoch 002:    341 / 459 loss=13.238, nll_loss=9.839, mask_ins=1.843, word_ins_ml=10.549, word_reposition=0.846, ppl=9664.46, wps=14047, ups=0.96, wpb=14580.4, bsz=444.2, num_updates=800, lr=8.0084e-05, gnorm=1.544, clip=0, loss_scale=256, train_wall=103, wall=860
2022-10-12 07:15:53 | INFO | train_inner | epoch 002:    441 / 459 loss=13.125, nll_loss=9.742, mask_ins=1.814, word_ins_ml=10.466, word_reposition=0.845, ppl=8931.19, wps=13988.1, ups=0.94, wpb=14819.9, bsz=449.2, num_updates=900, lr=9.0082e-05, gnorm=1.546, clip=0, loss_scale=256, train_wall=105, wall=966
2022-10-12 07:16:12 | INFO | train | epoch 002 | loss 13.315 | nll_loss 9.914 | mask_ins 1.841 | word_ins_ml 10.615 | word_reposition 0.859 | ppl 10190 | wps 13384 | ups 0.91 | wpb 14642.3 | bsz 444.1 | num_updates 918 | lr 9.18816e-05 | gnorm 1.534 | clip 0 | loss_scale 241 | train_wall 486 | wall 984
2022-10-12 07:16:21 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 14.656 | nll_loss 11.133 | mask_ins 2.194 | word_ins_ml 11.696 | word_reposition 0.767 | ppl 25822.5 | wps 34885.6 | wpb 1628.7 | bsz 55.8 | num_updates 918 | best_loss 14.197
2022-10-12 07:16:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 2 @ 918 updates, score 14.656) (writing took 2.842636838002363 seconds)
2022-10-12 07:17:48 | INFO | train_inner | epoch 003:     82 / 459 loss=13.041, nll_loss=9.639, mask_ins=1.809, word_ins_ml=10.377, word_reposition=0.854, ppl=8427.08, wps=12702.2, ups=0.87, wpb=14589.7, bsz=442.4, num_updates=1000, lr=0.00010008, gnorm=1.518, clip=0, loss_scale=256, train_wall=102, wall=1081
2022-10-12 07:19:32 | INFO | train_inner | epoch 003:    182 / 459 loss=12.964, nll_loss=9.543, mask_ins=1.814, word_ins_ml=10.295, word_reposition=0.855, ppl=7991.34, wps=14513.1, ups=0.97, wpb=15015.9, bsz=455.2, num_updates=1100, lr=0.000110078, gnorm=1.518, clip=0, loss_scale=453, train_wall=103, wall=1184
2022-10-12 07:21:15 | INFO | train_inner | epoch 003:    282 / 459 loss=12.877, nll_loss=9.464, mask_ins=1.806, word_ins_ml=10.228, word_reposition=0.843, ppl=7522.58, wps=14062.5, ups=0.96, wpb=14582.9, bsz=441.7, num_updates=1200, lr=0.000120076, gnorm=1.475, clip=0, loss_scale=512, train_wall=103, wall=1288
2022-10-12 07:22:59 | INFO | train_inner | epoch 003:    382 / 459 loss=12.794, nll_loss=9.373, mask_ins=1.8, word_ins_ml=10.149, word_reposition=0.845, ppl=7103.62, wps=13740.8, ups=0.96, wpb=14285.5, bsz=432.2, num_updates=1300, lr=0.000130074, gnorm=1.49, clip=0, loss_scale=512, train_wall=103, wall=1392
2022-10-12 07:24:19 | INFO | train | epoch 003 | loss 12.88 | nll_loss 9.453 | mask_ins 1.807 | word_ins_ml 10.218 | word_reposition 0.856 | ppl 7540.29 | wps 13778.3 | ups 0.94 | wpb 14641.9 | bsz 444.1 | num_updates 1377 | lr 0.000137772 | gnorm 1.51 | clip 0 | loss_scale 453 | train_wall 471 | wall 1472
2022-10-12 07:24:29 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 14.773 | nll_loss 11.167 | mask_ins 2.209 | word_ins_ml 11.774 | word_reposition 0.789 | ppl 27987.8 | wps 35035 | wpb 1628.7 | bsz 55.8 | num_updates 1377 | best_loss 14.197
2022-10-12 07:24:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 3 @ 1377 updates, score 14.773) (writing took 2.8298250780062517 seconds)
2022-10-12 07:24:56 | INFO | train_inner | epoch 004:     23 / 459 loss=12.72, nll_loss=9.225, mask_ins=1.809, word_ins_ml=10.019, word_reposition=0.892, ppl=6746.19, wps=12630.2, ups=0.86, wpb=14688.4, bsz=448.3, num_updates=1400, lr=0.000140072, gnorm=1.608, clip=0, loss_scale=512, train_wall=103, wall=1508
2022-10-12 07:26:39 | INFO | train_inner | epoch 004:    123 / 459 loss=12.512, nll_loss=8.986, mask_ins=1.789, word_ins_ml=9.807, word_reposition=0.916, ppl=5841.79, wps=14168.9, ups=0.96, wpb=14689.6, bsz=445, num_updates=1500, lr=0.00015007, gnorm=1.591, clip=0, loss_scale=512, train_wall=103, wall=1612
2022-10-12 07:28:24 | INFO | train_inner | epoch 004:    223 / 459 loss=12.397, nll_loss=8.834, mask_ins=1.795, word_ins_ml=9.674, word_reposition=0.928, ppl=5394.66, wps=14211.6, ups=0.96, wpb=14848.3, bsz=449.1, num_updates=1600, lr=0.000160068, gnorm=1.836, clip=0, loss_scale=845, train_wall=104, wall=1716
2022-10-12 07:30:07 | INFO | train_inner | epoch 004:    323 / 459 loss=12.291, nll_loss=8.709, mask_ins=1.788, word_ins_ml=9.565, word_reposition=0.937, ppl=5009.75, wps=13951.3, ups=0.97, wpb=14327.9, bsz=432.7, num_updates=1700, lr=0.000170066, gnorm=2.049, clip=0, loss_scale=1024, train_wall=102, wall=1819
2022-10-12 07:31:50 | INFO | train_inner | epoch 004:    423 / 459 loss=12.18, nll_loss=8.59, mask_ins=1.777, word_ins_ml=9.463, word_reposition=0.94, ppl=4640.78, wps=14279.5, ups=0.96, wpb=14800.1, bsz=451, num_updates=1800, lr=0.000180064, gnorm=2.142, clip=0, loss_scale=1024, train_wall=103, wall=1922
2022-10-12 07:32:27 | INFO | train | epoch 004 | loss 12.347 | nll_loss 8.78 | mask_ins 1.788 | word_ins_ml 9.628 | word_reposition 0.931 | ppl 5209.49 | wps 13788.5 | ups 0.94 | wpb 14637.9 | bsz 444 | num_updates 1836 | lr 0.000183663 | gnorm 1.913 | clip 0 | loss_scale 848 | train_wall 471 | wall 1959
2022-10-12 07:32:36 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 14.385 | nll_loss 10.994 | mask_ins 1.924 | word_ins_ml 11.643 | word_reposition 0.817 | ppl 21396.3 | wps 34890 | wpb 1628.7 | bsz 55.8 | num_updates 1836 | best_loss 14.197
2022-10-12 07:32:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 4 @ 1836 updates, score 14.385) (writing took 2.8441697239904897 seconds)
2022-10-12 07:33:46 | INFO | train_inner | epoch 005:     64 / 459 loss=12.106, nll_loss=8.491, mask_ins=1.785, word_ins_ml=9.377, word_reposition=0.944, ppl=4407.13, wps=12761.4, ups=0.86, wpb=14762.5, bsz=447.6, num_updates=1900, lr=0.000190062, gnorm=2.139, clip=0, loss_scale=1024, train_wall=103, wall=2038
2022-10-12 07:35:28 | INFO | train_inner | epoch 005:    164 / 459 loss=12.014, nll_loss=8.378, mask_ins=1.79, word_ins_ml=9.279, word_reposition=0.945, ppl=4136.47, wps=14095.1, ups=0.98, wpb=14453.5, bsz=437.6, num_updates=2000, lr=0.00020006, gnorm=2.152, clip=0, loss_scale=1024, train_wall=102, wall=2141
2022-10-12 07:37:11 | INFO | train_inner | epoch 005:    264 / 459 loss=11.953, nll_loss=8.319, mask_ins=1.773, word_ins_ml=9.23, word_reposition=0.95, ppl=3963.5, wps=14146.6, ups=0.97, wpb=14545.7, bsz=443.1, num_updates=2100, lr=0.000210058, gnorm=2.157, clip=0, loss_scale=1567, train_wall=102, wall=2243
2022-10-12 07:38:55 | INFO | train_inner | epoch 005:    364 / 459 loss=11.909, nll_loss=8.252, mask_ins=1.77, word_ins_ml=9.172, word_reposition=0.966, ppl=3845.38, wps=14116.6, ups=0.96, wpb=14693, bsz=443.5, num_updates=2200, lr=0.000220056, gnorm=2.084, clip=0, loss_scale=2048, train_wall=103, wall=2348
2022-10-12 07:40:34 | INFO | train | epoch 005 | loss 11.952 | nll_loss 8.308 | mask_ins 1.778 | word_ins_ml 9.22 | word_reposition 0.954 | ppl 3963.15 | wps 13801.1 | ups 0.94 | wpb 14641.8 | bsz 444.1 | num_updates 2295 | lr 0.000229554 | gnorm 2.14 | clip 0 | loss_scale 1577 | train_wall 471 | wall 2446
2022-10-12 07:40:43 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 14.388 | nll_loss 10.973 | mask_ins 1.83 | word_ins_ml 11.648 | word_reposition 0.91 | ppl 21437.1 | wps 35038.9 | wpb 1628.7 | bsz 55.8 | num_updates 2295 | best_loss 14.197
2022-10-12 07:40:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 5 @ 2295 updates, score 14.388) (writing took 2.836247061000904 seconds)
2022-10-12 07:40:51 | INFO | train_inner | epoch 006:      5 / 459 loss=11.841, nll_loss=8.168, mask_ins=1.775, word_ins_ml=9.1, word_reposition=0.965, ppl=3668.24, wps=12569.8, ups=0.86, wpb=14544.2, bsz=441.9, num_updates=2300, lr=0.000230054, gnorm=2.201, clip=0, loss_scale=2048, train_wall=103, wall=2463
2022-10-12 07:42:36 | INFO | train_inner | epoch 006:    105 / 459 loss=11.719, nll_loss=8.033, mask_ins=1.763, word_ins_ml=8.982, word_reposition=0.973, ppl=3369.93, wps=13667.4, ups=0.95, wpb=14391.3, bsz=439.1, num_updates=2400, lr=0.000240052, gnorm=2.124, clip=0, loss_scale=2048, train_wall=104, wall=2569
2022-10-12 07:44:24 | INFO | train_inner | epoch 006:    205 / 459 loss=11.625, nll_loss=7.935, mask_ins=1.75, word_ins_ml=8.897, word_reposition=0.978, ppl=3157.85, wps=13628.5, ups=0.93, wpb=14681.6, bsz=444.6, num_updates=2500, lr=0.00025005, gnorm=2.052, clip=0, loss_scale=2048, train_wall=107, wall=2676
2022-10-12 07:46:09 | INFO | train_inner | epoch 006:    305 / 459 loss=11.57, nll_loss=7.862, mask_ins=1.751, word_ins_ml=8.835, word_reposition=0.984, ppl=3040.52, wps=14011.3, ups=0.95, wpb=14716.8, bsz=448.5, num_updates=2600, lr=0.000260048, gnorm=1.961, clip=0, loss_scale=2888, train_wall=104, wall=2781
2022-10-12 07:47:54 | INFO | train_inner | epoch 006:    405 / 459 loss=11.5, nll_loss=7.756, mask_ins=1.747, word_ins_ml=8.742, word_reposition=1.011, ppl=2895.91, wps=14017.9, ups=0.96, wpb=14645.8, bsz=440.4, num_updates=2700, lr=0.000270046, gnorm=2.045, clip=0, loss_scale=4096, train_wall=104, wall=2886
2022-10-12 07:48:52 | INFO | train | epoch 006 | loss 11.584 | nll_loss 7.872 | mask_ins 1.752 | word_ins_ml 8.843 | word_reposition 0.989 | ppl 3069.31 | wps 13492.9 | ups 0.92 | wpb 14638 | bsz 443.9 | num_updates 2754 | lr 0.000275445 | gnorm 2.052 | clip 0 | loss_scale 2918 | train_wall 482 | wall 2944
2022-10-12 07:49:01 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 14.17 | nll_loss 10.631 | mask_ins 1.842 | word_ins_ml 11.373 | word_reposition 0.955 | ppl 18430.7 | wps 34981.6 | wpb 1628.7 | bsz 55.8 | num_updates 2754 | best_loss 14.17
2022-10-12 07:49:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 6 @ 2754 updates, score 14.17) (writing took 4.12872994299687 seconds)
2022-10-12 07:49:53 | INFO | train_inner | epoch 007:     46 / 459 loss=11.379, nll_loss=7.623, mask_ins=1.743, word_ins_ml=8.627, word_reposition=1.009, ppl=2662.74, wps=12316, ups=0.84, wpb=14660.8, bsz=442.1, num_updates=2800, lr=0.000280044, gnorm=2.071, clip=0, loss_scale=4096, train_wall=105, wall=3005
2022-10-12 07:50:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-10-12 07:51:39 | INFO | train_inner | epoch 007:    147 / 459 loss=11.208, nll_loss=7.44, mask_ins=1.732, word_ins_ml=8.467, word_reposition=1.009, ppl=2366.04, wps=14271.9, ups=0.94, wpb=15224.5, bsz=463.8, num_updates=2900, lr=0.000290042, gnorm=2.061, clip=0, loss_scale=2656, train_wall=106, wall=3112
2022-10-12 07:53:25 | INFO | train_inner | epoch 007:    247 / 459 loss=11.124, nll_loss=7.338, mask_ins=1.723, word_ins_ml=8.379, word_reposition=1.022, ppl=2232.18, wps=13723.5, ups=0.95, wpb=14461.7, bsz=439.3, num_updates=3000, lr=0.00030004, gnorm=1.945, clip=0, loss_scale=2048, train_wall=104, wall=3217
2022-10-12 07:55:09 | INFO | train_inner | epoch 007:    347 / 459 loss=11.026, nll_loss=7.229, mask_ins=1.716, word_ins_ml=8.284, word_reposition=1.026, ppl=2085.07, wps=13883.5, ups=0.96, wpb=14474.1, bsz=438.1, num_updates=3100, lr=0.000310038, gnorm=1.976, clip=0, loss_scale=2048, train_wall=103, wall=3321
2022-10-12 07:56:53 | INFO | train_inner | epoch 007:    447 / 459 loss=10.867, nll_loss=7.077, mask_ins=1.692, word_ins_ml=8.152, word_reposition=1.024, ppl=1868.04, wps=14105, ups=0.96, wpb=14623.5, bsz=445.2, num_updates=3200, lr=0.000320036, gnorm=1.926, clip=0, loss_scale=2048, train_wall=103, wall=3425
2022-10-12 07:57:05 | INFO | train | epoch 007 | loss 11.08 | nll_loss 7.295 | mask_ins 1.719 | word_ins_ml 8.342 | word_reposition 1.02 | ppl 2165.39 | wps 13590.2 | ups 0.93 | wpb 14637.1 | bsz 443.9 | num_updates 3212 | lr 0.000321236 | gnorm 2.009 | clip 0 | loss_scale 2387 | train_wall 476 | wall 3437
2022-10-12 07:57:14 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 14.086 | nll_loss 10.426 | mask_ins 1.784 | word_ins_ml 11.21 | word_reposition 1.092 | ppl 17396.1 | wps 35030.5 | wpb 1628.7 | bsz 55.8 | num_updates 3212 | best_loss 14.086
2022-10-12 07:57:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 7 @ 3212 updates, score 14.086) (writing took 3.9985893749981187 seconds)
2022-10-12 07:58:50 | INFO | train_inner | epoch 008:     88 / 459 loss=10.707, nll_loss=6.875, mask_ins=1.696, word_ins_ml=7.975, word_reposition=1.037, ppl=1671.8, wps=12213.3, ups=0.85, wpb=14313.2, bsz=430.8, num_updates=3300, lr=0.000330034, gnorm=2.054, clip=0, loss_scale=2048, train_wall=103, wall=3542
2022-10-12 08:00:35 | INFO | train_inner | epoch 008:    188 / 459 loss=10.524, nll_loss=6.718, mask_ins=1.658, word_ins_ml=7.839, word_reposition=1.026, ppl=1471.97, wps=14121.3, ups=0.95, wpb=14915, bsz=453.6, num_updates=3400, lr=0.000340032, gnorm=2.064, clip=0, loss_scale=3256, train_wall=105, wall=3648
2022-10-12 08:02:21 | INFO | train_inner | epoch 008:    288 / 459 loss=10.406, nll_loss=6.602, mask_ins=1.649, word_ins_ml=7.739, word_reposition=1.017, ppl=1356.45, wps=13782.5, ups=0.95, wpb=14490.6, bsz=437.4, num_updates=3500, lr=0.00035003, gnorm=2.076, clip=0, loss_scale=4096, train_wall=104, wall=3753
2022-10-12 08:04:06 | INFO | train_inner | epoch 008:    388 / 459 loss=10.276, nll_loss=6.473, mask_ins=1.63, word_ins_ml=7.628, word_reposition=1.019, ppl=1240.16, wps=13996.3, ups=0.94, wpb=14823.8, bsz=449.7, num_updates=3600, lr=0.000360028, gnorm=1.892, clip=0, loss_scale=4096, train_wall=105, wall=3859
2022-10-12 08:05:21 | INFO | train | epoch 008 | loss 10.427 | nll_loss 6.615 | mask_ins 1.652 | word_ins_ml 7.751 | word_reposition 1.025 | ppl 1377.01 | wps 13555.9 | ups 0.93 | wpb 14640.6 | bsz 444 | num_updates 3671 | lr 0.000367127 | gnorm 2.031 | clip 0 | loss_scale 3520 | train_wall 478 | wall 3933
2022-10-12 08:05:30 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 13.725 | nll_loss 10.24 | mask_ins 1.705 | word_ins_ml 11.064 | word_reposition 0.957 | ppl 13539.2 | wps 35036.7 | wpb 1628.7 | bsz 55.8 | num_updates 3671 | best_loss 13.725
2022-10-12 08:05:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 8 @ 3671 updates, score 13.725) (writing took 4.141280297990306 seconds)
2022-10-12 08:05:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-10-12 08:06:05 | INFO | train_inner | epoch 009:     30 / 459 loss=10.205, nll_loss=6.379, mask_ins=1.625, word_ins_ml=7.547, word_reposition=1.034, ppl=1180.68, wps=12066.9, ups=0.84, wpb=14313.4, bsz=435.1, num_updates=3700, lr=0.000370026, gnorm=2.455, clip=0, loss_scale=3589, train_wall=104, wall=3977
2022-10-12 08:07:51 | INFO | train_inner | epoch 009:    130 / 459 loss=10.022, nll_loss=6.21, mask_ins=1.603, word_ins_ml=7.401, word_reposition=1.018, ppl=1039.53, wps=13986.1, ups=0.94, wpb=14848.1, bsz=452.4, num_updates=3800, lr=0.000380024, gnorm=1.764, clip=0, loss_scale=2048, train_wall=105, wall=4083
2022-10-12 08:09:36 | INFO | train_inner | epoch 009:    230 / 459 loss=9.936, nll_loss=6.122, mask_ins=1.594, word_ins_ml=7.326, word_reposition=1.016, ppl=979.34, wps=13932.7, ups=0.95, wpb=14593.1, bsz=443.4, num_updates=3900, lr=0.000390022, gnorm=1.862, clip=0, loss_scale=2048, train_wall=104, wall=4188
2022-10-12 08:11:21 | INFO | train_inner | epoch 009:    330 / 459 loss=9.904, nll_loss=6.109, mask_ins=1.583, word_ins_ml=7.317, word_reposition=1.003, ppl=957.97, wps=13936.8, ups=0.95, wpb=14618.4, bsz=442.3, num_updates=4000, lr=0.00040002, gnorm=1.981, clip=0, loss_scale=2048, train_wall=104, wall=4293
2022-10-12 08:13:06 | INFO | train_inner | epoch 009:    430 / 459 loss=9.817, nll_loss=6.048, mask_ins=1.563, word_ins_ml=7.265, word_reposition=0.988, ppl=902.07, wps=14206.4, ups=0.95, wpb=14954.9, bsz=452.5, num_updates=4100, lr=0.000410018, gnorm=1.905, clip=0, loss_scale=2048, train_wall=104, wall=4398
2022-10-12 08:13:36 | INFO | train | epoch 009 | loss 9.926 | nll_loss 6.128 | mask_ins 1.587 | word_ins_ml 7.332 | word_reposition 1.007 | ppl 972.96 | wps 13539 | ups 0.92 | wpb 14642.9 | bsz 444.1 | num_updates 4129 | lr 0.000412917 | gnorm 1.979 | clip 0 | loss_scale 2070 | train_wall 478 | wall 4428
2022-10-12 08:13:45 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 13.681 | nll_loss 9.97 | mask_ins 1.844 | word_ins_ml 10.833 | word_reposition 1.005 | ppl 13132.1 | wps 34996.1 | wpb 1628.7 | bsz 55.8 | num_updates 4129 | best_loss 13.681
2022-10-12 08:13:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 9 @ 4129 updates, score 13.681) (writing took 4.22281645599287 seconds)
2022-10-12 08:15:03 | INFO | train_inner | epoch 010:     71 / 459 loss=9.725, nll_loss=5.933, mask_ins=1.566, word_ins_ml=7.164, word_reposition=0.995, ppl=846.32, wps=12121.2, ups=0.85, wpb=14192.3, bsz=430.4, num_updates=4200, lr=0.000420016, gnorm=2.279, clip=0, loss_scale=2314, train_wall=103, wall=4515
2022-10-12 08:16:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-10-12 08:16:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-10-12 08:16:51 | INFO | train_inner | epoch 010:    173 / 459 loss=9.644, nll_loss=5.885, mask_ins=1.545, word_ins_ml=7.125, word_reposition=0.974, ppl=800.12, wps=14108, ups=0.93, wpb=15199.4, bsz=461.9, num_updates=4300, lr=0.000430014, gnorm=2.342, clip=0, loss_scale=2721, train_wall=107, wall=4623
2022-10-12 08:18:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-10-12 08:18:37 | INFO | train_inner | epoch 010:    274 / 459 loss=9.659, nll_loss=5.899, mask_ins=1.542, word_ins_ml=7.14, word_reposition=0.978, ppl=808.59, wps=13662, ups=0.94, wpb=14484.4, bsz=437.4, num_updates=4400, lr=0.000440012, gnorm=3.596, clip=0, loss_scale=953, train_wall=105, wall=4729
2022-10-12 08:18:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 08:20:22 | INFO | train_inner | epoch 010:    375 / 459 loss=9.66, nll_loss=5.914, mask_ins=1.539, word_ins_ml=7.156, word_reposition=0.965, ppl=809.15, wps=13723.7, ups=0.95, wpb=14398, bsz=435.4, num_updates=4500, lr=0.00045001, gnorm=5.618, clip=1, loss_scale=307, train_wall=104, wall=4834
2022-10-12 08:21:49 | INFO | train | epoch 010 | loss 9.656 | nll_loss 5.899 | mask_ins 1.545 | word_ins_ml 7.139 | word_reposition 0.971 | ppl 806.53 | wps 13508.8 | ups 0.92 | wpb 14637.4 | bsz 444 | num_updates 4584 | lr 0.000458408 | gnorm 4.096 | clip 0.4 | loss_scale 1303 | train_wall 475 | wall 4921
2022-10-12 08:21:58 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 13.412 | nll_loss 10.01 | mask_ins 1.645 | word_ins_ml 10.894 | word_reposition 0.873 | ppl 10902 | wps 35034.8 | wpb 1628.7 | bsz 55.8 | num_updates 4584 | best_loss 13.412
2022-10-12 08:22:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 10 @ 4584 updates, score 13.412) (writing took 4.102633259011782 seconds)
2022-10-12 08:22:19 | INFO | train_inner | epoch 011:     16 / 459 loss=9.629, nll_loss=5.899, mask_ins=1.538, word_ins_ml=7.143, word_reposition=0.949, ppl=791.95, wps=12451.2, ups=0.86, wpb=14529.5, bsz=443.2, num_updates=4600, lr=0.000460008, gnorm=6.176, clip=1, loss_scale=256, train_wall=102, wall=4951
2022-10-12 08:24:01 | INFO | train_inner | epoch 011:    116 / 459 loss=9.542, nll_loss=5.8, mask_ins=1.54, word_ins_ml=7.059, word_reposition=0.944, ppl=745.68, wps=14271.3, ups=0.98, wpb=14623.8, bsz=443, num_updates=4700, lr=0.000470006, gnorm=5.257, clip=3, loss_scale=256, train_wall=102, wall=5053
2022-10-12 08:25:45 | INFO | train_inner | epoch 011:    216 / 459 loss=9.538, nll_loss=5.803, mask_ins=1.539, word_ins_ml=7.062, word_reposition=0.937, ppl=743.37, wps=13955.9, ups=0.96, wpb=14491.7, bsz=440.6, num_updates=4800, lr=0.000480004, gnorm=4.09, clip=0, loss_scale=256, train_wall=103, wall=5157
2022-10-12 08:27:28 | INFO | train_inner | epoch 011:    316 / 459 loss=9.472, nll_loss=5.745, mask_ins=1.531, word_ins_ml=7.012, word_reposition=0.929, ppl=710.18, wps=13918, ups=0.97, wpb=14373.6, bsz=434.8, num_updates=4900, lr=0.000490002, gnorm=3.699, clip=1, loss_scale=256, train_wall=102, wall=5260
2022-10-12 08:28:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 08:28:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 08:29:14 | INFO | train_inner | epoch 011:    418 / 459 loss=9.46, nll_loss=5.75, mask_ins=1.528, word_ins_ml=7.018, word_reposition=0.914, ppl=704.2, wps=14203.3, ups=0.94, wpb=15058, bsz=456.4, num_updates=5000, lr=0.0005, gnorm=6.769, clip=3, loss_scale=245, train_wall=105, wall=5366
2022-10-12 08:29:57 | INFO | train | epoch 011 | loss 9.506 | nll_loss 5.782 | mask_ins 1.531 | word_ins_ml 7.045 | word_reposition 0.931 | ppl 727.3 | wps 13706.1 | ups 0.94 | wpb 14632.8 | bsz 443.7 | num_updates 5041 | lr 0.000497963 | gnorm 5.398 | clip 2.6 | loss_scale 242 | train_wall 470 | wall 5409
2022-10-12 08:30:06 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 13.243 | nll_loss 9.816 | mask_ins 1.642 | word_ins_ml 10.731 | word_reposition 0.87 | ppl 9694.62 | wps 34971.2 | wpb 1628.7 | bsz 55.8 | num_updates 5041 | best_loss 13.243
2022-10-12 08:30:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 11 @ 5041 updates, score 13.243) (writing took 4.181001595003181 seconds)
2022-10-12 08:30:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 08:30:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 08:31:13 | INFO | train_inner | epoch 012:     61 / 459 loss=9.467, nll_loss=5.787, mask_ins=1.499, word_ins_ml=7.051, word_reposition=0.917, ppl=707.69, wps=12338.8, ups=0.84, wpb=14666.3, bsz=445.6, num_updates=5100, lr=0.000495074, gnorm=13.37, clip=13, loss_scale=83, train_wall=104, wall=5485
2022-10-12 08:31:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2022-10-12 08:32:58 | INFO | train_inner | epoch 012:    162 / 459 loss=9.433, nll_loss=5.735, mask_ins=1.523, word_ins_ml=7.007, word_reposition=0.904, ppl=691.4, wps=14087.8, ups=0.96, wpb=14747.8, bsz=447.6, num_updates=5200, lr=0.00049029, gnorm=23.819, clip=13, loss_scale=16, train_wall=104, wall=5590
2022-10-12 08:34:44 | INFO | train_inner | epoch 012:    262 / 459 loss=9.342, nll_loss=5.679, mask_ins=1.491, word_ins_ml=6.958, word_reposition=0.893, ppl=649.11, wps=13864.6, ups=0.94, wpb=14768.5, bsz=450.7, num_updates=5300, lr=0.000485643, gnorm=19.244, clip=4, loss_scale=16, train_wall=106, wall=5697
2022-10-12 08:36:29 | INFO | train_inner | epoch 012:    362 / 459 loss=9.401, nll_loss=5.731, mask_ins=1.511, word_ins_ml=7.003, word_reposition=0.886, ppl=676.03, wps=13968.3, ups=0.96, wpb=14612.8, bsz=441.9, num_updates=5400, lr=0.000481125, gnorm=10.593, clip=10, loss_scale=16, train_wall=104, wall=5801
2022-10-12 08:38:10 | INFO | train | epoch 012 | loss 9.379 | nll_loss 5.707 | mask_ins 1.504 | word_ins_ml 6.982 | word_reposition 0.894 | ppl 665.92 | wps 13518.6 | ups 0.92 | wpb 14635.1 | bsz 443.9 | num_updates 5497 | lr 0.000476861 | gnorm 17.285 | clip 9.6 | loss_scale 21 | train_wall 476 | wall 5903
2022-10-12 08:38:20 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 13.455 | nll_loss 9.819 | mask_ins 1.75 | word_ins_ml 10.73 | word_reposition 0.975 | ppl 11231.5 | wps 35138.3 | wpb 1628.7 | bsz 55.8 | num_updates 5497 | best_loss 13.243
2022-10-12 08:38:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 12 @ 5497 updates, score 13.455) (writing took 3.004324737994466 seconds)
2022-10-12 08:38:26 | INFO | train_inner | epoch 013:      3 / 459 loss=9.278, nll_loss=5.626, mask_ins=1.488, word_ins_ml=6.911, word_reposition=0.878, ppl=620.75, wps=12371.1, ups=0.85, wpb=14469.7, bsz=437, num_updates=5500, lr=0.000476731, gnorm=16.189, clip=9, loss_scale=16, train_wall=104, wall=5918
2022-10-12 08:40:11 | INFO | train_inner | epoch 013:    103 / 459 loss=9.277, nll_loss=5.608, mask_ins=1.502, word_ins_ml=6.896, word_reposition=0.88, ppl=620.56, wps=14197.2, ups=0.95, wpb=14879.8, bsz=454.4, num_updates=5600, lr=0.000472456, gnorm=11.193, clip=14, loss_scale=16, train_wall=104, wall=6023
2022-10-12 08:41:54 | INFO | train_inner | epoch 013:    203 / 459 loss=9.181, nll_loss=5.54, mask_ins=1.486, word_ins_ml=6.837, word_reposition=0.857, ppl=580.25, wps=13945.8, ups=0.96, wpb=14460.8, bsz=438.1, num_updates=5700, lr=0.000468293, gnorm=6.851, clip=5, loss_scale=30, train_wall=103, wall=6127
2022-10-12 08:43:39 | INFO | train_inner | epoch 013:    303 / 459 loss=9.193, nll_loss=5.57, mask_ins=1.474, word_ins_ml=6.864, word_reposition=0.855, ppl=585.2, wps=13970, ups=0.96, wpb=14607.2, bsz=442.3, num_updates=5800, lr=0.000464238, gnorm=7.488, clip=6, loss_scale=32, train_wall=104, wall=6231
2022-10-12 08:45:24 | INFO | train_inner | epoch 013:    403 / 459 loss=9.153, nll_loss=5.516, mask_ins=1.478, word_ins_ml=6.817, word_reposition=0.858, ppl=569.37, wps=13874.6, ups=0.95, wpb=14618.3, bsz=441.8, num_updates=5900, lr=0.000460287, gnorm=5.164, clip=1, loss_scale=32, train_wall=104, wall=6337
2022-10-12 08:46:23 | INFO | train | epoch 013 | loss 9.196 | nll_loss 5.557 | mask_ins 1.482 | word_ins_ml 6.852 | word_reposition 0.862 | ppl 586.56 | wps 13640.9 | ups 0.93 | wpb 14642.7 | bsz 444.1 | num_updates 5956 | lr 0.000458118 | gnorm 8.441 | clip 8.3 | loss_scale 28 | train_wall 476 | wall 6395
2022-10-12 08:46:32 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 13.214 | nll_loss 9.868 | mask_ins 1.644 | word_ins_ml 10.765 | word_reposition 0.805 | ppl 9505.05 | wps 35138.3 | wpb 1628.7 | bsz 55.8 | num_updates 5956 | best_loss 13.214
2022-10-12 08:46:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 13 @ 5956 updates, score 13.214) (writing took 4.391762811996159 seconds)
2022-10-12 08:47:25 | INFO | train_inner | epoch 014:     44 / 459 loss=9.171, nll_loss=5.549, mask_ins=1.465, word_ins_ml=6.845, word_reposition=0.861, ppl=576.33, wps=12269.9, ups=0.83, wpb=14769.3, bsz=448.2, num_updates=6000, lr=0.000456435, gnorm=15.31, clip=18, loss_scale=32, train_wall=106, wall=6457
2022-10-12 08:49:11 | INFO | train_inner | epoch 014:    144 / 459 loss=9.102, nll_loss=5.465, mask_ins=1.472, word_ins_ml=6.773, word_reposition=0.858, ppl=549.67, wps=13780.1, ups=0.94, wpb=14623.7, bsz=443.5, num_updates=6100, lr=0.000452679, gnorm=13.73, clip=9, loss_scale=32, train_wall=105, wall=6563
2022-10-12 08:50:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 08:50:57 | INFO | train_inner | epoch 014:    245 / 459 loss=9.056, nll_loss=5.42, mask_ins=1.459, word_ins_ml=6.734, word_reposition=0.863, ppl=532.21, wps=14006.3, ups=0.94, wpb=14909.8, bsz=450.6, num_updates=6200, lr=0.000449013, gnorm=6.001, clip=3, loss_scale=47, train_wall=106, wall=6669
2022-10-12 08:52:40 | INFO | train_inner | epoch 014:    345 / 459 loss=9.041, nll_loss=5.42, mask_ins=1.461, word_ins_ml=6.734, word_reposition=0.846, ppl=526.63, wps=14230.6, ups=0.97, wpb=14605, bsz=443.6, num_updates=6300, lr=0.000445435, gnorm=5.808, clip=2, loss_scale=32, train_wall=102, wall=6772
2022-10-12 08:54:23 | INFO | train_inner | epoch 014:    445 / 459 loss=9.044, nll_loss=5.453, mask_ins=1.447, word_ins_ml=6.762, word_reposition=0.835, ppl=528, wps=14042.5, ups=0.97, wpb=14437.5, bsz=439.5, num_updates=6400, lr=0.000441942, gnorm=19.428, clip=15, loss_scale=32, train_wall=102, wall=6875
2022-10-12 08:54:37 | INFO | train | epoch 014 | loss 9.076 | nll_loss 5.453 | mask_ins 1.461 | word_ins_ml 6.763 | word_reposition 0.852 | ppl 539.6 | wps 13584.6 | ups 0.93 | wpb 14640.9 | bsz 444.1 | num_updates 6414 | lr 0.000441459 | gnorm 11.814 | clip 8.1 | loss_scale 35 | train_wall 476 | wall 6889
2022-10-12 08:54:46 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 13.148 | nll_loss 9.675 | mask_ins 1.67 | word_ins_ml 10.605 | word_reposition 0.873 | ppl 9078.87 | wps 34829.3 | wpb 1628.7 | bsz 55.8 | num_updates 6414 | best_loss 13.148
2022-10-12 08:54:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 14 @ 6414 updates, score 13.148) (writing took 4.220424091996392 seconds)
2022-10-12 08:56:20 | INFO | train_inner | epoch 015:     86 / 459 loss=8.956, nll_loss=5.35, mask_ins=1.455, word_ins_ml=6.673, word_reposition=0.828, ppl=496.59, wps=12297.6, ups=0.85, wpb=14391.1, bsz=435.8, num_updates=6500, lr=0.000438529, gnorm=4.897, clip=3, loss_scale=32, train_wall=103, wall=6992
2022-10-12 08:58:05 | INFO | train_inner | epoch 015:    186 / 459 loss=8.847, nll_loss=5.236, mask_ins=1.443, word_ins_ml=6.574, word_reposition=0.829, ppl=460.33, wps=13945.3, ups=0.95, wpb=14632.3, bsz=443.8, num_updates=6600, lr=0.000435194, gnorm=5.175, clip=4, loss_scale=32, train_wall=104, wall=7097
2022-10-12 08:59:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 08:59:52 | INFO | train_inner | epoch 015:    287 / 459 loss=8.967, nll_loss=5.361, mask_ins=1.447, word_ins_ml=6.682, word_reposition=0.838, ppl=500.5, wps=13472.2, ups=0.93, wpb=14496.5, bsz=439.8, num_updates=6700, lr=0.000431934, gnorm=12.128, clip=10, loss_scale=35, train_wall=107, wall=7204
2022-10-12 09:01:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2022-10-12 09:01:38 | INFO | train_inner | epoch 015:    388 / 459 loss=8.968, nll_loss=5.372, mask_ins=1.437, word_ins_ml=6.693, word_reposition=0.838, ppl=500.63, wps=14064.1, ups=0.94, wpb=14944.4, bsz=453.3, num_updates=6800, lr=0.000428746, gnorm=8.313, clip=6, loss_scale=31, train_wall=105, wall=7311
2022-10-12 09:02:51 | INFO | train | epoch 015 | loss 8.928 | nll_loss 5.323 | mask_ins 1.445 | word_ins_ml 6.65 | word_reposition 0.833 | ppl 487.07 | wps 13528.5 | ups 0.92 | wpb 14636.9 | bsz 444 | num_updates 6871 | lr 0.000426526 | gnorm 7.895 | clip 6.3 | loss_scale 30 | train_wall 477 | wall 7384
2022-10-12 09:03:00 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 13.13 | nll_loss 9.665 | mask_ins 1.615 | word_ins_ml 10.609 | word_reposition 0.907 | ppl 8967.23 | wps 35085.2 | wpb 1628.7 | bsz 55.8 | num_updates 6871 | best_loss 13.13
2022-10-12 09:03:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 15 @ 6871 updates, score 13.13) (writing took 4.323019643998123 seconds)
2022-10-12 09:03:35 | INFO | train_inner | epoch 016:     29 / 459 loss=8.947, nll_loss=5.355, mask_ins=1.441, word_ins_ml=6.677, word_reposition=0.83, ppl=493.66, wps=12640.5, ups=0.86, wpb=14682.3, bsz=445.9, num_updates=6900, lr=0.000425628, gnorm=11.826, clip=12, loss_scale=16, train_wall=102, wall=7427
2022-10-12 09:05:19 | INFO | train_inner | epoch 016:    129 / 459 loss=8.856, nll_loss=5.297, mask_ins=1.415, word_ins_ml=6.627, word_reposition=0.815, ppl=463.46, wps=14065.1, ups=0.96, wpb=14676.3, bsz=447.1, num_updates=7000, lr=0.000422577, gnorm=16.503, clip=12, loss_scale=16, train_wall=103, wall=7531
2022-10-12 09:07:04 | INFO | train_inner | epoch 016:    229 / 459 loss=8.837, nll_loss=5.251, mask_ins=1.428, word_ins_ml=6.588, word_reposition=0.821, ppl=457.46, wps=13911.5, ups=0.96, wpb=14547, bsz=439.1, num_updates=7100, lr=0.000419591, gnorm=15.568, clip=13, loss_scale=16, train_wall=104, wall=7636
2022-10-12 09:08:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0
2022-10-12 09:08:50 | INFO | train_inner | epoch 016:    330 / 459 loss=8.882, nll_loss=5.325, mask_ins=1.415, word_ins_ml=6.652, word_reposition=0.814, ppl=471.63, wps=13625.2, ups=0.94, wpb=14475.7, bsz=436.8, num_updates=7200, lr=0.000416667, gnorm=12.326, clip=7, loss_scale=14, train_wall=105, wall=7742
2022-10-12 09:10:34 | INFO | train_inner | epoch 016:    430 / 459 loss=8.969, nll_loss=5.409, mask_ins=1.432, word_ins_ml=6.726, word_reposition=0.811, ppl=501.08, wps=14182.5, ups=0.96, wpb=14836.5, bsz=452.6, num_updates=7300, lr=0.000413803, gnorm=20.766, clip=19, loss_scale=8, train_wall=104, wall=7847
2022-10-12 09:11:04 | INFO | train | epoch 016 | loss 8.893 | nll_loss 5.327 | mask_ins 1.424 | word_ins_ml 6.654 | word_reposition 0.816 | ppl 475.46 | wps 13598 | ups 0.93 | wpb 14634.8 | bsz 443.9 | num_updates 7329 | lr 0.000412983 | gnorm 16.259 | clip 12.7 | loss_scale 13 | train_wall 475 | wall 7876
2022-10-12 09:11:13 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 13.108 | nll_loss 9.713 | mask_ins 1.662 | word_ins_ml 10.653 | word_reposition 0.793 | ppl 8828.03 | wps 35056.9 | wpb 1628.7 | bsz 55.8 | num_updates 7329 | best_loss 13.108
2022-10-12 09:11:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 16 @ 7329 updates, score 13.108) (writing took 4.035433323995676 seconds)
2022-10-12 09:12:32 | INFO | train_inner | epoch 017:     71 / 459 loss=8.813, nll_loss=5.239, mask_ins=1.424, word_ins_ml=6.577, word_reposition=0.812, ppl=449.76, wps=12192.4, ups=0.85, wpb=14338.1, bsz=436.6, num_updates=7400, lr=0.000410997, gnorm=14.618, clip=5, loss_scale=8, train_wall=103, wall=7964
2022-10-12 09:14:17 | INFO | train_inner | epoch 017:    171 / 459 loss=8.742, nll_loss=5.164, mask_ins=1.418, word_ins_ml=6.512, word_reposition=0.812, ppl=428.08, wps=13944.4, ups=0.95, wpb=14706.4, bsz=447.5, num_updates=7500, lr=0.000408248, gnorm=6.301, clip=4, loss_scale=8, train_wall=105, wall=8070
2022-10-12 09:16:01 | INFO | train_inner | epoch 017:    271 / 459 loss=8.782, nll_loss=5.195, mask_ins=1.42, word_ins_ml=6.539, word_reposition=0.824, ppl=440.34, wps=14186.5, ups=0.96, wpb=14711.4, bsz=443.3, num_updates=7600, lr=0.000405554, gnorm=10.825, clip=3, loss_scale=8, train_wall=103, wall=8173
2022-10-12 09:17:44 | INFO | train_inner | epoch 017:    371 / 459 loss=8.73, nll_loss=5.159, mask_ins=1.409, word_ins_ml=6.506, word_reposition=0.815, ppl=424.65, wps=14119.9, ups=0.97, wpb=14561.3, bsz=442.8, num_updates=7700, lr=0.000402911, gnorm=4.725, clip=2, loss_scale=9, train_wall=102, wall=8277
2022-10-12 09:19:15 | INFO | train | epoch 017 | loss 8.75 | nll_loss 5.174 | mask_ins 1.415 | word_ins_ml 6.52 | word_reposition 0.815 | ppl 430.5 | wps 13680.7 | ups 0.93 | wpb 14640.6 | bsz 444.1 | num_updates 7788 | lr 0.000400629 | gnorm 7.941 | clip 3.1 | loss_scale 10 | train_wall 474 | wall 8368
2022-10-12 09:19:25 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 13.123 | nll_loss 9.595 | mask_ins 1.649 | word_ins_ml 10.549 | word_reposition 0.925 | ppl 8920.76 | wps 35070.1 | wpb 1628.7 | bsz 55.8 | num_updates 7788 | best_loss 13.108
2022-10-12 09:19:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 17 @ 7788 updates, score 13.123) (writing took 2.764989104005508 seconds)
2022-10-12 09:19:40 | INFO | train_inner | epoch 018:     12 / 459 loss=8.706, nll_loss=5.142, mask_ins=1.404, word_ins_ml=6.492, word_reposition=0.81, ppl=417.68, wps=12903.6, ups=0.86, wpb=14932.9, bsz=452.6, num_updates=7800, lr=0.00040032, gnorm=4.845, clip=3, loss_scale=16, train_wall=103, wall=8392
2022-10-12 09:21:24 | INFO | train_inner | epoch 018:    112 / 459 loss=8.645, nll_loss=5.073, mask_ins=1.413, word_ins_ml=6.431, word_reposition=0.801, ppl=400.19, wps=14208, ups=0.96, wpb=14727, bsz=446.1, num_updates=7900, lr=0.000397779, gnorm=6.11, clip=4, loss_scale=16, train_wall=103, wall=8496
2022-10-12 09:23:07 | INFO | train_inner | epoch 018:    212 / 459 loss=8.679, nll_loss=5.116, mask_ins=1.404, word_ins_ml=6.469, word_reposition=0.805, ppl=409.76, wps=14309.1, ups=0.97, wpb=14791.6, bsz=446.2, num_updates=8000, lr=0.000395285, gnorm=12.445, clip=11, loss_scale=16, train_wall=102, wall=8599
2022-10-12 09:24:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0
2022-10-12 09:24:53 | INFO | train_inner | epoch 018:    313 / 459 loss=8.653, nll_loss=5.087, mask_ins=1.405, word_ins_ml=6.443, word_reposition=0.804, ppl=402.5, wps=13872.7, ups=0.94, wpb=14688.2, bsz=447.8, num_updates=8100, lr=0.000392837, gnorm=6.949, clip=4, loss_scale=13, train_wall=105, wall=8705
2022-10-12 09:26:36 | INFO | train_inner | epoch 018:    413 / 459 loss=8.643, nll_loss=5.089, mask_ins=1.402, word_ins_ml=6.445, word_reposition=0.796, ppl=399.65, wps=14002.1, ups=0.97, wpb=14375.5, bsz=434.3, num_updates=8200, lr=0.000390434, gnorm=8.513, clip=5, loss_scale=8, train_wall=102, wall=8808
2022-10-12 09:27:23 | INFO | train | epoch 018 | loss 8.651 | nll_loss 5.088 | mask_ins 1.405 | word_ins_ml 6.444 | word_reposition 0.803 | ppl 402.12 | wps 13745.5 | ups 0.94 | wpb 14635.5 | bsz 443.9 | num_updates 8246 | lr 0.000389344 | gnorm 8.148 | clip 5.5 | loss_scale 13 | train_wall 471 | wall 8855
2022-10-12 09:27:32 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 13.149 | nll_loss 9.67 | mask_ins 1.617 | word_ins_ml 10.614 | word_reposition 0.918 | ppl 9084.55 | wps 34948.3 | wpb 1628.7 | bsz 55.8 | num_updates 8246 | best_loss 13.108
2022-10-12 09:27:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 18 @ 8246 updates, score 13.149) (writing took 2.7780135909997625 seconds)
2022-10-12 09:28:31 | INFO | train_inner | epoch 019:     54 / 459 loss=8.572, nll_loss=5.018, mask_ins=1.391, word_ins_ml=6.383, word_reposition=0.798, ppl=380.46, wps=12549.9, ups=0.86, wpb=14534.2, bsz=441.3, num_updates=8300, lr=0.000388075, gnorm=5.772, clip=3, loss_scale=8, train_wall=103, wall=8924
2022-10-12 09:30:15 | INFO | train_inner | epoch 019:    154 / 459 loss=8.554, nll_loss=5.009, mask_ins=1.397, word_ins_ml=6.376, word_reposition=0.782, ppl=375.84, wps=14170, ups=0.96, wpb=14686.1, bsz=444.3, num_updates=8400, lr=0.000385758, gnorm=3.872, clip=1, loss_scale=8, train_wall=103, wall=9027
2022-10-12 09:31:58 | INFO | train_inner | epoch 019:    254 / 459 loss=8.564, nll_loss=5.018, mask_ins=1.383, word_ins_ml=6.384, word_reposition=0.798, ppl=378.46, wps=14333.6, ups=0.97, wpb=14742.5, bsz=445.7, num_updates=8500, lr=0.000383482, gnorm=8.539, clip=10, loss_scale=8, train_wall=102, wall=9130
2022-10-12 09:33:40 | INFO | train_inner | epoch 019:    354 / 459 loss=8.569, nll_loss=5.024, mask_ins=1.387, word_ins_ml=6.388, word_reposition=0.794, ppl=379.75, wps=14185.4, ups=0.98, wpb=14495.4, bsz=441.5, num_updates=8600, lr=0.000381246, gnorm=17.863, clip=5, loss_scale=10, train_wall=101, wall=9232
2022-10-12 09:35:23 | INFO | train_inner | epoch 019:    454 / 459 loss=8.584, nll_loss=5.053, mask_ins=1.39, word_ins_ml=6.413, word_reposition=0.782, ppl=383.87, wps=14247.7, ups=0.97, wpb=14679.6, bsz=446.5, num_updates=8700, lr=0.000379049, gnorm=8.364, clip=7, loss_scale=16, train_wall=102, wall=9335
2022-10-12 09:35:28 | INFO | train | epoch 019 | loss 8.564 | nll_loss 5.021 | mask_ins 1.388 | word_ins_ml 6.386 | word_reposition 0.79 | ppl 378.39 | wps 13860 | ups 0.95 | wpb 14642.3 | bsz 444.1 | num_updates 8705 | lr 0.00037894 | gnorm 9.037 | clip 5.4 | loss_scale 10 | train_wall 469 | wall 9340
2022-10-12 09:35:37 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 13.076 | nll_loss 9.676 | mask_ins 1.633 | word_ins_ml 10.62 | word_reposition 0.822 | ppl 8635.15 | wps 35137.9 | wpb 1628.7 | bsz 55.8 | num_updates 8705 | best_loss 13.076
2022-10-12 09:35:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 19 @ 8705 updates, score 13.076) (writing took 4.105257636008901 seconds)
2022-10-12 09:37:20 | INFO | train_inner | epoch 020:     95 / 459 loss=8.464, nll_loss=4.918, mask_ins=1.376, word_ins_ml=6.296, word_reposition=0.792, ppl=353.09, wps=12570.8, ups=0.85, wpb=14732.6, bsz=450, num_updates=8800, lr=0.000376889, gnorm=6.851, clip=6, loss_scale=16, train_wall=103, wall=9453
2022-10-12 09:39:04 | INFO | train_inner | epoch 020:    195 / 459 loss=8.457, nll_loss=4.912, mask_ins=1.383, word_ins_ml=6.292, word_reposition=0.782, ppl=351.53, wps=13910.9, ups=0.97, wpb=14396.2, bsz=436.7, num_updates=8900, lr=0.000374766, gnorm=2.93, clip=1, loss_scale=16, train_wall=103, wall=9556
2022-10-12 09:40:48 | INFO | train_inner | epoch 020:    295 / 459 loss=8.483, nll_loss=4.943, mask_ins=1.38, word_ins_ml=6.318, word_reposition=0.784, ppl=357.78, wps=13917.6, ups=0.96, wpb=14511.8, bsz=438.7, num_updates=9000, lr=0.000372678, gnorm=4.29, clip=3, loss_scale=16, train_wall=103, wall=9660
2022-10-12 09:42:31 | INFO | train_inner | epoch 020:    395 / 459 loss=8.482, nll_loss=4.957, mask_ins=1.371, word_ins_ml=6.33, word_reposition=0.782, ppl=357.54, wps=14266.4, ups=0.97, wpb=14678, bsz=444.8, num_updates=9100, lr=0.000370625, gnorm=6.609, clip=3, loss_scale=17, train_wall=102, wall=9763
2022-10-12 09:43:37 | INFO | train | epoch 020 | loss 8.474 | nll_loss 4.935 | mask_ins 1.378 | word_ins_ml 6.311 | word_reposition 0.785 | ppl 355.62 | wps 13729.6 | ups 0.94 | wpb 14636.5 | bsz 443.9 | num_updates 9164 | lr 0.000369328 | gnorm 6.698 | clip 3.9 | loss_scale 19 | train_wall 472 | wall 9830
2022-10-12 09:43:46 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 13.037 | nll_loss 9.627 | mask_ins 1.608 | word_ins_ml 10.577 | word_reposition 0.852 | ppl 8406.84 | wps 35070.1 | wpb 1628.7 | bsz 55.8 | num_updates 9164 | best_loss 13.037
2022-10-12 09:43:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 20 @ 9164 updates, score 13.037) (writing took 3.9632073510001646 seconds)
2022-10-12 09:44:28 | INFO | train_inner | epoch 021:     36 / 459 loss=8.515, nll_loss=4.958, mask_ins=1.388, word_ins_ml=6.331, word_reposition=0.795, ppl=365.8, wps=12809.1, ups=0.86, wpb=14959.9, bsz=451.8, num_updates=9200, lr=0.000368605, gnorm=14.548, clip=9, loss_scale=32, train_wall=103, wall=9880
2022-10-12 09:46:12 | INFO | train_inner | epoch 021:    136 / 459 loss=8.408, nll_loss=4.876, mask_ins=1.37, word_ins_ml=6.261, word_reposition=0.777, ppl=339.7, wps=14132.8, ups=0.96, wpb=14702.4, bsz=446.4, num_updates=9300, lr=0.000366618, gnorm=10.657, clip=7, loss_scale=32, train_wall=103, wall=9984
2022-10-12 09:47:56 | INFO | train_inner | epoch 021:    236 / 459 loss=8.432, nll_loss=4.904, mask_ins=1.363, word_ins_ml=6.285, word_reposition=0.784, ppl=345.44, wps=14056.8, ups=0.96, wpb=14635.7, bsz=443.1, num_updates=9400, lr=0.000364662, gnorm=5.926, clip=3, loss_scale=32, train_wall=103, wall=10088
2022-10-12 09:49:39 | INFO | train_inner | epoch 021:    336 / 459 loss=8.387, nll_loss=4.869, mask_ins=1.369, word_ins_ml=6.254, word_reposition=0.763, ppl=334.72, wps=13921.5, ups=0.97, wpb=14296.5, bsz=433.3, num_updates=9500, lr=0.000362738, gnorm=3.843, clip=2, loss_scale=32, train_wall=102, wall=10191
2022-10-12 09:51:23 | INFO | train_inner | epoch 021:    436 / 459 loss=8.408, nll_loss=4.875, mask_ins=1.37, word_ins_ml=6.259, word_reposition=0.779, ppl=339.65, wps=14109.2, ups=0.95, wpb=14802.4, bsz=451, num_updates=9600, lr=0.000360844, gnorm=4.99, clip=3, loss_scale=32, train_wall=104, wall=10296
2022-10-12 09:51:47 | INFO | train | epoch 021 | loss 8.419 | nll_loss 4.887 | mask_ins 1.371 | word_ins_ml 6.27 | word_reposition 0.779 | ppl 342.34 | wps 13709.6 | ups 0.94 | wpb 14642.2 | bsz 444.1 | num_updates 9623 | lr 0.000360412 | gnorm 6.857 | clip 4.4 | loss_scale 33 | train_wall 473 | wall 10320
2022-10-12 09:51:57 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 13.197 | nll_loss 9.681 | mask_ins 1.632 | word_ins_ml 10.628 | word_reposition 0.937 | ppl 9393.46 | wps 35095.9 | wpb 1628.7 | bsz 55.8 | num_updates 9623 | best_loss 13.037
2022-10-12 09:52:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 21 @ 9623 updates, score 13.197) (writing took 2.836824425001396 seconds)
2022-10-12 09:53:20 | INFO | train_inner | epoch 022:     77 / 459 loss=8.355, nll_loss=4.819, mask_ins=1.372, word_ins_ml=6.211, word_reposition=0.772, ppl=327.39, wps=12687.1, ups=0.86, wpb=14794.1, bsz=448.3, num_updates=9700, lr=0.000358979, gnorm=4.568, clip=1, loss_scale=63, train_wall=104, wall=10412
2022-10-12 09:55:03 | INFO | train_inner | epoch 022:    177 / 459 loss=8.334, nll_loss=4.809, mask_ins=1.36, word_ins_ml=6.203, word_reposition=0.771, ppl=322.69, wps=14153.6, ups=0.97, wpb=14617, bsz=444.2, num_updates=9800, lr=0.000357143, gnorm=3.394, clip=1, loss_scale=64, train_wall=102, wall=10516
2022-10-12 09:56:46 | INFO | train_inner | epoch 022:    277 / 459 loss=8.351, nll_loss=4.819, mask_ins=1.361, word_ins_ml=6.21, word_reposition=0.779, ppl=326.41, wps=14428.4, ups=0.98, wpb=14794.9, bsz=449.2, num_updates=9900, lr=0.000355335, gnorm=4.604, clip=2, loss_scale=64, train_wall=102, wall=10618
2022-10-12 09:58:29 | INFO | train_inner | epoch 022:    377 / 459 loss=8.352, nll_loss=4.836, mask_ins=1.362, word_ins_ml=6.225, word_reposition=0.764, ppl=326.69, wps=14195.2, ups=0.97, wpb=14580.7, bsz=438.2, num_updates=10000, lr=0.000353553, gnorm=5.702, clip=2, loss_scale=64, train_wall=102, wall=10721
2022-10-12 09:58:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 09:59:52 | INFO | train | epoch 022 | loss 8.342 | nll_loss 4.818 | mask_ins 1.363 | word_ins_ml 6.21 | word_reposition 0.768 | ppl 324.43 | wps 13838.3 | ups 0.95 | wpb 14636.9 | bsz 443.8 | num_updates 10081 | lr 0.00035213 | gnorm 4.313 | clip 1.3 | loss_scale 59 | train_wall 468 | wall 10804
2022-10-12 10:00:01 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 13.208 | nll_loss 9.672 | mask_ins 1.644 | word_ins_ml 10.626 | word_reposition 0.939 | ppl 9464.86 | wps 35074.9 | wpb 1628.7 | bsz 55.8 | num_updates 10081 | best_loss 13.037
2022-10-12 10:00:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 22 @ 10081 updates, score 13.208) (writing took 2.605137869992177 seconds)
2022-10-12 10:00:23 | INFO | train_inner | epoch 023:     19 / 459 loss=8.335, nll_loss=4.82, mask_ins=1.365, word_ins_ml=6.211, word_reposition=0.759, ppl=323, wps=12399.7, ups=0.87, wpb=14187.9, bsz=430.3, num_updates=10100, lr=0.000351799, gnorm=5.405, clip=4, loss_scale=35, train_wall=102, wall=10835
2022-10-12 10:02:06 | INFO | train_inner | epoch 023:    119 / 459 loss=8.28, nll_loss=4.772, mask_ins=1.354, word_ins_ml=6.171, word_reposition=0.756, ppl=310.89, wps=14551.4, ups=0.97, wpb=15012.5, bsz=453.9, num_updates=10200, lr=0.00035007, gnorm=3.823, clip=1, loss_scale=32, train_wall=102, wall=10938
2022-10-12 10:03:48 | INFO | train_inner | epoch 023:    219 / 459 loss=8.261, nll_loss=4.752, mask_ins=1.353, word_ins_ml=6.152, word_reposition=0.756, ppl=306.71, wps=14238.9, ups=0.98, wpb=14501.1, bsz=442.4, num_updates=10300, lr=0.000348367, gnorm=3.457, clip=2, loss_scale=32, train_wall=101, wall=11040
2022-10-12 10:05:31 | INFO | train_inner | epoch 023:    319 / 459 loss=8.273, nll_loss=4.75, mask_ins=1.355, word_ins_ml=6.151, word_reposition=0.768, ppl=309.28, wps=14181.4, ups=0.97, wpb=14604.9, bsz=442.3, num_updates=10400, lr=0.000346688, gnorm=3.574, clip=1, loss_scale=32, train_wall=102, wall=11143
2022-10-12 10:07:14 | INFO | train_inner | epoch 023:    419 / 459 loss=8.27, nll_loss=4.755, mask_ins=1.357, word_ins_ml=6.154, word_reposition=0.759, ppl=308.77, wps=14234.7, ups=0.97, wpb=14708.4, bsz=447.5, num_updates=10500, lr=0.000345033, gnorm=3.002, clip=1, loss_scale=32, train_wall=102, wall=11247
2022-10-12 10:07:56 | INFO | train | epoch 023 | loss 8.274 | nll_loss 4.758 | mask_ins 1.355 | word_ins_ml 6.158 | word_reposition 0.761 | ppl 309.58 | wps 13888.6 | ups 0.95 | wpb 14641 | bsz 444 | num_updates 10540 | lr 0.000344377 | gnorm 4.109 | clip 2 | loss_scale 33 | train_wall 468 | wall 11288
2022-10-12 10:08:05 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 13.125 | nll_loss 9.686 | mask_ins 1.617 | word_ins_ml 10.635 | word_reposition 0.873 | ppl 8934.37 | wps 35083.5 | wpb 1628.7 | bsz 55.8 | num_updates 10540 | best_loss 13.037
2022-10-12 10:08:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 23 @ 10540 updates, score 13.125) (writing took 2.824314174009487 seconds)
2022-10-12 10:09:11 | INFO | train_inner | epoch 024:     60 / 459 loss=8.225, nll_loss=4.706, mask_ins=1.353, word_ins_ml=6.113, word_reposition=0.759, ppl=299.23, wps=12872.8, ups=0.86, wpb=15017.4, bsz=455.5, num_updates=10600, lr=0.000343401, gnorm=4.193, clip=1, loss_scale=57, train_wall=104, wall=11363
2022-10-12 10:10:54 | INFO | train_inner | epoch 024:    160 / 459 loss=8.202, nll_loss=4.68, mask_ins=1.354, word_ins_ml=6.089, word_reposition=0.759, ppl=294.42, wps=14155.7, ups=0.97, wpb=14625.2, bsz=441.8, num_updates=10700, lr=0.000341793, gnorm=3.284, clip=1, loss_scale=64, train_wall=102, wall=11467
2022-10-12 10:12:37 | INFO | train_inner | epoch 024:    260 / 459 loss=8.181, nll_loss=4.672, mask_ins=1.339, word_ins_ml=6.082, word_reposition=0.76, ppl=290.15, wps=14123.9, ups=0.98, wpb=14449.1, bsz=439.2, num_updates=10800, lr=0.000340207, gnorm=6.102, clip=4, loss_scale=64, train_wall=101, wall=11569
2022-10-12 10:14:19 | INFO | train_inner | epoch 024:    360 / 459 loss=8.19, nll_loss=4.695, mask_ins=1.333, word_ins_ml=6.102, word_reposition=0.755, ppl=291.97, wps=14156.6, ups=0.97, wpb=14546.9, bsz=441.2, num_updates=10900, lr=0.000338643, gnorm=10.25, clip=8, loss_scale=64, train_wall=102, wall=11672
2022-10-12 10:16:01 | INFO | train | epoch 024 | loss 8.192 | nll_loss 4.683 | mask_ins 1.345 | word_ins_ml 6.091 | word_reposition 0.756 | ppl 292.45 | wps 13835.6 | ups 0.95 | wpb 14640.3 | bsz 444 | num_updates 10999 | lr 0.000337115 | gnorm 5.545 | clip 3.5 | loss_scale 64 | train_wall 469 | wall 11774
2022-10-12 10:16:11 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 13.202 | nll_loss 9.691 | mask_ins 1.642 | word_ins_ml 10.637 | word_reposition 0.923 | ppl 9426.28 | wps 35072.3 | wpb 1628.7 | bsz 55.8 | num_updates 10999 | best_loss 13.037
2022-10-12 10:16:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 24 @ 10999 updates, score 13.202) (writing took 2.898596020997502 seconds)
2022-10-12 10:16:15 | INFO | train_inner | epoch 025:      1 / 459 loss=8.193, nll_loss=4.683, mask_ins=1.348, word_ins_ml=6.091, word_reposition=0.754, ppl=292.67, wps=12533.7, ups=0.87, wpb=14440.8, bsz=437.4, num_updates=11000, lr=0.0003371, gnorm=4.556, clip=3, loss_scale=64, train_wall=102, wall=11787
2022-10-12 10:17:58 | INFO | train_inner | epoch 025:    101 / 459 loss=8.135, nll_loss=4.623, mask_ins=1.348, word_ins_ml=6.04, word_reposition=0.747, ppl=281.15, wps=14176.1, ups=0.97, wpb=14608.8, bsz=442.5, num_updates=11100, lr=0.000335578, gnorm=4.622, clip=4, loss_scale=107, train_wall=102, wall=11890
2022-10-12 10:19:42 | INFO | train_inner | epoch 025:    201 / 459 loss=8.101, nll_loss=4.599, mask_ins=1.333, word_ins_ml=6.019, word_reposition=0.75, ppl=274.49, wps=14099.3, ups=0.96, wpb=14715.5, bsz=445.8, num_updates=11200, lr=0.000334077, gnorm=4.219, clip=4, loss_scale=128, train_wall=103, wall=11994
2022-10-12 10:21:25 | INFO | train_inner | epoch 025:    301 / 459 loss=8.109, nll_loss=4.605, mask_ins=1.334, word_ins_ml=6.024, word_reposition=0.752, ppl=276.17, wps=14309.9, ups=0.97, wpb=14796.1, bsz=451.4, num_updates=11300, lr=0.000332595, gnorm=3.917, clip=2, loss_scale=128, train_wall=102, wall=12098
2022-10-12 10:23:08 | INFO | train_inner | epoch 025:    401 / 459 loss=8.138, nll_loss=4.639, mask_ins=1.333, word_ins_ml=6.053, word_reposition=0.752, ppl=281.63, wps=13960.4, ups=0.98, wpb=14281.4, bsz=433.2, num_updates=11400, lr=0.000331133, gnorm=3.015, clip=0, loss_scale=128, train_wall=101, wall=12200
2022-10-12 10:24:08 | INFO | train | epoch 025 | loss 8.125 | nll_loss 4.62 | mask_ins 1.337 | word_ins_ml 6.037 | word_reposition 0.751 | ppl 279.13 | wps 13828.2 | ups 0.94 | wpb 14644.1 | bsz 444.1 | num_updates 11458 | lr 0.000330294 | gnorm 3.838 | clip 2.2 | loss_scale 123 | train_wall 470 | wall 12260
2022-10-12 10:24:17 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 13.289 | nll_loss 9.754 | mask_ins 1.609 | word_ins_ml 10.699 | word_reposition 0.981 | ppl 10009.2 | wps 35018.4 | wpb 1628.7 | bsz 55.8 | num_updates 11458 | best_loss 13.037
2022-10-12 10:24:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 25 @ 11458 updates, score 13.289) (writing took 2.7139975559985032 seconds)
2022-10-12 10:25:02 | INFO | train_inner | epoch 026:     42 / 459 loss=8.131, nll_loss=4.617, mask_ins=1.337, word_ins_ml=6.034, word_reposition=0.76, ppl=280.31, wps=12713, ups=0.87, wpb=14582.2, bsz=439.9, num_updates=11500, lr=0.00032969, gnorm=2.76, clip=0, loss_scale=128, train_wall=102, wall=12315
2022-10-12 10:26:47 | INFO | train_inner | epoch 026:    142 / 459 loss=8.028, nll_loss=4.542, mask_ins=1.314, word_ins_ml=5.969, word_reposition=0.745, ppl=261.01, wps=14531.1, ups=0.96, wpb=15154.8, bsz=460.3, num_updates=11600, lr=0.000328266, gnorm=4.25, clip=3, loss_scale=198, train_wall=103, wall=12419
2022-10-12 10:28:30 | INFO | train_inner | epoch 026:    242 / 459 loss=8.054, nll_loss=4.566, mask_ins=1.325, word_ins_ml=5.99, word_reposition=0.739, ppl=265.72, wps=14139.9, ups=0.97, wpb=14541.5, bsz=442.8, num_updates=11700, lr=0.00032686, gnorm=2.885, clip=1, loss_scale=256, train_wall=102, wall=12522
2022-10-12 10:30:13 | INFO | train_inner | epoch 026:    342 / 459 loss=8.068, nll_loss=4.581, mask_ins=1.318, word_ins_ml=6.003, word_reposition=0.747, ppl=268.44, wps=14120.7, ups=0.97, wpb=14593.5, bsz=442.9, num_updates=11800, lr=0.000325472, gnorm=3.112, clip=0, loss_scale=256, train_wall=102, wall=12625
2022-10-12 10:31:55 | INFO | train_inner | epoch 026:    442 / 459 loss=8.081, nll_loss=4.589, mask_ins=1.324, word_ins_ml=6.009, word_reposition=0.748, ppl=270.79, wps=14315.7, ups=0.98, wpb=14630.6, bsz=441.3, num_updates=11900, lr=0.000324102, gnorm=3.772, clip=0, loss_scale=256, train_wall=101, wall=12727
2022-10-12 10:32:12 | INFO | train | epoch 026 | loss 8.06 | nll_loss 4.57 | mask_ins 1.322 | word_ins_ml 5.993 | word_reposition 0.745 | ppl 266.85 | wps 13863.8 | ups 0.95 | wpb 14641.7 | bsz 444.1 | num_updates 11917 | lr 0.000323871 | gnorm 3.406 | clip 0.9 | loss_scale 232 | train_wall 469 | wall 12745
2022-10-12 10:32:22 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 13.227 | nll_loss 9.715 | mask_ins 1.656 | word_ins_ml 10.659 | word_reposition 0.912 | ppl 9588.78 | wps 35016.3 | wpb 1628.7 | bsz 55.8 | num_updates 11917 | best_loss 13.037
2022-10-12 10:32:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 26 @ 11917 updates, score 13.227) (writing took 2.7186686860077316 seconds)
2022-10-12 10:33:51 | INFO | train_inner | epoch 027:     83 / 459 loss=8.003, nll_loss=4.519, mask_ins=1.316, word_ins_ml=5.948, word_reposition=0.738, ppl=256.51, wps=12675.8, ups=0.86, wpb=14696.2, bsz=446.1, num_updates=12000, lr=0.000322749, gnorm=3.328, clip=0, loss_scale=256, train_wall=103, wall=12843
2022-10-12 10:35:35 | INFO | train_inner | epoch 027:    183 / 459 loss=7.977, nll_loss=4.48, mask_ins=1.315, word_ins_ml=5.915, word_reposition=0.746, ppl=251.92, wps=13862.6, ups=0.96, wpb=14421, bsz=435.8, num_updates=12100, lr=0.000321412, gnorm=2.541, clip=0, loss_scale=366, train_wall=103, wall=12947
2022-10-12 10:37:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 10:37:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 10:37:21 | INFO | train_inner | epoch 027:    285 / 459 loss=8.071, nll_loss=4.569, mask_ins=1.331, word_ins_ml=5.993, word_reposition=0.747, ppl=269.01, wps=14006.1, ups=0.94, wpb=14888.1, bsz=451, num_updates=12200, lr=0.000320092, gnorm=5.242, clip=2, loss_scale=444, train_wall=105, wall=13054
2022-10-12 10:39:05 | INFO | train_inner | epoch 027:    385 / 459 loss=8.019, nll_loss=4.525, mask_ins=1.323, word_ins_ml=5.955, word_reposition=0.741, ppl=259.35, wps=13925.8, ups=0.96, wpb=14476.7, bsz=439.6, num_updates=12300, lr=0.000318788, gnorm=4.057, clip=1, loss_scale=128, train_wall=103, wall=13158
2022-10-12 10:40:23 | INFO | train | epoch 027 | loss 8.01 | nll_loss 4.517 | mask_ins 1.32 | word_ins_ml 5.947 | word_reposition 0.743 | ppl 257.75 | wps 13644.2 | ups 0.93 | wpb 14642.9 | bsz 444.2 | num_updates 12374 | lr 0.000317834 | gnorm 3.732 | clip 0.7 | loss_scale 273 | train_wall 474 | wall 13235
2022-10-12 10:40:32 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 13.322 | nll_loss 9.76 | mask_ins 1.65 | word_ins_ml 10.71 | word_reposition 0.962 | ppl 10241.2 | wps 34950.9 | wpb 1628.7 | bsz 55.8 | num_updates 12374 | best_loss 13.037
2022-10-12 10:40:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 27 @ 12374 updates, score 13.322) (writing took 2.711927996992017 seconds)
2022-10-12 10:41:02 | INFO | train_inner | epoch 028:     26 / 459 loss=7.99, nll_loss=4.504, mask_ins=1.315, word_ins_ml=5.936, word_reposition=0.74, ppl=254.25, wps=12637.3, ups=0.86, wpb=14772.3, bsz=449.1, num_updates=12400, lr=0.0003175, gnorm=4.552, clip=2, loss_scale=128, train_wall=104, wall=13275
2022-10-12 10:42:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 10:42:47 | INFO | train_inner | epoch 028:    127 / 459 loss=7.899, nll_loss=4.423, mask_ins=1.301, word_ins_ml=5.865, word_reposition=0.732, ppl=238.64, wps=13897.4, ups=0.96, wpb=14531.2, bsz=443.8, num_updates=12500, lr=0.000316228, gnorm=4.005, clip=1, loss_scale=109, train_wall=104, wall=13379
2022-10-12 10:44:30 | INFO | train_inner | epoch 028:    227 / 459 loss=8, nll_loss=4.512, mask_ins=1.311, word_ins_ml=5.943, word_reposition=0.746, ppl=256.07, wps=14053.9, ups=0.97, wpb=14452, bsz=433.4, num_updates=12600, lr=0.00031497, gnorm=4.307, clip=1, loss_scale=64, train_wall=102, wall=13482
2022-10-12 10:46:12 | INFO | train_inner | epoch 028:    327 / 459 loss=7.961, nll_loss=4.492, mask_ins=1.309, word_ins_ml=5.925, word_reposition=0.727, ppl=249.22, wps=14268.4, ups=0.97, wpb=14662.7, bsz=448.6, num_updates=12700, lr=0.000313728, gnorm=3.515, clip=1, loss_scale=64, train_wall=102, wall=13585
2022-10-12 10:47:55 | INFO | train_inner | epoch 028:    427 / 459 loss=7.976, nll_loss=4.491, mask_ins=1.317, word_ins_ml=5.924, word_reposition=0.735, ppl=251.76, wps=14186.5, ups=0.97, wpb=14572, bsz=442.9, num_updates=12800, lr=0.0003125, gnorm=3.763, clip=1, loss_scale=64, train_wall=102, wall=13687
2022-10-12 10:48:28 | INFO | train | epoch 028 | loss 7.969 | nll_loss 4.487 | mask_ins 1.311 | word_ins_ml 5.921 | word_reposition 0.736 | ppl 250.51 | wps 13801.6 | ups 0.94 | wpb 14635.3 | bsz 443.9 | num_updates 12832 | lr 0.00031211 | gnorm 4.037 | clip 1.3 | loss_scale 78 | train_wall 469 | wall 13721
2022-10-12 10:48:38 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 13.247 | nll_loss 9.767 | mask_ins 1.646 | word_ins_ml 10.714 | word_reposition 0.888 | ppl 9720.54 | wps 35096.6 | wpb 1628.7 | bsz 55.8 | num_updates 12832 | best_loss 13.037
2022-10-12 10:48:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 28 @ 12832 updates, score 13.247) (writing took 2.586840903997654 seconds)
2022-10-12 10:49:51 | INFO | train_inner | epoch 029:     68 / 459 loss=7.947, nll_loss=4.454, mask_ins=1.316, word_ins_ml=5.892, word_reposition=0.74, ppl=246.8, wps=12857.9, ups=0.87, wpb=14860.2, bsz=451.1, num_updates=12900, lr=0.000311286, gnorm=2.35, clip=0, loss_scale=64, train_wall=103, wall=13803
2022-10-12 10:51:35 | INFO | train_inner | epoch 029:    168 / 459 loss=8.007, nll_loss=4.531, mask_ins=1.312, word_ins_ml=5.959, word_reposition=0.735, ppl=257.16, wps=13895.4, ups=0.96, wpb=14425.1, bsz=435.6, num_updates=13000, lr=0.000310087, gnorm=8.5, clip=6, loss_scale=76, train_wall=103, wall=13907
2022-10-12 10:52:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 10:53:19 | INFO | train_inner | epoch 029:    269 / 459 loss=7.984, nll_loss=4.514, mask_ins=1.304, word_ins_ml=5.945, word_reposition=0.735, ppl=253.18, wps=14171.3, ups=0.96, wpb=14809.6, bsz=449.8, num_updates=13100, lr=0.000308901, gnorm=8.279, clip=6, loss_scale=93, train_wall=104, wall=14011
2022-10-12 10:55:02 | INFO | train_inner | epoch 029:    369 / 459 loss=7.936, nll_loss=4.451, mask_ins=1.307, word_ins_ml=5.89, word_reposition=0.738, ppl=244.89, wps=14001.4, ups=0.97, wpb=14464, bsz=436.7, num_updates=13200, lr=0.000307729, gnorm=3.566, clip=1, loss_scale=64, train_wall=102, wall=14115
2022-10-12 10:56:36 | INFO | train | epoch 029 | loss 7.953 | nll_loss 4.475 | mask_ins 1.308 | word_ins_ml 5.911 | word_reposition 0.734 | ppl 247.81 | wps 13760.6 | ups 0.94 | wpb 14639.1 | bsz 443.9 | num_updates 13290 | lr 0.000306685 | gnorm 5.318 | clip 2.8 | loss_scale 73 | train_wall 471 | wall 14208
2022-10-12 10:56:45 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 13.311 | nll_loss 9.728 | mask_ins 1.673 | word_ins_ml 10.677 | word_reposition 0.962 | ppl 10164.4 | wps 34958.4 | wpb 1628.7 | bsz 55.8 | num_updates 13290 | best_loss 13.037
2022-10-12 10:56:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 29 @ 13290 updates, score 13.311) (writing took 2.7161574130004738 seconds)
2022-10-12 10:56:58 | INFO | train_inner | epoch 030:     10 / 459 loss=7.917, nll_loss=4.452, mask_ins=1.303, word_ins_ml=5.891, word_reposition=0.722, ppl=241.63, wps=12759.5, ups=0.87, wpb=14750.5, bsz=446.5, num_updates=13300, lr=0.00030657, gnorm=2.691, clip=0, loss_scale=64, train_wall=103, wall=14230
2022-10-12 10:58:42 | INFO | train_inner | epoch 030:    110 / 459 loss=7.895, nll_loss=4.421, mask_ins=1.298, word_ins_ml=5.865, word_reposition=0.731, ppl=238.05, wps=14421.8, ups=0.96, wpb=15026.9, bsz=455.3, num_updates=13400, lr=0.000305424, gnorm=5.943, clip=4, loss_scale=64, train_wall=103, wall=14334
2022-10-12 11:00:25 | INFO | train_inner | epoch 030:    210 / 459 loss=7.874, nll_loss=4.392, mask_ins=1.305, word_ins_ml=5.839, word_reposition=0.73, ppl=234.59, wps=14092.2, ups=0.97, wpb=14497, bsz=437.3, num_updates=13500, lr=0.00030429, gnorm=3.978, clip=2, loss_scale=64, train_wall=102, wall=14437
2022-10-12 11:02:08 | INFO | train_inner | epoch 030:    310 / 459 loss=7.945, nll_loss=4.484, mask_ins=1.303, word_ins_ml=5.919, word_reposition=0.724, ppl=246.46, wps=14187.5, ups=0.97, wpb=14679.1, bsz=447.5, num_updates=13600, lr=0.00030317, gnorm=8.526, clip=8, loss_scale=92, train_wall=103, wall=14541
2022-10-12 11:03:52 | INFO | train_inner | epoch 030:    410 / 459 loss=7.891, nll_loss=4.419, mask_ins=1.298, word_ins_ml=5.863, word_reposition=0.73, ppl=237.32, wps=13947.9, ups=0.97, wpb=14405.7, bsz=436.7, num_updates=13700, lr=0.000302061, gnorm=3.923, clip=2, loss_scale=128, train_wall=102, wall=14644
2022-10-12 11:04:42 | INFO | train | epoch 030 | loss 7.894 | nll_loss 4.425 | mask_ins 1.299 | word_ins_ml 5.868 | word_reposition 0.727 | ppl 237.93 | wps 13800.7 | ups 0.94 | wpb 14633.2 | bsz 443.7 | num_updates 13749 | lr 0.000301522 | gnorm 5.203 | clip 3.5 | loss_scale 91 | train_wall 470 | wall 14695
2022-10-12 11:04:52 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 13.271 | nll_loss 9.785 | mask_ins 1.649 | word_ins_ml 10.733 | word_reposition 0.888 | ppl 9883.22 | wps 35064.6 | wpb 1628.7 | bsz 55.8 | num_updates 13749 | best_loss 13.037
2022-10-12 11:04:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 30 @ 13749 updates, score 13.271) (writing took 2.7438540940056555 seconds)
2022-10-12 11:05:47 | INFO | train_inner | epoch 031:     51 / 459 loss=7.815, nll_loss=4.361, mask_ins=1.286, word_ins_ml=5.812, word_reposition=0.717, ppl=225.11, wps=12539.2, ups=0.87, wpb=14398.5, bsz=438.2, num_updates=13800, lr=0.000300965, gnorm=2.861, clip=0, loss_scale=128, train_wall=102, wall=14759
2022-10-12 11:06:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 11:07:31 | INFO | train_inner | epoch 031:    152 / 459 loss=7.874, nll_loss=4.398, mask_ins=1.3, word_ins_ml=5.845, word_reposition=0.729, ppl=234.59, wps=13941.6, ups=0.96, wpb=14511.9, bsz=435.1, num_updates=13900, lr=0.00029988, gnorm=3.342, clip=1, loss_scale=91, train_wall=103, wall=14863
2022-10-12 11:09:14 | INFO | train_inner | epoch 031:    252 / 459 loss=7.869, nll_loss=4.399, mask_ins=1.304, word_ins_ml=5.845, word_reposition=0.72, ppl=233.86, wps=14138.5, ups=0.97, wpb=14550.6, bsz=442.6, num_updates=14000, lr=0.000298807, gnorm=7.267, clip=5, loss_scale=64, train_wall=102, wall=14966
2022-10-12 11:10:57 | INFO | train_inner | epoch 031:    352 / 459 loss=7.848, nll_loss=4.375, mask_ins=1.299, word_ins_ml=5.824, word_reposition=0.725, ppl=230.37, wps=14244.9, ups=0.96, wpb=14763.1, bsz=449.9, num_updates=14100, lr=0.000297746, gnorm=3.758, clip=1, loss_scale=64, train_wall=103, wall=15070
2022-10-12 11:12:41 | INFO | train_inner | epoch 031:    452 / 459 loss=7.87, nll_loss=4.421, mask_ins=1.282, word_ins_ml=5.864, word_reposition=0.724, ppl=233.91, wps=14464.8, ups=0.96, wpb=15063.2, bsz=458.2, num_updates=14200, lr=0.000296695, gnorm=3.383, clip=1, loss_scale=64, train_wall=103, wall=15174
2022-10-12 11:12:48 | INFO | train | epoch 031 | loss 7.857 | nll_loss 4.392 | mask_ins 1.295 | word_ins_ml 5.839 | word_reposition 0.723 | ppl 231.85 | wps 13793 | ups 0.94 | wpb 14637.9 | bsz 443.9 | num_updates 14207 | lr 0.000296622 | gnorm 4.262 | clip 1.7 | loss_scale 77 | train_wall 470 | wall 15181
2022-10-12 11:12:58 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 13.246 | nll_loss 9.698 | mask_ins 1.642 | word_ins_ml 10.658 | word_reposition 0.945 | ppl 9711.75 | wps 35007.5 | wpb 1628.7 | bsz 55.8 | num_updates 14207 | best_loss 13.037
2022-10-12 11:13:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 31 @ 14207 updates, score 13.246) (writing took 2.764871506005875 seconds)
2022-10-12 11:14:36 | INFO | train_inner | epoch 032:     93 / 459 loss=7.782, nll_loss=4.315, mask_ins=1.288, word_ins_ml=5.773, word_reposition=0.721, ppl=220.1, wps=12812.3, ups=0.87, wpb=14718.6, bsz=446.8, num_updates=14300, lr=0.000295656, gnorm=3.911, clip=2, loss_scale=64, train_wall=102, wall=15289
2022-10-12 11:16:21 | INFO | train_inner | epoch 032:    193 / 459 loss=7.852, nll_loss=4.366, mask_ins=1.288, word_ins_ml=5.816, word_reposition=0.748, ppl=231.08, wps=14180.9, ups=0.95, wpb=14884.6, bsz=450.4, num_updates=14400, lr=0.000294628, gnorm=3.744, clip=2, loss_scale=94, train_wall=104, wall=15393
2022-10-12 11:18:04 | INFO | train_inner | epoch 032:    293 / 459 loss=7.796, nll_loss=4.348, mask_ins=1.279, word_ins_ml=5.801, word_reposition=0.717, ppl=222.25, wps=14096.5, ups=0.97, wpb=14511.1, bsz=440.3, num_updates=14500, lr=0.00029361, gnorm=3.343, clip=2, loss_scale=128, train_wall=102, wall=15496
2022-10-12 11:19:48 | INFO | train_inner | epoch 032:    393 / 459 loss=7.813, nll_loss=4.37, mask_ins=1.286, word_ins_ml=5.82, word_reposition=0.708, ppl=224.87, wps=14003.4, ups=0.96, wpb=14559.8, bsz=441.2, num_updates=14600, lr=0.000292603, gnorm=3.009, clip=0, loss_scale=128, train_wall=103, wall=15600
2022-10-12 11:20:56 | INFO | train | epoch 032 | loss 7.821 | nll_loss 4.356 | mask_ins 1.288 | word_ins_ml 5.808 | word_reposition 0.725 | ppl 226.15 | wps 13789.7 | ups 0.94 | wpb 14634.8 | bsz 443.7 | num_updates 14666 | lr 0.000291944 | gnorm 3.68 | clip 1.7 | loss_scale 108 | train_wall 471 | wall 15668
2022-10-12 11:21:05 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 13.363 | nll_loss 9.781 | mask_ins 1.653 | word_ins_ml 10.73 | word_reposition 0.98 | ppl 10534.2 | wps 35016.5 | wpb 1628.7 | bsz 55.8 | num_updates 14666 | best_loss 13.037
2022-10-12 11:21:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 32 @ 14666 updates, score 13.363) (writing took 2.766495304007549 seconds)
2022-10-12 11:21:42 | INFO | train_inner | epoch 033:     34 / 459 loss=7.84, nll_loss=4.364, mask_ins=1.301, word_ins_ml=5.814, word_reposition=0.725, ppl=229.12, wps=12498.4, ups=0.88, wpb=14269, bsz=432.5, num_updates=14700, lr=0.000291606, gnorm=3.903, clip=2, loss_scale=128, train_wall=101, wall=15715
2022-10-12 11:23:26 | INFO | train_inner | epoch 033:    134 / 459 loss=7.757, nll_loss=4.297, mask_ins=1.292, word_ins_ml=5.756, word_reposition=0.709, ppl=216.35, wps=14043.7, ups=0.96, wpb=14600.6, bsz=441.7, num_updates=14800, lr=0.000290619, gnorm=2.713, clip=1, loss_scale=128, train_wall=103, wall=15819
2022-10-12 11:24:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 11:25:12 | INFO | train_inner | epoch 033:    235 / 459 loss=7.764, nll_loss=4.303, mask_ins=1.285, word_ins_ml=5.761, word_reposition=0.718, ppl=217.34, wps=14005.4, ups=0.95, wpb=14818.5, bsz=450.9, num_updates=14900, lr=0.000289642, gnorm=6.97, clip=4, loss_scale=153, train_wall=105, wall=15924
2022-10-12 11:26:58 | INFO | train_inner | epoch 033:    335 / 459 loss=7.802, nll_loss=4.335, mask_ins=1.288, word_ins_ml=5.789, word_reposition=0.726, ppl=223.24, wps=14099.6, ups=0.95, wpb=14881.2, bsz=451.4, num_updates=15000, lr=0.000288675, gnorm=6.002, clip=2, loss_scale=128, train_wall=105, wall=16030
2022-10-12 11:28:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 11:28:43 | INFO | train_inner | epoch 033:    436 / 459 loss=7.796, nll_loss=4.351, mask_ins=1.277, word_ins_ml=5.802, word_reposition=0.716, ppl=222.25, wps=13701.3, ups=0.95, wpb=14495.8, bsz=439.1, num_updates=15100, lr=0.000287718, gnorm=9.659, clip=5, loss_scale=113, train_wall=105, wall=16136
2022-10-12 11:29:07 | INFO | train | epoch 033 | loss 7.781 | nll_loss 4.323 | mask_ins 1.285 | word_ins_ml 5.778 | word_reposition 0.717 | ppl 219.88 | wps 13614.6 | ups 0.93 | wpb 14638.1 | bsz 443.8 | num_updates 15123 | lr 0.000287499 | gnorm 6.617 | clip 3.1 | loss_scale 127 | train_wall 475 | wall 16159
2022-10-12 11:29:16 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 13.315 | nll_loss 9.741 | mask_ins 1.645 | word_ins_ml 10.693 | word_reposition 0.977 | ppl 10191 | wps 34943.9 | wpb 1628.7 | bsz 55.8 | num_updates 15123 | best_loss 13.037
2022-10-12 11:29:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 33 @ 15123 updates, score 13.315) (writing took 2.7762834479945013 seconds)
2022-10-12 11:30:38 | INFO | train_inner | epoch 034:     77 / 459 loss=7.781, nll_loss=4.308, mask_ins=1.293, word_ins_ml=5.766, word_reposition=0.723, ppl=220, wps=12414.8, ups=0.87, wpb=14231.2, bsz=433.8, num_updates=15200, lr=0.00028677, gnorm=11.675, clip=10, loss_scale=64, train_wall=102, wall=16250
2022-10-12 11:31:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 11:31:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2022-10-12 11:32:24 | INFO | train_inner | epoch 034:    179 / 459 loss=7.788, nll_loss=4.341, mask_ins=1.282, word_ins_ml=5.794, word_reposition=0.711, ppl=220.95, wps=13856.9, ups=0.94, wpb=14682.7, bsz=446, num_updates=15300, lr=0.000285831, gnorm=23.198, clip=9, loss_scale=36, train_wall=105, wall=16356
2022-10-12 11:34:07 | INFO | train_inner | epoch 034:    279 / 459 loss=7.779, nll_loss=4.327, mask_ins=1.288, word_ins_ml=5.783, word_reposition=0.708, ppl=219.59, wps=14163.8, ups=0.97, wpb=14657.2, bsz=441.1, num_updates=15400, lr=0.000284901, gnorm=7.735, clip=6, loss_scale=16, train_wall=103, wall=16460
2022-10-12 11:35:50 | INFO | train_inner | epoch 034:    379 / 459 loss=7.755, nll_loss=4.285, mask_ins=1.288, word_ins_ml=5.746, word_reposition=0.722, ppl=216.06, wps=14183.2, ups=0.98, wpb=14543.7, bsz=441.3, num_updates=15500, lr=0.000283981, gnorm=5.032, clip=3, loss_scale=16, train_wall=102, wall=16562
2022-10-12 11:37:14 | INFO | train | epoch 034 | loss 7.771 | nll_loss 4.31 | mask_ins 1.287 | word_ins_ml 5.768 | word_reposition 0.716 | ppl 218.36 | wps 13723.5 | ups 0.94 | wpb 14638.3 | bsz 443.9 | num_updates 15580 | lr 0.000283251 | gnorm 10.05 | clip 6.1 | loss_scale 28 | train_wall 471 | wall 16647
2022-10-12 11:37:24 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 13.458 | nll_loss 9.817 | mask_ins 1.651 | word_ins_ml 10.762 | word_reposition 1.045 | ppl 11251.1 | wps 35012 | wpb 1628.7 | bsz 55.8 | num_updates 15580 | best_loss 13.037
2022-10-12 11:37:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 34 @ 15580 updates, score 13.458) (writing took 2.856077465999988 seconds)
2022-10-12 11:37:47 | INFO | train_inner | epoch 035:     20 / 459 loss=7.749, nll_loss=4.292, mask_ins=1.28, word_ins_ml=5.751, word_reposition=0.718, ppl=215.18, wps=12765.7, ups=0.85, wpb=14946.2, bsz=454.3, num_updates=15600, lr=0.000283069, gnorm=3.665, clip=3, loss_scale=16, train_wall=104, wall=16679
2022-10-12 11:39:32 | INFO | train_inner | epoch 035:    120 / 459 loss=7.731, nll_loss=4.26, mask_ins=1.289, word_ins_ml=5.724, word_reposition=0.718, ppl=212.41, wps=14062.6, ups=0.95, wpb=14731.7, bsz=447.7, num_updates=15700, lr=0.000282166, gnorm=3.69, clip=2, loss_scale=16, train_wall=104, wall=16784
2022-10-12 11:41:16 | INFO | train_inner | epoch 035:    220 / 459 loss=7.746, nll_loss=4.288, mask_ins=1.29, word_ins_ml=5.748, word_reposition=0.709, ppl=214.74, wps=13906.2, ups=0.96, wpb=14486.4, bsz=437.7, num_updates=15800, lr=0.000281272, gnorm=10.83, clip=6, loss_scale=21, train_wall=103, wall=16888
2022-10-12 11:43:00 | INFO | train_inner | epoch 035:    320 / 459 loss=7.72, nll_loss=4.275, mask_ins=1.271, word_ins_ml=5.737, word_reposition=0.711, ppl=210.85, wps=13987.8, ups=0.96, wpb=14507.7, bsz=439.3, num_updates=15900, lr=0.000280386, gnorm=10.845, clip=6, loss_scale=32, train_wall=103, wall=16992
2022-10-12 11:44:44 | INFO | train_inner | epoch 035:    420 / 459 loss=7.773, nll_loss=4.318, mask_ins=1.285, word_ins_ml=5.774, word_reposition=0.715, ppl=218.72, wps=14147.1, ups=0.96, wpb=14781.4, bsz=448.7, num_updates=16000, lr=0.000279508, gnorm=7.937, clip=4, loss_scale=32, train_wall=104, wall=17097
2022-10-12 11:45:25 | INFO | train | epoch 035 | loss 7.745 | nll_loss 4.287 | mask_ins 1.283 | word_ins_ml 5.748 | word_reposition 0.714 | ppl 214.58 | wps 13699.4 | ups 0.94 | wpb 14640.9 | bsz 444 | num_updates 16039 | lr 0.000279168 | gnorm 8.012 | clip 4.6 | loss_scale 25 | train_wall 474 | wall 17137
2022-10-12 11:45:34 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 13.311 | nll_loss 9.768 | mask_ins 1.655 | word_ins_ml 10.717 | word_reposition 0.939 | ppl 10162.6 | wps 34940.4 | wpb 1628.7 | bsz 55.8 | num_updates 16039 | best_loss 13.037
2022-10-12 11:45:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 35 @ 16039 updates, score 13.311) (writing took 2.8255949360027444 seconds)
2022-10-12 11:46:41 | INFO | train_inner | epoch 036:     61 / 459 loss=7.76, nll_loss=4.289, mask_ins=1.289, word_ins_ml=5.749, word_reposition=0.722, ppl=216.75, wps=12478.9, ups=0.85, wpb=14596.9, bsz=443.6, num_updates=16100, lr=0.000278639, gnorm=9.905, clip=6, loss_scale=32, train_wall=104, wall=17213
2022-10-12 11:48:26 | INFO | train_inner | epoch 036:    161 / 459 loss=7.722, nll_loss=4.261, mask_ins=1.276, word_ins_ml=5.725, word_reposition=0.721, ppl=211.1, wps=14112.6, ups=0.95, wpb=14785.7, bsz=446.8, num_updates=16200, lr=0.000277778, gnorm=16.317, clip=7, loss_scale=32, train_wall=104, wall=17318
2022-10-12 11:50:10 | INFO | train_inner | epoch 036:    261 / 459 loss=7.727, nll_loss=4.283, mask_ins=1.27, word_ins_ml=5.744, word_reposition=0.714, ppl=211.89, wps=14524.8, ups=0.96, wpb=15113.6, bsz=461.1, num_updates=16300, lr=0.000276924, gnorm=9.617, clip=8, loss_scale=39, train_wall=103, wall=17422
2022-10-12 11:51:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 11:51:55 | INFO | train_inner | epoch 036:    362 / 459 loss=7.73, nll_loss=4.274, mask_ins=1.279, word_ins_ml=5.736, word_reposition=0.714, ppl=212.26, wps=13944.9, ups=0.95, wpb=14687.3, bsz=445.4, num_updates=16400, lr=0.000276079, gnorm=8.349, clip=5, loss_scale=47, train_wall=104, wall=17528
2022-10-12 11:53:36 | INFO | train | epoch 036 | loss 7.728 | nll_loss 4.274 | mask_ins 1.278 | word_ins_ml 5.736 | word_reposition 0.714 | ppl 212.04 | wps 13662.9 | ups 0.93 | wpb 14641.1 | bsz 444.2 | num_updates 16497 | lr 0.000275266 | gnorm 11.975 | clip 7.4 | loss_scale 37 | train_wall 474 | wall 17628
2022-10-12 11:53:45 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 13.331 | nll_loss 9.776 | mask_ins 1.651 | word_ins_ml 10.727 | word_reposition 0.953 | ppl 10301.4 | wps 34717 | wpb 1628.7 | bsz 55.8 | num_updates 16497 | best_loss 13.037
2022-10-12 11:53:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 36 @ 16497 updates, score 13.331) (writing took 2.834959665997303 seconds)
2022-10-12 11:53:51 | INFO | train_inner | epoch 037:      3 / 459 loss=7.719, nll_loss=4.278, mask_ins=1.274, word_ins_ml=5.739, word_reposition=0.706, ppl=210.66, wps=12155.7, ups=0.86, wpb=14054.4, bsz=424.7, num_updates=16500, lr=0.000275241, gnorm=12.955, clip=10, loss_scale=32, train_wall=103, wall=17643
2022-10-12 11:55:36 | INFO | train_inner | epoch 037:    103 / 459 loss=7.685, nll_loss=4.241, mask_ins=1.269, word_ins_ml=5.708, word_reposition=0.708, ppl=205.84, wps=14141.9, ups=0.95, wpb=14875.9, bsz=452.1, num_updates=16600, lr=0.000274411, gnorm=9.728, clip=6, loss_scale=32, train_wall=104, wall=17748
2022-10-12 11:57:21 | INFO | train_inner | epoch 037:    203 / 459 loss=7.705, nll_loss=4.258, mask_ins=1.272, word_ins_ml=5.723, word_reposition=0.709, ppl=208.59, wps=13963.9, ups=0.95, wpb=14680.4, bsz=444, num_updates=16700, lr=0.000273588, gnorm=6.237, clip=4, loss_scale=32, train_wall=104, wall=17854
2022-10-12 11:59:05 | INFO | train_inner | epoch 037:    303 / 459 loss=7.653, nll_loss=4.21, mask_ins=1.272, word_ins_ml=5.681, word_reposition=0.699, ppl=201.23, wps=14090.4, ups=0.96, wpb=14667.7, bsz=445, num_updates=16800, lr=0.000272772, gnorm=3.066, clip=0, loss_scale=32, train_wall=103, wall=17958
2022-10-12 12:00:49 | INFO | train_inner | epoch 037:    403 / 459 loss=7.641, nll_loss=4.223, mask_ins=1.251, word_ins_ml=5.691, word_reposition=0.699, ppl=199.61, wps=14090.8, ups=0.96, wpb=14614, bsz=443.5, num_updates=16900, lr=0.000271964, gnorm=3.491, clip=0, loss_scale=45, train_wall=103, wall=18061
2022-10-12 12:01:46 | INFO | train | epoch 037 | loss 7.67 | nll_loss 4.234 | mask_ins 1.265 | word_ins_ml 5.701 | word_reposition 0.703 | ppl 203.69 | wps 13690 | ups 0.94 | wpb 14637.4 | bsz 444 | num_updates 16956 | lr 0.000271515 | gnorm 5.267 | clip 2.2 | loss_scale 39 | train_wall 474 | wall 18119
2022-10-12 12:01:56 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 13.414 | nll_loss 9.804 | mask_ins 1.703 | word_ins_ml 10.748 | word_reposition 0.963 | ppl 10917.4 | wps 34957.1 | wpb 1628.7 | bsz 55.8 | num_updates 16956 | best_loss 13.037
2022-10-12 12:01:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 37 @ 16956 updates, score 13.414) (writing took 2.7759671160019934 seconds)
2022-10-12 12:02:44 | INFO | train_inner | epoch 038:     44 / 459 loss=7.65, nll_loss=4.22, mask_ins=1.26, word_ins_ml=5.689, word_reposition=0.701, ppl=200.88, wps=12525.9, ups=0.87, wpb=14374.9, bsz=436.5, num_updates=17000, lr=0.000271163, gnorm=2.923, clip=0, loss_scale=64, train_wall=102, wall=18176
2022-10-12 12:04:28 | INFO | train_inner | epoch 038:    144 / 459 loss=7.638, nll_loss=4.189, mask_ins=1.264, word_ins_ml=5.663, word_reposition=0.712, ppl=199.22, wps=14127.3, ups=0.96, wpb=14673.3, bsz=442.5, num_updates=17100, lr=0.000270369, gnorm=3.274, clip=1, loss_scale=64, train_wall=103, wall=18280
2022-10-12 12:06:13 | INFO | train_inner | epoch 038:    244 / 459 loss=7.624, nll_loss=4.173, mask_ins=1.269, word_ins_ml=5.648, word_reposition=0.707, ppl=197.22, wps=14013.4, ups=0.95, wpb=14820, bsz=449.8, num_updates=17200, lr=0.000269582, gnorm=2.911, clip=1, loss_scale=64, train_wall=105, wall=18386
2022-10-12 12:07:59 | INFO | train_inner | epoch 038:    344 / 459 loss=7.628, nll_loss=4.185, mask_ins=1.257, word_ins_ml=5.658, word_reposition=0.713, ppl=197.79, wps=14001.5, ups=0.95, wpb=14806.9, bsz=450.2, num_updates=17300, lr=0.000268802, gnorm=2.849, clip=0, loss_scale=64, train_wall=105, wall=18492
2022-10-12 12:09:43 | INFO | train_inner | epoch 038:    444 / 459 loss=7.619, nll_loss=4.172, mask_ins=1.267, word_ins_ml=5.647, word_reposition=0.705, ppl=196.52, wps=13852.7, ups=0.96, wpb=14421, bsz=438.3, num_updates=17400, lr=0.000268028, gnorm=3.055, clip=2, loss_scale=83, train_wall=103, wall=18596
2022-10-12 12:09:59 | INFO | train | epoch 038 | loss 7.625 | nll_loss 4.18 | mask_ins 1.263 | word_ins_ml 5.654 | word_reposition 0.708 | ppl 197.41 | wps 13647.5 | ups 0.93 | wpb 14637.3 | bsz 444 | num_updates 17415 | lr 0.000267913 | gnorm 3.017 | clip 0.9 | loss_scale 70 | train_wall 476 | wall 18611
2022-10-12 12:10:08 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 13.512 | nll_loss 9.836 | mask_ins 1.692 | word_ins_ml 10.782 | word_reposition 1.039 | ppl 11682 | wps 35043.7 | wpb 1628.7 | bsz 55.8 | num_updates 17415 | best_loss 13.037
2022-10-12 12:10:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 38 @ 17415 updates, score 13.512) (writing took 2.746913663999294 seconds)
2022-10-12 12:11:40 | INFO | train_inner | epoch 039:     85 / 459 loss=7.555, nll_loss=4.118, mask_ins=1.255, word_ins_ml=5.6, word_reposition=0.699, ppl=188.06, wps=12170.3, ups=0.86, wpb=14140.3, bsz=428.7, num_updates=17500, lr=0.000267261, gnorm=2.612, clip=0, loss_scale=128, train_wall=103, wall=18712
2022-10-12 12:13:24 | INFO | train_inner | epoch 039:    185 / 459 loss=7.539, nll_loss=4.106, mask_ins=1.257, word_ins_ml=5.589, word_reposition=0.692, ppl=186.01, wps=13989.8, ups=0.96, wpb=14588.1, bsz=445, num_updates=17600, lr=0.000266501, gnorm=3.059, clip=1, loss_scale=128, train_wall=103, wall=18816
2022-10-12 12:15:08 | INFO | train_inner | epoch 039:    285 / 459 loss=7.592, nll_loss=4.139, mask_ins=1.263, word_ins_ml=5.618, word_reposition=0.711, ppl=193, wps=14012.2, ups=0.96, wpb=14631.2, bsz=438.9, num_updates=17700, lr=0.000265747, gnorm=3.477, clip=2, loss_scale=128, train_wall=103, wall=18921
2022-10-12 12:16:53 | INFO | train_inner | epoch 039:    385 / 459 loss=7.586, nll_loss=4.155, mask_ins=1.254, word_ins_ml=5.632, word_reposition=0.7, ppl=192.18, wps=14532.9, ups=0.95, wpb=15226.1, bsz=464.4, num_updates=17800, lr=0.000264999, gnorm=2.212, clip=0, loss_scale=128, train_wall=104, wall=19025
2022-10-12 12:18:09 | INFO | train | epoch 039 | loss 7.571 | nll_loss 4.131 | mask_ins 1.257 | word_ins_ml 5.611 | word_reposition 0.703 | ppl 190.11 | wps 13697.6 | ups 0.94 | wpb 14641.5 | bsz 444.1 | num_updates 17874 | lr 0.00026445 | gnorm 2.728 | clip 0.7 | loss_scale 128 | train_wall 474 | wall 19102
2022-10-12 12:18:19 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 13.439 | nll_loss 9.838 | mask_ins 1.718 | word_ins_ml 10.78 | word_reposition 0.941 | ppl 11107.3 | wps 34842 | wpb 1628.7 | bsz 55.8 | num_updates 17874 | best_loss 13.037
2022-10-12 12:18:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 39 @ 17874 updates, score 13.439) (writing took 2.7495702739979606 seconds)
2022-10-12 12:18:48 | INFO | train_inner | epoch 040:     26 / 459 loss=7.581, nll_loss=4.135, mask_ins=1.257, word_ins_ml=5.614, word_reposition=0.709, ppl=191.41, wps=12564.8, ups=0.87, wpb=14494.9, bsz=438.4, num_updates=17900, lr=0.000264258, gnorm=2.033, clip=0, loss_scale=150, train_wall=102, wall=19141
2022-10-12 12:20:33 | INFO | train_inner | epoch 040:    126 / 459 loss=7.525, nll_loss=4.083, mask_ins=1.252, word_ins_ml=5.569, word_reposition=0.703, ppl=184.14, wps=14168.4, ups=0.96, wpb=14757.2, bsz=447.1, num_updates=18000, lr=0.000263523, gnorm=2.285, clip=0, loss_scale=256, train_wall=103, wall=19245
2022-10-12 12:22:17 | INFO | train_inner | epoch 040:    226 / 459 loss=7.549, nll_loss=4.105, mask_ins=1.261, word_ins_ml=5.588, word_reposition=0.7, ppl=187.26, wps=14100.4, ups=0.96, wpb=14688.4, bsz=446.2, num_updates=18100, lr=0.000262794, gnorm=1.956, clip=0, loss_scale=256, train_wall=103, wall=19349
2022-10-12 12:24:03 | INFO | train_inner | epoch 040:    326 / 459 loss=7.562, nll_loss=4.135, mask_ins=1.252, word_ins_ml=5.613, word_reposition=0.698, ppl=189.01, wps=13698.1, ups=0.94, wpb=14531.8, bsz=439.7, num_updates=18200, lr=0.000262071, gnorm=2.301, clip=1, loss_scale=256, train_wall=105, wall=19455
2022-10-12 12:25:48 | INFO | train_inner | epoch 040:    426 / 459 loss=7.575, nll_loss=4.139, mask_ins=1.257, word_ins_ml=5.617, word_reposition=0.701, ppl=190.63, wps=13859.7, ups=0.95, wpb=14647.5, bsz=445, num_updates=18300, lr=0.000261354, gnorm=3.594, clip=2, loss_scale=256, train_wall=105, wall=19561
2022-10-12 12:26:24 | INFO | train | epoch 040 | loss 7.555 | nll_loss 4.116 | mask_ins 1.257 | word_ins_ml 5.598 | word_reposition 0.7 | ppl 188.03 | wps 13595.7 | ups 0.93 | wpb 14640.5 | bsz 444 | num_updates 18333 | lr 0.000261119 | gnorm 2.602 | clip 0.7 | loss_scale 253 | train_wall 478 | wall 19596
2022-10-12 12:26:33 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 13.455 | nll_loss 9.934 | mask_ins 1.67 | word_ins_ml 10.868 | word_reposition 0.917 | ppl 11232.7 | wps 34934.2 | wpb 1628.7 | bsz 55.8 | num_updates 18333 | best_loss 13.037
2022-10-12 12:26:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 40 @ 18333 updates, score 13.455) (writing took 2.971439859989914 seconds)
2022-10-12 12:27:46 | INFO | train_inner | epoch 041:     67 / 459 loss=7.524, nll_loss=4.09, mask_ins=1.255, word_ins_ml=5.574, word_reposition=0.695, ppl=184.07, wps=12446.8, ups=0.85, wpb=14674.3, bsz=447.4, num_updates=18400, lr=0.000260643, gnorm=4.139, clip=1, loss_scale=269, train_wall=105, wall=19679
2022-10-12 12:28:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 12:29:33 | INFO | train_inner | epoch 041:    168 / 459 loss=7.503, nll_loss=4.08, mask_ins=1.249, word_ins_ml=5.566, word_reposition=0.688, ppl=181.39, wps=13611.1, ups=0.93, wpb=14575.4, bsz=440.2, num_updates=18500, lr=0.000259938, gnorm=4.637, clip=2, loss_scale=383, train_wall=106, wall=19786
2022-10-12 12:30:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 12:31:19 | INFO | train_inner | epoch 041:    269 / 459 loss=7.527, nll_loss=4.08, mask_ins=1.254, word_ins_ml=5.565, word_reposition=0.708, ppl=184.47, wps=13871.7, ups=0.95, wpb=14652, bsz=444.2, num_updates=18600, lr=0.000259238, gnorm=5.744, clip=3, loss_scale=218, train_wall=105, wall=19891
2022-10-12 12:33:04 | INFO | train_inner | epoch 041:    369 / 459 loss=7.534, nll_loss=4.091, mask_ins=1.25, word_ins_ml=5.576, word_reposition=0.708, ppl=185.32, wps=14185.7, ups=0.95, wpb=14868.8, bsz=450.2, num_updates=18700, lr=0.000258544, gnorm=2.71, clip=0, loss_scale=128, train_wall=104, wall=19996
2022-10-12 12:34:37 | INFO | train | epoch 041 | loss 7.514 | nll_loss 4.078 | mask_ins 1.25 | word_ins_ml 5.564 | word_reposition 0.699 | ppl 182.73 | wps 13553.4 | ups 0.93 | wpb 14641.5 | bsz 444.1 | num_updates 18790 | lr 0.000257924 | gnorm 4.167 | clip 1.5 | loss_scale 225 | train_wall 477 | wall 20090
2022-10-12 12:34:47 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 13.524 | nll_loss 9.921 | mask_ins 1.669 | word_ins_ml 10.858 | word_reposition 0.997 | ppl 11781.5 | wps 34996.7 | wpb 1628.7 | bsz 55.8 | num_updates 18790 | best_loss 13.037
2022-10-12 12:34:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 41 @ 18790 updates, score 13.524) (writing took 2.832338988999254 seconds)
2022-10-12 12:35:00 | INFO | train_inner | epoch 042:     10 / 459 loss=7.49, nll_loss=4.06, mask_ins=1.246, word_ins_ml=5.548, word_reposition=0.696, ppl=179.76, wps=12317.2, ups=0.86, wpb=14283.3, bsz=434, num_updates=18800, lr=0.000257855, gnorm=3.452, clip=1, loss_scale=128, train_wall=103, wall=20112
2022-10-12 12:36:44 | INFO | train_inner | epoch 042:    110 / 459 loss=7.492, nll_loss=4.059, mask_ins=1.25, word_ins_ml=5.548, word_reposition=0.693, ppl=179.99, wps=14131.9, ups=0.96, wpb=14657.1, bsz=443.8, num_updates=18900, lr=0.000257172, gnorm=3.079, clip=2, loss_scale=128, train_wall=103, wall=20216
2022-10-12 12:38:26 | INFO | train_inner | epoch 042:    210 / 459 loss=7.459, nll_loss=4.023, mask_ins=1.242, word_ins_ml=5.517, word_reposition=0.7, ppl=175.96, wps=14087.1, ups=0.98, wpb=14441.8, bsz=437.7, num_updates=19000, lr=0.000256495, gnorm=2.803, clip=1, loss_scale=128, train_wall=102, wall=20318
2022-10-12 12:40:09 | INFO | train_inner | epoch 042:    310 / 459 loss=7.483, nll_loss=4.053, mask_ins=1.249, word_ins_ml=5.541, word_reposition=0.693, ppl=178.85, wps=14208.4, ups=0.97, wpb=14602.4, bsz=443.1, num_updates=19100, lr=0.000255822, gnorm=2.46, clip=0, loss_scale=151, train_wall=102, wall=20421
2022-10-12 12:41:54 | INFO | train_inner | epoch 042:    410 / 459 loss=7.459, nll_loss=4.032, mask_ins=1.244, word_ins_ml=5.525, word_reposition=0.691, ppl=176, wps=14252, ups=0.95, wpb=14930.9, bsz=455.3, num_updates=19200, lr=0.000255155, gnorm=3.33, clip=0, loss_scale=256, train_wall=104, wall=20526
2022-10-12 12:42:45 | INFO | train | epoch 042 | loss 7.479 | nll_loss 4.045 | mask_ins 1.246 | word_ins_ml 5.536 | word_reposition 0.697 | ppl 178.39 | wps 13790.1 | ups 0.94 | wpb 14640.1 | bsz 444 | num_updates 19249 | lr 0.00025483 | gnorm 2.974 | clip 0.7 | loss_scale 175 | train_wall 471 | wall 20577
2022-10-12 12:42:54 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 13.519 | nll_loss 9.911 | mask_ins 1.698 | word_ins_ml 10.849 | word_reposition 0.972 | ppl 11741.1 | wps 34836.1 | wpb 1628.7 | bsz 55.8 | num_updates 19249 | best_loss 13.037
2022-10-12 12:42:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 42 @ 19249 updates, score 13.519) (writing took 2.7286326289904537 seconds)
2022-10-12 12:43:49 | INFO | train_inner | epoch 043:     51 / 459 loss=7.46, nll_loss=4.028, mask_ins=1.248, word_ins_ml=5.52, word_reposition=0.693, ppl=176.12, wps=12433, ups=0.86, wpb=14391.6, bsz=437, num_updates=19300, lr=0.000254493, gnorm=3.251, clip=0, loss_scale=256, train_wall=103, wall=20642
2022-10-12 12:45:34 | INFO | train_inner | epoch 043:    151 / 459 loss=7.413, nll_loss=3.981, mask_ins=1.242, word_ins_ml=5.479, word_reposition=0.692, ppl=170.4, wps=14151.4, ups=0.95, wpb=14851.9, bsz=452.1, num_updates=19400, lr=0.000253837, gnorm=2.749, clip=0, loss_scale=256, train_wall=104, wall=20747
2022-10-12 12:47:19 | INFO | train_inner | epoch 043:    251 / 459 loss=7.449, nll_loss=4.024, mask_ins=1.242, word_ins_ml=5.518, word_reposition=0.689, ppl=174.7, wps=13748.6, ups=0.95, wpb=14432.1, bsz=438, num_updates=19500, lr=0.000253185, gnorm=2.971, clip=0, loss_scale=256, train_wall=104, wall=20852
2022-10-12 12:49:05 | INFO | train_inner | epoch 043:    351 / 459 loss=7.48, nll_loss=4.049, mask_ins=1.24, word_ins_ml=5.539, word_reposition=0.701, ppl=178.53, wps=13938.2, ups=0.95, wpb=14677.2, bsz=441.4, num_updates=19600, lr=0.000252538, gnorm=2.782, clip=0, loss_scale=271, train_wall=104, wall=20957
2022-10-12 12:50:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 12:50:52 | INFO | train_inner | epoch 043:    452 / 459 loss=7.455, nll_loss=4.034, mask_ins=1.233, word_ins_ml=5.525, word_reposition=0.697, ppl=175.43, wps=13844.8, ups=0.93, wpb=14882.4, bsz=451.7, num_updates=19700, lr=0.000251896, gnorm=4.829, clip=4, loss_scale=499, train_wall=107, wall=21064
2022-10-12 12:50:59 | INFO | train | epoch 043 | loss 7.443 | nll_loss 4.017 | mask_ins 1.239 | word_ins_ml 5.511 | word_reposition 0.693 | ppl 173.98 | wps 13552.1 | ups 0.93 | wpb 14635.8 | bsz 443.9 | num_updates 19707 | lr 0.000251852 | gnorm 3.395 | clip 0.9 | loss_scale 313 | train_wall 478 | wall 21072
2022-10-12 12:51:09 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 13.538 | nll_loss 9.964 | mask_ins 1.708 | word_ins_ml 10.901 | word_reposition 0.93 | ppl 11895.7 | wps 34802 | wpb 1628.7 | bsz 55.8 | num_updates 19707 | best_loss 13.037
2022-10-12 12:51:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 43 @ 19707 updates, score 13.538) (writing took 2.9523417460004566 seconds)
2022-10-12 12:51:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 12:52:49 | INFO | train_inner | epoch 044:     94 / 459 loss=7.465, nll_loss=4.031, mask_ins=1.242, word_ins_ml=5.523, word_reposition=0.701, ppl=176.68, wps=12457, ups=0.85, wpb=14602.5, bsz=441.8, num_updates=19800, lr=0.000251259, gnorm=7.124, clip=3, loss_scale=174, train_wall=104, wall=21182
2022-10-12 12:54:35 | INFO | train_inner | epoch 044:    194 / 459 loss=7.427, nll_loss=4.014, mask_ins=1.228, word_ins_ml=5.509, word_reposition=0.689, ppl=172.03, wps=13958, ups=0.95, wpb=14698.3, bsz=446.2, num_updates=19900, lr=0.000250627, gnorm=5.525, clip=4, loss_scale=128, train_wall=104, wall=21287
2022-10-12 12:56:21 | INFO | train_inner | epoch 044:    294 / 459 loss=7.443, nll_loss=4.01, mask_ins=1.241, word_ins_ml=5.505, word_reposition=0.697, ppl=173.95, wps=14039.3, ups=0.94, wpb=14870, bsz=453, num_updates=20000, lr=0.00025, gnorm=3.585, clip=1, loss_scale=128, train_wall=105, wall=21393
2022-10-12 12:58:05 | INFO | train_inner | epoch 044:    394 / 459 loss=7.455, nll_loss=4.028, mask_ins=1.244, word_ins_ml=5.52, word_reposition=0.691, ppl=175.51, wps=13984.2, ups=0.96, wpb=14583.5, bsz=441.8, num_updates=20100, lr=0.000249377, gnorm=4.592, clip=1, loss_scale=128, train_wall=103, wall=21497
2022-10-12 12:59:13 | INFO | train | epoch 044 | loss 7.446 | nll_loss 4.02 | mask_ins 1.239 | word_ins_ml 5.513 | word_reposition 0.693 | ppl 174.33 | wps 13591.8 | ups 0.93 | wpb 14643.1 | bsz 444.1 | num_updates 20165 | lr 0.000248975 | gnorm 5.253 | clip 2.6 | loss_scale 136 | train_wall 477 | wall 21565
2022-10-12 12:59:22 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 13.697 | nll_loss 9.988 | mask_ins 1.685 | word_ins_ml 10.917 | word_reposition 1.096 | ppl 13280.8 | wps 34925.3 | wpb 1628.7 | bsz 55.8 | num_updates 20165 | best_loss 13.037
2022-10-12 12:59:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 44 @ 20165 updates, score 13.697) (writing took 2.847157748998143 seconds)
2022-10-12 13:00:02 | INFO | train_inner | epoch 045:     35 / 459 loss=7.426, nll_loss=4.001, mask_ins=1.242, word_ins_ml=5.497, word_reposition=0.686, ppl=171.94, wps=12245.3, ups=0.85, wpb=14341.4, bsz=434.5, num_updates=20200, lr=0.000248759, gnorm=6.493, clip=6, loss_scale=128, train_wall=104, wall=21614
2022-10-12 13:01:48 | INFO | train_inner | epoch 045:    135 / 459 loss=7.379, nll_loss=3.957, mask_ins=1.236, word_ins_ml=5.459, word_reposition=0.684, ppl=166.41, wps=13987.9, ups=0.94, wpb=14816.8, bsz=451.8, num_updates=20300, lr=0.000248146, gnorm=4.196, clip=3, loss_scale=196, train_wall=105, wall=21720
2022-10-12 13:03:34 | INFO | train_inner | epoch 045:    235 / 459 loss=7.408, nll_loss=3.977, mask_ins=1.244, word_ins_ml=5.477, word_reposition=0.687, ppl=169.81, wps=13877.8, ups=0.94, wpb=14701.8, bsz=444.6, num_updates=20400, lr=0.000247537, gnorm=5.33, clip=4, loss_scale=256, train_wall=105, wall=21826
2022-10-12 13:05:17 | INFO | train_inner | epoch 045:    335 / 459 loss=7.431, nll_loss=4.008, mask_ins=1.239, word_ins_ml=5.502, word_reposition=0.689, ppl=172.54, wps=14109.5, ups=0.97, wpb=14510.6, bsz=440.9, num_updates=20500, lr=0.000246932, gnorm=4.376, clip=3, loss_scale=256, train_wall=102, wall=21929
2022-10-12 13:07:00 | INFO | train_inner | epoch 045:    435 / 459 loss=7.476, nll_loss=4.041, mask_ins=1.248, word_ins_ml=5.532, word_reposition=0.696, ppl=177.98, wps=14167.3, ups=0.97, wpb=14674.8, bsz=442.1, num_updates=20600, lr=0.000246332, gnorm=4.493, clip=0, loss_scale=256, train_wall=103, wall=22033
2022-10-12 13:07:25 | INFO | train | epoch 045 | loss 7.423 | nll_loss 3.996 | mask_ins 1.242 | word_ins_ml 5.493 | word_reposition 0.688 | ppl 171.59 | wps 13652.5 | ups 0.93 | wpb 14637.4 | bsz 443.9 | num_updates 20624 | lr 0.000246189 | gnorm 4.917 | clip 2.8 | loss_scale 233 | train_wall 476 | wall 22057
2022-10-12 13:07:34 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 13.65 | nll_loss 9.998 | mask_ins 1.687 | word_ins_ml 10.93 | word_reposition 1.032 | ppl 12857.3 | wps 34945.3 | wpb 1628.7 | bsz 55.8 | num_updates 20624 | best_loss 13.037
2022-10-12 13:07:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 45 @ 20624 updates, score 13.65) (writing took 2.5372380510088988 seconds)
2022-10-12 13:08:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 13:08:57 | INFO | train_inner | epoch 046:     77 / 459 loss=7.417, nll_loss=3.997, mask_ins=1.239, word_ins_ml=5.493, word_reposition=0.685, ppl=170.85, wps=12549.4, ups=0.86, wpb=14669.2, bsz=444.6, num_updates=20700, lr=0.000245737, gnorm=7.625, clip=6, loss_scale=212, train_wall=104, wall=22149
2022-10-12 13:10:41 | INFO | train_inner | epoch 046:    177 / 459 loss=7.388, nll_loss=3.97, mask_ins=1.237, word_ins_ml=5.471, word_reposition=0.68, ppl=167.47, wps=13999.9, ups=0.96, wpb=14528.3, bsz=442.2, num_updates=20800, lr=0.000245145, gnorm=5.468, clip=4, loss_scale=128, train_wall=103, wall=22253
2022-10-12 13:10:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 13:12:25 | INFO | train_inner | epoch 046:    278 / 459 loss=7.419, nll_loss=4, mask_ins=1.236, word_ins_ml=5.497, word_reposition=0.686, ppl=171.11, wps=14092.6, ups=0.96, wpb=14685.3, bsz=447.8, num_updates=20900, lr=0.000244558, gnorm=9.873, clip=12, loss_scale=64, train_wall=103, wall=22357
2022-10-12 13:14:10 | INFO | train_inner | epoch 046:    378 / 459 loss=7.405, nll_loss=3.993, mask_ins=1.235, word_ins_ml=5.49, word_reposition=0.68, ppl=169.48, wps=14193.8, ups=0.96, wpb=14856.2, bsz=451.5, num_updates=21000, lr=0.000243975, gnorm=5.827, clip=3, loss_scale=64, train_wall=104, wall=22462
2022-10-12 13:15:33 | INFO | train | epoch 046 | loss 7.407 | nll_loss 3.989 | mask_ins 1.236 | word_ins_ml 5.487 | word_reposition 0.684 | ppl 169.67 | wps 13710.4 | ups 0.94 | wpb 14642.2 | bsz 444.2 | num_updates 21081 | lr 0.000243506 | gnorm 6.771 | clip 5.9 | loss_scale 100 | train_wall 472 | wall 22545
2022-10-12 13:15:42 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 13.741 | nll_loss 10.035 | mask_ins 1.695 | word_ins_ml 10.965 | word_reposition 1.081 | ppl 13692.7 | wps 34881.9 | wpb 1628.7 | bsz 55.8 | num_updates 21081 | best_loss 13.037
2022-10-12 13:15:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 46 @ 21081 updates, score 13.741) (writing took 2.7562305399915203 seconds)
2022-10-12 13:16:05 | INFO | train_inner | epoch 047:     19 / 459 loss=7.416, nll_loss=3.99, mask_ins=1.239, word_ins_ml=5.488, word_reposition=0.689, ppl=170.8, wps=12526.4, ups=0.87, wpb=14407.2, bsz=433.8, num_updates=21100, lr=0.000243396, gnorm=4.556, clip=2, loss_scale=64, train_wall=102, wall=22577
2022-10-12 13:17:49 | INFO | train_inner | epoch 047:    119 / 459 loss=7.324, nll_loss=3.914, mask_ins=1.216, word_ins_ml=5.422, word_reposition=0.685, ppl=160.21, wps=13994.8, ups=0.96, wpb=14514.8, bsz=437.4, num_updates=21200, lr=0.000242821, gnorm=4.101, clip=1, loss_scale=64, train_wall=103, wall=22681
2022-10-12 13:18:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 13:19:33 | INFO | train_inner | epoch 047:    220 / 459 loss=7.38, nll_loss=3.961, mask_ins=1.232, word_ins_ml=5.462, word_reposition=0.686, ppl=166.61, wps=14214.4, ups=0.96, wpb=14843, bsz=452.1, num_updates=21300, lr=0.000242251, gnorm=8.816, clip=7, loss_scale=49, train_wall=103, wall=22785
2022-10-12 13:21:16 | INFO | train_inner | epoch 047:    320 / 459 loss=7.385, nll_loss=3.973, mask_ins=1.232, word_ins_ml=5.473, word_reposition=0.68, ppl=167.12, wps=14227.9, ups=0.97, wpb=14628.7, bsz=443.4, num_updates=21400, lr=0.000241684, gnorm=5.864, clip=3, loss_scale=32, train_wall=102, wall=22888
2022-10-12 13:22:59 | INFO | train_inner | epoch 047:    420 / 459 loss=7.367, nll_loss=3.954, mask_ins=1.229, word_ins_ml=5.456, word_reposition=0.682, ppl=165.12, wps=14210.9, ups=0.97, wpb=14652.7, bsz=446, num_updates=21500, lr=0.000241121, gnorm=5.705, clip=3, loss_scale=32, train_wall=102, wall=22991
2022-10-12 13:23:39 | INFO | train | epoch 047 | loss 7.368 | nll_loss 3.951 | mask_ins 1.23 | word_ins_ml 5.454 | word_reposition 0.685 | ppl 165.2 | wps 13797 | ups 0.94 | wpb 14641.1 | bsz 444.1 | num_updates 21539 | lr 0.000240903 | gnorm 5.882 | clip 3.1 | loss_scale 44 | train_wall 470 | wall 23031
2022-10-12 13:23:48 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 13.716 | nll_loss 10.003 | mask_ins 1.728 | word_ins_ml 10.935 | word_reposition 1.053 | ppl 13458.7 | wps 34987.1 | wpb 1628.7 | bsz 55.8 | num_updates 21539 | best_loss 13.037
2022-10-12 13:23:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 47 @ 21539 updates, score 13.716) (writing took 2.9957519750023494 seconds)
2022-10-12 13:24:55 | INFO | train_inner | epoch 048:     61 / 459 loss=7.361, nll_loss=3.937, mask_ins=1.236, word_ins_ml=5.441, word_reposition=0.684, ppl=164.37, wps=12562, ups=0.86, wpb=14601.2, bsz=441.5, num_updates=21600, lr=0.000240563, gnorm=4.444, clip=1, loss_scale=32, train_wall=103, wall=23107
2022-10-12 13:26:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2022-10-12 13:26:40 | INFO | train_inner | epoch 048:    162 / 459 loss=7.346, nll_loss=3.916, mask_ins=1.237, word_ins_ml=5.423, word_reposition=0.686, ppl=162.68, wps=13947, ups=0.95, wpb=14680.8, bsz=446.6, num_updates=21700, lr=0.000240008, gnorm=4.794, clip=3, loss_scale=29, train_wall=104, wall=23213
2022-10-12 13:28:26 | INFO | train_inner | epoch 048:    262 / 459 loss=7.333, nll_loss=3.917, mask_ins=1.219, word_ins_ml=5.424, word_reposition=0.691, ppl=161.27, wps=14254.2, ups=0.95, wpb=15053.2, bsz=457, num_updates=21800, lr=0.000239457, gnorm=5.668, clip=2, loss_scale=16, train_wall=105, wall=23318
2022-10-12 13:30:11 | INFO | train_inner | epoch 048:    362 / 459 loss=7.347, nll_loss=3.938, mask_ins=1.228, word_ins_ml=5.441, word_reposition=0.678, ppl=162.81, wps=13700, ups=0.95, wpb=14354.3, bsz=435.5, num_updates=21900, lr=0.000238909, gnorm=2.862, clip=0, loss_scale=16, train_wall=104, wall=23423
2022-10-12 13:31:52 | INFO | train | epoch 048 | loss 7.348 | nll_loss 3.933 | mask_ins 1.227 | word_ins_ml 5.437 | word_reposition 0.684 | ppl 162.91 | wps 13608.8 | ups 0.93 | wpb 14639.6 | bsz 444 | num_updates 21997 | lr 0.000238382 | gnorm 4.536 | clip 1.5 | loss_scale 21 | train_wall 476 | wall 23524
2022-10-12 13:32:01 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 13.603 | nll_loss 9.99 | mask_ins 1.641 | word_ins_ml 10.924 | word_reposition 1.037 | ppl 12439 | wps 34915.3 | wpb 1628.7 | bsz 55.8 | num_updates 21997 | best_loss 13.037
2022-10-12 13:32:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 48 @ 21997 updates, score 13.603) (writing took 2.8323474940116284 seconds)
2022-10-12 13:32:07 | INFO | train_inner | epoch 049:      3 / 459 loss=7.364, nll_loss=3.962, mask_ins=1.219, word_ins_ml=5.462, word_reposition=0.683, ppl=164.76, wps=12375.4, ups=0.86, wpb=14350.2, bsz=433.3, num_updates=22000, lr=0.000238366, gnorm=4.731, clip=1, loss_scale=16, train_wall=103, wall=23539
2022-10-12 13:33:49 | INFO | train_inner | epoch 049:    103 / 459 loss=7.295, nll_loss=3.871, mask_ins=1.226, word_ins_ml=5.383, word_reposition=0.686, ppl=157.02, wps=14215, ups=0.97, wpb=14599.3, bsz=442.2, num_updates=22100, lr=0.000237826, gnorm=3.321, clip=0, loss_scale=16, train_wall=102, wall=23642
2022-10-12 13:35:33 | INFO | train_inner | epoch 049:    203 / 459 loss=7.312, nll_loss=3.891, mask_ins=1.226, word_ins_ml=5.401, word_reposition=0.685, ppl=158.85, wps=14181.9, ups=0.96, wpb=14716, bsz=444.1, num_updates=22200, lr=0.000237289, gnorm=2.463, clip=0, loss_scale=17, train_wall=103, wall=23745
2022-10-12 13:37:16 | INFO | train_inner | epoch 049:    303 / 459 loss=7.332, nll_loss=3.921, mask_ins=1.23, word_ins_ml=5.427, word_reposition=0.676, ppl=161.13, wps=14174.3, ups=0.97, wpb=14629.1, bsz=444.5, num_updates=22300, lr=0.000236757, gnorm=3.274, clip=2, loss_scale=32, train_wall=102, wall=23849
2022-10-12 13:39:00 | INFO | train_inner | epoch 049:    403 / 459 loss=7.291, nll_loss=3.88, mask_ins=1.222, word_ins_ml=5.391, word_reposition=0.679, ppl=156.62, wps=14411.3, ups=0.97, wpb=14863.1, bsz=454.1, num_updates=22400, lr=0.000236228, gnorm=2.935, clip=1, loss_scale=32, train_wall=102, wall=23952
2022-10-12 13:39:56 | INFO | train | epoch 049 | loss 7.311 | nll_loss 3.895 | mask_ins 1.227 | word_ins_ml 5.404 | word_reposition 0.68 | ppl 158.82 | wps 13871.9 | ups 0.95 | wpb 14641.9 | bsz 444.1 | num_updates 22456 | lr 0.000235933 | gnorm 2.94 | clip 0.7 | loss_scale 25 | train_wall 468 | wall 24008
2022-10-12 13:40:05 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 13.681 | nll_loss 10.031 | mask_ins 1.723 | word_ins_ml 10.958 | word_reposition 1 | ppl 13131.6 | wps 34955.8 | wpb 1628.7 | bsz 55.8 | num_updates 22456 | best_loss 13.037
2022-10-12 13:40:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 49 @ 22456 updates, score 13.681) (writing took 2.7785602880030638 seconds)
2022-10-12 13:40:54 | INFO | train_inner | epoch 050:     44 / 459 loss=7.347, nll_loss=3.928, mask_ins=1.235, word_ins_ml=5.433, word_reposition=0.679, ppl=162.79, wps=12686.2, ups=0.88, wpb=14463.2, bsz=436.6, num_updates=22500, lr=0.000235702, gnorm=2.576, clip=0, loss_scale=32, train_wall=101, wall=24066
2022-10-12 13:42:36 | INFO | train_inner | epoch 050:    144 / 459 loss=7.306, nll_loss=3.877, mask_ins=1.236, word_ins_ml=5.389, word_reposition=0.681, ppl=158.21, wps=14284.8, ups=0.97, wpb=14652.5, bsz=442.7, num_updates=22600, lr=0.00023518, gnorm=3.739, clip=2, loss_scale=32, train_wall=102, wall=24168
2022-10-12 13:44:19 | INFO | train_inner | epoch 050:    244 / 459 loss=7.32, nll_loss=3.903, mask_ins=1.231, word_ins_ml=5.412, word_reposition=0.677, ppl=159.83, wps=14243.2, ups=0.97, wpb=14677.9, bsz=445.4, num_updates=22700, lr=0.000234662, gnorm=2.179, clip=0, loss_scale=32, train_wall=102, wall=24271
2022-10-12 13:45:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 13:46:03 | INFO | train_inner | epoch 050:    345 / 459 loss=7.323, nll_loss=3.902, mask_ins=1.23, word_ins_ml=5.41, word_reposition=0.683, ppl=160.15, wps=14034.9, ups=0.96, wpb=14556.8, bsz=438.6, num_updates=22800, lr=0.000234146, gnorm=4.73, clip=2, loss_scale=57, train_wall=103, wall=24375
2022-10-12 13:47:47 | INFO | train_inner | epoch 050:    445 / 459 loss=7.308, nll_loss=3.909, mask_ins=1.22, word_ins_ml=5.415, word_reposition=0.673, ppl=158.44, wps=14151, ups=0.96, wpb=14664.2, bsz=450.2, num_updates=22900, lr=0.000233635, gnorm=4.699, clip=0, loss_scale=32, train_wall=103, wall=24479
2022-10-12 13:48:01 | INFO | train | epoch 050 | loss 7.316 | nll_loss 3.898 | mask_ins 1.23 | word_ins_ml 5.407 | word_reposition 0.679 | ppl 159.38 | wps 13824 | ups 0.94 | wpb 14636.9 | bsz 443.9 | num_updates 22914 | lr 0.000233563 | gnorm 3.757 | clip 0.9 | loss_scale 38 | train_wall 469 | wall 24493
2022-10-12 13:48:10 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 13.627 | nll_loss 9.99 | mask_ins 1.722 | word_ins_ml 10.921 | word_reposition 0.984 | ppl 12652.3 | wps 34967.2 | wpb 1628.7 | bsz 55.8 | num_updates 22914 | best_loss 13.037
2022-10-12 13:48:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 50 @ 22914 updates, score 13.627) (writing took 2.778481396002462 seconds)
2022-10-12 13:49:41 | INFO | train_inner | epoch 051:     86 / 459 loss=7.261, nll_loss=3.852, mask_ins=1.221, word_ins_ml=5.367, word_reposition=0.673, ppl=153.37, wps=12647.2, ups=0.87, wpb=14531.9, bsz=437.7, num_updates=23000, lr=0.000233126, gnorm=5.11, clip=1, loss_scale=32, train_wall=102, wall=24594
2022-10-12 13:51:25 | INFO | train_inner | epoch 051:    186 / 459 loss=7.264, nll_loss=3.855, mask_ins=1.212, word_ins_ml=5.369, word_reposition=0.682, ppl=153.66, wps=14249.3, ups=0.97, wpb=14721.7, bsz=446.2, num_updates=23100, lr=0.000232621, gnorm=4.072, clip=2, loss_scale=32, train_wall=102, wall=24697
2022-10-12 13:53:09 | INFO | train_inner | epoch 051:    286 / 459 loss=7.239, nll_loss=3.833, mask_ins=1.212, word_ins_ml=5.35, word_reposition=0.677, ppl=151.06, wps=14576.3, ups=0.96, wpb=15151.6, bsz=465.2, num_updates=23200, lr=0.000232119, gnorm=3.655, clip=0, loss_scale=32, train_wall=103, wall=24801
2022-10-12 13:54:51 | INFO | train_inner | epoch 051:    386 / 459 loss=7.293, nll_loss=3.87, mask_ins=1.224, word_ins_ml=5.383, word_reposition=0.686, ppl=156.82, wps=14214.5, ups=0.98, wpb=14486.3, bsz=437.4, num_updates=23300, lr=0.000231621, gnorm=3.506, clip=1, loss_scale=33, train_wall=101, wall=24903
2022-10-12 13:56:06 | INFO | train | epoch 051 | loss 7.263 | nll_loss 3.853 | mask_ins 1.216 | word_ins_ml 5.368 | word_reposition 0.679 | ppl 153.58 | wps 13871.9 | ups 0.95 | wpb 14644.4 | bsz 444.2 | num_updates 23373 | lr 0.000231259 | gnorm 4.007 | clip 0.9 | loss_scale 37 | train_wall 468 | wall 24978
2022-10-12 13:56:15 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 13.722 | nll_loss 10.091 | mask_ins 1.73 | word_ins_ml 11.016 | word_reposition 0.976 | ppl 13514.1 | wps 34958.8 | wpb 1628.7 | bsz 55.8 | num_updates 23373 | best_loss 13.037
2022-10-12 13:56:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 51 @ 23373 updates, score 13.722) (writing took 2.8345778209913988 seconds)
2022-10-12 13:56:46 | INFO | train_inner | epoch 052:     27 / 459 loss=7.255, nll_loss=3.847, mask_ins=1.219, word_ins_ml=5.362, word_reposition=0.674, ppl=152.72, wps=12426.5, ups=0.87, wpb=14345.1, bsz=434.1, num_updates=23400, lr=0.000231125, gnorm=3.453, clip=0, loss_scale=64, train_wall=102, wall=25018
2022-10-12 13:58:30 | INFO | train_inner | epoch 052:    127 / 459 loss=7.236, nll_loss=3.832, mask_ins=1.211, word_ins_ml=5.349, word_reposition=0.676, ppl=150.76, wps=13979.3, ups=0.96, wpb=14530.1, bsz=438.8, num_updates=23500, lr=0.000230633, gnorm=3.042, clip=0, loss_scale=64, train_wall=103, wall=25122
2022-10-12 14:00:14 | INFO | train_inner | epoch 052:    227 / 459 loss=7.233, nll_loss=3.831, mask_ins=1.214, word_ins_ml=5.348, word_reposition=0.671, ppl=150.48, wps=14310.6, ups=0.96, wpb=14923.6, bsz=455.5, num_updates=23600, lr=0.000230144, gnorm=2.428, clip=0, loss_scale=64, train_wall=103, wall=25227
2022-10-12 14:01:59 | INFO | train_inner | epoch 052:    327 / 459 loss=7.25, nll_loss=3.841, mask_ins=1.22, word_ins_ml=5.356, word_reposition=0.673, ppl=152.17, wps=14060, ups=0.96, wpb=14679, bsz=446.6, num_updates=23700, lr=0.000229658, gnorm=1.917, clip=0, loss_scale=64, train_wall=103, wall=25331
2022-10-12 14:03:43 | INFO | train_inner | epoch 052:    427 / 459 loss=7.243, nll_loss=3.856, mask_ins=1.205, word_ins_ml=5.369, word_reposition=0.668, ppl=151.44, wps=14069.9, ups=0.96, wpb=14661.3, bsz=443.8, num_updates=23800, lr=0.000229175, gnorm=2.027, clip=0, loss_scale=64, train_wall=103, wall=25435
2022-10-12 14:04:16 | INFO | train | epoch 052 | loss 7.243 | nll_loss 3.841 | mask_ins 1.214 | word_ins_ml 5.357 | word_reposition 0.672 | ppl 151.43 | wps 13715.3 | ups 0.94 | wpb 14642.1 | bsz 444.1 | num_updates 23832 | lr 0.000229021 | gnorm 2.357 | clip 0 | loss_scale 67 | train_wall 474 | wall 25468
2022-10-12 14:04:25 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 13.725 | nll_loss 10.098 | mask_ins 1.742 | word_ins_ml 11.022 | word_reposition 0.961 | ppl 13542 | wps 35107.3 | wpb 1628.7 | bsz 55.8 | num_updates 23832 | best_loss 13.037
2022-10-12 14:04:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 52 @ 23832 updates, score 13.725) (writing took 3.1959784729988314 seconds)
2022-10-12 14:05:39 | INFO | train_inner | epoch 053:     68 / 459 loss=7.22, nll_loss=3.826, mask_ins=1.209, word_ins_ml=5.344, word_reposition=0.668, ppl=149.12, wps=12417.5, ups=0.86, wpb=14398.3, bsz=435.4, num_updates=23900, lr=0.000228695, gnorm=2.229, clip=0, loss_scale=122, train_wall=103, wall=25551
2022-10-12 14:07:24 | INFO | train_inner | epoch 053:    168 / 459 loss=7.196, nll_loss=3.797, mask_ins=1.21, word_ins_ml=5.319, word_reposition=0.666, ppl=146.58, wps=14008.9, ups=0.95, wpb=14728.1, bsz=448.7, num_updates=24000, lr=0.000228218, gnorm=2.141, clip=0, loss_scale=128, train_wall=104, wall=25656
2022-10-12 14:09:10 | INFO | train_inner | epoch 053:    268 / 459 loss=7.215, nll_loss=3.81, mask_ins=1.211, word_ins_ml=5.33, word_reposition=0.674, ppl=148.61, wps=13811.2, ups=0.94, wpb=14637.9, bsz=443.8, num_updates=24100, lr=0.000227744, gnorm=2.03, clip=0, loss_scale=128, train_wall=105, wall=25762
2022-10-12 14:10:55 | INFO | train_inner | epoch 053:    368 / 459 loss=7.24, nll_loss=3.854, mask_ins=1.208, word_ins_ml=5.367, word_reposition=0.665, ppl=151.16, wps=13922.3, ups=0.95, wpb=14618.9, bsz=441.1, num_updates=24200, lr=0.000227273, gnorm=2.078, clip=0, loss_scale=128, train_wall=104, wall=25867
2022-10-12 14:12:30 | INFO | train | epoch 053 | loss 7.217 | nll_loss 3.822 | mask_ins 1.209 | word_ins_ml 5.34 | word_reposition 0.668 | ppl 148.82 | wps 13586.5 | ups 0.93 | wpb 14641.6 | bsz 444.1 | num_updates 24291 | lr 0.000226847 | gnorm 2.097 | clip 0 | loss_scale 128 | train_wall 478 | wall 25962
2022-10-12 14:12:40 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 13.761 | nll_loss 10.063 | mask_ins 1.733 | word_ins_ml 10.988 | word_reposition 1.04 | ppl 13879 | wps 34839.3 | wpb 1628.7 | bsz 55.8 | num_updates 24291 | best_loss 13.037
2022-10-12 14:12:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 53 @ 24291 updates, score 13.761) (writing took 2.8528285969950957 seconds)
2022-10-12 14:12:52 | INFO | train_inner | epoch 054:      9 / 459 loss=7.223, nll_loss=3.827, mask_ins=1.213, word_ins_ml=5.344, word_reposition=0.666, ppl=149.36, wps=12458.3, ups=0.86, wpb=14540.9, bsz=441.4, num_updates=24300, lr=0.000226805, gnorm=2.065, clip=0, loss_scale=128, train_wall=104, wall=25984
2022-10-12 14:14:37 | INFO | train_inner | epoch 054:    109 / 459 loss=7.144, nll_loss=3.764, mask_ins=1.197, word_ins_ml=5.29, word_reposition=0.657, ppl=141.4, wps=13812.1, ups=0.95, wpb=14502.6, bsz=440.5, num_updates=24400, lr=0.000226339, gnorm=2.5, clip=0, loss_scale=228, train_wall=104, wall=26089
2022-10-12 14:16:23 | INFO | train_inner | epoch 054:    209 / 459 loss=7.15, nll_loss=3.761, mask_ins=1.204, word_ins_ml=5.287, word_reposition=0.659, ppl=142.05, wps=13958, ups=0.94, wpb=14811.4, bsz=448.6, num_updates=24500, lr=0.000225877, gnorm=2.264, clip=0, loss_scale=256, train_wall=105, wall=26195
2022-10-12 14:17:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 14:18:09 | INFO | train_inner | epoch 054:    310 / 459 loss=7.182, nll_loss=3.783, mask_ins=1.212, word_ins_ml=5.306, word_reposition=0.664, ppl=145.18, wps=14036.2, ups=0.94, wpb=14886.8, bsz=452.8, num_updates=24600, lr=0.000225417, gnorm=3.059, clip=2, loss_scale=186, train_wall=105, wall=26301
2022-10-12 14:19:54 | INFO | train_inner | epoch 054:    410 / 459 loss=7.213, nll_loss=3.811, mask_ins=1.207, word_ins_ml=5.33, word_reposition=0.676, ppl=148.4, wps=13909.5, ups=0.95, wpb=14630.9, bsz=442.1, num_updates=24700, lr=0.000224961, gnorm=2.67, clip=1, loss_scale=128, train_wall=104, wall=26406
2022-10-12 14:20:45 | INFO | train | epoch 054 | loss 7.172 | nll_loss 3.78 | mask_ins 1.204 | word_ins_ml 5.303 | word_reposition 0.664 | ppl 144.18 | wps 13560.8 | ups 0.93 | wpb 14642.3 | bsz 444.1 | num_updates 24749 | lr 0.000224738 | gnorm 2.6 | clip 0.7 | loss_scale 190 | train_wall 478 | wall 26457
2022-10-12 14:20:54 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 13.735 | nll_loss 10.066 | mask_ins 1.733 | word_ins_ml 10.99 | word_reposition 1.013 | ppl 13638.8 | wps 35040.2 | wpb 1628.7 | bsz 55.8 | num_updates 24749 | best_loss 13.037
2022-10-12 14:20:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 54 @ 24749 updates, score 13.735) (writing took 3.044388137990609 seconds)
2022-10-12 14:21:51 | INFO | train_inner | epoch 055:     51 / 459 loss=7.15, nll_loss=3.772, mask_ins=1.187, word_ins_ml=5.296, word_reposition=0.668, ppl=142.06, wps=12190, ups=0.86, wpb=14224, bsz=432.8, num_updates=24800, lr=0.000224507, gnorm=2.364, clip=0, loss_scale=128, train_wall=103, wall=26523
2022-10-12 14:23:36 | INFO | train_inner | epoch 055:    151 / 459 loss=7.151, nll_loss=3.762, mask_ins=1.206, word_ins_ml=5.288, word_reposition=0.657, ppl=142.12, wps=14101.7, ups=0.95, wpb=14875, bsz=452.7, num_updates=24900, lr=0.000224055, gnorm=3.038, clip=0, loss_scale=128, train_wall=104, wall=26628
2022-10-12 14:25:21 | INFO | train_inner | epoch 055:    251 / 459 loss=7.166, nll_loss=3.762, mask_ins=1.201, word_ins_ml=5.287, word_reposition=0.678, ppl=143.64, wps=14029.2, ups=0.95, wpb=14723.8, bsz=445.4, num_updates=25000, lr=0.000223607, gnorm=4.049, clip=2, loss_scale=128, train_wall=104, wall=26733
2022-10-12 14:27:05 | INFO | train_inner | epoch 055:    351 / 459 loss=7.174, nll_loss=3.783, mask_ins=1.202, word_ins_ml=5.306, word_reposition=0.666, ppl=144.4, wps=13891.1, ups=0.96, wpb=14450.7, bsz=438.1, num_updates=25100, lr=0.000223161, gnorm=2.784, clip=0, loss_scale=183, train_wall=103, wall=26837
2022-10-12 14:28:52 | INFO | train_inner | epoch 055:    451 / 459 loss=7.224, nll_loss=3.813, mask_ins=1.22, word_ins_ml=5.331, word_reposition=0.672, ppl=149.46, wps=13867.3, ups=0.94, wpb=14796, bsz=448.5, num_updates=25200, lr=0.000222718, gnorm=3.823, clip=1, loss_scale=256, train_wall=106, wall=26944
2022-10-12 14:29:00 | INFO | train | epoch 055 | loss 7.174 | nll_loss 3.777 | mask_ins 1.205 | word_ins_ml 5.301 | word_reposition 0.668 | ppl 144.4 | wps 13556.4 | ups 0.93 | wpb 14630.2 | bsz 443.7 | num_updates 25208 | lr 0.000222682 | gnorm 3.371 | clip 0.9 | loss_scale 170 | train_wall 479 | wall 26952
2022-10-12 14:29:09 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 13.703 | nll_loss 10.018 | mask_ins 1.732 | word_ins_ml 10.941 | word_reposition 1.03 | ppl 13338.7 | wps 34928.7 | wpb 1628.7 | bsz 55.8 | num_updates 25208 | best_loss 13.037
2022-10-12 14:29:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 55 @ 25208 updates, score 13.703) (writing took 3.077041290991474 seconds)
2022-10-12 14:30:50 | INFO | train_inner | epoch 056:     92 / 459 loss=7.143, nll_loss=3.744, mask_ins=1.206, word_ins_ml=5.272, word_reposition=0.666, ppl=141.35, wps=12262.5, ups=0.85, wpb=14486.4, bsz=437.8, num_updates=25300, lr=0.000222277, gnorm=3.647, clip=2, loss_scale=256, train_wall=105, wall=27062
2022-10-12 14:32:37 | INFO | train_inner | epoch 056:    192 / 459 loss=7.128, nll_loss=3.728, mask_ins=1.21, word_ins_ml=5.258, word_reposition=0.66, ppl=139.85, wps=13785.6, ups=0.93, wpb=14775.9, bsz=452, num_updates=25400, lr=0.000221839, gnorm=2.513, clip=0, loss_scale=256, train_wall=106, wall=27169
2022-10-12 14:34:23 | INFO | train_inner | epoch 056:    292 / 459 loss=7.177, nll_loss=3.78, mask_ins=1.203, word_ins_ml=5.303, word_reposition=0.671, ppl=144.68, wps=14077.7, ups=0.95, wpb=14877.9, bsz=449.1, num_updates=25500, lr=0.000221404, gnorm=2.536, clip=0, loss_scale=256, train_wall=105, wall=27275
2022-10-12 14:36:09 | INFO | train_inner | epoch 056:    392 / 459 loss=7.178, nll_loss=3.78, mask_ins=1.212, word_ins_ml=5.304, word_reposition=0.662, ppl=144.83, wps=13852.6, ups=0.94, wpb=14685.5, bsz=445, num_updates=25600, lr=0.000220971, gnorm=3.575, clip=2, loss_scale=335, train_wall=105, wall=27381
2022-10-12 14:37:20 | INFO | train | epoch 056 | loss 7.156 | nll_loss 3.76 | mask_ins 1.205 | word_ins_ml 5.286 | word_reposition 0.665 | ppl 142.62 | wps 13449.1 | ups 0.92 | wpb 14640.2 | bsz 444 | num_updates 25667 | lr 0.000220682 | gnorm 2.969 | clip 0.7 | loss_scale 311 | train_wall 483 | wall 27452
2022-10-12 14:37:29 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 13.809 | nll_loss 10.109 | mask_ins 1.739 | word_ins_ml 11.03 | word_reposition 1.04 | ppl 14354.4 | wps 34926.3 | wpb 1628.7 | bsz 55.8 | num_updates 25667 | best_loss 13.037
2022-10-12 14:37:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 56 @ 25667 updates, score 13.809) (writing took 2.790773792003165 seconds)
2022-10-12 14:38:07 | INFO | train_inner | epoch 057:     33 / 459 loss=7.146, nll_loss=3.754, mask_ins=1.202, word_ins_ml=5.28, word_reposition=0.663, ppl=141.62, wps=11919, ups=0.85, wpb=14066.6, bsz=425, num_updates=25700, lr=0.000220541, gnorm=2.883, clip=1, loss_scale=512, train_wall=105, wall=27499
2022-10-12 14:39:53 | INFO | train_inner | epoch 057:    133 / 459 loss=7.113, nll_loss=3.717, mask_ins=1.198, word_ins_ml=5.248, word_reposition=0.667, ppl=138.45, wps=13992.6, ups=0.94, wpb=14888.4, bsz=454.6, num_updates=25800, lr=0.000220113, gnorm=2.663, clip=0, loss_scale=512, train_wall=105, wall=27606
2022-10-12 14:39:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 14:41:38 | INFO | train_inner | epoch 057:    234 / 459 loss=7.112, nll_loss=3.727, mask_ins=1.199, word_ins_ml=5.257, word_reposition=0.656, ppl=138.31, wps=13926.2, ups=0.95, wpb=14633.1, bsz=445.2, num_updates=25900, lr=0.000219687, gnorm=2.363, clip=0, loss_scale=259, train_wall=104, wall=27711
2022-10-12 14:43:24 | INFO | train_inner | epoch 057:    334 / 459 loss=7.16, nll_loss=3.778, mask_ins=1.198, word_ins_ml=5.301, word_reposition=0.661, ppl=142.99, wps=14022.1, ups=0.94, wpb=14839.2, bsz=448.6, num_updates=26000, lr=0.000219265, gnorm=3.575, clip=0, loss_scale=256, train_wall=105, wall=27816
2022-10-12 14:45:10 | INFO | train_inner | epoch 057:    434 / 459 loss=7.163, nll_loss=3.767, mask_ins=1.209, word_ins_ml=5.291, word_reposition=0.663, ppl=143.33, wps=13724.3, ups=0.95, wpb=14516.5, bsz=438.9, num_updates=26100, lr=0.000218844, gnorm=5.067, clip=5, loss_scale=256, train_wall=105, wall=27922
2022-10-12 14:45:36 | INFO | train | epoch 057 | loss 7.137 | nll_loss 3.745 | mask_ins 1.203 | word_ins_ml 5.272 | word_reposition 0.662 | ppl 140.79 | wps 13508.4 | ups 0.92 | wpb 14640.2 | bsz 444 | num_updates 26125 | lr 0.000218739 | gnorm 3.372 | clip 1.3 | loss_scale 331 | train_wall 480 | wall 27948
2022-10-12 14:45:45 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 13.864 | nll_loss 10.114 | mask_ins 1.758 | word_ins_ml 11.036 | word_reposition 1.07 | ppl 14909.9 | wps 34980.7 | wpb 1628.7 | bsz 55.8 | num_updates 26125 | best_loss 13.037
2022-10-12 14:45:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 57 @ 26125 updates, score 13.864) (writing took 3.0084945609996794 seconds)
2022-10-12 14:47:07 | INFO | train_inner | epoch 058:     75 / 459 loss=7.105, nll_loss=3.707, mask_ins=1.204, word_ins_ml=5.239, word_reposition=0.661, ppl=137.62, wps=12269.9, ups=0.85, wpb=14356.4, bsz=437.4, num_updates=26200, lr=0.000218426, gnorm=4.502, clip=3, loss_scale=256, train_wall=104, wall=28039
2022-10-12 14:48:53 | INFO | train_inner | epoch 058:    175 / 459 loss=7.11, nll_loss=3.716, mask_ins=1.203, word_ins_ml=5.248, word_reposition=0.66, ppl=138.16, wps=14101.5, ups=0.95, wpb=14907.5, bsz=450.7, num_updates=26300, lr=0.00021801, gnorm=3.966, clip=1, loss_scale=256, train_wall=105, wall=28145
2022-10-12 14:50:38 | INFO | train_inner | epoch 058:    275 / 459 loss=7.078, nll_loss=3.696, mask_ins=1.193, word_ins_ml=5.23, word_reposition=0.654, ppl=135.09, wps=14081.2, ups=0.95, wpb=14822.9, bsz=449.2, num_updates=26400, lr=0.000217597, gnorm=4.558, clip=4, loss_scale=481, train_wall=104, wall=28250
2022-10-12 14:51:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 14:52:22 | INFO | train_inner | epoch 058:    376 / 459 loss=7.159, nll_loss=3.771, mask_ins=1.196, word_ins_ml=5.296, word_reposition=0.668, ppl=142.95, wps=14026.1, ups=0.96, wpb=14609.6, bsz=443.3, num_updates=26500, lr=0.000217186, gnorm=3.462, clip=0, loss_scale=393, train_wall=103, wall=28354
2022-10-12 14:53:47 | INFO | train | epoch 058 | loss 7.112 | nll_loss 3.722 | mask_ins 1.199 | word_ins_ml 5.252 | word_reposition 0.66 | ppl 138.36 | wps 13655.6 | ups 0.93 | wpb 14639.4 | bsz 444 | num_updates 26583 | lr 0.000216847 | gnorm 4.137 | clip 1.7 | loss_scale 335 | train_wall 475 | wall 28439
2022-10-12 14:53:56 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 13.808 | nll_loss 10.136 | mask_ins 1.733 | word_ins_ml 11.054 | word_reposition 1.022 | ppl 14344.2 | wps 35033.1 | wpb 1628.7 | bsz 55.8 | num_updates 26583 | best_loss 13.037
2022-10-12 14:54:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 58 @ 26583 updates, score 13.808) (writing took 3.431931209997856 seconds)
2022-10-12 14:54:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 14:54:19 | INFO | train_inner | epoch 059:     18 / 459 loss=7.127, nll_loss=3.728, mask_ins=1.205, word_ins_ml=5.257, word_reposition=0.665, ppl=139.81, wps=12402.1, ups=0.86, wpb=14477.1, bsz=439.6, num_updates=26600, lr=0.000216777, gnorm=4.546, clip=2, loss_scale=253, train_wall=103, wall=28471
2022-10-12 14:56:02 | INFO | train_inner | epoch 059:    118 / 459 loss=7.098, nll_loss=3.703, mask_ins=1.203, word_ins_ml=5.237, word_reposition=0.658, ppl=136.96, wps=14136.3, ups=0.97, wpb=14531, bsz=439, num_updates=26700, lr=0.000216371, gnorm=4.116, clip=1, loss_scale=128, train_wall=102, wall=28574
2022-10-12 14:56:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 14:57:46 | INFO | train_inner | epoch 059:    219 / 459 loss=7.094, nll_loss=3.696, mask_ins=1.195, word_ins_ml=5.23, word_reposition=0.669, ppl=136.64, wps=13931.5, ups=0.96, wpb=14494.9, bsz=439.7, num_updates=26800, lr=0.000215967, gnorm=3.839, clip=2, loss_scale=77, train_wall=103, wall=28678
2022-10-12 14:59:31 | INFO | train_inner | epoch 059:    319 / 459 loss=7.159, nll_loss=3.74, mask_ins=1.215, word_ins_ml=5.269, word_reposition=0.675, ppl=142.88, wps=13808.8, ups=0.95, wpb=14549.2, bsz=439.8, num_updates=26900, lr=0.000215565, gnorm=4.403, clip=3, loss_scale=64, train_wall=104, wall=28783
2022-10-12 15:01:15 | INFO | train_inner | epoch 059:    419 / 459 loss=7.125, nll_loss=3.748, mask_ins=1.189, word_ins_ml=5.274, word_reposition=0.662, ppl=139.6, wps=14423.1, ups=0.96, wpb=14981.1, bsz=455.3, num_updates=27000, lr=0.000215166, gnorm=4.517, clip=0, loss_scale=64, train_wall=103, wall=28887
2022-10-12 15:01:56 | INFO | train | epoch 059 | loss 7.115 | nll_loss 3.72 | mask_ins 1.199 | word_ins_ml 5.25 | word_reposition 0.666 | ppl 138.62 | wps 13692.1 | ups 0.94 | wpb 14636.1 | bsz 444 | num_updates 27040 | lr 0.000215007 | gnorm 4.605 | clip 2 | loss_scale 88 | train_wall 472 | wall 28928
2022-10-12 15:02:05 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 13.861 | nll_loss 10.115 | mask_ins 1.756 | word_ins_ml 11.037 | word_reposition 1.069 | ppl 14879.1 | wps 35061.2 | wpb 1628.7 | bsz 55.8 | num_updates 27040 | best_loss 13.037
2022-10-12 15:02:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 59 @ 27040 updates, score 13.861) (writing took 2.903136976005044 seconds)
2022-10-12 15:03:09 | INFO | train_inner | epoch 060:     60 / 459 loss=7.063, nll_loss=3.682, mask_ins=1.188, word_ins_ml=5.217, word_reposition=0.657, ppl=133.67, wps=12567.7, ups=0.88, wpb=14361.2, bsz=436.5, num_updates=27100, lr=0.000214768, gnorm=7.574, clip=6, loss_scale=64, train_wall=101, wall=29001
2022-10-12 15:04:53 | INFO | train_inner | epoch 060:    160 / 459 loss=7.051, nll_loss=3.672, mask_ins=1.194, word_ins_ml=5.208, word_reposition=0.649, ppl=132.58, wps=14278.3, ups=0.96, wpb=14859, bsz=451.4, num_updates=27200, lr=0.000214373, gnorm=4.15, clip=2, loss_scale=64, train_wall=103, wall=29106
2022-10-12 15:06:38 | INFO | train_inner | epoch 060:    260 / 459 loss=7.054, nll_loss=3.677, mask_ins=1.19, word_ins_ml=5.214, word_reposition=0.649, ppl=132.85, wps=13913, ups=0.96, wpb=14543.8, bsz=443.4, num_updates=27300, lr=0.00021398, gnorm=3.182, clip=0, loss_scale=108, train_wall=104, wall=29210
2022-10-12 15:08:22 | INFO | train_inner | epoch 060:    360 / 459 loss=7.095, nll_loss=3.704, mask_ins=1.201, word_ins_ml=5.237, word_reposition=0.658, ppl=136.76, wps=14167.6, ups=0.96, wpb=14777.5, bsz=448.8, num_updates=27400, lr=0.000213589, gnorm=2.031, clip=0, loss_scale=128, train_wall=103, wall=29314
2022-10-12 15:10:06 | INFO | train | epoch 060 | loss 7.076 | nll_loss 3.69 | mask_ins 1.195 | word_ins_ml 5.224 | word_reposition 0.657 | ppl 134.9 | wps 13715.2 | ups 0.94 | wpb 14638.8 | bsz 444 | num_updates 27499 | lr 0.000213205 | gnorm 3.61 | clip 1.5 | loss_scale 101 | train_wall 474 | wall 29418
2022-10-12 15:10:15 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 13.848 | nll_loss 10.179 | mask_ins 1.736 | word_ins_ml 11.098 | word_reposition 1.014 | ppl 14741.6 | wps 34863 | wpb 1628.7 | bsz 55.8 | num_updates 27499 | best_loss 13.037
2022-10-12 15:10:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 60 @ 27499 updates, score 13.848) (writing took 2.9303477789944736 seconds)
2022-10-12 15:10:19 | INFO | train_inner | epoch 061:      1 / 459 loss=7.115, nll_loss=3.718, mask_ins=1.194, word_ins_ml=5.248, word_reposition=0.673, ppl=138.64, wps=12391.5, ups=0.86, wpb=14450.8, bsz=434.5, num_updates=27500, lr=0.000213201, gnorm=2.559, clip=0, loss_scale=128, train_wall=103, wall=29431
2022-10-12 15:11:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 15:12:02 | INFO | train_inner | epoch 061:    102 / 459 loss=7.065, nll_loss=3.668, mask_ins=1.196, word_ins_ml=5.205, word_reposition=0.664, ppl=133.93, wps=13813.6, ups=0.97, wpb=14290.7, bsz=431.3, num_updates=27600, lr=0.000212814, gnorm=3.573, clip=1, loss_scale=102, train_wall=103, wall=29534
2022-10-12 15:13:45 | INFO | train_inner | epoch 061:    202 / 459 loss=7.107, nll_loss=3.706, mask_ins=1.197, word_ins_ml=5.239, word_reposition=0.671, ppl=137.86, wps=14099.6, ups=0.98, wpb=14444.3, bsz=435.9, num_updates=27700, lr=0.00021243, gnorm=4.023, clip=2, loss_scale=64, train_wall=102, wall=29637
2022-10-12 15:15:30 | INFO | train_inner | epoch 061:    302 / 459 loss=7.054, nll_loss=3.676, mask_ins=1.193, word_ins_ml=5.211, word_reposition=0.65, ppl=132.91, wps=14394.2, ups=0.95, wpb=15103.2, bsz=462.3, num_updates=27800, lr=0.000212047, gnorm=4.295, clip=3, loss_scale=64, train_wall=104, wall=29742
2022-10-12 15:17:12 | INFO | train_inner | epoch 061:    402 / 459 loss=7.037, nll_loss=3.657, mask_ins=1.193, word_ins_ml=5.195, word_reposition=0.649, ppl=131.36, wps=14431, ups=0.97, wpb=14816.9, bsz=450.3, num_updates=27900, lr=0.000211667, gnorm=2.027, clip=0, loss_scale=64, train_wall=102, wall=29845
2022-10-12 15:18:11 | INFO | train | epoch 061 | loss 7.067 | nll_loss 3.677 | mask_ins 1.195 | word_ins_ml 5.213 | word_reposition 0.659 | ppl 134.06 | wps 13820.1 | ups 0.94 | wpb 14636.2 | bsz 443.8 | num_updates 27957 | lr 0.000211451 | gnorm 3.311 | clip 1.3 | loss_scale 73 | train_wall 469 | wall 29903
2022-10-12 15:18:20 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 13.827 | nll_loss 10.149 | mask_ins 1.735 | word_ins_ml 11.067 | word_reposition 1.025 | ppl 14536.7 | wps 34934.9 | wpb 1628.7 | bsz 55.8 | num_updates 27957 | best_loss 13.037
2022-10-12 15:18:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 61 @ 27957 updates, score 13.827) (writing took 2.7425028379948344 seconds)
2022-10-12 15:19:07 | INFO | train_inner | epoch 062:     43 / 459 loss=7.071, nll_loss=3.674, mask_ins=1.203, word_ins_ml=5.211, word_reposition=0.657, ppl=134.44, wps=12741.7, ups=0.87, wpb=14585.8, bsz=442.3, num_updates=28000, lr=0.000211289, gnorm=2.003, clip=0, loss_scale=64, train_wall=102, wall=29959
2022-10-12 15:20:50 | INFO | train_inner | epoch 062:    143 / 459 loss=7.035, nll_loss=3.655, mask_ins=1.185, word_ins_ml=5.194, word_reposition=0.656, ppl=131.13, wps=14462.6, ups=0.96, wpb=14990.2, bsz=454.1, num_updates=28100, lr=0.000210912, gnorm=2.331, clip=0, loss_scale=83, train_wall=103, wall=30063
2022-10-12 15:22:33 | INFO | train_inner | epoch 062:    243 / 459 loss=7.023, nll_loss=3.644, mask_ins=1.187, word_ins_ml=5.184, word_reposition=0.652, ppl=130.02, wps=14104.4, ups=0.97, wpb=14546, bsz=442.2, num_updates=28200, lr=0.000210538, gnorm=3.969, clip=1, loss_scale=128, train_wall=102, wall=30166
2022-10-12 15:24:16 | INFO | train_inner | epoch 062:    343 / 459 loss=7.058, nll_loss=3.692, mask_ins=1.188, word_ins_ml=5.225, word_reposition=0.645, ppl=133.28, wps=14121.9, ups=0.98, wpb=14481.6, bsz=440.2, num_updates=28300, lr=0.000210166, gnorm=6.806, clip=3, loss_scale=128, train_wall=102, wall=30268
2022-10-12 15:26:00 | INFO | train_inner | epoch 062:    443 / 459 loss=7.049, nll_loss=3.67, mask_ins=1.186, word_ins_ml=5.207, word_reposition=0.657, ppl=132.47, wps=14209.4, ups=0.97, wpb=14705.2, bsz=443.9, num_updates=28400, lr=0.000209795, gnorm=3.433, clip=1, loss_scale=128, train_wall=103, wall=30372
2022-10-12 15:26:16 | INFO | train | epoch 062 | loss 7.046 | nll_loss 3.666 | mask_ins 1.19 | word_ins_ml 5.203 | word_reposition 0.653 | ppl 132.13 | wps 13845.1 | ups 0.95 | wpb 14634.8 | bsz 443.8 | num_updates 28416 | lr 0.000209736 | gnorm 3.914 | clip 1.1 | loss_scale 112 | train_wall 469 | wall 30388
2022-10-12 15:26:25 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 13.704 | nll_loss 10.047 | mask_ins 1.73 | word_ins_ml 10.972 | word_reposition 1.002 | ppl 13346.3 | wps 34946.1 | wpb 1628.7 | bsz 55.8 | num_updates 28416 | best_loss 13.037
2022-10-12 15:26:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 62 @ 28416 updates, score 13.704) (writing took 3.090984637994552 seconds)
2022-10-12 15:27:54 | INFO | train_inner | epoch 063:     84 / 459 loss=7.014, nll_loss=3.628, mask_ins=1.187, word_ins_ml=5.171, word_reposition=0.656, ppl=129.22, wps=12479.8, ups=0.87, wpb=14315.2, bsz=434.9, num_updates=28500, lr=0.000209427, gnorm=3.153, clip=1, loss_scale=128, train_wall=101, wall=30487
2022-10-12 15:29:37 | INFO | train_inner | epoch 063:    184 / 459 loss=7.009, nll_loss=3.645, mask_ins=1.181, word_ins_ml=5.184, word_reposition=0.643, ppl=128.8, wps=14322.8, ups=0.97, wpb=14756.3, bsz=449.5, num_updates=28600, lr=0.000209061, gnorm=3.739, clip=3, loss_scale=150, train_wall=102, wall=30590
2022-10-12 15:31:21 | INFO | train_inner | epoch 063:    284 / 459 loss=7.038, nll_loss=3.657, mask_ins=1.189, word_ins_ml=5.196, word_reposition=0.653, ppl=131.46, wps=14411, ups=0.97, wpb=14886.7, bsz=450.1, num_updates=28700, lr=0.000208696, gnorm=2.697, clip=0, loss_scale=256, train_wall=102, wall=30693
2022-10-12 15:33:02 | INFO | train_inner | epoch 063:    384 / 459 loss=7.028, nll_loss=3.656, mask_ins=1.183, word_ins_ml=5.194, word_reposition=0.652, ppl=130.55, wps=14257, ups=0.98, wpb=14529.5, bsz=436.8, num_updates=28800, lr=0.000208333, gnorm=2.927, clip=0, loss_scale=256, train_wall=101, wall=30795
2022-10-12 15:34:19 | INFO | train | epoch 063 | loss 7.026 | nll_loss 3.649 | mask_ins 1.187 | word_ins_ml 5.189 | word_reposition 0.651 | ppl 130.36 | wps 13898.7 | ups 0.95 | wpb 14640.5 | bsz 444 | num_updates 28875 | lr 0.000208063 | gnorm 3.171 | clip 1.1 | loss_scale 209 | train_wall 467 | wall 30872
2022-10-12 15:34:29 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 13.912 | nll_loss 10.263 | mask_ins 1.74 | word_ins_ml 11.17 | word_reposition 1.002 | ppl 15418.1 | wps 34867.9 | wpb 1628.7 | bsz 55.8 | num_updates 28875 | best_loss 13.037
2022-10-12 15:34:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 63 @ 28875 updates, score 13.912) (writing took 3.4324948500143364 seconds)
2022-10-12 15:34:58 | INFO | train_inner | epoch 064:     25 / 459 loss=7.03, nll_loss=3.645, mask_ins=1.193, word_ins_ml=5.184, word_reposition=0.653, ppl=130.68, wps=12492.7, ups=0.87, wpb=14400.8, bsz=440, num_updates=28900, lr=0.000207973, gnorm=3.428, clip=1, loss_scale=256, train_wall=102, wall=30910
2022-10-12 15:36:40 | INFO | train_inner | epoch 064:    125 / 459 loss=6.996, nll_loss=3.615, mask_ins=1.188, word_ins_ml=5.158, word_reposition=0.65, ppl=127.68, wps=14184.8, ups=0.98, wpb=14509.2, bsz=438.9, num_updates=29000, lr=0.000207614, gnorm=3.831, clip=0, loss_scale=256, train_wall=101, wall=31012
2022-10-12 15:38:24 | INFO | train_inner | epoch 064:    225 / 459 loss=7.015, nll_loss=3.635, mask_ins=1.183, word_ins_ml=5.176, word_reposition=0.656, ppl=129.32, wps=14490.9, ups=0.96, wpb=15047.3, bsz=456.6, num_updates=29100, lr=0.000207257, gnorm=4.549, clip=2, loss_scale=269, train_wall=103, wall=31116
2022-10-12 15:39:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 15:40:07 | INFO | train_inner | epoch 064:    326 / 459 loss=7.006, nll_loss=3.639, mask_ins=1.179, word_ins_ml=5.179, word_reposition=0.648, ppl=128.54, wps=14081.9, ups=0.97, wpb=14521.8, bsz=438.3, num_updates=29200, lr=0.000206901, gnorm=3.753, clip=0, loss_scale=479, train_wall=102, wall=31219
2022-10-12 15:41:50 | INFO | train_inner | epoch 064:    426 / 459 loss=7.039, nll_loss=3.652, mask_ins=1.191, word_ins_ml=5.191, word_reposition=0.656, ppl=131.47, wps=14287.7, ups=0.97, wpb=14756, bsz=449.9, num_updates=29300, lr=0.000206548, gnorm=3.601, clip=1, loss_scale=256, train_wall=102, wall=31323
2022-10-12 15:42:24 | INFO | train | epoch 064 | loss 7.012 | nll_loss 3.634 | mask_ins 1.185 | word_ins_ml 5.175 | word_reposition 0.651 | ppl 129.03 | wps 13840.2 | ups 0.95 | wpb 14634.3 | bsz 443.8 | num_updates 29333 | lr 0.000206432 | gnorm 3.864 | clip 0.7 | loss_scale 308 | train_wall 467 | wall 31356
2022-10-12 15:42:33 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 13.907 | nll_loss 10.188 | mask_ins 1.765 | word_ins_ml 11.099 | word_reposition 1.043 | ppl 15356.9 | wps 34954.3 | wpb 1628.7 | bsz 55.8 | num_updates 29333 | best_loss 13.037
2022-10-12 15:42:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 64 @ 29333 updates, score 13.907) (writing took 3.302732168987859 seconds)
2022-10-12 15:43:45 | INFO | train_inner | epoch 065:     67 / 459 loss=7.006, nll_loss=3.638, mask_ins=1.183, word_ins_ml=5.178, word_reposition=0.645, ppl=128.55, wps=12816.7, ups=0.88, wpb=14641.2, bsz=443.1, num_updates=29400, lr=0.000206197, gnorm=3.619, clip=0, loss_scale=256, train_wall=101, wall=31437
2022-10-12 15:45:27 | INFO | train_inner | epoch 065:    167 / 459 loss=6.97, nll_loss=3.583, mask_ins=1.191, word_ins_ml=5.13, word_reposition=0.648, ppl=125.36, wps=14286.2, ups=0.97, wpb=14671.1, bsz=445.1, num_updates=29500, lr=0.000205847, gnorm=3.596, clip=1, loss_scale=256, train_wall=102, wall=31540
2022-10-12 15:47:09 | INFO | train_inner | epoch 065:    267 / 459 loss=7.014, nll_loss=3.635, mask_ins=1.189, word_ins_ml=5.176, word_reposition=0.649, ppl=129.26, wps=14098.4, ups=0.98, wpb=14337.5, bsz=432.2, num_updates=29600, lr=0.000205499, gnorm=5.698, clip=4, loss_scale=256, train_wall=101, wall=31641
2022-10-12 15:48:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 15:48:53 | INFO | train_inner | epoch 065:    368 / 459 loss=7.019, nll_loss=3.638, mask_ins=1.187, word_ins_ml=5.179, word_reposition=0.653, ppl=129.72, wps=14318.3, ups=0.96, wpb=14888, bsz=453.9, num_updates=29700, lr=0.000205152, gnorm=5.606, clip=3, loss_scale=191, train_wall=103, wall=31745
2022-10-12 15:50:26 | INFO | train | epoch 065 | loss 7.004 | nll_loss 3.626 | mask_ins 1.186 | word_ins_ml 5.168 | word_reposition 0.65 | ppl 128.37 | wps 13906.3 | ups 0.95 | wpb 14647.5 | bsz 444.3 | num_updates 29791 | lr 0.000204839 | gnorm 4.642 | clip 2 | loss_scale 216 | train_wall 466 | wall 31838
2022-10-12 15:50:35 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 13.95 | nll_loss 10.211 | mask_ins 1.713 | word_ins_ml 11.124 | word_reposition 1.113 | ppl 15830.8 | wps 34977.9 | wpb 1628.7 | bsz 55.8 | num_updates 29791 | best_loss 13.037
2022-10-12 15:50:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 65 @ 29791 updates, score 13.95) (writing took 2.9225598159828223 seconds)
2022-10-12 15:50:47 | INFO | train_inner | epoch 066:      9 / 459 loss=7.018, nll_loss=3.648, mask_ins=1.179, word_ins_ml=5.187, word_reposition=0.652, ppl=129.62, wps=12693.6, ups=0.87, wpb=14514.9, bsz=440.2, num_updates=29800, lr=0.000204808, gnorm=4.139, clip=1, loss_scale=128, train_wall=101, wall=31860
2022-10-12 15:52:29 | INFO | train_inner | epoch 066:    109 / 459 loss=6.956, nll_loss=3.572, mask_ins=1.184, word_ins_ml=5.122, word_reposition=0.65, ppl=124.12, wps=14155.1, ups=0.98, wpb=14423.5, bsz=437.7, num_updates=29900, lr=0.000204465, gnorm=4.043, clip=2, loss_scale=128, train_wall=101, wall=31961
2022-10-12 15:54:12 | INFO | train_inner | epoch 066:    209 / 459 loss=6.96, nll_loss=3.588, mask_ins=1.181, word_ins_ml=5.136, word_reposition=0.644, ppl=124.53, wps=14312.1, ups=0.97, wpb=14733, bsz=450.5, num_updates=30000, lr=0.000204124, gnorm=3.471, clip=0, loss_scale=128, train_wall=102, wall=32064
2022-10-12 15:55:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 15:55:56 | INFO | train_inner | epoch 066:    310 / 459 loss=6.978, nll_loss=3.606, mask_ins=1.175, word_ins_ml=5.151, word_reposition=0.652, ppl=126.03, wps=14098.1, ups=0.96, wpb=14709.9, bsz=444.6, num_updates=30100, lr=0.000203785, gnorm=6.193, clip=4, loss_scale=114, train_wall=103, wall=32169
2022-10-12 15:57:40 | INFO | train_inner | epoch 066:    410 / 459 loss=6.992, nll_loss=3.605, mask_ins=1.189, word_ins_ml=5.149, word_reposition=0.654, ppl=127.27, wps=14325.2, ups=0.96, wpb=14874.6, bsz=449.7, num_updates=30200, lr=0.000203447, gnorm=3.606, clip=0, loss_scale=64, train_wall=103, wall=32273
2022-10-12 15:58:30 | INFO | train | epoch 066 | loss 6.972 | nll_loss 3.595 | mask_ins 1.182 | word_ins_ml 5.141 | word_reposition 0.649 | ppl 125.57 | wps 13860 | ups 0.95 | wpb 14639.4 | bsz 444 | num_updates 30249 | lr 0.000203282 | gnorm 4.429 | clip 1.5 | loss_scale 104 | train_wall 467 | wall 32322
2022-10-12 15:58:39 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 13.912 | nll_loss 10.215 | mask_ins 1.731 | word_ins_ml 11.126 | word_reposition 1.055 | ppl 15415 | wps 35012.3 | wpb 1628.7 | bsz 55.8 | num_updates 30249 | best_loss 13.037
2022-10-12 15:58:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 66 @ 30249 updates, score 13.912) (writing took 2.833756053005345 seconds)
2022-10-12 15:59:34 | INFO | train_inner | epoch 067:     51 / 459 loss=6.956, nll_loss=3.583, mask_ins=1.182, word_ins_ml=5.131, word_reposition=0.643, ppl=124.13, wps=12720.1, ups=0.88, wpb=14467.2, bsz=440.1, num_updates=30300, lr=0.000203111, gnorm=4.201, clip=1, loss_scale=64, train_wall=101, wall=32386
2022-10-12 16:01:17 | INFO | train_inner | epoch 067:    151 / 459 loss=6.952, nll_loss=3.58, mask_ins=1.181, word_ins_ml=5.128, word_reposition=0.643, ppl=123.81, wps=14516.6, ups=0.97, wpb=15008.3, bsz=455.8, num_updates=30400, lr=0.000202777, gnorm=3.054, clip=0, loss_scale=64, train_wall=102, wall=32490
2022-10-12 16:02:59 | INFO | train_inner | epoch 067:    251 / 459 loss=6.986, nll_loss=3.596, mask_ins=1.188, word_ins_ml=5.142, word_reposition=0.656, ppl=126.76, wps=14159.5, ups=0.98, wpb=14458.3, bsz=434.8, num_updates=30500, lr=0.000202444, gnorm=2.481, clip=0, loss_scale=64, train_wall=101, wall=32592
2022-10-12 16:04:43 | INFO | train_inner | epoch 067:    351 / 459 loss=6.98, nll_loss=3.592, mask_ins=1.184, word_ins_ml=5.138, word_reposition=0.658, ppl=126.24, wps=14418.4, ups=0.97, wpb=14916.5, bsz=452.8, num_updates=30600, lr=0.000202113, gnorm=3.242, clip=1, loss_scale=70, train_wall=103, wall=32695
2022-10-12 16:06:25 | INFO | train_inner | epoch 067:    451 / 459 loss=6.943, nll_loss=3.569, mask_ins=1.189, word_ins_ml=5.117, word_reposition=0.637, ppl=123.05, wps=14032.2, ups=0.98, wpb=14297.7, bsz=433.8, num_updates=30700, lr=0.000201784, gnorm=3.16, clip=1, loss_scale=128, train_wall=101, wall=32797
2022-10-12 16:06:33 | INFO | train | epoch 067 | loss 6.96 | nll_loss 3.581 | mask_ins 1.185 | word_ins_ml 5.128 | word_reposition 0.647 | ppl 124.52 | wps 13903.6 | ups 0.95 | wpb 14636.1 | bsz 443.8 | num_updates 30708 | lr 0.000201757 | gnorm 2.961 | clip 0.4 | loss_scale 80 | train_wall 467 | wall 32805
2022-10-12 16:06:42 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 13.968 | nll_loss 10.278 | mask_ins 1.741 | word_ins_ml 11.183 | word_reposition 1.043 | ppl 16021.9 | wps 34973.9 | wpb 1628.7 | bsz 55.8 | num_updates 30708 | best_loss 13.037
2022-10-12 16:06:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 67 @ 30708 updates, score 13.968) (writing took 2.952765655994881 seconds)
2022-10-12 16:08:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 16:08:20 | INFO | train_inner | epoch 068:     93 / 459 loss=6.912, nll_loss=3.537, mask_ins=1.178, word_ins_ml=5.09, word_reposition=0.644, ppl=120.4, wps=12753.5, ups=0.87, wpb=14742.1, bsz=450.4, num_updates=30800, lr=0.000201456, gnorm=2.643, clip=1, loss_scale=120, train_wall=102, wall=32913
2022-10-12 16:10:03 | INFO | train_inner | epoch 068:    193 / 459 loss=6.907, nll_loss=3.527, mask_ins=1.175, word_ins_ml=5.081, word_reposition=0.651, ppl=120.01, wps=14028, ups=0.97, wpb=14419.2, bsz=435.5, num_updates=30900, lr=0.000201129, gnorm=2.668, clip=1, loss_scale=64, train_wall=102, wall=33016
2022-10-12 16:11:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 16:11:48 | INFO | train_inner | epoch 068:    294 / 459 loss=6.966, nll_loss=3.593, mask_ins=1.179, word_ins_ml=5.138, word_reposition=0.649, ppl=125.02, wps=14412.3, ups=0.96, wpb=15079.2, bsz=453.9, num_updates=31000, lr=0.000200805, gnorm=6.046, clip=2, loss_scale=54, train_wall=104, wall=33120
2022-10-12 16:13:30 | INFO | train_inner | epoch 068:    394 / 459 loss=6.89, nll_loss=3.534, mask_ins=1.166, word_ins_ml=5.087, word_reposition=0.636, ppl=118.57, wps=13946, ups=0.98, wpb=14210.5, bsz=432.8, num_updates=31100, lr=0.000200482, gnorm=6.137, clip=3, loss_scale=32, train_wall=101, wall=33222
2022-10-12 16:14:36 | INFO | train | epoch 068 | loss 6.928 | nll_loss 3.557 | mask_ins 1.175 | word_ins_ml 5.107 | word_reposition 0.646 | ppl 121.76 | wps 13834.7 | ups 0.95 | wpb 14636.5 | bsz 443.8 | num_updates 31165 | lr 0.000200273 | gnorm 4.451 | clip 2 | loss_scale 61 | train_wall 467 | wall 33289
2022-10-12 16:14:46 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 14.107 | nll_loss 10.315 | mask_ins 1.766 | word_ins_ml 11.216 | word_reposition 1.126 | ppl 17651 | wps 34877.4 | wpb 1628.7 | bsz 55.8 | num_updates 31165 | best_loss 13.037
2022-10-12 16:14:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 68 @ 31165 updates, score 14.107) (writing took 3.2134482709807344 seconds)
2022-10-12 16:15:25 | INFO | train_inner | epoch 069:     35 / 459 loss=6.958, nll_loss=3.592, mask_ins=1.175, word_ins_ml=5.138, word_reposition=0.645, ppl=124.36, wps=12676.9, ups=0.87, wpb=14625.6, bsz=442.9, num_updates=31200, lr=0.00020016, gnorm=4.671, clip=3, loss_scale=32, train_wall=102, wall=33337
2022-10-12 16:17:09 | INFO | train_inner | epoch 069:    135 / 459 loss=6.898, nll_loss=3.524, mask_ins=1.179, word_ins_ml=5.078, word_reposition=0.64, ppl=119.28, wps=14315.7, ups=0.97, wpb=14834.6, bsz=451.4, num_updates=31300, lr=0.00019984, gnorm=3.282, clip=0, loss_scale=32, train_wall=103, wall=33441
2022-10-12 16:18:51 | INFO | train_inner | epoch 069:    235 / 459 loss=6.919, nll_loss=3.553, mask_ins=1.176, word_ins_ml=5.105, word_reposition=0.638, ppl=120.98, wps=14122.4, ups=0.97, wpb=14500.2, bsz=438.8, num_updates=31400, lr=0.000199522, gnorm=3.775, clip=1, loss_scale=32, train_wall=102, wall=33544
2022-10-12 16:20:34 | INFO | train_inner | epoch 069:    335 / 459 loss=6.922, nll_loss=3.548, mask_ins=1.177, word_ins_ml=5.1, word_reposition=0.645, ppl=121.22, wps=14153.1, ups=0.97, wpb=14588.2, bsz=441.6, num_updates=31500, lr=0.000199205, gnorm=3.037, clip=1, loss_scale=38, train_wall=102, wall=33647
2022-10-12 16:22:18 | INFO | train_inner | epoch 069:    435 / 459 loss=6.91, nll_loss=3.541, mask_ins=1.17, word_ins_ml=5.093, word_reposition=0.647, ppl=120.24, wps=14334.5, ups=0.97, wpb=14789.9, bsz=449.8, num_updates=31600, lr=0.000198889, gnorm=3.254, clip=1, loss_scale=64, train_wall=102, wall=33750
2022-10-12 16:22:42 | INFO | train | epoch 069 | loss 6.914 | nll_loss 3.544 | mask_ins 1.176 | word_ins_ml 5.096 | word_reposition 0.643 | ppl 120.61 | wps 13845.5 | ups 0.95 | wpb 14645 | bsz 444.2 | num_updates 31624 | lr 0.000198814 | gnorm 3.49 | clip 0.9 | loss_scale 42 | train_wall 469 | wall 33774
2022-10-12 16:22:51 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 14.034 | nll_loss 10.292 | mask_ins 1.768 | word_ins_ml 11.196 | word_reposition 1.069 | ppl 16771.5 | wps 34898.9 | wpb 1628.7 | bsz 55.8 | num_updates 31624 | best_loss 13.037
2022-10-12 16:22:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 69 @ 31624 updates, score 14.034) (writing took 2.790713701979257 seconds)
2022-10-12 16:24:13 | INFO | train_inner | epoch 070:     76 / 459 loss=6.906, nll_loss=3.526, mask_ins=1.178, word_ins_ml=5.08, word_reposition=0.648, ppl=119.91, wps=12725.6, ups=0.87, wpb=14664.1, bsz=444.2, num_updates=31700, lr=0.000198575, gnorm=4.273, clip=1, loss_scale=64, train_wall=102, wall=33865
2022-10-12 16:25:55 | INFO | train_inner | epoch 070:    176 / 459 loss=6.879, nll_loss=3.506, mask_ins=1.171, word_ins_ml=5.063, word_reposition=0.645, ppl=117.71, wps=14226, ups=0.98, wpb=14574.2, bsz=441, num_updates=31800, lr=0.000198263, gnorm=4.054, clip=1, loss_scale=64, train_wall=102, wall=33968
2022-10-12 16:26:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 16:27:39 | INFO | train_inner | epoch 070:    277 / 459 loss=6.91, nll_loss=3.545, mask_ins=1.171, word_ins_ml=5.096, word_reposition=0.643, ppl=120.25, wps=13986.5, ups=0.96, wpb=14513.2, bsz=440.6, num_updates=31900, lr=0.000197952, gnorm=6.59, clip=3, loss_scale=34, train_wall=103, wall=34071
2022-10-12 16:29:22 | INFO | train_inner | epoch 070:    377 / 459 loss=6.864, nll_loss=3.514, mask_ins=1.164, word_ins_ml=5.069, word_reposition=0.632, ppl=116.51, wps=14049.5, ups=0.98, wpb=14409.1, bsz=436.2, num_updates=32000, lr=0.000197642, gnorm=5.267, clip=1, loss_scale=32, train_wall=102, wall=34174
2022-10-12 16:30:46 | INFO | train | epoch 070 | loss 6.891 | nll_loss 3.527 | mask_ins 1.17 | word_ins_ml 5.08 | word_reposition 0.64 | ppl 118.69 | wps 13852.8 | ups 0.95 | wpb 14641.8 | bsz 444.1 | num_updates 32082 | lr 0.00019739 | gnorm 4.945 | clip 1.7 | loss_scale 45 | train_wall 468 | wall 34258
2022-10-12 16:30:55 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 14.063 | nll_loss 10.311 | mask_ins 1.766 | word_ins_ml 11.215 | word_reposition 1.081 | ppl 17114.3 | wps 34900.5 | wpb 1628.7 | bsz 55.8 | num_updates 32082 | best_loss 13.037
2022-10-12 16:30:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 70 @ 32082 updates, score 14.063) (writing took 2.7615283300110605 seconds)
2022-10-12 16:31:17 | INFO | train_inner | epoch 071:     18 / 459 loss=6.901, nll_loss=3.545, mask_ins=1.17, word_ins_ml=5.097, word_reposition=0.635, ppl=119.54, wps=12865.6, ups=0.87, wpb=14772.4, bsz=450.6, num_updates=32100, lr=0.000197334, gnorm=4.375, clip=2, loss_scale=32, train_wall=102, wall=34289
2022-10-12 16:33:00 | INFO | train_inner | epoch 071:    118 / 459 loss=6.858, nll_loss=3.494, mask_ins=1.165, word_ins_ml=5.053, word_reposition=0.641, ppl=116, wps=14380.3, ups=0.97, wpb=14896.9, bsz=453.8, num_updates=32200, lr=0.000197028, gnorm=7.346, clip=4, loss_scale=32, train_wall=103, wall=34392
2022-10-12 16:34:43 | INFO | train_inner | epoch 071:    218 / 459 loss=6.898, nll_loss=3.529, mask_ins=1.171, word_ins_ml=5.083, word_reposition=0.645, ppl=119.23, wps=14153.7, ups=0.97, wpb=14549.8, bsz=440.6, num_updates=32300, lr=0.000196722, gnorm=9.132, clip=6, loss_scale=32, train_wall=102, wall=34495
2022-10-12 16:36:25 | INFO | train_inner | epoch 071:    318 / 459 loss=6.87, nll_loss=3.512, mask_ins=1.169, word_ins_ml=5.068, word_reposition=0.633, ppl=116.95, wps=14264.8, ups=0.98, wpb=14549.7, bsz=439.1, num_updates=32400, lr=0.000196419, gnorm=5.733, clip=5, loss_scale=59, train_wall=101, wall=34597
2022-10-12 16:38:08 | INFO | train_inner | epoch 071:    418 / 459 loss=6.925, nll_loss=3.569, mask_ins=1.179, word_ins_ml=5.118, word_reposition=0.628, ppl=121.51, wps=14571, ups=0.97, wpb=14969.8, bsz=454.3, num_updates=32500, lr=0.000196116, gnorm=11.235, clip=7, loss_scale=64, train_wall=102, wall=34700
2022-10-12 16:38:49 | INFO | train | epoch 071 | loss 6.891 | nll_loss 3.527 | mask_ins 1.172 | word_ins_ml 5.081 | word_reposition 0.638 | ppl 118.65 | wps 13913 | ups 0.95 | wpb 14642.1 | bsz 444.1 | num_updates 32541 | lr 0.000195993 | gnorm 8.426 | clip 5.7 | loss_scale 48 | train_wall 467 | wall 34741
2022-10-12 16:38:58 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 13.917 | nll_loss 10.257 | mask_ins 1.734 | word_ins_ml 11.166 | word_reposition 1.016 | ppl 15464.3 | wps 34908.8 | wpb 1628.7 | bsz 55.8 | num_updates 32541 | best_loss 13.037
2022-10-12 16:39:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 71 @ 32541 updates, score 13.917) (writing took 2.7897846499981824 seconds)
2022-10-12 16:40:01 | INFO | train_inner | epoch 072:     59 / 459 loss=6.896, nll_loss=3.51, mask_ins=1.184, word_ins_ml=5.066, word_reposition=0.645, ppl=119.07, wps=12498.2, ups=0.88, wpb=14225, bsz=430.2, num_updates=32600, lr=0.000195815, gnorm=8.135, clip=7, loss_scale=64, train_wall=101, wall=34814
2022-10-12 16:40:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 16:41:45 | INFO | train_inner | epoch 072:    160 / 459 loss=6.884, nll_loss=3.523, mask_ins=1.17, word_ins_ml=5.077, word_reposition=0.637, ppl=118.11, wps=14320.4, ups=0.96, wpb=14899.9, bsz=455, num_updates=32700, lr=0.000195515, gnorm=10.488, clip=10, loss_scale=46, train_wall=103, wall=34918
2022-10-12 16:43:28 | INFO | train_inner | epoch 072:    260 / 459 loss=6.905, nll_loss=3.534, mask_ins=1.17, word_ins_ml=5.087, word_reposition=0.647, ppl=119.82, wps=14182.9, ups=0.98, wpb=14470.6, bsz=437.3, num_updates=32800, lr=0.000195217, gnorm=10.003, clip=7, loss_scale=32, train_wall=101, wall=35020
2022-10-12 16:45:11 | INFO | train_inner | epoch 072:    360 / 459 loss=6.901, nll_loss=3.537, mask_ins=1.171, word_ins_ml=5.09, word_reposition=0.64, ppl=119.54, wps=14470.6, ups=0.96, wpb=15007, bsz=456.5, num_updates=32900, lr=0.00019492, gnorm=6.588, clip=5, loss_scale=32, train_wall=103, wall=35124
2022-10-12 16:46:51 | INFO | train | epoch 072 | loss 6.89 | nll_loss 3.524 | mask_ins 1.169 | word_ins_ml 5.079 | word_reposition 0.642 | ppl 118.57 | wps 13901.4 | ups 0.95 | wpb 14641.3 | bsz 444.1 | num_updates 32999 | lr 0.000194628 | gnorm 9.08 | clip 6.6 | loss_scale 39 | train_wall 466 | wall 35224
2022-10-12 16:47:01 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 14.148 | nll_loss 10.366 | mask_ins 1.761 | word_ins_ml 11.268 | word_reposition 1.12 | ppl 18159.6 | wps 34919.5 | wpb 1628.7 | bsz 55.8 | num_updates 32999 | best_loss 13.037
2022-10-12 16:47:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 72 @ 32999 updates, score 14.148) (writing took 2.7436185220140032 seconds)
2022-10-12 16:47:04 | INFO | train_inner | epoch 073:      1 / 459 loss=6.882, nll_loss=3.528, mask_ins=1.161, word_ins_ml=5.081, word_reposition=0.64, ppl=117.97, wps=12612.5, ups=0.88, wpb=14277.8, bsz=431, num_updates=33000, lr=0.000194625, gnorm=10.797, clip=5, loss_scale=32, train_wall=100, wall=35237
2022-10-12 16:48:47 | INFO | train_inner | epoch 073:    101 / 459 loss=6.856, nll_loss=3.49, mask_ins=1.165, word_ins_ml=5.049, word_reposition=0.642, ppl=115.87, wps=14200.1, ups=0.98, wpb=14562.5, bsz=443, num_updates=33100, lr=0.000194331, gnorm=4.714, clip=3, loss_scale=32, train_wall=102, wall=35339
2022-10-12 16:50:30 | INFO | train_inner | epoch 073:    201 / 459 loss=6.836, nll_loss=3.467, mask_ins=1.172, word_ins_ml=5.029, word_reposition=0.636, ppl=114.26, wps=14298.9, ups=0.97, wpb=14692, bsz=445.9, num_updates=33200, lr=0.000194038, gnorm=9.468, clip=6, loss_scale=46, train_wall=102, wall=35442
2022-10-12 16:52:12 | INFO | train_inner | epoch 073:    301 / 459 loss=6.872, nll_loss=3.519, mask_ins=1.167, word_ins_ml=5.075, word_reposition=0.63, ppl=117.11, wps=14249.2, ups=0.98, wpb=14570.1, bsz=441.5, num_updates=33300, lr=0.000193746, gnorm=16.219, clip=10, loss_scale=64, train_wall=101, wall=35544
2022-10-12 16:53:54 | INFO | train_inner | epoch 073:    401 / 459 loss=6.909, nll_loss=3.536, mask_ins=1.173, word_ins_ml=5.089, word_reposition=0.647, ppl=120.19, wps=14336.2, ups=0.98, wpb=14688.6, bsz=443.1, num_updates=33400, lr=0.000193456, gnorm=7.605, clip=8, loss_scale=64, train_wall=102, wall=35647
2022-10-12 16:54:54 | INFO | train | epoch 073 | loss 6.867 | nll_loss 3.502 | mask_ins 1.168 | word_ins_ml 5.06 | word_reposition 0.639 | ppl 116.72 | wps 13931.2 | ups 0.95 | wpb 14638.3 | bsz 444 | num_updates 33458 | lr 0.000193288 | gnorm 9.102 | clip 6.5 | loss_scale 53 | train_wall 466 | wall 35706
2022-10-12 16:55:03 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 14.037 | nll_loss 10.278 | mask_ins 1.781 | word_ins_ml 11.184 | word_reposition 1.072 | ppl 16815.4 | wps 34946 | wpb 1628.7 | bsz 55.8 | num_updates 33458 | best_loss 13.037
2022-10-12 16:55:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 73 @ 33458 updates, score 14.037) (writing took 2.660733833006816 seconds)
2022-10-12 16:55:49 | INFO | train_inner | epoch 074:     42 / 459 loss=6.842, nll_loss=3.475, mask_ins=1.163, word_ins_ml=5.036, word_reposition=0.643, ppl=114.71, wps=12734.5, ups=0.88, wpb=14533.6, bsz=440.2, num_updates=33500, lr=0.000193167, gnorm=6.73, clip=5, loss_scale=64, train_wall=101, wall=35761
2022-10-12 16:57:32 | INFO | train_inner | epoch 074:    142 / 459 loss=6.868, nll_loss=3.496, mask_ins=1.171, word_ins_ml=5.053, word_reposition=0.643, ppl=116.78, wps=14290.7, ups=0.97, wpb=14805.3, bsz=446.9, num_updates=33600, lr=0.000192879, gnorm=6.448, clip=3, loss_scale=64, train_wall=103, wall=35864
2022-10-12 16:59:16 | INFO | train_inner | epoch 074:    242 / 459 loss=6.851, nll_loss=3.493, mask_ins=1.158, word_ins_ml=5.052, word_reposition=0.642, ppl=115.45, wps=14092.5, ups=0.96, wpb=14635.1, bsz=443.8, num_updates=33700, lr=0.000192593, gnorm=5.736, clip=2, loss_scale=85, train_wall=103, wall=35968
2022-10-12 17:01:00 | INFO | train_inner | epoch 074:    342 / 459 loss=6.862, nll_loss=3.497, mask_ins=1.173, word_ins_ml=5.054, word_reposition=0.635, ppl=116.34, wps=14257.4, ups=0.96, wpb=14878.2, bsz=453.2, num_updates=33800, lr=0.000192308, gnorm=3.853, clip=1, loss_scale=128, train_wall=103, wall=36073
2022-10-12 17:02:44 | INFO | train_inner | epoch 074:    442 / 459 loss=6.835, nll_loss=3.472, mask_ins=1.17, word_ins_ml=5.033, word_reposition=0.632, ppl=114.2, wps=14106.1, ups=0.97, wpb=14564.4, bsz=444.2, num_updates=33900, lr=0.000192024, gnorm=4.845, clip=5, loss_scale=128, train_wall=102, wall=36176
2022-10-12 17:03:01 | INFO | train | epoch 074 | loss 6.852 | nll_loss 3.486 | mask_ins 1.168 | word_ins_ml 5.045 | word_reposition 0.639 | ppl 115.51 | wps 13799.1 | ups 0.94 | wpb 14639.7 | bsz 444 | num_updates 33917 | lr 0.000191976 | gnorm 5.323 | clip 2.8 | loss_scale 99 | train_wall 471 | wall 36193
2022-10-12 17:03:10 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 14.176 | nll_loss 10.395 | mask_ins 1.791 | word_ins_ml 11.295 | word_reposition 1.091 | ppl 18508.2 | wps 34835 | wpb 1628.7 | bsz 55.8 | num_updates 33917 | best_loss 13.037
2022-10-12 17:03:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 74 @ 33917 updates, score 14.176) (writing took 2.9707429709960707 seconds)
2022-10-12 17:04:38 | INFO | train_inner | epoch 075:     83 / 459 loss=6.847, nll_loss=3.475, mask_ins=1.175, word_ins_ml=5.035, word_reposition=0.636, ppl=115.11, wps=12673.1, ups=0.87, wpb=14554.5, bsz=443.4, num_updates=34000, lr=0.000191741, gnorm=3.104, clip=0, loss_scale=128, train_wall=102, wall=36291
2022-10-12 17:06:22 | INFO | train_inner | epoch 075:    183 / 459 loss=6.819, nll_loss=3.459, mask_ins=1.168, word_ins_ml=5.022, word_reposition=0.628, ppl=112.87, wps=14211.9, ups=0.97, wpb=14720.5, bsz=447.8, num_updates=34100, lr=0.00019146, gnorm=3.571, clip=0, loss_scale=128, train_wall=103, wall=36394
2022-10-12 17:07:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 17:08:06 | INFO | train_inner | epoch 075:    284 / 459 loss=6.834, nll_loss=3.484, mask_ins=1.161, word_ins_ml=5.043, word_reposition=0.63, ppl=114.06, wps=14412.3, ups=0.96, wpb=14949.3, bsz=453.3, num_updates=34200, lr=0.00019118, gnorm=4.256, clip=2, loss_scale=92, train_wall=103, wall=36498
2022-10-12 17:09:49 | INFO | train_inner | epoch 075:    384 / 459 loss=6.853, nll_loss=3.48, mask_ins=1.168, word_ins_ml=5.04, word_reposition=0.644, ppl=115.58, wps=14049.4, ups=0.97, wpb=14445.7, bsz=435.6, num_updates=34300, lr=0.000190901, gnorm=4.279, clip=3, loss_scale=64, train_wall=102, wall=36601
2022-10-12 17:11:05 | INFO | train | epoch 075 | loss 6.834 | nll_loss 3.472 | mask_ins 1.167 | word_ins_ml 5.033 | word_reposition 0.634 | ppl 114.09 | wps 13835.8 | ups 0.94 | wpb 14642.8 | bsz 444.1 | num_updates 34375 | lr 0.000190693 | gnorm 3.883 | clip 1.7 | loss_scale 96 | train_wall 468 | wall 36678
2022-10-12 17:11:15 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 14.025 | nll_loss 10.263 | mask_ins 1.791 | word_ins_ml 11.169 | word_reposition 1.065 | ppl 16673.1 | wps 34954.3 | wpb 1628.7 | bsz 55.8 | num_updates 34375 | best_loss 13.037
2022-10-12 17:11:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 75 @ 34375 updates, score 14.025) (writing took 2.857256636983948 seconds)
2022-10-12 17:11:43 | INFO | train_inner | epoch 076:     25 / 459 loss=6.824, nll_loss=3.466, mask_ins=1.166, word_ins_ml=5.027, word_reposition=0.63, ppl=113.28, wps=12476.6, ups=0.87, wpb=14261.3, bsz=431.4, num_updates=34400, lr=0.000190623, gnorm=4.808, clip=5, loss_scale=64, train_wall=101, wall=36715
2022-10-12 17:13:25 | INFO | train_inner | epoch 076:    125 / 459 loss=6.8, nll_loss=3.432, mask_ins=1.156, word_ins_ml=4.998, word_reposition=0.646, ppl=111.41, wps=14359, ups=0.98, wpb=14661.1, bsz=442.9, num_updates=34500, lr=0.000190347, gnorm=5.785, clip=4, loss_scale=64, train_wall=101, wall=36817
2022-10-12 17:15:08 | INFO | train_inner | epoch 076:    225 / 459 loss=6.774, nll_loss=3.432, mask_ins=1.148, word_ins_ml=4.998, word_reposition=0.629, ppl=109.48, wps=14397.4, ups=0.97, wpb=14804.2, bsz=450.1, num_updates=34600, lr=0.000190071, gnorm=4.022, clip=1, loss_scale=64, train_wall=102, wall=36920
2022-10-12 17:16:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 17:16:51 | INFO | train_inner | epoch 076:    326 / 459 loss=6.821, nll_loss=3.457, mask_ins=1.162, word_ins_ml=5.02, word_reposition=0.639, ppl=113.06, wps=14038.6, ups=0.96, wpb=14551.1, bsz=440, num_updates=34700, lr=0.000189797, gnorm=3.299, clip=0, loss_scale=67, train_wall=103, wall=37024
2022-10-12 17:18:35 | INFO | train_inner | epoch 076:    426 / 459 loss=6.821, nll_loss=3.47, mask_ins=1.156, word_ins_ml=5.03, word_reposition=0.635, ppl=113.08, wps=14249, ups=0.97, wpb=14702.4, bsz=447.2, num_updates=34800, lr=0.000189525, gnorm=4.517, clip=3, loss_scale=64, train_wall=102, wall=37127
2022-10-12 17:19:09 | INFO | train | epoch 076 | loss 6.805 | nll_loss 3.45 | mask_ins 1.155 | word_ins_ml 5.013 | word_reposition 0.637 | ppl 111.83 | wps 13874.1 | ups 0.95 | wpb 14641.8 | bsz 444 | num_updates 34833 | lr 0.000189435 | gnorm 4.513 | clip 2.2 | loss_scale 65 | train_wall 467 | wall 37161
2022-10-12 17:19:18 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 14.061 | nll_loss 10.291 | mask_ins 1.745 | word_ins_ml 11.196 | word_reposition 1.12 | ppl 17089.3 | wps 34908.6 | wpb 1628.7 | bsz 55.8 | num_updates 34833 | best_loss 13.037
2022-10-12 17:19:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 76 @ 34833 updates, score 14.061) (writing took 2.9309397760080174 seconds)
2022-10-12 17:20:30 | INFO | train_inner | epoch 077:     67 / 459 loss=6.806, nll_loss=3.46, mask_ins=1.157, word_ins_ml=5.023, word_reposition=0.626, ppl=111.9, wps=12774.6, ups=0.87, wpb=14721.2, bsz=447.2, num_updates=34900, lr=0.000189253, gnorm=4.065, clip=0, loss_scale=64, train_wall=102, wall=37242
2022-10-12 17:22:12 | INFO | train_inner | epoch 077:    167 / 459 loss=6.811, nll_loss=3.448, mask_ins=1.167, word_ins_ml=5.012, word_reposition=0.631, ppl=112.29, wps=14162.3, ups=0.98, wpb=14478.1, bsz=438.9, num_updates=35000, lr=0.000188982, gnorm=6.29, clip=3, loss_scale=64, train_wall=101, wall=37344
2022-10-12 17:23:55 | INFO | train_inner | epoch 077:    267 / 459 loss=6.834, nll_loss=3.475, mask_ins=1.163, word_ins_ml=5.036, word_reposition=0.635, ppl=114.12, wps=14456.9, ups=0.98, wpb=14827.4, bsz=447.6, num_updates=35100, lr=0.000188713, gnorm=9.699, clip=8, loss_scale=64, train_wall=102, wall=37447
2022-10-12 17:24:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 17:25:38 | INFO | train_inner | epoch 077:    368 / 459 loss=6.777, nll_loss=3.429, mask_ins=1.16, word_ins_ml=4.996, word_reposition=0.622, ppl=109.68, wps=14089.7, ups=0.97, wpb=14516.6, bsz=444, num_updates=35200, lr=0.000188445, gnorm=13.833, clip=9, loss_scale=39, train_wall=102, wall=37550
2022-10-12 17:27:11 | INFO | train | epoch 077 | loss 6.808 | nll_loss 3.454 | mask_ins 1.161 | word_ins_ml 5.017 | word_reposition 0.629 | ppl 112.05 | wps 13895.3 | ups 0.95 | wpb 14635.7 | bsz 443.9 | num_updates 35291 | lr 0.000188201 | gnorm 9.264 | clip 5.9 | loss_scale 52 | train_wall 466 | wall 37643
2022-10-12 17:27:20 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 14.183 | nll_loss 10.389 | mask_ins 1.747 | word_ins_ml 11.288 | word_reposition 1.148 | ppl 18594.8 | wps 35014.6 | wpb 1628.7 | bsz 55.8 | num_updates 35291 | best_loss 13.037
2022-10-12 17:27:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 77 @ 35291 updates, score 14.183) (writing took 2.8082706239947584 seconds)
2022-10-12 17:27:33 | INFO | train_inner | epoch 078:      9 / 459 loss=6.805, nll_loss=3.456, mask_ins=1.154, word_ins_ml=5.018, word_reposition=0.633, ppl=111.85, wps=12696.5, ups=0.87, wpb=14571.2, bsz=440.2, num_updates=35300, lr=0.000188177, gnorm=15.093, clip=9, loss_scale=32, train_wall=102, wall=37665
2022-10-12 17:29:16 | INFO | train_inner | epoch 078:    109 / 459 loss=6.823, nll_loss=3.461, mask_ins=1.162, word_ins_ml=5.023, word_reposition=0.638, ppl=113.22, wps=14376.9, ups=0.96, wpb=14910.9, bsz=450.3, num_updates=35400, lr=0.000187912, gnorm=7.97, clip=5, loss_scale=32, train_wall=103, wall=37769
2022-10-12 17:29:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2022-10-12 17:30:59 | INFO | train_inner | epoch 078:    210 / 459 loss=6.775, nll_loss=3.413, mask_ins=1.165, word_ins_ml=4.982, word_reposition=0.628, ppl=109.5, wps=14106.2, ups=0.97, wpb=14552, bsz=442.2, num_updates=35500, lr=0.000187647, gnorm=7.119, clip=4, loss_scale=21, train_wall=102, wall=37872
2022-10-12 17:32:41 | INFO | train_inner | epoch 078:    310 / 459 loss=6.783, nll_loss=3.43, mask_ins=1.158, word_ins_ml=4.996, word_reposition=0.629, ppl=110.11, wps=14247.3, ups=0.98, wpb=14520.8, bsz=440.4, num_updates=35600, lr=0.000187383, gnorm=5.97, clip=3, loss_scale=16, train_wall=101, wall=37974
2022-10-12 17:34:24 | INFO | train_inner | epoch 078:    410 / 459 loss=6.822, nll_loss=3.463, mask_ins=1.159, word_ins_ml=5.024, word_reposition=0.639, ppl=113.15, wps=14301.8, ups=0.97, wpb=14735.4, bsz=449, num_updates=35700, lr=0.00018712, gnorm=4.108, clip=0, loss_scale=16, train_wall=102, wall=38077
2022-10-12 17:35:14 | INFO | train | epoch 078 | loss 6.806 | nll_loss 3.444 | mask_ins 1.162 | word_ins_ml 5.008 | word_reposition 0.635 | ppl 111.86 | wps 13885 | ups 0.95 | wpb 14642.7 | bsz 444.1 | num_updates 35749 | lr 0.000186992 | gnorm 7.107 | clip 3.3 | loss_scale 21 | train_wall 467 | wall 38126
2022-10-12 17:35:23 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 14.264 | nll_loss 10.388 | mask_ins 1.761 | word_ins_ml 11.282 | word_reposition 1.222 | ppl 19680.6 | wps 34891 | wpb 1628.7 | bsz 55.8 | num_updates 35749 | best_loss 13.037
2022-10-12 17:35:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 78 @ 35749 updates, score 14.264) (writing took 2.9431066359975375 seconds)
2022-10-12 17:36:19 | INFO | train_inner | epoch 079:     51 / 459 loss=6.811, nll_loss=3.437, mask_ins=1.167, word_ins_ml=5.001, word_reposition=0.643, ppl=112.26, wps=12774.6, ups=0.87, wpb=14610, bsz=442.2, num_updates=35800, lr=0.000186859, gnorm=4.376, clip=2, loss_scale=16, train_wall=101, wall=38191
2022-10-12 17:38:02 | INFO | train_inner | epoch 079:    151 / 459 loss=6.757, nll_loss=3.414, mask_ins=1.151, word_ins_ml=4.982, word_reposition=0.624, ppl=108.19, wps=14261.9, ups=0.97, wpb=14666.3, bsz=445, num_updates=35900, lr=0.000186598, gnorm=3.739, clip=1, loss_scale=16, train_wall=102, wall=38294
2022-10-12 17:39:44 | INFO | train_inner | epoch 079:    251 / 459 loss=6.793, nll_loss=3.421, mask_ins=1.17, word_ins_ml=4.988, word_reposition=0.635, ppl=110.88, wps=14220.2, ups=0.98, wpb=14540.8, bsz=442.6, num_updates=36000, lr=0.000186339, gnorm=5.097, clip=3, loss_scale=25, train_wall=101, wall=38396
2022-10-12 17:41:26 | INFO | train_inner | epoch 079:    351 / 459 loss=6.756, nll_loss=3.4, mask_ins=1.155, word_ins_ml=4.969, word_reposition=0.632, ppl=108.08, wps=14150.2, ups=0.97, wpb=14531.4, bsz=441, num_updates=36100, lr=0.000186081, gnorm=6.558, clip=2, loss_scale=32, train_wall=102, wall=38499
2022-10-12 17:43:10 | INFO | train_inner | epoch 079:    451 / 459 loss=6.845, nll_loss=3.492, mask_ins=1.149, word_ins_ml=5.05, word_reposition=0.646, ppl=114.96, wps=14446.2, ups=0.97, wpb=14900.1, bsz=448.9, num_updates=36200, lr=0.000185824, gnorm=7.96, clip=6, loss_scale=32, train_wall=102, wall=38602
2022-10-12 17:43:17 | INFO | train | epoch 079 | loss 6.783 | nll_loss 3.425 | mask_ins 1.157 | word_ins_ml 4.991 | word_reposition 0.635 | ppl 110.1 | wps 13898.8 | ups 0.95 | wpb 14635.5 | bsz 443.8 | num_updates 36208 | lr 0.000185803 | gnorm 5.637 | clip 2.8 | loss_scale 25 | train_wall 467 | wall 38610
2022-10-12 17:43:27 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 14.256 | nll_loss 10.485 | mask_ins 1.81 | word_ins_ml 11.375 | word_reposition 1.071 | ppl 19567.3 | wps 34928.6 | wpb 1628.7 | bsz 55.8 | num_updates 36208 | best_loss 13.037
2022-10-12 17:43:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 79 @ 36208 updates, score 14.256) (writing took 3.172853864991339 seconds)
2022-10-12 17:45:05 | INFO | train_inner | epoch 080:     92 / 459 loss=6.788, nll_loss=3.425, mask_ins=1.16, word_ins_ml=4.992, word_reposition=0.636, ppl=110.53, wps=12653.7, ups=0.87, wpb=14555.1, bsz=442, num_updates=36300, lr=0.000185567, gnorm=5.73, clip=1, loss_scale=32, train_wall=102, wall=38717
2022-10-12 17:46:48 | INFO | train_inner | epoch 080:    192 / 459 loss=6.748, nll_loss=3.403, mask_ins=1.146, word_ins_ml=4.973, word_reposition=0.629, ppl=107.51, wps=14399.2, ups=0.97, wpb=14825.9, bsz=448.7, num_updates=36400, lr=0.000185312, gnorm=7.662, clip=5, loss_scale=32, train_wall=102, wall=38820
2022-10-12 17:48:30 | INFO | train_inner | epoch 080:    292 / 459 loss=6.774, nll_loss=3.414, mask_ins=1.16, word_ins_ml=4.983, word_reposition=0.63, ppl=109.41, wps=14200.3, ups=0.98, wpb=14524.6, bsz=439.4, num_updates=36500, lr=0.000185058, gnorm=3.574, clip=1, loss_scale=46, train_wall=101, wall=38922
2022-10-12 17:50:13 | INFO | train_inner | epoch 080:    392 / 459 loss=6.798, nll_loss=3.435, mask_ins=1.159, word_ins_ml=5, word_reposition=0.639, ppl=111.27, wps=14231.5, ups=0.97, wpb=14642.5, bsz=443.1, num_updates=36600, lr=0.000184805, gnorm=5.388, clip=1, loss_scale=64, train_wall=102, wall=39025
2022-10-12 17:51:21 | INFO | train | epoch 080 | loss 6.775 | nll_loss 3.419 | mask_ins 1.155 | word_ins_ml 4.986 | word_reposition 0.633 | ppl 109.53 | wps 13886.9 | ups 0.95 | wpb 14637.6 | bsz 444 | num_updates 36667 | lr 0.000184636 | gnorm 5.545 | clip 2 | loss_scale 47 | train_wall 467 | wall 39094
2022-10-12 17:51:31 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 14.2 | nll_loss 10.381 | mask_ins 1.758 | word_ins_ml 11.283 | word_reposition 1.159 | ppl 18822.3 | wps 35015.9 | wpb 1628.7 | bsz 55.8 | num_updates 36667 | best_loss 13.037
2022-10-12 17:51:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 80 @ 36667 updates, score 14.2) (writing took 3.076642175001325 seconds)
2022-10-12 17:52:08 | INFO | train_inner | epoch 081:     33 / 459 loss=6.727, nll_loss=3.381, mask_ins=1.144, word_ins_ml=4.953, word_reposition=0.63, ppl=105.9, wps=12644.4, ups=0.87, wpb=14542.2, bsz=445.4, num_updates=36700, lr=0.000184553, gnorm=4.897, clip=1, loss_scale=64, train_wall=102, wall=39140
2022-10-12 17:53:51 | INFO | train_inner | epoch 081:    133 / 459 loss=6.732, nll_loss=3.387, mask_ins=1.151, word_ins_ml=4.958, word_reposition=0.622, ppl=106.29, wps=14528.3, ups=0.97, wpb=14948.2, bsz=453.3, num_updates=36800, lr=0.000184302, gnorm=4.207, clip=2, loss_scale=64, train_wall=102, wall=39243
2022-10-12 17:55:34 | INFO | train_inner | epoch 081:    233 / 459 loss=6.744, nll_loss=3.388, mask_ins=1.152, word_ins_ml=4.959, word_reposition=0.633, ppl=107.2, wps=14283.8, ups=0.97, wpb=14693.6, bsz=447.1, num_updates=36900, lr=0.000184053, gnorm=3.2, clip=0, loss_scale=64, train_wall=102, wall=39346
2022-10-12 17:57:17 | INFO | train_inner | epoch 081:    333 / 459 loss=6.757, nll_loss=3.402, mask_ins=1.16, word_ins_ml=4.971, word_reposition=0.626, ppl=108.16, wps=14014.5, ups=0.97, wpb=14453.4, bsz=435.6, num_updates=37000, lr=0.000183804, gnorm=3.469, clip=0, loss_scale=85, train_wall=102, wall=39449
2022-10-12 17:58:59 | INFO | train_inner | epoch 081:    433 / 459 loss=6.77, nll_loss=3.427, mask_ins=1.149, word_ins_ml=4.993, word_reposition=0.628, ppl=109.13, wps=14052.5, ups=0.98, wpb=14358.4, bsz=435.4, num_updates=37100, lr=0.000183556, gnorm=4.319, clip=2, loss_scale=128, train_wall=101, wall=39551
2022-10-12 17:59:25 | INFO | train | epoch 081 | loss 6.751 | nll_loss 3.4 | mask_ins 1.153 | word_ins_ml 4.97 | word_reposition 0.628 | ppl 107.7 | wps 13882.1 | ups 0.95 | wpb 14642.9 | bsz 444.1 | num_updates 37126 | lr 0.000183491 | gnorm 4.13 | clip 1.5 | loss_scale 86 | train_wall 468 | wall 39578
2022-10-12 17:59:35 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 14.211 | nll_loss 10.456 | mask_ins 1.774 | word_ins_ml 11.347 | word_reposition 1.09 | ppl 18968.3 | wps 34987.4 | wpb 1628.7 | bsz 55.8 | num_updates 37126 | best_loss 13.037
2022-10-12 17:59:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 81 @ 37126 updates, score 14.211) (writing took 2.8250327330024447 seconds)
2022-10-12 18:00:54 | INFO | train_inner | epoch 082:     74 / 459 loss=6.729, nll_loss=3.386, mask_ins=1.153, word_ins_ml=4.957, word_reposition=0.619, ppl=106.08, wps=12995.6, ups=0.87, wpb=14901.4, bsz=454.9, num_updates=37200, lr=0.000183309, gnorm=4.016, clip=3, loss_scale=128, train_wall=102, wall=39666
2022-10-12 18:02:35 | INFO | train_inner | epoch 082:    174 / 459 loss=6.739, nll_loss=3.388, mask_ins=1.15, word_ins_ml=4.96, word_reposition=0.63, ppl=106.84, wps=14137.4, ups=0.98, wpb=14363.1, bsz=431.8, num_updates=37300, lr=0.000183063, gnorm=2.456, clip=0, loss_scale=128, train_wall=101, wall=39767
2022-10-12 18:04:18 | INFO | train_inner | epoch 082:    274 / 459 loss=6.738, nll_loss=3.394, mask_ins=1.149, word_ins_ml=4.964, word_reposition=0.625, ppl=106.75, wps=14301.1, ups=0.97, wpb=14685.9, bsz=447.6, num_updates=37400, lr=0.000182818, gnorm=2.749, clip=0, loss_scale=128, train_wall=102, wall=39870
2022-10-12 18:06:00 | INFO | train_inner | epoch 082:    374 / 459 loss=6.74, nll_loss=3.385, mask_ins=1.149, word_ins_ml=4.956, word_reposition=0.634, ppl=106.86, wps=14124.4, ups=0.98, wpb=14363.9, bsz=435.3, num_updates=37500, lr=0.000182574, gnorm=3.392, clip=1, loss_scale=155, train_wall=101, wall=39972
2022-10-12 18:07:27 | INFO | train | epoch 082 | loss 6.735 | nll_loss 3.391 | mask_ins 1.147 | word_ins_ml 4.961 | word_reposition 0.627 | ppl 106.54 | wps 13954.2 | ups 0.95 | wpb 14638 | bsz 443.9 | num_updates 37585 | lr 0.000182368 | gnorm 2.738 | clip 0.2 | loss_scale 158 | train_wall 465 | wall 40059
2022-10-12 18:07:36 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 14.247 | nll_loss 10.458 | mask_ins 1.791 | word_ins_ml 11.348 | word_reposition 1.108 | ppl 19441.4 | wps 35069 | wpb 1628.7 | bsz 55.8 | num_updates 37585 | best_loss 13.037
2022-10-12 18:07:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 82 @ 37585 updates, score 14.247) (writing took 2.807394035975449 seconds)
2022-10-12 18:07:54 | INFO | train_inner | epoch 083:     15 / 459 loss=6.736, nll_loss=3.4, mask_ins=1.142, word_ins_ml=4.969, word_reposition=0.625, ppl=106.56, wps=12964.6, ups=0.87, wpb=14857.3, bsz=449.6, num_updates=37600, lr=0.000182331, gnorm=2.605, clip=0, loss_scale=256, train_wall=102, wall=40086
2022-10-12 18:09:37 | INFO | train_inner | epoch 083:    115 / 459 loss=6.721, nll_loss=3.371, mask_ins=1.15, word_ins_ml=4.944, word_reposition=0.626, ppl=105.48, wps=14416.1, ups=0.98, wpb=14780.7, bsz=447.4, num_updates=37700, lr=0.000182089, gnorm=2.611, clip=0, loss_scale=256, train_wall=102, wall=40189
2022-10-12 18:11:19 | INFO | train_inner | epoch 083:    215 / 459 loss=6.681, nll_loss=3.331, mask_ins=1.143, word_ins_ml=4.909, word_reposition=0.63, ppl=102.62, wps=14236.1, ups=0.97, wpb=14630.4, bsz=448.6, num_updates=37800, lr=0.000181848, gnorm=2.44, clip=0, loss_scale=256, train_wall=102, wall=40292
2022-10-12 18:13:02 | INFO | train_inner | epoch 083:    315 / 459 loss=6.703, nll_loss=3.355, mask_ins=1.143, word_ins_ml=4.931, word_reposition=0.629, ppl=104.16, wps=14247, ups=0.97, wpb=14641.5, bsz=441.1, num_updates=37900, lr=0.000181608, gnorm=2.024, clip=0, loss_scale=256, train_wall=102, wall=40394
2022-10-12 18:14:45 | INFO | train_inner | epoch 083:    415 / 459 loss=6.72, nll_loss=3.369, mask_ins=1.151, word_ins_ml=4.942, word_reposition=0.627, ppl=105.41, wps=14438.1, ups=0.97, wpb=14838.5, bsz=450.3, num_updates=38000, lr=0.000181369, gnorm=2.011, clip=0, loss_scale=279, train_wall=102, wall=40497
2022-10-12 18:15:30 | INFO | train | epoch 083 | loss 6.704 | nll_loss 3.357 | mask_ins 1.145 | word_ins_ml 4.931 | word_reposition 0.627 | ppl 104.26 | wps 13920.5 | ups 0.95 | wpb 14642.4 | bsz 444.1 | num_updates 38044 | lr 0.000181264 | gnorm 2.249 | clip 0 | loss_scale 286 | train_wall 467 | wall 40542
2022-10-12 18:15:39 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 14.372 | nll_loss 10.498 | mask_ins 1.833 | word_ins_ml 11.388 | word_reposition 1.151 | ppl 21205 | wps 35015.2 | wpb 1628.7 | bsz 55.8 | num_updates 38044 | best_loss 13.037
2022-10-12 18:15:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 83 @ 38044 updates, score 14.372) (writing took 2.904138874000637 seconds)
2022-10-12 18:16:39 | INFO | train_inner | epoch 084:     56 / 459 loss=6.696, nll_loss=3.355, mask_ins=1.142, word_ins_ml=4.93, word_reposition=0.624, ppl=103.7, wps=12578.4, ups=0.88, wpb=14367.1, bsz=437.6, num_updates=38100, lr=0.000181131, gnorm=2.12, clip=0, loss_scale=512, train_wall=101, wall=40611
2022-10-12 18:18:22 | INFO | train_inner | epoch 084:    156 / 459 loss=6.696, nll_loss=3.355, mask_ins=1.148, word_ins_ml=4.93, word_reposition=0.618, ppl=103.68, wps=14339.3, ups=0.98, wpb=14693.3, bsz=445.1, num_updates=38200, lr=0.000180894, gnorm=2.052, clip=0, loss_scale=512, train_wall=102, wall=40714
2022-10-12 18:19:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 18:20:06 | INFO | train_inner | epoch 084:    257 / 459 loss=6.702, nll_loss=3.354, mask_ins=1.147, word_ins_ml=4.929, word_reposition=0.625, ppl=104.1, wps=14035.1, ups=0.96, wpb=14663.7, bsz=444.8, num_updates=38300, lr=0.000180657, gnorm=2.859, clip=1, loss_scale=444, train_wall=104, wall=40818
2022-10-12 18:21:49 | INFO | train_inner | epoch 084:    357 / 459 loss=6.705, nll_loss=3.362, mask_ins=1.146, word_ins_ml=4.935, word_reposition=0.623, ppl=104.32, wps=14352.1, ups=0.97, wpb=14720.2, bsz=444.7, num_updates=38400, lr=0.000180422, gnorm=3.279, clip=1, loss_scale=256, train_wall=102, wall=40921
2022-10-12 18:23:31 | INFO | train_inner | epoch 084:    457 / 459 loss=6.739, nll_loss=3.398, mask_ins=1.15, word_ins_ml=4.967, word_reposition=0.622, ppl=106.84, wps=14229, ups=0.98, wpb=14584.5, bsz=441.8, num_updates=38500, lr=0.000180187, gnorm=3.863, clip=2, loss_scale=256, train_wall=102, wall=41023
2022-10-12 18:23:33 | INFO | train | epoch 084 | loss 6.706 | nll_loss 3.362 | mask_ins 1.148 | word_ins_ml 4.936 | word_reposition 0.622 | ppl 104.41 | wps 13871.5 | ups 0.95 | wpb 14640.6 | bsz 444 | num_updates 38502 | lr 0.000180183 | gnorm 2.936 | clip 0.9 | loss_scale 384 | train_wall 467 | wall 41025
2022-10-12 18:23:42 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 14.21 | nll_loss 10.426 | mask_ins 1.762 | word_ins_ml 11.317 | word_reposition 1.131 | ppl 18948.8 | wps 34995.4 | wpb 1628.7 | bsz 55.8 | num_updates 38502 | best_loss 13.037
2022-10-12 18:23:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 84 @ 38502 updates, score 14.21) (writing took 2.542968514986569 seconds)
2022-10-12 18:25:26 | INFO | train_inner | epoch 085:     98 / 459 loss=6.671, nll_loss=3.326, mask_ins=1.139, word_ins_ml=4.905, word_reposition=0.626, ppl=101.88, wps=12715.8, ups=0.87, wpb=14582.3, bsz=439.2, num_updates=38600, lr=0.000179954, gnorm=4.303, clip=1, loss_scale=256, train_wall=102, wall=41138
2022-10-12 18:27:09 | INFO | train_inner | epoch 085:    198 / 459 loss=6.651, nll_loss=3.312, mask_ins=1.132, word_ins_ml=4.893, word_reposition=0.626, ppl=100.49, wps=14498.1, ups=0.97, wpb=14951.6, bsz=455.8, num_updates=38700, lr=0.000179721, gnorm=3.5, clip=0, loss_scale=256, train_wall=102, wall=41241
2022-10-12 18:28:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 18:28:52 | INFO | train_inner | epoch 085:    299 / 459 loss=6.656, nll_loss=3.307, mask_ins=1.143, word_ins_ml=4.888, word_reposition=0.626, ppl=100.87, wps=14154.2, ups=0.97, wpb=14633.6, bsz=442.1, num_updates=38800, lr=0.00017949, gnorm=3.645, clip=0, loss_scale=264, train_wall=102, wall=41345
2022-10-12 18:30:34 | INFO | train_inner | epoch 085:    399 / 459 loss=6.679, nll_loss=3.338, mask_ins=1.146, word_ins_ml=4.915, word_reposition=0.618, ppl=102.46, wps=14265.6, ups=0.98, wpb=14550.2, bsz=442.1, num_updates=38900, lr=0.000179259, gnorm=4.83, clip=2, loss_scale=256, train_wall=101, wall=41447
2022-10-12 18:31:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 18:31:36 | INFO | train | epoch 085 | loss 6.666 | nll_loss 3.323 | mask_ins 1.14 | word_ins_ml 4.902 | word_reposition 0.623 | ppl 101.52 | wps 13866.9 | ups 0.95 | wpb 14640.5 | bsz 444 | num_updates 38959 | lr 0.000179123 | gnorm 4.158 | clip 1.1 | loss_scale 248 | train_wall 467 | wall 41508
2022-10-12 18:31:45 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 14.213 | nll_loss 10.423 | mask_ins 1.812 | word_ins_ml 11.322 | word_reposition 1.08 | ppl 18994.7 | wps 34976.2 | wpb 1628.7 | bsz 55.8 | num_updates 38959 | best_loss 13.037
2022-10-12 18:31:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 85 @ 38959 updates, score 14.213) (writing took 3.015116485999897 seconds)
2022-10-12 18:32:29 | INFO | train_inner | epoch 086:     41 / 459 loss=6.663, nll_loss=3.324, mask_ins=1.14, word_ins_ml=4.903, word_reposition=0.62, ppl=101.31, wps=12326.9, ups=0.87, wpb=14176.4, bsz=431.3, num_updates=39000, lr=0.000179029, gnorm=5.649, clip=4, loss_scale=158, train_wall=102, wall=41562
2022-10-12 18:34:11 | INFO | train_inner | epoch 086:    141 / 459 loss=6.631, nll_loss=3.293, mask_ins=1.138, word_ins_ml=4.877, word_reposition=0.616, ppl=99.12, wps=13912.8, ups=0.99, wpb=14122.1, bsz=425.5, num_updates=39100, lr=0.0001788, gnorm=3.639, clip=1, loss_scale=128, train_wall=101, wall=41663
2022-10-12 18:35:54 | INFO | train_inner | epoch 086:    241 / 459 loss=6.629, nll_loss=3.305, mask_ins=1.127, word_ins_ml=4.885, word_reposition=0.617, ppl=98.99, wps=14746.5, ups=0.97, wpb=15258, bsz=465.4, num_updates=39200, lr=0.000178571, gnorm=4.877, clip=2, loss_scale=128, train_wall=103, wall=41767
2022-10-12 18:37:37 | INFO | train_inner | epoch 086:    341 / 459 loss=6.644, nll_loss=3.313, mask_ins=1.135, word_ins_ml=4.893, word_reposition=0.616, ppl=99.98, wps=14296.1, ups=0.98, wpb=14639.8, bsz=444.5, num_updates=39300, lr=0.000178344, gnorm=3.445, clip=0, loss_scale=128, train_wall=101, wall=41869
2022-10-12 18:39:20 | INFO | train_inner | epoch 086:    441 / 459 loss=6.682, nll_loss=3.347, mask_ins=1.135, word_ins_ml=4.923, word_reposition=0.625, ppl=102.71, wps=14317.2, ups=0.97, wpb=14767.5, bsz=448.1, num_updates=39400, lr=0.000178118, gnorm=3.388, clip=2, loss_scale=128, train_wall=102, wall=41972
2022-10-12 18:39:38 | INFO | train | epoch 086 | loss 6.652 | nll_loss 3.318 | mask_ins 1.135 | word_ins_ml 4.897 | word_reposition 0.62 | ppl 100.54 | wps 13922.6 | ups 0.95 | wpb 14636.5 | bsz 443.9 | num_updates 39418 | lr 0.000178077 | gnorm 4.375 | clip 1.7 | loss_scale 128 | train_wall 466 | wall 41990
2022-10-12 18:39:47 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 14.398 | nll_loss 10.533 | mask_ins 1.837 | word_ins_ml 11.416 | word_reposition 1.146 | ppl 21593.7 | wps 34979.4 | wpb 1628.7 | bsz 55.8 | num_updates 39418 | best_loss 13.037
2022-10-12 18:39:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 86 @ 39418 updates, score 14.398) (writing took 2.9299345769977663 seconds)
2022-10-12 18:41:14 | INFO | train_inner | epoch 087:     82 / 459 loss=6.661, nll_loss=3.316, mask_ins=1.145, word_ins_ml=4.896, word_reposition=0.62, ppl=101.21, wps=12740.8, ups=0.88, wpb=14521, bsz=442.7, num_updates=39500, lr=0.000177892, gnorm=4.196, clip=1, loss_scale=211, train_wall=101, wall=42086
2022-10-12 18:42:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 18:42:57 | INFO | train_inner | epoch 087:    183 / 459 loss=6.623, nll_loss=3.285, mask_ins=1.139, word_ins_ml=4.869, word_reposition=0.616, ppl=98.58, wps=13977, ups=0.97, wpb=14390.1, bsz=438.5, num_updates=39600, lr=0.000177667, gnorm=3.443, clip=1, loss_scale=229, train_wall=102, wall=42189
2022-10-12 18:44:40 | INFO | train_inner | epoch 087:    283 / 459 loss=6.651, nll_loss=3.32, mask_ins=1.132, word_ins_ml=4.899, word_reposition=0.62, ppl=100.51, wps=14306, ups=0.97, wpb=14738.2, bsz=442.9, num_updates=39700, lr=0.000177443, gnorm=3.181, clip=0, loss_scale=128, train_wall=102, wall=42292
2022-10-12 18:46:22 | INFO | train_inner | epoch 087:    383 / 459 loss=6.643, nll_loss=3.306, mask_ins=1.14, word_ins_ml=4.887, word_reposition=0.616, ppl=99.94, wps=14421.4, ups=0.98, wpb=14785.8, bsz=448.9, num_updates=39800, lr=0.00017722, gnorm=2.9, clip=1, loss_scale=128, train_wall=102, wall=42395
2022-10-12 18:47:40 | INFO | train | epoch 087 | loss 6.646 | nll_loss 3.309 | mask_ins 1.138 | word_ins_ml 4.89 | word_reposition 0.618 | ppl 100.14 | wps 13909.5 | ups 0.95 | wpb 14643.2 | bsz 444.1 | num_updates 39876 | lr 0.000177051 | gnorm 2.957 | clip 0.4 | loss_scale 168 | train_wall 466 | wall 42473
2022-10-12 18:47:50 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 14.389 | nll_loss 10.446 | mask_ins 1.802 | word_ins_ml 11.337 | word_reposition 1.25 | ppl 21454.6 | wps 35009.4 | wpb 1628.7 | bsz 55.8 | num_updates 39876 | best_loss 13.037
2022-10-12 18:47:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 87 @ 39876 updates, score 14.389) (writing took 2.8825067679863423 seconds)
2022-10-12 18:48:17 | INFO | train_inner | epoch 088:     24 / 459 loss=6.659, nll_loss=3.322, mask_ins=1.135, word_ins_ml=4.901, word_reposition=0.623, ppl=101.03, wps=12790.6, ups=0.87, wpb=14685.3, bsz=446.2, num_updates=39900, lr=0.000176998, gnorm=2.274, clip=0, loss_scale=128, train_wall=102, wall=42510
2022-10-12 18:50:00 | INFO | train_inner | epoch 088:    124 / 459 loss=6.603, nll_loss=3.275, mask_ins=1.13, word_ins_ml=4.86, word_reposition=0.614, ppl=97.24, wps=14386.4, ups=0.97, wpb=14835.3, bsz=448.6, num_updates=40000, lr=0.000176777, gnorm=2.347, clip=0, loss_scale=128, train_wall=102, wall=42613
2022-10-12 18:51:43 | INFO | train_inner | epoch 088:    224 / 459 loss=6.66, nll_loss=3.326, mask_ins=1.141, word_ins_ml=4.905, word_reposition=0.615, ppl=101.14, wps=14263.8, ups=0.97, wpb=14662.9, bsz=443.8, num_updates=40100, lr=0.000176556, gnorm=2.002, clip=0, loss_scale=140, train_wall=102, wall=42715
2022-10-12 18:53:25 | INFO | train_inner | epoch 088:    324 / 459 loss=6.651, nll_loss=3.304, mask_ins=1.141, word_ins_ml=4.884, word_reposition=0.626, ppl=100.52, wps=14354.8, ups=0.98, wpb=14674.4, bsz=447.8, num_updates=40200, lr=0.000176336, gnorm=2.288, clip=0, loss_scale=256, train_wall=101, wall=42818
2022-10-12 18:55:08 | INFO | train_inner | epoch 088:    424 / 459 loss=6.662, nll_loss=3.323, mask_ins=1.144, word_ins_ml=4.902, word_reposition=0.616, ppl=101.28, wps=14301.8, ups=0.98, wpb=14619.1, bsz=441.4, num_updates=40300, lr=0.000176117, gnorm=2.093, clip=0, loss_scale=256, train_wall=101, wall=42920
2022-10-12 18:55:43 | INFO | train | epoch 088 | loss 6.64 | nll_loss 3.303 | mask_ins 1.139 | word_ins_ml 4.884 | word_reposition 0.618 | ppl 99.75 | wps 13916.5 | ups 0.95 | wpb 14634 | bsz 443.8 | num_updates 40335 | lr 0.000176041 | gnorm 2.207 | clip 0 | loss_scale 196 | train_wall 466 | wall 42955
2022-10-12 18:55:52 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 14.377 | nll_loss 10.525 | mask_ins 1.815 | word_ins_ml 11.411 | word_reposition 1.15 | ppl 21272.5 | wps 34994.8 | wpb 1628.7 | bsz 55.8 | num_updates 40335 | best_loss 13.037
2022-10-12 18:55:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 88 @ 40335 updates, score 14.377) (writing took 3.0024255799944513 seconds)
2022-10-12 18:57:03 | INFO | train_inner | epoch 089:     65 / 459 loss=6.625, nll_loss=3.286, mask_ins=1.14, word_ins_ml=4.869, word_reposition=0.617, ppl=98.71, wps=12718.4, ups=0.86, wpb=14725.9, bsz=448.6, num_updates=40400, lr=0.000175899, gnorm=2.371, clip=0, loss_scale=256, train_wall=103, wall=43036
2022-10-12 18:58:46 | INFO | train_inner | epoch 089:    165 / 459 loss=6.581, nll_loss=3.255, mask_ins=1.131, word_ins_ml=4.842, word_reposition=0.608, ppl=95.72, wps=14156, ups=0.97, wpb=14567.2, bsz=446, num_updates=40500, lr=0.000175682, gnorm=1.644, clip=0, loss_scale=256, train_wall=102, wall=43139
2022-10-12 19:00:29 | INFO | train_inner | epoch 089:    265 / 459 loss=6.604, nll_loss=3.277, mask_ins=1.126, word_ins_ml=4.861, word_reposition=0.617, ppl=97.28, wps=14299.5, ups=0.97, wpb=14716.4, bsz=446.3, num_updates=40600, lr=0.000175466, gnorm=1.711, clip=0, loss_scale=256, train_wall=102, wall=43241
2022-10-12 19:02:12 | INFO | train_inner | epoch 089:    365 / 459 loss=6.625, nll_loss=3.285, mask_ins=1.132, word_ins_ml=4.868, word_reposition=0.625, ppl=98.7, wps=14085.9, ups=0.97, wpb=14496.2, bsz=436.2, num_updates=40700, lr=0.00017525, gnorm=1.939, clip=1, loss_scale=504, train_wall=102, wall=43344
2022-10-12 19:03:48 | INFO | train | epoch 089 | loss 6.612 | nll_loss 3.28 | mask_ins 1.132 | word_ins_ml 4.864 | word_reposition 0.617 | ppl 97.83 | wps 13851.2 | ups 0.95 | wpb 14643.9 | bsz 444.1 | num_updates 40794 | lr 0.000175048 | gnorm 1.808 | clip 0.2 | loss_scale 363 | train_wall 469 | wall 43441
2022-10-12 19:03:58 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 14.352 | nll_loss 10.476 | mask_ins 1.798 | word_ins_ml 11.369 | word_reposition 1.186 | ppl 20917.7 | wps 34785.2 | wpb 1628.7 | bsz 55.8 | num_updates 40794 | best_loss 13.037
2022-10-12 19:04:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 89 @ 40794 updates, score 14.352) (writing took 2.8570779849833343 seconds)
2022-10-12 19:04:06 | INFO | train_inner | epoch 090:      6 / 459 loss=6.636, nll_loss=3.303, mask_ins=1.133, word_ins_ml=4.883, word_reposition=0.62, ppl=99.47, wps=12692.7, ups=0.87, wpb=14511.5, bsz=436.8, num_updates=40800, lr=0.000175035, gnorm=1.646, clip=0, loss_scale=512, train_wall=101, wall=43459
2022-10-12 19:05:49 | INFO | train_inner | epoch 090:    106 / 459 loss=6.581, nll_loss=3.25, mask_ins=1.13, word_ins_ml=4.838, word_reposition=0.613, ppl=95.74, wps=14225.1, ups=0.97, wpb=14638.1, bsz=443.8, num_updates=40900, lr=0.000174821, gnorm=1.917, clip=0, loss_scale=512, train_wall=102, wall=43562
2022-10-12 19:07:33 | INFO | train_inner | epoch 090:    206 / 459 loss=6.616, nll_loss=3.282, mask_ins=1.135, word_ins_ml=4.865, word_reposition=0.616, ppl=98.07, wps=14538.6, ups=0.97, wpb=15065.1, bsz=459, num_updates=41000, lr=0.000174608, gnorm=1.728, clip=0, loss_scale=512, train_wall=103, wall=43665
2022-10-12 19:09:15 | INFO | train_inner | epoch 090:    306 / 459 loss=6.6, nll_loss=3.269, mask_ins=1.128, word_ins_ml=4.855, word_reposition=0.618, ppl=97.04, wps=14042.7, ups=0.98, wpb=14376.6, bsz=435.2, num_updates=41100, lr=0.000174395, gnorm=1.928, clip=0, loss_scale=512, train_wall=101, wall=43768
2022-10-12 19:10:57 | INFO | train_inner | epoch 090:    406 / 459 loss=6.634, nll_loss=3.297, mask_ins=1.134, word_ins_ml=4.879, word_reposition=0.621, ppl=99.32, wps=14207.7, ups=0.98, wpb=14452.2, bsz=438.2, num_updates=41200, lr=0.000174183, gnorm=1.675, clip=0, loss_scale=947, train_wall=101, wall=43869
2022-10-12 19:11:51 | INFO | train | epoch 090 | loss 6.613 | nll_loss 3.278 | mask_ins 1.132 | word_ins_ml 4.862 | word_reposition 0.618 | ppl 97.88 | wps 13911.6 | ups 0.95 | wpb 14637.4 | bsz 443.9 | num_updates 41253 | lr 0.000174071 | gnorm 1.8 | clip 0 | loss_scale 666 | train_wall 466 | wall 43923
2022-10-12 19:12:00 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 14.4 | nll_loss 10.549 | mask_ins 1.842 | word_ins_ml 11.432 | word_reposition 1.127 | ppl 21616.2 | wps 35019.3 | wpb 1628.7 | bsz 55.8 | num_updates 41253 | best_loss 13.037
2022-10-12 19:12:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 90 @ 41253 updates, score 14.4) (writing took 2.885035996005172 seconds)
2022-10-12 19:12:51 | INFO | train_inner | epoch 091:     47 / 459 loss=6.605, nll_loss=3.267, mask_ins=1.133, word_ins_ml=4.853, word_reposition=0.62, ppl=97.36, wps=12645, ups=0.88, wpb=14357.4, bsz=434.2, num_updates=41300, lr=0.000173972, gnorm=1.732, clip=0, loss_scale=1024, train_wall=100, wall=43983
2022-10-12 19:14:33 | INFO | train_inner | epoch 091:    147 / 459 loss=6.559, nll_loss=3.224, mask_ins=1.127, word_ins_ml=4.814, word_reposition=0.618, ppl=94.29, wps=14308.7, ups=0.98, wpb=14636.2, bsz=447, num_updates=41400, lr=0.000173762, gnorm=1.689, clip=0, loss_scale=1024, train_wall=101, wall=44085
2022-10-12 19:16:16 | INFO | train_inner | epoch 091:    247 / 459 loss=6.582, nll_loss=3.239, mask_ins=1.13, word_ins_ml=4.828, word_reposition=0.623, ppl=95.77, wps=14141.4, ups=0.97, wpb=14531.1, bsz=438.4, num_updates=41500, lr=0.000173553, gnorm=1.619, clip=0, loss_scale=1024, train_wall=102, wall=44188
2022-10-12 19:17:58 | INFO | train_inner | epoch 091:    347 / 459 loss=6.603, nll_loss=3.272, mask_ins=1.136, word_ins_ml=4.856, word_reposition=0.61, ppl=97.2, wps=14365.5, ups=0.98, wpb=14680.6, bsz=445, num_updates=41600, lr=0.000173344, gnorm=1.658, clip=0, loss_scale=1024, train_wall=101, wall=44290
2022-10-12 19:19:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-10-12 19:19:43 | INFO | train_inner | epoch 091:    448 / 459 loss=6.636, nll_loss=3.305, mask_ins=1.13, word_ins_ml=4.885, word_reposition=0.62, ppl=99.43, wps=14370, ups=0.95, wpb=15073.2, bsz=457, num_updates=41700, lr=0.000173136, gnorm=1.67, clip=0, loss_scale=1571, train_wall=104, wall=44395
2022-10-12 19:19:54 | INFO | train | epoch 091 | loss 6.59 | nll_loss 3.257 | mask_ins 1.13 | word_ins_ml 4.843 | word_reposition 0.617 | ppl 96.33 | wps 13886.7 | ups 0.95 | wpb 14639.9 | bsz 444 | num_updates 41711 | lr 0.000173113 | gnorm 1.678 | clip 0 | loss_scale 1144 | train_wall 467 | wall 44406
2022-10-12 19:20:03 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 14.507 | nll_loss 10.629 | mask_ins 1.846 | word_ins_ml 11.505 | word_reposition 1.157 | ppl 23285.3 | wps 35052.2 | wpb 1628.7 | bsz 55.8 | num_updates 41711 | best_loss 13.037
2022-10-12 19:20:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 91 @ 41711 updates, score 14.507) (writing took 3.107166954985587 seconds)
2022-10-12 19:21:38 | INFO | train_inner | epoch 092:     89 / 459 loss=6.565, nll_loss=3.232, mask_ins=1.132, word_ins_ml=4.822, word_reposition=0.61, ppl=94.67, wps=12588.8, ups=0.87, wpb=14506.8, bsz=438.6, num_updates=41800, lr=0.000172929, gnorm=2.039, clip=0, loss_scale=1024, train_wall=102, wall=44510
2022-10-12 19:23:22 | INFO | train_inner | epoch 092:    189 / 459 loss=6.568, nll_loss=3.258, mask_ins=1.115, word_ins_ml=4.845, word_reposition=0.608, ppl=94.87, wps=14450, ups=0.96, wpb=14993, bsz=456.1, num_updates=41900, lr=0.000172722, gnorm=1.989, clip=0, loss_scale=1024, train_wall=103, wall=44614
2022-10-12 19:24:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-10-12 19:25:05 | INFO | train_inner | epoch 092:    290 / 459 loss=6.541, nll_loss=3.224, mask_ins=1.127, word_ins_ml=4.814, word_reposition=0.6, ppl=93.11, wps=14053.5, ups=0.97, wpb=14518.1, bsz=442.7, num_updates=42000, lr=0.000172516, gnorm=2.069, clip=0, loss_scale=882, train_wall=102, wall=44717
2022-10-12 19:26:48 | INFO | train_inner | epoch 092:    390 / 459 loss=6.575, nll_loss=3.244, mask_ins=1.133, word_ins_ml=4.832, word_reposition=0.61, ppl=95.36, wps=14324.3, ups=0.97, wpb=14739.4, bsz=447.7, num_updates=42100, lr=0.000172311, gnorm=1.909, clip=0, loss_scale=512, train_wall=102, wall=44820
2022-10-12 19:27:58 | INFO | train | epoch 092 | loss 6.569 | nll_loss 3.243 | mask_ins 1.128 | word_ins_ml 4.831 | word_reposition 0.609 | ppl 94.92 | wps 13844.5 | ups 0.95 | wpb 14644.1 | bsz 444.1 | num_updates 42169 | lr 0.00017217 | gnorm 2.047 | clip 0 | loss_scale 804 | train_wall 468 | wall 44891
2022-10-12 19:28:08 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 14.375 | nll_loss 10.575 | mask_ins 1.815 | word_ins_ml 11.453 | word_reposition 1.107 | ppl 21245.7 | wps 35024.8 | wpb 1628.7 | bsz 55.8 | num_updates 42169 | best_loss 13.037
2022-10-12 19:28:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 92 @ 42169 updates, score 14.375) (writing took 2.9542849519930314 seconds)
2022-10-12 19:28:42 | INFO | train_inner | epoch 093:     31 / 459 loss=6.586, nll_loss=3.248, mask_ins=1.132, word_ins_ml=4.835, word_reposition=0.619, ppl=96.07, wps=12409, ups=0.87, wpb=14213.9, bsz=427.8, num_updates=42200, lr=0.000172107, gnorm=2.535, clip=0, loss_scale=512, train_wall=101, wall=44935
2022-10-12 19:30:25 | INFO | train_inner | epoch 093:    131 / 459 loss=6.546, nll_loss=3.224, mask_ins=1.121, word_ins_ml=4.815, word_reposition=0.611, ppl=93.43, wps=14149, ups=0.97, wpb=14549.3, bsz=441.8, num_updates=42300, lr=0.000171904, gnorm=1.992, clip=0, loss_scale=512, train_wall=102, wall=45038
2022-10-12 19:32:09 | INFO | train_inner | epoch 093:    231 / 459 loss=6.624, nll_loss=3.269, mask_ins=1.141, word_ins_ml=4.854, word_reposition=0.629, ppl=98.62, wps=14396.9, ups=0.97, wpb=14857.5, bsz=449, num_updates=42400, lr=0.000171701, gnorm=2.047, clip=0, loss_scale=512, train_wall=102, wall=45141
2022-10-12 19:33:52 | INFO | train_inner | epoch 093:    331 / 459 loss=6.567, nll_loss=3.241, mask_ins=1.131, word_ins_ml=4.829, word_reposition=0.607, ppl=94.79, wps=14443.9, ups=0.97, wpb=14901.9, bsz=454, num_updates=42500, lr=0.000171499, gnorm=2.212, clip=0, loss_scale=594, train_wall=102, wall=45244
2022-10-12 19:35:35 | INFO | train_inner | epoch 093:    431 / 459 loss=6.585, nll_loss=3.255, mask_ins=1.13, word_ins_ml=4.842, word_reposition=0.614, ppl=96, wps=14232.4, ups=0.97, wpb=14662.1, bsz=445.3, num_updates=42600, lr=0.000171297, gnorm=2.417, clip=0, loss_scale=1024, train_wall=102, wall=45347
2022-10-12 19:36:03 | INFO | train | epoch 093 | loss 6.576 | nll_loss 3.243 | mask_ins 1.13 | word_ins_ml 4.831 | word_reposition 0.615 | ppl 95.43 | wps 13867.2 | ups 0.95 | wpb 14641.1 | bsz 444 | num_updates 42628 | lr 0.000171241 | gnorm 2.234 | clip 0 | loss_scale 673 | train_wall 468 | wall 45375
2022-10-12 19:36:12 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 14.509 | nll_loss 10.609 | mask_ins 1.841 | word_ins_ml 11.486 | word_reposition 1.182 | ppl 23314.4 | wps 35063.7 | wpb 1628.7 | bsz 55.8 | num_updates 42628 | best_loss 13.037
2022-10-12 19:36:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 93 @ 42628 updates, score 14.509) (writing took 2.8579283010039944 seconds)
2022-10-12 19:37:30 | INFO | train_inner | epoch 094:     72 / 459 loss=6.549, nll_loss=3.216, mask_ins=1.13, word_ins_ml=4.808, word_reposition=0.611, ppl=93.66, wps=12530.1, ups=0.87, wpb=14419.2, bsz=436.3, num_updates=42700, lr=0.000171096, gnorm=2.194, clip=0, loss_scale=1024, train_wall=102, wall=45462
2022-10-12 19:38:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-10-12 19:39:14 | INFO | train_inner | epoch 094:    173 / 459 loss=6.555, nll_loss=3.218, mask_ins=1.133, word_ins_ml=4.809, word_reposition=0.613, ppl=94.01, wps=14226.1, ups=0.96, wpb=14782.7, bsz=449.9, num_updates=42800, lr=0.000170896, gnorm=1.929, clip=0, loss_scale=735, train_wall=103, wall=45566
2022-10-12 19:40:57 | INFO | train_inner | epoch 094:    273 / 459 loss=6.563, nll_loss=3.231, mask_ins=1.123, word_ins_ml=4.821, word_reposition=0.619, ppl=94.53, wps=14425.5, ups=0.97, wpb=14872.3, bsz=449.6, num_updates=42900, lr=0.000170697, gnorm=2.011, clip=0, loss_scale=512, train_wall=102, wall=45669
2022-10-12 19:42:38 | INFO | train_inner | epoch 094:    373 / 459 loss=6.56, nll_loss=3.226, mask_ins=1.13, word_ins_ml=4.816, word_reposition=0.614, ppl=94.35, wps=13967, ups=0.99, wpb=14171.4, bsz=427.1, num_updates=43000, lr=0.000170499, gnorm=2.098, clip=0, loss_scale=512, train_wall=101, wall=45771
2022-10-12 19:44:07 | INFO | train | epoch 094 | loss 6.553 | nll_loss 3.223 | mask_ins 1.127 | word_ins_ml 4.814 | word_reposition 0.612 | ppl 93.93 | wps 13858.7 | ups 0.95 | wpb 14637.5 | bsz 443.9 | num_updates 43086 | lr 0.000170328 | gnorm 2.108 | clip 0 | loss_scale 641 | train_wall 467 | wall 45859
2022-10-12 19:44:16 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 14.423 | nll_loss 10.596 | mask_ins 1.82 | word_ins_ml 11.472 | word_reposition 1.131 | ppl 21962.3 | wps 35064.4 | wpb 1628.7 | bsz 55.8 | num_updates 43086 | best_loss 13.037
2022-10-12 19:44:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 94 @ 43086 updates, score 14.423) (writing took 3.137648084986722 seconds)
2022-10-12 19:44:34 | INFO | train_inner | epoch 095:     14 / 459 loss=6.523, nll_loss=3.219, mask_ins=1.113, word_ins_ml=4.809, word_reposition=0.601, ppl=91.98, wps=12830.9, ups=0.87, wpb=14825.3, bsz=451.6, num_updates=43100, lr=0.000170301, gnorm=2.507, clip=0, loss_scale=512, train_wall=102, wall=45886
2022-10-12 19:46:16 | INFO | train_inner | epoch 095:    114 / 459 loss=6.476, nll_loss=3.156, mask_ins=1.121, word_ins_ml=4.755, word_reposition=0.6, ppl=89, wps=14210.2, ups=0.98, wpb=14540, bsz=444.1, num_updates=43200, lr=0.000170103, gnorm=2.058, clip=0, loss_scale=512, train_wall=101, wall=45988
2022-10-12 19:48:00 | INFO | train_inner | epoch 095:    214 / 459 loss=6.519, nll_loss=3.19, mask_ins=1.122, word_ins_ml=4.785, word_reposition=0.611, ppl=91.69, wps=14026.1, ups=0.96, wpb=14534.9, bsz=441, num_updates=43300, lr=0.000169907, gnorm=2.258, clip=0, loss_scale=742, train_wall=103, wall=46092
2022-10-12 19:48:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-10-12 19:49:44 | INFO | train_inner | epoch 095:    315 / 459 loss=6.555, nll_loss=3.219, mask_ins=1.128, word_ins_ml=4.81, word_reposition=0.616, ppl=94.01, wps=14206, ups=0.96, wpb=14834.7, bsz=448.4, num_updates=43400, lr=0.000169711, gnorm=2.926, clip=1, loss_scale=573, train_wall=104, wall=46196
2022-10-12 19:51:27 | INFO | train_inner | epoch 095:    415 / 459 loss=6.559, nll_loss=3.234, mask_ins=1.118, word_ins_ml=4.823, word_reposition=0.619, ppl=94.3, wps=14379.7, ups=0.97, wpb=14790.4, bsz=447.9, num_updates=43500, lr=0.000169516, gnorm=2.96, clip=2, loss_scale=512, train_wall=102, wall=46299
2022-10-12 19:52:12 | INFO | train | epoch 095 | loss 6.527 | nll_loss 3.201 | mask_ins 1.121 | word_ins_ml 4.795 | word_reposition 0.611 | ppl 92.21 | wps 13826.8 | ups 0.94 | wpb 14639.4 | bsz 444.1 | num_updates 43544 | lr 0.00016943 | gnorm 2.544 | clip 0.7 | loss_scale 576 | train_wall 468 | wall 46344
2022-10-12 19:52:21 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 14.486 | nll_loss 10.575 | mask_ins 1.836 | word_ins_ml 11.453 | word_reposition 1.197 | ppl 22946.1 | wps 35022.1 | wpb 1628.7 | bsz 55.8 | num_updates 43544 | best_loss 13.037
2022-10-12 19:52:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 95 @ 43544 updates, score 14.486) (writing took 2.977420967014041 seconds)
2022-10-12 19:53:22 | INFO | train_inner | epoch 096:     56 / 459 loss=6.506, nll_loss=3.186, mask_ins=1.116, word_ins_ml=4.781, word_reposition=0.609, ppl=90.9, wps=12539.3, ups=0.87, wpb=14460.4, bsz=439.9, num_updates=43600, lr=0.000169321, gnorm=2.123, clip=0, loss_scale=512, train_wall=102, wall=46415
2022-10-12 19:55:06 | INFO | train_inner | epoch 096:    156 / 459 loss=6.514, nll_loss=3.18, mask_ins=1.136, word_ins_ml=4.776, word_reposition=0.603, ppl=91.42, wps=14027.7, ups=0.97, wpb=14506.2, bsz=437.9, num_updates=43700, lr=0.000169128, gnorm=2.756, clip=1, loss_scale=512, train_wall=102, wall=46518
2022-10-12 19:56:49 | INFO | train_inner | epoch 096:    256 / 459 loss=6.5, nll_loss=3.169, mask_ins=1.121, word_ins_ml=4.767, word_reposition=0.612, ppl=90.54, wps=14134.4, ups=0.97, wpb=14574.3, bsz=440.5, num_updates=43800, lr=0.000168934, gnorm=2.543, clip=0, loss_scale=512, train_wall=102, wall=46621
2022-10-12 19:58:32 | INFO | train_inner | epoch 096:    356 / 459 loss=6.531, nll_loss=3.209, mask_ins=1.117, word_ins_ml=4.801, word_reposition=0.614, ppl=92.48, wps=14333.8, ups=0.97, wpb=14769.3, bsz=450.2, num_updates=43900, lr=0.000168742, gnorm=2.261, clip=0, loss_scale=906, train_wall=102, wall=46724
2022-10-12 20:00:15 | INFO | train_inner | epoch 096:    456 / 459 loss=6.547, nll_loss=3.224, mask_ins=1.123, word_ins_ml=4.814, word_reposition=0.61, ppl=93.51, wps=14271, ups=0.97, wpb=14773.6, bsz=447.8, num_updates=44000, lr=0.00016855, gnorm=3.181, clip=0, loss_scale=1024, train_wall=103, wall=46828
2022-10-12 20:00:18 | INFO | train | epoch 096 | loss 6.517 | nll_loss 3.191 | mask_ins 1.122 | word_ins_ml 4.786 | word_reposition 0.609 | ppl 91.59 | wps 13815.2 | ups 0.94 | wpb 14642.8 | bsz 444.1 | num_updates 44003 | lr 0.000168544 | gnorm 2.668 | clip 0.4 | loss_scale 713 | train_wall 470 | wall 46831
2022-10-12 20:00:27 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 14.553 | nll_loss 10.579 | mask_ins 1.846 | word_ins_ml 11.459 | word_reposition 1.249 | ppl 24044.8 | wps 35017.7 | wpb 1628.7 | bsz 55.8 | num_updates 44003 | best_loss 13.037
2022-10-12 20:00:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 96 @ 44003 updates, score 14.553) (writing took 2.986375358013902 seconds)
2022-10-12 20:01:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-10-12 20:02:11 | INFO | train_inner | epoch 097:     98 / 459 loss=6.471, nll_loss=3.152, mask_ins=1.117, word_ins_ml=4.751, word_reposition=0.603, ppl=88.73, wps=12649.3, ups=0.86, wpb=14639.8, bsz=441.3, num_updates=44100, lr=0.000168359, gnorm=3.193, clip=2, loss_scale=933, train_wall=103, wall=46943
2022-10-12 20:03:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 20:03:55 | INFO | train_inner | epoch 097:    199 / 459 loss=6.496, nll_loss=3.181, mask_ins=1.116, word_ins_ml=4.777, word_reposition=0.603, ppl=90.25, wps=14194.7, ups=0.96, wpb=14785.8, bsz=448.3, num_updates=44200, lr=0.000168168, gnorm=4.013, clip=1, loss_scale=497, train_wall=103, wall=47048
2022-10-12 20:05:39 | INFO | train_inner | epoch 097:    299 / 459 loss=6.51, nll_loss=3.188, mask_ins=1.118, word_ins_ml=4.783, word_reposition=0.609, ppl=91.13, wps=14379.4, ups=0.97, wpb=14887.7, bsz=453.6, num_updates=44300, lr=0.000167978, gnorm=3.365, clip=0, loss_scale=256, train_wall=103, wall=47151
2022-10-12 20:06:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 20:07:22 | INFO | train_inner | epoch 097:    400 / 459 loss=6.539, nll_loss=3.208, mask_ins=1.129, word_ins_ml=4.8, word_reposition=0.61, ppl=93, wps=14028.4, ups=0.97, wpb=14494.8, bsz=440.6, num_updates=44400, lr=0.000167789, gnorm=4.895, clip=2, loss_scale=227, train_wall=102, wall=47255
2022-10-12 20:08:22 | INFO | train | epoch 097 | loss 6.507 | nll_loss 3.184 | mask_ins 1.121 | word_ins_ml 4.779 | word_reposition 0.607 | ppl 90.96 | wps 13809.5 | ups 0.94 | wpb 14649 | bsz 444.2 | num_updates 44459 | lr 0.000167678 | gnorm 4.37 | clip 1.5 | loss_scale 430 | train_wall 467 | wall 47314
2022-10-12 20:08:31 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 14.439 | nll_loss 10.572 | mask_ins 1.795 | word_ins_ml 11.449 | word_reposition 1.194 | ppl 22206 | wps 34863.9 | wpb 1628.7 | bsz 55.8 | num_updates 44459 | best_loss 13.037
2022-10-12 20:08:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 97 @ 44459 updates, score 14.439) (writing took 2.8518404490023386 seconds)
2022-10-12 20:09:16 | INFO | train_inner | epoch 098:     41 / 459 loss=6.52, nll_loss=3.191, mask_ins=1.124, word_ins_ml=4.785, word_reposition=0.61, ppl=91.76, wps=12706.1, ups=0.88, wpb=14499.5, bsz=439.3, num_updates=44500, lr=0.0001676, gnorm=6.746, clip=3, loss_scale=128, train_wall=101, wall=47369
2022-10-12 20:10:58 | INFO | train_inner | epoch 098:    141 / 459 loss=6.478, nll_loss=3.146, mask_ins=1.122, word_ins_ml=4.747, word_reposition=0.61, ppl=89.14, wps=14234.1, ups=0.98, wpb=14532.3, bsz=438.8, num_updates=44600, lr=0.000167412, gnorm=5.183, clip=1, loss_scale=128, train_wall=101, wall=47471
2022-10-12 20:12:41 | INFO | train_inner | epoch 098:    241 / 459 loss=6.502, nll_loss=3.173, mask_ins=1.119, word_ins_ml=4.77, word_reposition=0.613, ppl=90.65, wps=14217.1, ups=0.98, wpb=14523.7, bsz=442, num_updates=44700, lr=0.000167225, gnorm=4.002, clip=0, loss_scale=128, train_wall=101, wall=47573
2022-10-12 20:14:23 | INFO | train_inner | epoch 098:    341 / 459 loss=6.519, nll_loss=3.195, mask_ins=1.119, word_ins_ml=4.789, word_reposition=0.611, ppl=91.69, wps=14380.5, ups=0.97, wpb=14781, bsz=447.4, num_updates=44800, lr=0.000167038, gnorm=4.4, clip=1, loss_scale=128, train_wall=102, wall=47676
2022-10-12 20:16:06 | INFO | train_inner | epoch 098:    441 / 459 loss=6.472, nll_loss=3.172, mask_ins=1.111, word_ins_ml=4.768, word_reposition=0.594, ppl=88.8, wps=14107, ups=0.97, wpb=14501.5, bsz=441.7, num_updates=44900, lr=0.000166852, gnorm=4.405, clip=1, loss_scale=142, train_wall=102, wall=47778
2022-10-12 20:16:24 | INFO | train | epoch 098 | loss 6.5 | nll_loss 3.177 | mask_ins 1.12 | word_ins_ml 4.773 | word_reposition 0.607 | ppl 90.49 | wps 13920.5 | ups 0.95 | wpb 14634.2 | bsz 443.8 | num_updates 44918 | lr 0.000166819 | gnorm 4.547 | clip 0.7 | loss_scale 136 | train_wall 466 | wall 47797
2022-10-12 20:16:34 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 14.57 | nll_loss 10.655 | mask_ins 1.805 | word_ins_ml 11.528 | word_reposition 1.236 | ppl 24319.5 | wps 35046.8 | wpb 1628.7 | bsz 55.8 | num_updates 44918 | best_loss 13.037
2022-10-12 20:16:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 98 @ 44918 updates, score 14.57) (writing took 3.023600887012435 seconds)
2022-10-12 20:18:01 | INFO | train_inner | epoch 099:     82 / 459 loss=6.538, nll_loss=3.198, mask_ins=1.133, word_ins_ml=4.792, word_reposition=0.613, ppl=92.93, wps=12860, ups=0.87, wpb=14750.4, bsz=446.9, num_updates=45000, lr=0.000166667, gnorm=4.12, clip=0, loss_scale=256, train_wall=102, wall=47893
2022-10-12 20:19:44 | INFO | train_inner | epoch 099:    182 / 459 loss=6.498, nll_loss=3.168, mask_ins=1.12, word_ins_ml=4.766, word_reposition=0.612, ppl=90.39, wps=14157.5, ups=0.97, wpb=14536.4, bsz=441.6, num_updates=45100, lr=0.000166482, gnorm=3.209, clip=0, loss_scale=256, train_wall=102, wall=47996
2022-10-12 20:21:26 | INFO | train_inner | epoch 099:    282 / 459 loss=6.507, nll_loss=3.175, mask_ins=1.125, word_ins_ml=4.771, word_reposition=0.611, ppl=90.94, wps=14520, ups=0.98, wpb=14868.5, bsz=451, num_updates=45200, lr=0.000166298, gnorm=2.858, clip=0, loss_scale=256, train_wall=101, wall=48098
2022-10-12 20:22:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 20:23:10 | INFO | train_inner | epoch 099:    383 / 459 loss=6.472, nll_loss=3.155, mask_ins=1.114, word_ins_ml=4.754, word_reposition=0.604, ppl=88.77, wps=13971.2, ups=0.96, wpb=14484.9, bsz=439.5, num_updates=45300, lr=0.000166114, gnorm=2.822, clip=0, loss_scale=226, train_wall=103, wall=48202
2022-10-12 20:24:28 | INFO | train | epoch 099 | loss 6.504 | nll_loss 3.175 | mask_ins 1.123 | word_ins_ml 4.771 | word_reposition 0.609 | ppl 90.75 | wps 13861.1 | ups 0.95 | wpb 14632.5 | bsz 443.9 | num_updates 45376 | lr 0.000165975 | gnorm 3.077 | clip 0 | loss_scale 228 | train_wall 467 | wall 48280
2022-10-12 20:24:38 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 14.585 | nll_loss 10.649 | mask_ins 1.844 | word_ins_ml 11.523 | word_reposition 1.218 | ppl 24582.3 | wps 33917.7 | wpb 1628.7 | bsz 55.8 | num_updates 45376 | best_loss 13.037
2022-10-12 20:24:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 99 @ 45376 updates, score 14.585) (writing took 2.941696834983304 seconds)
2022-10-12 20:25:06 | INFO | train_inner | epoch 100:     24 / 459 loss=6.505, nll_loss=3.178, mask_ins=1.125, word_ins_ml=4.774, word_reposition=0.607, ppl=90.83, wps=12576.1, ups=0.86, wpb=14573.7, bsz=442.6, num_updates=45400, lr=0.000165931, gnorm=2.813, clip=0, loss_scale=128, train_wall=102, wall=48318
2022-10-12 20:26:49 | INFO | train_inner | epoch 100:    124 / 459 loss=6.428, nll_loss=3.113, mask_ins=1.111, word_ins_ml=4.717, word_reposition=0.599, ppl=86.1, wps=14177.7, ups=0.97, wpb=14624.5, bsz=443.6, num_updates=45500, lr=0.000165748, gnorm=3.382, clip=1, loss_scale=128, train_wall=102, wall=48421
2022-10-12 20:27:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 20:28:33 | INFO | train_inner | epoch 100:    225 / 459 loss=6.458, nll_loss=3.138, mask_ins=1.114, word_ins_ml=4.74, word_reposition=0.604, ppl=87.9, wps=14045.7, ups=0.96, wpb=14588.7, bsz=441.4, num_updates=45600, lr=0.000165567, gnorm=2.525, clip=1, loss_scale=105, train_wall=103, wall=48525
2022-10-12 20:30:15 | INFO | train_inner | epoch 100:    325 / 459 loss=6.465, nll_loss=3.161, mask_ins=1.112, word_ins_ml=4.758, word_reposition=0.594, ppl=88.31, wps=14211.6, ups=0.98, wpb=14569.5, bsz=442.2, num_updates=45700, lr=0.000165385, gnorm=2.44, clip=0, loss_scale=64, train_wall=102, wall=48627
2022-10-12 20:31:58 | INFO | train_inner | epoch 100:    425 / 459 loss=6.495, nll_loss=3.179, mask_ins=1.111, word_ins_ml=4.774, word_reposition=0.61, ppl=90.21, wps=14413.2, ups=0.97, wpb=14825.2, bsz=450, num_updates=45800, lr=0.000165205, gnorm=2.078, clip=0, loss_scale=64, train_wall=102, wall=48730
2022-10-12 20:32:33 | INFO | train | epoch 100 | loss 6.461 | nll_loss 3.149 | mask_ins 1.111 | word_ins_ml 4.748 | word_reposition 0.602 | ppl 88.1 | wps 13837.7 | ups 0.94 | wpb 14646.3 | bsz 444.2 | num_updates 45834 | lr 0.000165143 | gnorm 2.633 | clip 0.4 | loss_scale 90 | train_wall 468 | wall 48765
2022-10-12 20:32:42 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 14.683 | nll_loss 10.767 | mask_ins 1.857 | word_ins_ml 11.631 | word_reposition 1.196 | ppl 26309.9 | wps 34947.1 | wpb 1628.7 | bsz 55.8 | num_updates 45834 | best_loss 13.037
2022-10-12 20:32:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 100 @ 45834 updates, score 14.683) (writing took 2.835785417992156 seconds)
2022-10-12 20:33:52 | INFO | train_inner | epoch 101:     66 / 459 loss=6.476, nll_loss=3.151, mask_ins=1.123, word_ins_ml=4.75, word_reposition=0.603, ppl=89, wps=12588, ups=0.87, wpb=14419.4, bsz=437.2, num_updates=45900, lr=0.000165025, gnorm=2.906, clip=0, loss_scale=64, train_wall=102, wall=48845
2022-10-12 20:35:35 | INFO | train_inner | epoch 101:    166 / 459 loss=6.442, nll_loss=3.124, mask_ins=1.116, word_ins_ml=4.726, word_reposition=0.6, ppl=86.94, wps=14373.5, ups=0.97, wpb=14791.2, bsz=452.9, num_updates=46000, lr=0.000164845, gnorm=2.671, clip=0, loss_scale=64, train_wall=102, wall=48948
2022-10-12 20:37:19 | INFO | train_inner | epoch 101:    266 / 459 loss=6.508, nll_loss=3.175, mask_ins=1.121, word_ins_ml=4.771, word_reposition=0.615, ppl=90.98, wps=14426.3, ups=0.97, wpb=14906.1, bsz=449.6, num_updates=46100, lr=0.000164666, gnorm=2.401, clip=0, loss_scale=80, train_wall=102, wall=49051
2022-10-12 20:39:02 | INFO | train_inner | epoch 101:    366 / 459 loss=6.465, nll_loss=3.157, mask_ins=1.111, word_ins_ml=4.755, word_reposition=0.6, ppl=88.37, wps=14197.3, ups=0.97, wpb=14662.4, bsz=445.4, num_updates=46200, lr=0.000164488, gnorm=2.756, clip=0, loss_scale=128, train_wall=102, wall=49154
2022-10-12 20:40:37 | INFO | train | epoch 101 | loss 6.47 | nll_loss 3.149 | mask_ins 1.117 | word_ins_ml 4.748 | word_reposition 0.605 | ppl 88.67 | wps 13870 | ups 0.95 | wpb 14633.5 | bsz 443.8 | num_updates 46293 | lr 0.000164323 | gnorm 3.064 | clip 0.2 | loss_scale 94 | train_wall 468 | wall 49249
2022-10-12 20:40:46 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 14.588 | nll_loss 10.622 | mask_ins 1.886 | word_ins_ml 11.494 | word_reposition 1.209 | ppl 24633 | wps 34992.7 | wpb 1628.7 | bsz 55.8 | num_updates 46293 | best_loss 13.037
2022-10-12 20:40:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 101 @ 46293 updates, score 14.588) (writing took 2.816596986987861 seconds)
2022-10-12 20:40:56 | INFO | train_inner | epoch 102:      7 / 459 loss=6.468, nll_loss=3.152, mask_ins=1.109, word_ins_ml=4.751, word_reposition=0.608, ppl=88.54, wps=12462.2, ups=0.88, wpb=14221.8, bsz=427.3, num_updates=46300, lr=0.00016431, gnorm=4.401, clip=1, loss_scale=128, train_wall=101, wall=49268
2022-10-12 20:41:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 20:42:39 | INFO | train_inner | epoch 102:    108 / 459 loss=6.432, nll_loss=3.108, mask_ins=1.115, word_ins_ml=4.713, word_reposition=0.604, ppl=86.36, wps=14178.3, ups=0.97, wpb=14602.3, bsz=441.7, num_updates=46400, lr=0.000164133, gnorm=3.784, clip=1, loss_scale=91, train_wall=102, wall=49371
2022-10-12 20:44:22 | INFO | train_inner | epoch 102:    208 / 459 loss=6.439, nll_loss=3.129, mask_ins=1.109, word_ins_ml=4.731, word_reposition=0.599, ppl=86.76, wps=14219.5, ups=0.97, wpb=14687.7, bsz=444.5, num_updates=46500, lr=0.000163956, gnorm=2.89, clip=0, loss_scale=64, train_wall=102, wall=49475
2022-10-12 20:46:05 | INFO | train_inner | epoch 102:    308 / 459 loss=6.46, nll_loss=3.135, mask_ins=1.123, word_ins_ml=4.736, word_reposition=0.6, ppl=88.01, wps=14245.1, ups=0.97, wpb=14646.8, bsz=447.2, num_updates=46600, lr=0.00016378, gnorm=2.95, clip=0, loss_scale=64, train_wall=102, wall=49577
2022-10-12 20:47:49 | INFO | train_inner | epoch 102:    408 / 459 loss=6.477, nll_loss=3.173, mask_ins=1.111, word_ins_ml=4.769, word_reposition=0.597, ppl=89.09, wps=14503, ups=0.97, wpb=14998.2, bsz=455, num_updates=46700, lr=0.000163605, gnorm=3.495, clip=1, loss_scale=64, train_wall=102, wall=49681
2022-10-12 20:48:41 | INFO | train | epoch 102 | loss 6.452 | nll_loss 3.136 | mask_ins 1.115 | word_ins_ml 4.737 | word_reposition 0.6 | ppl 87.53 | wps 13858.6 | ups 0.95 | wpb 14636.9 | bsz 444 | num_updates 46751 | lr 0.000163516 | gnorm 3.375 | clip 0.7 | loss_scale 71 | train_wall 467 | wall 49733
2022-10-12 20:48:50 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 14.576 | nll_loss 10.739 | mask_ins 1.816 | word_ins_ml 11.608 | word_reposition 1.152 | ppl 24417 | wps 34962.5 | wpb 1628.7 | bsz 55.8 | num_updates 46751 | best_loss 13.037
2022-10-12 20:48:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 102 @ 46751 updates, score 14.576) (writing took 2.879606236005202 seconds)
2022-10-12 20:49:43 | INFO | train_inner | epoch 103:     49 / 459 loss=6.465, nll_loss=3.135, mask_ins=1.127, word_ins_ml=4.737, word_reposition=0.601, ppl=88.34, wps=12491.1, ups=0.87, wpb=14349.2, bsz=434.6, num_updates=46800, lr=0.00016343, gnorm=3.802, clip=1, loss_scale=64, train_wall=102, wall=49796
2022-10-12 20:51:27 | INFO | train_inner | epoch 103:    149 / 459 loss=6.43, nll_loss=3.116, mask_ins=1.111, word_ins_ml=4.72, word_reposition=0.6, ppl=86.25, wps=14206.1, ups=0.96, wpb=14747.3, bsz=448.1, num_updates=46900, lr=0.000163256, gnorm=3.409, clip=1, loss_scale=94, train_wall=103, wall=49900
2022-10-12 20:53:10 | INFO | train_inner | epoch 103:    249 / 459 loss=6.455, nll_loss=3.136, mask_ins=1.117, word_ins_ml=4.737, word_reposition=0.601, ppl=87.75, wps=14014.6, ups=0.97, wpb=14429.5, bsz=435.9, num_updates=47000, lr=0.000163082, gnorm=5.375, clip=2, loss_scale=128, train_wall=102, wall=50003
2022-10-12 20:54:53 | INFO | train_inner | epoch 103:    349 / 459 loss=6.449, nll_loss=3.127, mask_ins=1.116, word_ins_ml=4.729, word_reposition=0.604, ppl=87.35, wps=14372.1, ups=0.97, wpb=14835.5, bsz=453.1, num_updates=47100, lr=0.000162909, gnorm=3.501, clip=1, loss_scale=128, train_wall=102, wall=50106
2022-10-12 20:56:36 | INFO | train_inner | epoch 103:    449 / 459 loss=6.476, nll_loss=3.163, mask_ins=1.106, word_ins_ml=4.76, word_reposition=0.61, ppl=89.04, wps=14188.7, ups=0.97, wpb=14569.7, bsz=439.8, num_updates=47200, lr=0.000162736, gnorm=2.913, clip=0, loss_scale=128, train_wall=102, wall=50208
2022-10-12 20:56:47 | INFO | train | epoch 103 | loss 6.456 | nll_loss 3.136 | mask_ins 1.115 | word_ins_ml 4.737 | word_reposition 0.604 | ppl 87.78 | wps 13833.6 | ups 0.94 | wpb 14641.3 | bsz 444 | num_updates 47210 | lr 0.000162719 | gnorm 3.737 | clip 0.9 | loss_scale 114 | train_wall 469 | wall 50219
2022-10-12 20:56:56 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 14.702 | nll_loss 10.724 | mask_ins 1.878 | word_ins_ml 11.588 | word_reposition 1.236 | ppl 26661.2 | wps 34939.4 | wpb 1628.7 | bsz 55.8 | num_updates 47210 | best_loss 13.037
2022-10-12 20:56:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 103 @ 47210 updates, score 14.702) (writing took 2.9572126339771785 seconds)
2022-10-12 20:58:30 | INFO | train_inner | epoch 104:     90 / 459 loss=6.41, nll_loss=3.099, mask_ins=1.109, word_ins_ml=4.705, word_reposition=0.596, ppl=85.04, wps=12840.2, ups=0.88, wpb=14647.3, bsz=442.9, num_updates=47300, lr=0.000162564, gnorm=2.766, clip=0, loss_scale=128, train_wall=101, wall=50323
2022-10-12 21:00:14 | INFO | train_inner | epoch 104:    190 / 459 loss=6.42, nll_loss=3.117, mask_ins=1.112, word_ins_ml=4.721, word_reposition=0.587, ppl=85.65, wps=14484.8, ups=0.97, wpb=14981.2, bsz=458.2, num_updates=47400, lr=0.000162392, gnorm=2.399, clip=0, loss_scale=173, train_wall=103, wall=50426
2022-10-12 21:01:56 | INFO | train_inner | epoch 104:    290 / 459 loss=6.422, nll_loss=3.105, mask_ins=1.117, word_ins_ml=4.71, word_reposition=0.595, ppl=85.73, wps=14197.2, ups=0.98, wpb=14547, bsz=444.7, num_updates=47500, lr=0.000162221, gnorm=3.075, clip=2, loss_scale=256, train_wall=102, wall=50528
2022-10-12 21:03:38 | INFO | train_inner | epoch 104:    390 / 459 loss=6.465, nll_loss=3.143, mask_ins=1.115, word_ins_ml=4.743, word_reposition=0.607, ppl=88.32, wps=14233.1, ups=0.98, wpb=14549.2, bsz=437.8, num_updates=47600, lr=0.000162051, gnorm=2.077, clip=0, loss_scale=256, train_wall=101, wall=50631
2022-10-12 21:04:49 | INFO | train | epoch 104 | loss 6.433 | nll_loss 3.12 | mask_ins 1.112 | word_ins_ml 4.723 | word_reposition 0.598 | ppl 86.39 | wps 13919.1 | ups 0.95 | wpb 14638.5 | bsz 443.9 | num_updates 47669 | lr 0.000161934 | gnorm 2.572 | clip 0.7 | loss_scale 213 | train_wall 466 | wall 50702
2022-10-12 21:04:59 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 14.732 | nll_loss 10.8 | mask_ins 1.865 | word_ins_ml 11.662 | word_reposition 1.205 | ppl 27211.6 | wps 35031.2 | wpb 1628.7 | bsz 55.8 | num_updates 47669 | best_loss 13.037
2022-10-12 21:05:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 104 @ 47669 updates, score 14.732) (writing took 2.9484468510199804 seconds)
2022-10-12 21:05:33 | INFO | train_inner | epoch 105:     31 / 459 loss=6.425, nll_loss=3.121, mask_ins=1.103, word_ins_ml=4.723, word_reposition=0.599, ppl=85.95, wps=12605.2, ups=0.87, wpb=14466.2, bsz=437.7, num_updates=47700, lr=0.000161881, gnorm=2.643, clip=1, loss_scale=256, train_wall=102, wall=50745
2022-10-12 21:07:16 | INFO | train_inner | epoch 105:    131 / 459 loss=6.418, nll_loss=3.091, mask_ins=1.12, word_ins_ml=4.698, word_reposition=0.601, ppl=85.53, wps=14305.1, ups=0.97, wpb=14745, bsz=447, num_updates=47800, lr=0.000161712, gnorm=2.918, clip=0, loss_scale=256, train_wall=102, wall=50848
2022-10-12 21:08:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 21:09:00 | INFO | train_inner | epoch 105:    232 / 459 loss=6.427, nll_loss=3.099, mask_ins=1.12, word_ins_ml=4.704, word_reposition=0.603, ppl=86.03, wps=14238.8, ups=0.96, wpb=14840.2, bsz=449, num_updates=47900, lr=0.000161543, gnorm=2.841, clip=0, loss_scale=289, train_wall=103, wall=50953
2022-10-12 21:09:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 21:10:44 | INFO | train_inner | epoch 105:    333 / 459 loss=6.442, nll_loss=3.13, mask_ins=1.107, word_ins_ml=4.731, word_reposition=0.604, ppl=86.93, wps=14156.6, ups=0.96, wpb=14678.7, bsz=445.2, num_updates=48000, lr=0.000161374, gnorm=2.709, clip=0, loss_scale=194, train_wall=103, wall=51056
2022-10-12 21:12:26 | INFO | train_inner | epoch 105:    433 / 459 loss=6.444, nll_loss=3.134, mask_ins=1.109, word_ins_ml=4.735, word_reposition=0.6, ppl=87.07, wps=14183.7, ups=0.98, wpb=14422.4, bsz=436, num_updates=48100, lr=0.000161206, gnorm=3.198, clip=2, loss_scale=128, train_wall=101, wall=51158
2022-10-12 21:12:52 | INFO | train | epoch 105 | loss 6.425 | nll_loss 3.108 | mask_ins 1.111 | word_ins_ml 4.712 | word_reposition 0.602 | ppl 85.93 | wps 13852.9 | ups 0.95 | wpb 14647.2 | bsz 444.3 | num_updates 48126 | lr 0.000161163 | gnorm 3.049 | clip 0.7 | loss_scale 214 | train_wall 467 | wall 51185
2022-10-12 21:13:02 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 14.551 | nll_loss 10.669 | mask_ins 1.831 | word_ins_ml 11.536 | word_reposition 1.184 | ppl 24007.7 | wps 34946.7 | wpb 1628.7 | bsz 55.8 | num_updates 48126 | best_loss 13.037
2022-10-12 21:13:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 105 @ 48126 updates, score 14.551) (writing took 2.855352677986957 seconds)
2022-10-12 21:14:20 | INFO | train_inner | epoch 106:     74 / 459 loss=6.387, nll_loss=3.081, mask_ins=1.103, word_ins_ml=4.689, word_reposition=0.595, ppl=83.68, wps=12782.7, ups=0.87, wpb=14651.8, bsz=444.8, num_updates=48200, lr=0.000161039, gnorm=3.556, clip=2, loss_scale=128, train_wall=102, wall=51273
2022-10-12 21:16:03 | INFO | train_inner | epoch 106:    174 / 459 loss=6.36, nll_loss=3.054, mask_ins=1.102, word_ins_ml=4.666, word_reposition=0.593, ppl=82.13, wps=14196.3, ups=0.98, wpb=14539.8, bsz=439.3, num_updates=48300, lr=0.000160872, gnorm=2.63, clip=0, loss_scale=128, train_wall=102, wall=51375
2022-10-12 21:17:45 | INFO | train_inner | epoch 106:    274 / 459 loss=6.405, nll_loss=3.091, mask_ins=1.103, word_ins_ml=4.697, word_reposition=0.605, ppl=84.73, wps=14238.4, ups=0.98, wpb=14601.8, bsz=441.3, num_updates=48400, lr=0.000160706, gnorm=2.582, clip=0, loss_scale=128, train_wall=102, wall=51478
2022-10-12 21:19:28 | INFO | train_inner | epoch 106:    374 / 459 loss=6.434, nll_loss=3.112, mask_ins=1.114, word_ins_ml=4.716, word_reposition=0.604, ppl=86.48, wps=14306.4, ups=0.97, wpb=14689.9, bsz=446.6, num_updates=48500, lr=0.00016054, gnorm=3.31, clip=1, loss_scale=175, train_wall=102, wall=51580
2022-10-12 21:20:55 | INFO | train | epoch 106 | loss 6.403 | nll_loss 3.091 | mask_ins 1.107 | word_ins_ml 4.698 | word_reposition 0.598 | ppl 84.63 | wps 13923.2 | ups 0.95 | wpb 14641.2 | bsz 444.1 | num_updates 48585 | lr 0.0001604 | gnorm 2.94 | clip 0.4 | loss_scale 162 | train_wall 466 | wall 51667
2022-10-12 21:21:04 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 14.628 | nll_loss 10.773 | mask_ins 1.868 | word_ins_ml 11.63 | word_reposition 1.13 | ppl 25317.7 | wps 35004.1 | wpb 1628.7 | bsz 55.8 | num_updates 48585 | best_loss 13.037
2022-10-12 21:21:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 106 @ 48585 updates, score 14.628) (writing took 3.117295575008029 seconds)
2022-10-12 21:21:23 | INFO | train_inner | epoch 107:     15 / 459 loss=6.426, nll_loss=3.117, mask_ins=1.11, word_ins_ml=4.72, word_reposition=0.596, ppl=85.97, wps=12708.4, ups=0.87, wpb=14611.6, bsz=445.5, num_updates=48600, lr=0.000160375, gnorm=3.317, clip=0, loss_scale=256, train_wall=102, wall=51695
2022-10-12 21:23:06 | INFO | train_inner | epoch 107:    115 / 459 loss=6.36, nll_loss=3.049, mask_ins=1.105, word_ins_ml=4.661, word_reposition=0.595, ppl=82.13, wps=14266.1, ups=0.98, wpb=14623.9, bsz=442.9, num_updates=48700, lr=0.00016021, gnorm=4.46, clip=3, loss_scale=256, train_wall=102, wall=51798
2022-10-12 21:24:49 | INFO | train_inner | epoch 107:    215 / 459 loss=6.383, nll_loss=3.065, mask_ins=1.109, word_ins_ml=4.675, word_reposition=0.599, ppl=83.44, wps=14260.4, ups=0.97, wpb=14698.4, bsz=447.2, num_updates=48800, lr=0.000160046, gnorm=2.841, clip=0, loss_scale=256, train_wall=102, wall=51901
2022-10-12 21:25:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 21:26:33 | INFO | train_inner | epoch 107:    316 / 459 loss=6.401, nll_loss=3.092, mask_ins=1.107, word_ins_ml=4.698, word_reposition=0.597, ppl=84.52, wps=14399.1, ups=0.96, wpb=14991.9, bsz=456, num_updates=48900, lr=0.000159882, gnorm=2.246, clip=0, loss_scale=162, train_wall=103, wall=52005
2022-10-12 21:28:15 | INFO | train_inner | epoch 107:    416 / 459 loss=6.413, nll_loss=3.101, mask_ins=1.106, word_ins_ml=4.706, word_reposition=0.601, ppl=85.2, wps=13966.1, ups=0.98, wpb=14312.2, bsz=434.8, num_updates=49000, lr=0.000159719, gnorm=3.015, clip=2, loss_scale=128, train_wall=102, wall=52108
2022-10-12 21:28:59 | INFO | train | epoch 107 | loss 6.391 | nll_loss 3.081 | mask_ins 1.105 | word_ins_ml 4.688 | word_reposition 0.598 | ppl 83.95 | wps 13855.2 | ups 0.95 | wpb 14641.7 | bsz 444 | num_updates 49043 | lr 0.000159649 | gnorm 3.053 | clip 1.1 | loss_scale 195 | train_wall 467 | wall 52151
2022-10-12 21:29:08 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 14.68 | nll_loss 10.76 | mask_ins 1.89 | word_ins_ml 11.617 | word_reposition 1.173 | ppl 26243.3 | wps 34999.1 | wpb 1628.7 | bsz 55.8 | num_updates 49043 | best_loss 13.037
2022-10-12 21:29:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 107 @ 49043 updates, score 14.68) (writing took 2.832464033999713 seconds)
2022-10-12 21:30:10 | INFO | train_inner | epoch 108:     57 / 459 loss=6.387, nll_loss=3.084, mask_ins=1.096, word_ins_ml=4.691, word_reposition=0.6, ppl=83.67, wps=12785.9, ups=0.87, wpb=14651.6, bsz=439.8, num_updates=49100, lr=0.000159556, gnorm=2.17, clip=0, loss_scale=128, train_wall=102, wall=52222
2022-10-12 21:31:52 | INFO | train_inner | epoch 108:    157 / 459 loss=6.334, nll_loss=3.04, mask_ins=1.094, word_ins_ml=4.653, word_reposition=0.587, ppl=80.67, wps=14326.1, ups=0.98, wpb=14685, bsz=448.6, num_updates=49200, lr=0.000159394, gnorm=2.021, clip=0, loss_scale=128, train_wall=102, wall=52325
2022-10-12 21:33:35 | INFO | train_inner | epoch 108:    257 / 459 loss=6.391, nll_loss=3.068, mask_ins=1.109, word_ins_ml=4.677, word_reposition=0.605, ppl=83.9, wps=14091.9, ups=0.98, wpb=14431.5, bsz=432.2, num_updates=49300, lr=0.000159232, gnorm=1.783, clip=0, loss_scale=128, train_wall=102, wall=52427
2022-10-12 21:35:17 | INFO | train_inner | epoch 108:    357 / 459 loss=6.388, nll_loss=3.095, mask_ins=1.098, word_ins_ml=4.7, word_reposition=0.589, ppl=83.72, wps=14041.1, ups=0.98, wpb=14393.2, bsz=438.1, num_updates=49400, lr=0.000159071, gnorm=2.488, clip=1, loss_scale=207, train_wall=102, wall=52530
2022-10-12 21:37:02 | INFO | train_inner | epoch 108:    457 / 459 loss=6.421, nll_loss=3.107, mask_ins=1.109, word_ins_ml=4.711, word_reposition=0.6, ppl=85.67, wps=14344.6, ups=0.95, wpb=15060.6, bsz=460.1, num_updates=49500, lr=0.00015891, gnorm=1.986, clip=0, loss_scale=256, train_wall=104, wall=52635
2022-10-12 21:37:04 | INFO | train | epoch 108 | loss 6.381 | nll_loss 3.076 | mask_ins 1.102 | word_ins_ml 4.684 | word_reposition 0.596 | ppl 83.37 | wps 13849.3 | ups 0.95 | wpb 14633.9 | bsz 443.8 | num_updates 49502 | lr 0.000158907 | gnorm 2.084 | clip 0.2 | loss_scale 174 | train_wall 469 | wall 52636
2022-10-12 21:37:13 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 14.767 | nll_loss 10.804 | mask_ins 1.878 | word_ins_ml 11.66 | word_reposition 1.228 | ppl 27877.8 | wps 34969.9 | wpb 1628.7 | bsz 55.8 | num_updates 49502 | best_loss 13.037
2022-10-12 21:37:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 108 @ 49502 updates, score 14.767) (writing took 3.0280112539767288 seconds)
2022-10-12 21:37:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 21:38:59 | INFO | train_inner | epoch 109:     99 / 459 loss=6.335, nll_loss=3.034, mask_ins=1.1, word_ins_ml=4.647, word_reposition=0.588, ppl=80.74, wps=12490.1, ups=0.86, wpb=14559.3, bsz=441.9, num_updates=49600, lr=0.00015875, gnorm=1.948, clip=0, loss_scale=157, train_wall=103, wall=52751
2022-10-12 21:40:43 | INFO | train_inner | epoch 109:    199 / 459 loss=6.349, nll_loss=3.055, mask_ins=1.096, word_ins_ml=4.665, word_reposition=0.588, ppl=81.52, wps=14172.8, ups=0.96, wpb=14741.9, bsz=448.5, num_updates=49700, lr=0.00015859, gnorm=1.98, clip=0, loss_scale=128, train_wall=103, wall=52855
2022-10-12 21:42:26 | INFO | train_inner | epoch 109:    299 / 459 loss=6.385, nll_loss=3.065, mask_ins=1.105, word_ins_ml=4.674, word_reposition=0.605, ppl=83.55, wps=14355.5, ups=0.97, wpb=14776.4, bsz=449.8, num_updates=49800, lr=0.000158431, gnorm=2.743, clip=1, loss_scale=128, train_wall=102, wall=52958
2022-10-12 21:44:10 | INFO | train_inner | epoch 109:    399 / 459 loss=6.413, nll_loss=3.104, mask_ins=1.108, word_ins_ml=4.708, word_reposition=0.597, ppl=85.2, wps=14252, ups=0.96, wpb=14792.9, bsz=447.4, num_updates=49900, lr=0.000158272, gnorm=3.375, clip=2, loss_scale=128, train_wall=103, wall=53062
2022-10-12 21:45:10 | INFO | train | epoch 109 | loss 6.373 | nll_loss 3.065 | mask_ins 1.102 | word_ins_ml 4.675 | word_reposition 0.596 | ppl 82.88 | wps 13794.8 | ups 0.94 | wpb 14646.8 | bsz 444.2 | num_updates 49960 | lr 0.000158177 | gnorm 2.548 | clip 0.7 | loss_scale 134 | train_wall 470 | wall 53123
2022-10-12 21:45:20 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 14.688 | nll_loss 10.763 | mask_ins 1.914 | word_ins_ml 11.62 | word_reposition 1.154 | ppl 26396.9 | wps 34973.3 | wpb 1628.7 | bsz 55.8 | num_updates 49960 | best_loss 13.037
2022-10-12 21:45:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 109 @ 49960 updates, score 14.688) (writing took 2.9281940259970725 seconds)
2022-10-12 21:46:04 | INFO | train_inner | epoch 110:     40 / 459 loss=6.359, nll_loss=3.049, mask_ins=1.097, word_ins_ml=4.66, word_reposition=0.601, ppl=82.06, wps=12375, ups=0.88, wpb=14137.3, bsz=425.4, num_updates=50000, lr=0.000158114, gnorm=2.605, clip=0, loss_scale=128, train_wall=101, wall=53176
2022-10-12 21:47:46 | INFO | train_inner | epoch 110:    140 / 459 loss=6.325, nll_loss=3.019, mask_ins=1.096, word_ins_ml=4.634, word_reposition=0.595, ppl=80.18, wps=14254.8, ups=0.98, wpb=14614.7, bsz=444.1, num_updates=50100, lr=0.000157956, gnorm=2.744, clip=1, loss_scale=212, train_wall=102, wall=53279
2022-10-12 21:49:29 | INFO | train_inner | epoch 110:    240 / 459 loss=6.365, nll_loss=3.057, mask_ins=1.102, word_ins_ml=4.667, word_reposition=0.596, ppl=82.41, wps=14183.5, ups=0.97, wpb=14599.8, bsz=443.1, num_updates=50200, lr=0.000157799, gnorm=2.069, clip=0, loss_scale=256, train_wall=102, wall=53382
2022-10-12 21:51:13 | INFO | train_inner | epoch 110:    340 / 459 loss=6.369, nll_loss=3.067, mask_ins=1.095, word_ins_ml=4.676, word_reposition=0.599, ppl=82.68, wps=14327.2, ups=0.97, wpb=14830.1, bsz=449.5, num_updates=50300, lr=0.000157642, gnorm=2.282, clip=0, loss_scale=256, train_wall=103, wall=53485
2022-10-12 21:52:57 | INFO | train_inner | epoch 110:    440 / 459 loss=6.394, nll_loss=3.081, mask_ins=1.106, word_ins_ml=4.688, word_reposition=0.601, ppl=84.12, wps=14212.9, ups=0.96, wpb=14760.5, bsz=447.9, num_updates=50400, lr=0.000157485, gnorm=2.023, clip=0, loss_scale=256, train_wall=103, wall=53589
2022-10-12 21:53:16 | INFO | train | epoch 110 | loss 6.36 | nll_loss 3.053 | mask_ins 1.099 | word_ins_ml 4.664 | word_reposition 0.597 | ppl 82.13 | wps 13834 | ups 0.94 | wpb 14642.4 | bsz 444.1 | num_updates 50419 | lr 0.000157456 | gnorm 2.268 | clip 0.2 | loss_scale 235 | train_wall 469 | wall 53609
2022-10-12 21:53:26 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 14.73 | nll_loss 10.804 | mask_ins 1.859 | word_ins_ml 11.664 | word_reposition 1.207 | ppl 27178.5 | wps 35038.7 | wpb 1628.7 | bsz 55.8 | num_updates 50419 | best_loss 13.037
2022-10-12 21:53:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 110 @ 50419 updates, score 14.73) (writing took 2.975934779009549 seconds)
2022-10-12 21:54:52 | INFO | train_inner | epoch 111:     81 / 459 loss=6.341, nll_loss=3.042, mask_ins=1.098, word_ins_ml=4.655, word_reposition=0.589, ppl=81.06, wps=12662.9, ups=0.87, wpb=14627.5, bsz=442.2, num_updates=50500, lr=0.000157329, gnorm=1.858, clip=0, loss_scale=256, train_wall=102, wall=53704
2022-10-12 21:56:35 | INFO | train_inner | epoch 111:    181 / 459 loss=6.342, nll_loss=3.043, mask_ins=1.098, word_ins_ml=4.655, word_reposition=0.589, ppl=81.11, wps=14197.9, ups=0.97, wpb=14635, bsz=445.9, num_updates=50600, lr=0.000157174, gnorm=1.92, clip=0, loss_scale=394, train_wall=102, wall=53807
2022-10-12 21:58:19 | INFO | train_inner | epoch 111:    281 / 459 loss=6.346, nll_loss=3.048, mask_ins=1.1, word_ins_ml=4.659, word_reposition=0.587, ppl=81.36, wps=14260, ups=0.97, wpb=14739.1, bsz=445.5, num_updates=50700, lr=0.000157019, gnorm=1.979, clip=0, loss_scale=512, train_wall=102, wall=53911
2022-10-12 22:00:02 | INFO | train_inner | epoch 111:    381 / 459 loss=6.332, nll_loss=3.04, mask_ins=1.097, word_ins_ml=4.652, word_reposition=0.583, ppl=80.56, wps=14153.5, ups=0.97, wpb=14600.2, bsz=442.5, num_updates=50800, lr=0.000156864, gnorm=1.99, clip=0, loss_scale=512, train_wall=102, wall=54014
2022-10-12 22:01:22 | INFO | train | epoch 111 | loss 6.339 | nll_loss 3.044 | mask_ins 1.097 | word_ins_ml 4.656 | word_reposition 0.587 | ppl 80.95 | wps 13829 | ups 0.95 | wpb 14633.5 | bsz 443.7 | num_updates 50878 | lr 0.000156744 | gnorm 1.919 | clip 0 | loss_scale 441 | train_wall 469 | wall 54094
2022-10-12 22:01:31 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 14.788 | nll_loss 10.809 | mask_ins 1.861 | word_ins_ml 11.669 | word_reposition 1.257 | ppl 28284.6 | wps 34946.7 | wpb 1628.7 | bsz 55.8 | num_updates 50878 | best_loss 13.037
2022-10-12 22:01:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 111 @ 50878 updates, score 14.788) (writing took 3.0098376789828762 seconds)
2022-10-12 22:01:57 | INFO | train_inner | epoch 112:     22 / 459 loss=6.326, nll_loss=3.038, mask_ins=1.088, word_ins_ml=4.65, word_reposition=0.588, ppl=80.21, wps=12610.7, ups=0.87, wpb=14540.5, bsz=442.2, num_updates=50900, lr=0.00015671, gnorm=2.016, clip=0, loss_scale=512, train_wall=102, wall=54129
2022-10-12 22:02:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 22:03:41 | INFO | train_inner | epoch 112:    123 / 459 loss=6.318, nll_loss=3.023, mask_ins=1.09, word_ins_ml=4.637, word_reposition=0.59, ppl=79.76, wps=14195.7, ups=0.96, wpb=14801.6, bsz=450, num_updates=51000, lr=0.000156556, gnorm=2.966, clip=1, loss_scale=294, train_wall=103, wall=54234
2022-10-12 22:04:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 22:05:25 | INFO | train_inner | epoch 112:    224 / 459 loss=6.316, nll_loss=3.012, mask_ins=1.102, word_ins_ml=4.628, word_reposition=0.587, ppl=79.68, wps=14048.3, ups=0.96, wpb=14619.5, bsz=442.7, num_updates=51100, lr=0.000156403, gnorm=2.903, clip=0, loss_scale=162, train_wall=103, wall=54338
2022-10-12 22:07:08 | INFO | train_inner | epoch 112:    324 / 459 loss=6.35, nll_loss=3.047, mask_ins=1.103, word_ins_ml=4.658, word_reposition=0.589, ppl=81.59, wps=13964.2, ups=0.97, wpb=14389.2, bsz=436.9, num_updates=51200, lr=0.00015625, gnorm=3.855, clip=1, loss_scale=128, train_wall=102, wall=54441
2022-10-12 22:08:52 | INFO | train_inner | epoch 112:    424 / 459 loss=6.338, nll_loss=3.04, mask_ins=1.095, word_ins_ml=4.652, word_reposition=0.591, ppl=80.89, wps=14148, ups=0.97, wpb=14621.4, bsz=442.9, num_updates=51300, lr=0.000156098, gnorm=4.041, clip=1, loss_scale=128, train_wall=102, wall=54544
2022-10-12 22:09:28 | INFO | train | epoch 112 | loss 6.328 | nll_loss 3.03 | mask_ins 1.096 | word_ins_ml 4.643 | word_reposition 0.589 | ppl 80.33 | wps 13762.3 | ups 0.94 | wpb 14637.8 | bsz 444 | num_updates 51335 | lr 0.000156044 | gnorm 3.357 | clip 0.7 | loss_scale 190 | train_wall 470 | wall 54580
2022-10-12 22:09:37 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 14.942 | nll_loss 10.925 | mask_ins 1.87 | word_ins_ml 11.775 | word_reposition 1.297 | ppl 31475.5 | wps 35025.8 | wpb 1628.7 | bsz 55.8 | num_updates 51335 | best_loss 13.037
2022-10-12 22:09:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 112 @ 51335 updates, score 14.942) (writing took 2.9655795190192293 seconds)
2022-10-12 22:10:47 | INFO | train_inner | epoch 113:     65 / 459 loss=6.309, nll_loss=3.005, mask_ins=1.103, word_ins_ml=4.622, word_reposition=0.583, ppl=79.26, wps=12537.4, ups=0.87, wpb=14428.8, bsz=437.4, num_updates=51400, lr=0.000155946, gnorm=2.977, clip=0, loss_scale=128, train_wall=102, wall=54659
2022-10-12 22:12:30 | INFO | train_inner | epoch 113:    165 / 459 loss=6.302, nll_loss=3.005, mask_ins=1.094, word_ins_ml=4.622, word_reposition=0.585, ppl=78.89, wps=14001.7, ups=0.97, wpb=14395.1, bsz=436.6, num_updates=51500, lr=0.000155794, gnorm=4.192, clip=2, loss_scale=128, train_wall=102, wall=54762
2022-10-12 22:13:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-12 22:14:14 | INFO | train_inner | epoch 113:    266 / 459 loss=6.334, nll_loss=3.028, mask_ins=1.095, word_ins_ml=4.641, word_reposition=0.598, ppl=80.69, wps=14362.2, ups=0.96, wpb=14978.5, bsz=454.1, num_updates=51600, lr=0.000155643, gnorm=3.029, clip=0, loss_scale=85, train_wall=103, wall=54866
2022-10-12 22:15:57 | INFO | train_inner | epoch 113:    366 / 459 loss=6.34, nll_loss=3.026, mask_ins=1.105, word_ins_ml=4.64, word_reposition=0.595, ppl=81.02, wps=14178.2, ups=0.97, wpb=14610, bsz=442.2, num_updates=51700, lr=0.000155493, gnorm=3.976, clip=1, loss_scale=64, train_wall=102, wall=54969
2022-10-12 22:17:34 | INFO | train | epoch 113 | loss 6.329 | nll_loss 3.023 | mask_ins 1.1 | word_ins_ml 4.638 | word_reposition 0.591 | ppl 80.39 | wps 13808.3 | ups 0.94 | wpb 14640.8 | bsz 444 | num_updates 51793 | lr 0.000155353 | gnorm 3.673 | clip 0.9 | loss_scale 92 | train_wall 469 | wall 55066
2022-10-12 22:17:43 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 14.745 | nll_loss 10.863 | mask_ins 1.927 | word_ins_ml 11.718 | word_reposition 1.1 | ppl 27456.9 | wps 34947.3 | wpb 1628.7 | bsz 55.8 | num_updates 51793 | best_loss 13.037
2022-10-12 22:17:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 113 @ 51793 updates, score 14.745) (writing took 2.995533384004375 seconds)
2022-10-12 22:17:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-12 22:17:54 | INFO | train_inner | epoch 114:      8 / 459 loss=6.36, nll_loss=3.062, mask_ins=1.1, word_ins_ml=4.671, word_reposition=0.589, ppl=82.11, wps=12647, ups=0.85, wpb=14800.7, bsz=450.2, num_updates=51800, lr=0.000155342, gnorm=4.019, clip=1, loss_scale=63, train_wall=104, wall=55086
2022-10-12 22:19:38 | INFO | train_inner | epoch 114:    108 / 459 loss=6.329, nll_loss=3.023, mask_ins=1.104, word_ins_ml=4.638, word_reposition=0.587, ppl=80.38, wps=14361.8, ups=0.96, wpb=14905.7, bsz=452.9, num_updates=51900, lr=0.000155193, gnorm=4.272, clip=3, loss_scale=32, train_wall=103, wall=55190
2022-10-12 22:21:21 | INFO | train_inner | epoch 114:    208 / 459 loss=6.316, nll_loss=3.015, mask_ins=1.1, word_ins_ml=4.631, word_reposition=0.584, ppl=79.66, wps=14126.8, ups=0.96, wpb=14642.7, bsz=443.4, num_updates=52000, lr=0.000155043, gnorm=5.235, clip=3, loss_scale=32, train_wall=103, wall=55294
2022-10-12 22:23:04 | INFO | train_inner | epoch 114:    308 / 459 loss=6.347, nll_loss=3.03, mask_ins=1.107, word_ins_ml=4.644, word_reposition=0.597, ppl=81.43, wps=14265.7, ups=0.97, wpb=14660.8, bsz=444.6, num_updates=52100, lr=0.000154895, gnorm=5.742, clip=3, loss_scale=32, train_wall=102, wall=55397
2022-10-12 22:24:47 | INFO | train_inner | epoch 114:    408 / 459 loss=6.323, nll_loss=3.021, mask_ins=1.107, word_ins_ml=4.635, word_reposition=0.581, ppl=80.07, wps=14207.5, ups=0.97, wpb=14636.2, bsz=444.2, num_updates=52200, lr=0.000154746, gnorm=4.153, clip=2, loss_scale=32, train_wall=102, wall=55500
2022-10-12 22:25:38 | INFO | train | epoch 114 | loss 6.327 | nll_loss 3.02 | mask_ins 1.104 | word_ins_ml 4.635 | word_reposition 0.588 | ppl 80.27 | wps 13831.4 | ups 0.94 | wpb 14641.3 | bsz 444.1 | num_updates 52251 | lr 0.000154671 | gnorm 4.917 | clip 2.4 | loss_scale 32 | train_wall 468 | wall 55551
2022-10-12 22:25:48 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 14.894 | nll_loss 10.954 | mask_ins 1.894 | word_ins_ml 11.804 | word_reposition 1.197 | ppl 30452.3 | wps 34935.8 | wpb 1628.7 | bsz 55.8 | num_updates 52251 | best_loss 13.037
2022-10-12 22:25:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 114 @ 52251 updates, score 14.894) (writing took 2.7472979680169374 seconds)
2022-10-12 22:26:40 | INFO | train_inner | epoch 115:     49 / 459 loss=6.312, nll_loss=3.002, mask_ins=1.099, word_ins_ml=4.619, word_reposition=0.595, ppl=79.47, wps=12565.6, ups=0.88, wpb=14200.7, bsz=428.8, num_updates=52300, lr=0.000154598, gnorm=7.006, clip=4, loss_scale=32, train_wall=100, wall=55613
2022-10-12 22:28:22 | INFO | train_inner | epoch 115:    149 / 459 loss=6.262, nll_loss=2.958, mask_ins=1.09, word_ins_ml=4.581, word_reposition=0.59, ppl=76.76, wps=14088.4, ups=0.98, wpb=14343, bsz=435.4, num_updates=52400, lr=0.000154451, gnorm=7.982, clip=7, loss_scale=61, train_wall=101, wall=55714
2022-10-12 22:30:05 | INFO | train_inner | epoch 115:    249 / 459 loss=6.304, nll_loss=3.01, mask_ins=1.095, word_ins_ml=4.626, word_reposition=0.583, ppl=78.99, wps=14312.6, ups=0.97, wpb=14782.9, bsz=446.6, num_updates=52500, lr=0.000154303, gnorm=9.812, clip=4, loss_scale=64, train_wall=102, wall=55818
2022-10-12 22:31:50 | INFO | train_inner | epoch 115:    349 / 459 loss=6.303, nll_loss=3.016, mask_ins=1.095, word_ins_ml=4.631, word_reposition=0.577, ppl=78.96, wps=14400.4, ups=0.96, wpb=15035.9, bsz=458.4, num_updates=52600, lr=0.000154157, gnorm=4.893, clip=3, loss_scale=64, train_wall=104, wall=55922
2022-10-12 22:33:33 | INFO | train_inner | epoch 115:    449 / 459 loss=6.278, nll_loss=2.988, mask_ins=1.086, word_ins_ml=4.607, word_reposition=0.585, ppl=77.62, wps=14260.2, ups=0.97, wpb=14716.7, bsz=447.8, num_updates=52700, lr=0.00015401, gnorm=2.424, clip=0, loss_scale=64, train_wall=102, wall=56025
2022-10-12 22:33:43 | INFO | train | epoch 115 | loss 6.292 | nll_loss 2.996 | mask_ins 1.093 | word_ins_ml 4.614 | word_reposition 0.585 | ppl 78.34 | wps 13871.4 | ups 0.95 | wpb 14640.6 | bsz 444 | num_updates 52710 | lr 0.000153996 | gnorm 6.458 | clip 3.9 | loss_scale 60 | train_wall 468 | wall 56035
2022-10-12 22:33:52 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 14.968 | nll_loss 10.892 | mask_ins 1.884 | word_ins_ml 11.746 | word_reposition 1.338 | ppl 32048.6 | wps 34995.4 | wpb 1628.7 | bsz 55.8 | num_updates 52710 | best_loss 13.037
2022-10-12 22:33:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 115 @ 52710 updates, score 14.968) (writing took 2.830010912992293 seconds)
2022-10-12 22:35:27 | INFO | train_inner | epoch 116:     90 / 459 loss=6.265, nll_loss=2.959, mask_ins=1.092, word_ins_ml=4.582, word_reposition=0.591, ppl=76.89, wps=12703.3, ups=0.88, wpb=14501.3, bsz=440.3, num_updates=52800, lr=0.000153864, gnorm=3.792, clip=2, loss_scale=64, train_wall=101, wall=56139
2022-10-12 22:37:10 | INFO | train_inner | epoch 116:    190 / 459 loss=6.266, nll_loss=2.979, mask_ins=1.083, word_ins_ml=4.599, word_reposition=0.583, ppl=76.94, wps=14209, ups=0.97, wpb=14577.6, bsz=441.1, num_updates=52900, lr=0.000153719, gnorm=2.535, clip=0, loss_scale=115, train_wall=102, wall=56242
2022-10-12 22:38:53 | INFO | train_inner | epoch 116:    290 / 459 loss=6.285, nll_loss=2.987, mask_ins=1.094, word_ins_ml=4.606, word_reposition=0.585, ppl=77.99, wps=14178.9, ups=0.97, wpb=14645.5, bsz=444.6, num_updates=53000, lr=0.000153574, gnorm=5.503, clip=2, loss_scale=128, train_wall=102, wall=56345
2022-10-12 22:40:36 | INFO | train_inner | epoch 116:    390 / 459 loss=6.302, nll_loss=3.018, mask_ins=1.081, word_ins_ml=4.633, word_reposition=0.588, ppl=78.91, wps=14470.2, ups=0.97, wpb=14886.7, bsz=451.7, num_updates=53100, lr=0.000153429, gnorm=2.913, clip=0, loss_scale=128, train_wall=102, wall=56448
2022-10-12 22:41:47 | INFO | train | epoch 116 | loss 6.285 | nll_loss 2.99 | mask_ins 1.089 | word_ins_ml 4.608 | word_reposition 0.588 | ppl 78 | wps 13898.7 | ups 0.95 | wpb 14643.2 | bsz 444.1 | num_updates 53169 | lr 0.00015333 | gnorm 3.606 | clip 0.9 | loss_scale 113 | train_wall 467 | wall 56519
2022-10-12 22:41:56 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 14.897 | nll_loss 10.839 | mask_ins 1.89 | word_ins_ml 11.69 | word_reposition 1.318 | ppl 30517.4 | wps 35009.4 | wpb 1628.7 | bsz 55.8 | num_updates 53169 | best_loss 13.037
2022-10-12 22:41:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 116 @ 53169 updates, score 14.897) (writing took 2.866398219979601 seconds)
2022-10-12 22:42:30 | INFO | train_inner | epoch 117:     31 / 459 loss=6.307, nll_loss=2.999, mask_ins=1.099, word_ins_ml=4.616, word_reposition=0.592, ppl=79.19, wps=12591.7, ups=0.88, wpb=14371.2, bsz=433.4, num_updates=53200, lr=0.000153285, gnorm=3.26, clip=0, loss_scale=128, train_wall=101, wall=56562
2022-10-12 22:44:12 | INFO | train_inner | epoch 117:    131 / 459 loss=6.236, nll_loss=2.937, mask_ins=1.093, word_ins_ml=4.562, word_reposition=0.581, ppl=75.36, wps=14327.6, ups=0.98, wpb=14648.5, bsz=445.5, num_updates=53300, lr=0.000153141, gnorm=4.621, clip=2, loss_scale=128, train_wall=101, wall=56665
2022-10-12 22:45:56 | INFO | train_inner | epoch 117:    231 / 459 loss=6.293, nll_loss=2.998, mask_ins=1.092, word_ins_ml=4.615, word_reposition=0.586, ppl=78.43, wps=14467.4, ups=0.97, wpb=14974.6, bsz=457.2, num_updates=53400, lr=0.000152998, gnorm=3.845, clip=1, loss_scale=215, train_wall=103, wall=56768
2022-10-12 22:47:39 | INFO | train_inner | epoch 117:    331 / 459 loss=6.313, nll_loss=3.023, mask_ins=1.088, word_ins_ml=4.637, word_reposition=0.588, ppl=79.49, wps=14360.2, ups=0.97, wpb=14794.4, bsz=446.6, num_updates=53500, lr=0.000152854, gnorm=3.99, clip=1, loss_scale=256, train_wall=102, wall=56871
2022-10-12 22:49:22 | INFO | train_inner | epoch 117:    431 / 459 loss=6.284, nll_loss=2.984, mask_ins=1.091, word_ins_ml=4.603, word_reposition=0.59, ppl=77.91, wps=14144.3, ups=0.97, wpb=14527.7, bsz=439.6, num_updates=53600, lr=0.000152712, gnorm=2.642, clip=0, loss_scale=256, train_wall=102, wall=56974
2022-10-12 22:49:50 | INFO | train | epoch 117 | loss 6.277 | nll_loss 2.981 | mask_ins 1.091 | word_ins_ml 4.601 | word_reposition 0.585 | ppl 77.56 | wps 13906 | ups 0.95 | wpb 14637.2 | bsz 443.9 | num_updates 53628 | lr 0.000152672 | gnorm 3.79 | clip 0.9 | loss_scale 211 | train_wall 467 | wall 57002
2022-10-12 22:49:59 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 14.808 | nll_loss 10.869 | mask_ins 1.897 | word_ins_ml 11.721 | word_reposition 1.189 | ppl 28687.2 | wps 35014.5 | wpb 1628.7 | bsz 55.8 | num_updates 53628 | best_loss 13.037
2022-10-12 22:50:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 117 @ 53628 updates, score 14.808) (writing took 2.877801423019264 seconds)
2022-10-12 22:51:16 | INFO | train_inner | epoch 118:     72 / 459 loss=6.252, nll_loss=2.959, mask_ins=1.089, word_ins_ml=4.582, word_reposition=0.581, ppl=76.23, wps=12800.5, ups=0.87, wpb=14652.5, bsz=444.4, num_updates=53700, lr=0.00015257, gnorm=2.942, clip=0, loss_scale=256, train_wall=101, wall=57088
2022-10-12 22:52:58 | INFO | train_inner | epoch 118:    172 / 459 loss=6.268, nll_loss=2.981, mask_ins=1.083, word_ins_ml=4.6, word_reposition=0.585, ppl=77.05, wps=14243.7, ups=0.98, wpb=14568.2, bsz=441.6, num_updates=53800, lr=0.000152428, gnorm=2.305, clip=1, loss_scale=256, train_wall=101, wall=57191
2022-10-12 22:54:40 | INFO | train_inner | epoch 118:    272 / 459 loss=6.277, nll_loss=2.969, mask_ins=1.095, word_ins_ml=4.59, word_reposition=0.592, ppl=77.55, wps=14159.1, ups=0.98, wpb=14467, bsz=438.2, num_updates=53900, lr=0.000152286, gnorm=2.087, clip=0, loss_scale=399, train_wall=101, wall=57293
2022-10-12 22:56:24 | INFO | train_inner | epoch 118:    372 / 459 loss=6.28, nll_loss=2.979, mask_ins=1.091, word_ins_ml=4.598, word_reposition=0.591, ppl=77.7, wps=14162.4, ups=0.97, wpb=14607.3, bsz=444.4, num_updates=54000, lr=0.000152145, gnorm=2.519, clip=0, loss_scale=512, train_wall=102, wall=57396
2022-10-12 22:57:52 | INFO | train | epoch 118 | loss 6.27 | nll_loss 2.973 | mask_ins 1.088 | word_ins_ml 4.593 | word_reposition 0.589 | ppl 77.19 | wps 13919.4 | ups 0.95 | wpb 14635.7 | bsz 443.8 | num_updates 54087 | lr 0.000152023 | gnorm 2.631 | clip 0.2 | loss_scale 392 | train_wall 466 | wall 57485
2022-10-12 22:58:02 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 14.734 | nll_loss 10.847 | mask_ins 1.886 | word_ins_ml 11.703 | word_reposition 1.146 | ppl 27255.2 | wps 34947 | wpb 1628.7 | bsz 55.8 | num_updates 54087 | best_loss 13.037
2022-10-12 22:58:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 118 @ 54087 updates, score 14.734) (writing took 2.800109315023292 seconds)
2022-10-12 22:58:18 | INFO | train_inner | epoch 119:     13 / 459 loss=6.279, nll_loss=2.979, mask_ins=1.089, word_ins_ml=4.598, word_reposition=0.592, ppl=77.64, wps=12797.6, ups=0.87, wpb=14628.8, bsz=442.9, num_updates=54100, lr=0.000152004, gnorm=3.855, clip=0, loss_scale=512, train_wall=101, wall=57510
2022-10-12 23:00:01 | INFO | train_inner | epoch 119:    113 / 459 loss=6.255, nll_loss=2.957, mask_ins=1.097, word_ins_ml=4.58, word_reposition=0.578, ppl=76.35, wps=14329.6, ups=0.97, wpb=14706, bsz=447, num_updates=54200, lr=0.000151864, gnorm=3.572, clip=0, loss_scale=512, train_wall=102, wall=57613
2022-10-12 23:01:44 | INFO | train_inner | epoch 119:    213 / 459 loss=6.275, nll_loss=2.98, mask_ins=1.086, word_ins_ml=4.6, word_reposition=0.589, ppl=77.45, wps=14257, ups=0.97, wpb=14698.4, bsz=443.6, num_updates=54300, lr=0.000151724, gnorm=3.518, clip=0, loss_scale=512, train_wall=102, wall=57716
2022-10-12 23:03:26 | INFO | train_inner | epoch 119:    313 / 459 loss=6.257, nll_loss=2.96, mask_ins=1.088, word_ins_ml=4.582, word_reposition=0.586, ppl=76.46, wps=14098.8, ups=0.97, wpb=14470.8, bsz=440.5, num_updates=54400, lr=0.000151585, gnorm=4.238, clip=1, loss_scale=737, train_wall=102, wall=57819
2022-10-12 23:05:10 | INFO | train_inner | epoch 119:    413 / 459 loss=6.257, nll_loss=2.962, mask_ins=1.088, word_ins_ml=4.583, word_reposition=0.586, ppl=76.48, wps=14313, ups=0.96, wpb=14860.4, bsz=451.8, num_updates=54500, lr=0.000151446, gnorm=3.003, clip=0, loss_scale=1024, train_wall=103, wall=57922
2022-10-12 23:05:57 | INFO | train | epoch 119 | loss 6.266 | nll_loss 2.97 | mask_ins 1.09 | word_ins_ml 4.591 | word_reposition 0.585 | ppl 76.96 | wps 13853.6 | ups 0.95 | wpb 14639.4 | bsz 444 | num_updates 54546 | lr 0.000151382 | gnorm 3.492 | clip 0.2 | loss_scale 724 | train_wall 469 | wall 57970
2022-10-12 23:06:07 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 14.985 | nll_loss 10.954 | mask_ins 1.902 | word_ins_ml 11.797 | word_reposition 1.285 | ppl 32424.2 | wps 35030 | wpb 1628.7 | bsz 55.8 | num_updates 54546 | best_loss 13.037
2022-10-12 23:06:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 119 @ 54546 updates, score 14.985) (writing took 2.93070153798908 seconds)
2022-10-12 23:07:05 | INFO | train_inner | epoch 120:     54 / 459 loss=6.274, nll_loss=2.986, mask_ins=1.087, word_ins_ml=4.605, word_reposition=0.583, ppl=77.41, wps=12570.4, ups=0.87, wpb=14457, bsz=438.7, num_updates=54600, lr=0.000151307, gnorm=2.497, clip=0, loss_scale=1024, train_wall=102, wall=58037
2022-10-12 23:08:48 | INFO | train_inner | epoch 120:    154 / 459 loss=6.206, nll_loss=2.922, mask_ins=1.076, word_ins_ml=4.549, word_reposition=0.581, ppl=73.83, wps=14067.7, ups=0.97, wpb=14441.3, bsz=438.4, num_updates=54700, lr=0.000151169, gnorm=2.547, clip=0, loss_scale=1024, train_wall=102, wall=58140
2022-10-12 23:10:30 | INFO | train_inner | epoch 120:    254 / 459 loss=6.284, nll_loss=2.982, mask_ins=1.093, word_ins_ml=4.602, word_reposition=0.59, ppl=77.93, wps=14416.6, ups=0.98, wpb=14785.5, bsz=446.2, num_updates=54800, lr=0.000151031, gnorm=2.998, clip=0, loss_scale=1024, train_wall=102, wall=58243
2022-10-12 23:10:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-10-12 23:11:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-12 23:12:16 | INFO | train_inner | epoch 120:    356 / 459 loss=6.244, nll_loss=2.958, mask_ins=1.088, word_ins_ml=4.58, word_reposition=0.576, ppl=75.78, wps=13937.8, ups=0.95, wpb=14729.2, bsz=446.9, num_updates=54900, lr=0.000150893, gnorm=4.337, clip=1, loss_scale=522, train_wall=105, wall=58348
2022-10-12 23:13:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 23:13:59 | INFO | train_inner | epoch 120:    457 / 459 loss=6.282, nll_loss=2.993, mask_ins=1.091, word_ins_ml=4.61, word_reposition=0.581, ppl=77.81, wps=14200.7, ups=0.97, wpb=14681.3, bsz=444.7, num_updates=55000, lr=0.000150756, gnorm=7.071, clip=3, loss_scale=189, train_wall=102, wall=58452
2022-10-12 23:14:01 | INFO | train | epoch 120 | loss 6.254 | nll_loss 2.964 | mask_ins 1.087 | word_ins_ml 4.585 | word_reposition 0.582 | ppl 76.34 | wps 13792.4 | ups 0.94 | wpb 14636.9 | bsz 443.9 | num_updates 55002 | lr 0.000150753 | gnorm 4.033 | clip 0.9 | loss_scale 725 | train_wall 467 | wall 58454
2022-10-12 23:14:10 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 14.802 | nll_loss 10.852 | mask_ins 1.885 | word_ins_ml 11.709 | word_reposition 1.208 | ppl 28561.6 | wps 35031.8 | wpb 1628.7 | bsz 55.8 | num_updates 55002 | best_loss 13.037
2022-10-12 23:14:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 120 @ 55002 updates, score 14.802) (writing took 2.8530507030081935 seconds)
2022-10-12 23:15:54 | INFO | train_inner | epoch 121:     98 / 459 loss=6.2, nll_loss=2.922, mask_ins=1.075, word_ins_ml=4.549, word_reposition=0.576, ppl=73.53, wps=12685.7, ups=0.87, wpb=14520.5, bsz=442.9, num_updates=55100, lr=0.000150619, gnorm=8.762, clip=5, loss_scale=128, train_wall=101, wall=58566
2022-10-12 23:17:36 | INFO | train_inner | epoch 121:    198 / 459 loss=6.247, nll_loss=2.95, mask_ins=1.085, word_ins_ml=4.574, word_reposition=0.589, ppl=75.97, wps=14075.2, ups=0.98, wpb=14336.2, bsz=430.9, num_updates=55200, lr=0.000150482, gnorm=6.916, clip=3, loss_scale=128, train_wall=101, wall=58668
2022-10-12 23:19:18 | INFO | train_inner | epoch 121:    298 / 459 loss=6.216, nll_loss=2.931, mask_ins=1.084, word_ins_ml=4.556, word_reposition=0.576, ppl=74.33, wps=14341.1, ups=0.97, wpb=14740, bsz=446.2, num_updates=55300, lr=0.000150346, gnorm=4.762, clip=2, loss_scale=128, train_wall=102, wall=58771
2022-10-12 23:21:02 | INFO | train_inner | epoch 121:    398 / 459 loss=6.28, nll_loss=2.989, mask_ins=1.087, word_ins_ml=4.606, word_reposition=0.587, ppl=77.69, wps=14494.9, ups=0.97, wpb=14982.7, bsz=455.7, num_updates=55400, lr=0.00015021, gnorm=4.287, clip=1, loss_scale=128, train_wall=102, wall=58874
2022-10-12 23:22:04 | INFO | train | epoch 121 | loss 6.238 | nll_loss 2.951 | mask_ins 1.084 | word_ins_ml 4.574 | word_reposition 0.58 | ppl 75.49 | wps 13912.9 | ups 0.95 | wpb 14636.2 | bsz 443.9 | num_updates 55461 | lr 0.000150128 | gnorm 6.456 | clip 2.8 | loss_scale 129 | train_wall 466 | wall 58936
2022-10-12 23:22:13 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 15.085 | nll_loss 10.995 | mask_ins 1.922 | word_ins_ml 11.837 | word_reposition 1.325 | ppl 34753.3 | wps 35077.2 | wpb 1628.7 | bsz 55.8 | num_updates 55461 | best_loss 13.037
2022-10-12 23:22:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 121 @ 55461 updates, score 15.085) (writing took 2.8439112560008653 seconds)
2022-10-12 23:22:56 | INFO | train_inner | epoch 122:     39 / 459 loss=6.218, nll_loss=2.932, mask_ins=1.091, word_ins_ml=4.557, word_reposition=0.57, ppl=74.45, wps=12693, ups=0.88, wpb=14474.2, bsz=441.4, num_updates=55500, lr=0.000150075, gnorm=7.132, clip=3, loss_scale=180, train_wall=101, wall=58988
2022-10-12 23:24:39 | INFO | train_inner | epoch 122:    139 / 459 loss=6.244, nll_loss=2.95, mask_ins=1.085, word_ins_ml=4.574, word_reposition=0.585, ppl=75.78, wps=14335, ups=0.97, wpb=14732.3, bsz=446.2, num_updates=55600, lr=0.00014994, gnorm=3.706, clip=2, loss_scale=256, train_wall=102, wall=59091
2022-10-12 23:26:21 | INFO | train_inner | epoch 122:    239 / 459 loss=6.224, nll_loss=2.931, mask_ins=1.081, word_ins_ml=4.556, word_reposition=0.587, ppl=74.75, wps=14174.8, ups=0.97, wpb=14572, bsz=443.9, num_updates=55700, lr=0.000149805, gnorm=3.447, clip=0, loss_scale=256, train_wall=102, wall=59194
2022-10-12 23:28:04 | INFO | train_inner | epoch 122:    339 / 459 loss=6.25, nll_loss=2.954, mask_ins=1.084, word_ins_ml=4.576, word_reposition=0.59, ppl=76.09, wps=14264.6, ups=0.98, wpb=14626.4, bsz=441.8, num_updates=55800, lr=0.000149671, gnorm=4.116, clip=1, loss_scale=256, train_wall=102, wall=59296
2022-10-12 23:28:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 23:29:48 | INFO | train_inner | epoch 122:    440 / 459 loss=6.221, nll_loss=2.939, mask_ins=1.081, word_ins_ml=4.563, word_reposition=0.577, ppl=74.6, wps=14273.3, ups=0.96, wpb=14824.5, bsz=449.5, num_updates=55900, lr=0.000149537, gnorm=5.624, clip=1, loss_scale=189, train_wall=103, wall=59400
2022-10-12 23:30:07 | INFO | train | epoch 122 | loss 6.229 | nll_loss 2.937 | mask_ins 1.084 | word_ins_ml 4.562 | word_reposition 0.584 | ppl 75.03 | wps 13881.6 | ups 0.95 | wpb 14642.5 | bsz 444.1 | num_updates 55919 | lr 0.000149512 | gnorm 4.362 | clip 1.1 | loss_scale 236 | train_wall 467 | wall 59419
2022-10-12 23:30:16 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 14.924 | nll_loss 10.96 | mask_ins 1.92 | word_ins_ml 11.803 | word_reposition 1.202 | ppl 31096.5 | wps 35048 | wpb 1628.7 | bsz 55.8 | num_updates 55919 | best_loss 13.037
2022-10-12 23:30:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 122 @ 55919 updates, score 14.924) (writing took 2.8357967310003005 seconds)
2022-10-12 23:31:42 | INFO | train_inner | epoch 123:     81 / 459 loss=6.231, nll_loss=2.929, mask_ins=1.089, word_ins_ml=4.555, word_reposition=0.586, ppl=75.11, wps=12769.6, ups=0.88, wpb=14568.6, bsz=441.9, num_updates=56000, lr=0.000149404, gnorm=5.749, clip=2, loss_scale=128, train_wall=101, wall=59514
2022-10-12 23:33:25 | INFO | train_inner | epoch 123:    181 / 459 loss=6.235, nll_loss=2.93, mask_ins=1.095, word_ins_ml=4.556, word_reposition=0.584, ppl=75.33, wps=14485.3, ups=0.97, wpb=14939.4, bsz=454.6, num_updates=56100, lr=0.00014927, gnorm=3.274, clip=0, loss_scale=128, train_wall=102, wall=59617
2022-10-12 23:35:08 | INFO | train_inner | epoch 123:    281 / 459 loss=6.242, nll_loss=2.95, mask_ins=1.081, word_ins_ml=4.573, word_reposition=0.588, ppl=75.67, wps=14214.2, ups=0.97, wpb=14612.9, bsz=443, num_updates=56200, lr=0.000149137, gnorm=2.893, clip=1, loss_scale=128, train_wall=102, wall=59720
2022-10-12 23:36:51 | INFO | train_inner | epoch 123:    381 / 459 loss=6.251, nll_loss=2.944, mask_ins=1.096, word_ins_ml=4.567, word_reposition=0.588, ppl=76.16, wps=14165.4, ups=0.97, wpb=14667.5, bsz=444.4, num_updates=56300, lr=0.000149005, gnorm=3.011, clip=0, loss_scale=128, train_wall=103, wall=59824
2022-10-12 23:38:11 | INFO | train | epoch 123 | loss 6.238 | nll_loss 2.939 | mask_ins 1.088 | word_ins_ml 4.564 | word_reposition 0.586 | ppl 75.46 | wps 13891.7 | ups 0.95 | wpb 14637.3 | bsz 443.9 | num_updates 56378 | lr 0.000148902 | gnorm 3.546 | clip 0.7 | loss_scale 133 | train_wall 467 | wall 59903
2022-10-12 23:38:20 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 15.005 | nll_loss 10.973 | mask_ins 1.898 | word_ins_ml 11.82 | word_reposition 1.288 | ppl 32881.5 | wps 34808.9 | wpb 1628.7 | bsz 55.8 | num_updates 56378 | best_loss 13.037
2022-10-12 23:38:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 123 @ 56378 updates, score 15.005) (writing took 2.9547390649968293 seconds)
2022-10-12 23:38:46 | INFO | train_inner | epoch 124:     22 / 459 loss=6.217, nll_loss=2.93, mask_ins=1.078, word_ins_ml=4.556, word_reposition=0.583, ppl=74.36, wps=12352, ups=0.88, wpb=14110.8, bsz=426.6, num_updates=56400, lr=0.000148873, gnorm=3.111, clip=0, loss_scale=180, train_wall=101, wall=59938
2022-10-12 23:40:29 | INFO | train_inner | epoch 124:    122 / 459 loss=6.168, nll_loss=2.889, mask_ins=1.076, word_ins_ml=4.52, word_reposition=0.572, ppl=71.89, wps=14197.1, ups=0.97, wpb=14632, bsz=441.7, num_updates=56500, lr=0.000148741, gnorm=2.793, clip=0, loss_scale=256, train_wall=102, wall=60041
2022-10-12 23:41:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 23:42:12 | INFO | train_inner | epoch 124:    223 / 459 loss=6.203, nll_loss=2.906, mask_ins=1.081, word_ins_ml=4.535, word_reposition=0.587, ppl=73.67, wps=14265, ups=0.96, wpb=14796.8, bsz=447.3, num_updates=56600, lr=0.00014861, gnorm=3.828, clip=1, loss_scale=174, train_wall=103, wall=60145
2022-10-12 23:43:55 | INFO | train_inner | epoch 124:    323 / 459 loss=6.229, nll_loss=2.933, mask_ins=1.093, word_ins_ml=4.558, word_reposition=0.578, ppl=75.02, wps=14312, ups=0.98, wpb=14674.9, bsz=447.4, num_updates=56700, lr=0.000148478, gnorm=3.168, clip=2, loss_scale=128, train_wall=102, wall=60247
2022-10-12 23:45:38 | INFO | train_inner | epoch 124:    423 / 459 loss=6.271, nll_loss=2.979, mask_ins=1.092, word_ins_ml=4.598, word_reposition=0.581, ppl=77.24, wps=14261.3, ups=0.97, wpb=14713.9, bsz=446, num_updates=56800, lr=0.000148348, gnorm=2.454, clip=0, loss_scale=128, train_wall=102, wall=60350
2022-10-12 23:46:15 | INFO | train | epoch 124 | loss 6.215 | nll_loss 2.924 | mask_ins 1.085 | word_ins_ml 4.55 | word_reposition 0.58 | ppl 74.27 | wps 13849.3 | ups 0.95 | wpb 14641.8 | bsz 444 | num_updates 56836 | lr 0.000148301 | gnorm 3.031 | clip 0.7 | loss_scale 172 | train_wall 468 | wall 60387
2022-10-12 23:46:24 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 14.897 | nll_loss 10.972 | mask_ins 1.925 | word_ins_ml 11.81 | word_reposition 1.162 | ppl 30518.3 | wps 35132.6 | wpb 1628.7 | bsz 55.8 | num_updates 56836 | best_loss 13.037
2022-10-12 23:46:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 124 @ 56836 updates, score 14.897) (writing took 2.8330328649899457 seconds)
2022-10-12 23:47:32 | INFO | train_inner | epoch 125:     64 / 459 loss=6.202, nll_loss=2.904, mask_ins=1.087, word_ins_ml=4.533, word_reposition=0.582, ppl=73.6, wps=12544.8, ups=0.88, wpb=14290.3, bsz=432.7, num_updates=56900, lr=0.000148217, gnorm=2.439, clip=0, loss_scale=128, train_wall=101, wall=60464
2022-10-12 23:49:13 | INFO | train_inner | epoch 125:    164 / 459 loss=6.19, nll_loss=2.899, mask_ins=1.085, word_ins_ml=4.528, word_reposition=0.577, ppl=73.01, wps=13909.1, ups=0.99, wpb=14104.7, bsz=425.8, num_updates=57000, lr=0.000148087, gnorm=2.356, clip=0, loss_scale=128, train_wall=101, wall=60566
2022-10-12 23:50:57 | INFO | train_inner | epoch 125:    264 / 459 loss=6.232, nll_loss=2.926, mask_ins=1.097, word_ins_ml=4.552, word_reposition=0.583, ppl=75.18, wps=14369.7, ups=0.97, wpb=14853.3, bsz=450.4, num_updates=57100, lr=0.000147957, gnorm=2.73, clip=0, loss_scale=196, train_wall=102, wall=60669
2022-10-12 23:52:40 | INFO | train_inner | epoch 125:    364 / 459 loss=6.219, nll_loss=2.931, mask_ins=1.077, word_ins_ml=4.556, word_reposition=0.586, ppl=74.49, wps=14460.9, ups=0.97, wpb=14901.9, bsz=451.7, num_updates=57200, lr=0.000147828, gnorm=5.556, clip=5, loss_scale=256, train_wall=102, wall=60772
2022-10-12 23:53:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-12 23:54:19 | INFO | train | epoch 125 | loss 6.212 | nll_loss 2.92 | mask_ins 1.085 | word_ins_ml 4.546 | word_reposition 0.58 | ppl 74.14 | wps 13857.5 | ups 0.95 | wpb 14633.7 | bsz 443.8 | num_updates 57294 | lr 0.000147707 | gnorm 4.003 | clip 2 | loss_scale 180 | train_wall 467 | wall 60871
2022-10-12 23:54:28 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 14.937 | nll_loss 10.964 | mask_ins 1.91 | word_ins_ml 11.809 | word_reposition 1.218 | ppl 31372.3 | wps 35088.9 | wpb 1628.7 | bsz 55.8 | num_updates 57294 | best_loss 13.037
2022-10-12 23:54:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 125 @ 57294 updates, score 14.937) (writing took 2.9585201499867253 seconds)
2022-10-12 23:54:37 | INFO | train_inner | epoch 126:      6 / 459 loss=6.211, nll_loss=2.939, mask_ins=1.074, word_ins_ml=4.563, word_reposition=0.574, ppl=74.09, wps=12707.2, ups=0.85, wpb=14889.1, bsz=454.7, num_updates=57300, lr=0.000147699, gnorm=6.316, clip=4, loss_scale=169, train_wall=104, wall=60889
2022-10-12 23:56:20 | INFO | train_inner | epoch 126:    106 / 459 loss=6.178, nll_loss=2.89, mask_ins=1.076, word_ins_ml=4.521, word_reposition=0.582, ppl=72.41, wps=14026.3, ups=0.97, wpb=14462.9, bsz=435.2, num_updates=57400, lr=0.00014757, gnorm=6.981, clip=3, loss_scale=128, train_wall=102, wall=60992
2022-10-12 23:58:03 | INFO | train_inner | epoch 126:    206 / 459 loss=6.166, nll_loss=2.876, mask_ins=1.078, word_ins_ml=4.509, word_reposition=0.578, ppl=71.82, wps=14194.5, ups=0.97, wpb=14643.8, bsz=443.4, num_updates=57500, lr=0.000147442, gnorm=6.761, clip=2, loss_scale=128, train_wall=102, wall=61096
2022-10-12 23:59:46 | INFO | train_inner | epoch 126:    306 / 459 loss=6.181, nll_loss=2.896, mask_ins=1.069, word_ins_ml=4.525, word_reposition=0.587, ppl=72.58, wps=14203.4, ups=0.97, wpb=14611.5, bsz=445.8, num_updates=57600, lr=0.000147314, gnorm=7.387, clip=7, loss_scale=128, train_wall=102, wall=61199
2022-10-13 00:01:30 | INFO | train_inner | epoch 126:    406 / 459 loss=6.219, nll_loss=2.94, mask_ins=1.078, word_ins_ml=4.564, word_reposition=0.577, ppl=74.48, wps=14408.2, ups=0.97, wpb=14898.9, bsz=452.6, num_updates=57700, lr=0.000147186, gnorm=6.51, clip=4, loss_scale=128, train_wall=102, wall=61302
2022-10-13 00:02:24 | INFO | train | epoch 126 | loss 6.188 | nll_loss 2.903 | mask_ins 1.075 | word_ins_ml 4.532 | word_reposition 0.581 | ppl 72.91 | wps 13847.2 | ups 0.95 | wpb 14640.6 | bsz 444.1 | num_updates 57753 | lr 0.000147119 | gnorm 6.376 | clip 3.5 | loss_scale 131 | train_wall 469 | wall 61356
2022-10-13 00:02:33 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 15.079 | nll_loss 10.994 | mask_ins 1.929 | word_ins_ml 11.833 | word_reposition 1.317 | ppl 34605.5 | wps 35081.1 | wpb 1628.7 | bsz 55.8 | num_updates 57753 | best_loss 13.037
2022-10-13 00:02:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 126 @ 57753 updates, score 15.079) (writing took 2.9494927549967542 seconds)
2022-10-13 00:03:25 | INFO | train_inner | epoch 127:     47 / 459 loss=6.201, nll_loss=2.917, mask_ins=1.076, word_ins_ml=4.543, word_reposition=0.582, ppl=73.55, wps=12892.1, ups=0.87, wpb=14838.3, bsz=452.1, num_updates=57800, lr=0.000147059, gnorm=2.731, clip=0, loss_scale=201, train_wall=102, wall=61417
2022-10-13 00:05:07 | INFO | train_inner | epoch 127:    147 / 459 loss=6.159, nll_loss=2.875, mask_ins=1.078, word_ins_ml=4.508, word_reposition=0.574, ppl=71.48, wps=14257.2, ups=0.98, wpb=14572.5, bsz=445, num_updates=57900, lr=0.000146932, gnorm=2.83, clip=0, loss_scale=256, train_wall=101, wall=61519
2022-10-13 00:06:50 | INFO | train_inner | epoch 127:    247 / 459 loss=6.186, nll_loss=2.896, mask_ins=1.086, word_ins_ml=4.525, word_reposition=0.575, ppl=72.82, wps=14333.4, ups=0.97, wpb=14737.9, bsz=445.3, num_updates=58000, lr=0.000146805, gnorm=3.156, clip=0, loss_scale=256, train_wall=102, wall=61622
2022-10-13 00:08:32 | INFO | train_inner | epoch 127:    347 / 459 loss=6.207, nll_loss=2.924, mask_ins=1.077, word_ins_ml=4.55, word_reposition=0.581, ppl=73.9, wps=14337.5, ups=0.98, wpb=14647.7, bsz=443.8, num_updates=58100, lr=0.000146679, gnorm=3.176, clip=0, loss_scale=256, train_wall=101, wall=61724
2022-10-13 00:10:14 | INFO | train_inner | epoch 127:    447 / 459 loss=6.231, nll_loss=2.935, mask_ins=1.092, word_ins_ml=4.559, word_reposition=0.58, ppl=75.09, wps=14237.4, ups=0.98, wpb=14583.7, bsz=440.8, num_updates=58200, lr=0.000146553, gnorm=3.034, clip=0, loss_scale=256, train_wall=102, wall=61827
2022-10-13 00:10:26 | INFO | train | epoch 127 | loss 6.193 | nll_loss 2.905 | mask_ins 1.082 | word_ins_ml 4.533 | word_reposition 0.578 | ppl 73.14 | wps 13941.4 | ups 0.95 | wpb 14644.1 | bsz 444.1 | num_updates 58212 | lr 0.000146537 | gnorm 3.064 | clip 0 | loss_scale 256 | train_wall 466 | wall 61838
2022-10-13 00:10:35 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 14.988 | nll_loss 11.025 | mask_ins 1.9 | word_ins_ml 11.862 | word_reposition 1.226 | ppl 32505.1 | wps 35019.5 | wpb 1628.7 | bsz 55.8 | num_updates 58212 | best_loss 13.037
2022-10-13 00:10:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 127 @ 58212 updates, score 14.988) (writing took 2.8611037130176555 seconds)
2022-10-13 00:12:09 | INFO | train_inner | epoch 128:     88 / 459 loss=6.105, nll_loss=2.834, mask_ins=1.069, word_ins_ml=4.471, word_reposition=0.565, ppl=68.82, wps=12640.3, ups=0.87, wpb=14482.4, bsz=443.7, num_updates=58300, lr=0.000146427, gnorm=3.092, clip=0, loss_scale=371, train_wall=102, wall=61941
2022-10-13 00:13:51 | INFO | train_inner | epoch 128:    188 / 459 loss=6.205, nll_loss=2.909, mask_ins=1.081, word_ins_ml=4.537, word_reposition=0.588, ppl=73.79, wps=14276.2, ups=0.98, wpb=14571.6, bsz=438.2, num_updates=58400, lr=0.000146301, gnorm=3.416, clip=0, loss_scale=512, train_wall=101, wall=62043
2022-10-13 00:15:34 | INFO | train_inner | epoch 128:    288 / 459 loss=6.172, nll_loss=2.886, mask_ins=1.074, word_ins_ml=4.516, word_reposition=0.582, ppl=72.11, wps=14248.7, ups=0.97, wpb=14712, bsz=446.5, num_updates=58500, lr=0.000146176, gnorm=3.037, clip=0, loss_scale=512, train_wall=102, wall=62147
2022-10-13 00:16:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-13 00:17:18 | INFO | train_inner | epoch 128:    389 / 459 loss=6.178, nll_loss=2.904, mask_ins=1.07, word_ins_ml=4.532, word_reposition=0.576, ppl=72.42, wps=14183.7, ups=0.96, wpb=14708.3, bsz=444.4, num_updates=58600, lr=0.000146052, gnorm=2.986, clip=0, loss_scale=342, train_wall=103, wall=62250
2022-10-13 00:18:30 | INFO | train | epoch 128 | loss 6.171 | nll_loss 2.889 | mask_ins 1.074 | word_ins_ml 4.519 | word_reposition 0.578 | ppl 72.06 | wps 13861.4 | ups 0.95 | wpb 14638.7 | bsz 443.9 | num_updates 58670 | lr 0.000145964 | gnorm 3.141 | clip 0 | loss_scale 412 | train_wall 467 | wall 62322
2022-10-13 00:18:39 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 15.06 | nll_loss 10.956 | mask_ins 1.945 | word_ins_ml 11.799 | word_reposition 1.315 | ppl 34159.8 | wps 35008 | wpb 1628.7 | bsz 55.8 | num_updates 58670 | best_loss 13.037
2022-10-13 00:18:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 128 @ 58670 updates, score 15.06) (writing took 2.9941010260081384 seconds)
2022-10-13 00:19:13 | INFO | train_inner | epoch 129:     30 / 459 loss=6.21, nll_loss=2.912, mask_ins=1.088, word_ins_ml=4.539, word_reposition=0.582, ppl=74.01, wps=12667.7, ups=0.87, wpb=14510.6, bsz=439, num_updates=58700, lr=0.000145927, gnorm=3.326, clip=0, loss_scale=256, train_wall=101, wall=62365
2022-10-13 00:19:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-13 00:20:56 | INFO | train_inner | epoch 129:    131 / 459 loss=6.153, nll_loss=2.874, mask_ins=1.071, word_ins_ml=4.507, word_reposition=0.576, ppl=71.17, wps=14315.7, ups=0.97, wpb=14789.7, bsz=446.5, num_updates=58800, lr=0.000145803, gnorm=3.728, clip=1, loss_scale=128, train_wall=102, wall=62468
2022-10-13 00:22:39 | INFO | train_inner | epoch 129:    231 / 459 loss=6.178, nll_loss=2.891, mask_ins=1.081, word_ins_ml=4.521, word_reposition=0.577, ppl=72.42, wps=14327.8, ups=0.97, wpb=14713.6, bsz=447.1, num_updates=58900, lr=0.000145679, gnorm=3.677, clip=1, loss_scale=128, train_wall=102, wall=62571
2022-10-13 00:24:21 | INFO | train_inner | epoch 129:    331 / 459 loss=6.169, nll_loss=2.883, mask_ins=1.076, word_ins_ml=4.515, word_reposition=0.578, ppl=71.93, wps=14223.5, ups=0.97, wpb=14596, bsz=442.7, num_updates=59000, lr=0.000145556, gnorm=4, clip=1, loss_scale=128, train_wall=102, wall=62673
2022-10-13 00:26:04 | INFO | train_inner | epoch 129:    431 / 459 loss=6.131, nll_loss=2.859, mask_ins=1.061, word_ins_ml=4.493, word_reposition=0.577, ppl=70.09, wps=14351, ups=0.97, wpb=14798.5, bsz=451, num_updates=59100, lr=0.000145432, gnorm=2.719, clip=0, loss_scale=128, train_wall=102, wall=62777
2022-10-13 00:26:33 | INFO | train | epoch 129 | loss 6.161 | nll_loss 2.877 | mask_ins 1.074 | word_ins_ml 4.509 | word_reposition 0.577 | ppl 71.55 | wps 13891.7 | ups 0.95 | wpb 14641.4 | bsz 444 | num_updates 59128 | lr 0.000145398 | gnorm 3.591 | clip 0.9 | loss_scale 136 | train_wall 466 | wall 62805
2022-10-13 00:26:42 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 15.153 | nll_loss 11.117 | mask_ins 1.93 | word_ins_ml 11.948 | word_reposition 1.274 | ppl 36430.6 | wps 35000.2 | wpb 1628.7 | bsz 55.8 | num_updates 59128 | best_loss 13.037
2022-10-13 00:26:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 129 @ 59128 updates, score 15.153) (writing took 3.0187599409837276 seconds)
2022-10-13 00:27:59 | INFO | train_inner | epoch 130:     72 / 459 loss=6.103, nll_loss=2.829, mask_ins=1.063, word_ins_ml=4.467, word_reposition=0.573, ppl=68.72, wps=12284.2, ups=0.87, wpb=14043.6, bsz=427.9, num_updates=59200, lr=0.00014531, gnorm=3.547, clip=2, loss_scale=128, train_wall=101, wall=62891
2022-10-13 00:29:43 | INFO | train_inner | epoch 130:    172 / 459 loss=6.163, nll_loss=2.879, mask_ins=1.075, word_ins_ml=4.51, word_reposition=0.578, ppl=71.66, wps=14501.3, ups=0.96, wpb=15119.6, bsz=456.2, num_updates=59300, lr=0.000145187, gnorm=2.995, clip=0, loss_scale=242, train_wall=103, wall=62995
2022-10-13 00:31:25 | INFO | train_inner | epoch 130:    272 / 459 loss=6.166, nll_loss=2.88, mask_ins=1.082, word_ins_ml=4.511, word_reposition=0.573, ppl=71.79, wps=14359.9, ups=0.98, wpb=14605.2, bsz=441.6, num_updates=59400, lr=0.000145065, gnorm=2.474, clip=0, loss_scale=256, train_wall=101, wall=63097
2022-10-13 00:33:08 | INFO | train_inner | epoch 130:    372 / 459 loss=6.141, nll_loss=2.867, mask_ins=1.069, word_ins_ml=4.499, word_reposition=0.573, ppl=70.58, wps=14259.2, ups=0.97, wpb=14717.1, bsz=449.2, num_updates=59500, lr=0.000144943, gnorm=4.592, clip=4, loss_scale=256, train_wall=102, wall=63200
2022-10-13 00:34:36 | INFO | train | epoch 130 | loss 6.15 | nll_loss 2.868 | mask_ins 1.073 | word_ins_ml 4.501 | word_reposition 0.576 | ppl 71 | wps 13893.9 | ups 0.95 | wpb 14640.5 | bsz 444 | num_updates 59587 | lr 0.000144837 | gnorm 3.311 | clip 1.5 | loss_scale 233 | train_wall 467 | wall 63288
2022-10-13 00:34:45 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 15.074 | nll_loss 11.088 | mask_ins 1.887 | word_ins_ml 11.92 | word_reposition 1.268 | ppl 34502.1 | wps 35050.1 | wpb 1628.7 | bsz 55.8 | num_updates 59587 | best_loss 13.037
2022-10-13 00:34:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 130 @ 59587 updates, score 15.074) (writing took 2.896033772994997 seconds)
2022-10-13 00:35:02 | INFO | train_inner | epoch 131:     13 / 459 loss=6.162, nll_loss=2.876, mask_ins=1.071, word_ins_ml=4.507, word_reposition=0.583, ppl=71.6, wps=12649.4, ups=0.88, wpb=14397.8, bsz=435.4, num_updates=59600, lr=0.000144821, gnorm=3.467, clip=2, loss_scale=256, train_wall=101, wall=63314
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
