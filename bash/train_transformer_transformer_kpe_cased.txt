nohup: ignoring input
2022-08-15 15:13:25 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:17446
2022-08-15 15:13:25 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:17446
2022-08-15 15:13:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-08-15 15:13:25 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:17446
2022-08-15 15:13:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-08-15 15:13:25 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:17446
2022-08-15 15:13:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-08-15 15:13:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-08-15 15:13:25 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-15 15:13:25 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-08-15 15:13:25 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-15 15:13:25 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-08-15 15:13:25 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-15 15:13:25 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-08-15 15:13:25 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-15 15:13:25 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-08-15 15:13:31 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:17446', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_transformer_transformer_kpe_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='6,6,6', layers_num='6,6,6', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='transformer', decoder='transformer', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-08-15 15:13:31 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-08-15 15:13:31 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
start load cached examples valid ...
0it [00:00, ?it/s]2022-08-15 15:13:31 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.source
2022-08-15 15:13:31 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.target
2022-08-15 15:13:31 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]385it [00:00, 3843.63it/s]362it [00:00, 3607.89it/s]383it [00:00, 3822.89it/s]368it [00:00, 3677.50it/s]723it [00:00, 3355.06it/s]770it [00:00, 3449.41it/s]766it [00:00, 3446.09it/s]736it [00:00, 3341.10it/s]1060it [00:00, 3342.56it/s]1132it [00:00, 3519.42it/s]1122it [00:00, 3492.05it/s]1097it [00:00, 3455.33it/s]1396it [00:00, 3224.09it/s]1486it [00:00, 3359.98it/s]1473it [00:00, 3349.49it/s]1445it [00:00, 3271.83it/s]1749it [00:00, 3329.23it/s]1871it [00:00, 3524.47it/s]1852it [00:00, 3499.58it/s]1828it [00:00, 3460.63it/s]2125it [00:00, 3468.64it/s]2199it [00:00, 3539.81it/s]2236it [00:00, 3417.74it/s]2221it [00:00, 3416.74it/s]2473it [00:00, 3393.55it/s]2627it [00:00, 3569.11it/s]2605it [00:00, 3547.27it/s]2555it [00:00, 3464.60it/s]2837it [00:00, 3469.38it/s]3019it [00:00, 3674.97it/s]2991it [00:00, 3638.33it/s]2950it [00:00, 3610.99it/s]3185it [00:00, 3385.10it/s]3389it [00:00, 3538.00it/s]3357it [00:00, 3517.45it/s]3313it [00:00, 3470.00it/s]3543it [00:01, 3443.14it/s]3775it [00:01, 3632.54it/s]3733it [00:01, 3587.14it/s]3698it [00:01, 3581.87it/s]3916it [00:01, 3365.51it/s]4094it [00:01, 3488.68it/s]4141it [00:01, 3501.20it/s]4059it [00:01, 3442.83it/s]4272it [00:01, 3420.03it/s]4517it [00:01, 3574.67it/s]4455it [00:01, 3508.58it/s]4438it [00:01, 3543.02it/s]4638it [00:01, 3489.97it/s]4877it [00:01, 3442.95it/s]4807it [00:01, 3369.30it/s]4795it [00:01, 3394.50it/s]4988it [00:01, 3311.47it/s]5248it [00:01, 3517.55it/s]5172it [00:01, 3447.80it/s]5171it [00:01, 3497.69it/s]5351it [00:01, 3401.57it/s]5536it [00:01, 3501.65it/s]5524it [00:01, 3505.26it/s]5602it [00:01, 3335.66it/s]5694it [00:01, 3292.25it/s]5949it [00:01, 3372.98it/s]5888it [00:01, 3314.26it/s]5877it [00:01, 3232.83it/s]6026it [00:01, 3290.95it/s]6291it [00:01, 3386.13it/s]6224it [00:01, 3325.42it/s]6218it [00:01, 3281.62it/s]6365it [00:01, 3318.06it/s]6559it [00:02, 2065.52it/s]6632it [00:02, 1886.88it/s]6894it [00:02, 2325.49it/s]6550it [00:02, 1664.98it/s]6943it [00:02, 2116.32it/s]6698it [00:02, 1785.52it/s]7230it [00:02, 2556.71it/s]6885it [00:02, 1951.03it/s]7276it [00:02, 2366.39it/s]6957it [00:02, 1920.13it/s]7532it [00:02, 2543.49it/s]7201it [00:02, 2186.19it/s]7213it [00:02, 2043.29it/s]7570it [00:02, 2410.89it/s]7821it [00:02, 2626.10it/s]7491it [00:02, 2278.11it/s]7867it [00:02, 2545.20it/s]7468it [00:02, 2008.07it/s]8128it [00:02, 2740.57it/s]7803it [00:02, 2473.07it/s]8154it [00:02, 2561.64it/s]7778it [00:02, 2260.96it/s]8421it [00:02, 2723.63it/s]8124it [00:02, 2656.70it/s]8433it [00:02, 2555.31it/s]8124it [00:02, 2558.79it/s]8753it [00:02, 2886.00it/s]8423it [00:02, 2724.17it/s]8758it [00:02, 2741.45it/s]8409it [00:02, 2629.84it/s]9053it [00:02, 2876.39it/s]8776it [00:02, 2942.05it/s]9049it [00:03, 2786.25it/s]8753it [00:03, 2848.15it/s]9386it [00:03, 3004.78it/s]9090it [00:03, 2942.24it/s]9392it [00:03, 2968.47it/s]9055it [00:03, 2864.48it/s]9720it [00:03, 3099.43it/s]9443it [00:03, 3105.52it/s]9748it [00:03, 3137.61it/s]9396it [00:03, 3018.93it/s]10035it [00:03, 3051.40it/s]9796it [00:03, 3217.59it/s]10068it [00:03, 3051.79it/s]9742it [00:03, 3144.22it/s]10357it [00:03, 3099.68it/s]10126it [00:03, 3133.71it/s]10417it [00:03, 3177.49it/s]10064it [00:03, 3071.91it/s]10691it [00:03, 3015.01it/s]10476it [00:03, 3237.17it/s]10408it [00:03, 3176.36it/s]10739it [00:03, 3076.13it/s]11011it [00:03, 3067.29it/s]10805it [00:03, 3139.39it/s]11086it [00:03, 3188.37it/s]10730it [00:03, 3090.71it/s]11354it [00:03, 3171.88it/s]11152it [00:03, 3233.36it/s]11428it [00:03, 3255.34it/s]11075it [00:03, 3191.80it/s]11673it [00:03, 3054.01it/s]11503it [00:03, 3313.13it/s]11421it [00:03, 3267.97it/s]11756it [00:03, 3149.59it/s]12020it [00:03, 3172.12it/s]11837it [00:03, 3193.54it/s]12101it [00:03, 3235.37it/s]11750it [00:03, 3166.29it/s]12365it [00:03, 3251.08it/s]12188it [00:04, 3281.91it/s]12095it [00:04, 3246.21it/s]12427it [00:04, 3141.79it/s]12692it [00:04, 3091.13it/s]12519it [00:04, 3177.14it/s]12764it [00:04, 3205.11it/s]12422it [00:04, 3113.29it/s]13035it [00:04, 3185.75it/s]12859it [00:04, 3240.09it/s]13115it [00:04, 3292.14it/s]12776it [00:04, 3233.32it/s]13356it [00:04, 3082.15it/s]13368it [00:04, 3090.41it/s]
2022-08-15 15:13:35 | INFO | root | success load 13368 data
2022-08-15 15:13:35 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-08-15 15:13:35 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-08-15 15:13:35 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-08-15 15:13:35 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-08-15 15:13:35 | INFO | transformer.tokenization_utils | loading file None
2022-08-15 15:13:35 | INFO | transformer.tokenization_utils | loading file None
2022-08-15 15:13:35 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
13198it [00:04, 3281.53it/s]13368it [00:04, 3067.73it/s]
13119it [00:04, 3289.12it/s]13368it [00:04, 3037.54it/s]
13368it [00:04, 2992.23it/s]
2022-08-15 15:13:36 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-15 15:13:36 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

Trained parameters: len 418
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 298
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 418
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 298
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 418
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 298
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
2022-08-15 15:13:38 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): EditorTransformerEncoder(
    (embed_tokens): Embedding(28996, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): EditorTransformerDecoder(
    (embed_tokens): Embedding(28996, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
    (embed_mask_ins): Embedding(256, 1536)
    (layers_reposition): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-08-15 15:13:38 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-08-15 15:13:38 | INFO | fairseq_cli.train | num. model params: 151463939 (num. trained: 151463939)
2022-08-15 15:13:38 | INFO | fairseq_cli.train | num. Encoder model params: 56142083 (Encoder num. trained: 56142083)
2022-08-15 15:13:38 | INFO | fairseq_cli.train | num. Decoder model params: 117590784 (Decoder num. trained: 117590784)
Trained parameters: len 418
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 298
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
start load cached examples train ...
0it [00:00, ?it/s]310it [00:00, 3091.21it/s]639it [00:00, 3202.66it/s]960it [00:00, 2742.25it/s]1281it [00:00, 2907.70it/s]1594it [00:00, 2980.51it/s]1897it [00:00, 2852.30it/s]2206it [00:00, 2924.74it/s]2502it [00:00, 2034.60it/s]2742it [00:01, 1774.19it/s]2948it [00:01, 1682.43it/s]3150it [00:01, 1701.66it/s]3334it [00:01, 1588.10it/s]3651it [00:01, 1958.27it/s]3943it [00:01, 2198.29it/s]4192it [00:01, 2274.25it/s]4527it [00:01, 2568.35it/s]2022-08-15 15:13:40 | INFO | fairseq_cli.train | training on 4 GPUs
2022-08-15 15:13:40 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-08-15 15:13:40 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_transformer_transformer_kpe_cased/checkpoint_last.pt
2022-08-15 15:13:40 | INFO | fairseq.trainer | loading train data for epoch 1
4857it [00:02, 2772.05it/s]2022-08-15 15:13:41 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.source
2022-08-15 15:13:41 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.target
2022-08-15 15:13:41 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]
start load cached examples train ...
0it [00:00, ?it/s]5144it [00:02, 2782.54it/s]372it [00:00, 3710.00it/s]
start load cached examples train ...
0it [00:00, ?it/s]371it [00:00, 3705.56it/s]5481it [00:02, 2950.18it/s]743it [00:00, 3698.32it/s]358it [00:00, 3559.70it/s]742it [00:00, 3672.52it/s]5818it [00:02, 2969.68it/s]1113it [00:00, 3448.14it/s]716it [00:00, 3571.33it/s]1110it [00:00, 3436.50it/s]6173it [00:02, 3134.85it/s]1479it [00:00, 3527.91it/s]1074it [00:00, 3337.80it/s]1472it [00:00, 3502.36it/s]6527it [00:02, 3251.04it/s]1834it [00:00, 3367.46it/s]1432it [00:00, 3424.24it/s]1824it [00:00, 3413.14it/s]6855it [00:02, 3182.08it/s]2206it [00:00, 3478.25it/s]2187it [00:00, 3481.04it/s]1776it [00:00, 3292.37it/s]7211it [00:02, 3290.20it/s]2556it [00:00, 3376.85it/s]2139it [00:00, 3401.33it/s]2537it [00:00, 3363.48it/s]7542it [00:02, 3184.61it/s]2913it [00:00, 3435.27it/s]2901it [00:00, 3446.41it/s]2481it [00:00, 3271.94it/s]7897it [00:03, 3289.78it/s]3288it [00:00, 3527.78it/s]3267it [00:00, 3510.91it/s]2844it [00:00, 3378.29it/s]8252it [00:03, 3363.41it/s]3642it [00:01, 3358.89it/s]3202it [00:00, 3437.95it/s]3620it [00:01, 3377.04it/s]4005it [00:01, 3435.65it/s]3987it [00:01, 3460.58it/s]3548it [00:01, 3321.92it/s]4351it [00:01, 3356.19it/s]3899it [00:01, 3376.43it/s]4335it [00:01, 3359.43it/s]4701it [00:01, 3395.70it/s]4695it [00:01, 3426.49it/s]4238it [00:01, 3282.84it/s]5042it [00:01, 3313.50it/s]4591it [00:01, 3354.54it/s]5039it [00:01, 3284.07it/s]5413it [00:01, 3426.08it/s]4959it [00:01, 3447.78it/s]5392it [00:01, 3352.63it/s]5762it [00:01, 3444.34it/s]5754it [00:01, 3428.63it/s]5305it [00:01, 3308.63it/s]6108it [00:01, 3357.36it/s]5648it [00:01, 3342.60it/s]6099it [00:01, 3351.56it/s]6479it [00:01, 3459.60it/s]6464it [00:01, 3436.47it/s]5984it [00:01, 3094.81it/s]6826it [00:01, 3352.18it/s]6809it [00:01, 3355.34it/s]8590it [00:04, 906.25it/s] 6318it [00:01, 3161.82it/s]7180it [00:02, 3405.23it/s]7146it [00:02, 3349.98it/s]8893it [00:04, 1121.74it/s]6638it [00:02, 3117.81it/s]7522it [00:02, 3205.30it/s]9191it [00:04, 1357.03it/s]7482it [00:02, 3248.50it/s]6953it [00:02, 2955.76it/s]7846it [00:02, 3027.94it/s]9465it [00:04, 1560.05it/s]7808it [00:02, 3090.09it/s]7252it [00:02, 2925.69it/s]8169it [00:02, 3083.27it/s]9781it [00:04, 1844.96it/s]8136it [00:02, 3142.84it/s]7547it [00:02, 2853.14it/s]10087it [00:04, 2068.93it/s]7911it [00:02, 3072.54it/s]10428it [00:04, 2367.95it/s]8277it [00:02, 3240.22it/s]10781it [00:04, 2648.39it/s]11099it [00:04, 2667.20it/s]11474it [00:05, 2946.86it/s]11799it [00:05, 2928.99it/s]12173it [00:05, 3149.06it/s]12552it [00:05, 3325.93it/s]8481it [00:03, 969.51it/s] 8452it [00:03, 967.61it/s] 12898it [00:05, 3211.46it/s]8849it [00:03, 1272.87it/s]8821it [00:03, 1271.48it/s]13283it [00:05, 3388.70it/s]9210it [00:03, 1595.21it/s]9161it [00:03, 1564.35it/s]13630it [00:05, 3276.37it/s]9515it [00:03, 1834.67it/s]9472it [00:03, 1818.15it/s]8604it [00:03, 897.50it/s] 14007it [00:05, 3413.49it/s]9887it [00:03, 2193.40it/s]9843it [00:03, 2177.23it/s]8970it [00:03, 1181.48it/s]14354it [00:05, 3350.83it/s]10209it [00:03, 2380.35it/s]10163it [00:03, 2311.03it/s]9247it [00:03, 1382.27it/s]14708it [00:06, 3402.54it/s]10582it [00:03, 2692.38it/s]10539it [00:03, 2639.87it/s]9616it [00:03, 1738.52it/s]15092it [00:06, 3528.88it/s]10926it [00:04, 2789.35it/s]10904it [00:04, 2887.32it/s]9986it [00:03, 2092.38it/s]11298it [00:04, 3025.64it/s]15448it [00:06, 3349.50it/s]11243it [00:04, 2961.68it/s]10310it [00:04, 2300.96it/s]11654it [00:04, 3167.72it/s]15823it [00:06, 3460.75it/s]11612it [00:04, 3154.82it/s]10629it [00:04, 2499.36it/s]11998it [00:04, 3123.02it/s]16173it [00:06, 3272.50it/s]11955it [00:04, 3122.86it/s]10948it [00:04, 2632.95it/s]12382it [00:04, 3318.77it/s]16553it [00:06, 3419.18it/s]12332it [00:04, 3300.08it/s]11322it [00:04, 2914.63it/s]12729it [00:04, 3294.31it/s]16899it [00:06, 3342.86it/s]12677it [00:04, 3245.93it/s]11689it [00:04, 3114.76it/s]13093it [00:04, 3390.76it/s]17263it [00:06, 3425.03it/s]13056it [00:04, 3397.91it/s]12032it [00:04, 3106.60it/s]17629it [00:06, 3490.96it/s]13446it [00:04, 3327.71it/s]13422it [00:04, 3470.71it/s]12393it [00:04, 3245.16it/s]13829it [00:04, 3470.96it/s]17980it [00:06, 3386.61it/s]13776it [00:04, 3376.94it/s]12734it [00:04, 3213.36it/s]14198it [00:04, 3532.84it/s]18347it [00:07, 3461.51it/s]14155it [00:04, 3494.30it/s]13113it [00:04, 3374.11it/s]14555it [00:05, 3421.86it/s]14509it [00:05, 3424.65it/s]13460it [00:05, 3124.14it/s]14919it [00:05, 3482.99it/s]14875it [00:05, 3492.17it/s]13821it [00:05, 3256.54it/s]15270it [00:05, 3288.69it/s]15227it [00:05, 3310.06it/s]14159it [00:05, 3289.30it/s]15603it [00:05, 3276.58it/s]15562it [00:05, 3297.57it/s]14494it [00:05, 3087.42it/s]15934it [00:05, 3232.27it/s]15894it [00:05, 3258.13it/s]14827it [00:05, 3153.96it/s]16259it [00:05, 3063.52it/s]16222it [00:05, 3097.27it/s]15148it [00:05, 3083.81it/s]16644it [00:05, 3281.14it/s]16589it [00:05, 3255.63it/s]15519it [00:05, 3259.51it/s]16976it [00:05, 3239.59it/s]16918it [00:05, 3262.63it/s]15874it [00:05, 3340.48it/s]17359it [00:05, 3408.21it/s]17295it [00:05, 3408.05it/s]16211it [00:05, 3197.44it/s]17703it [00:06, 3356.95it/s]17648it [00:06, 3310.78it/s]16593it [00:05, 3374.12it/s]18066it [00:06, 3429.68it/s]18026it [00:06, 3443.28it/s]16934it [00:06, 3263.75it/s]18450it [00:06, 3547.82it/s]18406it [00:06, 3545.95it/s]17308it [00:06, 3396.75it/s]18695it [00:08, 717.98it/s] 17651it [00:06, 3327.16it/s]19066it [00:08, 954.93it/s]18012it [00:06, 3408.23it/s]19397it [00:08, 1181.58it/s]18395it [00:06, 3529.41it/s]19770it [00:08, 1501.73it/s]20144it [00:08, 1841.81it/s]20477it [00:09, 2055.63it/s]20847it [00:09, 2382.35it/s]21183it [00:09, 2513.76it/s]21556it [00:09, 2798.42it/s]21917it [00:09, 2886.95it/s]22265it [00:09, 3038.44it/s]18763it [00:07, 847.25it/s] 18807it [00:07, 809.52it/s] 22642it [00:09, 3233.70it/s]19143it [00:07, 1115.22it/s]19186it [00:07, 1069.33it/s]22990it [00:09, 3188.50it/s]19469it [00:07, 1361.31it/s]19496it [00:07, 1294.42it/s]23330it [00:09, 3245.51it/s]19831it [00:07, 1679.06it/s]19874it [00:07, 1634.14it/s]20216it [00:07, 2042.29it/s]23667it [00:09, 3189.81it/s]20236it [00:07, 1912.79it/s]18750it [00:07, 771.66it/s] 20557it [00:07, 2280.04it/s]24017it [00:10, 3276.14it/s]20611it [00:08, 2253.68it/s]19112it [00:07, 1010.30it/s]20937it [00:08, 2601.06it/s]24368it [00:10, 3335.07it/s]20966it [00:08, 2524.65it/s]19400it [00:08, 1190.39it/s]21285it [00:08, 2714.49it/s]24707it [00:10, 3129.79it/s]21308it [00:08, 2664.10it/s]19746it [00:08, 1486.68it/s]21620it [00:08, 2840.18it/s]25026it [00:10, 3087.36it/s]21642it [00:08, 2827.35it/s]20079it [00:08, 1775.98it/s]21951it [00:08, 2839.92it/s]25339it [00:10, 2976.78it/s]21975it [00:08, 2754.67it/s]20387it [00:08, 1960.76it/s]25643it [00:10, 2993.18it/s]22268it [00:08, 2695.23it/s]22287it [00:08, 2847.09it/s]20700it [00:08, 2197.33it/s]25967it [00:10, 3062.22it/s]22604it [00:08, 2863.31it/s]22636it [00:08, 3018.54it/s]21062it [00:08, 2516.02it/s]26276it [00:10, 2997.11it/s]22912it [00:08, 2920.60it/s]22959it [00:08, 2930.94it/s]21381it [00:08, 2651.37it/s]26608it [00:10, 3086.48it/s]23280it [00:08, 3127.78it/s]23318it [00:08, 3110.79it/s]21754it [00:08, 2926.89it/s]26945it [00:11, 3167.23it/s]23605it [00:08, 2997.79it/s]23642it [00:08, 3104.81it/s]22087it [00:08, 2961.58it/s]27263it [00:11, 3138.52it/s]23973it [00:09, 3184.37it/s]24013it [00:09, 3275.84it/s]22464it [00:08, 3179.20it/s]27621it [00:11, 3265.60it/s]24344it [00:09, 3333.12it/s]24378it [00:09, 3383.04it/s]22804it [00:09, 3155.63it/s]27949it [00:11, 3206.77it/s]24684it [00:09, 3259.11it/s]24722it [00:09, 3296.56it/s]23163it [00:09, 3274.80it/s]28315it [00:11, 3338.49it/s]25050it [00:09, 3372.89it/s]25078it [00:09, 3372.11it/s]23525it [00:09, 3373.31it/s]28650it [00:11, 3193.32it/s]25392it [00:09, 3295.25it/s]25419it [00:09, 3291.69it/s]23871it [00:09, 3276.80it/s]29014it [00:11, 3319.16it/s]25754it [00:09, 3369.90it/s]25773it [00:09, 3361.63it/s]24237it [00:09, 3385.89it/s]29363it [00:11, 3368.66it/s]26118it [00:09, 3274.74it/s]26116it [00:09, 3268.31it/s]24581it [00:09, 3261.67it/s]29702it [00:11, 3260.70it/s]26482it [00:09, 3377.69it/s]26480it [00:09, 3373.99it/s]24944it [00:09, 3364.63it/s]30069it [00:11, 3375.93it/s]26848it [00:09, 3456.49it/s]26844it [00:09, 3449.85it/s]25285it [00:09, 3263.16it/s]30409it [00:12, 3216.72it/s]27196it [00:09, 3319.50it/s]27191it [00:10, 3315.41it/s]25656it [00:09, 3389.32it/s]30774it [00:12, 3337.93it/s]27568it [00:10, 3432.64it/s]27564it [00:10, 3432.54it/s]25998it [00:10, 3340.40it/s]31137it [00:12, 3421.35it/s]27914it [00:10, 3345.24it/s]27910it [00:10, 3345.19it/s]26334it [00:10, 3241.01it/s]28282it [00:10, 3440.23it/s]28260it [00:10, 3389.06it/s]26702it [00:10, 3364.75it/s]28624it [00:10, 3460.92it/s]28637it [00:10, 3282.80it/s]27041it [00:10, 3266.51it/s]28972it [00:10, 3343.08it/s]29001it [00:10, 3382.51it/s]27417it [00:10, 3405.73it/s]29326it [00:10, 3399.54it/s]29370it [00:10, 3470.10it/s]27787it [00:10, 3489.04it/s]29668it [00:10, 3241.28it/s]29720it [00:10, 3356.82it/s]28138it [00:10, 3241.69it/s]30035it [00:10, 3361.88it/s]30068it [00:10, 3391.84it/s]28497it [00:10, 3335.92it/s]30374it [00:10, 3230.16it/s]30409it [00:10, 3230.78it/s]28835it [00:10, 3230.74it/s]30739it [00:11, 3347.62it/s]30775it [00:11, 3351.50it/s]29161it [00:10, 3141.81it/s]31105it [00:11, 3436.81it/s]31142it [00:11, 3441.37it/s]29478it [00:11, 2866.25it/s]29784it [00:11, 2917.74it/s]30107it [00:11, 3003.45it/s]30412it [00:11, 2857.18it/s]30774it [00:11, 3066.68it/s]31143it [00:11, 3241.86it/s]31482it [00:14, 532.09it/s] 31841it [00:14, 717.71it/s]32140it [00:14, 898.88it/s]32487it [00:14, 1161.11it/s]32845it [00:14, 1469.36it/s]31451it [00:12, 688.29it/s] 33165it [00:14, 1717.38it/s]31489it [00:12, 671.17it/s] 31806it [00:12, 908.01it/s]33510it [00:14, 2026.58it/s]31857it [00:12, 895.98it/s]32122it [00:12, 1130.48it/s]33833it [00:14, 2238.26it/s]32171it [00:12, 1112.65it/s]32488it [00:12, 1443.31it/s]34212it [00:15, 2583.07it/s]32520it [00:12, 1400.68it/s]32840it [00:13, 1747.28it/s]34560it [00:15, 2798.31it/s]32879it [00:13, 1723.61it/s]33162it [00:13, 1991.31it/s]34899it [00:15, 2860.60it/s]33204it [00:13, 1968.43it/s]31472it [00:13, 666.99it/s] 33525it [00:13, 2318.01it/s]35257it [00:15, 3047.41it/s]33563it [00:13, 2288.64it/s]31832it [00:13, 894.90it/s]33857it [00:13, 2502.28it/s]35594it [00:15, 2980.20it/s]33894it [00:13, 2460.01it/s]32116it [00:13, 1089.03it/s]34212it [00:13, 2750.85it/s]35947it [00:15, 3127.41it/s]34265it [00:13, 2753.66it/s]32473it [00:13, 1398.94it/s]34577it [00:13, 2978.05it/s]32819it [00:13, 1711.04it/s]34602it [00:13, 2843.01it/s]36278it [00:15, 2936.28it/s]34921it [00:13, 2999.52it/s]34961it [00:13, 3036.06it/s]36630it [00:15, 3092.92it/s]33134it [00:13, 1903.48it/s]35269it [00:13, 3128.58it/s]35324it [00:13, 3195.50it/s]36981it [00:15, 3207.97it/s]33485it [00:13, 2220.37it/s]35606it [00:13, 3130.70it/s]35669it [00:13, 3125.50it/s]37311it [00:16, 3050.76it/s]33800it [00:13, 2356.94it/s]35968it [00:13, 3266.33it/s]36036it [00:14, 3275.63it/s]37659it [00:16, 3168.60it/s]34167it [00:13, 2662.70it/s]36308it [00:14, 3175.23it/s]36378it [00:14, 3215.56it/s]34515it [00:14, 2866.51it/s]37983it [00:16, 3026.10it/s]36672it [00:14, 3305.02it/s]36740it [00:14, 3274.91it/s]38295it [00:16, 3051.56it/s]34846it [00:14, 2758.80it/s]37018it [00:14, 3349.39it/s]38605it [00:16, 3014.01it/s]35155it [00:14, 2843.08it/s]37075it [00:14, 2856.38it/s]37359it [00:14, 2858.90it/s]38910it [00:16, 2828.12it/s]35463it [00:14, 2708.91it/s]37375it [00:14, 2584.82it/s]37661it [00:14, 2774.97it/s]39202it [00:16, 2852.41it/s]35775it [00:14, 2815.56it/s]37658it [00:14, 2645.79it/s]37950it [00:14, 2698.65it/s]39538it [00:16, 2993.95it/s]36119it [00:14, 2985.77it/s]37947it [00:14, 2698.28it/s]38310it [00:14, 2936.73it/s]39841it [00:16, 2925.83it/s]36429it [00:14, 2877.25it/s]38320it [00:14, 2977.30it/s]38623it [00:14, 2988.51it/s]40183it [00:17, 3065.24it/s]36780it [00:14, 3049.99it/s]38677it [00:14, 3142.75it/s]38929it [00:14, 2984.95it/s]40492it [00:17, 2972.77it/s]37108it [00:14, 2973.59it/s]38999it [00:15, 3135.19it/s]39295it [00:15, 3176.47it/s]40843it [00:17, 3125.63it/s]37459it [00:15, 3121.62it/s]39356it [00:15, 3259.95it/s]39626it [00:15, 3129.68it/s]41177it [00:17, 3187.63it/s]37808it [00:15, 3223.21it/s]39686it [00:15, 3221.63it/s]39989it [00:15, 3271.17it/s]41498it [00:17, 3068.80it/s]38135it [00:15, 3060.24it/s]40050it [00:15, 3342.16it/s]40354it [00:15, 3380.74it/s]41853it [00:17, 3205.22it/s]38477it [00:15, 3159.64it/s]40417it [00:15, 3435.65it/s]40695it [00:15, 3281.18it/s]42176it [00:17, 3035.66it/s]40763it [00:15, 3305.33it/s]38797it [00:15, 3034.55it/s]41063it [00:15, 3395.62it/s]42533it [00:17, 3184.62it/s]41129it [00:15, 3405.28it/s]39146it [00:15, 3160.43it/s]41405it [00:15, 3293.04it/s]42883it [00:17, 3274.45it/s]39503it [00:15, 3275.94it/s]41472it [00:15, 3336.06it/s]41775it [00:15, 3408.66it/s]43213it [00:17, 3072.34it/s]41813it [00:15, 3357.33it/s]39834it [00:15, 3088.91it/s]42140it [00:15, 3477.76it/s]43560it [00:18, 3181.18it/s]42150it [00:15, 3295.50it/s]40189it [00:15, 3216.24it/s]42490it [00:16, 3372.53it/s]42519it [00:16, 3408.50it/s]43882it [00:18, 3029.39it/s]40515it [00:15, 3078.97it/s]42830it [00:16, 3378.06it/s]42882it [00:16, 3471.81it/s]44239it [00:18, 3178.49it/s]40868it [00:16, 3205.22it/s]43169it [00:16, 3272.72it/s]44597it [00:18, 3292.19it/s]43231it [00:16, 3346.49it/s]41227it [00:16, 3314.76it/s]43526it [00:16, 3357.29it/s]43595it [00:16, 3428.69it/s]44930it [00:18, 3102.59it/s]41562it [00:16, 3105.42it/s]43863it [00:16, 3275.75it/s]43940it [00:16, 3346.72it/s]45286it [00:18, 3228.86it/s]41918it [00:16, 3230.29it/s]44232it [00:16, 3393.33it/s]44310it [00:16, 3447.54it/s]45613it [00:18, 3091.38it/s]44597it [00:16, 3466.06it/s]42245it [00:16, 3100.87it/s]44667it [00:16, 3359.06it/s]45945it [00:18, 3154.19it/s]42600it [00:16, 3226.06it/s]44945it [00:16, 3317.73it/s]45040it [00:16, 3461.29it/s]46295it [00:18, 3251.12it/s]42950it [00:16, 3303.03it/s]45303it [00:16, 3390.91it/s]45396it [00:16, 3487.20it/s]45644it [00:16, 3314.92it/s]43283it [00:16, 3092.64it/s]45746it [00:17, 3375.01it/s]46001it [00:17, 3386.42it/s]43634it [00:16, 3207.19it/s]46112it [00:17, 3456.60it/s]46347it [00:17, 3311.37it/s]43959it [00:17, 3079.22it/s]44317it [00:17, 3217.85it/s]44643it [00:17, 3181.72it/s]44964it [00:17, 2887.99it/s]45276it [00:17, 2949.01it/s]45576it [00:17, 2830.39it/s]45919it [00:17, 2993.25it/s]46257it [00:17, 3101.88it/s]46459it [00:18, 564.94it/s] 46680it [00:19, 554.55it/s] 46830it [00:19, 765.09it/s]47034it [00:19, 746.66it/s]47202it [00:19, 1011.36it/s]47347it [00:19, 947.33it/s]46623it [00:21, 416.51it/s] 47519it [00:19, 1240.75it/s]47714it [00:19, 1239.42it/s]46977it [00:21, 573.08it/s]47887it [00:19, 1561.06it/s]48079it [00:19, 1559.57it/s]47257it [00:21, 720.74it/s]48216it [00:19, 1822.68it/s]48405it [00:19, 1814.18it/s]47613it [00:21, 967.97it/s]48542it [00:19, 2086.18it/s]48760it [00:19, 2133.03it/s]47944it [00:21, 1226.86it/s]48917it [00:19, 2431.40it/s]49091it [00:19, 2348.12it/s]48250it [00:21, 1455.65it/s]49257it [00:19, 2611.66it/s]49462it [00:19, 2656.58it/s]48610it [00:22, 1801.14it/s]49633it [00:19, 2890.30it/s]49801it [00:19, 2783.01it/s]48937it [00:22, 2021.58it/s]49979it [00:19, 2964.90it/s]50171it [00:20, 3015.68it/s]49263it [00:22, 2277.21it/s]50345it [00:20, 3147.87it/s]50536it [00:20, 3184.22it/s]49613it [00:22, 2554.42it/s]50691it [00:20, 3150.85it/s]50886it [00:20, 3116.55it/s]46571it [00:20, 411.92it/s] 49937it [00:22, 2516.16it/s]51220it [00:20, 3167.56it/s]51028it [00:20, 2779.96it/s]46877it [00:20, 548.32it/s]50256it [00:22, 2679.45it/s]51329it [00:20, 2828.94it/s]51553it [00:20, 2761.97it/s]47186it [00:20, 722.48it/s]50573it [00:22, 2804.68it/s]51629it [00:20, 2789.75it/s]51884it [00:20, 2900.90it/s]47453it [00:20, 882.66it/s]50882it [00:22, 2643.61it/s]51979it [00:20, 2980.99it/s]52239it [00:20, 3073.13it/s]47788it [00:20, 1157.53it/s]51238it [00:22, 2881.84it/s]52297it [00:20, 3011.03it/s]52560it [00:20, 3047.19it/s]48096it [00:20, 1390.84it/s]51545it [00:23, 2835.84it/s]52663it [00:20, 3193.71it/s]52926it [00:20, 3217.92it/s]48457it [00:20, 1747.80it/s]51898it [00:23, 3024.62it/s]53035it [00:20, 3343.22it/s]53256it [00:21, 3175.79it/s]48820it [00:20, 2099.50it/s]52255it [00:23, 3176.00it/s]53375it [00:21, 3245.15it/s]53615it [00:21, 3293.35it/s]49140it [00:21, 2241.27it/s]53723it [00:21, 3312.37it/s]52582it [00:23, 3018.54it/s]53976it [00:21, 3246.66it/s]49496it [00:21, 2533.07it/s]52939it [00:23, 3169.46it/s]54058it [00:21, 3259.52it/s]54333it [00:21, 3337.33it/s]49815it [00:21, 2602.93it/s]54427it [00:21, 3382.31it/s]53263it [00:23, 3046.19it/s]54696it [00:21, 3421.57it/s]50172it [00:21, 2843.75it/s]54790it [00:21, 3453.53it/s]53613it [00:23, 3171.46it/s]55041it [00:21, 3291.77it/s]50523it [00:21, 3019.40it/s]55137it [00:21, 3330.50it/s]53957it [00:23, 3246.30it/s]55394it [00:21, 3359.30it/s]50854it [00:21, 2948.26it/s]55495it [00:21, 3401.89it/s]54286it [00:23, 3115.12it/s]55732it [00:21, 3224.78it/s]51218it [00:21, 3134.09it/s]55837it [00:21, 3299.30it/s]54643it [00:23, 3241.29it/s]56102it [00:21, 3359.53it/s]51548it [00:21, 3026.17it/s]56191it [00:21, 3367.34it/s]56469it [00:21, 3448.51it/s]54971it [00:24, 3058.16it/s]51907it [00:21, 3179.40it/s]56530it [00:22, 3292.18it/s]55320it [00:24, 3177.05it/s]56816it [00:22, 3354.46it/s]52265it [00:22, 3291.14it/s]56895it [00:22, 3394.86it/s]57168it [00:22, 3400.71it/s]55657it [00:24, 3034.46it/s]57267it [00:22, 3481.08it/s]52602it [00:22, 3130.97it/s]57510it [00:22, 3303.16it/s]56009it [00:24, 3167.31it/s]52959it [00:22, 3251.30it/s]57617it [00:22, 3360.28it/s]57874it [00:22, 3399.79it/s]56357it [00:24, 3239.97it/s]57982it [00:22, 3443.45it/s]53290it [00:22, 3077.87it/s]58216it [00:22, 3289.88it/s]56684it [00:24, 3106.13it/s]58328it [00:22, 3279.49it/s]53642it [00:22, 3198.61it/s]58560it [00:22, 3329.99it/s]57044it [00:24, 3243.57it/s]58692it [00:22, 3380.98it/s]53976it [00:22, 3080.77it/s]58919it [00:22, 3403.65it/s]57372it [00:24, 3097.02it/s]59033it [00:22, 3279.83it/s]54340it [00:22, 3234.55it/s]59261it [00:22, 3298.59it/s]57727it [00:24, 3222.89it/s]59394it [00:22, 3371.35it/s]54695it [00:22, 3323.33it/s]59618it [00:22, 3375.15it/s]58070it [00:25, 3280.17it/s]59754it [00:22, 3435.60it/s]55031it [00:22, 3149.58it/s]59957it [00:23, 3286.39it/s]58401it [00:25, 3111.64it/s]60100it [00:23, 3328.23it/s]55376it [00:22, 3233.33it/s]60316it [00:23, 3372.63it/s]58748it [00:25, 3210.85it/s]60464it [00:23, 3416.06it/s]60663it [00:23, 3398.99it/s]55703it [00:23, 3024.84it/s]59072it [00:25, 3029.01it/s]60808it [00:23, 3198.80it/s]61004it [00:23, 3200.93it/s]56012it [00:23, 3042.69it/s]59379it [00:25, 2918.36it/s]61132it [00:23, 3205.06it/s]61328it [00:23, 3211.80it/s]56320it [00:23, 3031.65it/s]59690it [00:25, 2971.08it/s]61456it [00:23, 3212.88it/s]61652it [00:23, 3033.28it/s]56626it [00:23, 2759.51it/s]59990it [00:25, 2802.74it/s]61780it [00:23, 3057.07it/s]61980it [00:23, 3101.74it/s]56950it [00:23, 2887.90it/s]60316it [00:25, 2927.70it/s]62139it [00:23, 3206.87it/s]62327it [00:23, 3204.31it/s]57321it [00:23, 3116.13it/s]60668it [00:25, 3093.18it/s]62463it [00:23, 3175.70it/s]62650it [00:23, 3172.40it/s]57639it [00:23, 3113.08it/s]60981it [00:26, 2982.19it/s]62828it [00:23, 3310.41it/s]63009it [00:23, 3291.64it/s]58009it [00:23, 3281.66it/s]61340it [00:26, 3153.28it/s]63184it [00:24, 3382.52it/s]63340it [00:24, 3241.54it/s]58341it [00:23, 3080.72it/s]61659it [00:26, 2966.74it/s]63524it [00:24, 3287.66it/s]63701it [00:24, 3348.46it/s]58710it [00:24, 3250.70it/s]62013it [00:26, 3125.76it/s]63890it [00:24, 3395.23it/s]64053it [00:24, 3397.32it/s]59040it [00:24, 3201.29it/s]62366it [00:26, 3240.38it/s]64231it [00:24, 3294.93it/s]64394it [00:24, 3291.39it/s]59406it [00:24, 3331.19it/s]62694it [00:26, 3086.31it/s]64599it [00:24, 3405.45it/s]64758it [00:24, 3389.79it/s]59770it [00:24, 3420.27it/s]63032it [00:26, 3140.33it/s]64942it [00:24, 3266.99it/s]65099it [00:24, 3295.91it/s]60115it [00:24, 3322.52it/s]65300it [00:24, 3353.87it/s]63349it [00:26, 2943.40it/s]65456it [00:24, 3365.79it/s]60481it [00:24, 3417.23it/s]65658it [00:24, 3417.92it/s]63698it [00:26, 3092.77it/s]65794it [00:24, 3230.70it/s]60825it [00:24, 3325.72it/s]64051it [00:27, 3214.90it/s]61189it [00:24, 3414.64it/s]64377it [00:27, 3060.69it/s]61536it [00:24, 3307.07it/s]64711it [00:27, 3138.60it/s]61905it [00:25, 3415.02it/s]65028it [00:27, 3009.80it/s]62273it [00:25, 3491.08it/s]65375it [00:27, 3138.59it/s]62624it [00:25, 3209.53it/s]65721it [00:27, 3228.36it/s]62988it [00:25, 3328.94it/s]63326it [00:25, 3273.89it/s]63693it [00:25, 3384.91it/s]64056it [00:25, 3319.58it/s]64424it [00:25, 3419.45it/s]64798it [00:25, 3509.26it/s]65151it [00:25, 3391.23it/s]65515it [00:26, 3462.04it/s]66002it [00:27, 441.73it/s] 66368it [00:27, 606.19it/s]66670it [00:27, 770.44it/s]66119it [00:27, 397.27it/s] 67039it [00:27, 1028.95it/s]66482it [00:27, 551.25it/s]67409it [00:27, 1328.91it/s]66773it [00:27, 701.54it/s]67738it [00:27, 1577.19it/s]67140it [00:27, 949.05it/s]68106it [00:27, 1919.41it/s]67486it [00:27, 1202.69it/s]68440it [00:27, 2158.09it/s]67861it [00:27, 1535.26it/s]68810it [00:27, 2481.59it/s]68227it [00:28, 1869.12it/s]69167it [00:28, 2637.94it/s]68566it [00:28, 2109.53it/s]69513it [00:28, 2835.95it/s]68914it [00:28, 2388.75it/s]69876it [00:28, 3037.82it/s]69249it [00:28, 2546.03it/s]70220it [00:28, 3062.37it/s]69612it [00:28, 2805.40it/s]66047it [00:30, 336.18it/s] 70586it [00:28, 3222.27it/s]69980it [00:28, 3027.68it/s]66402it [00:30, 468.75it/s]70930it [00:28, 3168.44it/s]70326it [00:28, 3046.42it/s]66672it [00:30, 592.85it/s]71293it [00:28, 3296.27it/s]70674it [00:28, 3161.85it/s]67009it [00:30, 797.21it/s]71656it [00:28, 3389.88it/s]71013it [00:28, 3125.67it/s]67365it [00:31, 1060.02it/s]72004it [00:28, 3260.04it/s]71374it [00:28, 3258.82it/s]67674it [00:31, 1286.00it/s]72360it [00:29, 3344.50it/s]71712it [00:29, 3197.97it/s]68021it [00:31, 1600.72it/s]72700it [00:29, 3272.08it/s]72077it [00:29, 3325.05it/s]68335it [00:31, 1790.56it/s]65863it [00:29, 362.42it/s] 73032it [00:29, 3265.26it/s]72417it [00:29, 3247.55it/s]68635it [00:31, 2020.01it/s]66110it [00:29, 446.25it/s]73362it [00:29, 3214.47it/s]72747it [00:29, 3066.46it/s]68950it [00:31, 2259.87it/s]66345it [00:29, 552.12it/s]73686it [00:29, 3077.05it/s]73079it [00:29, 3135.07it/s]69252it [00:31, 2292.27it/s]66580it [00:29, 684.00it/s]74021it [00:29, 3151.33it/s]73397it [00:29, 3007.44it/s]69582it [00:31, 2532.69it/s]66815it [00:29, 836.60it/s]74339it [00:29, 3067.14it/s]73764it [00:29, 3190.60it/s]69931it [00:31, 2747.65it/s]67136it [00:29, 1116.98it/s]74699it [00:29, 3218.31it/s]74136it [00:29, 3331.41it/s]70240it [00:32, 2752.92it/s]67486it [00:29, 1409.24it/s]75047it [00:29, 3172.88it/s]74473it [00:29, 3222.30it/s]70591it [00:32, 2953.86it/s]67823it [00:29, 1735.18it/s]75409it [00:30, 3299.17it/s]74836it [00:30, 3334.84it/s]70906it [00:32, 2887.70it/s]68178it [00:30, 2083.13it/s]75774it [00:30, 3398.10it/s]75173it [00:30, 3254.79it/s]71258it [00:32, 3060.66it/s]68484it [00:30, 2212.59it/s]76116it [00:30, 3274.30it/s]75535it [00:30, 3357.28it/s]71596it [00:32, 3149.72it/s]68821it [00:30, 2477.32it/s]76473it [00:30, 3358.02it/s]75887it [00:30, 3267.18it/s]71920it [00:32, 3048.79it/s]76811it [00:30, 3225.11it/s]69166it [00:30, 2575.91it/s]76230it [00:30, 3313.30it/s]72271it [00:32, 3177.79it/s]77171it [00:30, 3329.30it/s]69503it [00:30, 2774.00it/s]76588it [00:30, 3388.58it/s]72594it [00:32, 3059.51it/s]77539it [00:30, 3429.81it/s]69859it [00:30, 2979.86it/s]76929it [00:30, 3294.51it/s]72957it [00:32, 3219.52it/s]77884it [00:30, 3315.52it/s]70183it [00:30, 2889.35it/s]77290it [00:30, 3384.62it/s]73296it [00:32, 3265.88it/s]78247it [00:30, 3403.54it/s]70535it [00:30, 3057.49it/s]77630it [00:30, 3290.94it/s]73626it [00:33, 3136.13it/s]78590it [00:30, 3306.64it/s]77970it [00:31, 3321.27it/s]70856it [00:30, 2960.78it/s]73985it [00:33, 3263.67it/s]78936it [00:31, 3349.21it/s]78335it [00:31, 3415.00it/s]71196it [00:30, 3072.70it/s]74315it [00:33, 3121.69it/s]79273it [00:31, 3253.95it/s]71548it [00:31, 3198.11it/s]78678it [00:31, 3311.19it/s]74648it [00:33, 3180.09it/s]79630it [00:31, 3344.26it/s]79033it [00:31, 3379.58it/s]71875it [00:31, 3033.84it/s]74999it [00:33, 3273.37it/s]79993it [00:31, 3426.59it/s]79373it [00:31, 3267.87it/s]72222it [00:31, 3153.91it/s]75329it [00:33, 3106.23it/s]80337it [00:31, 3317.09it/s]79721it [00:31, 3328.42it/s]72543it [00:31, 2992.17it/s]75681it [00:33, 3222.77it/s]80704it [00:31, 3418.56it/s]80086it [00:31, 3420.46it/s]72911it [00:31, 3182.02it/s]76007it [00:33, 3074.68it/s]81048it [00:31, 3304.07it/s]80430it [00:31, 3315.49it/s]73243it [00:31, 3219.42it/s]76335it [00:33, 3130.70it/s]81389it [00:31, 3333.61it/s]80792it [00:31, 3401.16it/s]73569it [00:31, 3097.40it/s]76690it [00:34, 3248.78it/s]81748it [00:31, 3405.73it/s]81134it [00:31, 3293.00it/s]73909it [00:31, 3181.77it/s]77018it [00:34, 3070.37it/s]82090it [00:32, 3291.53it/s]81487it [00:32, 3357.44it/s]74230it [00:31, 2974.38it/s]77370it [00:34, 3195.04it/s]82452it [00:32, 3385.46it/s]81825it [00:32, 3272.81it/s]74532it [00:32, 2968.62it/s]82792it [00:32, 3198.12it/s]77693it [00:34, 2920.52it/s]82154it [00:32, 3269.08it/s]74832it [00:32, 2964.20it/s]78000it [00:34, 2960.97it/s]83115it [00:32, 3199.81it/s]82482it [00:32, 2961.93it/s]75131it [00:32, 2760.87it/s]78315it [00:34, 3013.46it/s]83438it [00:32, 3208.18it/s]82784it [00:32, 2878.63it/s]75427it [00:32, 2813.20it/s]83761it [00:32, 3003.44it/s]78621it [00:34, 2799.97it/s]83105it [00:32, 2968.25it/s]75744it [00:32, 2913.31it/s]84118it [00:32, 3161.19it/s]78950it [00:34, 2933.48it/s]83447it [00:32, 3007.36it/s]76039it [00:32, 2786.94it/s]84438it [00:32, 3121.44it/s]79249it [00:34, 2853.96it/s]83811it [00:32, 3183.07it/s]76380it [00:32, 2960.13it/s]84799it [00:32, 3259.57it/s]79598it [00:35, 3030.55it/s]84173it [00:32, 3306.23it/s]76720it [00:32, 3085.46it/s]85128it [00:32, 3185.10it/s]79955it [00:35, 3184.05it/s]84507it [00:33, 3238.17it/s]77032it [00:32, 2957.45it/s]85488it [00:33, 3301.76it/s]80277it [00:35, 3059.45it/s]84863it [00:33, 3330.62it/s]77375it [00:33, 3089.48it/s]85848it [00:33, 3386.26it/s]80617it [00:35, 3154.41it/s]85198it [00:33, 3253.34it/s]77687it [00:33, 2982.17it/s]86189it [00:33, 3256.65it/s]80936it [00:35, 3035.41it/s]85544it [00:33, 3311.78it/s]78030it [00:33, 3107.15it/s]86551it [00:33, 3359.63it/s]81281it [00:35, 3152.09it/s]85900it [00:33, 3381.77it/s]78383it [00:33, 3228.19it/s]86889it [00:33, 3249.35it/s]81631it [00:35, 3250.05it/s]86240it [00:33, 3316.01it/s]78708it [00:33, 2988.97it/s]87252it [00:33, 3355.79it/s]86599it [00:33, 3394.57it/s]81959it [00:35, 3069.42it/s]79059it [00:33, 3133.57it/s]87614it [00:33, 3431.29it/s]82309it [00:35, 3189.29it/s]86940it [00:33, 3293.07it/s]79377it [00:33, 3020.95it/s]87959it [00:33, 3324.98it/s]87302it [00:33, 3386.37it/s]82631it [00:35, 3045.96it/s]79728it [00:33, 3158.00it/s]88307it [00:33, 3369.47it/s]87647it [00:33, 3307.02it/s]82986it [00:36, 3185.87it/s]80049it [00:33, 3121.67it/s]88646it [00:34, 3294.43it/s]88015it [00:34, 3412.98it/s]83340it [00:36, 3285.18it/s]80364it [00:34, 3014.64it/s]89007it [00:34, 3385.39it/s]88379it [00:34, 3478.90it/s]83672it [00:36, 3078.50it/s]80717it [00:34, 3153.68it/s]89347it [00:34, 3287.85it/s]88728it [00:34, 3372.95it/s]84025it [00:36, 3203.95it/s]81035it [00:34, 3096.81it/s]89711it [00:34, 3389.17it/s]89075it [00:34, 3399.44it/s]84350it [00:36, 3068.73it/s]81394it [00:34, 3237.12it/s]90074it [00:34, 3456.96it/s]89416it [00:34, 3300.91it/s]84699it [00:36, 3184.44it/s]81751it [00:34, 3333.67it/s]89784it [00:34, 3408.16it/s]85044it [00:36, 3228.02it/s]82086it [00:34, 3173.27it/s]90144it [00:34, 3463.31it/s]85370it [00:36, 3101.90it/s]82433it [00:34, 3255.35it/s]85736it [00:36, 3258.22it/s]82761it [00:34, 3053.72it/s]86065it [00:37, 3195.47it/s]83109it [00:34, 3170.14it/s]86432it [00:37, 3329.37it/s]83431it [00:34, 3183.79it/s]86793it [00:37, 3410.04it/s]83752it [00:35, 2998.88it/s]84063it [00:35, 2970.56it/s]87136it [00:37, 2664.14it/s]87438it [00:37, 2744.00it/s]84363it [00:35, 2775.81it/s]84680it [00:35, 2882.38it/s]87733it [00:37, 2636.14it/s]84979it [00:35, 2911.67it/s]88068it [00:37, 2815.63it/s]85273it [00:35, 2824.74it/s]88392it [00:37, 2928.12it/s]85629it [00:35, 3031.40it/s]88695it [00:37, 2855.70it/s]85957it [00:35, 3102.76it/s]89048it [00:38, 3040.27it/s]86270it [00:35, 2985.42it/s]89359it [00:38, 2940.80it/s]86597it [00:36, 3063.04it/s]89705it [00:38, 3084.08it/s]86906it [00:36, 2949.21it/s]90046it [00:38, 3175.54it/s]87257it [00:36, 3106.76it/s]87592it [00:36, 3175.28it/s]87912it [00:36, 3051.00it/s]88243it [00:36, 3123.91it/s]88558it [00:36, 3013.56it/s]88896it [00:36, 3116.55it/s]89250it [00:36, 3238.72it/s]89576it [00:37, 3011.41it/s]89931it [00:37, 3161.72it/s]90492it [00:37, 398.11it/s] 90852it [00:37, 545.36it/s]90421it [00:37, 349.66it/s] 91134it [00:37, 685.51it/s]90777it [00:37, 480.28it/s]91491it [00:37, 918.83it/s]91077it [00:37, 617.53it/s]91846it [00:37, 1190.71it/s]91432it [00:37, 830.53it/s]92165it [00:37, 1437.70it/s]91787it [00:37, 1086.17it/s]92523it [00:38, 1768.57it/s]92107it [00:38, 1325.81it/s]92850it [00:38, 2013.05it/s]92442it [00:38, 1616.56it/s]93182it [00:38, 2278.43it/s]93506it [00:38, 2459.23it/s]92762it [00:38, 1759.15it/s]93076it [00:38, 2014.47it/s]93824it [00:38, 2505.20it/s]93390it [00:38, 2248.81it/s]94142it [00:38, 2670.41it/s]93693it [00:38, 2400.35it/s]94449it [00:38, 2720.67it/s]94041it [00:38, 2662.07it/s]94803it [00:38, 2938.67it/s]94400it [00:38, 2902.41it/s]95157it [00:38, 3103.91it/s]94727it [00:38, 2909.25it/s]95485it [00:38, 3084.08it/s]95063it [00:39, 3013.03it/s]95838it [00:39, 3209.82it/s]95384it [00:39, 2986.03it/s]96168it [00:39, 3154.27it/s]95739it [00:39, 3142.86it/s]96525it [00:39, 3272.69it/s]96097it [00:39, 3265.93it/s]96881it [00:39, 3355.22it/s]96432it [00:39, 3185.26it/s]97221it [00:39, 3207.91it/s]96787it [00:39, 3288.56it/s]97575it [00:39, 3301.43it/s]97121it [00:39, 3188.99it/s]97909it [00:39, 3209.46it/s]97468it [00:39, 3267.27it/s]98262it [00:39, 3299.70it/s]97798it [00:39, 3175.60it/s]98616it [00:39, 3367.91it/s]98153it [00:39, 3281.97it/s]98955it [00:40, 3257.09it/s]98508it [00:40, 3357.34it/s]99310it [00:40, 3340.77it/s]98846it [00:40, 3245.40it/s]99646it [00:40, 3234.45it/s]99198it [00:40, 3323.08it/s]99998it [00:40, 3314.21it/s]99532it [00:40, 3219.01it/s]100331it [00:40, 3176.03it/s]99888it [00:40, 3315.92it/s]100685it [00:40, 3277.85it/s]100241it [00:40, 3375.43it/s]101037it [00:40, 3346.16it/s]90368it [00:42, 238.78it/s] 100580it [00:40, 3248.61it/s]90702it [00:42, 331.98it/s]101374it [00:40, 3233.86it/s]100917it [00:40, 3282.57it/s]91023it [00:42, 450.45it/s]101723it [00:40, 3305.11it/s]101247it [00:40, 3184.41it/s]91301it [00:43, 577.23it/s]102056it [00:40, 3217.17it/s]101599it [00:41, 3278.99it/s]91615it [00:43, 764.81it/s]102409it [00:41, 3306.26it/s]101953it [00:41, 3353.94it/s]102762it [00:41, 3369.34it/s]91916it [00:43, 945.91it/s]102290it [00:41, 3072.76it/s]92201it [00:43, 1166.19it/s]103101it [00:41, 3092.51it/s]102603it [00:41, 3078.07it/s]92500it [00:43, 1424.80it/s]103416it [00:41, 3082.85it/s]102915it [00:41, 2892.56it/s]103728it [00:41, 2920.49it/s]92780it [00:43, 1574.93it/s]103237it [00:41, 2981.29it/s]104065it [00:41, 3042.55it/s]93106it [00:43, 1886.61it/s]103581it [00:41, 3108.58it/s]104410it [00:41, 3155.79it/s]93452it [00:43, 2216.06it/s]103896it [00:41, 3063.40it/s]104729it [00:41, 3106.05it/s]93751it [00:43, 2283.15it/s]104237it [00:41, 3155.53it/s]105082it [00:41, 3225.48it/s]94090it [00:44, 2547.73it/s]104555it [00:41, 3103.79it/s]105407it [00:42, 3154.96it/s]94419it [00:44, 2735.27it/s]104909it [00:42, 3228.37it/s]105762it [00:42, 3266.41it/s]94728it [00:44, 2688.09it/s]105259it [00:42, 3305.33it/s]106099it [00:42, 3294.06it/s]95047it [00:44, 2820.14it/s]105591it [00:42, 3198.14it/s]106430it [00:42, 3203.07it/s]95348it [00:44, 2754.36it/s]105945it [00:42, 3296.22it/s]106786it [00:42, 3304.67it/s]95664it [00:44, 2864.11it/s]106277it [00:42, 3196.83it/s]90252it [00:42, 199.73it/s] 107118it [00:42, 3199.28it/s]96004it [00:44, 3012.40it/s]106630it [00:42, 3291.55it/s]90578it [00:42, 276.18it/s]107470it [00:42, 3289.33it/s]96314it [00:44, 2831.20it/s]106983it [00:42, 3358.94it/s]90919it [00:42, 384.15it/s]107823it [00:42, 3356.92it/s]96651it [00:44, 2979.19it/s]107321it [00:42, 3244.36it/s]91201it [00:42, 495.55it/s]108160it [00:42, 3248.59it/s]96956it [00:45, 2826.63it/s]107672it [00:42, 3318.70it/s]91540it [00:42, 676.28it/s]108512it [00:42, 3325.85it/s]97298it [00:45, 2989.42it/s]91878it [00:42, 897.70it/s]108006it [00:43, 3160.12it/s]108846it [00:43, 3206.83it/s]97638it [00:45, 3104.91it/s]108360it [00:43, 3267.42it/s]109196it [00:43, 3289.03it/s]92184it [00:43, 1100.43it/s]97954it [00:45, 2921.72it/s]108715it [00:43, 3346.76it/s]109547it [00:43, 3352.41it/s]92534it [00:43, 1406.57it/s]98278it [00:45, 3009.77it/s]109052it [00:43, 3238.13it/s]92842it [00:43, 1648.76it/s]109884it [00:43, 3239.08it/s]98617it [00:45, 3116.68it/s]109405it [00:43, 3321.34it/s]93190it [00:43, 1976.44it/s]110235it [00:43, 3315.45it/s]98933it [00:45, 2910.46it/s]109739it [00:43, 3217.28it/s]93506it [00:43, 2178.67it/s]110568it [00:43, 3213.58it/s]99268it [00:45, 3030.80it/s]110094it [00:43, 3310.55it/s]93815it [00:43, 2357.48it/s]110923it [00:43, 3308.90it/s]99576it [00:45, 2863.65it/s]94163it [00:43, 2624.64it/s]110427it [00:43, 3204.16it/s]111256it [00:43, 3215.20it/s]99912it [00:46, 2998.83it/s]110778it [00:43, 3290.60it/s]111608it [00:43, 3300.50it/s]94481it [00:43, 2577.21it/s]100224it [00:46, 3032.02it/s]111133it [00:43, 3365.68it/s]111951it [00:44, 3335.61it/s]94815it [00:43, 2768.50it/s]100531it [00:46, 2847.21it/s]95123it [00:44, 2688.30it/s]111471it [00:44, 2877.93it/s]112286it [00:44, 2838.67it/s]100820it [00:46, 2856.22it/s]111772it [00:44, 2874.49it/s]112596it [00:44, 2905.76it/s]95414it [00:44, 2490.66it/s]101120it [00:46, 2894.44it/s]112905it [00:44, 2954.48it/s]112077it [00:44, 2788.89it/s]95697it [00:44, 2575.89it/s]101412it [00:46, 2632.34it/s]112387it [00:44, 2872.09it/s]113208it [00:44, 2854.32it/s]95987it [00:44, 2661.81it/s]101737it [00:46, 2799.28it/s]112732it [00:44, 3031.67it/s]113555it [00:44, 3024.37it/s]96264it [00:44, 2593.20it/s]102023it [00:46, 2718.45it/s]113863it [00:44, 3004.14it/s]113041it [00:44, 2936.20it/s]96596it [00:44, 2793.04it/s]102345it [00:46, 2855.85it/s]114219it [00:44, 3162.02it/s]113394it [00:44, 3100.99it/s]96928it [00:44, 2940.06it/s]102682it [00:46, 3001.56it/s]114561it [00:44, 3236.32it/s]113744it [00:44, 3215.20it/s]97228it [00:44, 2806.71it/s]102986it [00:47, 2840.81it/s]114888it [00:45, 3149.35it/s]114069it [00:44, 3128.71it/s]97518it [00:44, 2830.67it/s]103326it [00:47, 2995.71it/s]115238it [00:45, 3248.41it/s]114421it [00:45, 3239.55it/s]97805it [00:45, 2765.96it/s]103646it [00:47, 3052.38it/s]115565it [00:45, 3156.08it/s]98134it [00:45, 2912.95it/s]114748it [00:45, 3024.88it/s]103955it [00:47, 2922.00it/s]115917it [00:45, 3260.69it/s]98475it [00:45, 3054.23it/s]115098it [00:45, 3155.43it/s]104279it [00:47, 3011.87it/s]116273it [00:45, 3346.33it/s]115437it [00:45, 3094.40it/s]98783it [00:45, 2884.04it/s]104583it [00:47, 2881.67it/s]116610it [00:45, 3237.13it/s]115790it [00:45, 3214.63it/s]99104it [00:45, 2974.90it/s]104905it [00:47, 2975.67it/s]116967it [00:45, 3332.46it/s]116143it [00:45, 3303.89it/s]99447it [00:45, 3103.38it/s]105240it [00:47, 3080.75it/s]117302it [00:45, 3187.48it/s]116476it [00:45, 3208.86it/s]99760it [00:45, 2944.63it/s]105551it [00:47, 2926.37it/s]117653it [00:45, 3277.41it/s]116828it [00:45, 3295.47it/s]100077it [00:45, 3008.08it/s]105871it [00:48, 3002.98it/s]117983it [00:45, 3183.43it/s]117160it [00:45, 3181.02it/s]100381it [00:45, 2881.45it/s]118334it [00:46, 3275.09it/s]106197it [00:48, 2874.07it/s]117511it [00:46, 3272.97it/s]100701it [00:45, 2970.97it/s]118685it [00:46, 3341.79it/s]106540it [00:48, 3028.00it/s]117863it [00:46, 3343.62it/s]101042it [00:46, 3096.18it/s]106881it [00:48, 3132.89it/s]119021it [00:46, 3220.50it/s]118199it [00:46, 3208.53it/s]101354it [00:46, 2890.17it/s]119371it [00:46, 3298.77it/s]107198it [00:48, 2974.14it/s]118548it [00:46, 3287.41it/s]101694it [00:46, 3031.25it/s]119703it [00:46, 3196.71it/s]107533it [00:48, 3078.64it/s]118879it [00:46, 3173.10it/s]102001it [00:46, 2849.49it/s]120044it [00:46, 3256.05it/s]119210it [00:46, 3210.85it/s]107877it [00:48, 2946.25it/s]102345it [00:46, 3010.46it/s]120398it [00:46, 3338.11it/s]119559it [00:46, 3290.53it/s]108221it [00:48, 3079.68it/s]102662it [00:46, 3053.34it/s]108564it [00:48, 3175.74it/s]119890it [00:46, 3182.13it/s]102971it [00:46, 2929.77it/s]120241it [00:46, 3274.98it/s]108885it [00:49, 2992.59it/s]103302it [00:46, 3034.99it/s]109207it [00:49, 3052.64it/s]103609it [00:46, 2778.23it/s]109516it [00:49, 3051.21it/s]103893it [00:47, 2472.24it/s]109824it [00:49, 2636.77it/s]104162it [00:47, 2526.66it/s]110099it [00:49, 2592.09it/s]104463it [00:47, 2654.48it/s]110397it [00:49, 2542.20it/s]104735it [00:47, 2588.79it/s]110741it [00:49, 2778.49it/s]105087it [00:47, 2845.44it/s]111083it [00:49, 2952.74it/s]105377it [00:47, 2823.68it/s]111385it [00:49, 2784.97it/s]105717it [00:47, 2985.99it/s]111721it [00:50, 2940.95it/s]106047it [00:47, 3075.58it/s]112062it [00:50, 3070.59it/s]106358it [00:47, 2857.88it/s]112374it [00:50, 2891.29it/s]106698it [00:48, 3008.90it/s]112713it [00:50, 3027.48it/s]107025it [00:48, 3077.72it/s]113021it [00:50, 2914.40it/s]107337it [00:48, 2879.93it/s]113338it [00:50, 2983.92it/s]107673it [00:48, 3010.96it/s]113686it [00:50, 3124.32it/s]107979it [00:48, 2851.13it/s]114002it [00:50, 2975.76it/s]108317it [00:48, 2997.15it/s]114338it [00:50, 3081.70it/s]108659it [00:48, 3115.82it/s]114650it [00:51, 2946.85it/s]108975it [00:48, 2931.12it/s]114975it [00:51, 3031.41it/s]109313it [00:48, 3053.81it/s]115318it [00:51, 3144.32it/s]109623it [00:49, 2886.56it/s]115635it [00:51, 2995.12it/s]109958it [00:49, 3013.63it/s]115963it [00:51, 3074.53it/s]110277it [00:49, 3062.55it/s]116277it [00:51, 2946.59it/s]110587it [00:49, 2928.60it/s]116625it [00:51, 3094.83it/s]110913it [00:49, 3020.13it/s]116949it [00:51, 3134.29it/s]111236it [00:49, 2906.64it/s]117265it [00:51, 2975.22it/s]111539it [00:49, 2927.06it/s]117606it [00:52, 3095.56it/s]111876it [00:49, 3050.93it/s]117929it [00:52, 3126.10it/s]112184it [00:49, 2730.90it/s]118244it [00:52, 2842.99it/s]112465it [00:50, 2686.71it/s]118548it [00:52, 2895.37it/s]112763it [00:50, 2766.65it/s]120734it [00:50, 295.78it/s] 118843it [00:52, 2633.00it/s]121050it [00:50, 398.04it/s]113044it [00:50, 2434.20it/s]119169it [00:52, 2798.10it/s]121385it [00:50, 537.38it/s]113376it [00:50, 2663.69it/s]119501it [00:52, 2941.32it/s]121735it [00:50, 727.64it/s]113693it [00:50, 2799.35it/s]119802it [00:52, 2850.95it/s]122080it [00:50, 956.80it/s]113982it [00:50, 2726.30it/s]120131it [00:52, 2973.25it/s]122389it [00:50, 1172.25it/s]114318it [00:50, 2901.60it/s]120462it [00:52, 3067.91it/s]122724it [00:50, 1458.82it/s]114614it [00:50, 2736.05it/s]123065it [00:51, 1725.00it/s]114946it [00:50, 2896.42it/s]123411it [00:51, 2038.56it/s]115260it [00:51, 2965.08it/s]123758it [00:51, 2331.33it/s]120570it [00:51, 246.32it/s] 115561it [00:51, 2850.77it/s]120910it [00:51, 341.80it/s]124085it [00:51, 2480.62it/s]115882it [00:51, 2951.53it/s]121246it [00:51, 466.90it/s]124431it [00:51, 2715.34it/s]116218it [00:51, 3066.59it/s]121532it [00:51, 598.58it/s]124757it [00:51, 2771.36it/s]116528it [00:51, 2890.01it/s]121878it [00:51, 809.41it/s]125090it [00:51, 2916.32it/s]116866it [00:51, 3025.79it/s]122202it [00:51, 1040.76it/s]125423it [00:51, 3028.38it/s]117173it [00:51, 2886.33it/s]122510it [00:51, 1272.17it/s]125748it [00:51, 2975.39it/s]117512it [00:51, 3025.49it/s]122855it [00:51, 1587.35it/s]126092it [00:52, 3103.13it/s]117846it [00:51, 3113.13it/s]123169it [00:52, 1807.66it/s]126425it [00:52, 3026.77it/s]118161it [00:52, 2959.56it/s]123511it [00:52, 2118.49it/s]126770it [00:52, 3142.98it/s]118499it [00:52, 3077.36it/s]123841it [00:52, 2372.93it/s]127118it [00:52, 3238.45it/s]118810it [00:52, 2942.67it/s]124160it [00:52, 2506.71it/s]127448it [00:52, 3124.83it/s]119146it [00:52, 3035.60it/s]124474it [00:52, 2662.42it/s]127765it [00:52, 3070.82it/s]119488it [00:52, 3144.43it/s]124785it [00:52, 2702.59it/s]128098it [00:52, 3143.29it/s]119805it [00:52, 3027.95it/s]125096it [00:52, 2806.62it/s]128415it [00:52, 3023.52it/s]120111it [00:52, 3024.50it/s]125422it [00:52, 2931.33it/s]128746it [00:52, 3102.53it/s]120416it [00:52, 3004.41it/s]125733it [00:52, 2893.43it/s]129059it [00:52, 3026.91it/s]126041it [00:52, 2943.93it/s]129370it [00:53, 3050.45it/s]126355it [00:53, 2999.14it/s]129679it [00:53, 3059.53it/s]126662it [00:53, 2802.15it/s]129986it [00:53, 2858.93it/s]126961it [00:53, 2853.72it/s]130295it [00:53, 2922.66it/s]127266it [00:53, 2797.08it/s]130625it [00:53, 2939.45it/s]127616it [00:53, 2990.62it/s]130979it [00:53, 3107.97it/s]127944it [00:53, 3072.40it/s]131321it [00:53, 3196.39it/s]128255it [00:53, 2946.33it/s]131643it [00:53, 3031.33it/s]128580it [00:53, 3030.90it/s]131984it [00:53, 3138.28it/s]128919it [00:53, 3133.65it/s]132305it [00:54, 2972.30it/s]129235it [00:54, 2950.93it/s]132649it [00:54, 3101.84it/s]129559it [00:54, 3030.38it/s]132978it [00:54, 3155.13it/s]129865it [00:54, 2962.20it/s]133297it [00:54, 3056.34it/s]130186it [00:54, 3031.87it/s]133635it [00:54, 3139.26it/s]130517it [00:54, 3111.65it/s]133974it [00:54, 3209.58it/s]130830it [00:54, 2962.51it/s]134297it [00:54, 3083.87it/s]131163it [00:54, 3066.02it/s]134632it [00:54, 3159.25it/s]131472it [00:54, 2942.10it/s]134950it [00:54, 3066.52it/s]131811it [00:54, 3068.62it/s]135295it [00:54, 3173.81it/s]132156it [00:55, 3177.07it/s]135637it [00:55, 3243.80it/s]132476it [00:55, 3048.37it/s]135963it [00:55, 3155.62it/s]132826it [00:55, 3175.30it/s]136315it [00:55, 3259.43it/s]133146it [00:55, 3082.86it/s]136643it [00:55, 3174.45it/s]133497it [00:55, 3203.82it/s]136996it [00:55, 3273.30it/s]133840it [00:55, 3266.84it/s]137346it [00:55, 3181.34it/s]134169it [00:55, 3175.20it/s]137702it [00:55, 3287.84it/s]134500it [00:55, 3213.11it/s]138054it [00:55, 3352.74it/s]134826it [00:55, 3125.35it/s]138391it [00:55, 3168.22it/s]135148it [00:55, 3151.76it/s]138711it [00:56, 2926.62it/s]135465it [00:56, 2982.76it/s]139009it [00:56, 2823.71it/s]135766it [00:56, 2863.36it/s]139295it [00:56, 2764.27it/s]136077it [00:56, 2931.36it/s]139646it [00:56, 2966.47it/s]136413it [00:56, 3053.03it/s]139946it [00:56, 2974.90it/s]136721it [00:56, 2993.96it/s]140307it [00:56, 3155.61it/s]137037it [00:56, 3040.10it/s]140667it [00:56, 3282.05it/s]137346it [00:56, 3010.17it/s]140998it [00:56, 3183.39it/s]137702it [00:56, 3167.93it/s]141344it [00:56, 3261.95it/s]138045it [00:56, 3242.46it/s]141672it [00:57, 3101.27it/s]138371it [00:57, 3064.07it/s]142010it [00:57, 3179.51it/s]138705it [00:57, 3140.14it/s]142349it [00:57, 3239.54it/s]139026it [00:57, 2930.35it/s]142675it [00:57, 3044.08it/s]139371it [00:57, 3073.13it/s]143017it [00:57, 3148.94it/s]139709it [00:57, 3158.63it/s]143335it [00:57, 3061.25it/s]140029it [00:57, 3029.42it/s]143671it [00:57, 3143.95it/s]140370it [00:57, 3134.36it/s]144011it [00:57, 3216.12it/s]140706it [00:57, 3050.00it/s]144335it [00:57, 3129.75it/s]120773it [00:59, 147.27it/s] 141039it [00:57, 3126.93it/s]144685it [00:57, 3235.29it/s]121115it [01:00, 210.75it/s]141383it [00:57, 3215.66it/s]121386it [01:00, 277.84it/s]145011it [00:58, 3141.33it/s]141707it [00:58, 3096.00it/s]121732it [01:00, 395.71it/s]145348it [00:58, 3206.12it/s]142033it [00:58, 3140.77it/s]122075it [01:00, 548.16it/s]145689it [00:58, 3264.60it/s]142370it [00:58, 3204.76it/s]122377it [01:00, 708.15it/s]146017it [00:58, 3175.05it/s]142692it [00:58, 3086.87it/s]122726it [01:00, 949.17it/s]146363it [00:58, 3256.69it/s]143045it [00:58, 3213.39it/s]146690it [00:58, 3180.63it/s]123066it [01:00, 1195.46it/s]143369it [00:58, 3138.80it/s]147049it [00:58, 3296.98it/s]123388it [01:00, 1465.91it/s]143716it [00:58, 3226.37it/s]147410it [00:58, 3386.71it/s]123719it [01:00, 1760.83it/s]144050it [00:58, 3204.28it/s]147750it [00:58, 3223.37it/s]124034it [01:01, 1952.59it/s]144372it [00:58, 2985.64it/s]148075it [00:59, 3209.80it/s]124335it [01:01, 2152.55it/s]144694it [00:59, 3049.46it/s]124639it [01:01, 2351.22it/s]148398it [00:59, 3041.46it/s]145002it [00:59, 2904.55it/s]148722it [00:59, 3095.44it/s]124939it [01:01, 2237.56it/s]145336it [00:59, 3024.18it/s]149063it [00:59, 3185.35it/s]125270it [01:01, 2490.82it/s]145691it [00:59, 3172.75it/s]149384it [00:59, 3120.72it/s]125586it [01:01, 2514.32it/s]146012it [00:59, 3096.21it/s]149735it [00:59, 3231.59it/s]125931it [01:01, 2750.37it/s]146359it [00:59, 3201.20it/s]150060it [00:59, 3137.51it/s]126270it [01:01, 2918.94it/s]146682it [00:59, 3085.11it/s]150407it [00:59, 3231.57it/s]126581it [01:01, 2838.37it/s]147028it [00:59, 3191.44it/s]150756it [00:59, 3305.03it/s]126925it [01:02, 3001.17it/s]147373it [00:59, 3264.61it/s]151088it [00:59, 3208.57it/s]127266it [01:02, 2974.46it/s]120718it [00:59, 140.36it/s] 147702it [01:00, 3180.84it/s]151435it [01:00, 3283.33it/s]127595it [01:02, 3059.94it/s]121049it [01:00, 200.34it/s]148048it [01:00, 3260.25it/s]151765it [01:00, 3180.08it/s]127940it [01:02, 3168.76it/s]121358it [01:00, 276.41it/s]148376it [01:00, 3140.74it/s]152107it [01:00, 3247.66it/s]128262it [01:02, 3047.20it/s]121625it [01:00, 361.81it/s]148697it [01:00, 3158.97it/s]152445it [01:00, 3285.83it/s]128591it [01:02, 3115.75it/s]121932it [01:00, 494.66it/s]149038it [01:00, 3231.26it/s]152775it [01:00, 3075.89it/s]128935it [01:02, 3208.28it/s]122225it [01:00, 646.86it/s]149363it [01:00, 3032.28it/s]153122it [01:00, 3186.31it/s]129259it [01:02, 3059.63it/s]122543it [01:00, 861.16it/s]149703it [01:00, 3116.16it/s]153444it [01:00, 3056.63it/s]129576it [01:02, 3082.02it/s]122864it [01:00, 1114.49it/s]150018it [01:00, 3035.00it/s]153794it [01:00, 3180.46it/s]129887it [01:02, 2992.20it/s]123158it [01:00, 1349.33it/s]150368it [01:00, 3165.72it/s]154146it [01:00, 3123.27it/s]130222it [01:03, 3092.70it/s]123484it [01:00, 1653.47it/s]150716it [01:00, 3255.48it/s]154489it [01:01, 3209.00it/s]130547it [01:03, 3136.70it/s]123823it [01:00, 1974.90it/s]151044it [01:01, 3049.51it/s]154843it [01:01, 3302.42it/s]130863it [01:03, 3016.29it/s]124135it [01:01, 2141.95it/s]151400it [01:01, 3190.77it/s]155176it [01:01, 3225.03it/s]131208it [01:03, 3138.59it/s]124482it [01:01, 2438.63it/s]151723it [01:01, 3133.44it/s]155537it [01:01, 3333.84it/s]124795it [01:01, 2550.80it/s]131524it [01:03, 3003.96it/s]152065it [01:01, 3214.88it/s]155872it [01:01, 3249.51it/s]125121it [01:01, 2728.74it/s]131867it [01:03, 3124.11it/s]152410it [01:01, 3282.03it/s]156215it [01:01, 3299.31it/s]125465it [01:01, 2916.30it/s]132202it [01:03, 3170.73it/s]156552it [01:01, 3319.63it/s]152740it [01:01, 3134.41it/s]125786it [01:01, 2765.23it/s]132521it [01:03, 2938.21it/s]153056it [01:01, 3126.36it/s]156885it [01:01, 3008.43it/s]126113it [01:01, 2898.87it/s]132853it [01:03, 3041.91it/s]153371it [01:01, 2938.77it/s]157208it [01:01, 3067.85it/s]126420it [01:01, 2938.66it/s]133161it [01:04, 2858.05it/s]153668it [01:01, 2905.41it/s]157520it [01:02, 2706.59it/s]133471it [01:04, 2924.37it/s]153974it [01:02, 2948.79it/s]126727it [01:01, 2459.97it/s]157810it [01:02, 2755.60it/s]133777it [01:04, 2961.65it/s]127023it [01:02, 2584.01it/s]154271it [01:02, 2596.42it/s]158102it [01:02, 2798.03it/s]134076it [01:04, 2786.07it/s]127299it [01:02, 2588.75it/s]154587it [01:02, 2743.62it/s]158388it [01:02, 2595.11it/s]134411it [01:04, 2940.57it/s]127618it [01:02, 2750.24it/s]154933it [01:02, 2938.45it/s]134748it [01:04, 3062.48it/s]127956it [01:02, 2924.64it/s]155234it [01:02, 2895.12it/s]135058it [01:04, 2977.33it/s]128258it [01:02, 2882.84it/s]155572it [01:02, 3030.43it/s]135386it [01:04, 3062.10it/s]128584it [01:02, 2988.49it/s]155880it [01:02, 3004.41it/s]135695it [01:04, 2992.51it/s]128925it [01:02, 3109.81it/s]156231it [01:02, 3148.46it/s]136041it [01:05, 3124.60it/s]129240it [01:02, 2993.86it/s]156564it [01:02, 3199.62it/s]136369it [01:05, 3168.96it/s]129588it [01:02, 3131.87it/s]156886it [01:03, 3120.74it/s]136688it [01:05, 3086.76it/s]157238it [01:03, 3235.33it/s]129905it [01:03, 3024.47it/s]137038it [01:05, 3204.86it/s]130255it [01:03, 3158.83it/s]157564it [01:03, 3159.97it/s]137360it [01:05, 3053.40it/s]130583it [01:03, 3193.75it/s]157899it [01:03, 3213.27it/s]137708it [01:05, 3173.24it/s]158255it [01:03, 3312.12it/s]130905it [01:03, 3096.80it/s]138054it [01:05, 3254.64it/s]131255it [01:03, 3204.29it/s]138382it [01:05, 3092.98it/s]131578it [01:03, 3070.73it/s]138725it [01:05, 3188.45it/s]131927it [01:03, 3187.83it/s]139047it [01:05, 3070.86it/s]132248it [01:03, 3185.12it/s]139376it [01:06, 3127.60it/s]132568it [01:03, 3105.78it/s]139726it [01:06, 3228.77it/s]132894it [01:03, 3148.84it/s]140051it [01:06, 3135.19it/s]133210it [01:04, 3079.76it/s]140399it [01:06, 3233.84it/s]133538it [01:04, 3136.71it/s]140724it [01:06, 3082.13it/s]133889it [01:04, 3245.48it/s]141076it [01:06, 3204.67it/s]134215it [01:04, 3144.24it/s]141427it [01:06, 3289.95it/s]134547it [01:04, 3193.63it/s]141758it [01:06, 2989.41it/s]134868it [01:04, 3004.83it/s]142101it [01:06, 3108.62it/s]135186it [01:04, 3052.45it/s]135500it [01:04, 3075.11it/s]142418it [01:07, 2896.08it/s]135810it [01:04, 2787.96it/s]142714it [01:07, 2782.35it/s]136115it [01:05, 2858.97it/s]143015it [01:07, 2842.67it/s]136406it [01:05, 2864.71it/s]143303it [01:07, 2687.79it/s]136696it [01:05, 2812.44it/s]143628it [01:07, 2838.96it/s]137022it [01:05, 2939.06it/s]143971it [01:07, 3002.30it/s]137345it [01:05, 2914.13it/s]144276it [01:07, 2947.16it/s]137693it [01:05, 3073.88it/s]144617it [01:07, 3077.06it/s]138026it [01:05, 3146.08it/s]144928it [01:07, 3009.12it/s]138343it [01:05, 3071.52it/s]145280it [01:08, 3155.18it/s]138676it [01:05, 3143.96it/s]145634it [01:08, 3266.47it/s]139026it [01:05, 3090.53it/s]145963it [01:08, 3133.42it/s]139363it [01:06, 3167.41it/s]146312it [01:08, 3235.22it/s]139713it [01:06, 3261.37it/s]146638it [01:08, 3130.14it/s]140041it [01:06, 3125.41it/s]146990it [01:08, 3241.43it/s]140390it [01:06, 3227.79it/s]147328it [01:08, 3281.18it/s]140715it [01:06, 3130.84it/s]147658it [01:08, 3157.89it/s]141054it [01:06, 3202.79it/s]148007it [01:08, 3252.48it/s]141404it [01:06, 3287.88it/s]148334it [01:08, 3113.52it/s]158654it [01:06, 199.26it/s] 141735it [01:06, 3114.64it/s]148669it [01:09, 3179.79it/s]159012it [01:07, 295.55it/s]142087it [01:06, 3229.43it/s]149021it [01:09, 3277.42it/s]159301it [01:07, 396.95it/s]142413it [01:07, 3080.47it/s]149351it [01:09, 3119.14it/s]159660it [01:07, 565.82it/s]142761it [01:07, 3191.61it/s]149670it [01:09, 3137.04it/s]160018it [01:07, 777.51it/s]143096it [01:07, 3234.47it/s]149986it [01:09, 3060.78it/s]160325it [01:07, 985.26it/s]143422it [01:07, 3127.71it/s]150326it [01:09, 3156.28it/s]160694it [01:07, 1296.64it/s]143776it [01:07, 3220.60it/s]150677it [01:09, 3258.76it/s]161019it [01:07, 1561.55it/s]144100it [01:07, 3020.76it/s]151005it [01:09, 3016.38it/s]161386it [01:07, 1913.21it/s]144428it [01:07, 3092.39it/s]151344it [01:09, 3119.84it/s]161738it [01:07, 2221.47it/s]144754it [01:07, 3138.08it/s]151660it [01:10, 2872.01it/s]162078it [01:08, 2323.46it/s]145070it [01:07, 2929.93it/s]151973it [01:10, 2939.48it/s]162394it [01:08, 2491.72it/s]145376it [01:08, 2963.42it/s]152272it [01:10, 2940.68it/s]162707it [01:08, 2505.39it/s]145676it [01:08, 2870.36it/s]152570it [01:10, 2808.95it/s]163053it [01:08, 2740.01it/s]145966it [01:08, 2847.24it/s]152907it [01:10, 2963.81it/s]163411it [01:08, 2956.38it/s]146307it [01:08, 3005.76it/s]153249it [01:10, 3092.00it/s]163736it [01:08, 2982.69it/s]146610it [01:08, 2952.77it/s]153562it [01:10, 3015.32it/s]164096it [01:08, 3152.63it/s]146958it [01:08, 3102.19it/s]153883it [01:10, 3070.90it/s]164427it [01:08, 3123.51it/s]147305it [01:08, 3208.81it/s]154192it [01:10, 3011.18it/s]164783it [01:08, 3245.55it/s]147628it [01:08, 3022.47it/s]154542it [01:11, 3150.26it/s]165136it [01:08, 3176.40it/s]147976it [01:08, 3151.11it/s]154893it [01:11, 3253.70it/s]165510it [01:09, 3333.05it/s]148294it [01:08, 3076.83it/s]155220it [01:11, 3145.15it/s]165886it [01:09, 3455.35it/s]148640it [01:09, 3185.42it/s]155559it [01:11, 3214.17it/s]166236it [01:09, 3381.43it/s]148982it [01:09, 3251.33it/s]155882it [01:11, 3124.34it/s]166610it [01:09, 3483.44it/s]149309it [01:09, 3150.63it/s]156235it [01:11, 3239.08it/s]166962it [01:09, 3333.63it/s]149643it [01:09, 3202.96it/s]156585it [01:11, 3312.95it/s]167310it [01:09, 3373.94it/s]149965it [01:09, 3101.04it/s]156918it [01:11, 3137.82it/s]167656it [01:09, 3155.12it/s]150314it [01:09, 3210.84it/s]157268it [01:11, 3239.46it/s]168008it [01:09, 3254.65it/s]150649it [01:09, 3251.00it/s]157595it [01:11, 3148.12it/s]168368it [01:09, 3350.70it/s]150976it [01:09, 3139.10it/s]157939it [01:12, 3231.11it/s]168707it [01:10, 3158.71it/s]151315it [01:09, 3208.86it/s]158284it [01:12, 3291.24it/s]169063it [01:10, 3268.33it/s]151638it [01:10, 3096.12it/s]169394it [01:10, 3197.10it/s]151971it [01:10, 3161.96it/s]158588it [01:10, 159.46it/s] 169738it [01:10, 3264.11it/s]152318it [01:10, 3248.38it/s]158918it [01:10, 221.73it/s]170080it [01:10, 3307.96it/s]152645it [01:10, 3144.13it/s]159255it [01:10, 307.15it/s]170413it [01:10, 3190.27it/s]152978it [01:10, 3195.24it/s]159585it [01:10, 419.99it/s]170771it [01:10, 3300.46it/s]153306it [01:10, 3009.10it/s]159941it [01:10, 580.41it/s]171103it [01:10, 3210.42it/s]153642it [01:10, 3106.76it/s]160250it [01:10, 749.03it/s]171445it [01:10, 3267.60it/s]153961it [01:10, 3128.29it/s]160561it [01:10, 957.57it/s]171774it [01:10, 3221.48it/s]160873it [01:10, 1199.64it/s]154276it [01:10, 2960.03it/s]172098it [01:11, 3049.67it/s]154590it [01:10, 3009.88it/s]161182it [01:11, 1391.75it/s]172413it [01:11, 3076.27it/s]154894it [01:11, 2981.13it/s]161494it [01:11, 1664.64it/s]172723it [01:11, 2967.46it/s]155194it [01:11, 2903.01it/s]161787it [01:11, 1890.39it/s]173096it [01:11, 3181.36it/s]155544it [01:11, 3072.93it/s]162144it [01:11, 2238.51it/s]173461it [01:11, 3315.39it/s]155854it [01:11, 2985.09it/s]162511it [01:11, 2565.20it/s]173795it [01:11, 3262.74it/s]156209it [01:11, 3145.80it/s]162839it [01:11, 2631.80it/s]174149it [01:11, 3341.73it/s]156564it [01:11, 3260.45it/s]163205it [01:11, 2891.18it/s]174485it [01:11, 3290.54it/s]156892it [01:11, 3120.33it/s]163535it [01:11, 2934.70it/s]174833it [01:11, 3344.05it/s]157249it [01:11, 3246.50it/s]163886it [01:11, 3087.94it/s]175190it [01:12, 3409.29it/s]164224it [01:12, 3165.99it/s]157576it [01:11, 3128.20it/s]175532it [01:12, 3208.28it/s]157926it [01:12, 3227.03it/s]164557it [01:12, 3061.88it/s]175882it [01:12, 3288.69it/s]158260it [01:12, 3258.48it/s]164909it [01:12, 3188.46it/s]176214it [01:12, 3184.89it/s]165237it [01:12, 3098.00it/s]176559it [01:12, 3258.67it/s]165587it [01:12, 3210.16it/s]176896it [01:12, 3182.84it/s]165930it [01:12, 3272.67it/s]177238it [01:12, 3249.26it/s]166262it [01:12, 3212.57it/s]177596it [01:12, 3337.83it/s]166616it [01:12, 3306.52it/s]177932it [01:12, 3288.27it/s]166950it [01:12, 3254.51it/s]178301it [01:12, 3403.37it/s]167302it [01:12, 3331.42it/s]178643it [01:13, 3320.07it/s]167654it [01:13, 3265.37it/s]179005it [01:13, 3405.92it/s]168005it [01:13, 3335.05it/s]179367it [01:13, 3467.84it/s]168377it [01:13, 3445.66it/s]179715it [01:13, 3374.28it/s]168723it [01:13, 3327.87it/s]180082it [01:13, 3458.13it/s]169093it [01:13, 3433.89it/s]180429it [01:13, 3326.12it/s]169438it [01:13, 3204.41it/s]180764it [01:13, 3326.75it/s]169764it [01:13, 3218.83it/s]181098it [01:13, 3065.80it/s]170096it [01:13, 3246.32it/s]181409it [01:13, 3076.93it/s]170423it [01:13, 2926.82it/s]181720it [01:14, 2931.77it/s]170731it [01:14, 2967.75it/s]182017it [01:14, 2803.57it/s]171033it [01:14, 2842.57it/s]182351it [01:14, 2950.43it/s]171356it [01:14, 2947.55it/s]182706it [01:14, 3117.65it/s]171699it [01:14, 3082.44it/s]183022it [01:14, 3086.05it/s]172011it [01:14, 3021.48it/s]183377it [01:14, 3218.94it/s]172355it [01:14, 3139.22it/s]183702it [01:14, 3089.48it/s]172688it [01:14, 3193.59it/s]184042it [01:14, 3177.36it/s]173010it [01:14, 3118.90it/s]184390it [01:14, 3264.59it/s]173345it [01:14, 3184.41it/s]184719it [01:15, 3186.06it/s]173665it [01:15, 3125.77it/s]185070it [01:15, 3279.29it/s]174008it [01:15, 3212.59it/s]185400it [01:15, 3161.74it/s]174340it [01:15, 3242.87it/s]185760it [01:15, 3286.82it/s]174666it [01:15, 3212.04it/s]186136it [01:15, 3266.11it/s]175032it [01:15, 3341.24it/s]186504it [01:15, 3381.77it/s]175367it [01:15, 3224.28it/s]186874it [01:15, 3473.30it/s]175737it [01:15, 3359.81it/s]187223it [01:15, 3362.56it/s]176075it [01:15, 3259.53it/s]187591it [01:15, 3453.12it/s]176423it [01:15, 3321.52it/s]187938it [01:15, 3374.43it/s]176794it [01:15, 3434.12it/s]188306it [01:16, 3457.15it/s]177139it [01:16, 3318.90it/s]188656it [01:16, 3370.84it/s]177505it [01:16, 3414.84it/s]189020it [01:16, 3445.40it/s]177848it [01:16, 3225.27it/s]189391it [01:16, 3520.91it/s]178207it [01:16, 3324.07it/s]189745it [01:16, 3395.99it/s]178571it [01:16, 3413.54it/s]190102it [01:16, 3444.84it/s]178915it [01:16, 3291.02it/s]190448it [01:16, 3346.39it/s]179274it [01:16, 3375.43it/s]190817it [01:16, 3442.97it/s]179614it [01:16, 3310.34it/s]191176it [01:16, 3333.63it/s]179952it [01:16, 3328.89it/s]191512it [01:17, 3328.64it/s]180287it [01:17, 3036.78it/s]191846it [01:17, 3296.06it/s]180606it [01:17, 3076.62it/s]192177it [01:17, 3120.90it/s]180918it [01:17, 3081.99it/s]192515it [01:17, 3191.89it/s]181230it [01:17, 2952.88it/s]192856it [01:17, 3181.78it/s]181588it [01:17, 3127.91it/s]193214it [01:17, 3294.95it/s]158615it [01:19, 145.23it/s] 181934it [01:17, 3118.57it/s]193570it [01:17, 3369.82it/s]158977it [01:19, 208.03it/s]182293it [01:17, 3251.40it/s]193909it [01:17, 3305.66it/s]159278it [01:19, 278.54it/s]182668it [01:17, 3394.67it/s]194285it [01:17, 3437.52it/s]159643it [01:19, 395.53it/s]183010it [01:17, 3315.38it/s]194630it [01:17, 3358.45it/s]160007it [01:20, 548.79it/s]183364it [01:17, 3378.54it/s]194993it [01:18, 3436.13it/s]160330it [01:20, 713.24it/s]183704it [01:18, 3302.77it/s]195376it [01:18, 3392.90it/s]160692it [01:20, 952.45it/s]184071it [01:18, 3406.60it/s]195751it [01:18, 3493.55it/s]161020it [01:20, 1185.00it/s]184430it [01:18, 3459.00it/s]196114it [01:18, 3530.54it/s]161376it [01:20, 1492.40it/s]184777it [01:18, 3368.15it/s]196469it [01:18, 3417.98it/s]161715it [01:20, 1788.01it/s]185136it [01:18, 3431.57it/s]196840it [01:18, 3501.90it/s]162048it [01:20, 2007.07it/s]185481it [01:18, 3324.25it/s]197192it [01:18, 3388.63it/s]162405it [01:20, 2318.55it/s]185836it [01:18, 3388.41it/s]197568it [01:18, 3493.48it/s]162734it [01:20, 2475.13it/s]186176it [01:18, 3329.58it/s]197919it [01:18, 3424.70it/s]163081it [01:21, 2710.31it/s]186535it [01:18, 3402.87it/s]198280it [01:19, 3475.36it/s]163438it [01:21, 2927.79it/s]186899it [01:19, 3470.57it/s]198637it [01:19, 3501.11it/s]163775it [01:21, 2951.84it/s]187247it [01:19, 3359.04it/s]198988it [01:19, 3406.36it/s]164138it [01:21, 3132.18it/s]187618it [01:19, 3458.47it/s]199366it [01:19, 3507.74it/s]164475it [01:21, 3092.39it/s]187966it [01:19, 3351.20it/s]199718it [01:19, 3423.17it/s]164820it [01:21, 3190.54it/s]188337it [01:19, 3452.66it/s]200062it [01:19, 3394.38it/s]165152it [01:21, 3141.13it/s]188684it [01:19, 3251.83it/s]200409it [01:19, 3416.25it/s]165506it [01:21, 3253.18it/s]189036it [01:19, 3326.23it/s]200752it [01:19, 3197.23it/s]165851it [01:21, 3306.16it/s]189405it [01:19, 3429.05it/s]201116it [01:19, 3320.59it/s]166187it [01:21, 3098.38it/s]189751it [01:19, 3228.06it/s]201451it [01:19, 3149.00it/s]166503it [01:22, 3088.23it/s]190078it [01:19, 3236.55it/s]201778it [01:20, 3181.01it/s]166816it [01:22, 2965.92it/s]190405it [01:20, 3007.10it/s]202099it [01:20, 3019.31it/s]167128it [01:22, 3008.26it/s]190732it [01:20, 3078.88it/s]202412it [01:20, 3047.42it/s]167454it [01:22, 3078.77it/s]191071it [01:20, 3165.11it/s]202757it [01:20, 3159.88it/s]167765it [01:22, 3052.94it/s]191391it [01:20, 3097.75it/s]203076it [01:20, 3087.88it/s]168110it [01:22, 3165.59it/s]191741it [01:20, 3210.51it/s]203428it [01:20, 3211.47it/s]168474it [01:22, 3302.92it/s]192065it [01:20, 3085.91it/s]158588it [01:20, 129.28it/s] 168806it [01:22, 3202.08it/s]203776it [01:20, 3139.94it/s]192435it [01:20, 3259.50it/s]158946it [01:20, 185.32it/s]169167it [01:22, 3318.73it/s]204147it [01:20, 3300.23it/s]192805it [01:20, 3385.67it/s]159254it [01:20, 250.60it/s]204521it [01:20, 3424.96it/s]169501it [01:23, 3214.46it/s]193146it [01:20, 3312.59it/s]159609it [01:20, 353.96it/s]204866it [01:21, 3348.76it/s]169843it [01:23, 3272.79it/s]193507it [01:21, 3396.88it/s]159949it [01:20, 484.45it/s]205232it [01:21, 3436.87it/s]170175it [01:23, 3190.87it/s]193849it [01:21, 3336.28it/s]160260it [01:21, 632.76it/s]170535it [01:23, 3306.61it/s]194203it [01:21, 3394.55it/s]160615it [01:21, 853.69it/s]170897it [01:23, 3397.53it/s]194544it [01:21, 3181.40it/s]160934it [01:21, 1065.36it/s]171238it [01:23, 3262.51it/s]194915it [01:21, 3328.92it/s]161288it [01:21, 1363.92it/s]171580it [01:23, 3305.98it/s]195296it [01:21, 3464.93it/s]161643it [01:21, 1685.29it/s]171913it [01:23, 3224.30it/s]195646it [01:21, 3383.23it/s]161974it [01:21, 1915.38it/s]172271it [01:23, 3324.44it/s]196012it [01:21, 3461.16it/s]162332it [01:21, 2238.56it/s]172633it [01:23, 3407.85it/s]196360it [01:21, 3353.66it/s]162662it [01:21, 2405.99it/s]172975it [01:24, 3304.17it/s]196735it [01:21, 3465.20it/s]163000it [01:21, 2631.07it/s]173337it [01:24, 3395.16it/s]197084it [01:22, 3395.52it/s]163360it [01:22, 2870.89it/s]173678it [01:24, 3244.32it/s]197454it [01:22, 3483.39it/s]163695it [01:22, 2902.57it/s]174033it [01:24, 3330.14it/s]197828it [01:22, 3556.86it/s]164049it [01:22, 3070.91it/s]174375it [01:24, 3191.40it/s]198185it [01:22, 3419.12it/s]164382it [01:22, 2962.62it/s]174742it [01:24, 3325.85it/s]198564it [01:22, 3524.32it/s]164741it [01:22, 3131.29it/s]175107it [01:24, 3418.40it/s]198919it [01:22, 3405.60it/s]165104it [01:22, 3268.05it/s]175451it [01:24, 3199.41it/s]199297it [01:22, 3510.75it/s]165443it [01:22, 3007.74it/s]175799it [01:24, 3276.16it/s]199650it [01:22, 3423.96it/s]165789it [01:22, 3129.54it/s]176130it [01:25, 3098.41it/s]199994it [01:22, 3368.23it/s]166112it [01:22, 2895.02it/s]176453it [01:25, 3133.65it/s]200332it [01:23, 3304.85it/s]166431it [01:23, 2972.22it/s]176770it [01:25, 3113.16it/s]200664it [01:23, 3118.01it/s]166745it [01:23, 3016.38it/s]177084it [01:25, 2983.80it/s]200978it [01:23, 2927.20it/s]167053it [01:23, 2834.17it/s]177435it [01:25, 3129.72it/s]201274it [01:23, 2934.20it/s]167382it [01:23, 2957.94it/s]177751it [01:25, 3019.18it/s]201630it [01:23, 3106.91it/s]167683it [01:23, 2918.76it/s]178116it [01:25, 3196.49it/s]201994it [01:23, 3257.16it/s]168039it [01:23, 3098.16it/s]178462it [01:25, 3266.03it/s]202323it [01:23, 3183.00it/s]168381it [01:23, 3189.53it/s]178791it [01:25, 3188.53it/s]202697it [01:23, 3340.44it/s]168703it [01:23, 3134.57it/s]179152it [01:26, 3309.12it/s]203034it [01:23, 3283.12it/s]169062it [01:23, 3266.02it/s]179485it [01:26, 3201.63it/s]203391it [01:24, 3364.52it/s]169391it [01:23, 3132.92it/s]179857it [01:26, 3347.67it/s]203750it [01:24, 3430.19it/s]169755it [01:24, 3271.25it/s]180230it [01:26, 3456.28it/s]204095it [01:24, 3294.86it/s]170123it [01:24, 3388.19it/s]180578it [01:26, 3276.58it/s]204467it [01:24, 3416.54it/s]170464it [01:24, 3210.33it/s]180942it [01:26, 3378.68it/s]204811it [01:24, 3275.64it/s]170819it [01:24, 3304.34it/s]181283it [01:26, 3199.67it/s]205174it [01:24, 3375.05it/s]171153it [01:24, 3179.08it/s]181642it [01:26, 3308.14it/s]205514it [01:24, 3245.22it/s]171491it [01:24, 3235.65it/s]181976it [01:26, 3173.60it/s]171852it [01:24, 3341.27it/s]182348it [01:26, 3324.71it/s]172189it [01:24, 3246.86it/s]182720it [01:27, 3436.24it/s]172555it [01:24, 3357.86it/s]183067it [01:27, 3295.10it/s]172893it [01:25, 3187.86it/s]183410it [01:27, 3333.19it/s]173256it [01:25, 3310.98it/s]183746it [01:27, 3247.01it/s]173590it [01:25, 3225.59it/s]184110it [01:27, 3358.89it/s]173915it [01:25, 3229.87it/s]184455it [01:27, 3215.31it/s]174269it [01:25, 3318.88it/s]184812it [01:27, 3313.64it/s]174603it [01:25, 3169.67it/s]185155it [01:27, 3341.09it/s]174926it [01:25, 3185.82it/s]185491it [01:27, 3117.92it/s]175247it [01:25, 3066.16it/s]185807it [01:28, 3021.50it/s]175556it [01:25, 2980.89it/s]186112it [01:28, 3003.02it/s]175867it [01:25, 3016.65it/s]186415it [01:28, 2865.68it/s]176170it [01:26, 2890.38it/s]186766it [01:28, 3043.95it/s]176488it [01:26, 2970.67it/s]187074it [01:28, 2891.11it/s]176829it [01:26, 3095.58it/s]187442it [01:28, 3108.69it/s]177141it [01:26, 3048.73it/s]187810it [01:28, 3270.43it/s]177502it [01:26, 3211.25it/s]188141it [01:28, 3209.39it/s]177825it [01:26, 3095.03it/s]188496it [01:28, 3306.33it/s]178183it [01:26, 3233.94it/s]205578it [01:26, 196.60it/s] 188829it [01:29, 3126.39it/s]178528it [01:26, 3295.00it/s]205945it [01:26, 277.41it/s]189178it [01:29, 3227.47it/s]178859it [01:26, 3145.63it/s]206314it [01:27, 387.19it/s]189504it [01:29, 3098.69it/s]179199it [01:27, 3216.82it/s]206622it [01:27, 505.49it/s]189864it [01:29, 3238.36it/s]206988it [01:27, 691.74it/s]179523it [01:27, 3060.08it/s]190229it [01:29, 3354.27it/s]179885it [01:27, 3217.65it/s]207313it [01:27, 885.87it/s]190567it [01:29, 3171.76it/s]180236it [01:27, 3298.95it/s]207683it [01:27, 1165.86it/s]190930it [01:29, 3300.55it/s]180569it [01:27, 3206.33it/s]208045it [01:27, 1447.40it/s]191264it [01:29, 3218.38it/s]180940it [01:27, 3349.27it/s]208428it [01:27, 1802.04it/s]191631it [01:29, 3345.84it/s]208802it [01:27, 2141.02it/s]181277it [01:27, 3197.44it/s]192009it [01:29, 3469.38it/s]209156it [01:27, 2372.01it/s]181640it [01:27, 3319.00it/s]192359it [01:30, 3295.30it/s]209507it [01:28, 2619.91it/s]181975it [01:27, 3220.26it/s]192730it [01:30, 3411.95it/s]209853it [01:28, 2741.73it/s]182322it [01:27, 3291.06it/s]193075it [01:30, 3274.57it/s]210211it [01:28, 2949.06it/s]182685it [01:28, 3388.02it/s]193438it [01:30, 3372.53it/s]210565it [01:28, 2990.82it/s]183026it [01:28, 3252.34it/s]193778it [01:30, 3270.29it/s]210930it [01:28, 3163.72it/s]183377it [01:28, 3324.30it/s]194117it [01:30, 3304.20it/s]211296it [01:28, 3298.58it/s]183712it [01:28, 3180.61it/s]194476it [01:30, 3384.44it/s]211645it [01:28, 3209.14it/s]184074it [01:28, 3303.94it/s]194816it [01:30, 3272.30it/s]212013it [01:28, 3338.19it/s]184434it [01:28, 3388.25it/s]195179it [01:30, 3372.80it/s]212358it [01:28, 3156.09it/s]184775it [01:28, 3081.17it/s]195518it [01:31, 3060.85it/s]212685it [01:28, 3186.52it/s]185094it [01:28, 3108.10it/s]195846it [01:31, 3119.93it/s]213011it [01:29, 3173.87it/s]185410it [01:29, 2646.34it/s]196165it [01:31, 3137.27it/s]213333it [01:29, 2906.01it/s]185689it [01:29, 2574.69it/s]196483it [01:31, 2987.20it/s]213631it [01:29, 2911.30it/s]196811it [01:31, 3068.01it/s]185956it [01:29, 2509.85it/s]213928it [01:29, 2832.26it/s]197121it [01:31, 3026.17it/s]186214it [01:29, 2497.95it/s]214281it [01:29, 3025.80it/s]197470it [01:31, 3156.69it/s]186580it [01:29, 2811.73it/s]214630it [01:29, 3155.38it/s]197839it [01:31, 3310.97it/s]186963it [01:29, 3094.49it/s]214950it [01:29, 3094.62it/s]198173it [01:31, 3244.39it/s]187280it [01:29, 3018.75it/s]215309it [01:29, 3235.62it/s]198543it [01:32, 3376.46it/s]187651it [01:29, 3213.58it/s]215635it [01:29, 3159.47it/s]198883it [01:32, 3263.39it/s]187977it [01:29, 3153.71it/s]215990it [01:30, 3269.42it/s]199232it [01:32, 3326.59it/s]188336it [01:29, 3277.97it/s]216343it [01:30, 3343.56it/s]199576it [01:32, 3193.36it/s]188667it [01:30, 3137.54it/s]216679it [01:30, 3246.45it/s]199917it [01:32, 3253.16it/s]189010it [01:30, 3218.64it/s]217050it [01:30, 3378.85it/s]200270it [01:32, 3332.81it/s]189372it [01:30, 3333.23it/s]217390it [01:30, 3282.42it/s]200605it [01:32, 3144.94it/s]189708it [01:30, 3118.13it/s]217767it [01:30, 3420.54it/s]200938it [01:32, 3194.87it/s]190054it [01:30, 3212.84it/s]218124it [01:30, 3289.23it/s]201260it [01:32, 3118.67it/s]190379it [01:30, 3145.35it/s]218485it [01:30, 3379.40it/s]201627it [01:32, 3274.14it/s]190743it [01:30, 3285.09it/s]218859it [01:30, 3480.59it/s]201995it [01:33, 3390.98it/s]191093it [01:30, 3346.59it/s]219209it [01:31, 3371.50it/s]202336it [01:33, 3292.90it/s]191430it [01:30, 3236.82it/s]219577it [01:31, 3459.86it/s]202691it [01:33, 3365.65it/s]191796it [01:31, 3357.85it/s]219925it [01:31, 3361.68it/s]203030it [01:33, 3297.31it/s]192134it [01:31, 3272.01it/s]220286it [01:31, 3432.69it/s]203383it [01:33, 3363.92it/s]192463it [01:31, 3247.89it/s]220644it [01:31, 3347.66it/s]203733it [01:33, 3402.03it/s]192825it [01:31, 3354.21it/s]221006it [01:31, 3423.67it/s]204075it [01:33, 3221.90it/s]193162it [01:31, 3210.92it/s]221375it [01:31, 3499.31it/s]204413it [01:33, 3265.02it/s]193519it [01:31, 3312.49it/s]221727it [01:31, 3419.15it/s]204742it [01:33, 3065.66it/s]193853it [01:31, 3091.37it/s]222089it [01:31, 3474.77it/s]205084it [01:34, 3164.24it/s]194205it [01:31, 3209.60it/s]222438it [01:31, 3295.47it/s]205416it [01:34, 3207.21it/s]194530it [01:31, 3017.44it/s]222770it [01:32, 3265.62it/s]223099it [01:32, 3228.21it/s]194836it [01:32, 2886.22it/s]195175it [01:32, 3023.01it/s]223423it [01:32, 3060.66it/s]223753it [01:32, 3126.15it/s]195482it [01:32, 2915.63it/s]224068it [01:32, 3111.34it/s]195851it [01:32, 3130.49it/s]205841it [01:32, 142.14it/s] 224409it [01:32, 3196.54it/s]196200it [01:32, 3231.73it/s]206204it [01:32, 202.85it/s]224765it [01:32, 3299.95it/s]206505it [01:32, 270.96it/s]196527it [01:32, 3152.42it/s]206865it [01:32, 382.47it/s]225097it [01:32, 3199.89it/s]196904it [01:32, 3328.46it/s]225448it [01:32, 3288.21it/s]197240it [01:32, 3224.42it/s]207202it [01:32, 515.45it/s]225779it [01:33, 3196.56it/s]197608it [01:32, 3354.32it/s]207560it [01:32, 701.95it/s]226125it [01:33, 3271.05it/s]207921it [01:33, 934.48it/s]197946it [01:32, 3241.38it/s]226454it [01:33, 3259.04it/s]198309it [01:33, 3350.46it/s]208252it [01:33, 1159.42it/s]198665it [01:33, 3410.95it/s]208604it [01:33, 1456.59it/s]226781it [01:33, 3085.46it/s]227109it [01:33, 3138.32it/s]199008it [01:33, 3284.86it/s]208932it [01:33, 1673.78it/s]199356it [01:33, 3340.33it/s]227425it [01:33, 3007.80it/s]209291it [01:33, 2006.28it/s]199692it [01:33, 3260.62it/s]227767it [01:33, 3123.57it/s]209637it [01:33, 2294.46it/s]200048it [01:33, 3344.24it/s]228116it [01:33, 3227.33it/s]209968it [01:33, 2441.42it/s]200409it [01:33, 3419.06it/s]228441it [01:33, 3101.32it/s]210325it [01:33, 2705.18it/s]200753it [01:33, 3291.79it/s]228791it [01:33, 3213.65it/s]210655it [01:33, 2682.55it/s]201100it [01:33, 3341.99it/s]229115it [01:34, 3140.58it/s]211015it [01:34, 2912.51it/s]229480it [01:34, 3286.79it/s]201436it [01:34, 3233.31it/s]211382it [01:34, 3110.76it/s]229842it [01:34, 3381.65it/s]201807it [01:34, 3367.32it/s]211719it [01:34, 3051.43it/s]202146it [01:34, 3264.70it/s]230182it [01:34, 3229.32it/s]212076it [01:34, 3191.81it/s]202523it [01:34, 3409.31it/s]230524it [01:34, 3283.42it/s]212410it [01:34, 3101.11it/s]202882it [01:34, 3460.35it/s]230855it [01:34, 3149.02it/s]212760it [01:34, 3211.19it/s]203230it [01:34, 3333.10it/s]231220it [01:34, 3290.37it/s]213090it [01:34, 3163.96it/s]203580it [01:34, 3378.54it/s]231564it [01:34, 3222.78it/s]213449it [01:34, 3284.04it/s]231895it [01:34, 3246.17it/s]203920it [01:34, 3204.08it/s]213782it [01:34, 2806.68it/s]204243it [01:34, 3203.92it/s]232222it [01:35, 2970.98it/s]214078it [01:35, 2755.82it/s]204566it [01:34, 3149.03it/s]232525it [01:35, 2901.67it/s]214401it [01:35, 2880.60it/s]204883it [01:35, 2928.01it/s]232840it [01:35, 2968.56it/s]214698it [01:35, 2892.82it/s]205224it [01:35, 3059.73it/s]233174it [01:35, 3063.21it/s]214994it [01:35, 2804.98it/s]233483it [01:35, 3014.97it/s]215366it [01:35, 3059.05it/s]233838it [01:35, 3167.89it/s]215677it [01:35, 3004.06it/s]234157it [01:35, 3120.44it/s]216022it [01:35, 3127.03it/s]234508it [01:35, 3231.37it/s]216376it [01:35, 3244.22it/s]234870it [01:35, 3343.52it/s]216704it [01:35, 3204.33it/s]235206it [01:35, 3271.86it/s]217068it [01:36, 3330.47it/s]235554it [01:36, 3324.65it/s]217403it [01:36, 3248.66it/s]235888it [01:36, 3260.21it/s]217782it [01:36, 3404.83it/s]236246it [01:36, 3352.17it/s]218125it [01:36, 3198.08it/s]236591it [01:36, 3379.19it/s]218481it [01:36, 3298.30it/s]236930it [01:36, 3188.55it/s]218847it [01:36, 3401.46it/s]237273it [01:36, 3255.01it/s]219190it [01:36, 3184.33it/s]237601it [01:36, 3065.35it/s]219547it [01:36, 3291.25it/s]237944it [01:36, 3166.96it/s]219880it [01:36, 3224.59it/s]238284it [01:36, 3134.79it/s]220267it [01:36, 3406.45it/s]238643it [01:37, 3262.63it/s]220627it [01:37, 3461.25it/s]238984it [01:37, 3298.19it/s]220976it [01:37, 3331.88it/s]239316it [01:37, 3189.56it/s]221337it [01:37, 3408.80it/s]239672it [01:37, 3294.40it/s]221680it [01:37, 3323.39it/s]240004it [01:37, 3215.82it/s]222053it [01:37, 3438.00it/s]240369it [01:37, 3340.43it/s]222399it [01:37, 3339.40it/s]240722it [01:37, 3394.93it/s]222759it [01:37, 3412.53it/s]241063it [01:37, 3318.08it/s]223102it [01:37, 3406.69it/s]241403it [01:37, 3340.42it/s]223444it [01:37, 3151.49it/s]241738it [01:38, 3094.68it/s]223764it [01:38, 3088.07it/s]242052it [01:38, 3098.96it/s]224076it [01:38, 2873.99it/s]242365it [01:38, 3074.64it/s]224430it [01:38, 3052.97it/s]242675it [01:38, 2969.61it/s]224772it [01:38, 3152.66it/s]243023it [01:38, 3113.20it/s]225092it [01:38, 3074.90it/s]243337it [01:38, 3003.57it/s]225448it [01:38, 3212.08it/s]243698it [01:38, 3175.39it/s]225773it [01:38, 3046.35it/s]244044it [01:38, 3256.04it/s]226119it [01:38, 3159.89it/s]244372it [01:38, 3161.04it/s]226480it [01:38, 3288.03it/s]244719it [01:38, 3247.46it/s]226812it [01:39, 3204.93it/s]245046it [01:39, 3189.36it/s]227153it [01:39, 3261.65it/s]245410it [01:39, 3319.74it/s]227481it [01:39, 3193.02it/s]245775it [01:39, 3413.09it/s]227829it [01:39, 3273.71it/s]246118it [01:39, 3324.30it/s]228192it [01:39, 3374.91it/s]246477it [01:39, 3398.73it/s]228531it [01:39, 3237.09it/s]246818it [01:39, 3242.62it/s]228863it [01:39, 3258.67it/s]247168it [01:39, 3314.02it/s]247502it [01:39, 3314.88it/s]229191it [01:39, 3066.12it/s]229554it [01:39, 3222.59it/s]247835it [01:39, 3109.61it/s]248201it [01:40, 3263.06it/s]229882it [01:39, 3100.54it/s]205740it [01:42, 133.65it/s] 230247it [01:40, 3253.40it/s]248531it [01:40, 3192.86it/s]206097it [01:42, 191.63it/s]230586it [01:40, 3291.05it/s]248893it [01:40, 3312.88it/s]206406it [01:42, 259.85it/s]230918it [01:40, 3217.20it/s]249227it [01:40, 3236.67it/s]206777it [01:42, 372.07it/s]231282it [01:40, 3332.46it/s]249584it [01:40, 3331.59it/s]207142it [01:42, 517.63it/s]249927it [01:40, 3359.46it/s]231617it [01:40, 3200.20it/s]207466it [01:42, 672.37it/s]231975it [01:40, 3307.23it/s]250265it [01:40, 3264.24it/s]207810it [01:42, 887.38it/s]232318it [01:40, 3341.38it/s]250615it [01:40, 3331.38it/s]208129it [01:42, 1097.28it/s]250950it [01:40, 3141.35it/s]232654it [01:40, 3056.52it/s]208469it [01:43, 1379.85it/s]251267it [01:40, 3043.47it/s]232966it [01:40, 2932.67it/s]208807it [01:43, 1677.87it/s]251574it [01:41, 2779.88it/s]233264it [01:41, 2764.97it/s]209129it [01:43, 1903.16it/s]233617it [01:41, 2968.44it/s]251857it [01:41, 2667.55it/s]209493it [01:43, 2244.58it/s]233982it [01:41, 3155.64it/s]252215it [01:41, 2907.30it/s]209819it [01:43, 2403.73it/s]234303it [01:41, 3108.89it/s]252565it [01:41, 2958.52it/s]210183it [01:43, 2692.15it/s]234664it [01:41, 3249.53it/s]252932it [01:41, 3153.16it/s]210535it [01:43, 2898.73it/s]234993it [01:41, 3149.54it/s]253292it [01:41, 3278.35it/s]210872it [01:43, 2887.37it/s]235352it [01:41, 3273.95it/s]253624it [01:41, 3193.77it/s]211225it [01:43, 3056.67it/s]235696it [01:41, 3319.01it/s]253969it [01:41, 3265.45it/s]211556it [01:44, 2926.17it/s]236030it [01:41, 3225.39it/s]254298it [01:41, 3145.72it/s]211910it [01:44, 3088.68it/s]236371it [01:42, 3277.07it/s]254654it [01:42, 3261.20it/s]212244it [01:44, 2981.31it/s]236701it [01:42, 3183.99it/s]255014it [01:42, 3357.10it/s]212601it [01:44, 3140.30it/s]237054it [01:42, 3281.22it/s]255352it [01:42, 3253.67it/s]212955it [01:44, 3252.01it/s]237409it [01:42, 3358.91it/s]255715it [01:42, 3351.99it/s]213288it [01:44, 3062.82it/s]237747it [01:42, 3215.98it/s]256052it [01:42, 3231.97it/s]213627it [01:44, 3151.92it/s]238080it [01:42, 3246.70it/s]256407it [01:42, 3320.81it/s]213948it [01:44, 3020.62it/s]238407it [01:42, 3130.40it/s]256763it [01:42, 3389.37it/s]214296it [01:44, 3146.66it/s]238759it [01:42, 3239.51it/s]257104it [01:42, 3270.48it/s]214647it [01:44, 3248.14it/s]239102it [01:42, 3292.41it/s]257455it [01:42, 3339.07it/s]239433it [01:42, 3192.93it/s]214976it [01:45, 3047.73it/s]257791it [01:43, 3248.58it/s]239782it [01:43, 3276.05it/s]215332it [01:45, 3188.12it/s]258138it [01:43, 3310.23it/s]240111it [01:43, 3188.25it/s]215656it [01:45, 3057.70it/s]258471it [01:43, 3225.46it/s]240456it [01:43, 3261.87it/s]216002it [01:45, 3167.93it/s]258836it [01:43, 3344.76it/s]240801it [01:43, 3182.44it/s]216367it [01:45, 3304.79it/s]259197it [01:43, 3419.94it/s]241151it [01:43, 3266.52it/s]259541it [01:43, 3303.59it/s]216701it [01:45, 3132.29it/s]241512it [01:43, 3364.39it/s]259906it [01:43, 3401.00it/s]217062it [01:45, 3264.81it/s]241850it [01:43, 3248.16it/s]217392it [01:45, 3062.04it/s]260248it [01:43, 3133.71it/s]242179it [01:43, 3257.29it/s]217723it [01:45, 3130.58it/s]260567it [01:43, 3119.55it/s]242506it [01:43, 3050.48it/s]218040it [01:46, 3130.95it/s]260895it [01:43, 3158.00it/s]242826it [01:44, 3090.03it/s]261214it [01:44, 3017.91it/s]218356it [01:46, 2881.36it/s]205534it [01:43, 120.62it/s] 243138it [01:44, 3069.40it/s]261551it [01:44, 3115.41it/s]218702it [01:46, 3039.10it/s]205881it [01:44, 172.83it/s]243447it [01:44, 3025.02it/s]261866it [01:44, 3087.41it/s]219011it [01:46, 2952.87it/s]206223it [01:44, 243.55it/s]243797it [01:44, 3160.11it/s]262226it [01:44, 3234.47it/s]219382it [01:46, 3162.60it/s]206505it [01:44, 320.64it/s]244157it [01:44, 3285.93it/s]262577it [01:44, 3313.65it/s]219736it [01:46, 3268.84it/s]206862it [01:44, 454.63it/s]244487it [01:44, 3181.67it/s]262910it [01:44, 3255.45it/s]220067it [01:46, 3154.88it/s]207203it [01:44, 612.36it/s]244842it [01:44, 3286.49it/s]263269it [01:44, 3352.94it/s]220441it [01:46, 3320.23it/s]207548it [01:44, 819.07it/s]245173it [01:44, 3172.03it/s]263606it [01:44, 3258.22it/s]220776it [01:46, 3186.70it/s]207904it [01:44, 1077.44it/s]245534it [01:44, 3296.03it/s]263972it [01:44, 3373.82it/s]221140it [01:47, 3314.21it/s]208228it [01:44, 1316.81it/s]245866it [01:44, 3177.07it/s]264325it [01:45, 3289.81it/s]221485it [01:47, 3205.10it/s]208591it [01:44, 1648.40it/s]246223it [01:45, 3288.63it/s]221857it [01:47, 3350.32it/s]208920it [01:45, 1895.70it/s]246569it [01:45, 3336.00it/s]222223it [01:47, 3438.02it/s]209273it [01:45, 2209.70it/s]246905it [01:45, 3241.02it/s]222570it [01:47, 3291.54it/s]209615it [01:45, 2470.75it/s]247251it [01:45, 3304.02it/s]222933it [01:47, 3387.19it/s]209949it [01:45, 2613.20it/s]247583it [01:45, 3232.09it/s]223275it [01:47, 3289.60it/s]210316it [01:45, 2874.59it/s]247932it [01:45, 3304.09it/s]223612it [01:47, 3310.19it/s]210654it [01:45, 2928.62it/s]248293it [01:45, 3391.46it/s]223984it [01:47, 3428.53it/s]211004it [01:45, 3078.86it/s]248634it [01:45, 3298.68it/s]224329it [01:47, 3312.94it/s]211369it [01:45, 3235.53it/s]248984it [01:45, 3355.14it/s]224684it [01:48, 3379.00it/s]211713it [01:45, 3157.82it/s]249321it [01:46, 3252.74it/s]212075it [01:45, 3285.50it/s]225024it [01:48, 3213.01it/s]249683it [01:46, 3357.02it/s]225389it [01:48, 3333.99it/s]212415it [01:46, 3191.88it/s]250042it [01:46, 3234.61it/s]225725it [01:48, 3244.73it/s]212766it [01:46, 3280.71it/s]250414it [01:46, 3370.94it/s]226068it [01:48, 3295.94it/s]213101it [01:46, 3209.09it/s]250784it [01:46, 3464.51it/s]226413it [01:48, 3340.43it/s]213427it [01:46, 3222.23it/s]251133it [01:46, 3346.53it/s]213782it [01:46, 3315.72it/s]226749it [01:48, 3114.89it/s]251474it [01:46, 3364.38it/s]227067it [01:48, 3130.50it/s]214117it [01:46, 3109.18it/s]251812it [01:46, 3130.35it/s]214433it [01:46, 3071.07it/s]227383it [01:48, 2933.14it/s]252129it [01:46, 2800.93it/s]214743it [01:46, 3068.17it/s]227688it [01:49, 2963.23it/s]252417it [01:47, 2752.13it/s]228002it [01:49, 3012.55it/s]215052it [01:46, 2897.27it/s]252702it [01:47, 2769.47it/s]215412it [01:47, 3090.76it/s]228306it [01:49, 2880.33it/s]253055it [01:47, 2977.46it/s]228659it [01:49, 3061.69it/s]215725it [01:47, 3003.45it/s]253397it [01:47, 3100.46it/s]229005it [01:49, 3175.11it/s]216065it [01:47, 3114.05it/s]253711it [01:47, 3073.62it/s]216423it [01:47, 3247.53it/s]229326it [01:49, 3066.32it/s]254072it [01:47, 3227.44it/s]229690it [01:49, 3227.65it/s]216751it [01:47, 3185.02it/s]254398it [01:47, 3124.18it/s]217115it [01:47, 3315.03it/s]230016it [01:49, 3125.74it/s]254761it [01:47, 3268.58it/s]230367it [01:49, 3233.99it/s]217449it [01:47, 3166.80it/s]255091it [01:47, 3201.25it/s]230703it [01:50, 3270.17it/s]217826it [01:47, 3337.17it/s]255438it [01:47, 3278.53it/s]231032it [01:50, 3186.35it/s]218163it [01:47, 3279.42it/s]255801it [01:48, 3378.81it/s]231388it [01:50, 3293.26it/s]218525it [01:47, 3376.51it/s]256141it [01:48, 3219.40it/s]218901it [01:48, 3485.87it/s]231719it [01:50, 3194.74it/s]256490it [01:48, 3296.28it/s]232069it [01:50, 3280.83it/s]219252it [01:48, 3326.08it/s]256822it [01:48, 3111.42it/s]219622it [01:48, 3430.61it/s]232405it [01:50, 3133.86it/s]257176it [01:48, 3231.36it/s]232757it [01:50, 3242.30it/s]219968it [01:48, 3328.97it/s]257532it [01:48, 3317.54it/s]233118it [01:50, 3347.69it/s]220350it [01:48, 3468.23it/s]257867it [01:48, 3196.30it/s]220699it [01:48, 3363.37it/s]233455it [01:50, 3213.48it/s]258232it [01:48, 3322.17it/s]221048it [01:48, 3398.55it/s]233812it [01:50, 3313.40it/s]258567it [01:48, 3253.76it/s]221420it [01:48, 3491.23it/s]234146it [01:51, 3209.97it/s]258917it [01:49, 3323.19it/s]221771it [01:48, 3382.06it/s]234489it [01:51, 3272.47it/s]259267it [01:49, 3373.95it/s]222117it [01:49, 3403.60it/s]234818it [01:51, 3269.11it/s]259606it [01:49, 3195.01it/s]235147it [01:51, 3134.67it/s]222459it [01:49, 3117.01it/s]259930it [01:49, 3207.50it/s]235485it [01:51, 3200.06it/s]222781it [01:49, 3145.34it/s]260253it [01:49, 2950.61it/s]223131it [01:49, 3243.82it/s]235807it [01:51, 2986.94it/s]260587it [01:49, 3056.17it/s]236125it [01:51, 3038.33it/s]223459it [01:49, 2978.01it/s]260897it [01:49, 3016.65it/s]236440it [01:51, 3068.42it/s]223763it [01:49, 2973.28it/s]261202it [01:49, 2798.46it/s]236749it [01:51, 2897.46it/s]224065it [01:49, 2854.87it/s]261510it [01:49, 2872.65it/s]237057it [01:52, 2946.10it/s]224355it [01:49, 2865.27it/s]261802it [01:50, 2774.66it/s]237398it [01:52, 3077.60it/s]224703it [01:49, 3034.31it/s]262152it [01:50, 2971.21it/s]237709it [01:52, 3036.08it/s]225010it [01:50, 2961.44it/s]262515it [01:50, 3156.66it/s]238069it [01:52, 3197.69it/s]225358it [01:50, 3108.13it/s]262835it [01:50, 3099.91it/s]238391it [01:52, 3117.26it/s]225683it [01:50, 3077.23it/s]263200it [01:50, 3256.52it/s]238743it [01:52, 3232.10it/s]226020it [01:50, 3161.33it/s]263529it [01:50, 3195.85it/s]239093it [01:52, 3307.59it/s]226370it [01:50, 3257.85it/s]263900it [01:50, 3342.18it/s]239426it [01:52, 3128.84it/s]226698it [01:50, 3103.65it/s]264252it [01:50, 3391.52it/s]239789it [01:52, 3271.16it/s]227049it [01:50, 3219.48it/s]240119it [01:52, 3174.24it/s]227374it [01:50, 3099.45it/s]240478it [01:53, 3292.35it/s]227737it [01:50, 3249.75it/s]240810it [01:53, 3192.22it/s]228079it [01:50, 3296.41it/s]241174it [01:53, 3318.18it/s]228411it [01:51, 3199.09it/s]241532it [01:53, 3392.35it/s]228764it [01:51, 3292.09it/s]241873it [01:53, 3181.09it/s]229095it [01:51, 3098.13it/s]242230it [01:53, 3288.34it/s]229459it [01:51, 3248.06it/s]242562it [01:53, 3141.58it/s]229803it [01:51, 3300.93it/s]242911it [01:53, 3238.69it/s]230136it [01:51, 3121.54it/s]243270it [01:53, 3337.97it/s]230471it [01:51, 3185.59it/s]243607it [01:54, 3198.17it/s]230793it [01:51, 3095.72it/s]243970it [01:54, 3319.12it/s]231149it [01:51, 3226.29it/s]231495it [01:52, 3292.66it/s]244305it [01:54, 3136.54it/s]244663it [01:54, 3259.08it/s]231827it [01:52, 3168.07it/s]232175it [01:52, 3254.80it/s]245005it [01:54, 3064.98it/s]245352it [01:54, 3175.93it/s]232503it [01:52, 3020.29it/s]245678it [01:54, 3199.43it/s]232825it [01:52, 3074.74it/s]233150it [01:52, 3123.24it/s]246001it [01:54, 3005.99it/s]246306it [01:54, 2947.67it/s]233465it [01:52, 2802.63it/s]246615it [01:55, 2983.31it/s]233784it [01:52, 2906.83it/s]246916it [01:55, 2943.20it/s]234083it [01:52, 2893.83it/s]247276it [01:55, 3129.28it/s]234433it [01:53, 3054.26it/s]247591it [01:55, 3074.74it/s]234793it [01:53, 3208.26it/s]247949it [01:55, 3219.03it/s]235118it [01:53, 3104.36it/s]248309it [01:55, 3327.69it/s]235476it [01:53, 3240.02it/s]264656it [01:53, 130.87it/s] 248644it [01:55, 3223.46it/s]235803it [01:53, 3149.47it/s]265003it [01:53, 184.32it/s]248974it [01:55, 3244.11it/s]236135it [01:53, 3195.89it/s]265304it [01:53, 247.49it/s]249300it [01:55, 3102.53it/s]236481it [01:53, 3271.19it/s]265658it [01:53, 349.37it/s]249653it [01:55, 3223.07it/s]236810it [01:53, 3078.30it/s]266025it [01:53, 489.95it/s]250012it [01:56, 3327.50it/s]237163it [01:53, 3200.89it/s]266345it [01:54, 642.14it/s]250347it [01:56, 3161.28it/s]237486it [01:54, 3026.65it/s]266694it [01:54, 855.87it/s]250709it [01:56, 3289.32it/s]237841it [01:54, 3170.88it/s]267019it [01:54, 1081.17it/s]251041it [01:56, 3180.45it/s]238183it [01:54, 3239.61it/s]267380it [01:54, 1386.63it/s]251375it [01:56, 3224.47it/s]238510it [01:54, 3139.56it/s]267729it [01:54, 1695.42it/s]251725it [01:56, 3154.40it/s]238850it [01:54, 3212.46it/s]268066it [01:54, 1947.91it/s]252080it [01:56, 3264.18it/s]239174it [01:54, 3119.53it/s]268412it [01:54, 2242.85it/s]252438it [01:56, 3354.20it/s]239532it [01:54, 3249.35it/s]268744it [01:54, 2432.30it/s]252776it [01:56, 3237.37it/s]239876it [01:54, 3303.98it/s]269094it [01:54, 2681.57it/s]253127it [01:57, 3314.49it/s]240208it [01:54, 3174.59it/s]269433it [01:55, 2784.86it/s]253461it [01:57, 3175.37it/s]240543it [01:54, 3224.06it/s]269785it [01:55, 2973.63it/s]253792it [01:57, 3211.04it/s]240867it [01:55, 3064.31it/s]270141it [01:55, 3108.01it/s]254141it [01:57, 3289.12it/s]241198it [01:55, 3133.35it/s]270478it [01:55, 3107.33it/s]254472it [01:57, 3063.33it/s]241534it [01:55, 3197.15it/s]270840it [01:55, 3247.67it/s]254805it [01:57, 3134.79it/s]241856it [01:55, 2944.83it/s]271179it [01:55, 3033.54it/s]242172it [01:55, 3002.94it/s]255122it [01:57, 2739.66it/s]271495it [01:55, 2929.92it/s]242477it [01:55, 3004.55it/s]255433it [01:57, 2835.50it/s]271802it [01:55, 2966.08it/s]255726it [01:57, 2814.93it/s]242781it [01:55, 2839.76it/s]272106it [01:55, 2835.06it/s]243114it [01:55, 2974.65it/s]272438it [01:55, 2967.80it/s]256014it [01:58, 2629.70it/s]243415it [01:55, 2919.25it/s]256380it [01:58, 2904.43it/s]272794it [01:56, 2993.62it/s]243781it [01:56, 3127.80it/s]256747it [01:58, 3115.62it/s]273142it [01:56, 3128.29it/s]244116it [01:56, 3191.30it/s]257066it [01:58, 3084.04it/s]273487it [01:56, 3219.58it/s]244438it [01:56, 3089.80it/s]257417it [01:58, 3203.70it/s]273812it [01:56, 3069.15it/s]244776it [01:56, 3171.62it/s]257742it [01:58, 3150.46it/s]274176it [01:56, 3228.90it/s]245095it [01:56, 3117.32it/s]258093it [01:58, 3251.87it/s]274503it [01:56, 3139.35it/s]245427it [01:56, 3174.64it/s]258445it [01:58, 3194.55it/s]274864it [01:56, 3271.73it/s]245790it [01:56, 3307.24it/s]258813it [01:58, 3331.52it/s]275210it [01:56, 3325.28it/s]246122it [01:56, 3227.65it/s]259182it [01:59, 3433.06it/s]275545it [01:56, 3254.13it/s]246460it [01:56, 3269.66it/s]259528it [01:59, 3323.22it/s]275896it [01:57, 3326.58it/s]246788it [01:56, 3204.33it/s]259896it [01:59, 3423.60it/s]276231it [01:57, 3255.03it/s]247129it [01:57, 3263.75it/s]260241it [01:59, 3308.21it/s]276571it [01:57, 3296.38it/s]247479it [01:57, 3331.49it/s]260588it [01:59, 3352.81it/s]276917it [01:57, 3344.08it/s]247813it [01:57, 3126.15it/s]260949it [01:59, 3426.70it/s]277253it [01:57, 3151.43it/s]248170it [01:57, 3250.10it/s]261293it [01:59, 3323.21it/s]277604it [01:57, 3250.79it/s]248498it [01:57, 3134.70it/s]261662it [01:59, 3428.56it/s]277932it [01:57, 3170.75it/s]248863it [01:57, 3279.19it/s]262007it [01:59, 3322.10it/s]278284it [01:57, 3268.83it/s]249203it [01:57, 3217.63it/s]262370it [01:59, 3408.77it/s]278641it [01:57, 3353.55it/s]249542it [01:57, 3264.66it/s]262713it [02:00, 3311.81it/s]278978it [01:57, 3238.20it/s]249903it [01:57, 3356.98it/s]263064it [02:00, 3366.25it/s]279337it [01:58, 3319.24it/s]250241it [01:58, 3215.95it/s]263428it [02:00, 3445.09it/s]279671it [01:58, 3214.17it/s]250600it [01:58, 3321.68it/s]263774it [02:00, 3324.72it/s]280015it [01:58, 3277.38it/s]264121it [02:00, 3366.07it/s]250935it [01:58, 3010.52it/s]280352it [01:58, 3303.64it/s]251297it [01:58, 3174.87it/s]280684it [01:58, 3035.29it/s]251621it [01:58, 3011.39it/s]280997it [01:58, 3060.86it/s]251928it [01:58, 2809.81it/s]281307it [01:58, 2921.95it/s]252253it [01:58, 2926.66it/s]281614it [01:58, 2958.29it/s]281948it [01:58, 3042.59it/s]252563it [01:58, 2838.21it/s]282255it [01:59, 3032.47it/s]252919it [01:58, 3034.33it/s]282612it [01:59, 3187.67it/s]253278it [01:59, 3189.36it/s]282933it [01:59, 3127.73it/s]253602it [01:59, 3095.63it/s]283276it [01:59, 3213.82it/s]253961it [01:59, 3233.73it/s]283628it [01:59, 3301.73it/s]254288it [01:59, 3105.34it/s]283960it [01:59, 3169.05it/s]254630it [01:59, 3192.38it/s]284285it [01:59, 3191.26it/s]254963it [01:59, 3231.44it/s]284606it [01:59, 3006.09it/s]255289it [01:59, 3130.95it/s]284931it [01:59, 3074.50it/s]255624it [01:59, 3191.71it/s]285260it [02:00, 3134.34it/s]255945it [01:59, 3124.63it/s]256311it [01:59, 3277.16it/s]285576it [02:00, 2982.37it/s]256657it [02:00, 3330.32it/s]285907it [02:00, 3074.29it/s]256992it [02:00, 3230.17it/s]286234it [02:00, 2935.34it/s]257331it [02:00, 3275.70it/s]286564it [02:00, 3034.59it/s]257660it [02:00, 3168.72it/s]286918it [02:00, 3176.74it/s]287112it [02:00, 2380.42it/s]
2022-08-15 15:15:41 | INFO | root | success load 287112 data
2022-08-15 15:15:41 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-08-15 15:15:41 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-08-15 15:15:41 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-08-15 15:15:41 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-08-15 15:15:41 | INFO | transformer.tokenization_utils | loading file None
2022-08-15 15:15:41 | INFO | transformer.tokenization_utils | loading file None
2022-08-15 15:15:41 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
257992it [02:00, 3211.76it/s]258345it [02:00, 3302.78it/s]258677it [02:00, 3193.86it/s]259034it [02:00, 3300.04it/s]259366it [02:00, 3222.27it/s]259737it [02:01, 3362.03it/s]260098it [02:01, 3433.14it/s]260443it [02:01, 3176.49it/s]260798it [02:01, 3279.27it/s]264252it [02:01, 3391.52it/s]264392it [02:01, 86.80it/s]  264671it [02:01, 121.50it/s]261130it [02:01, 3096.02it/s]261445it [02:01, 3110.29it/s]264993it [02:01, 177.03it/s]261760it [02:01, 3084.33it/s]265308it [02:01, 250.10it/s]265660it [02:01, 362.43it/s]262071it [02:01, 2838.02it/s]266015it [02:02, 511.55it/s]262434it [02:01, 3051.85it/s]266341it [02:02, 680.27it/s]262745it [02:02, 3016.60it/s]266687it [02:02, 907.43it/s]263100it [02:02, 3166.06it/s]263457it [02:02, 3280.12it/s]267014it [02:02, 1141.82it/s]267358it [02:02, 1437.56it/s]263788it [02:02, 3106.29it/s]267699it [02:02, 1743.52it/s]264157it [02:02, 3270.18it/s]268029it [02:02, 2004.47it/s]268380it [02:02, 2303.30it/s]268711it [02:02, 2444.96it/s]269059it [02:02, 2690.05it/s]269430it [02:03, 2947.89it/s]269770it [02:03, 2987.30it/s]270137it [02:03, 3169.29it/s]270479it [02:03, 3154.64it/s]270836it [02:03, 3269.89it/s]271176it [02:03, 3128.71it/s]271518it [02:03, 3209.32it/s]271857it [02:03, 3260.23it/s]272189it [02:03, 3051.45it/s]272525it [02:04, 3135.24it/s]272844it [02:04, 2984.01it/s]273194it [02:04, 3125.13it/s]273541it [02:04, 3222.52it/s]273867it [02:04, 3126.90it/s]274199it [02:04, 3181.77it/s]274520it [02:04, 3106.54it/s]274866it [02:04, 3206.41it/s]275189it [02:04, 3198.13it/s]275510it [02:04, 3027.41it/s]275816it [02:05, 3016.00it/s]276120it [02:05, 2881.88it/s]276411it [02:05, 2883.93it/s]276724it [02:05, 2952.66it/s]277031it [02:05, 2985.31it/s]277395it [02:05, 3175.03it/s]277741it [02:05, 3258.64it/s]278068it [02:05, 3185.42it/s]278410it [02:05, 3253.30it/s]278737it [02:06, 3191.55it/s]279086it [02:06, 3277.23it/s]279428it [02:06, 3317.44it/s]279761it [02:06, 3141.39it/s]280078it [02:06, 3141.15it/s]280394it [02:06, 2958.07it/s]280715it [02:06, 3028.46it/s]281052it [02:06, 3124.32it/s]281367it [02:06, 2919.77it/s]281713it [02:06, 3068.78it/s]282031it [02:07, 2969.52it/s]282386it [02:07, 3131.16it/s]282725it [02:07, 3203.69it/s]283049it [02:07, 3118.54it/s]283404it [02:07, 3241.31it/s]283731it [02:07, 3123.89it/s]284093it [02:07, 3263.99it/s]284435it [02:07, 3308.24it/s]284768it [02:07, 3218.39it/s]285112it [02:08, 3280.61it/s]264459it [02:10, 116.12it/s] 285442it [02:08, 3218.59it/s]264815it [02:10, 164.79it/s]285785it [02:08, 3279.00it/s]265180it [02:10, 233.77it/s]286153it [02:08, 3395.22it/s]265481it [02:10, 309.93it/s]286494it [02:08, 3303.77it/s]265844it [02:10, 435.32it/s]286846it [02:08, 3365.96it/s]266162it [02:10, 573.12it/s]287112it [02:08, 2232.04it/s]
266506it [02:10, 767.60it/s]266867it [02:10, 1018.68it/s]267200it [02:11, 1259.50it/s]267551it [02:11, 1565.73it/s]267881it [02:11, 1805.13it/s]268244it [02:11, 2138.97it/s]268594it [02:11, 2316.55it/s]268953it [02:11, 2597.94it/s]269312it [02:11, 2834.97it/s]269650it [02:11, 2873.62it/s]270011it [02:11, 3064.20it/s]270348it [02:11, 3039.81it/s]270709it [02:12, 3194.29it/s]271054it [02:12, 3264.04it/s]271393it [02:12, 3177.56it/s]271748it [02:12, 3280.12it/s]272083it [02:12, 3144.83it/s]272423it [02:12, 3215.87it/s]272755it [02:12, 3245.60it/s]273083it [02:12, 3021.85it/s]273391it [02:13, 2591.23it/s]273664it [02:13, 2208.24it/s]273966it [02:13, 2397.26it/s]274299it [02:13, 2629.50it/s]274579it [02:13, 2600.24it/s]274934it [02:13, 2853.16it/s]275295it [02:13, 3060.98it/s]275611it [02:13, 2967.67it/s]275972it [02:13, 3146.78it/s]276293it [02:14, 3051.75it/s]276637it [02:14, 3160.75it/s]276966it [02:14, 3195.76it/s]277289it [02:14, 3060.43it/s]277616it [02:14, 3119.47it/s]277931it [02:14, 3093.42it/s]278291it [02:14, 3238.40it/s]278635it [02:14, 3296.66it/s]278966it [02:14, 3243.81it/s]279330it [02:14, 3358.38it/s]279667it [02:15, 3280.40it/s]280035it [02:15, 3395.53it/s]280376it [02:15, 3298.31it/s]280749it [02:15, 3423.01it/s]281100it [02:15, 3448.35it/s]281446it [02:15, 3353.97it/s]281814it [02:15, 3447.03it/s]282160it [02:15, 3353.64it/s]282523it [02:15, 3430.86it/s]282868it [02:15, 3420.12it/s]264157it [02:13, 3270.18it/s]264393it [02:13, 90.24it/s]  283211it [02:16, 3210.89it/s]264737it [02:13, 130.94it/s]283535it [02:16, 3205.33it/s]265093it [02:14, 189.43it/s]283858it [02:16, 3008.33it/s]265434it [02:14, 265.31it/s]284203it [02:16, 3130.53it/s]265780it [02:14, 370.04it/s]284538it [02:16, 3190.75it/s]266115it [02:14, 500.70it/s]284860it [02:16, 3088.86it/s]266464it [02:14, 679.31it/s]285218it [02:16, 3228.34it/s]266827it [02:14, 911.40it/s]285544it [02:16, 3126.98it/s]267168it [02:14, 1145.64it/s]285887it [02:16, 3211.81it/s]267533it [02:14, 1457.04it/s]286234it [02:17, 3155.32it/s]267872it [02:14, 1708.27it/s]286566it [02:17, 3201.79it/s]268225it [02:14, 2024.93it/s]286917it [02:17, 3288.90it/s]268562it [02:15, 2292.10it/s]287112it [02:17, 2090.14it/s]
268897it [02:15, 2422.89it/s]269239it [02:15, 2653.39it/s]269565it [02:15, 2687.55it/s]269922it [02:15, 2909.77it/s]270264it [02:15, 3043.89it/s]270594it [02:15, 2963.55it/s]270947it [02:15, 3116.76it/s]271273it [02:15, 2962.53it/s]271627it [02:16, 3119.86it/s]271952it [02:16, 3054.00it/s]272302it [02:16, 3151.59it/s]272660it [02:16, 3271.60it/s]272992it [02:16, 3164.73it/s]273348it [02:16, 3274.82it/s]273679it [02:16, 3151.67it/s]274040it [02:16, 3280.60it/s]274401it [02:16, 3373.85it/s]274741it [02:16, 3263.75it/s]275087it [02:17, 3318.36it/s]275421it [02:17, 3175.77it/s]275741it [02:17, 3168.82it/s]276068it [02:17, 3197.48it/s]276389it [02:17, 2924.23it/s]276687it [02:17, 2909.98it/s]276992it [02:17, 2758.20it/s]277314it [02:17, 2882.55it/s]277624it [02:17, 2941.74it/s]277922it [02:18, 2867.75it/s]278268it [02:18, 3035.64it/s]278610it [02:18, 3144.92it/s]278927it [02:18, 3028.70it/s]279281it [02:18, 3173.82it/s]279601it [02:18, 3064.10it/s]279961it [02:18, 3215.17it/s]280306it [02:18, 3281.35it/s]280636it [02:18, 3206.57it/s]281004it [02:19, 3342.74it/s]281340it [02:19, 3214.27it/s]281705it [02:19, 3337.98it/s]282041it [02:19, 3207.03it/s]282406it [02:19, 3331.70it/s]282770it [02:19, 3419.68it/s]283114it [02:19, 3209.87it/s]283456it [02:19, 3266.75it/s]283786it [02:19, 3093.96it/s]284129it [02:19, 3186.29it/s]284482it [02:20, 3273.83it/s]284812it [02:20, 3123.84it/s]285168it [02:20, 3244.30it/s]285496it [02:20, 3072.36it/s]285838it [02:20, 3167.82it/s]286198it [02:20, 3288.64it/s]286530it [02:20, 3143.95it/s]286891it [02:20, 3274.39it/s]287112it [02:20, 2037.14it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-08-15 15:21:26 | INFO | train_inner | epoch 001:    100 / 1122 loss=nan, nll_loss=14.051, mask_ins=7.571, word_ins_ml=14.153, word_reposition=3.005, kpe=nan, ppl=nan, wps=6374, ups=0.31, wpb=20527, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=18.283, clip=0, loss_scale=128, train_wall=300, wall=466
2022-08-15 15:26:37 | INFO | train_inner | epoch 001:    200 / 1122 loss=20.586, nll_loss=12.501, mask_ins=4.36, word_ins_ml=12.765, word_reposition=1.561, kpe=1.899, ppl=1.57366e+06, wps=6624.2, ups=0.32, wpb=20583.2, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=19.051, clip=0, loss_scale=128, train_wall=265, wall=777
2022-08-15 15:31:44 | INFO | train_inner | epoch 001:    300 / 1122 loss=17.159, nll_loss=11.407, mask_ins=2.196, word_ins_ml=11.819, word_reposition=1.359, kpe=1.784, ppl=146304, wps=6709.6, ups=0.33, wpb=20561.3, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=4.008, clip=0, loss_scale=128, train_wall=263, wall=1083
2022-08-15 15:36:57 | INFO | train_inner | epoch 001:    400 / 1122 loss=16.411, nll_loss=10.862, mask_ins=2, word_ins_ml=11.397, word_reposition=1.3, kpe=1.714, ppl=87156.8, wps=6574.1, ups=0.32, wpb=20576.5, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=1.618, clip=0, loss_scale=128, train_wall=265, wall=1396
2022-08-15 15:42:13 | INFO | train_inner | epoch 001:    500 / 1122 loss=16.134, nll_loss=10.758, mask_ins=1.852, word_ins_ml=11.331, word_reposition=1.304, kpe=1.647, ppl=71933.1, wps=6481.3, ups=0.32, wpb=20523.5, bsz=256, num_updates=500, lr=5.009e-05, gnorm=1.462, clip=0, loss_scale=128, train_wall=266, wall=1713
2022-08-15 15:47:37 | INFO | train_inner | epoch 001:    600 / 1122 loss=16.003, nll_loss=10.713, mask_ins=1.83, word_ins_ml=11.298, word_reposition=1.295, kpe=1.581, ppl=65693.3, wps=6338.5, ups=0.31, wpb=20491.4, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=1.397, clip=0, loss_scale=242, train_wall=270, wall=2036
2022-08-15 15:52:55 | INFO | train_inner | epoch 001:    700 / 1122 loss=15.92, nll_loss=10.671, mask_ins=1.845, word_ins_ml=11.262, word_reposition=1.289, kpe=1.524, ppl=62013.1, wps=6451.1, ups=0.31, wpb=20542.5, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=1.471, clip=0, loss_scale=256, train_wall=268, wall=2355
2022-08-15 15:58:17 | INFO | train_inner | epoch 001:    800 / 1122 loss=15.831, nll_loss=10.627, mask_ins=1.835, word_ins_ml=11.224, word_reposition=1.286, kpe=1.486, ppl=58298.9, wps=6387.1, ups=0.31, wpb=20579, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=1.508, clip=0, loss_scale=256, train_wall=269, wall=2677
2022-08-15 16:03:35 | INFO | train_inner | epoch 001:    900 / 1122 loss=15.746, nll_loss=10.571, mask_ins=1.835, word_ins_ml=11.175, word_reposition=1.286, kpe=1.449, ppl=54951.2, wps=6441.5, ups=0.31, wpb=20464, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=1.378, clip=0, loss_scale=256, train_wall=267, wall=2995
2022-08-15 16:08:51 | INFO | train_inner | epoch 001:   1000 / 1122 loss=15.667, nll_loss=10.51, mask_ins=1.837, word_ins_ml=11.123, word_reposition=1.279, kpe=1.428, ppl=52024.1, wps=6519.1, ups=0.32, wpb=20597.8, bsz=256, num_updates=1000, lr=0.00010008, gnorm=1.488, clip=0, loss_scale=256, train_wall=263, wall=3310
2022-08-15 16:14:10 | INFO | train_inner | epoch 001:   1100 / 1122 loss=15.575, nll_loss=10.456, mask_ins=1.818, word_ins_ml=11.077, word_reposition=1.275, kpe=1.405, ppl=48814.9, wps=6424.8, ups=0.31, wpb=20473.7, bsz=256, num_updates=1100, lr=0.000110078, gnorm=1.382, clip=0, loss_scale=453, train_wall=269, wall=3629
2022-08-15 16:15:20 | INFO | train | epoch 001 | loss nan | nll_loss 11.179 | mask_ins 2.619 | word_ins_ml 11.681 | word_reposition 1.472 | kpe nan | ppl nan | wps 6476.2 | ups 0.32 | wpb 20520.3 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 4.759 | clip 0 | loss_scale 220 | train_wall 3024 | wall 3699
2022-08-15 16:16:59 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 16.335 | nll_loss 10.606 | mask_ins 2.219 | word_ins_ml 11.212 | word_reposition 1.205 | kpe 1.699 | ppl 82661.6 | wps 9990.5 | wpb 2367.6 | bsz 32 | num_updates 1122
2022-08-15 16:17:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_best.pt (epoch 1 @ 1122 updates, score 16.335) (writing took 3.947732759639621 seconds)
2022-08-15 16:21:13 | INFO | train_inner | epoch 002:     78 / 1122 loss=15.522, nll_loss=10.389, mask_ins=1.836, word_ins_ml=11.02, word_reposition=1.272, kpe=1.393, ppl=47050.6, wps=4807.3, ups=0.24, wpb=20333.3, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=1.462, clip=0, loss_scale=512, train_wall=268, wall=4052
2022-08-15 16:26:35 | INFO | train_inner | epoch 002:    178 / 1122 loss=15.458, nll_loss=10.335, mask_ins=1.826, word_ins_ml=10.976, word_reposition=1.28, kpe=1.377, ppl=45022.2, wps=6381, ups=0.31, wpb=20587.3, bsz=256, num_updates=1300, lr=0.000130074, gnorm=1.417, clip=0, loss_scale=512, train_wall=270, wall=4375
2022-08-15 16:31:57 | INFO | train_inner | epoch 002:    278 / 1122 loss=15.404, nll_loss=10.285, mask_ins=1.837, word_ins_ml=10.933, word_reposition=1.271, kpe=1.363, ppl=43372.6, wps=6396.5, ups=0.31, wpb=20599.8, bsz=256, num_updates=1400, lr=0.000140072, gnorm=1.37, clip=0, loss_scale=512, train_wall=270, wall=4697
2022-08-15 16:37:13 | INFO | train_inner | epoch 002:    378 / 1122 loss=15.327, nll_loss=10.228, mask_ins=1.828, word_ins_ml=10.886, word_reposition=1.258, kpe=1.354, ppl=41102.2, wps=6444.2, ups=0.32, wpb=20347.3, bsz=256, num_updates=1500, lr=0.00015007, gnorm=1.476, clip=0, loss_scale=512, train_wall=265, wall=5013
2022-08-15 16:42:31 | INFO | train_inner | epoch 002:    478 / 1122 loss=15.284, nll_loss=10.173, mask_ins=1.833, word_ins_ml=10.839, word_reposition=1.268, kpe=1.344, ppl=39902.8, wps=6464.7, ups=0.31, wpb=20567.7, bsz=256, num_updates=1600, lr=0.000160068, gnorm=1.444, clip=0, loss_scale=845, train_wall=268, wall=5331
2022-08-15 16:48:09 | INFO | train_inner | epoch 002:    578 / 1122 loss=15.237, nll_loss=10.124, mask_ins=1.826, word_ins_ml=10.798, word_reposition=1.271, kpe=1.342, ppl=38617.3, wps=6084.5, ups=0.3, wpb=20536.9, bsz=256, num_updates=1700, lr=0.000170066, gnorm=1.31, clip=0, loss_scale=1024, train_wall=287, wall=5668
2022-08-15 16:53:31 | INFO | train_inner | epoch 002:    678 / 1122 loss=15.176, nll_loss=10.072, mask_ins=1.828, word_ins_ml=10.753, word_reposition=1.265, kpe=1.329, ppl=37015.8, wps=6344.3, ups=0.31, wpb=20477.4, bsz=256, num_updates=1800, lr=0.000180064, gnorm=1.336, clip=0, loss_scale=1024, train_wall=270, wall=5991
2022-08-15 16:58:51 | INFO | train_inner | epoch 002:    778 / 1122 loss=nan, nll_loss=10.031, mask_ins=1.814, word_ins_ml=10.72, word_reposition=1.264, kpe=nan, ppl=nan, wps=6434.6, ups=0.31, wpb=20576, bsz=256, num_updates=1900, lr=0.000190062, gnorm=1.361, clip=0, loss_scale=1024, train_wall=269, wall=6311
2022-08-15 17:04:12 | INFO | train_inner | epoch 002:    878 / 1122 loss=15.088, nll_loss=9.984, mask_ins=1.834, word_ins_ml=10.68, word_reposition=1.268, kpe=1.307, ppl=34838.9, wps=6366.8, ups=0.31, wpb=20447.7, bsz=256, num_updates=2000, lr=0.00020006, gnorm=1.352, clip=0, loss_scale=1024, train_wall=270, wall=6632
2022-08-15 17:09:32 | INFO | train_inner | epoch 002:    978 / 1122 loss=15.012, nll_loss=9.935, mask_ins=1.818, word_ins_ml=10.637, word_reposition=1.266, kpe=1.291, ppl=33049.3, wps=6411.2, ups=0.31, wpb=20513.5, bsz=256, num_updates=2100, lr=0.000210058, gnorm=1.34, clip=0, loss_scale=1567, train_wall=268, wall=6952
2022-08-15 17:14:50 | INFO | train_inner | epoch 002:   1078 / 1122 loss=nan, nll_loss=9.887, mask_ins=1.829, word_ins_ml=10.597, word_reposition=1.276, kpe=nan, ppl=nan, wps=6517.7, ups=0.31, wpb=20708.1, bsz=256, num_updates=2200, lr=0.000220056, gnorm=1.299, clip=0, loss_scale=2048, train_wall=267, wall=7270
2022-08-15 17:17:09 | INFO | train | epoch 002 | loss nan | nll_loss 10.115 | mask_ins 1.828 | word_ins_ml 10.79 | word_reposition 1.269 | kpe nan | ppl nan | wps 6206.3 | ups 0.3 | wpb 20521 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 1.378 | clip 0 | loss_scale 1015 | train_wall 3031 | wall 7409
2022-08-15 17:18:47 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 15.895 | nll_loss 10.003 | mask_ins 2.296 | word_ins_ml 10.709 | word_reposition 1.16 | kpe 1.73 | ppl 60934.6 | wps 10175 | wpb 2367.6 | bsz 32 | num_updates 2244 | best_loss 15.895
2022-08-15 17:19:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_best.pt (epoch 2 @ 2244 updates, score 15.895) (writing took 27.87962793931365 seconds)
2022-08-15 17:22:13 | INFO | train_inner | epoch 003:     56 / 1122 loss=nan, nll_loss=9.837, mask_ins=1.83, word_ins_ml=10.554, word_reposition=1.274, kpe=nan, ppl=nan, wps=4608, ups=0.23, wpb=20387.7, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=1.515, clip=0, loss_scale=2048, train_wall=267, wall=7712
2022-08-15 17:27:26 | INFO | train_inner | epoch 003:    156 / 1122 loss=14.847, nll_loss=9.764, mask_ins=1.826, word_ins_ml=10.491, word_reposition=1.286, kpe=1.244, ppl=29468.4, wps=6535.6, ups=0.32, wpb=20466.9, bsz=256, num_updates=2400, lr=0.000240052, gnorm=1.303, clip=0, loss_scale=2048, train_wall=261, wall=8025
2022-08-15 17:32:53 | INFO | train_inner | epoch 003:    256 / 1122 loss=14.805, nll_loss=9.69, mask_ins=1.811, word_ins_ml=10.427, word_reposition=1.33, kpe=1.236, ppl=28620.3, wps=6286, ups=0.31, wpb=20590.4, bsz=256, num_updates=2500, lr=0.00025005, gnorm=1.394, clip=0, loss_scale=2048, train_wall=275, wall=8353
2022-08-15 17:38:11 | INFO | train_inner | epoch 003:    356 / 1122 loss=14.756, nll_loss=9.617, mask_ins=1.826, word_ins_ml=10.364, word_reposition=1.345, kpe=1.221, ppl=27668, wps=6466.1, ups=0.31, wpb=20552.9, bsz=256, num_updates=2600, lr=0.000260048, gnorm=1.492, clip=0, loss_scale=2888, train_wall=266, wall=8671
2022-08-15 17:43:28 | INFO | train_inner | epoch 003:    456 / 1122 loss=14.696, nll_loss=9.541, mask_ins=1.822, word_ins_ml=10.299, word_reposition=1.353, kpe=1.221, ppl=26534.8, wps=6426.1, ups=0.32, wpb=20384, bsz=256, num_updates=2700, lr=0.000270046, gnorm=1.374, clip=0, loss_scale=4096, train_wall=267, wall=8988
2022-08-15 17:48:46 | INFO | train_inner | epoch 003:    556 / 1122 loss=14.651, nll_loss=9.49, mask_ins=1.827, word_ins_ml=10.255, word_reposition=1.364, kpe=1.205, ppl=25735.1, wps=6450.8, ups=0.31, wpb=20480.9, bsz=256, num_updates=2800, lr=0.000280044, gnorm=1.466, clip=0, loss_scale=4096, train_wall=267, wall=9305
2022-08-15 17:54:50 | INFO | train_inner | epoch 003:    656 / 1122 loss=14.621, nll_loss=9.431, mask_ins=1.823, word_ins_ml=10.205, word_reposition=1.393, kpe=1.2, ppl=25203, wps=5668.1, ups=0.27, wpb=20612.3, bsz=256, num_updates=2900, lr=0.000290042, gnorm=1.588, clip=0, loss_scale=4096, train_wall=312, wall=9669
2022-08-15 18:00:12 | INFO | train_inner | epoch 003:    756 / 1122 loss=14.538, nll_loss=9.365, mask_ins=1.809, word_ins_ml=10.148, word_reposition=1.387, kpe=1.194, ppl=23796.1, wps=6389.8, ups=0.31, wpb=20597.8, bsz=256, num_updates=3000, lr=0.00030004, gnorm=1.522, clip=0, loss_scale=4096, train_wall=271, wall=9991
2022-08-15 18:05:36 | INFO | train_inner | epoch 003:    856 / 1122 loss=14.492, nll_loss=9.303, mask_ins=1.823, word_ins_ml=10.096, word_reposition=1.388, kpe=1.185, ppl=23049.6, wps=6361.6, ups=0.31, wpb=20609.8, bsz=256, num_updates=3100, lr=0.000310038, gnorm=1.55, clip=0, loss_scale=5284, train_wall=272, wall=10315
2022-08-15 18:11:02 | INFO | train_inner | epoch 003:    956 / 1122 loss=nan, nll_loss=9.247, mask_ins=1.833, word_ins_ml=10.047, word_reposition=1.407, kpe=nan, ppl=nan, wps=6303.6, ups=0.31, wpb=20572.9, bsz=256, num_updates=3200, lr=0.000320036, gnorm=1.684, clip=0, loss_scale=8192, train_wall=273, wall=10642
2022-08-15 18:16:40 | INFO | train_inner | epoch 003:   1056 / 1122 loss=14.405, nll_loss=9.205, mask_ins=1.817, word_ins_ml=10.013, word_reposition=1.404, kpe=1.171, ppl=21696.2, wps=6078.8, ups=0.3, wpb=20512.4, bsz=256, num_updates=3300, lr=0.000330034, gnorm=1.613, clip=0, loss_scale=8192, train_wall=286, wall=10979
2022-08-15 18:20:11 | INFO | train | epoch 003 | loss nan | nll_loss 9.465 | mask_ins 1.822 | word_ins_ml 10.235 | word_reposition 1.363 | kpe nan | ppl nan | wps 6087.9 | ups 0.3 | wpb 20521.3 | bsz 255.8 | num_updates 3366 | lr 0.000336633 | gnorm 1.527 | clip 0 | loss_scale 4598 | train_wall 3077 | wall 11191
2022-08-15 18:21:49 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 15.323 | nll_loss 9.501 | mask_ins 1.985 | word_ins_ml 10.309 | word_reposition 1.396 | kpe 1.633 | ppl 40985.8 | wps 10123.7 | wpb 2367.6 | bsz 32 | num_updates 3366 | best_loss 15.323
2022-08-15 18:22:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_best.pt (epoch 3 @ 3366 updates, score 15.323) (writing took 22.425649328157306 seconds)
2022-08-15 18:24:00 | INFO | train_inner | epoch 004:     34 / 1122 loss=14.352, nll_loss=9.142, mask_ins=1.817, word_ins_ml=9.959, word_reposition=1.41, kpe=1.165, ppl=20907, wps=4627.2, ups=0.23, wpb=20354.1, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=1.842, clip=0, loss_scale=8192, train_wall=269, wall=11419
2022-08-15 18:29:17 | INFO | train_inner | epoch 004:    134 / 1122 loss=14.245, nll_loss=9.041, mask_ins=1.809, word_ins_ml=9.871, word_reposition=1.415, kpe=1.15, ppl=19422.6, wps=6440.7, ups=0.31, wpb=20472.6, bsz=256, num_updates=3500, lr=0.00035003, gnorm=1.63, clip=0, loss_scale=8192, train_wall=268, wall=11737
2022-08-15 18:34:35 | INFO | train_inner | epoch 004:    234 / 1122 loss=14.211, nll_loss=8.998, mask_ins=1.803, word_ins_ml=9.836, word_reposition=1.42, kpe=1.151, ppl=18958.3, wps=6501.2, ups=0.32, wpb=20634.1, bsz=256, num_updates=3600, lr=0.000360028, gnorm=1.593, clip=0, loss_scale=9585, train_wall=267, wall=12054
2022-08-15 18:39:55 | INFO | train_inner | epoch 004:    334 / 1122 loss=14.178, nll_loss=8.936, mask_ins=1.818, word_ins_ml=9.784, word_reposition=1.432, kpe=1.145, ppl=18538.5, wps=6370.1, ups=0.31, wpb=20411.2, bsz=256, num_updates=3700, lr=0.000370026, gnorm=1.733, clip=0, loss_scale=16384, train_wall=270, wall=12375
2022-08-15 18:41:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-15 18:45:19 | INFO | train_inner | epoch 004:    435 / 1122 loss=14.119, nll_loss=8.884, mask_ins=1.802, word_ins_ml=9.74, word_reposition=1.441, kpe=1.137, ppl=17796.4, wps=6375.6, ups=0.31, wpb=20661.7, bsz=256, num_updates=3800, lr=0.000380024, gnorm=1.629, clip=0, loss_scale=10869, train_wall=273, wall=12699
2022-08-15 18:50:34 | INFO | train_inner | epoch 004:    535 / 1122 loss=14.069, nll_loss=8.81, mask_ins=1.796, word_ins_ml=9.677, word_reposition=1.454, kpe=1.142, ppl=17186, wps=6506.8, ups=0.32, wpb=20481.9, bsz=256, num_updates=3900, lr=0.000390022, gnorm=1.707, clip=0, loss_scale=8192, train_wall=263, wall=13014
2022-08-15 18:55:58 | INFO | train_inner | epoch 004:    635 / 1122 loss=nan, nll_loss=8.749, mask_ins=1.822, word_ins_ml=9.624, word_reposition=1.464, kpe=nan, ppl=nan, wps=6329.6, ups=0.31, wpb=20519.3, bsz=256, num_updates=4000, lr=0.00040002, gnorm=1.828, clip=0, loss_scale=8192, train_wall=271, wall=13338
2022-08-15 19:01:18 | INFO | train_inner | epoch 004:    735 / 1122 loss=13.976, nll_loss=8.671, mask_ins=1.799, word_ins_ml=9.558, word_reposition=1.483, kpe=1.136, ppl=16110.9, wps=6454.4, ups=0.31, wpb=20609.6, bsz=256, num_updates=4100, lr=0.000410018, gnorm=1.836, clip=0, loss_scale=8192, train_wall=268, wall=13657
2022-08-15 19:06:37 | INFO | train_inner | epoch 004:    835 / 1122 loss=nan, nll_loss=8.564, mask_ins=1.8, word_ins_ml=9.465, word_reposition=1.49, kpe=nan, ppl=nan, wps=6395.7, ups=0.31, wpb=20446.8, bsz=256, num_updates=4200, lr=0.000420016, gnorm=1.921, clip=0, loss_scale=8192, train_wall=270, wall=13977
2022-08-15 19:11:55 | INFO | train_inner | epoch 004:    935 / 1122 loss=13.802, nll_loss=8.447, mask_ins=1.796, word_ins_ml=9.363, word_reposition=1.511, kpe=1.131, ppl=14282.3, wps=6489.5, ups=0.31, wpb=20633.8, bsz=256, num_updates=4300, lr=0.000430014, gnorm=2.003, clip=0, loss_scale=12780, train_wall=267, wall=14295
2022-08-15 19:17:14 | INFO | train_inner | epoch 004:   1035 / 1122 loss=13.666, nll_loss=8.291, mask_ins=1.786, word_ins_ml=9.227, word_reposition=1.528, kpe=1.126, ppl=13002, wps=6425.9, ups=0.31, wpb=20465.7, bsz=256, num_updates=4400, lr=0.000440012, gnorm=2.102, clip=0, loss_scale=16384, train_wall=267, wall=14613
2022-08-15 19:19:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-15 19:21:50 | INFO | train | epoch 004 | loss nan | nll_loss 8.704 | mask_ins 1.801 | word_ins_ml 9.584 | word_reposition 1.468 | kpe nan | ppl nan | wps 6213.3 | ups 0.3 | wpb 20518.7 | bsz 255.8 | num_updates 4486 | lr 0.00044861 | gnorm 1.818 | clip 0 | loss_scale 10674 | train_wall 3005 | wall 14890
2022-08-15 19:23:27 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 14.922 | nll_loss 8.752 | mask_ins 1.995 | word_ins_ml 9.694 | word_reposition 1.583 | kpe 1.649 | ppl 31051.3 | wps 10220.5 | wpb 2367.6 | bsz 32 | num_updates 4486 | best_loss 14.922
2022-08-15 19:23:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_best.pt (epoch 4 @ 4486 updates, score 14.922) (writing took 16.018150992691517 seconds)
2022-08-15 19:24:30 | INFO | train_inner | epoch 005:     14 / 1122 loss=13.524, nll_loss=8.132, mask_ins=1.767, word_ins_ml=9.088, word_reposition=1.535, kpe=1.134, ppl=11783.4, wps=4676.9, ups=0.23, wpb=20378.3, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=2.163, clip=0, loss_scale=10950, train_wall=271, wall=15049
2022-08-15 19:29:48 | INFO | train_inner | epoch 005:    114 / 1122 loss=13.318, nll_loss=7.884, mask_ins=1.767, word_ins_ml=8.873, word_reposition=1.565, kpe=1.112, ppl=10209.3, wps=6475.8, ups=0.31, wpb=20625.6, bsz=256, num_updates=4600, lr=0.000460008, gnorm=2.059, clip=0, loss_scale=8192, train_wall=267, wall=15368
2022-08-15 19:35:05 | INFO | train_inner | epoch 005:    214 / 1122 loss=13.173, nll_loss=7.714, mask_ins=1.77, word_ins_ml=8.726, word_reposition=1.563, kpe=1.113, ppl=9232.73, wps=6477.4, ups=0.32, wpb=20524.8, bsz=256, num_updates=4700, lr=0.000470006, gnorm=2.102, clip=0, loss_scale=8192, train_wall=267, wall=15684
2022-08-15 19:40:23 | INFO | train_inner | epoch 005:    314 / 1122 loss=13.018, nll_loss=7.538, mask_ins=1.766, word_ins_ml=8.573, word_reposition=1.568, kpe=1.111, ppl=8297.51, wps=6465.3, ups=0.31, wpb=20576.8, bsz=256, num_updates=4800, lr=0.000480004, gnorm=2.168, clip=0, loss_scale=8192, train_wall=268, wall=16003
2022-08-15 19:45:59 | INFO | train_inner | epoch 005:    414 / 1122 loss=nan, nll_loss=7.356, mask_ins=1.739, word_ins_ml=8.416, word_reposition=1.578, kpe=nan, ppl=nan, wps=6098.9, ups=0.3, wpb=20499.2, bsz=256, num_updates=4900, lr=0.000490002, gnorm=2.06, clip=0, loss_scale=8192, train_wall=286, wall=16339
2022-08-15 19:50:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-15 19:51:20 | INFO | train_inner | epoch 005:    515 / 1122 loss=12.652, nll_loss=7.147, mask_ins=1.746, word_ins_ml=8.235, word_reposition=1.568, kpe=1.104, ppl=6436.96, wps=6357.4, ups=0.31, wpb=20374.7, bsz=256, num_updates=5000, lr=0.0005, gnorm=2.227, clip=0, loss_scale=10787, train_wall=269, wall=16659
2022-08-15 19:56:38 | INFO | train_inner | epoch 005:    615 / 1122 loss=12.493, nll_loss=7.016, mask_ins=1.705, word_ins_ml=8.122, word_reposition=1.557, kpe=1.109, ppl=5765.35, wps=6452.1, ups=0.31, wpb=20503.9, bsz=256, num_updates=5100, lr=0.000495074, gnorm=2.065, clip=0, loss_scale=8192, train_wall=267, wall=16977
2022-08-15 20:01:59 | INFO | train_inner | epoch 005:    715 / 1122 loss=12.361, nll_loss=6.844, mask_ins=1.718, word_ins_ml=7.974, word_reposition=1.563, kpe=1.106, ppl=5258.85, wps=6395, ups=0.31, wpb=20525, bsz=256, num_updates=5200, lr=0.00049029, gnorm=2.041, clip=0, loss_scale=8192, train_wall=269, wall=17298
2022-08-15 20:07:16 | INFO | train_inner | epoch 005:    815 / 1122 loss=12.188, nll_loss=6.677, mask_ins=1.691, word_ins_ml=7.829, word_reposition=1.563, kpe=1.105, ppl=4667.26, wps=6506.8, ups=0.31, wpb=20661.6, bsz=256, num_updates=5300, lr=0.000485643, gnorm=2.006, clip=0, loss_scale=8192, train_wall=266, wall=17616
2022-08-15 20:12:29 | INFO | train_inner | epoch 005:    915 / 1122 loss=12.008, nll_loss=6.514, mask_ins=1.692, word_ins_ml=7.687, word_reposition=1.526, kpe=1.104, ppl=4119.67, wps=6555.9, ups=0.32, wpb=20542.5, bsz=256, num_updates=5400, lr=0.000481125, gnorm=2.096, clip=0, loss_scale=8192, train_wall=262, wall=17929
2022-08-15 20:17:48 | INFO | train_inner | epoch 005:   1015 / 1122 loss=nan, nll_loss=6.34, mask_ins=1.668, word_ins_ml=7.536, word_reposition=1.532, kpe=nan, ppl=nan, wps=6416.7, ups=0.31, wpb=20470.4, bsz=256, num_updates=5500, lr=0.000476731, gnorm=1.951, clip=0, loss_scale=9175, train_wall=267, wall=18248
2022-08-15 20:19:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-15 20:23:11 | INFO | train_inner | epoch 005:   1116 / 1122 loss=11.724, nll_loss=6.221, mask_ins=1.67, word_ins_ml=7.433, word_reposition=1.524, kpe=1.098, ppl=3383.8, wps=6382, ups=0.31, wpb=20585.5, bsz=256, num_updates=5600, lr=0.000472456, gnorm=2.085, clip=0, loss_scale=10220, train_wall=270, wall=18571
2022-08-15 20:23:29 | INFO | train | epoch 005 | loss nan | nll_loss 7.033 | mask_ins 1.721 | word_ins_ml 8.136 | word_reposition 1.555 | kpe nan | ppl nan | wps 6214.2 | ups 0.3 | wpb 20523 | bsz 255.8 | num_updates 5606 | lr 0.000472203 | gnorm 2.088 | clip 0 | loss_scale 8696 | train_wall 3014 | wall 18589
2022-08-15 20:25:06 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 13.651 | nll_loss 7.449 | mask_ins 1.881 | word_ins_ml 8.604 | word_reposition 1.583 | kpe 1.584 | ppl 12862 | wps 10183.3 | wpb 2367.6 | bsz 32 | num_updates 5606 | best_loss 13.651
2022-08-15 20:25:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_best.pt (epoch 5 @ 5606 updates, score 13.651) (writing took 15.358574610203505 seconds)
2022-08-15 20:30:20 | INFO | train_inner | epoch 006:     94 / 1122 loss=11.603, nll_loss=6.097, mask_ins=1.674, word_ins_ml=7.326, word_reposition=1.523, kpe=1.08, ppl=3110.47, wps=4740, ups=0.23, wpb=20344.4, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=2.209, clip=0, loss_scale=8192, train_wall=267, wall=19000
2022-08-15 20:35:38 | INFO | train_inner | epoch 006:    194 / 1122 loss=11.394, nll_loss=5.927, mask_ins=1.647, word_ins_ml=7.179, word_reposition=1.49, kpe=1.078, ppl=2690.61, wps=6483.3, ups=0.31, wpb=20599.6, bsz=256, num_updates=5800, lr=0.000464238, gnorm=1.938, clip=0, loss_scale=8192, train_wall=266, wall=19318
2022-08-15 20:40:59 | INFO | train_inner | epoch 006:    294 / 1122 loss=nan, nll_loss=5.872, mask_ins=1.649, word_ins_ml=7.131, word_reposition=1.494, kpe=nan, ppl=nan, wps=6407.7, ups=0.31, wpb=20599.2, bsz=256, num_updates=5900, lr=0.000460287, gnorm=2.021, clip=0, loss_scale=8192, train_wall=270, wall=19639
2022-08-15 20:45:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-15 20:46:25 | INFO | train_inner | epoch 006:    395 / 1122 loss=11.153, nll_loss=5.734, mask_ins=1.635, word_ins_ml=7.012, word_reposition=1.426, kpe=1.081, ppl=2277.88, wps=6317.7, ups=0.31, wpb=20562.2, bsz=256, num_updates=6000, lr=0.000456435, gnorm=2.022, clip=0, loss_scale=7462, train_wall=274, wall=19964
2022-08-15 20:51:42 | INFO | train_inner | epoch 006:    495 / 1122 loss=11.106, nll_loss=5.657, mask_ins=1.645, word_ins_ml=6.945, word_reposition=1.442, kpe=1.074, ppl=2203.63, wps=6470.1, ups=0.32, wpb=20499.6, bsz=256, num_updates=6100, lr=0.000452679, gnorm=2.1, clip=0, loss_scale=4096, train_wall=266, wall=20281
2022-08-15 20:56:59 | INFO | train_inner | epoch 006:    595 / 1122 loss=11.016, nll_loss=5.594, mask_ins=1.632, word_ins_ml=6.889, word_reposition=1.426, kpe=1.069, ppl=2071.12, wps=6424.4, ups=0.32, wpb=20353.6, bsz=256, num_updates=6200, lr=0.000449013, gnorm=2.01, clip=0, loss_scale=4096, train_wall=266, wall=20598
2022-08-15 21:02:21 | INFO | train_inner | epoch 006:    695 / 1122 loss=10.952, nll_loss=5.524, mask_ins=1.638, word_ins_ml=6.827, word_reposition=1.415, kpe=1.071, ppl=1980.35, wps=6405.6, ups=0.31, wpb=20646.7, bsz=256, num_updates=6300, lr=0.000445435, gnorm=2.044, clip=0, loss_scale=4096, train_wall=270, wall=20920
2022-08-15 21:07:41 | INFO | train_inner | epoch 006:    795 / 1122 loss=nan, nll_loss=5.426, mask_ins=1.625, word_ins_ml=6.742, word_reposition=1.388, kpe=nan, ppl=nan, wps=6408.2, ups=0.31, wpb=20504.7, bsz=256, num_updates=6400, lr=0.000441942, gnorm=1.992, clip=0, loss_scale=4096, train_wall=267, wall=21240
2022-08-15 21:13:19 | INFO | train_inner | epoch 006:    895 / 1122 loss=10.766, nll_loss=5.367, mask_ins=1.624, word_ins_ml=6.69, word_reposition=1.381, kpe=1.07, ppl=1741.54, wps=6099.5, ups=0.3, wpb=20646, bsz=256, num_updates=6500, lr=0.000438529, gnorm=1.986, clip=0, loss_scale=4342, train_wall=288, wall=21579
2022-08-15 21:18:37 | INFO | train_inner | epoch 006:    995 / 1122 loss=10.656, nll_loss=5.285, mask_ins=1.613, word_ins_ml=6.618, word_reposition=1.356, kpe=1.069, ppl=1613.9, wps=6452.9, ups=0.32, wpb=20475.3, bsz=256, num_updates=6600, lr=0.000435194, gnorm=1.968, clip=0, loss_scale=8192, train_wall=266, wall=21896
2022-08-15 21:23:55 | INFO | train_inner | epoch 006:   1095 / 1122 loss=10.574, nll_loss=5.223, mask_ins=1.595, word_ins_ml=6.563, word_reposition=1.351, kpe=1.065, ppl=1524.25, wps=6436.3, ups=0.31, wpb=20518.7, bsz=256, num_updates=6700, lr=0.000431934, gnorm=1.922, clip=0, loss_scale=8192, train_wall=269, wall=22215
2022-08-15 21:25:19 | INFO | train | epoch 006 | loss nan | nll_loss 5.597 | mask_ins 1.633 | word_ins_ml 6.891 | word_reposition 1.424 | kpe nan | ppl nan | wps 6199.6 | ups 0.3 | wpb 20519.9 | bsz 255.8 | num_updates 6727 | lr 0.000431067 | gnorm 2.016 | clip 0 | loss_scale 6323 | train_wall 3024 | wall 22299
2022-08-15 21:26:56 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 13.006 | nll_loss 6.914 | mask_ins 1.841 | word_ins_ml 8.151 | word_reposition 1.489 | kpe 1.524 | ppl 8223.97 | wps 10262.7 | wpb 2367.6 | bsz 32 | num_updates 6727 | best_loss 13.006
2022-08-15 21:27:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_best.pt (epoch 6 @ 6727 updates, score 13.006) (writing took 17.90968009084463 seconds)
2022-08-15 21:31:08 | INFO | train_inner | epoch 007:     73 / 1122 loss=10.499, nll_loss=5.164, mask_ins=1.6, word_ins_ml=6.511, word_reposition=1.337, kpe=1.051, ppl=1446.85, wps=4694.8, ups=0.23, wpb=20309.4, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=2.055, clip=0, loss_scale=8192, train_wall=266, wall=22648
2022-08-15 21:36:23 | INFO | train_inner | epoch 007:    173 / 1122 loss=10.298, nll_loss=5.008, mask_ins=1.577, word_ins_ml=6.374, word_reposition=1.304, kpe=1.042, ppl=1258.74, wps=6550.5, ups=0.32, wpb=20599.7, bsz=256, num_updates=6900, lr=0.000425628, gnorm=2.023, clip=0, loss_scale=8192, train_wall=262, wall=22962
2022-08-15 21:41:40 | INFO | train_inner | epoch 007:    273 / 1122 loss=10.294, nll_loss=5.015, mask_ins=1.571, word_ins_ml=6.38, word_reposition=1.297, kpe=1.046, ppl=1255.32, wps=6495.1, ups=0.31, wpb=20643.5, bsz=256, num_updates=7000, lr=0.000422577, gnorm=1.936, clip=0, loss_scale=8192, train_wall=268, wall=23280
2022-08-15 21:43:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-15 21:44:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-15 21:47:07 | INFO | train_inner | epoch 007:    375 / 1122 loss=10.204, nll_loss=4.92, mask_ins=1.58, word_ins_ml=6.296, word_reposition=1.287, kpe=1.041, ppl=1179.78, wps=6281.9, ups=0.31, wpb=20498.4, bsz=256, num_updates=7100, lr=0.000419591, gnorm=1.989, clip=0, loss_scale=8995, train_wall=274, wall=23606
2022-08-15 21:52:22 | INFO | train_inner | epoch 007:    475 / 1122 loss=nan, nll_loss=4.837, mask_ins=1.549, word_ins_ml=6.223, word_reposition=1.263, kpe=nan, ppl=nan, wps=6518.7, ups=0.32, wpb=20532.6, bsz=256, num_updates=7200, lr=0.000416667, gnorm=1.903, clip=0, loss_scale=4096, train_wall=265, wall=23921
2022-08-15 21:57:40 | INFO | train_inner | epoch 007:    575 / 1122 loss=nan, nll_loss=4.796, mask_ins=1.543, word_ins_ml=6.186, word_reposition=1.246, kpe=nan, ppl=nan, wps=6409.7, ups=0.31, wpb=20423.6, bsz=256, num_updates=7300, lr=0.000413803, gnorm=2.038, clip=0, loss_scale=4096, train_wall=268, wall=24240
2022-08-15 22:03:02 | INFO | train_inner | epoch 007:    675 / 1122 loss=9.986, nll_loss=4.761, mask_ins=1.54, word_ins_ml=6.154, word_reposition=1.25, kpe=1.043, ppl=1014.15, wps=6389.2, ups=0.31, wpb=20543.5, bsz=256, num_updates=7400, lr=0.000410997, gnorm=1.941, clip=0, loss_scale=4096, train_wall=270, wall=24561
2022-08-15 22:08:17 | INFO | train_inner | epoch 007:    775 / 1122 loss=9.898, nll_loss=4.656, mask_ins=1.543, word_ins_ml=6.061, word_reposition=1.253, kpe=1.04, ppl=953.81, wps=6514.1, ups=0.32, wpb=20543.4, bsz=256, num_updates=7500, lr=0.000408248, gnorm=1.981, clip=0, loss_scale=4096, train_wall=266, wall=24877
2022-08-15 22:13:36 | INFO | train_inner | epoch 007:    875 / 1122 loss=9.795, nll_loss=4.604, mask_ins=1.524, word_ins_ml=6.015, word_reposition=1.219, kpe=1.038, ppl=888.55, wps=6439.4, ups=0.31, wpb=20555.9, bsz=256, num_updates=7600, lr=0.000405554, gnorm=2.056, clip=0, loss_scale=5652, train_wall=268, wall=25196
2022-08-15 22:18:57 | INFO | train_inner | epoch 007:    975 / 1122 loss=9.737, nll_loss=4.561, mask_ins=1.506, word_ins_ml=5.976, word_reposition=1.216, kpe=1.039, ppl=853.58, wps=6409.2, ups=0.31, wpb=20560.2, bsz=256, num_updates=7700, lr=0.000402911, gnorm=1.943, clip=0, loss_scale=8192, train_wall=268, wall=25517
2022-08-15 22:24:57 | INFO | train_inner | epoch 007:   1075 / 1122 loss=9.703, nll_loss=4.551, mask_ins=1.494, word_ins_ml=5.967, word_reposition=1.207, kpe=1.036, ppl=833.67, wps=5707.2, ups=0.28, wpb=20532.8, bsz=256, num_updates=7800, lr=0.00040032, gnorm=1.983, clip=0, loss_scale=8192, train_wall=309, wall=25877
2022-08-15 22:27:24 | INFO | train | epoch 007 | loss nan | nll_loss 4.784 | mask_ins 1.544 | word_ins_ml 6.174 | word_reposition 1.257 | kpe nan | ppl nan | wps 6170 | ups 0.3 | wpb 20518.3 | bsz 255.8 | num_updates 7847 | lr 0.00039912 | gnorm 1.994 | clip 0 | loss_scale 6578 | train_wall 3039 | wall 26024
2022-08-15 22:29:01 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 12.721 | nll_loss 6.668 | mask_ins 1.778 | word_ins_ml 7.956 | word_reposition 1.409 | kpe 1.578 | ppl 6752.53 | wps 10166.5 | wpb 2367.6 | bsz 32 | num_updates 7847 | best_loss 12.721
2022-08-15 22:29:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_best.pt (epoch 7 @ 7847 updates, score 12.721) (writing took 32.489830795675516 seconds)
2022-08-15 22:32:23 | INFO | train_inner | epoch 008:     53 / 1122 loss=9.619, nll_loss=4.469, mask_ins=1.497, word_ins_ml=5.895, word_reposition=1.198, kpe=1.028, ppl=786.08, wps=4566.9, ups=0.22, wpb=20381.3, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=2.135, clip=0, loss_scale=8192, train_wall=267, wall=26323
2022-08-15 22:33:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-15 22:38:01 | INFO | train_inner | epoch 008:    154 / 1122 loss=nan, nll_loss=4.353, mask_ins=1.481, word_ins_ml=5.792, word_reposition=1.179, kpe=nan, ppl=nan, wps=6089.3, ups=0.3, wpb=20530, bsz=256, num_updates=8000, lr=0.000395285, gnorm=1.979, clip=0, loss_scale=4704, train_wall=286, wall=26660
2022-08-15 22:43:15 | INFO | train_inner | epoch 008:    254 / 1122 loss=9.404, nll_loss=4.334, mask_ins=1.456, word_ins_ml=5.774, word_reposition=1.161, kpe=1.012, ppl=677.34, wps=6525, ups=0.32, wpb=20490.6, bsz=256, num_updates=8100, lr=0.000392837, gnorm=1.975, clip=0, loss_scale=4096, train_wall=265, wall=26974
2022-08-15 22:48:33 | INFO | train_inner | epoch 008:    354 / 1122 loss=9.36, nll_loss=4.279, mask_ins=1.456, word_ins_ml=5.725, word_reposition=1.162, kpe=1.017, ppl=657.24, wps=6460.8, ups=0.31, wpb=20563.2, bsz=256, num_updates=8200, lr=0.000390434, gnorm=1.984, clip=0, loss_scale=4096, train_wall=267, wall=27292
2022-08-15 22:53:50 | INFO | train_inner | epoch 008:    454 / 1122 loss=9.299, nll_loss=4.259, mask_ins=1.439, word_ins_ml=5.707, word_reposition=1.142, kpe=1.011, ppl=630.04, wps=6470, ups=0.32, wpb=20538.8, bsz=256, num_updates=8300, lr=0.000388075, gnorm=2.024, clip=0, loss_scale=4096, train_wall=267, wall=27610
2022-08-15 22:59:04 | INFO | train_inner | epoch 008:    554 / 1122 loss=9.253, nll_loss=4.199, mask_ins=1.443, word_ins_ml=5.653, word_reposition=1.142, kpe=1.014, ppl=609.95, wps=6554.2, ups=0.32, wpb=20559.9, bsz=256, num_updates=8400, lr=0.000385758, gnorm=1.962, clip=0, loss_scale=4096, train_wall=261, wall=27924
2022-08-15 23:04:22 | INFO | train_inner | epoch 008:    654 / 1122 loss=9.2, nll_loss=4.175, mask_ins=1.42, word_ins_ml=5.632, word_reposition=1.132, kpe=1.016, ppl=588.09, wps=6495.4, ups=0.31, wpb=20632.9, bsz=256, num_updates=8500, lr=0.000383482, gnorm=2.104, clip=0, loss_scale=7127, train_wall=267, wall=28241
2022-08-15 23:09:40 | INFO | train_inner | epoch 008:    754 / 1122 loss=9.06, nll_loss=4.061, mask_ins=1.404, word_ins_ml=5.531, word_reposition=1.114, kpe=1.011, ppl=533.71, wps=6434.2, ups=0.31, wpb=20485.1, bsz=256, num_updates=8600, lr=0.000381246, gnorm=1.989, clip=0, loss_scale=8192, train_wall=267, wall=28560
2022-08-15 23:14:53 | INFO | train_inner | epoch 008:    854 / 1122 loss=nan, nll_loss=4.05, mask_ins=1.39, word_ins_ml=5.52, word_reposition=1.097, kpe=nan, ppl=nan, wps=6561, ups=0.32, wpb=20551.8, bsz=256, num_updates=8700, lr=0.000379049, gnorm=2.067, clip=0, loss_scale=8192, train_wall=264, wall=28873
2022-08-15 23:20:14 | INFO | train_inner | epoch 008:    954 / 1122 loss=9.007, nll_loss=4.006, mask_ins=1.403, word_ins_ml=5.481, word_reposition=1.108, kpe=1.015, ppl=514.51, wps=6416, ups=0.31, wpb=20579.1, bsz=256, num_updates=8800, lr=0.000376889, gnorm=2.049, clip=0, loss_scale=8192, train_wall=269, wall=29194
2022-08-15 23:25:27 | INFO | train_inner | epoch 008:   1054 / 1122 loss=8.953, nll_loss=3.982, mask_ins=1.38, word_ins_ml=5.459, word_reposition=1.096, kpe=1.017, ppl=495.47, wps=6502.9, ups=0.32, wpb=20351.6, bsz=256, num_updates=8900, lr=0.000374766, gnorm=2.051, clip=0, loss_scale=8192, train_wall=265, wall=29507
2022-08-15 23:29:02 | INFO | train | epoch 008 | loss nan | nll_loss 4.171 | mask_ins 1.428 | word_ins_ml 5.629 | word_reposition 1.133 | kpe nan | ppl nan | wps 6220.9 | ups 0.3 | wpb 20520.5 | bsz 255.8 | num_updates 8968 | lr 0.000373342 | gnorm 2.024 | clip 0 | loss_scale 6542 | train_wall 3001 | wall 29721
2022-08-15 23:30:38 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 12.374 | nll_loss 6.47 | mask_ins 1.641 | word_ins_ml 7.791 | word_reposition 1.455 | kpe 1.487 | ppl 5308.44 | wps 10250.3 | wpb 2367.6 | bsz 32 | num_updates 8968 | best_loss 12.374
2022-08-15 23:31:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_best.pt (epoch 8 @ 8968 updates, score 12.374) (writing took 23.689970484003425 seconds)
2022-08-15 23:32:43 | INFO | train_inner | epoch 009:     32 / 1122 loss=8.906, nll_loss=3.964, mask_ins=1.377, word_ins_ml=5.443, word_reposition=1.082, kpe=1.002, ppl=479.54, wps=4659.8, ups=0.23, wpb=20299.1, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=2.08, clip=0, loss_scale=13271, train_wall=265, wall=29942
2022-08-15 23:36:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-15 23:38:02 | INFO | train_inner | epoch 009:    133 / 1122 loss=8.786, nll_loss=3.882, mask_ins=1.356, word_ins_ml=5.371, word_reposition=1.075, kpe=0.984, ppl=441.56, wps=6469.6, ups=0.31, wpb=20649.4, bsz=256, num_updates=9100, lr=0.000370625, gnorm=2.027, clip=0, loss_scale=14113, train_wall=269, wall=30261
2022-08-15 23:39:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-15 23:43:23 | INFO | train_inner | epoch 009:    234 / 1122 loss=8.711, nll_loss=3.829, mask_ins=1.348, word_ins_ml=5.323, word_reposition=1.054, kpe=0.985, ppl=419, wps=6384.9, ups=0.31, wpb=20479.9, bsz=256, num_updates=9200, lr=0.000368605, gnorm=2.048, clip=0, loss_scale=4867, train_wall=270, wall=30582
2022-08-15 23:48:41 | INFO | train_inner | epoch 009:    334 / 1122 loss=8.649, nll_loss=3.772, mask_ins=1.333, word_ins_ml=5.273, word_reposition=1.056, kpe=0.987, ppl=401.5, wps=6433.8, ups=0.31, wpb=20459.5, bsz=256, num_updates=9300, lr=0.000366618, gnorm=2.03, clip=0, loss_scale=4096, train_wall=268, wall=30900
2022-08-15 23:54:00 | INFO | train_inner | epoch 009:    434 / 1122 loss=nan, nll_loss=3.757, mask_ins=1.314, word_ins_ml=5.259, word_reposition=1.049, kpe=nan, ppl=nan, wps=6403.3, ups=0.31, wpb=20467.1, bsz=256, num_updates=9400, lr=0.000364662, gnorm=2.028, clip=0, loss_scale=4096, train_wall=268, wall=31220
2022-08-15 23:59:20 | INFO | train_inner | epoch 009:    534 / 1122 loss=8.588, nll_loss=3.73, mask_ins=1.323, word_ins_ml=5.235, word_reposition=1.045, kpe=0.985, ppl=384.73, wps=6413.9, ups=0.31, wpb=20531, bsz=256, num_updates=9500, lr=0.000362738, gnorm=2.049, clip=0, loss_scale=4096, train_wall=269, wall=31540
2022-08-16 00:04:56 | INFO | train_inner | epoch 009:    634 / 1122 loss=8.565, nll_loss=3.721, mask_ins=1.313, word_ins_ml=5.226, word_reposition=1.038, kpe=0.987, ppl=378.63, wps=6122.7, ups=0.3, wpb=20558.5, bsz=256, num_updates=9600, lr=0.000360844, gnorm=2.057, clip=0, loss_scale=4096, train_wall=285, wall=31876
2022-08-16 00:10:12 | INFO | train_inner | epoch 009:    734 / 1122 loss=8.492, nll_loss=3.664, mask_ins=1.306, word_ins_ml=5.176, word_reposition=1.023, kpe=0.987, ppl=359.92, wps=6507.4, ups=0.32, wpb=20567.8, bsz=256, num_updates=9700, lr=0.000358979, gnorm=2.028, clip=0, loss_scale=6963, train_wall=266, wall=32192
2022-08-16 00:15:31 | INFO | train_inner | epoch 009:    834 / 1122 loss=8.471, nll_loss=3.651, mask_ins=1.292, word_ins_ml=5.163, word_reposition=1.025, kpe=0.991, ppl=354.78, wps=6438.8, ups=0.31, wpb=20505.7, bsz=256, num_updates=9800, lr=0.000357143, gnorm=2.068, clip=0, loss_scale=8192, train_wall=267, wall=32510
2022-08-16 00:20:52 | INFO | train_inner | epoch 009:    934 / 1122 loss=8.397, nll_loss=3.579, mask_ins=1.292, word_ins_ml=5.099, word_reposition=1.018, kpe=0.988, ppl=337.04, wps=6395.8, ups=0.31, wpb=20568.6, bsz=256, num_updates=9900, lr=0.000355335, gnorm=2.053, clip=0, loss_scale=8192, train_wall=269, wall=32832
2022-08-16 00:26:11 | INFO | train_inner | epoch 009:   1034 / 1122 loss=nan, nll_loss=3.614, mask_ins=1.277, word_ins_ml=5.129, word_reposition=1.014, kpe=nan, ppl=nan, wps=6442.5, ups=0.31, wpb=20519.8, bsz=256, num_updates=10000, lr=0.000353553, gnorm=2.048, clip=0, loss_scale=8192, train_wall=268, wall=33150
2022-08-16 00:30:48 | INFO | train | epoch 009 | loss nan | nll_loss 3.714 | mask_ins 1.314 | word_ins_ml 5.22 | word_reposition 1.039 | kpe nan | ppl nan | wps 6200.5 | ups 0.3 | wpb 20520.4 | bsz 255.8 | num_updates 10088 | lr 0.000352008 | gnorm 2.05 | clip 0 | loss_scale 7090 | train_wall 3016 | wall 33428
2022-08-16 00:32:25 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 12.28 | nll_loss 6.412 | mask_ins 1.65 | word_ins_ml 7.768 | word_reposition 1.357 | kpe 1.505 | ppl 4973.39 | wps 10256.4 | wpb 2367.6 | bsz 32 | num_updates 10088 | best_loss 12.28
2022-08-16 00:32:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_best.pt (epoch 9 @ 10088 updates, score 12.28) (writing took 30.619471542537212 seconds)
2022-08-16 00:33:34 | INFO | train_inner | epoch 010:     12 / 1122 loss=8.352, nll_loss=3.552, mask_ins=1.279, word_ins_ml=5.074, word_reposition=1.017, kpe=0.981, ppl=326.74, wps=4618.4, ups=0.23, wpb=20465.6, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=2.12, clip=0, loss_scale=8192, train_wall=265, wall=33593
2022-08-16 00:38:58 | INFO | train_inner | epoch 010:    112 / 1122 loss=8.241, nll_loss=3.505, mask_ins=1.249, word_ins_ml=5.033, word_reposition=1.002, kpe=0.957, ppl=302.52, wps=6343, ups=0.31, wpb=20574.6, bsz=256, num_updates=10200, lr=0.00035007, gnorm=2.079, clip=0, loss_scale=12943, train_wall=272, wall=33918
2022-08-16 00:42:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-16 00:44:18 | INFO | train_inner | epoch 010:    213 / 1122 loss=8.194, nll_loss=3.479, mask_ins=1.239, word_ins_ml=5.01, word_reposition=0.99, kpe=0.955, ppl=292.93, wps=6418.3, ups=0.31, wpb=20540, bsz=256, num_updates=10300, lr=0.000348367, gnorm=2.133, clip=0, loss_scale=13059, train_wall=269, wall=34238
2022-08-16 00:49:36 | INFO | train_inner | epoch 010:    313 / 1122 loss=8.173, nll_loss=3.474, mask_ins=1.23, word_ins_ml=5.005, word_reposition=0.98, kpe=0.959, ppl=288.66, wps=6487.8, ups=0.31, wpb=20613.4, bsz=256, num_updates=10400, lr=0.000346688, gnorm=2.068, clip=0, loss_scale=8192, train_wall=267, wall=34556
2022-08-16 00:54:51 | INFO | train_inner | epoch 010:    413 / 1122 loss=8.149, nll_loss=3.441, mask_ins=1.229, word_ins_ml=4.975, word_reposition=0.983, kpe=0.962, ppl=283.86, wps=6518.4, ups=0.32, wpb=20548.3, bsz=256, num_updates=10500, lr=0.000345033, gnorm=2.05, clip=0, loss_scale=8192, train_wall=265, wall=34871
2022-08-16 01:00:06 | INFO | train_inner | epoch 010:    513 / 1122 loss=8.094, nll_loss=3.405, mask_ins=1.223, word_ins_ml=4.943, word_reposition=0.973, kpe=0.956, ppl=273.24, wps=6511.3, ups=0.32, wpb=20515.9, bsz=256, num_updates=10600, lr=0.000343401, gnorm=2.08, clip=0, loss_scale=8192, train_wall=266, wall=35186
2022-08-16 01:05:25 | INFO | train_inner | epoch 010:    613 / 1122 loss=nan, nll_loss=3.357, mask_ins=1.211, word_ins_ml=4.9, word_reposition=0.975, kpe=nan, ppl=nan, wps=6502.3, ups=0.31, wpb=20696.7, bsz=256, num_updates=10700, lr=0.000341793, gnorm=2.061, clip=0, loss_scale=8192, train_wall=268, wall=35504
2022-08-16 01:10:40 | INFO | train_inner | epoch 010:    713 / 1122 loss=8.045, nll_loss=3.364, mask_ins=1.205, word_ins_ml=4.905, word_reposition=0.971, kpe=0.964, ppl=264.02, wps=6469.1, ups=0.32, wpb=20405.7, bsz=256, num_updates=10800, lr=0.000340207, gnorm=2.109, clip=0, loss_scale=10568, train_wall=266, wall=35820
2022-08-16 01:14:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-16 01:16:04 | INFO | train_inner | epoch 010:    814 / 1122 loss=nan, nll_loss=3.327, mask_ins=1.188, word_ins_ml=4.872, word_reposition=0.976, kpe=nan, ppl=nan, wps=6342.7, ups=0.31, wpb=20533, bsz=256, num_updates=10900, lr=0.000338643, gnorm=2.073, clip=0, loss_scale=14600, train_wall=271, wall=36143
2022-08-16 01:21:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-16 01:21:25 | INFO | train_inner | epoch 010:    915 / 1122 loss=7.976, nll_loss=3.311, mask_ins=1.193, word_ins_ml=4.857, word_reposition=0.96, kpe=0.966, ppl=251.73, wps=6397, ups=0.31, wpb=20538.4, bsz=256, num_updates=11000, lr=0.0003371, gnorm=2.09, clip=0, loss_scale=7868, train_wall=270, wall=36464
2022-08-16 01:26:45 | INFO | train_inner | epoch 010:   1015 / 1122 loss=7.936, nll_loss=3.293, mask_ins=1.182, word_ins_ml=4.841, word_reposition=0.947, kpe=0.967, ppl=244.96, wps=6390.8, ups=0.31, wpb=20464, bsz=256, num_updates=11100, lr=0.000335578, gnorm=2.042, clip=0, loss_scale=4096, train_wall=268, wall=36785
2022-08-16 01:32:19 | INFO | train_inner | epoch 010:   1115 / 1122 loss=7.84, nll_loss=3.213, mask_ins=1.163, word_ins_ml=4.77, word_reposition=0.944, kpe=0.963, ppl=229.2, wps=6126.4, ups=0.3, wpb=20477.9, bsz=256, num_updates=11200, lr=0.000334077, gnorm=2.051, clip=0, loss_scale=4096, train_wall=285, wall=37119
2022-08-16 01:32:40 | INFO | train | epoch 010 | loss nan | nll_loss 3.381 | mask_ins 1.211 | word_ins_ml 4.921 | word_reposition 0.973 | kpe nan | ppl nan | wps 6186.6 | ups 0.3 | wpb 20522.1 | bsz 255.8 | num_updates 11207 | lr 0.000333972 | gnorm 2.082 | clip 0 | loss_scale 9057 | train_wall 3016 | wall 37140
2022-08-16 01:34:17 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 12.162 | nll_loss 6.286 | mask_ins 1.56 | word_ins_ml 7.657 | word_reposition 1.438 | kpe 1.507 | ppl 4581.93 | wps 10266.6 | wpb 2367.6 | bsz 32 | num_updates 11207 | best_loss 12.162
2022-08-16 01:34:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_best.pt (epoch 10 @ 11207 updates, score 12.162) (writing took 21.091981695964932 seconds)
2022-08-16 01:39:34 | INFO | train_inner | epoch 011:     93 / 1122 loss=7.855, nll_loss=3.254, mask_ins=1.17, word_ins_ml=4.806, word_reposition=0.95, kpe=0.929, ppl=231.58, wps=4689.9, ups=0.23, wpb=20402, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=2.178, clip=0, loss_scale=4096, train_wall=266, wall=37554
2022-08-16 01:44:50 | INFO | train_inner | epoch 011:    193 / 1122 loss=nan, nll_loss=3.182, mask_ins=1.159, word_ins_ml=4.743, word_reposition=0.94, kpe=nan, ppl=nan, wps=6494.1, ups=0.32, wpb=20469.8, bsz=256, num_updates=11400, lr=0.000331133, gnorm=2.068, clip=0, loss_scale=4096, train_wall=262, wall=37869
2022-08-16 01:50:09 | INFO | train_inner | epoch 011:    293 / 1122 loss=nan, nll_loss=3.156, mask_ins=1.145, word_ins_ml=4.719, word_reposition=0.927, kpe=nan, ppl=nan, wps=6428.6, ups=0.31, wpb=20531, bsz=256, num_updates=11500, lr=0.00032969, gnorm=2.075, clip=0, loss_scale=4096, train_wall=269, wall=38189
2022-08-16 01:53:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-16 01:53:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-16 01:55:38 | INFO | train_inner | epoch 011:    395 / 1122 loss=7.712, nll_loss=3.146, mask_ins=1.142, word_ins_ml=4.709, word_reposition=0.929, kpe=0.931, ppl=209.61, wps=6257.1, ups=0.3, wpb=20559, bsz=256, num_updates=11600, lr=0.000328266, gnorm=2.069, clip=0, loss_scale=5562, train_wall=276, wall=38517
2022-08-16 02:00:56 | INFO | train_inner | epoch 011:    495 / 1122 loss=7.705, nll_loss=3.151, mask_ins=1.125, word_ins_ml=4.713, word_reposition=0.93, kpe=0.936, ppl=208.62, wps=6478.4, ups=0.31, wpb=20633.5, bsz=256, num_updates=11700, lr=0.00032686, gnorm=2.081, clip=0, loss_scale=2048, train_wall=269, wall=38836
2022-08-16 02:06:14 | INFO | train_inner | epoch 011:    595 / 1122 loss=7.664, nll_loss=3.131, mask_ins=1.116, word_ins_ml=4.695, word_reposition=0.92, kpe=0.933, ppl=202.86, wps=6471.9, ups=0.31, wpb=20551.4, bsz=256, num_updates=11800, lr=0.000325472, gnorm=2.044, clip=0, loss_scale=2048, train_wall=266, wall=39153
2022-08-16 02:11:35 | INFO | train_inner | epoch 011:    695 / 1122 loss=7.655, nll_loss=3.102, mask_ins=1.123, word_ins_ml=4.669, word_reposition=0.926, kpe=0.937, ppl=201.52, wps=6402.1, ups=0.31, wpb=20542.2, bsz=256, num_updates=11900, lr=0.000324102, gnorm=2.073, clip=0, loss_scale=2048, train_wall=270, wall=39474
2022-08-16 02:16:51 | INFO | train_inner | epoch 011:    795 / 1122 loss=7.645, nll_loss=3.1, mask_ins=1.12, word_ins_ml=4.667, word_reposition=0.914, kpe=0.944, ppl=200.21, wps=6419.2, ups=0.32, wpb=20333, bsz=256, num_updates=12000, lr=0.000322749, gnorm=2.056, clip=0, loss_scale=2048, train_wall=266, wall=39791
2022-08-16 02:22:10 | INFO | train_inner | epoch 011:    895 / 1122 loss=7.595, nll_loss=3.083, mask_ins=1.098, word_ins_ml=4.652, word_reposition=0.909, kpe=0.936, ppl=193.38, wps=6475.4, ups=0.31, wpb=20614.3, bsz=256, num_updates=12100, lr=0.000321412, gnorm=2.036, clip=0, loss_scale=2601, train_wall=268, wall=40109
2022-08-16 02:27:31 | INFO | train_inner | epoch 011:    995 / 1122 loss=7.634, nll_loss=3.093, mask_ins=1.112, word_ins_ml=4.66, word_reposition=0.921, kpe=0.941, ppl=198.61, wps=6384.1, ups=0.31, wpb=20487.8, bsz=256, num_updates=12200, lr=0.000320092, gnorm=2.079, clip=0, loss_scale=4096, train_wall=269, wall=40430
2022-08-16 02:28:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-16 02:32:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-16 02:32:57 | INFO | train_inner | epoch 011:   1097 / 1122 loss=7.54, nll_loss=3.03, mask_ins=1.088, word_ins_ml=4.604, word_reposition=0.904, kpe=0.944, ppl=186.11, wps=6314.3, ups=0.31, wpb=20583.7, bsz=256, num_updates=12300, lr=0.000318788, gnorm=2.277, clip=0, loss_scale=2480, train_wall=273, wall=40756
2022-08-16 02:34:13 | INFO | train | epoch 011 | loss nan | nll_loss 3.126 | mask_ins 1.126 | word_ins_ml 4.691 | word_reposition 0.924 | kpe nan | ppl nan | wps 6213.1 | ups 0.3 | wpb 20522.3 | bsz 255.8 | num_updates 12325 | lr 0.000318465 | gnorm 2.096 | clip 0 | loss_scale 3150 | train_wall 3001 | wall 40833
2022-08-16 02:35:51 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 11.972 | nll_loss 6.099 | mask_ins 1.59 | word_ins_ml 7.486 | word_reposition 1.429 | kpe 1.467 | ppl 4017.34 | wps 10146.2 | wpb 2367.6 | bsz 32 | num_updates 12325 | best_loss 11.972
2022-08-16 02:35:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_best.pt (epoch 11 @ 12325 updates, score 11.972) (writing took 6.442465744912624 seconds)
2022-08-16 02:39:54 | INFO | train_inner | epoch 012:     75 / 1122 loss=7.534, nll_loss=3.039, mask_ins=1.103, word_ins_ml=4.612, word_reposition=0.91, kpe=0.909, ppl=185.33, wps=4895.6, ups=0.24, wpb=20449.1, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=2.144, clip=0, loss_scale=1024, train_wall=265, wall=41174
2022-08-16 02:45:14 | INFO | train_inner | epoch 012:    175 / 1122 loss=7.409, nll_loss=2.963, mask_ins=1.076, word_ins_ml=4.544, word_reposition=0.892, kpe=0.897, ppl=169.97, wps=6409.1, ups=0.31, wpb=20487.2, bsz=256, num_updates=12500, lr=0.000316228, gnorm=2.056, clip=0, loss_scale=1024, train_wall=269, wall=41493
2022-08-16 02:50:36 | INFO | train_inner | epoch 012:    275 / 1122 loss=7.436, nll_loss=2.985, mask_ins=1.082, word_ins_ml=4.564, word_reposition=0.894, kpe=0.895, ppl=173.11, wps=6366.1, ups=0.31, wpb=20515.7, bsz=256, num_updates=12600, lr=0.00031497, gnorm=2.066, clip=0, loss_scale=1024, train_wall=270, wall=41816
2022-08-16 02:57:00 | INFO | train_inner | epoch 012:    375 / 1122 loss=7.385, nll_loss=2.937, mask_ins=1.069, word_ins_ml=4.521, word_reposition=0.891, kpe=0.904, ppl=167.1, wps=5371.1, ups=0.26, wpb=20616.1, bsz=256, num_updates=12700, lr=0.000313728, gnorm=2.037, clip=0, loss_scale=1024, train_wall=332, wall=42200
2022-08-16 03:02:18 | INFO | train_inner | epoch 012:    475 / 1122 loss=7.405, nll_loss=2.966, mask_ins=1.063, word_ins_ml=4.545, word_reposition=0.89, kpe=0.907, ppl=169.51, wps=6440, ups=0.31, wpb=20481.2, bsz=256, num_updates=12800, lr=0.0003125, gnorm=2.066, clip=0, loss_scale=1024, train_wall=270, wall=42518
2022-08-16 03:07:37 | INFO | train_inner | epoch 012:    575 / 1122 loss=7.358, nll_loss=2.926, mask_ins=1.055, word_ins_ml=4.51, word_reposition=0.886, kpe=0.907, ppl=164.07, wps=6466.3, ups=0.31, wpb=20605.8, bsz=256, num_updates=12900, lr=0.000311286, gnorm=2.071, clip=0, loss_scale=1956, train_wall=267, wall=42836
2022-08-16 03:13:04 | INFO | train_inner | epoch 012:    675 / 1122 loss=7.315, nll_loss=2.876, mask_ins=1.059, word_ins_ml=4.465, word_reposition=0.885, kpe=0.906, ppl=159.18, wps=6241.7, ups=0.31, wpb=20401.3, bsz=256, num_updates=13000, lr=0.000310087, gnorm=2.053, clip=0, loss_scale=2048, train_wall=272, wall=43163
2022-08-16 03:18:26 | INFO | train_inner | epoch 012:    775 / 1122 loss=7.343, nll_loss=2.909, mask_ins=1.052, word_ins_ml=4.494, word_reposition=0.886, kpe=0.911, ppl=162.32, wps=6387.6, ups=0.31, wpb=20576.5, bsz=256, num_updates=13100, lr=0.000308901, gnorm=2.123, clip=0, loss_scale=2048, train_wall=266, wall=43485
2022-08-16 03:23:43 | INFO | train_inner | epoch 012:    875 / 1122 loss=nan, nll_loss=2.906, mask_ins=1.054, word_ins_ml=4.491, word_reposition=0.879, kpe=nan, ppl=nan, wps=6480, ups=0.31, wpb=20573.1, bsz=256, num_updates=13200, lr=0.000307729, gnorm=2.086, clip=0, loss_scale=2048, train_wall=268, wall=43803
2022-08-16 03:29:00 | INFO | train_inner | epoch 012:    975 / 1122 loss=7.269, nll_loss=2.861, mask_ins=1.034, word_ins_ml=4.45, word_reposition=0.87, kpe=0.915, ppl=154.21, wps=6466.5, ups=0.32, wpb=20516.7, bsz=256, num_updates=13300, lr=0.00030657, gnorm=2.06, clip=0, loss_scale=2048, train_wall=267, wall=44120
2022-08-16 03:34:20 | INFO | train_inner | epoch 012:   1075 / 1122 loss=nan, nll_loss=2.858, mask_ins=1.043, word_ins_ml=4.447, word_reposition=0.879, kpe=nan, ppl=nan, wps=6409.1, ups=0.31, wpb=20485, bsz=256, num_updates=13400, lr=0.000305424, gnorm=2.068, clip=0, loss_scale=3666, train_wall=268, wall=44440
2022-08-16 03:36:47 | INFO | train | epoch 012 | loss nan | nll_loss 2.925 | mask_ins 1.061 | word_ins_ml 4.509 | word_reposition 0.886 | kpe nan | ppl nan | wps 6133.3 | ups 0.3 | wpb 20520.5 | bsz 255.8 | num_updates 13447 | lr 0.000304889 | gnorm 2.075 | clip 0 | loss_scale 1836 | train_wall 3070 | wall 44587
2022-08-16 03:38:24 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 12.14 | nll_loss 6.112 | mask_ins 1.638 | word_ins_ml 7.502 | word_reposition 1.419 | kpe 1.581 | ppl 4514.8 | wps 10228.4 | wpb 2367.6 | bsz 32 | num_updates 13447 | best_loss 11.972
2022-08-16 03:38:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_last.pt (epoch 12 @ 13447 updates, score 12.14) (writing took 21.049383571371436 seconds)
2022-08-16 03:41:35 | INFO | train_inner | epoch 013:     53 / 1122 loss=7.216, nll_loss=2.816, mask_ins=1.045, word_ins_ml=4.41, word_reposition=0.871, kpe=0.889, ppl=148.63, wps=4694.6, ups=0.23, wpb=20393.7, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=2.14, clip=0, loss_scale=4096, train_wall=266, wall=44874
2022-08-16 03:46:54 | INFO | train_inner | epoch 013:    153 / 1122 loss=7.198, nll_loss=2.842, mask_ins=1.028, word_ins_ml=4.433, word_reposition=0.871, kpe=0.865, ppl=146.83, wps=6454.2, ups=0.31, wpb=20589.9, bsz=256, num_updates=13600, lr=0.00030317, gnorm=2.07, clip=0, loss_scale=4096, train_wall=267, wall=45193
2022-08-16 03:52:13 | INFO | train_inner | epoch 013:    253 / 1122 loss=7.135, nll_loss=2.794, mask_ins=1.016, word_ins_ml=4.39, word_reposition=0.861, kpe=0.867, ppl=140.56, wps=6405, ups=0.31, wpb=20450.1, bsz=256, num_updates=13700, lr=0.000302061, gnorm=2.075, clip=0, loss_scale=4096, train_wall=268, wall=45512
2022-08-16 03:57:33 | INFO | train_inner | epoch 013:    353 / 1122 loss=nan, nll_loss=2.827, mask_ins=1.023, word_ins_ml=4.419, word_reposition=0.87, kpe=nan, ppl=nan, wps=6423.3, ups=0.31, wpb=20550.4, bsz=256, num_updates=13800, lr=0.000300965, gnorm=2.041, clip=0, loss_scale=4096, train_wall=269, wall=45832
2022-08-16 04:02:54 | INFO | train_inner | epoch 013:    453 / 1122 loss=7.153, nll_loss=2.802, mask_ins=1.018, word_ins_ml=4.396, word_reposition=0.863, kpe=0.876, ppl=142.32, wps=6414.7, ups=0.31, wpb=20578.3, bsz=256, num_updates=13900, lr=0.00029988, gnorm=2.08, clip=0, loss_scale=6840, train_wall=271, wall=46153
2022-08-16 04:08:16 | INFO | train_inner | epoch 013:    553 / 1122 loss=7.126, nll_loss=2.779, mask_ins=1.013, word_ins_ml=4.376, word_reposition=0.861, kpe=0.876, ppl=139.7, wps=6376.2, ups=0.31, wpb=20537.5, bsz=256, num_updates=14000, lr=0.000298807, gnorm=2.06, clip=0, loss_scale=8192, train_wall=270, wall=46475
2022-08-16 04:13:33 | INFO | train_inner | epoch 013:    653 / 1122 loss=7.1, nll_loss=2.753, mask_ins=1.014, word_ins_ml=4.352, word_reposition=0.854, kpe=0.88, ppl=137.17, wps=6438.4, ups=0.31, wpb=20453.5, bsz=256, num_updates=14100, lr=0.000297746, gnorm=2.049, clip=0, loss_scale=8192, train_wall=268, wall=46793
2022-08-16 04:18:51 | INFO | train_inner | epoch 013:    753 / 1122 loss=nan, nll_loss=2.785, mask_ins=1.003, word_ins_ml=4.38, word_reposition=0.853, kpe=nan, ppl=nan, wps=6468, ups=0.31, wpb=20537.9, bsz=256, num_updates=14200, lr=0.000296695, gnorm=2.074, clip=0, loss_scale=8192, train_wall=267, wall=47110
2022-08-16 04:24:14 | INFO | train_inner | epoch 013:    853 / 1122 loss=7.104, nll_loss=2.755, mask_ins=1.01, word_ins_ml=4.354, word_reposition=0.858, kpe=0.882, ppl=137.55, wps=6345.6, ups=0.31, wpb=20518.9, bsz=256, num_updates=14300, lr=0.000295656, gnorm=2.058, clip=0, loss_scale=8192, train_wall=274, wall=47434
2022-08-16 04:29:39 | INFO | train_inner | epoch 013:    953 / 1122 loss=7.057, nll_loss=2.73, mask_ins=0.995, word_ins_ml=4.331, word_reposition=0.845, kpe=0.886, ppl=133.19, wps=6309.4, ups=0.31, wpb=20467, bsz=256, num_updates=14400, lr=0.000294628, gnorm=2.06, clip=0, loss_scale=12698, train_wall=275, wall=47758
2022-08-16 04:34:54 | INFO | train_inner | epoch 013:   1053 / 1122 loss=7.057, nll_loss=2.742, mask_ins=0.99, word_ins_ml=4.341, word_reposition=0.842, kpe=0.883, ppl=133.13, wps=6561.2, ups=0.32, wpb=20658.7, bsz=256, num_updates=14500, lr=0.00029361, gnorm=2.06, clip=0, loss_scale=16384, train_wall=266, wall=48073
2022-08-16 04:38:30 | INFO | train | epoch 013 | loss nan | nll_loss 2.778 | mask_ins 1.012 | word_ins_ml 4.375 | word_reposition 0.858 | kpe nan | ppl nan | wps 6217.4 | ups 0.3 | wpb 20520.1 | bsz 255.8 | num_updates 14569 | lr 0.000292914 | gnorm 2.068 | clip 0 | loss_scale 8418 | train_wall 3020 | wall 48290
2022-08-16 04:40:09 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 12.112 | nll_loss 6.115 | mask_ins 1.58 | word_ins_ml 7.516 | word_reposition 1.466 | kpe 1.55 | ppl 4426.47 | wps 10074.1 | wpb 2367.6 | bsz 32 | num_updates 14569 | best_loss 11.972
2022-08-16 04:40:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_last.pt (epoch 13 @ 14569 updates, score 12.112) (writing took 9.610752381384373 seconds)
2022-08-16 04:41:59 | INFO | train_inner | epoch 014:     31 / 1122 loss=7.051, nll_loss=2.728, mask_ins=0.993, word_ins_ml=4.33, word_reposition=0.852, kpe=0.877, ppl=132.62, wps=4789.8, ups=0.24, wpb=20372.8, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=2.151, clip=0, loss_scale=16384, train_wall=268, wall=48498
2022-08-16 04:45:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-16 04:46:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-16 04:47:29 | INFO | train_inner | epoch 014:    133 / 1122 loss=7.022, nll_loss=2.737, mask_ins=1.001, word_ins_ml=4.337, word_reposition=0.853, kpe=0.832, ppl=129.94, wps=6210.7, ups=0.3, wpb=20525.5, bsz=256, num_updates=14700, lr=0.000291606, gnorm=2.108, clip=0, loss_scale=12810, train_wall=276, wall=48829
2022-08-16 04:52:46 | INFO | train_inner | epoch 014:    233 / 1122 loss=6.946, nll_loss=2.685, mask_ins=0.981, word_ins_ml=4.291, word_reposition=0.835, kpe=0.839, ppl=123.28, wps=6486.1, ups=0.32, wpb=20546.5, bsz=256, num_updates=14800, lr=0.000290619, gnorm=2.114, clip=0, loss_scale=4096, train_wall=266, wall=49146
2022-08-16 04:58:07 | INFO | train_inner | epoch 014:    333 / 1122 loss=nan, nll_loss=2.674, mask_ins=0.981, word_ins_ml=4.281, word_reposition=0.845, kpe=nan, ppl=nan, wps=6411.5, ups=0.31, wpb=20556.6, bsz=256, num_updates=14900, lr=0.000289642, gnorm=2.141, clip=0, loss_scale=4096, train_wall=269, wall=49466
2022-08-16 05:03:24 | INFO | train_inner | epoch 014:    433 / 1122 loss=6.915, nll_loss=2.657, mask_ins=0.972, word_ins_ml=4.265, word_reposition=0.836, kpe=0.842, ppl=120.69, wps=6474.9, ups=0.32, wpb=20535.9, bsz=256, num_updates=15000, lr=0.000288675, gnorm=2.045, clip=0, loss_scale=4096, train_wall=267, wall=49783
2022-08-16 05:08:42 | INFO | train_inner | epoch 014:    533 / 1122 loss=6.859, nll_loss=2.609, mask_ins=0.964, word_ins_ml=4.222, word_reposition=0.829, kpe=0.843, ppl=116.06, wps=6450.7, ups=0.31, wpb=20488.9, bsz=256, num_updates=15100, lr=0.000287718, gnorm=2.101, clip=0, loss_scale=4096, train_wall=267, wall=50101
2022-08-16 05:13:56 | INFO | train_inner | epoch 014:    633 / 1122 loss=6.882, nll_loss=2.617, mask_ins=0.968, word_ins_ml=4.229, word_reposition=0.835, kpe=0.85, ppl=117.97, wps=6508.4, ups=0.32, wpb=20496.4, bsz=256, num_updates=15200, lr=0.00028677, gnorm=2.066, clip=0, loss_scale=4219, train_wall=267, wall=50416
2022-08-16 05:19:14 | INFO | train_inner | epoch 014:    733 / 1122 loss=6.888, nll_loss=2.629, mask_ins=0.962, word_ins_ml=4.24, word_reposition=0.837, kpe=0.849, ppl=118.47, wps=6497.7, ups=0.31, wpb=20652.7, bsz=256, num_updates=15300, lr=0.000285831, gnorm=2.064, clip=0, loss_scale=8192, train_wall=267, wall=50734
2022-08-16 05:24:30 | INFO | train_inner | epoch 014:    833 / 1122 loss=6.875, nll_loss=2.626, mask_ins=0.953, word_ins_ml=4.237, word_reposition=0.827, kpe=0.859, ppl=117.41, wps=6464.3, ups=0.32, wpb=20413.6, bsz=256, num_updates=15400, lr=0.000284901, gnorm=2.04, clip=0, loss_scale=8192, train_wall=266, wall=51050
2022-08-16 05:29:47 | INFO | train_inner | epoch 014:    933 / 1122 loss=6.901, nll_loss=2.636, mask_ins=0.963, word_ins_ml=4.245, word_reposition=0.832, kpe=0.861, ppl=119.52, wps=6482.6, ups=0.32, wpb=20508.8, bsz=256, num_updates=15500, lr=0.000283981, gnorm=2.056, clip=0, loss_scale=8192, train_wall=267, wall=51366
2022-08-16 05:35:04 | INFO | train_inner | epoch 014:   1033 / 1122 loss=6.879, nll_loss=2.619, mask_ins=0.961, word_ins_ml=4.23, word_reposition=0.832, kpe=0.856, ppl=117.72, wps=6467, ups=0.31, wpb=20538.5, bsz=256, num_updates=15600, lr=0.000283069, gnorm=2.057, clip=0, loss_scale=8192, train_wall=267, wall=51684
2022-08-16 05:39:47 | INFO | train | epoch 014 | loss nan | nll_loss 2.648 | mask_ins 0.97 | word_ins_ml 4.257 | word_reposition 0.836 | kpe nan | ppl nan | wps 6251.7 | ups 0.3 | wpb 20521.4 | bsz 255.8 | num_updates 15689 | lr 0.000282265 | gnorm 2.09 | clip 0 | loss_scale 7024 | train_wall 3001 | wall 51966
2022-08-16 05:41:24 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.883 | nll_loss 5.879 | mask_ins 1.569 | word_ins_ml 7.279 | word_reposition 1.406 | kpe 1.629 | ppl 3777.93 | wps 10187.1 | wpb 2367.6 | bsz 32 | num_updates 15689 | best_loss 11.883
2022-08-16 05:41:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_best.pt (epoch 14 @ 15689 updates, score 11.883) (writing took 15.898148033767939 seconds)
2022-08-16 05:42:14 | INFO | train_inner | epoch 015:     11 / 1122 loss=nan, nll_loss=2.62, mask_ins=0.956, word_ins_ml=4.231, word_reposition=0.832, kpe=nan, ppl=nan, wps=4755.1, ups=0.23, wpb=20457.8, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=2.188, clip=0, loss_scale=8192, train_wall=266, wall=52114
2022-08-16 05:42:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-16 05:47:31 | INFO | train_inner | epoch 015:    112 / 1122 loss=6.776, nll_loss=2.576, mask_ins=0.953, word_ins_ml=4.192, word_reposition=0.826, kpe=0.805, ppl=109.6, wps=6521, ups=0.32, wpb=20658.1, bsz=256, num_updates=15800, lr=0.000281272, gnorm=2.065, clip=0, loss_scale=8354, train_wall=264, wall=52431
2022-08-16 05:52:56 | INFO | train_inner | epoch 015:    212 / 1122 loss=6.756, nll_loss=2.566, mask_ins=0.944, word_ins_ml=4.183, word_reposition=0.827, kpe=0.803, ppl=108.08, wps=6327.5, ups=0.31, wpb=20581.2, bsz=256, num_updates=15900, lr=0.000280386, gnorm=2.131, clip=0, loss_scale=8192, train_wall=275, wall=52756
2022-08-16 05:58:21 | INFO | train_inner | epoch 015:    312 / 1122 loss=nan, nll_loss=2.537, mask_ins=0.948, word_ins_ml=4.157, word_reposition=0.815, kpe=nan, ppl=nan, wps=6319.8, ups=0.31, wpb=20538.6, bsz=256, num_updates=16000, lr=0.000279508, gnorm=2.11, clip=0, loss_scale=8192, train_wall=275, wall=53081
2022-08-16 06:03:40 | INFO | train_inner | epoch 015:    412 / 1122 loss=6.774, nll_loss=2.564, mask_ins=0.952, word_ins_ml=4.18, word_reposition=0.828, kpe=0.814, ppl=109.45, wps=6434.7, ups=0.31, wpb=20532.3, bsz=256, num_updates=16100, lr=0.000278639, gnorm=2.104, clip=0, loss_scale=8192, train_wall=268, wall=53400
2022-08-16 06:09:03 | INFO | train_inner | epoch 015:    512 / 1122 loss=6.733, nll_loss=2.534, mask_ins=0.94, word_ins_ml=4.154, word_reposition=0.819, kpe=0.821, ppl=106.39, wps=6340.7, ups=0.31, wpb=20478.8, bsz=256, num_updates=16200, lr=0.000277778, gnorm=2.175, clip=0, loss_scale=8192, train_wall=270, wall=53723
2022-08-16 06:14:24 | INFO | train_inner | epoch 015:    612 / 1122 loss=nan, nll_loss=2.563, mask_ins=0.931, word_ins_ml=4.179, word_reposition=0.822, kpe=nan, ppl=nan, wps=6402, ups=0.31, wpb=20545.4, bsz=256, num_updates=16300, lr=0.000276924, gnorm=2.066, clip=0, loss_scale=14582, train_wall=270, wall=54044
2022-08-16 06:19:43 | INFO | train_inner | epoch 015:    712 / 1122 loss=6.773, nll_loss=2.577, mask_ins=0.94, word_ins_ml=4.192, word_reposition=0.818, kpe=0.823, ppl=109.38, wps=6430, ups=0.31, wpb=20458.1, bsz=256, num_updates=16400, lr=0.000276079, gnorm=2.093, clip=0, loss_scale=16384, train_wall=266, wall=54362
2022-08-16 06:25:02 | INFO | train_inner | epoch 015:    812 / 1122 loss=6.743, nll_loss=2.549, mask_ins=0.937, word_ins_ml=4.166, word_reposition=0.808, kpe=0.832, ppl=107.09, wps=6437.6, ups=0.31, wpb=20536.7, bsz=256, num_updates=16500, lr=0.000275241, gnorm=2.097, clip=0, loss_scale=16384, train_wall=268, wall=54681
2022-08-16 06:27:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-16 06:30:22 | INFO | train_inner | epoch 015:    913 / 1122 loss=6.734, nll_loss=2.544, mask_ins=0.932, word_ins_ml=4.161, word_reposition=0.816, kpe=0.824, ppl=106.43, wps=6387.3, ups=0.31, wpb=20496.2, bsz=256, num_updates=16600, lr=0.000274411, gnorm=2.081, clip=0, loss_scale=12329, train_wall=270, wall=55002
2022-08-16 06:35:43 | INFO | train_inner | epoch 015:   1013 / 1122 loss=6.735, nll_loss=2.533, mask_ins=0.935, word_ins_ml=4.152, word_reposition=0.817, kpe=0.831, ppl=106.49, wps=6397.1, ups=0.31, wpb=20505, bsz=256, num_updates=16700, lr=0.000273588, gnorm=2.05, clip=0, loss_scale=8192, train_wall=269, wall=55323
2022-08-16 06:41:03 | INFO | train_inner | epoch 015:   1113 / 1122 loss=6.674, nll_loss=2.481, mask_ins=0.926, word_ins_ml=4.105, word_reposition=0.809, kpe=0.834, ppl=102.1, wps=6445.1, ups=0.31, wpb=20621.4, bsz=256, num_updates=16800, lr=0.000272772, gnorm=2.044, clip=0, loss_scale=8192, train_wall=268, wall=55643
2022-08-16 06:41:30 | INFO | train | epoch 015 | loss nan | nll_loss 2.549 | mask_ins 0.94 | word_ins_ml 4.167 | word_reposition 0.818 | kpe nan | ppl nan | wps 6206.8 | ups 0.3 | wpb 20520.8 | bsz 255.8 | num_updates 16809 | lr 0.000272699 | gnorm 2.1 | clip 0 | loss_scale 10609 | train_wall 3014 | wall 55669
2022-08-16 06:43:07 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 11.919 | nll_loss 5.921 | mask_ins 1.572 | word_ins_ml 7.331 | word_reposition 1.389 | kpe 1.627 | ppl 3872.78 | wps 10160.2 | wpb 2367.6 | bsz 32 | num_updates 16809 | best_loss 11.883
2022-08-16 06:43:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_last.pt (epoch 15 @ 16809 updates, score 11.919) (writing took 14.708423342555761 seconds)
2022-08-16 06:48:15 | INFO | train_inner | epoch 016:     91 / 1122 loss=nan, nll_loss=2.525, mask_ins=0.923, word_ins_ml=4.145, word_reposition=0.811, kpe=nan, ppl=nan, wps=4700.6, ups=0.23, wpb=20285.1, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=2.194, clip=0, loss_scale=8192, train_wall=268, wall=56074
2022-08-16 06:53:33 | INFO | train_inner | epoch 016:    191 / 1122 loss=6.624, nll_loss=2.508, mask_ins=0.916, word_ins_ml=4.129, word_reposition=0.804, kpe=0.775, ppl=98.61, wps=6432.3, ups=0.31, wpb=20468.6, bsz=256, num_updates=17000, lr=0.000271163, gnorm=2.09, clip=0, loss_scale=8192, train_wall=267, wall=56392
2022-08-16 06:58:53 | INFO | train_inner | epoch 016:    291 / 1122 loss=6.621, nll_loss=2.493, mask_ins=0.925, word_ins_ml=4.115, word_reposition=0.804, kpe=0.777, ppl=98.43, wps=6449.2, ups=0.31, wpb=20657, bsz=256, num_updates=17100, lr=0.000270369, gnorm=2.102, clip=0, loss_scale=11305, train_wall=268, wall=56713
2022-08-16 07:03:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-16 07:04:11 | INFO | train_inner | epoch 016:    392 / 1122 loss=6.658, nll_loss=2.524, mask_ins=0.92, word_ins_ml=4.143, word_reposition=0.809, kpe=0.786, ppl=100.97, wps=6424.4, ups=0.31, wpb=20440.4, bsz=256, num_updates=17200, lr=0.000269582, gnorm=2.145, clip=0, loss_scale=15167, train_wall=269, wall=57031
2022-08-16 07:09:19 | INFO | train_inner | epoch 016:    492 / 1122 loss=6.596, nll_loss=2.463, mask_ins=0.911, word_ins_ml=4.089, word_reposition=0.807, kpe=0.789, ppl=96.71, wps=6715.3, ups=0.33, wpb=20636.2, bsz=256, num_updates=17300, lr=0.000268802, gnorm=2.053, clip=0, loss_scale=8192, train_wall=258, wall=57338
2022-08-16 07:14:47 | INFO | train_inner | epoch 016:    592 / 1122 loss=6.597, nll_loss=2.461, mask_ins=0.914, word_ins_ml=4.086, word_reposition=0.804, kpe=0.792, ppl=96.78, wps=6247.9, ups=0.3, wpb=20534.8, bsz=256, num_updates=17400, lr=0.000268028, gnorm=2.06, clip=0, loss_scale=8192, train_wall=278, wall=57667
2022-08-16 07:20:28 | INFO | train_inner | epoch 016:    692 / 1122 loss=6.62, nll_loss=2.495, mask_ins=0.906, word_ins_ml=4.116, word_reposition=0.801, kpe=0.796, ppl=98.33, wps=6044.7, ups=0.29, wpb=20586.7, bsz=256, num_updates=17500, lr=0.000267261, gnorm=2.078, clip=0, loss_scale=8192, train_wall=289, wall=58007
2022-08-16 07:26:05 | INFO | train_inner | epoch 016:    792 / 1122 loss=6.611, nll_loss=2.473, mask_ins=0.913, word_ins_ml=4.097, word_reposition=0.8, kpe=0.801, ppl=97.78, wps=6093, ups=0.3, wpb=20523.2, bsz=256, num_updates=17600, lr=0.000266501, gnorm=2.162, clip=0, loss_scale=8192, train_wall=287, wall=58344
2022-08-16 07:31:24 | INFO | train_inner | epoch 016:    892 / 1122 loss=6.609, nll_loss=2.486, mask_ins=0.909, word_ins_ml=4.108, word_reposition=0.794, kpe=0.798, ppl=97.61, wps=6429, ups=0.31, wpb=20546.7, bsz=256, num_updates=17700, lr=0.000265747, gnorm=2.097, clip=0, loss_scale=8438, train_wall=269, wall=58664
2022-08-16 07:36:40 | INFO | train_inner | epoch 016:    992 / 1122 loss=6.6, nll_loss=2.463, mask_ins=0.909, word_ins_ml=4.088, word_reposition=0.797, kpe=0.807, ppl=97.03, wps=6494.5, ups=0.32, wpb=20535.5, bsz=256, num_updates=17800, lr=0.000264999, gnorm=2.091, clip=0, loss_scale=16384, train_wall=266, wall=58980
2022-08-16 07:41:55 | INFO | train_inner | epoch 016:   1092 / 1122 loss=nan, nll_loss=2.426, mask_ins=0.908, word_ins_ml=4.054, word_reposition=0.789, kpe=nan, ppl=nan, wps=6506.1, ups=0.32, wpb=20492.2, bsz=256, num_updates=17900, lr=0.000264258, gnorm=2.078, clip=0, loss_scale=16384, train_wall=265, wall=59295
2022-08-16 07:43:27 | INFO | train | epoch 016 | loss nan | nll_loss 2.483 | mask_ins 0.914 | word_ins_ml 4.106 | word_reposition 0.802 | kpe nan | ppl nan | wps 6187.5 | ups 0.3 | wpb 20520.1 | bsz 255.8 | num_updates 17930 | lr 0.000264037 | gnorm 2.104 | clip 0 | loss_scale 10799 | train_wall 3040 | wall 59387
2022-08-16 07:45:03 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 12.043 | nll_loss 5.921 | mask_ins 1.624 | word_ins_ml 7.336 | word_reposition 1.435 | kpe 1.648 | ppl 4218.51 | wps 10323.2 | wpb 2367.6 | bsz 32 | num_updates 17930 | best_loss 11.883
2022-08-16 07:45:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_last.pt (epoch 16 @ 17930 updates, score 12.043) (writing took 6.33618376776576 seconds)
2022-08-16 07:46:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-16 07:48:53 | INFO | train_inner | epoch 017:     71 / 1122 loss=6.579, nll_loss=2.474, mask_ins=0.911, word_ins_ml=4.098, word_reposition=0.804, kpe=0.766, ppl=95.6, wps=4893.9, ups=0.24, wpb=20455.4, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=2.228, clip=0, loss_scale=11923, train_wall=267, wall=59713
2022-08-16 07:49:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-16 07:54:09 | INFO | train_inner | epoch 017:    172 / 1122 loss=6.526, nll_loss=2.437, mask_ins=0.914, word_ins_ml=4.064, word_reposition=0.796, kpe=0.752, ppl=92.15, wps=6493.1, ups=0.32, wpb=20487.2, bsz=256, num_updates=18100, lr=0.000262794, gnorm=2.146, clip=0, loss_scale=4623, train_wall=267, wall=60028
2022-08-16 07:59:27 | INFO | train_inner | epoch 017:    272 / 1122 loss=6.486, nll_loss=2.41, mask_ins=0.904, word_ins_ml=4.04, word_reposition=0.792, kpe=0.751, ppl=89.64, wps=6445.9, ups=0.31, wpb=20482.1, bsz=256, num_updates=18200, lr=0.000262071, gnorm=2.14, clip=0, loss_scale=4096, train_wall=267, wall=60346
2022-08-16 08:04:49 | INFO | train_inner | epoch 017:    372 / 1122 loss=6.444, nll_loss=2.373, mask_ins=0.898, word_ins_ml=4.007, word_reposition=0.778, kpe=0.761, ppl=87.06, wps=6337.8, ups=0.31, wpb=20436.9, bsz=256, num_updates=18300, lr=0.000261354, gnorm=2.091, clip=0, loss_scale=4096, train_wall=270, wall=60669
2022-08-16 08:10:05 | INFO | train_inner | epoch 017:    472 / 1122 loss=6.466, nll_loss=2.405, mask_ins=0.882, word_ins_ml=4.036, word_reposition=0.786, kpe=0.762, ppl=88.42, wps=6486.6, ups=0.32, wpb=20478.7, bsz=256, num_updates=18400, lr=0.000260643, gnorm=2.067, clip=0, loss_scale=4096, train_wall=267, wall=60984
2022-08-16 08:15:22 | INFO | train_inner | epoch 017:    572 / 1122 loss=6.457, nll_loss=2.381, mask_ins=0.897, word_ins_ml=4.014, word_reposition=0.785, kpe=0.762, ppl=87.88, wps=6462.6, ups=0.32, wpb=20497.2, bsz=256, num_updates=18500, lr=0.000259938, gnorm=2.115, clip=0, loss_scale=4096, train_wall=266, wall=61302
2022-08-16 08:20:37 | INFO | train_inner | epoch 017:    672 / 1122 loss=6.458, nll_loss=2.375, mask_ins=0.889, word_ins_ml=4.009, word_reposition=0.79, kpe=0.77, ppl=87.91, wps=6550.2, ups=0.32, wpb=20626.8, bsz=256, num_updates=18600, lr=0.000259238, gnorm=2.141, clip=0, loss_scale=7209, train_wall=266, wall=61616
2022-08-16 08:25:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-16 08:25:43 | INFO | train_inner | epoch 017:    773 / 1122 loss=6.428, nll_loss=2.348, mask_ins=0.885, word_ins_ml=3.984, word_reposition=0.79, kpe=0.769, ppl=86.13, wps=6778.4, ups=0.33, wpb=20738.5, bsz=256, num_updates=18700, lr=0.000258544, gnorm=2.113, clip=0, loss_scale=7705, train_wall=256, wall=61922
2022-08-16 08:28:59 | INFO | train_inner | epoch 017:    873 / 1122 loss=6.467, nll_loss=2.39, mask_ins=0.893, word_ins_ml=4.021, word_reposition=0.781, kpe=0.773, ppl=88.49, wps=10448.4, ups=0.51, wpb=20464.5, bsz=256, num_updates=18800, lr=0.000257855, gnorm=2.09, clip=0, loss_scale=4096, train_wall=151, wall=62118
2022-08-16 08:32:08 | INFO | train_inner | epoch 017:    973 / 1122 loss=6.495, nll_loss=2.412, mask_ins=0.886, word_ins_ml=4.04, word_reposition=0.791, kpe=0.778, ppl=90.18, wps=10852.2, ups=0.53, wpb=20488.1, bsz=256, num_updates=18900, lr=0.000257172, gnorm=2.091, clip=0, loss_scale=4096, train_wall=149, wall=62307
2022-08-16 08:35:15 | INFO | train_inner | epoch 017:   1073 / 1122 loss=nan, nll_loss=2.388, mask_ins=0.877, word_ins_ml=4.02, word_reposition=0.774, kpe=nan, ppl=nan, wps=10971.3, ups=0.53, wpb=20544.9, bsz=256, num_updates=19000, lr=0.000256495, gnorm=2.125, clip=0, loss_scale=4096, train_wall=147, wall=62494
2022-08-16 08:36:46 | INFO | train | epoch 017 | loss nan | nll_loss 2.392 | mask_ins 0.892 | word_ins_ml 4.024 | word_reposition 0.786 | kpe nan | ppl nan | wps 7177.9 | ups 0.35 | wpb 20518.7 | bsz 255.8 | num_updates 19049 | lr 0.000256164 | gnorm 2.12 | clip 0 | loss_scale 5122 | train_wall 2566 | wall 62586
2022-08-16 08:37:46 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.275 | nll_loss 6.01 | mask_ins 1.603 | word_ins_ml 7.419 | word_reposition 1.426 | kpe 1.827 | ppl 4956.15 | wps 16514.8 | wpb 2367.6 | bsz 32 | num_updates 19049 | best_loss 11.883
2022-08-16 08:38:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_last.pt (epoch 17 @ 19049 updates, score 12.275) (writing took 15.325228277593851 seconds)
2022-08-16 08:39:36 | INFO | train_inner | epoch 018:     51 / 1122 loss=nan, nll_loss=2.318, mask_ins=0.872, word_ins_ml=3.958, word_reposition=0.771, kpe=nan, ppl=nan, wps=7768.7, ups=0.38, wpb=20316.4, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=2.172, clip=0, loss_scale=4096, train_wall=146, wall=62756
2022-08-16 08:42:45 | INFO | train_inner | epoch 018:    151 / 1122 loss=6.384, nll_loss=2.361, mask_ins=0.884, word_ins_ml=3.996, word_reposition=0.785, kpe=0.719, ppl=83.5, wps=10887, ups=0.53, wpb=20556.5, bsz=256, num_updates=19200, lr=0.000255155, gnorm=2.115, clip=0, loss_scale=4096, train_wall=149, wall=62945
2022-08-16 08:44:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-16 08:45:53 | INFO | train_inner | epoch 018:    252 / 1122 loss=6.348, nll_loss=2.331, mask_ins=0.877, word_ins_ml=3.968, word_reposition=0.774, kpe=0.729, ppl=81.45, wps=10949.1, ups=0.53, wpb=20576.6, bsz=256, num_updates=19300, lr=0.000254493, gnorm=2.123, clip=0, loss_scale=6408, train_wall=148, wall=63133
2022-08-16 08:49:00 | INFO | train_inner | epoch 018:    352 / 1122 loss=6.386, nll_loss=2.368, mask_ins=0.873, word_ins_ml=4.002, word_reposition=0.779, kpe=0.732, ppl=83.62, wps=10965.6, ups=0.54, wpb=20473.8, bsz=256, num_updates=19400, lr=0.000253837, gnorm=2.145, clip=0, loss_scale=4096, train_wall=147, wall=63319
2022-08-16 08:52:08 | INFO | train_inner | epoch 018:    452 / 1122 loss=6.353, nll_loss=2.337, mask_ins=0.872, word_ins_ml=3.973, word_reposition=0.774, kpe=0.734, ppl=81.72, wps=10918.8, ups=0.53, wpb=20570.9, bsz=256, num_updates=19500, lr=0.000253185, gnorm=2.136, clip=0, loss_scale=4096, train_wall=149, wall=63508
2022-08-16 08:55:15 | INFO | train_inner | epoch 018:    552 / 1122 loss=6.351, nll_loss=2.337, mask_ins=0.872, word_ins_ml=3.974, word_reposition=0.766, kpe=0.74, ppl=81.62, wps=10937.2, ups=0.54, wpb=20442.1, bsz=256, num_updates=19600, lr=0.000252538, gnorm=2.134, clip=0, loss_scale=4096, train_wall=148, wall=63695
2022-08-16 08:58:22 | INFO | train_inner | epoch 018:    652 / 1122 loss=6.374, nll_loss=2.349, mask_ins=0.876, word_ins_ml=3.984, word_reposition=0.774, kpe=0.74, ppl=82.94, wps=10938.3, ups=0.54, wpb=20443.7, bsz=256, num_updates=19700, lr=0.000251896, gnorm=2.106, clip=0, loss_scale=4096, train_wall=148, wall=63882
2022-08-16 09:01:29 | INFO | train_inner | epoch 018:    752 / 1122 loss=nan, nll_loss=2.337, mask_ins=0.874, word_ins_ml=3.973, word_reposition=0.77, kpe=nan, ppl=nan, wps=11063.1, ups=0.54, wpb=20637.2, bsz=256, num_updates=19800, lr=0.000251259, gnorm=2.085, clip=0, loss_scale=5407, train_wall=147, wall=64068
2022-08-16 09:04:38 | INFO | train_inner | epoch 018:    852 / 1122 loss=6.387, nll_loss=2.342, mask_ins=0.88, word_ins_ml=3.977, word_reposition=0.782, kpe=0.748, ppl=83.69, wps=10919.8, ups=0.53, wpb=20693, bsz=256, num_updates=19900, lr=0.000250627, gnorm=2.1, clip=0, loss_scale=8192, train_wall=151, wall=64258
2022-08-16 09:07:44 | INFO | train_inner | epoch 018:    952 / 1122 loss=6.307, nll_loss=2.273, mask_ins=0.864, word_ins_ml=3.916, word_reposition=0.773, kpe=0.754, ppl=79.2, wps=10998.5, ups=0.54, wpb=20455.4, bsz=256, num_updates=20000, lr=0.00025, gnorm=2.09, clip=0, loss_scale=8192, train_wall=147, wall=64444
2022-08-16 09:10:51 | INFO | train_inner | epoch 018:   1052 / 1122 loss=6.361, nll_loss=2.333, mask_ins=0.866, word_ins_ml=3.969, word_reposition=0.775, kpe=0.751, ppl=82.22, wps=10957.6, ups=0.53, wpb=20524.2, bsz=256, num_updates=20100, lr=0.000249377, gnorm=2.077, clip=0, loss_scale=8192, train_wall=148, wall=64631
2022-08-16 09:13:04 | INFO | train | epoch 018 | loss nan | nll_loss 2.338 | mask_ins 0.874 | word_ins_ml 3.974 | word_reposition 0.775 | kpe nan | ppl nan | wps 10562.6 | ups 0.51 | wpb 20520.5 | bsz 255.8 | num_updates 20170 | lr 0.000248944 | gnorm 2.12 | clip 0 | loss_scale 5772 | train_wall 1661 | wall 64763
2022-08-16 09:14:04 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 12.057 | nll_loss 5.905 | mask_ins 1.557 | word_ins_ml 7.316 | word_reposition 1.487 | kpe 1.697 | ppl 4262.21 | wps 16442 | wpb 2367.6 | bsz 32 | num_updates 20170 | best_loss 11.883
2022-08-16 09:14:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased/checkpoint_last.pt (epoch 18 @ 20170 updates, score 12.057) (writing took 19.690116673707962 seconds)
2022-08-16 09:15:19 | INFO | train_inner | epoch 019:     30 / 1122 loss=6.359, nll_loss=2.342, mask_ins=0.866, word_ins_ml=3.977, word_reposition=0.775, kpe=0.742, ppl=82.08, wps=7581, ups=0.37, wpb=20307.9, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=2.179, clip=0, loss_scale=8192, train_wall=149, wall=64899
2022-08-16 09:17:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-16 09:18:27 | INFO | train_inner | epoch 019:    131 / 1122 loss=6.244, nll_loss=2.28, mask_ins=0.859, word_ins_ml=3.922, word_reposition=0.769, kpe=0.692, ppl=75.77, wps=10815.7, ups=0.53, wpb=20327.3, bsz=256, num_updates=20300, lr=0.000248146, gnorm=2.073, clip=0, loss_scale=7259, train_wall=148, wall=65087
2022-08-16 09:21:35 | INFO | train_inner | epoch 019:    231 / 1122 loss=nan, nll_loss=2.268, mask_ins=0.862, word_ins_ml=3.911, word_reposition=0.771, kpe=nan, ppl=nan, wps=10964.5, ups=0.53, wpb=20543.5, bsz=256, num_updates=20400, lr=0.000247537, gnorm=2.041, clip=0, loss_scale=4096, train_wall=148, wall=65274
2022-08-16 09:24:42 | INFO | train_inner | epoch 019:    331 / 1122 loss=6.279, nll_loss=2.303, mask_ins=0.868, word_ins_ml=3.942, word_reposition=0.764, kpe=0.705, ppl=77.66, wps=10963.1, ups=0.53, wpb=20526.1, bsz=256, num_updates=20500, lr=0.000246932, gnorm=2.141, clip=0, loss_scale=4096, train_wall=147, wall=65461
2022-08-16 09:27:48 | INFO | train_inner | epoch 019:    431 / 1122 loss=6.295, nll_loss=2.323, mask_ins=0.86, word_ins_ml=3.96, word_reposition=0.763, kpe=0.711, ppl=78.5, wps=10928.8, ups=0.54, wpb=20385.4, bsz=256, num_updates=20600, lr=0.000246332, gnorm=2.083, clip=0, loss_scale=4096, train_wall=147, wall=65648
2022-08-16 09:30:54 | INFO | train_inner | epoch 019:    531 / 1122 loss=6.281, nll_loss=2.296, mask_ins=0.855, word_ins_ml=3.936, word_reposition=0.774, kpe=0.716, ppl=77.76, wps=11166.3, ups=0.54, wpb=20682.3, bsz=256, num_updates=20700, lr=0.000245737, gnorm=2.104, clip=0, loss_scale=4096, train_wall=147, wall=65833
2022-08-16 09:33:58 | INFO | train_inner | epoch 019:    631 / 1122 loss=6.242, nll_loss=2.272, mask_ins=0.848, word_ins_ml=3.914, word_reposition=0.764, kpe=0.717, ppl=75.69, wps=11149.1, ups=0.54, wpb=20621, bsz=256, num_updates=20800, lr=0.000245145, gnorm=2.106, clip=0, loss_scale=4547, train_wall=146, wall=66018
2022-08-16 09:37:05 | INFO | train_inner | epoch 019:    731 / 1122 loss=6.259, nll_loss=2.287, mask_ins=0.847, word_ins_ml=3.927, word_reposition=0.769, kpe=0.717, ppl=76.6, wps=11093, ups=0.54, wpb=20690.8, bsz=256, num_updates=20900, lr=0.000244558, gnorm=2.12, clip=0, loss_scale=8192, train_wall=148, wall=66205
2022-08-16 09:40:10 | INFO | train_inner | epoch 019:    831 / 1122 loss=6.302, nll_loss=2.313, mask_ins=0.855, word_ins_ml=3.95, word_reposition=0.769, kpe=0.727, ppl=78.89, wps=11086, ups=0.54, wpb=20533.5, bsz=256, num_updates=21000, lr=0.000243975, gnorm=2.103, clip=0, loss_scale=8192, train_wall=146, wall=66390
2022-08-16 09:43:16 | INFO | train_inner | epoch 019:    931 / 1122 loss=nan, nll_loss=2.289, mask_ins=0.844, word_ins_ml=3.929, word_reposition=0.756, kpe=nan, ppl=nan, wps=11029.4, ups=0.54, wpb=20463.6, bsz=256, num_updates=21100, lr=0.000243396, gnorm=2.155, clip=0, loss_scale=8192, train_wall=147, wall=66575
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
train.sh: line 40: -fp16: command not found
