nohup: ignoring input
2022-07-10 14:45:48 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:18021
2022-07-10 14:45:48 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:18021
2022-07-10 14:45:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-10 14:45:48 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:18021
2022-07-10 14:45:48 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:18021
2022-07-10 14:45:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-10 14:45:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-10 14:45:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-10 14:45:48 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-10 14:45:48 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-10 14:45:48 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-10 14:45:48 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-10 14:45:48 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-10 14:45:48 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-10 14:45:48 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-10 14:45:48 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-10 14:45:52 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:18021', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_transformer_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='6,6,6', layers_num='6,6,6', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='/data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=False, no_share_maskpredictor=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert', decoder='transformer', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-10 14:45:52 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-10 14:45:52 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-07-10 14:45:52 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.source
2022-07-10 14:45:52 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.target
2022-07-10 14:45:52 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 valid source-target 13368 examples
2022-07-10 14:45:52 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-10 14:45:52 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-10 14:45:52 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-10 14:45:54 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 670
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.encoder_attn.k_proj.weight', 'decoder.layers_msk.0.encoder_attn.k_proj.bias', 'decoder.layers_msk.0.encoder_attn.v_proj.weight', 'decoder.layers_msk.0.encoder_attn.v_proj.bias', 'decoder.layers_msk.0.encoder_attn.q_proj.weight', 'decoder.layers_msk.0.encoder_attn.q_proj.bias', 'decoder.layers_msk.0.encoder_attn.out_proj.weight', 'decoder.layers_msk.0.encoder_attn.out_proj.bias', 'decoder.layers_msk.0.encoder_attn_layer_norm.weight', 'decoder.layers_msk.0.encoder_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.encoder_attn.k_proj.weight', 'decoder.layers_msk.1.encoder_attn.k_proj.bias', 'decoder.layers_msk.1.encoder_attn.v_proj.weight', 'decoder.layers_msk.1.encoder_attn.v_proj.bias', 'decoder.layers_msk.1.encoder_attn.q_proj.weight', 'decoder.layers_msk.1.encoder_attn.q_proj.bias', 'decoder.layers_msk.1.encoder_attn.out_proj.weight', 'decoder.layers_msk.1.encoder_attn.out_proj.bias', 'decoder.layers_msk.1.encoder_attn_layer_norm.weight', 'decoder.layers_msk.1.encoder_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.encoder_attn.k_proj.weight', 'decoder.layers_msk.2.encoder_attn.k_proj.bias', 'decoder.layers_msk.2.encoder_attn.v_proj.weight', 'decoder.layers_msk.2.encoder_attn.v_proj.bias', 'decoder.layers_msk.2.encoder_attn.q_proj.weight', 'decoder.layers_msk.2.encoder_attn.q_proj.bias', 'decoder.layers_msk.2.encoder_attn.out_proj.weight', 'decoder.layers_msk.2.encoder_attn.out_proj.bias', 'decoder.layers_msk.2.encoder_attn_layer_norm.weight', 'decoder.layers_msk.2.encoder_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.encoder_attn.k_proj.weight', 'decoder.layers_msk.3.encoder_attn.k_proj.bias', 'decoder.layers_msk.3.encoder_attn.v_proj.weight', 'decoder.layers_msk.3.encoder_attn.v_proj.bias', 'decoder.layers_msk.3.encoder_attn.q_proj.weight', 'decoder.layers_msk.3.encoder_attn.q_proj.bias', 'decoder.layers_msk.3.encoder_attn.out_proj.weight', 'decoder.layers_msk.3.encoder_attn.out_proj.bias', 'decoder.layers_msk.3.encoder_attn_layer_norm.weight', 'decoder.layers_msk.3.encoder_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.encoder_attn.k_proj.weight', 'decoder.layers_msk.4.encoder_attn.k_proj.bias', 'decoder.layers_msk.4.encoder_attn.v_proj.weight', 'decoder.layers_msk.4.encoder_attn.v_proj.bias', 'decoder.layers_msk.4.encoder_attn.q_proj.weight', 'decoder.layers_msk.4.encoder_attn.q_proj.bias', 'decoder.layers_msk.4.encoder_attn.out_proj.weight', 'decoder.layers_msk.4.encoder_attn.out_proj.bias', 'decoder.layers_msk.4.encoder_attn_layer_norm.weight', 'decoder.layers_msk.4.encoder_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.encoder_attn.k_proj.weight', 'decoder.layers_msk.5.encoder_attn.k_proj.bias', 'decoder.layers_msk.5.encoder_attn.v_proj.weight', 'decoder.layers_msk.5.encoder_attn.v_proj.bias', 'decoder.layers_msk.5.encoder_attn.q_proj.weight', 'decoder.layers_msk.5.encoder_attn.q_proj.bias', 'decoder.layers_msk.5.encoder_attn.out_proj.weight', 'decoder.layers_msk.5.encoder_attn.out_proj.bias', 'decoder.layers_msk.5.encoder_attn_layer_norm.weight', 'decoder.layers_msk.5.encoder_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 490
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 670
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.encoder_attn.k_proj.weight', 'decoder.layers_msk.0.encoder_attn.k_proj.bias', 'decoder.layers_msk.0.encoder_attn.v_proj.weight', 'decoder.layers_msk.0.encoder_attn.v_proj.bias', 'decoder.layers_msk.0.encoder_attn.q_proj.weight', 'decoder.layers_msk.0.encoder_attn.q_proj.bias', 'decoder.layers_msk.0.encoder_attn.out_proj.weight', 'decoder.layers_msk.0.encoder_attn.out_proj.bias', 'decoder.layers_msk.0.encoder_attn_layer_norm.weight', 'decoder.layers_msk.0.encoder_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.encoder_attn.k_proj.weight', 'decoder.layers_msk.1.encoder_attn.k_proj.bias', 'decoder.layers_msk.1.encoder_attn.v_proj.weight', 'decoder.layers_msk.1.encoder_attn.v_proj.bias', 'decoder.layers_msk.1.encoder_attn.q_proj.weight', 'decoder.layers_msk.1.encoder_attn.q_proj.bias', 'decoder.layers_msk.1.encoder_attn.out_proj.weight', 'decoder.layers_msk.1.encoder_attn.out_proj.bias', 'decoder.layers_msk.1.encoder_attn_layer_norm.weight', 'decoder.layers_msk.1.encoder_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.encoder_attn.k_proj.weight', 'decoder.layers_msk.2.encoder_attn.k_proj.bias', 'decoder.layers_msk.2.encoder_attn.v_proj.weight', 'decoder.layers_msk.2.encoder_attn.v_proj.bias', 'decoder.layers_msk.2.encoder_attn.q_proj.weight', 'decoder.layers_msk.2.encoder_attn.q_proj.bias', 'decoder.layers_msk.2.encoder_attn.out_proj.weight', 'decoder.layers_msk.2.encoder_attn.out_proj.bias', 'decoder.layers_msk.2.encoder_attn_layer_norm.weight', 'decoder.layers_msk.2.encoder_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.encoder_attn.k_proj.weight', 'decoder.layers_msk.3.encoder_attn.k_proj.bias', 'decoder.layers_msk.3.encoder_attn.v_proj.weight', 'decoder.layers_msk.3.encoder_attn.v_proj.bias', 'decoder.layers_msk.3.encoder_attn.q_proj.weight', 'decoder.layers_msk.3.encoder_attn.q_proj.bias', 'decoder.layers_msk.3.encoder_attn.out_proj.weight', 'decoder.layers_msk.3.encoder_attn.out_proj.bias', 'decoder.layers_msk.3.encoder_attn_layer_norm.weight', 'decoder.layers_msk.3.encoder_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.encoder_attn.k_proj.weight', 'decoder.layers_msk.4.encoder_attn.k_proj.bias', 'decoder.layers_msk.4.encoder_attn.v_proj.weight', 'decoder.layers_msk.4.encoder_attn.v_proj.bias', 'decoder.layers_msk.4.encoder_attn.q_proj.weight', 'decoder.layers_msk.4.encoder_attn.q_proj.bias', 'decoder.layers_msk.4.encoder_attn.out_proj.weight', 'decoder.layers_msk.4.encoder_attn.out_proj.bias', 'decoder.layers_msk.4.encoder_attn_layer_norm.weight', 'decoder.layers_msk.4.encoder_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.encoder_attn.k_proj.weight', 'decoder.layers_msk.5.encoder_attn.k_proj.bias', 'decoder.layers_msk.5.encoder_attn.v_proj.weight', 'decoder.layers_msk.5.encoder_attn.v_proj.bias', 'decoder.layers_msk.5.encoder_attn.q_proj.weight', 'decoder.layers_msk.5.encoder_attn.q_proj.bias', 'decoder.layers_msk.5.encoder_attn.out_proj.weight', 'decoder.layers_msk.5.encoder_attn.out_proj.bias', 'decoder.layers_msk.5.encoder_attn_layer_norm.weight', 'decoder.layers_msk.5.encoder_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 490
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
2022-07-10 14:45:55 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoder(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): EditorTransformerDecoder(
    (embed_tokens): Embedding(28996, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
    (embed_mask_ins): Embedding(256, 1536)
    (layers_msk): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layers_reposition): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-07-10 14:45:55 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-10 14:45:55 | INFO | fairseq_cli.train | num. model params: 273168384 (num. trained: 273168384)
2022-07-10 14:45:55 | INFO | fairseq_cli.train | num. Encoder model params: 108310272 (Encoder num. trained: 108310272)
2022-07-10 14:45:55 | INFO | fairseq_cli.train | num. Decoder model params: 164858112 (Decoder num. trained: 164858112)
Trained parameters: len 670
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.encoder_attn.k_proj.weight', 'decoder.layers_msk.0.encoder_attn.k_proj.bias', 'decoder.layers_msk.0.encoder_attn.v_proj.weight', 'decoder.layers_msk.0.encoder_attn.v_proj.bias', 'decoder.layers_msk.0.encoder_attn.q_proj.weight', 'decoder.layers_msk.0.encoder_attn.q_proj.bias', 'decoder.layers_msk.0.encoder_attn.out_proj.weight', 'decoder.layers_msk.0.encoder_attn.out_proj.bias', 'decoder.layers_msk.0.encoder_attn_layer_norm.weight', 'decoder.layers_msk.0.encoder_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.encoder_attn.k_proj.weight', 'decoder.layers_msk.1.encoder_attn.k_proj.bias', 'decoder.layers_msk.1.encoder_attn.v_proj.weight', 'decoder.layers_msk.1.encoder_attn.v_proj.bias', 'decoder.layers_msk.1.encoder_attn.q_proj.weight', 'decoder.layers_msk.1.encoder_attn.q_proj.bias', 'decoder.layers_msk.1.encoder_attn.out_proj.weight', 'decoder.layers_msk.1.encoder_attn.out_proj.bias', 'decoder.layers_msk.1.encoder_attn_layer_norm.weight', 'decoder.layers_msk.1.encoder_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.encoder_attn.k_proj.weight', 'decoder.layers_msk.2.encoder_attn.k_proj.bias', 'decoder.layers_msk.2.encoder_attn.v_proj.weight', 'decoder.layers_msk.2.encoder_attn.v_proj.bias', 'decoder.layers_msk.2.encoder_attn.q_proj.weight', 'decoder.layers_msk.2.encoder_attn.q_proj.bias', 'decoder.layers_msk.2.encoder_attn.out_proj.weight', 'decoder.layers_msk.2.encoder_attn.out_proj.bias', 'decoder.layers_msk.2.encoder_attn_layer_norm.weight', 'decoder.layers_msk.2.encoder_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.encoder_attn.k_proj.weight', 'decoder.layers_msk.3.encoder_attn.k_proj.bias', 'decoder.layers_msk.3.encoder_attn.v_proj.weight', 'decoder.layers_msk.3.encoder_attn.v_proj.bias', 'decoder.layers_msk.3.encoder_attn.q_proj.weight', 'decoder.layers_msk.3.encoder_attn.q_proj.bias', 'decoder.layers_msk.3.encoder_attn.out_proj.weight', 'decoder.layers_msk.3.encoder_attn.out_proj.bias', 'decoder.layers_msk.3.encoder_attn_layer_norm.weight', 'decoder.layers_msk.3.encoder_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.encoder_attn.k_proj.weight', 'decoder.layers_msk.4.encoder_attn.k_proj.bias', 'decoder.layers_msk.4.encoder_attn.v_proj.weight', 'decoder.layers_msk.4.encoder_attn.v_proj.bias', 'decoder.layers_msk.4.encoder_attn.q_proj.weight', 'decoder.layers_msk.4.encoder_attn.q_proj.bias', 'decoder.layers_msk.4.encoder_attn.out_proj.weight', 'decoder.layers_msk.4.encoder_attn.out_proj.bias', 'decoder.layers_msk.4.encoder_attn_layer_norm.weight', 'decoder.layers_msk.4.encoder_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.encoder_attn.k_proj.weight', 'decoder.layers_msk.5.encoder_attn.k_proj.bias', 'decoder.layers_msk.5.encoder_attn.v_proj.weight', 'decoder.layers_msk.5.encoder_attn.v_proj.bias', 'decoder.layers_msk.5.encoder_attn.q_proj.weight', 'decoder.layers_msk.5.encoder_attn.q_proj.bias', 'decoder.layers_msk.5.encoder_attn.out_proj.weight', 'decoder.layers_msk.5.encoder_attn.out_proj.bias', 'decoder.layers_msk.5.encoder_attn_layer_norm.weight', 'decoder.layers_msk.5.encoder_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 490
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 670
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.encoder_attn.k_proj.weight', 'decoder.layers_msk.0.encoder_attn.k_proj.bias', 'decoder.layers_msk.0.encoder_attn.v_proj.weight', 'decoder.layers_msk.0.encoder_attn.v_proj.bias', 'decoder.layers_msk.0.encoder_attn.q_proj.weight', 'decoder.layers_msk.0.encoder_attn.q_proj.bias', 'decoder.layers_msk.0.encoder_attn.out_proj.weight', 'decoder.layers_msk.0.encoder_attn.out_proj.bias', 'decoder.layers_msk.0.encoder_attn_layer_norm.weight', 'decoder.layers_msk.0.encoder_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.encoder_attn.k_proj.weight', 'decoder.layers_msk.1.encoder_attn.k_proj.bias', 'decoder.layers_msk.1.encoder_attn.v_proj.weight', 'decoder.layers_msk.1.encoder_attn.v_proj.bias', 'decoder.layers_msk.1.encoder_attn.q_proj.weight', 'decoder.layers_msk.1.encoder_attn.q_proj.bias', 'decoder.layers_msk.1.encoder_attn.out_proj.weight', 'decoder.layers_msk.1.encoder_attn.out_proj.bias', 'decoder.layers_msk.1.encoder_attn_layer_norm.weight', 'decoder.layers_msk.1.encoder_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.encoder_attn.k_proj.weight', 'decoder.layers_msk.2.encoder_attn.k_proj.bias', 'decoder.layers_msk.2.encoder_attn.v_proj.weight', 'decoder.layers_msk.2.encoder_attn.v_proj.bias', 'decoder.layers_msk.2.encoder_attn.q_proj.weight', 'decoder.layers_msk.2.encoder_attn.q_proj.bias', 'decoder.layers_msk.2.encoder_attn.out_proj.weight', 'decoder.layers_msk.2.encoder_attn.out_proj.bias', 'decoder.layers_msk.2.encoder_attn_layer_norm.weight', 'decoder.layers_msk.2.encoder_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.encoder_attn.k_proj.weight', 'decoder.layers_msk.3.encoder_attn.k_proj.bias', 'decoder.layers_msk.3.encoder_attn.v_proj.weight', 'decoder.layers_msk.3.encoder_attn.v_proj.bias', 'decoder.layers_msk.3.encoder_attn.q_proj.weight', 'decoder.layers_msk.3.encoder_attn.q_proj.bias', 'decoder.layers_msk.3.encoder_attn.out_proj.weight', 'decoder.layers_msk.3.encoder_attn.out_proj.bias', 'decoder.layers_msk.3.encoder_attn_layer_norm.weight', 'decoder.layers_msk.3.encoder_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.encoder_attn.k_proj.weight', 'decoder.layers_msk.4.encoder_attn.k_proj.bias', 'decoder.layers_msk.4.encoder_attn.v_proj.weight', 'decoder.layers_msk.4.encoder_attn.v_proj.bias', 'decoder.layers_msk.4.encoder_attn.q_proj.weight', 'decoder.layers_msk.4.encoder_attn.q_proj.bias', 'decoder.layers_msk.4.encoder_attn.out_proj.weight', 'decoder.layers_msk.4.encoder_attn.out_proj.bias', 'decoder.layers_msk.4.encoder_attn_layer_norm.weight', 'decoder.layers_msk.4.encoder_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.encoder_attn.k_proj.weight', 'decoder.layers_msk.5.encoder_attn.k_proj.bias', 'decoder.layers_msk.5.encoder_attn.v_proj.weight', 'decoder.layers_msk.5.encoder_attn.v_proj.bias', 'decoder.layers_msk.5.encoder_attn.q_proj.weight', 'decoder.layers_msk.5.encoder_attn.q_proj.bias', 'decoder.layers_msk.5.encoder_attn.out_proj.weight', 'decoder.layers_msk.5.encoder_attn.out_proj.bias', 'decoder.layers_msk.5.encoder_attn_layer_norm.weight', 'decoder.layers_msk.5.encoder_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 490
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_msk.0.self_attn.k_proj.weight', 'decoder.layers_msk.0.self_attn.k_proj.bias', 'decoder.layers_msk.0.self_attn.v_proj.weight', 'decoder.layers_msk.0.self_attn.v_proj.bias', 'decoder.layers_msk.0.self_attn.q_proj.weight', 'decoder.layers_msk.0.self_attn.q_proj.bias', 'decoder.layers_msk.0.self_attn.out_proj.weight', 'decoder.layers_msk.0.self_attn.out_proj.bias', 'decoder.layers_msk.0.self_attn_layer_norm.weight', 'decoder.layers_msk.0.self_attn_layer_norm.bias', 'decoder.layers_msk.0.fc1.weight', 'decoder.layers_msk.0.fc1.bias', 'decoder.layers_msk.0.fc2.weight', 'decoder.layers_msk.0.fc2.bias', 'decoder.layers_msk.0.final_layer_norm.weight', 'decoder.layers_msk.0.final_layer_norm.bias', 'decoder.layers_msk.1.self_attn.k_proj.weight', 'decoder.layers_msk.1.self_attn.k_proj.bias', 'decoder.layers_msk.1.self_attn.v_proj.weight', 'decoder.layers_msk.1.self_attn.v_proj.bias', 'decoder.layers_msk.1.self_attn.q_proj.weight', 'decoder.layers_msk.1.self_attn.q_proj.bias', 'decoder.layers_msk.1.self_attn.out_proj.weight', 'decoder.layers_msk.1.self_attn.out_proj.bias', 'decoder.layers_msk.1.self_attn_layer_norm.weight', 'decoder.layers_msk.1.self_attn_layer_norm.bias', 'decoder.layers_msk.1.fc1.weight', 'decoder.layers_msk.1.fc1.bias', 'decoder.layers_msk.1.fc2.weight', 'decoder.layers_msk.1.fc2.bias', 'decoder.layers_msk.1.final_layer_norm.weight', 'decoder.layers_msk.1.final_layer_norm.bias', 'decoder.layers_msk.2.self_attn.k_proj.weight', 'decoder.layers_msk.2.self_attn.k_proj.bias', 'decoder.layers_msk.2.self_attn.v_proj.weight', 'decoder.layers_msk.2.self_attn.v_proj.bias', 'decoder.layers_msk.2.self_attn.q_proj.weight', 'decoder.layers_msk.2.self_attn.q_proj.bias', 'decoder.layers_msk.2.self_attn.out_proj.weight', 'decoder.layers_msk.2.self_attn.out_proj.bias', 'decoder.layers_msk.2.self_attn_layer_norm.weight', 'decoder.layers_msk.2.self_attn_layer_norm.bias', 'decoder.layers_msk.2.fc1.weight', 'decoder.layers_msk.2.fc1.bias', 'decoder.layers_msk.2.fc2.weight', 'decoder.layers_msk.2.fc2.bias', 'decoder.layers_msk.2.final_layer_norm.weight', 'decoder.layers_msk.2.final_layer_norm.bias', 'decoder.layers_msk.3.self_attn.k_proj.weight', 'decoder.layers_msk.3.self_attn.k_proj.bias', 'decoder.layers_msk.3.self_attn.v_proj.weight', 'decoder.layers_msk.3.self_attn.v_proj.bias', 'decoder.layers_msk.3.self_attn.q_proj.weight', 'decoder.layers_msk.3.self_attn.q_proj.bias', 'decoder.layers_msk.3.self_attn.out_proj.weight', 'decoder.layers_msk.3.self_attn.out_proj.bias', 'decoder.layers_msk.3.self_attn_layer_norm.weight', 'decoder.layers_msk.3.self_attn_layer_norm.bias', 'decoder.layers_msk.3.fc1.weight', 'decoder.layers_msk.3.fc1.bias', 'decoder.layers_msk.3.fc2.weight', 'decoder.layers_msk.3.fc2.bias', 'decoder.layers_msk.3.final_layer_norm.weight', 'decoder.layers_msk.3.final_layer_norm.bias', 'decoder.layers_msk.4.self_attn.k_proj.weight', 'decoder.layers_msk.4.self_attn.k_proj.bias', 'decoder.layers_msk.4.self_attn.v_proj.weight', 'decoder.layers_msk.4.self_attn.v_proj.bias', 'decoder.layers_msk.4.self_attn.q_proj.weight', 'decoder.layers_msk.4.self_attn.q_proj.bias', 'decoder.layers_msk.4.self_attn.out_proj.weight', 'decoder.layers_msk.4.self_attn.out_proj.bias', 'decoder.layers_msk.4.self_attn_layer_norm.weight', 'decoder.layers_msk.4.self_attn_layer_norm.bias', 'decoder.layers_msk.4.fc1.weight', 'decoder.layers_msk.4.fc1.bias', 'decoder.layers_msk.4.fc2.weight', 'decoder.layers_msk.4.fc2.bias', 'decoder.layers_msk.4.final_layer_norm.weight', 'decoder.layers_msk.4.final_layer_norm.bias', 'decoder.layers_msk.5.self_attn.k_proj.weight', 'decoder.layers_msk.5.self_attn.k_proj.bias', 'decoder.layers_msk.5.self_attn.v_proj.weight', 'decoder.layers_msk.5.self_attn.v_proj.bias', 'decoder.layers_msk.5.self_attn.q_proj.weight', 'decoder.layers_msk.5.self_attn.q_proj.bias', 'decoder.layers_msk.5.self_attn.out_proj.weight', 'decoder.layers_msk.5.self_attn.out_proj.bias', 'decoder.layers_msk.5.self_attn_layer_norm.weight', 'decoder.layers_msk.5.self_attn_layer_norm.bias', 'decoder.layers_msk.5.fc1.weight', 'decoder.layers_msk.5.fc1.bias', 'decoder.layers_msk.5.fc2.weight', 'decoder.layers_msk.5.fc2.bias', 'decoder.layers_msk.5.final_layer_norm.weight', 'decoder.layers_msk.5.final_layer_norm.bias', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']2022-07-10 14:46:01 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-10 14:46:01 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-10 14:46:01 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_transformer_cased/checkpoint_last.pt
2022-07-10 14:46:01 | INFO | fairseq.trainer | loading train data for epoch 1
2022-07-10 14:46:01 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.source
2022-07-10 14:46:01 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.target
2022-07-10 14:46:01 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 train source-target 287112 examples
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-10 14:48:58 | INFO | train_inner | epoch 001:    100 / 1122 loss=23.982, nll_loss=13.924, mask_ins=7.371, word_ins_ml=14.093, word_reposition=2.518, ppl=1.65697e+07, wps=11887.9, ups=0.58, wpb=20527, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=21.625, clip=1, loss_scale=128, train_wall=174, wall=177
2022-07-10 14:51:51 | INFO | train_inner | epoch 001:    200 / 1122 loss=18.125, nll_loss=12.203, mask_ins=4.051, word_ins_ml=12.551, word_reposition=1.522, ppl=285783, wps=11895.8, ups=0.58, wpb=20583.2, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=18.993, clip=0, loss_scale=128, train_wall=172, wall=350
2022-07-10 14:54:43 | INFO | train_inner | epoch 001:    300 / 1122 loss=15.376, nll_loss=11.275, mask_ins=2.2, word_ins_ml=11.742, word_reposition=1.434, ppl=42530.2, wps=11913.6, ups=0.58, wpb=20561.3, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=3.952, clip=0, loss_scale=128, train_wall=172, wall=523
2022-07-10 14:57:37 | INFO | train_inner | epoch 001:    400 / 1122 loss=14.784, nll_loss=10.851, mask_ins=1.936, word_ins_ml=11.407, word_reposition=1.441, ppl=28213.6, wps=11851.6, ups=0.58, wpb=20576.5, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=2.088, clip=0, loss_scale=128, train_wall=173, wall=697
2022-07-10 15:00:28 | INFO | train_inner | epoch 001:    500 / 1122 loss=14.573, nll_loss=10.751, mask_ins=1.851, word_ins_ml=11.338, word_reposition=1.384, ppl=24365.8, wps=11985.8, ups=0.58, wpb=20523.5, bsz=256, num_updates=500, lr=5.009e-05, gnorm=1.403, clip=0, loss_scale=128, train_wall=170, wall=868
2022-07-10 15:03:21 | INFO | train_inner | epoch 001:    600 / 1122 loss=14.494, nll_loss=10.685, mask_ins=1.832, word_ins_ml=11.285, word_reposition=1.377, ppl=23073.2, wps=11865.3, ups=0.58, wpb=20491.4, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=1.374, clip=0, loss_scale=242, train_wall=172, wall=1040
2022-07-10 15:06:15 | INFO | train_inner | epoch 001:    700 / 1122 loss=14.444, nll_loss=10.617, mask_ins=1.836, word_ins_ml=11.226, word_reposition=1.382, ppl=22290.5, wps=11833, ups=0.58, wpb=20542.5, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=1.675, clip=0, loss_scale=256, train_wall=173, wall=1214
2022-07-10 15:09:10 | INFO | train_inner | epoch 001:    800 / 1122 loss=14.371, nll_loss=10.539, mask_ins=1.842, word_ins_ml=11.16, word_reposition=1.369, ppl=21181.8, wps=11716, ups=0.57, wpb=20579, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=1.551, clip=0, loss_scale=256, train_wall=175, wall=1390
2022-07-10 15:12:06 | INFO | train_inner | epoch 001:    900 / 1122 loss=14.308, nll_loss=10.468, mask_ins=1.832, word_ins_ml=11.099, word_reposition=1.377, ppl=20285.8, wps=11659.2, ups=0.57, wpb=20464, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=1.475, clip=0, loss_scale=256, train_wall=175, wall=1565
2022-07-10 15:15:00 | INFO | train_inner | epoch 001:   1000 / 1122 loss=14.211, nll_loss=10.382, mask_ins=1.824, word_ins_ml=11.026, word_reposition=1.361, ppl=18964.7, wps=11852.9, ups=0.58, wpb=20597.8, bsz=256, num_updates=1000, lr=0.00010008, gnorm=1.567, clip=0, loss_scale=256, train_wall=173, wall=1739
2022-07-10 15:17:51 | INFO | train_inner | epoch 001:   1100 / 1122 loss=14.173, nll_loss=10.317, mask_ins=1.842, word_ins_ml=10.971, word_reposition=1.36, ppl=18471.9, wps=11946.3, ups=0.58, wpb=20473.7, bsz=256, num_updates=1100, lr=0.000110078, gnorm=1.416, clip=0, loss_scale=453, train_wall=171, wall=1910
2022-07-10 15:18:28 | INFO | train | epoch 001 | loss 15.682 | nll_loss 11.076 | mask_ins 2.569 | word_ins_ml 11.614 | word_reposition 1.499 | ppl 52582.4 | wps 11849.3 | ups 0.58 | wpb 20520.3 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 5.131 | clip 0.1 | loss_scale 220 | train_wall 1937 | wall 1948
2022-07-10 15:18:55 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.533 | nll_loss 10.391 | mask_ins 2.225 | word_ins_ml 11.052 | word_reposition 1.256 | ppl 23702.3 | wps 37732.7 | wpb 2367.6 | bsz 32 | num_updates 1122
2022-07-10 15:19:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 1 @ 1122 updates, score 14.533) (writing took 7.870995725505054 seconds)
2022-07-10 15:21:18 | INFO | train_inner | epoch 002:     78 / 1122 loss=14.088, nll_loss=10.239, mask_ins=1.836, word_ins_ml=10.906, word_reposition=1.347, ppl=17420.5, wps=9811.6, ups=0.48, wpb=20333.3, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=1.581, clip=0, loss_scale=512, train_wall=172, wall=2118
2022-07-10 15:24:12 | INFO | train_inner | epoch 002:    178 / 1122 loss=14.039, nll_loss=10.176, mask_ins=1.828, word_ins_ml=10.852, word_reposition=1.359, ppl=16838.2, wps=11856.4, ups=0.58, wpb=20587.3, bsz=256, num_updates=1300, lr=0.000130074, gnorm=1.361, clip=0, loss_scale=512, train_wall=173, wall=2291
2022-07-10 15:27:06 | INFO | train_inner | epoch 002:    278 / 1122 loss=13.994, nll_loss=10.127, mask_ins=1.832, word_ins_ml=10.811, word_reposition=1.35, ppl=16317.3, wps=11837.9, ups=0.57, wpb=20599.8, bsz=256, num_updates=1400, lr=0.000140072, gnorm=1.392, clip=0, loss_scale=512, train_wall=173, wall=2465
2022-07-10 15:30:00 | INFO | train_inner | epoch 002:    378 / 1122 loss=13.927, nll_loss=10.07, mask_ins=1.823, word_ins_ml=10.763, word_reposition=1.341, ppl=15579.2, wps=11651.9, ups=0.57, wpb=20347.3, bsz=256, num_updates=1500, lr=0.00015007, gnorm=1.519, clip=0, loss_scale=512, train_wall=174, wall=2640
2022-07-10 15:32:55 | INFO | train_inner | epoch 002:    478 / 1122 loss=13.882, nll_loss=10.016, mask_ins=1.828, word_ins_ml=10.718, word_reposition=1.337, ppl=15101.3, wps=11810.7, ups=0.57, wpb=20567.7, bsz=256, num_updates=1600, lr=0.000160068, gnorm=1.462, clip=0, loss_scale=845, train_wall=173, wall=2814
2022-07-10 15:35:47 | INFO | train_inner | epoch 002:    578 / 1122 loss=13.853, nll_loss=9.975, mask_ins=1.828, word_ins_ml=10.683, word_reposition=1.341, ppl=14792.8, wps=11882.4, ups=0.58, wpb=20536.9, bsz=256, num_updates=1700, lr=0.000170066, gnorm=1.329, clip=0, loss_scale=1024, train_wall=172, wall=2987
2022-07-10 15:38:41 | INFO | train_inner | epoch 002:    678 / 1122 loss=13.833, nll_loss=9.923, mask_ins=1.849, word_ins_ml=10.639, word_reposition=1.346, ppl=14594.5, wps=11775.7, ups=0.58, wpb=20477.4, bsz=256, num_updates=1800, lr=0.000180064, gnorm=1.462, clip=0, loss_scale=1024, train_wall=173, wall=3161
2022-07-10 15:41:35 | INFO | train_inner | epoch 002:    778 / 1122 loss=13.769, nll_loss=9.89, mask_ins=1.824, word_ins_ml=10.611, word_reposition=1.334, ppl=13958.5, wps=11858, ups=0.58, wpb=20576, bsz=256, num_updates=1900, lr=0.000190062, gnorm=1.433, clip=0, loss_scale=1024, train_wall=173, wall=3334
2022-07-10 15:44:26 | INFO | train_inner | epoch 002:    878 / 1122 loss=13.722, nll_loss=9.849, mask_ins=1.818, word_ins_ml=10.577, word_reposition=1.327, ppl=13515.9, wps=11922.3, ups=0.58, wpb=20447.7, bsz=256, num_updates=2000, lr=0.00020006, gnorm=1.436, clip=0, loss_scale=1024, train_wall=171, wall=3506
2022-07-10 15:47:20 | INFO | train_inner | epoch 002:    978 / 1122 loss=13.704, nll_loss=9.821, mask_ins=1.821, word_ins_ml=10.553, word_reposition=1.33, ppl=13343.6, wps=11831, ups=0.58, wpb=20513.5, bsz=256, num_updates=2100, lr=0.000210058, gnorm=1.369, clip=0, loss_scale=1567, train_wall=173, wall=3679
2022-07-10 15:50:12 | INFO | train_inner | epoch 002:   1078 / 1122 loss=13.688, nll_loss=9.785, mask_ins=1.83, word_ins_ml=10.522, word_reposition=1.336, ppl=13201.2, wps=12017.7, ups=0.58, wpb=20708.1, bsz=256, num_updates=2200, lr=0.000220056, gnorm=1.393, clip=0, loss_scale=2048, train_wall=172, wall=3852
2022-07-10 15:51:28 | INFO | train | epoch 002 | loss 13.851 | nll_loss 9.974 | mask_ins 1.828 | word_ins_ml 10.682 | word_reposition 1.34 | ppl 14773.7 | wps 11628.2 | ups 0.57 | wpb 20521 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 1.421 | clip 0 | loss_scale 1015 | train_wall 1937 | wall 3928
2022-07-10 15:51:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 14.246 | nll_loss 9.871 | mask_ins 2.385 | word_ins_ml 10.613 | word_reposition 1.248 | ppl 19433 | wps 37777.4 | wpb 2367.6 | bsz 32 | num_updates 2244 | best_loss 14.246
2022-07-10 15:52:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 2 @ 2244 updates, score 14.246) (writing took 10.147172393277287 seconds)
2022-07-10 15:53:42 | INFO | train_inner | epoch 003:     56 / 1122 loss=13.647, nll_loss=9.737, mask_ins=1.833, word_ins_ml=10.48, word_reposition=1.334, ppl=12829.4, wps=9697.7, ups=0.48, wpb=20387.7, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=1.617, clip=0, loss_scale=2048, train_wall=173, wall=4062
2022-07-10 15:56:38 | INFO | train_inner | epoch 003:    156 / 1122 loss=13.583, nll_loss=9.684, mask_ins=1.827, word_ins_ml=10.434, word_reposition=1.321, ppl=12271.8, wps=11669.8, ups=0.57, wpb=20466.9, bsz=256, num_updates=2400, lr=0.000240052, gnorm=1.394, clip=0, loss_scale=2048, train_wall=175, wall=4237
2022-07-10 15:59:33 | INFO | train_inner | epoch 003:    256 / 1122 loss=13.571, nll_loss=9.67, mask_ins=1.816, word_ins_ml=10.423, word_reposition=1.331, ppl=12169.4, wps=11744.7, ups=0.57, wpb=20590.4, bsz=256, num_updates=2500, lr=0.00025005, gnorm=1.417, clip=0, loss_scale=2048, train_wall=174, wall=4412
2022-07-10 16:02:28 | INFO | train_inner | epoch 003:    356 / 1122 loss=13.537, nll_loss=9.634, mask_ins=1.825, word_ins_ml=10.392, word_reposition=1.32, ppl=11888.6, wps=11742.8, ups=0.57, wpb=20552.9, bsz=256, num_updates=2600, lr=0.000260048, gnorm=1.297, clip=0, loss_scale=2888, train_wall=174, wall=4588
2022-07-10 16:05:22 | INFO | train_inner | epoch 003:    456 / 1122 loss=13.496, nll_loss=9.604, mask_ins=1.806, word_ins_ml=10.367, word_reposition=1.324, ppl=11555.3, wps=11694.3, ups=0.57, wpb=20384, bsz=256, num_updates=2700, lr=0.000270046, gnorm=1.324, clip=0, loss_scale=4096, train_wall=174, wall=4762
2022-07-10 16:08:17 | INFO | train_inner | epoch 003:    556 / 1122 loss=13.499, nll_loss=9.6, mask_ins=1.829, word_ins_ml=10.363, word_reposition=1.307, ppl=11576.1, wps=11695.4, ups=0.57, wpb=20480.9, bsz=256, num_updates=2800, lr=0.000280044, gnorm=1.345, clip=0, loss_scale=4096, train_wall=174, wall=4937
2022-07-10 16:11:13 | INFO | train_inner | epoch 003:    656 / 1122 loss=13.469, nll_loss=9.575, mask_ins=1.818, word_ins_ml=10.342, word_reposition=1.309, ppl=11337.6, wps=11737.6, ups=0.57, wpb=20612.3, bsz=256, num_updates=2900, lr=0.000290042, gnorm=1.362, clip=0, loss_scale=4096, train_wall=175, wall=5113
2022-07-10 16:14:08 | INFO | train_inner | epoch 003:    756 / 1122 loss=13.451, nll_loss=9.543, mask_ins=1.824, word_ins_ml=10.315, word_reposition=1.312, ppl=11198.1, wps=11792.7, ups=0.57, wpb=20597.8, bsz=256, num_updates=3000, lr=0.00030004, gnorm=1.297, clip=0, loss_scale=4096, train_wall=174, wall=5287
2022-07-10 16:17:01 | INFO | train_inner | epoch 003:    856 / 1122 loss=13.438, nll_loss=9.504, mask_ins=1.814, word_ins_ml=10.28, word_reposition=1.344, ppl=11101.2, wps=11863.4, ups=0.58, wpb=20609.8, bsz=256, num_updates=3100, lr=0.000310038, gnorm=1.496, clip=0, loss_scale=5284, train_wall=173, wall=5461
2022-07-10 16:19:55 | INFO | train_inner | epoch 003:    956 / 1122 loss=13.394, nll_loss=9.398, mask_ins=1.833, word_ins_ml=10.186, word_reposition=1.375, ppl=10763.4, wps=11832.1, ups=0.58, wpb=20572.9, bsz=256, num_updates=3200, lr=0.000320036, gnorm=1.692, clip=0, loss_scale=8192, train_wall=173, wall=5635
2022-07-10 16:22:50 | INFO | train_inner | epoch 003:   1056 / 1122 loss=13.333, nll_loss=9.287, mask_ins=1.832, word_ins_ml=10.087, word_reposition=1.414, ppl=10319.8, wps=11752.6, ups=0.57, wpb=20512.4, bsz=256, num_updates=3300, lr=0.000330034, gnorm=1.821, clip=0, loss_scale=8192, train_wall=174, wall=5809
2022-07-10 16:24:45 | INFO | train | epoch 003 | loss 13.47 | nll_loss 9.538 | mask_ins 1.823 | word_ins_ml 10.308 | word_reposition 1.34 | ppl 11350.6 | wps 11531.7 | ups 0.56 | wpb 20521.3 | bsz 255.8 | num_updates 3366 | lr 0.000336633 | gnorm 1.51 | clip 0 | loss_scale 4598 | train_wall 1951 | wall 5924
2022-07-10 16:25:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 13.774 | nll_loss 9.546 | mask_ins 2.029 | word_ins_ml 10.349 | word_reposition 1.395 | ppl 14004.6 | wps 37696.1 | wpb 2367.6 | bsz 32 | num_updates 3366 | best_loss 13.774
2022-07-10 16:25:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 3 @ 3366 updates, score 13.774) (writing took 10.17805079370737 seconds)
2022-07-10 16:26:20 | INFO | train_inner | epoch 004:     34 / 1122 loss=13.211, nll_loss=9.175, mask_ins=1.815, word_ins_ml=9.989, word_reposition=1.406, ppl=9478.88, wps=9669.2, ups=0.48, wpb=20354.1, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=2.269, clip=0, loss_scale=8192, train_wall=173, wall=6020
2022-07-10 16:28:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-10 16:29:17 | INFO | train_inner | epoch 004:    135 / 1122 loss=13.138, nll_loss=9.044, mask_ins=1.832, word_ins_ml=9.874, word_reposition=1.432, ppl=9013.73, wps=11633.7, ups=0.57, wpb=20502.9, bsz=256, num_updates=3500, lr=0.00035003, gnorm=2.262, clip=0, loss_scale=7665, train_wall=175, wall=6196
2022-07-10 16:32:11 | INFO | train_inner | epoch 004:    235 / 1122 loss=13.08, nll_loss=8.991, mask_ins=1.808, word_ins_ml=9.829, word_reposition=1.443, ppl=8659.96, wps=11821.2, ups=0.57, wpb=20607.3, bsz=256, num_updates=3600, lr=0.000360028, gnorm=2.63, clip=0, loss_scale=4096, train_wall=174, wall=6370
2022-07-10 16:35:04 | INFO | train_inner | epoch 004:    335 / 1122 loss=13.049, nll_loss=8.918, mask_ins=1.824, word_ins_ml=9.767, word_reposition=1.458, ppl=8475.89, wps=11811.2, ups=0.58, wpb=20432.2, bsz=256, num_updates=3700, lr=0.000370026, gnorm=2.624, clip=0, loss_scale=4096, train_wall=172, wall=6543
2022-07-10 16:36:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-10 16:38:00 | INFO | train_inner | epoch 004:    436 / 1122 loss=13.001, nll_loss=8.865, mask_ins=1.816, word_ins_ml=9.721, word_reposition=1.464, ppl=8196.68, wps=11741.4, ups=0.57, wpb=20646, bsz=256, num_updates=3800, lr=0.000380024, gnorm=2.754, clip=0, loss_scale=3285, train_wall=175, wall=6719
2022-07-10 16:40:53 | INFO | train_inner | epoch 004:    536 / 1122 loss=12.946, nll_loss=8.81, mask_ins=1.811, word_ins_ml=9.674, word_reposition=1.46, ppl=7888.83, wps=11798.2, ups=0.58, wpb=20484.3, bsz=256, num_updates=3900, lr=0.000390022, gnorm=2.758, clip=0, loss_scale=2048, train_wall=173, wall=6893
2022-07-10 16:40:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 16:43:49 | INFO | train_inner | epoch 004:    637 / 1122 loss=12.951, nll_loss=8.781, mask_ins=1.822, word_ins_ml=9.649, word_reposition=1.48, ppl=7916.92, wps=11683.9, ups=0.57, wpb=20505.7, bsz=256, num_updates=4000, lr=0.00040002, gnorm=2.599, clip=0, loss_scale=1044, train_wall=175, wall=7068
2022-07-10 16:46:43 | INFO | train_inner | epoch 004:    737 / 1122 loss=12.852, nll_loss=8.716, mask_ins=1.796, word_ins_ml=9.592, word_reposition=1.465, ppl=7393.95, wps=11855, ups=0.57, wpb=20629.5, bsz=256, num_updates=4100, lr=0.000410018, gnorm=2.391, clip=0, loss_scale=1024, train_wall=173, wall=7242
2022-07-10 16:49:37 | INFO | train_inner | epoch 004:    837 / 1122 loss=12.82, nll_loss=8.658, mask_ins=1.802, word_ins_ml=9.541, word_reposition=1.477, ppl=7231.28, wps=11757.9, ups=0.58, wpb=20444.7, bsz=256, num_updates=4200, lr=0.000420016, gnorm=2.357, clip=0, loss_scale=1024, train_wall=173, wall=7416
2022-07-10 16:52:31 | INFO | train_inner | epoch 004:    937 / 1122 loss=12.787, nll_loss=8.609, mask_ins=1.801, word_ins_ml=9.498, word_reposition=1.487, ppl=7067.22, wps=11852.3, ups=0.57, wpb=20636.4, bsz=256, num_updates=4300, lr=0.000430014, gnorm=2.512, clip=0, loss_scale=1024, train_wall=173, wall=7590
2022-07-10 16:55:25 | INFO | train_inner | epoch 004:   1037 / 1122 loss=12.725, nll_loss=8.509, mask_ins=1.81, word_ins_ml=9.411, word_reposition=1.504, ppl=6771.17, wps=11786.1, ups=0.58, wpb=20459.3, bsz=256, num_updates=4400, lr=0.000440012, gnorm=2.533, clip=0, loss_scale=1024, train_wall=173, wall=7764
2022-07-10 16:57:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 16:57:51 | INFO | train | epoch 004 | loss 12.922 | nll_loss 8.772 | mask_ins 1.812 | word_ins_ml 9.64 | word_reposition 1.47 | ppl 7760.93 | wps 11549 | ups 0.56 | wpb 20519.6 | bsz 255.8 | num_updates 4484 | lr 0.00044841 | gnorm 2.629 | clip 0.1 | loss_scale 2747 | train_wall 1941 | wall 7911
2022-07-10 16:58:18 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 13.586 | nll_loss 9.154 | mask_ins 2.028 | word_ins_ml 10.026 | word_reposition 1.532 | ppl 12299.8 | wps 37790.1 | wpb 2367.6 | bsz 32 | num_updates 4484 | best_loss 13.586
2022-07-10 16:58:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 4 @ 4484 updates, score 13.586) (writing took 10.022117383778095 seconds)
2022-07-10 16:58:56 | INFO | train_inner | epoch 005:     16 / 1122 loss=12.637, nll_loss=8.392, mask_ins=1.801, word_ins_ml=9.309, word_reposition=1.528, ppl=6371.39, wps=9651.9, ups=0.47, wpb=20367.1, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=3.733, clip=1, loss_scale=1734, train_wall=174, wall=7975
2022-07-10 17:01:48 | INFO | train_inner | epoch 005:    116 / 1122 loss=12.442, nll_loss=8.143, mask_ins=1.811, word_ins_ml=9.091, word_reposition=1.54, ppl=5562.97, wps=11942, ups=0.58, wpb=20638.2, bsz=256, num_updates=4600, lr=0.000460008, gnorm=3.16, clip=0, loss_scale=1024, train_wall=172, wall=8148
2022-07-10 17:04:41 | INFO | train_inner | epoch 005:    216 / 1122 loss=12.273, nll_loss=7.965, mask_ins=1.799, word_ins_ml=8.935, word_reposition=1.539, ppl=4949.54, wps=11868.3, ups=0.58, wpb=20529.6, bsz=256, num_updates=4700, lr=0.000470006, gnorm=3.277, clip=0, loss_scale=1024, train_wall=172, wall=8321
2022-07-10 17:07:34 | INFO | train_inner | epoch 005:    316 / 1122 loss=12.152, nll_loss=7.811, mask_ins=1.81, word_ins_ml=8.801, word_reposition=1.541, ppl=4551.59, wps=11902.8, ups=0.58, wpb=20571, bsz=256, num_updates=4800, lr=0.000480004, gnorm=3.365, clip=0, loss_scale=1024, train_wall=172, wall=8494
2022-07-10 17:10:28 | INFO | train_inner | epoch 005:    416 / 1122 loss=11.986, nll_loss=7.619, mask_ins=1.794, word_ins_ml=8.634, word_reposition=1.558, ppl=4056.3, wps=11813.8, ups=0.58, wpb=20494.8, bsz=256, num_updates=4900, lr=0.000490002, gnorm=3.473, clip=0, loss_scale=1024, train_wall=173, wall=8667
2022-07-10 17:13:20 | INFO | train_inner | epoch 005:    516 / 1122 loss=11.787, nll_loss=7.377, mask_ins=1.805, word_ins_ml=8.424, word_reposition=1.558, ppl=3533.41, wps=11807, ups=0.58, wpb=20366.9, bsz=256, num_updates=5000, lr=0.0005, gnorm=3.633, clip=0, loss_scale=1085, train_wall=172, wall=8840
2022-07-10 17:16:13 | INFO | train_inner | epoch 005:    616 / 1122 loss=11.576, nll_loss=7.157, mask_ins=1.793, word_ins_ml=8.232, word_reposition=1.55, ppl=3052.72, wps=11846.1, ups=0.58, wpb=20492.1, bsz=256, num_updates=5100, lr=0.000495074, gnorm=3.595, clip=0, loss_scale=2048, train_wall=172, wall=9013
2022-07-10 17:17:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 17:17:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 17:19:09 | INFO | train_inner | epoch 005:    718 / 1122 loss=11.397, nll_loss=6.941, mask_ins=1.802, word_ins_ml=8.045, word_reposition=1.551, ppl=2697.36, wps=11712.6, ups=0.57, wpb=20548.7, bsz=256, num_updates=5200, lr=0.00049029, gnorm=3.984, clip=0, loss_scale=1029, train_wall=175, wall=9188
2022-07-10 17:21:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-10 17:22:04 | INFO | train_inner | epoch 005:    819 / 1122 loss=11.187, nll_loss=6.725, mask_ins=1.792, word_ins_ml=7.858, word_reposition=1.537, ppl=2331.39, wps=11795.9, ups=0.57, wpb=20641.6, bsz=256, num_updates=5300, lr=0.000485643, gnorm=4.687, clip=0, loss_scale=474, train_wall=174, wall=9363
2022-07-10 17:23:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-10 17:24:57 | INFO | train_inner | epoch 005:    920 / 1122 loss=10.975, nll_loss=6.47, mask_ins=1.804, word_ins_ml=7.637, word_reposition=1.534, ppl=2012.67, wps=11843.5, ups=0.58, wpb=20571.1, bsz=256, num_updates=5400, lr=0.000481125, gnorm=4.683, clip=0, loss_scale=171, train_wall=173, wall=9537
2022-07-10 17:27:49 | INFO | train_inner | epoch 005:   1020 / 1122 loss=10.718, nll_loss=6.214, mask_ins=1.786, word_ins_ml=7.414, word_reposition=1.519, ppl=1684.56, wps=11898.3, ups=0.58, wpb=20449.2, bsz=256, num_updates=5500, lr=0.000476731, gnorm=4.451, clip=0, loss_scale=128, train_wall=171, wall=9709
2022-07-10 17:30:42 | INFO | train_inner | epoch 005:   1120 / 1122 loss=10.543, nll_loss=6.001, mask_ins=1.806, word_ins_ml=7.229, word_reposition=1.509, ppl=1492.46, wps=11878.3, ups=0.58, wpb=20576.3, bsz=256, num_updates=5600, lr=0.000472456, gnorm=4.014, clip=0, loss_scale=128, train_wall=172, wall=9882
2022-07-10 17:30:45 | INFO | train | epoch 005 | loss 11.562 | nll_loss 7.145 | mask_ins 1.8 | word_ins_ml 8.222 | word_reposition 1.54 | ppl 3023.36 | wps 11622.4 | ups 0.57 | wpb 20521.3 | bsz 255.8 | num_updates 5602 | lr 0.000472371 | gnorm 3.847 | clip 0 | loss_scale 834 | train_wall 1929 | wall 9885
2022-07-10 17:31:12 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 12.975 | nll_loss 8.125 | mask_ins 2.112 | word_ins_ml 9.192 | word_reposition 1.671 | ppl 8048.94 | wps 37767.9 | wpb 2367.6 | bsz 32 | num_updates 5602 | best_loss 12.975
2022-07-10 17:31:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 5 @ 5602 updates, score 12.975) (writing took 10.426573001779616 seconds)
2022-07-10 17:34:14 | INFO | train_inner | epoch 006:     98 / 1122 loss=10.336, nll_loss=5.795, mask_ins=1.806, word_ins_ml=7.051, word_reposition=1.478, ppl=1292.29, wps=9623.5, ups=0.47, wpb=20368.7, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=4.188, clip=0, loss_scale=128, train_wall=174, wall=10093
2022-07-10 17:37:09 | INFO | train_inner | epoch 006:    198 / 1122 loss=10.124, nll_loss=5.577, mask_ins=1.792, word_ins_ml=6.862, word_reposition=1.47, ppl=1116.18, wps=11748.5, ups=0.57, wpb=20577.1, bsz=256, num_updates=5800, lr=0.000464238, gnorm=3.85, clip=0, loss_scale=128, train_wall=174, wall=10269
2022-07-10 17:40:03 | INFO | train_inner | epoch 006:    298 / 1122 loss=9.967, nll_loss=5.412, mask_ins=1.796, word_ins_ml=6.719, word_reposition=1.452, ppl=1000.68, wps=11856.7, ups=0.58, wpb=20619.6, bsz=256, num_updates=5900, lr=0.000460287, gnorm=3.91, clip=0, loss_scale=198, train_wall=173, wall=10443
2022-07-10 17:42:57 | INFO | train_inner | epoch 006:    398 / 1122 loss=9.798, nll_loss=5.25, mask_ins=1.796, word_ins_ml=6.579, word_reposition=1.423, ppl=890.42, wps=11821.9, ups=0.57, wpb=20571.1, bsz=256, num_updates=6000, lr=0.000456435, gnorm=3.921, clip=0, loss_scale=256, train_wall=173, wall=10617
2022-07-10 17:45:53 | INFO | train_inner | epoch 006:    498 / 1122 loss=9.631, nll_loss=5.066, mask_ins=1.799, word_ins_ml=6.418, word_reposition=1.413, ppl=792.91, wps=11676.4, ups=0.57, wpb=20488.9, bsz=256, num_updates=6100, lr=0.000452679, gnorm=3.94, clip=0, loss_scale=256, train_wall=175, wall=10792
2022-07-10 17:48:47 | INFO | train_inner | epoch 006:    598 / 1122 loss=9.447, nll_loss=4.897, mask_ins=1.787, word_ins_ml=6.271, word_reposition=1.39, ppl=698.15, wps=11681.6, ups=0.57, wpb=20356.2, bsz=256, num_updates=6200, lr=0.000449013, gnorm=3.775, clip=0, loss_scale=256, train_wall=173, wall=10966
2022-07-10 17:51:41 | INFO | train_inner | epoch 006:    698 / 1122 loss=9.344, nll_loss=4.782, mask_ins=1.796, word_ins_ml=6.17, word_reposition=1.378, ppl=650.04, wps=11864.4, ups=0.57, wpb=20667.1, bsz=256, num_updates=6300, lr=0.000445435, gnorm=3.948, clip=0, loss_scale=256, train_wall=173, wall=11140
2022-07-10 17:54:34 | INFO | train_inner | epoch 006:    798 / 1122 loss=9.138, nll_loss=4.584, mask_ins=1.793, word_ins_ml=5.998, word_reposition=1.347, ppl=563.21, wps=11809.3, ups=0.58, wpb=20483.8, bsz=256, num_updates=6400, lr=0.000441942, gnorm=3.653, clip=0, loss_scale=366, train_wall=173, wall=11314
2022-07-10 17:57:28 | INFO | train_inner | epoch 006:    898 / 1122 loss=9.007, nll_loss=4.459, mask_ins=1.792, word_ins_ml=5.888, word_reposition=1.328, ppl=514.44, wps=11879.2, ups=0.58, wpb=20613.2, bsz=256, num_updates=6500, lr=0.000438529, gnorm=4.057, clip=0, loss_scale=512, train_wall=173, wall=11487
2022-07-10 18:00:21 | INFO | train_inner | epoch 006:    998 / 1122 loss=8.866, nll_loss=4.346, mask_ins=1.782, word_ins_ml=5.788, word_reposition=1.296, ppl=466.73, wps=11883.7, ups=0.58, wpb=20502.7, bsz=256, num_updates=6600, lr=0.000435194, gnorm=3.923, clip=0, loss_scale=512, train_wall=172, wall=11660
2022-07-10 18:03:14 | INFO | train_inner | epoch 006:   1098 / 1122 loss=8.748, nll_loss=4.248, mask_ins=1.776, word_ins_ml=5.7, word_reposition=1.272, ppl=429.98, wps=11815.8, ups=0.58, wpb=20511.7, bsz=256, num_updates=6700, lr=0.000431934, gnorm=3.609, clip=0, loss_scale=512, train_wall=173, wall=11834
2022-07-10 18:03:55 | INFO | train | epoch 006 | loss 9.473 | nll_loss 4.93 | mask_ins 1.792 | word_ins_ml 6.298 | word_reposition 1.384 | ppl 710.85 | wps 11570.6 | ups 0.56 | wpb 20520.5 | bsz 255.8 | num_updates 6724 | lr 0.000431163 | gnorm 3.883 | clip 0 | loss_scale 312 | train_wall 1943 | wall 11875
2022-07-10 18:04:21 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.825 | nll_loss 7.086 | mask_ins 1.919 | word_ins_ml 8.313 | word_reposition 1.592 | ppl 3627.43 | wps 37908.7 | wpb 2367.6 | bsz 32 | num_updates 6724 | best_loss 11.825
2022-07-10 18:04:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 6 @ 6724 updates, score 11.825) (writing took 10.577445104718208 seconds)
2022-07-10 18:06:45 | INFO | train_inner | epoch 007:     76 / 1122 loss=8.612, nll_loss=4.123, mask_ins=1.765, word_ins_ml=5.591, word_reposition=1.256, ppl=391.21, wps=9644.7, ups=0.47, wpb=20308.2, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=3.769, clip=0, loss_scale=512, train_wall=173, wall=12044
2022-07-10 18:09:39 | INFO | train_inner | epoch 007:    176 / 1122 loss=8.513, nll_loss=3.981, mask_ins=1.81, word_ins_ml=5.465, word_reposition=1.239, ppl=365.44, wps=11790.3, ups=0.57, wpb=20586.9, bsz=256, num_updates=6900, lr=0.000425628, gnorm=3.458, clip=0, loss_scale=671, train_wall=174, wall=12219
2022-07-10 18:10:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 18:10:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-10 18:10:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-10 18:10:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-07-10 18:12:40 | INFO | train_inner | epoch 007:    280 / 1122 loss=8.588, nll_loss=4.102, mask_ins=1.786, word_ins_ml=5.569, word_reposition=1.232, ppl=384.76, wps=11401.2, ups=0.55, wpb=20651.7, bsz=256, num_updates=7000, lr=0.000422577, gnorm=5.999, clip=3, loss_scale=366, train_wall=180, wall=12400
2022-07-10 18:14:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-07-10 18:15:36 | INFO | train_inner | epoch 007:    381 / 1122 loss=8.35, nll_loss=3.87, mask_ins=1.791, word_ins_ml=5.365, word_reposition=1.194, ppl=326.2, wps=11711.9, ups=0.57, wpb=20529.3, bsz=256, num_updates=7100, lr=0.000419591, gnorm=5.319, clip=1, loss_scale=51, train_wall=174, wall=12575
2022-07-10 18:18:31 | INFO | train_inner | epoch 007:    481 / 1122 loss=8.241, nll_loss=3.788, mask_ins=1.782, word_ins_ml=5.292, word_reposition=1.166, ppl=302.44, wps=11721, ups=0.57, wpb=20507.2, bsz=256, num_updates=7200, lr=0.000416667, gnorm=3.78, clip=0, loss_scale=32, train_wall=174, wall=12750
2022-07-10 18:21:25 | INFO | train_inner | epoch 007:    581 / 1122 loss=8.163, nll_loss=3.693, mask_ins=1.799, word_ins_ml=5.207, word_reposition=1.158, ppl=286.69, wps=11768, ups=0.57, wpb=20473.2, bsz=256, num_updates=7300, lr=0.000413803, gnorm=3.352, clip=0, loss_scale=32, train_wall=173, wall=12924
2022-07-10 18:24:18 | INFO | train_inner | epoch 007:    681 / 1122 loss=8.096, nll_loss=3.645, mask_ins=1.792, word_ins_ml=5.163, word_reposition=1.141, ppl=273.58, wps=11826.2, ups=0.58, wpb=20514.4, bsz=256, num_updates=7400, lr=0.000410997, gnorm=3.63, clip=1, loss_scale=32, train_wall=173, wall=13098
2022-07-10 18:27:11 | INFO | train_inner | epoch 007:    781 / 1122 loss=7.987, nll_loss=3.527, mask_ins=1.804, word_ins_ml=5.057, word_reposition=1.125, ppl=253.64, wps=11931.3, ups=0.58, wpb=20574.4, bsz=256, num_updates=7500, lr=0.000408248, gnorm=3.175, clip=0, loss_scale=32, train_wall=172, wall=13270
2022-07-10 18:30:04 | INFO | train_inner | epoch 007:    881 / 1122 loss=7.927, nll_loss=3.515, mask_ins=1.784, word_ins_ml=5.046, word_reposition=1.098, ppl=243.42, wps=11820.8, ups=0.58, wpb=20517.4, bsz=256, num_updates=7600, lr=0.000405554, gnorm=3.127, clip=0, loss_scale=41, train_wall=173, wall=13444
2022-07-10 18:32:57 | INFO | train_inner | epoch 007:    981 / 1122 loss=7.861, nll_loss=3.453, mask_ins=1.786, word_ins_ml=4.99, word_reposition=1.085, ppl=232.53, wps=11883.9, ups=0.58, wpb=20579.1, bsz=256, num_updates=7700, lr=0.000402911, gnorm=3.14, clip=0, loss_scale=64, train_wall=172, wall=13617
2022-07-10 18:35:49 | INFO | train_inner | epoch 007:   1081 / 1122 loss=7.838, nll_loss=3.447, mask_ins=1.789, word_ins_ml=4.984, word_reposition=1.065, ppl=228.77, wps=11937.8, ups=0.58, wpb=20534.8, bsz=256, num_updates=7800, lr=0.00040032, gnorm=3.14, clip=0, loss_scale=64, train_wall=171, wall=13789
2022-07-10 18:36:59 | INFO | train | epoch 007 | loss 8.173 | nll_loss 3.718 | mask_ins 1.791 | word_ins_ml 5.228 | word_reposition 1.154 | ppl 288.59 | wps 11552.2 | ups 0.56 | wpb 20519.9 | bsz 255.8 | num_updates 7841 | lr 0.000399272 | gnorm 3.791 | clip 0.4 | loss_scale 162 | train_wall 1938 | wall 13859
2022-07-10 18:37:25 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.285 | nll_loss 6.548 | mask_ins 1.881 | word_ins_ml 7.811 | word_reposition 1.593 | ppl 2495.14 | wps 37889.7 | wpb 2367.6 | bsz 32 | num_updates 7841 | best_loss 11.285
2022-07-10 18:37:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 7 @ 7841 updates, score 11.285) (writing took 9.975163461640477 seconds)
2022-07-10 18:39:18 | INFO | train_inner | epoch 008:     59 / 1122 loss=7.795, nll_loss=3.388, mask_ins=1.808, word_ins_ml=4.93, word_reposition=1.056, ppl=222.03, wps=9777, ups=0.48, wpb=20376.9, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=3.318, clip=0, loss_scale=64, train_wall=171, wall=13997
2022-07-10 18:42:13 | INFO | train_inner | epoch 008:    159 / 1122 loss=7.683, nll_loss=3.299, mask_ins=1.79, word_ins_ml=4.851, word_reposition=1.042, ppl=205.5, wps=11707.2, ups=0.57, wpb=20527.7, bsz=256, num_updates=8000, lr=0.000395285, gnorm=3.143, clip=0, loss_scale=64, train_wall=175, wall=14173
2022-07-10 18:45:05 | INFO | train_inner | epoch 008:    259 / 1122 loss=7.657, nll_loss=3.296, mask_ins=1.779, word_ins_ml=4.847, word_reposition=1.031, ppl=201.85, wps=11876, ups=0.58, wpb=20472.6, bsz=256, num_updates=8100, lr=0.000392837, gnorm=3.129, clip=0, loss_scale=75, train_wall=172, wall=14345
2022-07-10 18:48:00 | INFO | train_inner | epoch 008:    359 / 1122 loss=7.614, nll_loss=3.259, mask_ins=1.782, word_ins_ml=4.814, word_reposition=1.018, ppl=195.92, wps=11799.2, ups=0.57, wpb=20571.1, bsz=256, num_updates=8200, lr=0.000390434, gnorm=3.147, clip=0, loss_scale=128, train_wall=174, wall=14519
2022-07-10 18:50:53 | INFO | train_inner | epoch 008:    459 / 1122 loss=7.554, nll_loss=3.209, mask_ins=1.779, word_ins_ml=4.768, word_reposition=1.007, ppl=187.98, wps=11862.9, ups=0.58, wpb=20549, bsz=256, num_updates=8300, lr=0.000388075, gnorm=3.015, clip=0, loss_scale=128, train_wall=172, wall=14692
2022-07-10 18:53:48 | INFO | train_inner | epoch 008:    559 / 1122 loss=7.506, nll_loss=3.158, mask_ins=1.785, word_ins_ml=4.722, word_reposition=0.998, ppl=181.81, wps=11772.8, ups=0.57, wpb=20547.4, bsz=256, num_updates=8400, lr=0.000385758, gnorm=3.022, clip=0, loss_scale=128, train_wall=174, wall=14867
2022-07-10 18:56:40 | INFO | train_inner | epoch 008:    659 / 1122 loss=7.461, nll_loss=3.144, mask_ins=1.763, word_ins_ml=4.709, word_reposition=0.989, ppl=176.23, wps=11937.6, ups=0.58, wpb=20620.8, bsz=256, num_updates=8500, lr=0.000383482, gnorm=2.924, clip=0, loss_scale=128, train_wall=172, wall=15040
2022-07-10 18:59:33 | INFO | train_inner | epoch 008:    759 / 1122 loss=7.402, nll_loss=3.091, mask_ins=1.763, word_ins_ml=4.661, word_reposition=0.978, ppl=169.1, wps=11857.8, ups=0.58, wpb=20487.2, bsz=256, num_updates=8600, lr=0.000381246, gnorm=3.003, clip=0, loss_scale=134, train_wall=172, wall=15213
2022-07-10 19:02:26 | INFO | train_inner | epoch 008:    859 / 1122 loss=7.395, nll_loss=3.09, mask_ins=1.76, word_ins_ml=4.659, word_reposition=0.976, ppl=168.28, wps=11915.1, ups=0.58, wpb=20610.7, bsz=256, num_updates=8700, lr=0.000379049, gnorm=2.991, clip=0, loss_scale=256, train_wall=172, wall=15386
2022-07-10 19:05:22 | INFO | train_inner | epoch 008:    959 / 1122 loss=7.299, nll_loss=3.014, mask_ins=1.746, word_ins_ml=4.591, word_reposition=0.962, ppl=157.51, wps=11698.4, ups=0.57, wpb=20548.3, bsz=256, num_updates=8800, lr=0.000376889, gnorm=2.863, clip=0, loss_scale=256, train_wall=175, wall=15561
2022-07-10 19:08:16 | INFO | train_inner | epoch 008:   1059 / 1122 loss=7.309, nll_loss=3.038, mask_ins=1.74, word_ins_ml=4.611, word_reposition=0.958, ppl=158.61, wps=11662.2, ups=0.57, wpb=20336.3, bsz=256, num_updates=8900, lr=0.000374766, gnorm=2.895, clip=0, loss_scale=256, train_wall=174, wall=15736
2022-07-10 19:10:05 | INFO | train | epoch 008 | loss 7.498 | nll_loss 3.169 | mask_ins 1.769 | word_ins_ml 4.732 | word_reposition 0.997 | ppl 180.73 | wps 11593.6 | ups 0.56 | wpb 20521.1 | bsz 255.8 | num_updates 8963 | lr 0.000373446 | gnorm 3.068 | clip 0 | loss_scale 156 | train_wall 1940 | wall 15845
2022-07-10 19:10:31 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.911 | nll_loss 6.25 | mask_ins 1.831 | word_ins_ml 7.548 | word_reposition 1.533 | ppl 1925.53 | wps 37922.3 | wpb 2367.6 | bsz 32 | num_updates 8963 | best_loss 10.911
2022-07-10 19:10:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 8 @ 8963 updates, score 10.911) (writing took 9.630661967210472 seconds)
2022-07-10 19:11:46 | INFO | train_inner | epoch 009:     37 / 1122 loss=7.34, nll_loss=3.08, mask_ins=1.736, word_ins_ml=4.649, word_reposition=0.955, ppl=162.03, wps=9694.2, ups=0.48, wpb=20309, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=3.498, clip=0, loss_scale=256, train_wall=173, wall=15945
2022-07-10 19:14:40 | INFO | train_inner | epoch 009:    137 / 1122 loss=7.23, nll_loss=2.974, mask_ins=1.724, word_ins_ml=4.554, word_reposition=0.952, ppl=150.16, wps=11864.1, ups=0.57, wpb=20646.2, bsz=256, num_updates=9100, lr=0.000370625, gnorm=2.788, clip=0, loss_scale=256, train_wall=173, wall=16119
2022-07-10 19:17:33 | INFO | train_inner | epoch 009:    237 / 1122 loss=7.196, nll_loss=2.936, mask_ins=1.742, word_ins_ml=4.519, word_reposition=0.935, ppl=146.67, wps=11799.8, ups=0.58, wpb=20475.7, bsz=256, num_updates=9200, lr=0.000368605, gnorm=2.79, clip=0, loss_scale=494, train_wall=173, wall=16293
2022-07-10 19:20:25 | INFO | train_inner | epoch 009:    337 / 1122 loss=7.12, nll_loss=2.898, mask_ins=1.708, word_ins_ml=4.485, word_reposition=0.927, ppl=139.11, wps=11880.5, ups=0.58, wpb=20454, bsz=256, num_updates=9300, lr=0.000366618, gnorm=2.826, clip=0, loss_scale=512, train_wall=171, wall=16465
2022-07-10 19:23:17 | INFO | train_inner | epoch 009:    437 / 1122 loss=7.073, nll_loss=2.87, mask_ins=1.701, word_ins_ml=4.46, word_reposition=0.912, ppl=134.61, wps=11931.9, ups=0.58, wpb=20454.8, bsz=256, num_updates=9400, lr=0.000364662, gnorm=2.96, clip=1, loss_scale=512, train_wall=171, wall=16636
2022-07-10 19:24:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-10 19:26:11 | INFO | train_inner | epoch 009:    538 / 1122 loss=7.102, nll_loss=2.905, mask_ins=1.692, word_ins_ml=4.49, word_reposition=0.92, ppl=137.41, wps=11801.9, ups=0.57, wpb=20538.6, bsz=256, num_updates=9500, lr=0.000362738, gnorm=3.537, clip=1, loss_scale=355, train_wall=173, wall=16810
2022-07-10 19:29:03 | INFO | train_inner | epoch 009:    638 / 1122 loss=7.065, nll_loss=2.885, mask_ins=1.681, word_ins_ml=4.472, word_reposition=0.912, ppl=133.89, wps=11971.6, ups=0.58, wpb=20584.1, bsz=256, num_updates=9600, lr=0.000360844, gnorm=3.315, clip=1, loss_scale=256, train_wall=171, wall=16982
2022-07-10 19:31:55 | INFO | train_inner | epoch 009:    738 / 1122 loss=6.967, nll_loss=2.81, mask_ins=1.658, word_ins_ml=4.404, word_reposition=0.905, ppl=125.09, wps=11893.6, ups=0.58, wpb=20545.1, bsz=256, num_updates=9700, lr=0.000358979, gnorm=2.852, clip=0, loss_scale=256, train_wall=172, wall=17155
2022-07-10 19:34:48 | INFO | train_inner | epoch 009:    838 / 1122 loss=6.95, nll_loss=2.849, mask_ins=1.607, word_ins_ml=4.438, word_reposition=0.905, ppl=123.65, wps=11842.8, ups=0.58, wpb=20492.9, bsz=256, num_updates=9800, lr=0.000357143, gnorm=2.892, clip=0, loss_scale=256, train_wall=172, wall=17328
2022-07-10 19:37:42 | INFO | train_inner | epoch 009:    938 / 1122 loss=6.813, nll_loss=2.781, mask_ins=1.54, word_ins_ml=4.377, word_reposition=0.897, ppl=112.47, wps=11871.8, ups=0.58, wpb=20599.4, bsz=256, num_updates=9900, lr=0.000355335, gnorm=2.89, clip=0, loss_scale=256, train_wall=173, wall=17501
2022-07-10 19:40:35 | INFO | train_inner | epoch 009:   1038 / 1122 loss=6.781, nll_loss=2.803, mask_ins=1.486, word_ins_ml=4.395, word_reposition=0.899, ppl=109.96, wps=11822, ups=0.58, wpb=20488.6, bsz=256, num_updates=10000, lr=0.000353553, gnorm=2.793, clip=0, loss_scale=384, train_wall=172, wall=17675
2022-07-10 19:43:01 | INFO | train | epoch 009 | loss 7.012 | nll_loss 2.869 | mask_ins 1.639 | word_ins_ml 4.457 | word_reposition 0.916 | ppl 129.1 | wps 11641.6 | ups 0.57 | wpb 20519.4 | bsz 255.8 | num_updates 10084 | lr 0.000352078 | gnorm 2.948 | clip 0.3 | loss_scale 362 | train_wall 1931 | wall 17821
2022-07-10 19:43:27 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.703 | nll_loss 6.188 | mask_ins 1.701 | word_ins_ml 7.504 | word_reposition 1.497 | ppl 1666.57 | wps 37964.3 | wpb 2367.6 | bsz 32 | num_updates 10084 | best_loss 10.703
2022-07-10 19:43:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 9 @ 10084 updates, score 10.703) (writing took 10.35188002884388 seconds)
2022-07-10 19:44:06 | INFO | train_inner | epoch 010:     16 / 1122 loss=6.698, nll_loss=2.782, mask_ins=1.422, word_ins_ml=4.376, word_reposition=0.9, ppl=103.84, wps=9743.9, ups=0.48, wpb=20499.8, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=2.767, clip=0, loss_scale=512, train_wall=173, wall=17885
2022-07-10 19:46:58 | INFO | train_inner | epoch 010:    116 / 1122 loss=6.569, nll_loss=2.692, mask_ins=1.382, word_ins_ml=4.297, word_reposition=0.89, ppl=94.95, wps=11962.3, ups=0.58, wpb=20579.8, bsz=256, num_updates=10200, lr=0.00035007, gnorm=2.784, clip=0, loss_scale=512, train_wall=171, wall=18057
2022-07-10 19:49:50 | INFO | train_inner | epoch 010:    216 / 1122 loss=6.545, nll_loss=2.72, mask_ins=1.352, word_ins_ml=4.32, word_reposition=0.873, ppl=93.41, wps=11901.5, ups=0.58, wpb=20526, bsz=256, num_updates=10300, lr=0.000348367, gnorm=2.657, clip=0, loss_scale=512, train_wall=172, wall=18230
2022-07-10 19:52:42 | INFO | train_inner | epoch 010:    316 / 1122 loss=6.504, nll_loss=2.707, mask_ins=1.32, word_ins_ml=4.308, word_reposition=0.876, ppl=90.73, wps=11952.3, ups=0.58, wpb=20577.8, bsz=256, num_updates=10400, lr=0.000346688, gnorm=2.451, clip=0, loss_scale=512, train_wall=171, wall=18402
2022-07-10 19:55:34 | INFO | train_inner | epoch 010:    416 / 1122 loss=6.46, nll_loss=2.696, mask_ins=1.288, word_ins_ml=4.298, word_reposition=0.874, ppl=88.01, wps=11997.5, ups=0.58, wpb=20553, bsz=256, num_updates=10500, lr=0.000345033, gnorm=2.498, clip=0, loss_scale=707, train_wall=171, wall=18573
2022-07-10 19:58:26 | INFO | train_inner | epoch 010:    516 / 1122 loss=6.406, nll_loss=2.651, mask_ins=1.281, word_ins_ml=4.257, word_reposition=0.868, ppl=84.78, wps=11913.4, ups=0.58, wpb=20520.6, bsz=256, num_updates=10600, lr=0.000343401, gnorm=2.578, clip=0, loss_scale=1024, train_wall=171, wall=18745
2022-07-10 20:01:20 | INFO | train_inner | epoch 010:    616 / 1122 loss=6.404, nll_loss=2.666, mask_ins=1.251, word_ins_ml=4.27, word_reposition=0.882, ppl=84.68, wps=11913.2, ups=0.58, wpb=20687.1, bsz=256, num_updates=10700, lr=0.000341793, gnorm=2.598, clip=0, loss_scale=1024, train_wall=173, wall=18919
2022-07-10 20:04:13 | INFO | train_inner | epoch 010:    716 / 1122 loss=6.354, nll_loss=2.645, mask_ins=1.233, word_ins_ml=4.251, word_reposition=0.869, ppl=81.77, wps=11778.4, ups=0.58, wpb=20419.2, bsz=256, num_updates=10800, lr=0.000340207, gnorm=2.479, clip=0, loss_scale=1024, train_wall=173, wall=19092
2022-07-10 20:07:07 | INFO | train_inner | epoch 010:    816 / 1122 loss=6.284, nll_loss=2.608, mask_ins=1.212, word_ins_ml=4.218, word_reposition=0.854, ppl=77.91, wps=11835.1, ups=0.58, wpb=20545.3, bsz=256, num_updates=10900, lr=0.000338643, gnorm=2.506, clip=0, loss_scale=1024, train_wall=173, wall=19266
2022-07-10 20:10:01 | INFO | train_inner | epoch 010:    916 / 1122 loss=6.266, nll_loss=2.604, mask_ins=1.19, word_ins_ml=4.214, word_reposition=0.863, ppl=76.97, wps=11790.9, ups=0.57, wpb=20527, bsz=256, num_updates=11000, lr=0.0003371, gnorm=2.47, clip=0, loss_scale=1290, train_wall=173, wall=19440
2022-07-10 20:12:54 | INFO | train_inner | epoch 010:   1016 / 1122 loss=6.262, nll_loss=2.623, mask_ins=1.177, word_ins_ml=4.23, word_reposition=0.855, ppl=76.76, wps=11777.9, ups=0.58, wpb=20469.8, bsz=256, num_updates=11100, lr=0.000335578, gnorm=2.606, clip=0, loss_scale=2048, train_wall=173, wall=19614
2022-07-10 20:15:47 | INFO | train_inner | epoch 010:   1116 / 1122 loss=6.176, nll_loss=2.548, mask_ins=1.157, word_ins_ml=4.163, word_reposition=0.855, ppl=72.3, wps=11850.3, ups=0.58, wpb=20461.5, bsz=256, num_updates=11200, lr=0.000334077, gnorm=2.414, clip=0, loss_scale=2048, train_wall=172, wall=19787
2022-07-10 20:15:57 | INFO | train | epoch 010 | loss 6.39 | nll_loss 2.653 | mask_ins 1.261 | word_ins_ml 4.259 | word_reposition 0.87 | ppl 83.84 | wps 11653.2 | ups 0.57 | wpb 20521.4 | bsz 255.8 | num_updates 11206 | lr 0.000333987 | gnorm 2.565 | clip 0 | loss_scale 1063 | train_wall 1930 | wall 19796
2022-07-10 20:16:23 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.409 | nll_loss 5.99 | mask_ins 1.633 | word_ins_ml 7.316 | word_reposition 1.46 | ppl 1359.71 | wps 37914.2 | wpb 2367.6 | bsz 32 | num_updates 11206 | best_loss 10.409
2022-07-10 20:16:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 10 @ 11206 updates, score 10.409) (writing took 10.356982952915132 seconds)
2022-07-10 20:19:16 | INFO | train_inner | epoch 011:     94 / 1122 loss=6.228, nll_loss=2.609, mask_ins=1.152, word_ins_ml=4.218, word_reposition=0.857, ppl=74.94, wps=9764.7, ups=0.48, wpb=20410.5, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=2.572, clip=0, loss_scale=2048, train_wall=171, wall=19996
2022-07-10 20:22:09 | INFO | train_inner | epoch 011:    194 / 1122 loss=6.08, nll_loss=2.486, mask_ins=1.132, word_ins_ml=4.107, word_reposition=0.84, ppl=67.63, wps=11851.3, ups=0.58, wpb=20464.3, bsz=256, num_updates=11400, lr=0.000331133, gnorm=2.34, clip=0, loss_scale=2048, train_wall=172, wall=20168
2022-07-10 20:25:01 | INFO | train_inner | epoch 011:    294 / 1122 loss=6.138, nll_loss=2.556, mask_ins=1.123, word_ins_ml=4.169, word_reposition=0.846, ppl=70.44, wps=11942.5, ups=0.58, wpb=20539.4, bsz=256, num_updates=11500, lr=0.00032969, gnorm=2.438, clip=0, loss_scale=2335, train_wall=171, wall=20340
2022-07-10 20:25:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-10 20:27:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 20:27:57 | INFO | train_inner | epoch 011:    396 / 1122 loss=6.095, nll_loss=2.514, mask_ins=1.121, word_ins_ml=4.131, word_reposition=0.842, ppl=68.36, wps=11690.7, ups=0.57, wpb=20577.2, bsz=256, num_updates=11600, lr=0.000328266, gnorm=2.386, clip=0, loss_scale=2018, train_wall=175, wall=20516
2022-07-10 20:30:49 | INFO | train_inner | epoch 011:    496 / 1122 loss=6.071, nll_loss=2.522, mask_ins=1.095, word_ins_ml=4.138, word_reposition=0.838, ppl=67.24, wps=11983, ups=0.58, wpb=20618.7, bsz=256, num_updates=11700, lr=0.00032686, gnorm=2.583, clip=0, loss_scale=1024, train_wall=171, wall=20688
2022-07-10 20:33:42 | INFO | train_inner | epoch 011:    596 / 1122 loss=6.104, nll_loss=2.554, mask_ins=1.093, word_ins_ml=4.166, word_reposition=0.846, ppl=68.8, wps=11879.4, ups=0.58, wpb=20560, bsz=256, num_updates=11800, lr=0.000325472, gnorm=2.359, clip=0, loss_scale=1024, train_wall=172, wall=20861
2022-07-10 20:36:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 20:36:38 | INFO | train_inner | epoch 011:    697 / 1122 loss=6.042, nll_loss=2.514, mask_ins=1.078, word_ins_ml=4.13, word_reposition=0.835, ppl=65.91, wps=11680.8, ups=0.57, wpb=20537.8, bsz=256, num_updates=11900, lr=0.000324102, gnorm=2.358, clip=0, loss_scale=912, train_wall=175, wall=21037
2022-07-10 20:39:32 | INFO | train_inner | epoch 011:    797 / 1122 loss=6.042, nll_loss=2.516, mask_ins=1.073, word_ins_ml=4.131, word_reposition=0.838, ppl=65.88, wps=11649.2, ups=0.57, wpb=20332.6, bsz=256, num_updates=12000, lr=0.000322749, gnorm=2.325, clip=0, loss_scale=512, train_wall=174, wall=21212
2022-07-10 20:42:26 | INFO | train_inner | epoch 011:    897 / 1122 loss=5.998, nll_loss=2.495, mask_ins=1.055, word_ins_ml=4.112, word_reposition=0.831, ppl=63.92, wps=11875.4, ups=0.58, wpb=20602.1, bsz=256, num_updates=12100, lr=0.000321412, gnorm=2.201, clip=0, loss_scale=512, train_wall=173, wall=21385
2022-07-10 20:45:19 | INFO | train_inner | epoch 011:    997 / 1122 loss=6.002, nll_loss=2.486, mask_ins=1.063, word_ins_ml=4.103, word_reposition=0.835, ppl=64.07, wps=11822.7, ups=0.58, wpb=20503.8, bsz=256, num_updates=12200, lr=0.000320092, gnorm=2.318, clip=0, loss_scale=512, train_wall=173, wall=21559
2022-07-10 20:48:12 | INFO | train_inner | epoch 011:   1097 / 1122 loss=5.962, nll_loss=2.469, mask_ins=1.047, word_ins_ml=4.087, word_reposition=0.828, ppl=62.36, wps=11886.8, ups=0.58, wpb=20577.4, bsz=256, num_updates=12300, lr=0.000318788, gnorm=2.237, clip=0, loss_scale=512, train_wall=172, wall=21732
2022-07-10 20:48:55 | INFO | train | epoch 011 | loss 6.065 | nll_loss 2.518 | mask_ins 1.092 | word_ins_ml 4.134 | word_reposition 0.839 | ppl 66.97 | wps 11610.2 | ups 0.57 | wpb 20522.7 | bsz 255.8 | num_updates 12325 | lr 0.000318465 | gnorm 2.369 | clip 0 | loss_scale 1204 | train_wall 1932 | wall 21774
2022-07-10 20:49:21 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.312 | nll_loss 5.952 | mask_ins 1.623 | word_ins_ml 7.281 | word_reposition 1.408 | ppl 1271.06 | wps 37822.4 | wpb 2367.6 | bsz 32 | num_updates 12325 | best_loss 10.312
2022-07-10 20:49:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 11 @ 12325 updates, score 10.312) (writing took 9.748352606780827 seconds)
2022-07-10 20:51:41 | INFO | train_inner | epoch 012:     75 / 1122 loss=6.008, nll_loss=2.513, mask_ins=1.047, word_ins_ml=4.127, word_reposition=0.834, ppl=64.37, wps=9782.9, ups=0.48, wpb=20449.1, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=2.484, clip=0, loss_scale=563, train_wall=172, wall=21941
2022-07-10 20:52:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 20:54:35 | INFO | train_inner | epoch 012:    176 / 1122 loss=5.892, nll_loss=2.421, mask_ins=1.024, word_ins_ml=4.045, word_reposition=0.824, ppl=59.39, wps=11780.1, ups=0.57, wpb=20492.5, bsz=256, num_updates=12500, lr=0.000316228, gnorm=2.464, clip=0, loss_scale=700, train_wall=173, wall=22115
2022-07-10 20:57:29 | INFO | train_inner | epoch 012:    276 / 1122 loss=5.937, nll_loss=2.47, mask_ins=1.024, word_ins_ml=4.088, word_reposition=0.826, ppl=61.28, wps=11790.6, ups=0.57, wpb=20515.5, bsz=256, num_updates=12600, lr=0.00031497, gnorm=2.239, clip=0, loss_scale=512, train_wall=173, wall=22289
2022-07-10 21:00:23 | INFO | train_inner | epoch 012:    376 / 1122 loss=5.896, nll_loss=2.442, mask_ins=1.016, word_ins_ml=4.062, word_reposition=0.818, ppl=59.57, wps=11848.2, ups=0.57, wpb=20609.7, bsz=256, num_updates=12700, lr=0.000313728, gnorm=2.291, clip=0, loss_scale=512, train_wall=173, wall=22463
2022-07-10 21:03:17 | INFO | train_inner | epoch 012:    476 / 1122 loss=5.904, nll_loss=2.444, mask_ins=1.016, word_ins_ml=4.065, word_reposition=0.823, ppl=59.87, wps=11794.5, ups=0.58, wpb=20489.6, bsz=256, num_updates=12800, lr=0.0003125, gnorm=2.261, clip=0, loss_scale=512, train_wall=173, wall=22636
2022-07-10 21:06:11 | INFO | train_inner | epoch 012:    576 / 1122 loss=5.884, nll_loss=2.426, mask_ins=1.011, word_ins_ml=4.047, word_reposition=0.826, ppl=59.04, wps=11859.8, ups=0.58, wpb=20594.2, bsz=256, num_updates=12900, lr=0.000311286, gnorm=2.29, clip=0, loss_scale=512, train_wall=173, wall=22810
2022-07-10 21:09:05 | INFO | train_inner | epoch 012:    676 / 1122 loss=5.848, nll_loss=2.4, mask_ins=1.008, word_ins_ml=4.025, word_reposition=0.815, ppl=57.59, wps=11677, ups=0.57, wpb=20403.4, bsz=256, num_updates=13000, lr=0.000310087, gnorm=2.307, clip=0, loss_scale=778, train_wall=174, wall=22985
2022-07-10 21:11:59 | INFO | train_inner | epoch 012:    776 / 1122 loss=5.839, nll_loss=2.412, mask_ins=0.996, word_ins_ml=4.034, word_reposition=0.809, ppl=57.25, wps=11872.4, ups=0.58, wpb=20588.5, bsz=256, num_updates=13100, lr=0.000308901, gnorm=2.211, clip=0, loss_scale=1024, train_wall=173, wall=23158
2022-07-10 21:14:52 | INFO | train_inner | epoch 012:    876 / 1122 loss=5.866, nll_loss=2.422, mask_ins=1.003, word_ins_ml=4.043, word_reposition=0.82, ppl=58.32, wps=11908.4, ups=0.58, wpb=20566.9, bsz=256, num_updates=13200, lr=0.000307729, gnorm=2.206, clip=0, loss_scale=1024, train_wall=172, wall=23331
2022-07-10 21:17:46 | INFO | train_inner | epoch 012:    976 / 1122 loss=5.807, nll_loss=2.379, mask_ins=0.988, word_ins_ml=4.005, word_reposition=0.814, ppl=55.97, wps=11795.3, ups=0.57, wpb=20524.5, bsz=256, num_updates=13300, lr=0.00030657, gnorm=2.227, clip=0, loss_scale=1024, train_wall=173, wall=23505
2022-07-10 21:20:40 | INFO | train_inner | epoch 012:   1076 / 1122 loss=5.799, nll_loss=2.384, mask_ins=0.983, word_ins_ml=4.009, word_reposition=0.807, ppl=55.68, wps=11753, ups=0.57, wpb=20485.2, bsz=256, num_updates=13400, lr=0.000305424, gnorm=2.234, clip=0, loss_scale=1024, train_wall=173, wall=23679
2022-07-10 21:22:00 | INFO | train | epoch 012 | loss 5.875 | nll_loss 2.426 | mask_ins 1.008 | word_ins_ml 4.048 | word_reposition 0.819 | ppl 58.68 | wps 11591.2 | ups 0.56 | wpb 20520.5 | bsz 255.8 | num_updates 13446 | lr 0.000304901 | gnorm 2.289 | clip 0 | loss_scale 761 | train_wall 1939 | wall 23759
2022-07-10 21:22:26 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.296 | nll_loss 5.888 | mask_ins 1.587 | word_ins_ml 7.22 | word_reposition 1.489 | ppl 1256.8 | wps 37791.1 | wpb 2367.6 | bsz 32 | num_updates 13446 | best_loss 10.296
2022-07-10 21:22:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 12 @ 13446 updates, score 10.296) (writing took 10.267843255773187 seconds)
2022-07-10 21:24:10 | INFO | train_inner | epoch 013:     54 / 1122 loss=5.798, nll_loss=2.381, mask_ins=0.982, word_ins_ml=4.006, word_reposition=0.81, ppl=55.65, wps=9699.2, ups=0.48, wpb=20386.8, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=2.38, clip=0, loss_scale=1434, train_wall=173, wall=23889
2022-07-10 21:25:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 21:27:05 | INFO | train_inner | epoch 013:    155 / 1122 loss=5.774, nll_loss=2.366, mask_ins=0.976, word_ins_ml=3.992, word_reposition=0.806, ppl=54.72, wps=11753.2, ups=0.57, wpb=20578.3, bsz=256, num_updates=13600, lr=0.00030317, gnorm=2.097, clip=0, loss_scale=1541, train_wall=174, wall=24065
2022-07-10 21:29:59 | INFO | train_inner | epoch 013:    255 / 1122 loss=5.807, nll_loss=2.408, mask_ins=0.976, word_ins_ml=4.029, word_reposition=0.801, ppl=55.97, wps=11772.2, ups=0.58, wpb=20456.7, bsz=256, num_updates=13700, lr=0.000302061, gnorm=2.434, clip=0, loss_scale=1024, train_wall=173, wall=24238
2022-07-10 21:32:53 | INFO | train_inner | epoch 013:    355 / 1122 loss=5.789, nll_loss=2.398, mask_ins=0.961, word_ins_ml=4.02, word_reposition=0.808, ppl=55.29, wps=11814.9, ups=0.58, wpb=20524.6, bsz=256, num_updates=13800, lr=0.000300965, gnorm=2.2, clip=0, loss_scale=1024, train_wall=173, wall=24412
2022-07-10 21:35:45 | INFO | train_inner | epoch 013:    455 / 1122 loss=5.785, nll_loss=2.391, mask_ins=0.967, word_ins_ml=4.013, word_reposition=0.804, ppl=55.13, wps=11974.8, ups=0.58, wpb=20610.1, bsz=256, num_updates=13900, lr=0.00029988, gnorm=2.394, clip=0, loss_scale=1024, train_wall=171, wall=24584
2022-07-10 21:38:39 | INFO | train_inner | epoch 013:    555 / 1122 loss=5.769, nll_loss=2.381, mask_ins=0.963, word_ins_ml=4.003, word_reposition=0.803, ppl=54.53, wps=11801.9, ups=0.58, wpb=20514.5, bsz=256, num_updates=14000, lr=0.000298807, gnorm=2.185, clip=0, loss_scale=1024, train_wall=173, wall=24758
2022-07-10 21:41:33 | INFO | train_inner | epoch 013:    655 / 1122 loss=5.75, nll_loss=2.367, mask_ins=0.96, word_ins_ml=3.992, word_reposition=0.798, ppl=53.82, wps=11724.8, ups=0.57, wpb=20474.8, bsz=256, num_updates=14100, lr=0.000297746, gnorm=2.24, clip=0, loss_scale=1413, train_wall=174, wall=24933
2022-07-10 21:44:28 | INFO | train_inner | epoch 013:    755 / 1122 loss=5.727, nll_loss=2.359, mask_ins=0.942, word_ins_ml=3.983, word_reposition=0.801, ppl=52.95, wps=11772.5, ups=0.57, wpb=20535, bsz=256, num_updates=14200, lr=0.000296695, gnorm=2.17, clip=0, loss_scale=2048, train_wall=174, wall=25107
2022-07-10 21:47:22 | INFO | train_inner | epoch 013:    855 / 1122 loss=5.726, nll_loss=2.344, mask_ins=0.954, word_ins_ml=3.97, word_reposition=0.803, ppl=52.92, wps=11786.1, ups=0.57, wpb=20503.4, bsz=256, num_updates=14300, lr=0.000295656, gnorm=2.138, clip=0, loss_scale=2048, train_wall=173, wall=25281
2022-07-10 21:50:14 | INFO | train_inner | epoch 013:    955 / 1122 loss=5.711, nll_loss=2.344, mask_ins=0.947, word_ins_ml=3.97, word_reposition=0.794, ppl=52.37, wps=11854.5, ups=0.58, wpb=20475.9, bsz=256, num_updates=14400, lr=0.000294628, gnorm=2.194, clip=0, loss_scale=2048, train_wall=172, wall=25454
2022-07-10 21:51:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 21:52:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 21:53:12 | INFO | train_inner | epoch 013:   1057 / 1122 loss=5.709, nll_loss=2.339, mask_ins=0.94, word_ins_ml=3.965, word_reposition=0.804, ppl=52.31, wps=11646.6, ups=0.56, wpb=20641.3, bsz=256, num_updates=14500, lr=0.00029361, gnorm=2.629, clip=0, loss_scale=1416, train_wall=176, wall=25631
2022-07-10 21:55:04 | INFO | train | epoch 013 | loss 5.755 | nll_loss 2.37 | mask_ins 0.959 | word_ins_ml 3.994 | word_reposition 0.802 | ppl 54.01 | wps 11568.3 | ups 0.56 | wpb 20518 | bsz 255.8 | num_updates 14565 | lr 0.000292954 | gnorm 2.267 | clip 0 | loss_scale 1421 | train_wall 1939 | wall 25744
2022-07-10 21:55:30 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.051 | nll_loss 5.714 | mask_ins 1.559 | word_ins_ml 7.057 | word_reposition 1.435 | ppl 1060.71 | wps 37914.9 | wpb 2367.6 | bsz 32 | num_updates 14565 | best_loss 10.051
2022-07-10 21:55:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 13 @ 14565 updates, score 10.051) (writing took 10.780922043137252 seconds)
2022-07-10 21:56:42 | INFO | train_inner | epoch 014:     35 / 1122 loss=5.714, nll_loss=2.352, mask_ins=0.935, word_ins_ml=3.976, word_reposition=0.803, ppl=52.49, wps=9696.3, ups=0.47, wpb=20428.6, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=2.277, clip=0, loss_scale=512, train_wall=173, wall=25842
2022-07-10 21:59:36 | INFO | train_inner | epoch 014:    135 / 1122 loss=5.693, nll_loss=2.315, mask_ins=0.947, word_ins_ml=3.944, word_reposition=0.802, ppl=51.74, wps=11798.1, ups=0.58, wpb=20472.7, bsz=256, num_updates=14700, lr=0.000291606, gnorm=2.182, clip=0, loss_scale=512, train_wall=173, wall=26015
2022-07-10 22:02:30 | INFO | train_inner | epoch 014:    235 / 1122 loss=5.673, nll_loss=2.319, mask_ins=0.935, word_ins_ml=3.947, word_reposition=0.792, ppl=51.01, wps=11801.4, ups=0.57, wpb=20537.2, bsz=256, num_updates=14800, lr=0.000290619, gnorm=2.201, clip=0, loss_scale=512, train_wall=173, wall=26189
2022-07-10 22:05:24 | INFO | train_inner | epoch 014:    335 / 1122 loss=5.677, nll_loss=2.327, mask_ins=0.929, word_ins_ml=3.954, word_reposition=0.794, ppl=51.15, wps=11775.9, ups=0.57, wpb=20564, bsz=256, num_updates=14900, lr=0.000289642, gnorm=2.613, clip=0, loss_scale=512, train_wall=174, wall=26364
2022-07-10 22:08:18 | INFO | train_inner | epoch 014:    435 / 1122 loss=5.668, nll_loss=2.312, mask_ins=0.934, word_ins_ml=3.94, word_reposition=0.794, ppl=50.84, wps=11793.6, ups=0.57, wpb=20517.6, bsz=256, num_updates=15000, lr=0.000288675, gnorm=2.262, clip=0, loss_scale=512, train_wall=173, wall=26538
2022-07-10 22:11:12 | INFO | train_inner | epoch 014:    535 / 1122 loss=5.614, nll_loss=2.273, mask_ins=0.92, word_ins_ml=3.905, word_reposition=0.79, ppl=48.98, wps=11787.3, ups=0.58, wpb=20487.2, bsz=256, num_updates=15100, lr=0.000287718, gnorm=2.31, clip=0, loss_scale=1014, train_wall=173, wall=26712
2022-07-10 22:14:05 | INFO | train_inner | epoch 014:    635 / 1122 loss=5.598, nll_loss=2.264, mask_ins=0.913, word_ins_ml=3.897, word_reposition=0.787, ppl=48.42, wps=11846.4, ups=0.58, wpb=20528.4, bsz=256, num_updates=15200, lr=0.00028677, gnorm=2.05, clip=0, loss_scale=1024, train_wall=173, wall=26885
2022-07-10 22:16:59 | INFO | train_inner | epoch 014:    735 / 1122 loss=5.626, nll_loss=2.304, mask_ins=0.907, word_ins_ml=3.932, word_reposition=0.787, ppl=49.39, wps=11862, ups=0.57, wpb=20634.6, bsz=256, num_updates=15300, lr=0.000285831, gnorm=2.065, clip=0, loss_scale=1024, train_wall=173, wall=27059
2022-07-10 22:19:53 | INFO | train_inner | epoch 014:    835 / 1122 loss=5.678, nll_loss=2.346, mask_ins=0.92, word_ins_ml=3.969, word_reposition=0.789, ppl=51.19, wps=11778.9, ups=0.58, wpb=20418.8, bsz=256, num_updates=15400, lr=0.000284901, gnorm=2.056, clip=0, loss_scale=1024, train_wall=172, wall=27232
2022-07-10 22:22:45 | INFO | train_inner | epoch 014:    935 / 1122 loss=5.658, nll_loss=2.323, mask_ins=0.913, word_ins_ml=3.948, word_reposition=0.796, ppl=50.48, wps=11904.4, ups=0.58, wpb=20504.7, bsz=256, num_updates=15500, lr=0.000283981, gnorm=2.144, clip=0, loss_scale=1024, train_wall=171, wall=27404
2022-07-10 22:25:38 | INFO | train_inner | epoch 014:   1035 / 1122 loss=5.618, nll_loss=2.291, mask_ins=0.911, word_ins_ml=3.919, word_reposition=0.788, ppl=49.13, wps=11875.8, ups=0.58, wpb=20540.5, bsz=256, num_updates=15600, lr=0.000283069, gnorm=2.17, clip=0, loss_scale=1905, train_wall=172, wall=27577
2022-07-10 22:28:08 | INFO | train | epoch 014 | loss 5.649 | nll_loss 2.306 | mask_ins 0.922 | word_ins_ml 3.934 | word_reposition 0.792 | ppl 50.17 | wps 11605.8 | ups 0.57 | wpb 20521.1 | bsz 255.8 | num_updates 15687 | lr 0.000282283 | gnorm 2.211 | clip 0 | loss_scale 982 | train_wall 1937 | wall 27728
2022-07-10 22:28:34 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 10.062 | nll_loss 5.747 | mask_ins 1.517 | word_ins_ml 7.095 | word_reposition 1.45 | ppl 1069.31 | wps 37968.2 | wpb 2367.6 | bsz 32 | num_updates 15687 | best_loss 10.051
2022-07-10 22:28:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 14 @ 15687 updates, score 10.062) (writing took 5.855102706700563 seconds)
2022-07-10 22:29:03 | INFO | train_inner | epoch 015:     13 / 1122 loss=5.609, nll_loss=2.28, mask_ins=0.908, word_ins_ml=3.91, word_reposition=0.79, ppl=48.8, wps=10005.2, ups=0.49, wpb=20470.5, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=2.2, clip=0, loss_scale=2048, train_wall=172, wall=27782
2022-07-10 22:31:55 | INFO | train_inner | epoch 015:    113 / 1122 loss=5.568, nll_loss=2.246, mask_ins=0.902, word_ins_ml=3.88, word_reposition=0.786, ppl=47.45, wps=11966.8, ups=0.58, wpb=20648.4, bsz=256, num_updates=15800, lr=0.000281272, gnorm=2.138, clip=0, loss_scale=2048, train_wall=172, wall=27955
2022-07-10 22:34:47 | INFO | train_inner | epoch 015:    213 / 1122 loss=5.561, nll_loss=2.25, mask_ins=0.894, word_ins_ml=3.883, word_reposition=0.785, ppl=47.21, wps=11951, ups=0.58, wpb=20568.7, bsz=256, num_updates=15900, lr=0.000280386, gnorm=1.957, clip=0, loss_scale=2048, train_wall=171, wall=28127
2022-07-10 22:37:39 | INFO | train_inner | epoch 015:    313 / 1122 loss=5.566, nll_loss=2.262, mask_ins=0.889, word_ins_ml=3.893, word_reposition=0.784, ppl=47.38, wps=11954.9, ups=0.58, wpb=20538.5, bsz=256, num_updates=16000, lr=0.000279508, gnorm=2.033, clip=0, loss_scale=2048, train_wall=171, wall=28298
2022-07-10 22:40:33 | INFO | train_inner | epoch 015:    413 / 1122 loss=5.589, nll_loss=2.264, mask_ins=0.906, word_ins_ml=3.894, word_reposition=0.79, ppl=48.15, wps=11821.2, ups=0.58, wpb=20537.2, bsz=256, num_updates=16100, lr=0.000278639, gnorm=2.012, clip=0, loss_scale=3564, train_wall=173, wall=28472
2022-07-10 22:43:26 | INFO | train_inner | epoch 015:    513 / 1122 loss=5.56, nll_loss=2.25, mask_ins=0.889, word_ins_ml=3.882, word_reposition=0.789, ppl=47.17, wps=11806.2, ups=0.58, wpb=20495.2, bsz=256, num_updates=16200, lr=0.000277778, gnorm=1.983, clip=0, loss_scale=4096, train_wall=173, wall=28646
2022-07-10 22:46:20 | INFO | train_inner | epoch 015:    613 / 1122 loss=5.563, nll_loss=2.268, mask_ins=0.885, word_ins_ml=3.898, word_reposition=0.78, ppl=47.27, wps=11801.8, ups=0.58, wpb=20515.6, bsz=256, num_updates=16300, lr=0.000276924, gnorm=1.991, clip=0, loss_scale=4096, train_wall=173, wall=28820
2022-07-10 22:49:14 | INFO | train_inner | epoch 015:    713 / 1122 loss=5.554, nll_loss=2.253, mask_ins=0.887, word_ins_ml=3.884, word_reposition=0.784, ppl=47, wps=11811.4, ups=0.58, wpb=20477.7, bsz=256, num_updates=16400, lr=0.000276079, gnorm=1.992, clip=0, loss_scale=4096, train_wall=173, wall=28993
2022-07-10 22:50:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-10 22:52:09 | INFO | train_inner | epoch 015:    814 / 1122 loss=5.563, nll_loss=2.276, mask_ins=0.881, word_ins_ml=3.904, word_reposition=0.777, ppl=47.26, wps=11713.4, ups=0.57, wpb=20545.9, bsz=256, num_updates=16500, lr=0.000275241, gnorm=2.095, clip=0, loss_scale=2920, train_wall=175, wall=29168
2022-07-10 22:54:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-10 22:55:04 | INFO | train_inner | epoch 015:    915 / 1122 loss=5.566, nll_loss=2.269, mask_ins=0.889, word_ins_ml=3.898, word_reposition=0.779, ppl=47.38, wps=11692.4, ups=0.57, wpb=20443.2, bsz=256, num_updates=16600, lr=0.000274411, gnorm=2.103, clip=0, loss_scale=1835, train_wall=174, wall=29343
2022-07-10 22:57:56 | INFO | train_inner | epoch 015:   1015 / 1122 loss=5.548, nll_loss=2.261, mask_ins=0.884, word_ins_ml=3.89, word_reposition=0.773, ppl=46.77, wps=11930, ups=0.58, wpb=20537.3, bsz=256, num_updates=16700, lr=0.000273588, gnorm=2.104, clip=0, loss_scale=1024, train_wall=171, wall=29515
2022-07-10 22:58:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-10 23:00:51 | INFO | train_inner | epoch 015:   1116 / 1122 loss=5.509, nll_loss=2.221, mask_ins=0.877, word_ins_ml=3.855, word_reposition=0.777, ppl=45.53, wps=11752.6, ups=0.57, wpb=20573.1, bsz=256, num_updates=16800, lr=0.000272772, gnorm=2.096, clip=0, loss_scale=608, train_wall=174, wall=29690
2022-07-10 23:01:01 | INFO | train | epoch 015 | loss 5.56 | nll_loss 2.257 | mask_ins 0.89 | word_ins_ml 3.888 | word_reposition 0.782 | ppl 47.18 | wps 11638.3 | ups 0.57 | wpb 20518.3 | bsz 255.8 | num_updates 16806 | lr 0.000272724 | gnorm 2.117 | clip 0.1 | loss_scale 2561 | train_wall 1932 | wall 29700
2022-07-10 23:01:27 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 10.148 | nll_loss 5.796 | mask_ins 1.553 | word_ins_ml 7.136 | word_reposition 1.458 | ppl 1134.34 | wps 37730 | wpb 2367.6 | bsz 32 | num_updates 16806 | best_loss 10.051
2022-07-10 23:01:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 15 @ 16806 updates, score 10.148) (writing took 5.824916165322065 seconds)
2022-07-10 23:02:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-10 23:02:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-10 23:04:19 | INFO | train_inner | epoch 016:     96 / 1122 loss=5.53, nll_loss=2.244, mask_ins=0.881, word_ins_ml=3.876, word_reposition=0.773, ppl=46.2, wps=9723.8, ups=0.48, wpb=20267.6, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=3.16, clip=2, loss_scale=253, train_wall=176, wall=29899
2022-07-10 23:07:13 | INFO | train_inner | epoch 016:    196 / 1122 loss=5.52, nll_loss=2.238, mask_ins=0.876, word_ins_ml=3.869, word_reposition=0.775, ppl=45.9, wps=11821.3, ups=0.58, wpb=20528.7, bsz=256, num_updates=17000, lr=0.000271163, gnorm=1.998, clip=0, loss_scale=128, train_wall=173, wall=30073
2022-07-10 23:10:06 | INFO | train_inner | epoch 016:    296 / 1122 loss=5.486, nll_loss=2.198, mask_ins=0.877, word_ins_ml=3.834, word_reposition=0.776, ppl=44.82, wps=11905.9, ups=0.58, wpb=20638.6, bsz=256, num_updates=17100, lr=0.000270369, gnorm=1.96, clip=0, loss_scale=128, train_wall=173, wall=30246
2022-07-10 23:13:01 | INFO | train_inner | epoch 016:    396 / 1122 loss=5.489, nll_loss=2.221, mask_ins=0.868, word_ins_ml=3.853, word_reposition=0.768, ppl=44.91, wps=11698, ups=0.57, wpb=20465, bsz=256, num_updates=17200, lr=0.000269582, gnorm=1.932, clip=0, loss_scale=128, train_wall=174, wall=30421
2022-07-10 23:15:57 | INFO | train_inner | epoch 016:    496 / 1122 loss=5.524, nll_loss=2.245, mask_ins=0.868, word_ins_ml=3.875, word_reposition=0.78, ppl=46, wps=11763.7, ups=0.57, wpb=20635.2, bsz=256, num_updates=17300, lr=0.000268802, gnorm=1.954, clip=0, loss_scale=128, train_wall=175, wall=30596
2022-07-10 23:18:51 | INFO | train_inner | epoch 016:    596 / 1122 loss=5.507, nll_loss=2.227, mask_ins=0.869, word_ins_ml=3.859, word_reposition=0.778, ppl=45.46, wps=11798.8, ups=0.57, wpb=20529.4, bsz=256, num_updates=17400, lr=0.000268028, gnorm=2.142, clip=0, loss_scale=200, train_wall=173, wall=30770
2022-07-10 23:21:45 | INFO | train_inner | epoch 016:    696 / 1122 loss=5.485, nll_loss=2.228, mask_ins=0.857, word_ins_ml=3.86, word_reposition=0.768, ppl=44.79, wps=11846, ups=0.58, wpb=20593, bsz=256, num_updates=17500, lr=0.000267261, gnorm=2.159, clip=0, loss_scale=256, train_wall=173, wall=30944
2022-07-10 23:24:38 | INFO | train_inner | epoch 016:    796 / 1122 loss=5.48, nll_loss=2.213, mask_ins=0.867, word_ins_ml=3.846, word_reposition=0.767, ppl=44.63, wps=11829.7, ups=0.58, wpb=20507.1, bsz=256, num_updates=17600, lr=0.000266501, gnorm=2.02, clip=0, loss_scale=256, train_wall=172, wall=31117
2022-07-10 23:27:31 | INFO | train_inner | epoch 016:    896 / 1122 loss=5.497, nll_loss=2.214, mask_ins=0.876, word_ins_ml=3.847, word_reposition=0.775, ppl=45.17, wps=11905.1, ups=0.58, wpb=20560.6, bsz=256, num_updates=17700, lr=0.000265747, gnorm=2.451, clip=1, loss_scale=256, train_wall=172, wall=31290
2022-07-10 23:30:24 | INFO | train_inner | epoch 016:    996 / 1122 loss=5.496, nll_loss=2.223, mask_ins=0.869, word_ins_ml=3.855, word_reposition=0.772, ppl=45.12, wps=11834.3, ups=0.58, wpb=20522, bsz=256, num_updates=17800, lr=0.000264999, gnorm=2.255, clip=0, loss_scale=256, train_wall=173, wall=31464
2022-07-10 23:33:18 | INFO | train_inner | epoch 016:   1096 / 1122 loss=5.469, nll_loss=2.206, mask_ins=0.857, word_ins_ml=3.839, word_reposition=0.773, ppl=44.29, wps=11784, ups=0.58, wpb=20487.6, bsz=256, num_updates=17900, lr=0.000264258, gnorm=2.092, clip=0, loss_scale=369, train_wall=173, wall=31637
2022-07-10 23:34:03 | INFO | train | epoch 016 | loss 5.499 | nll_loss 2.224 | mask_ins 0.869 | word_ins_ml 3.856 | word_reposition 0.773 | ppl 45.22 | wps 11598.1 | ups 0.57 | wpb 20520.3 | bsz 255.8 | num_updates 17926 | lr 0.000264067 | gnorm 2.127 | clip 0.2 | loss_scale 220 | train_wall 1940 | wall 31682
2022-07-10 23:34:29 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.985 | nll_loss 5.677 | mask_ins 1.567 | word_ins_ml 7.025 | word_reposition 1.394 | ppl 1013.72 | wps 37820.8 | wpb 2367.6 | bsz 32 | num_updates 17926 | best_loss 9.985
2022-07-10 23:34:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 16 @ 17926 updates, score 9.985) (writing took 10.364931077696383 seconds)
2022-07-10 23:36:48 | INFO | train_inner | epoch 017:     74 / 1122 loss=5.484, nll_loss=2.206, mask_ins=0.868, word_ins_ml=3.84, word_reposition=0.777, ppl=44.77, wps=9723.1, ups=0.48, wpb=20452.3, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=2.019, clip=0, loss_scale=512, train_wall=173, wall=31848
2022-07-10 23:39:42 | INFO | train_inner | epoch 017:    174 / 1122 loss=5.495, nll_loss=2.227, mask_ins=0.865, word_ins_ml=3.858, word_reposition=0.772, ppl=45.09, wps=11811.6, ups=0.58, wpb=20502.5, bsz=256, num_updates=18100, lr=0.000262794, gnorm=2.017, clip=0, loss_scale=512, train_wall=173, wall=32021
2022-07-10 23:40:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-10 23:42:37 | INFO | train_inner | epoch 017:    275 / 1122 loss=5.438, nll_loss=2.177, mask_ins=0.862, word_ins_ml=3.813, word_reposition=0.764, ppl=43.36, wps=11690.9, ups=0.57, wpb=20491.3, bsz=256, num_updates=18200, lr=0.000262071, gnorm=1.958, clip=0, loss_scale=350, train_wall=174, wall=32197
2022-07-10 23:45:31 | INFO | train_inner | epoch 017:    375 / 1122 loss=5.451, nll_loss=2.196, mask_ins=0.858, word_ins_ml=3.83, word_reposition=0.763, ppl=43.74, wps=11740.5, ups=0.58, wpb=20393.3, bsz=256, num_updates=18300, lr=0.000261354, gnorm=2.1, clip=0, loss_scale=256, train_wall=173, wall=32370
2022-07-10 23:48:24 | INFO | train_inner | epoch 017:    475 / 1122 loss=5.446, nll_loss=2.189, mask_ins=0.858, word_ins_ml=3.823, word_reposition=0.765, ppl=43.59, wps=11848, ups=0.58, wpb=20485.1, bsz=256, num_updates=18400, lr=0.000260643, gnorm=1.875, clip=0, loss_scale=256, train_wall=172, wall=32543
2022-07-10 23:51:15 | INFO | train_inner | epoch 017:    575 / 1122 loss=5.398, nll_loss=2.15, mask_ins=0.848, word_ins_ml=3.789, word_reposition=0.761, ppl=42.17, wps=11966, ups=0.58, wpb=20514.3, bsz=256, num_updates=18500, lr=0.000259938, gnorm=1.968, clip=0, loss_scale=256, train_wall=171, wall=32715
2022-07-10 23:54:08 | INFO | train_inner | epoch 017:    675 / 1122 loss=5.402, nll_loss=2.149, mask_ins=0.847, word_ins_ml=3.788, word_reposition=0.767, ppl=42.28, wps=11946.3, ups=0.58, wpb=20620.2, bsz=256, num_updates=18600, lr=0.000259238, gnorm=1.906, clip=0, loss_scale=256, train_wall=172, wall=32887
2022-07-10 23:57:00 | INFO | train_inner | epoch 017:    775 / 1122 loss=5.406, nll_loss=2.158, mask_ins=0.844, word_ins_ml=3.795, word_reposition=0.767, ppl=42.4, wps=12059.1, ups=0.58, wpb=20757.2, bsz=256, num_updates=18700, lr=0.000258544, gnorm=1.886, clip=0, loss_scale=389, train_wall=171, wall=33059
2022-07-10 23:59:55 | INFO | train_inner | epoch 017:    875 / 1122 loss=5.404, nll_loss=2.165, mask_ins=0.843, word_ins_ml=3.801, word_reposition=0.76, ppl=42.35, wps=11671.2, ups=0.57, wpb=20454.1, bsz=256, num_updates=18800, lr=0.000257855, gnorm=1.849, clip=0, loss_scale=512, train_wall=174, wall=33235
2022-07-11 00:02:49 | INFO | train_inner | epoch 017:    975 / 1122 loss=5.433, nll_loss=2.188, mask_ins=0.847, word_ins_ml=3.822, word_reposition=0.765, ppl=43.19, wps=11773, ups=0.57, wpb=20494.5, bsz=256, num_updates=18900, lr=0.000257172, gnorm=1.953, clip=0, loss_scale=512, train_wall=173, wall=33409
2022-07-11 00:05:44 | INFO | train_inner | epoch 017:   1075 / 1122 loss=5.387, nll_loss=2.144, mask_ins=0.843, word_ins_ml=3.783, word_reposition=0.762, ppl=41.85, wps=11768.8, ups=0.57, wpb=20538.5, bsz=256, num_updates=19000, lr=0.000256495, gnorm=1.93, clip=0, loss_scale=512, train_wall=174, wall=33583
2022-07-11 00:07:05 | INFO | train | epoch 017 | loss 5.423 | nll_loss 2.172 | mask_ins 0.851 | word_ins_ml 3.808 | word_reposition 0.764 | ppl 42.91 | wps 11601.2 | ups 0.57 | wpb 20519.2 | bsz 255.8 | num_updates 19047 | lr 0.000256178 | gnorm 1.947 | clip 0 | loss_scale 395 | train_wall 1937 | wall 33665
2022-07-11 00:07:32 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.007 | nll_loss 5.675 | mask_ins 1.536 | word_ins_ml 7.02 | word_reposition 1.451 | ppl 1028.69 | wps 37819.6 | wpb 2367.6 | bsz 32 | num_updates 19047 | best_loss 9.985
2022-07-11 00:07:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 17 @ 19047 updates, score 10.007) (writing took 5.574401133693755 seconds)
2022-07-11 00:09:10 | INFO | train_inner | epoch 018:     53 / 1122 loss=5.363, nll_loss=2.14, mask_ins=0.836, word_ins_ml=3.779, word_reposition=0.748, ppl=41.16, wps=9871, ups=0.49, wpb=20308.5, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=1.985, clip=0, loss_scale=512, train_wall=173, wall=33789
2022-07-11 00:12:03 | INFO | train_inner | epoch 018:    153 / 1122 loss=5.354, nll_loss=2.12, mask_ins=0.833, word_ins_ml=3.761, word_reposition=0.76, ppl=40.91, wps=11862.8, ups=0.58, wpb=20576.9, bsz=256, num_updates=19200, lr=0.000255155, gnorm=1.889, clip=0, loss_scale=717, train_wall=173, wall=33962
2022-07-11 00:14:55 | INFO | train_inner | epoch 018:    253 / 1122 loss=5.354, nll_loss=2.129, mask_ins=0.829, word_ins_ml=3.769, word_reposition=0.756, ppl=40.89, wps=11980.2, ups=0.58, wpb=20600.5, bsz=256, num_updates=19300, lr=0.000254493, gnorm=1.924, clip=0, loss_scale=1024, train_wall=171, wall=34134
2022-07-11 00:17:46 | INFO | train_inner | epoch 018:    353 / 1122 loss=5.399, nll_loss=2.155, mask_ins=0.841, word_ins_ml=3.792, word_reposition=0.766, ppl=42.19, wps=11938.8, ups=0.58, wpb=20465.5, bsz=256, num_updates=19400, lr=0.000253837, gnorm=1.96, clip=0, loss_scale=1024, train_wall=171, wall=34306
2022-07-11 00:20:39 | INFO | train_inner | epoch 018:    453 / 1122 loss=5.341, nll_loss=2.112, mask_ins=0.834, word_ins_ml=3.753, word_reposition=0.753, ppl=40.53, wps=11898.4, ups=0.58, wpb=20558.4, bsz=256, num_updates=19500, lr=0.000253185, gnorm=1.912, clip=0, loss_scale=1024, train_wall=172, wall=34479
2022-07-11 00:23:33 | INFO | train_inner | epoch 018:    553 / 1122 loss=5.363, nll_loss=2.141, mask_ins=0.829, word_ins_ml=3.779, word_reposition=0.755, ppl=41.15, wps=11738.3, ups=0.57, wpb=20426.6, bsz=256, num_updates=19600, lr=0.000252538, gnorm=1.919, clip=0, loss_scale=1024, train_wall=173, wall=34653
2022-07-11 00:26:28 | INFO | train_inner | epoch 018:    653 / 1122 loss=5.394, nll_loss=2.161, mask_ins=0.841, word_ins_ml=3.796, word_reposition=0.757, ppl=42.06, wps=11730.5, ups=0.57, wpb=20449.8, bsz=256, num_updates=19700, lr=0.000251896, gnorm=1.89, clip=0, loss_scale=1311, train_wall=174, wall=34827
2022-07-11 00:29:21 | INFO | train_inner | epoch 018:    753 / 1122 loss=5.386, nll_loss=2.15, mask_ins=0.837, word_ins_ml=3.787, word_reposition=0.763, ppl=41.81, wps=11870.7, ups=0.58, wpb=20640.4, bsz=256, num_updates=19800, lr=0.000251259, gnorm=1.891, clip=0, loss_scale=2048, train_wall=173, wall=35001
2022-07-11 00:32:17 | INFO | train_inner | epoch 018:    853 / 1122 loss=5.348, nll_loss=2.116, mask_ins=0.833, word_ins_ml=3.756, word_reposition=0.759, ppl=40.74, wps=11777.6, ups=0.57, wpb=20692.6, bsz=256, num_updates=19900, lr=0.000250627, gnorm=1.853, clip=0, loss_scale=2048, train_wall=175, wall=35177
2022-07-11 00:35:11 | INFO | train_inner | epoch 018:    953 / 1122 loss=5.373, nll_loss=2.145, mask_ins=0.835, word_ins_ml=3.781, word_reposition=0.757, ppl=41.43, wps=11786.7, ups=0.58, wpb=20469.1, bsz=256, num_updates=20000, lr=0.00025, gnorm=1.936, clip=0, loss_scale=2048, train_wall=173, wall=35350
2022-07-11 00:38:03 | INFO | train_inner | epoch 018:   1053 / 1122 loss=5.338, nll_loss=2.114, mask_ins=0.827, word_ins_ml=3.754, word_reposition=0.757, ppl=40.46, wps=11891.7, ups=0.58, wpb=20518.3, bsz=256, num_updates=20100, lr=0.000249377, gnorm=1.93, clip=0, loss_scale=2048, train_wall=172, wall=35523
2022-07-11 00:40:02 | INFO | train | epoch 018 | loss 5.367 | nll_loss 2.136 | mask_ins 0.835 | word_ins_ml 3.774 | word_reposition 0.758 | ppl 41.28 | wps 11646.7 | ups 0.57 | wpb 20521.1 | bsz 255.8 | num_updates 20169 | lr 0.00024895 | gnorm 1.922 | clip 0 | loss_scale 1426 | train_wall 1936 | wall 35642
2022-07-11 00:40:28 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.105 | nll_loss 5.699 | mask_ins 1.51 | word_ins_ml 7.039 | word_reposition 1.555 | ppl 1101.39 | wps 37828.7 | wpb 2367.6 | bsz 32 | num_updates 20169 | best_loss 9.985
2022-07-11 00:40:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 18 @ 20169 updates, score 10.105) (writing took 5.607394530437887 seconds)
2022-07-11 00:41:28 | INFO | train_inner | epoch 019:     31 / 1122 loss=5.349, nll_loss=2.102, mask_ins=0.841, word_ins_ml=3.744, word_reposition=0.764, ppl=40.75, wps=9937.5, ups=0.49, wpb=20308.2, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=2.001, clip=0, loss_scale=2376, train_wall=172, wall=35727
2022-07-11 00:44:22 | INFO | train_inner | epoch 019:    131 / 1122 loss=5.369, nll_loss=2.141, mask_ins=0.833, word_ins_ml=3.778, word_reposition=0.758, ppl=41.32, wps=11674.1, ups=0.57, wpb=20331.6, bsz=256, num_updates=20300, lr=0.000248146, gnorm=1.868, clip=0, loss_scale=4096, train_wall=173, wall=35901
2022-07-11 00:47:17 | INFO | train_inner | epoch 019:    231 / 1122 loss=5.333, nll_loss=2.121, mask_ins=0.822, word_ins_ml=3.761, word_reposition=0.75, ppl=40.31, wps=11703.6, ups=0.57, wpb=20543.5, bsz=256, num_updates=20400, lr=0.000247537, gnorm=1.876, clip=0, loss_scale=4096, train_wall=175, wall=36077
2022-07-11 00:50:12 | INFO | train_inner | epoch 019:    331 / 1122 loss=5.369, nll_loss=2.144, mask_ins=0.833, word_ins_ml=3.78, word_reposition=0.756, ppl=41.32, wps=11751.1, ups=0.57, wpb=20526.1, bsz=256, num_updates=20500, lr=0.000246932, gnorm=1.838, clip=0, loss_scale=4096, train_wall=174, wall=36251
2022-07-11 00:50:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 00:51:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-11 00:53:10 | INFO | train_inner | epoch 019:    433 / 1122 loss=5.352, nll_loss=2.129, mask_ins=0.83, word_ins_ml=3.767, word_reposition=0.756, ppl=40.85, wps=11475.2, ups=0.56, wpb=20399.5, bsz=256, num_updates=20600, lr=0.000246332, gnorm=1.923, clip=0, loss_scale=1727, train_wall=177, wall=36429
2022-07-11 00:56:03 | INFO | train_inner | epoch 019:    533 / 1122 loss=5.342, nll_loss=2.138, mask_ins=0.817, word_ins_ml=3.775, word_reposition=0.751, ppl=40.57, wps=11917.7, ups=0.58, wpb=20674.7, bsz=256, num_updates=20700, lr=0.000245737, gnorm=1.87, clip=0, loss_scale=1024, train_wall=173, wall=36603
2022-07-11 00:58:56 | INFO | train_inner | epoch 019:    633 / 1122 loss=5.339, nll_loss=2.131, mask_ins=0.818, word_ins_ml=3.769, word_reposition=0.752, ppl=40.48, wps=11926.1, ups=0.58, wpb=20608.5, bsz=256, num_updates=20800, lr=0.000245145, gnorm=1.845, clip=0, loss_scale=1024, train_wall=172, wall=36776
2022-07-11 01:01:48 | INFO | train_inner | epoch 019:    733 / 1122 loss=5.322, nll_loss=2.111, mask_ins=0.819, word_ins_ml=3.75, word_reposition=0.752, ppl=39.99, wps=12014.2, ups=0.58, wpb=20711, bsz=256, num_updates=20900, lr=0.000244558, gnorm=1.944, clip=0, loss_scale=1024, train_wall=172, wall=36948
2022-07-11 01:04:41 | INFO | train_inner | epoch 019:    833 / 1122 loss=5.322, nll_loss=2.111, mask_ins=0.821, word_ins_ml=3.75, word_reposition=0.751, ppl=40, wps=11908.5, ups=0.58, wpb=20552, bsz=256, num_updates=21000, lr=0.000243975, gnorm=1.86, clip=0, loss_scale=1024, train_wall=172, wall=37121
2022-07-11 01:07:36 | INFO | train_inner | epoch 019:    933 / 1122 loss=5.299, nll_loss=2.097, mask_ins=0.815, word_ins_ml=3.738, word_reposition=0.746, ppl=39.37, wps=11712.1, ups=0.57, wpb=20446.8, bsz=256, num_updates=21100, lr=0.000243396, gnorm=1.945, clip=0, loss_scale=1638, train_wall=174, wall=37295
2022-07-11 01:10:29 | INFO | train_inner | epoch 019:   1033 / 1122 loss=5.238, nll_loss=2.054, mask_ins=0.802, word_ins_ml=3.7, word_reposition=0.736, ppl=37.74, wps=11915.6, ups=0.58, wpb=20619.1, bsz=256, num_updates=21200, lr=0.000242821, gnorm=1.856, clip=0, loss_scale=2048, train_wall=172, wall=37468
2022-07-11 01:13:04 | INFO | train | epoch 019 | loss 5.329 | nll_loss 2.116 | mask_ins 0.822 | word_ins_ml 3.755 | word_reposition 0.751 | ppl 40.19 | wps 11596.1 | ups 0.57 | wpb 20520.4 | bsz 255.8 | num_updates 21289 | lr 0.000242313 | gnorm 1.887 | clip 0 | loss_scale 2194 | train_wall 1941 | wall 37624
2022-07-11 01:13:30 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 10.049 | nll_loss 5.639 | mask_ins 1.537 | word_ins_ml 6.989 | word_reposition 1.523 | ppl 1059.73 | wps 37978.5 | wpb 2367.6 | bsz 32 | num_updates 21289 | best_loss 9.985
2022-07-11 01:13:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 19 @ 21289 updates, score 10.049) (writing took 5.570195780135691 seconds)
2022-07-11 01:13:55 | INFO | train_inner | epoch 020:     11 / 1122 loss=5.334, nll_loss=2.118, mask_ins=0.826, word_ins_ml=3.755, word_reposition=0.753, ppl=40.34, wps=9903, ups=0.48, wpb=20423.6, bsz=253.8, num_updates=21300, lr=0.000242251, gnorm=1.944, clip=0, loss_scale=2048, train_wall=174, wall=37674
2022-07-11 01:16:50 | INFO | train_inner | epoch 020:    111 / 1122 loss=5.286, nll_loss=2.074, mask_ins=0.819, word_ins_ml=3.717, word_reposition=0.749, ppl=39.02, wps=11691.5, ups=0.57, wpb=20518.1, bsz=256, num_updates=21400, lr=0.000241684, gnorm=1.8, clip=0, loss_scale=2048, train_wall=175, wall=37850
2022-07-11 01:19:45 | INFO | train_inner | epoch 020:    211 / 1122 loss=5.269, nll_loss=2.077, mask_ins=0.802, word_ins_ml=3.72, word_reposition=0.747, ppl=38.55, wps=11788.4, ups=0.57, wpb=20528.1, bsz=256, num_updates=21500, lr=0.000241121, gnorm=1.842, clip=0, loss_scale=2048, train_wall=173, wall=38024
2022-07-11 01:20:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-11 01:22:41 | INFO | train_inner | epoch 020:    312 / 1122 loss=5.305, nll_loss=2.106, mask_ins=0.822, word_ins_ml=3.745, word_reposition=0.739, ppl=39.54, wps=11683.5, ups=0.57, wpb=20602.1, bsz=256, num_updates=21600, lr=0.000240563, gnorm=1.984, clip=0, loss_scale=1136, train_wall=176, wall=38200
2022-07-11 01:25:35 | INFO | train_inner | epoch 020:    412 / 1122 loss=5.274, nll_loss=2.079, mask_ins=0.805, word_ins_ml=3.721, word_reposition=0.748, ppl=38.68, wps=11794.3, ups=0.57, wpb=20584.1, bsz=256, num_updates=21700, lr=0.000240008, gnorm=1.83, clip=0, loss_scale=1024, train_wall=174, wall=38375
2022-07-11 01:26:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-11 01:28:31 | INFO | train_inner | epoch 020:    513 / 1122 loss=5.246, nll_loss=2.061, mask_ins=0.801, word_ins_ml=3.705, word_reposition=0.74, ppl=37.94, wps=11705.6, ups=0.57, wpb=20508.9, bsz=256, num_updates=21800, lr=0.000239457, gnorm=1.84, clip=0, loss_scale=720, train_wall=174, wall=38550
2022-07-11 01:31:24 | INFO | train_inner | epoch 020:    613 / 1122 loss=5.283, nll_loss=2.079, mask_ins=0.814, word_ins_ml=3.721, word_reposition=0.748, ppl=38.95, wps=11901.6, ups=0.58, wpb=20615.1, bsz=256, num_updates=21900, lr=0.000238909, gnorm=1.899, clip=0, loss_scale=512, train_wall=172, wall=38723
2022-07-11 01:34:17 | INFO | train_inner | epoch 020:    713 / 1122 loss=5.267, nll_loss=2.086, mask_ins=0.802, word_ins_ml=3.726, word_reposition=0.738, ppl=38.49, wps=11769.4, ups=0.58, wpb=20377.8, bsz=256, num_updates=22000, lr=0.000238366, gnorm=1.875, clip=0, loss_scale=512, train_wall=172, wall=38896
2022-07-11 01:37:11 | INFO | train_inner | epoch 020:    813 / 1122 loss=5.263, nll_loss=2.071, mask_ins=0.809, word_ins_ml=3.714, word_reposition=0.741, ppl=38.41, wps=11808, ups=0.58, wpb=20490.5, bsz=256, num_updates=22100, lr=0.000237826, gnorm=1.836, clip=0, loss_scale=512, train_wall=173, wall=39070
2022-07-11 01:40:04 | INFO | train_inner | epoch 020:    913 / 1122 loss=5.264, nll_loss=2.056, mask_ins=0.815, word_ins_ml=3.7, word_reposition=0.749, ppl=38.43, wps=11851.4, ups=0.58, wpb=20604.9, bsz=256, num_updates=22200, lr=0.000237289, gnorm=1.81, clip=0, loss_scale=512, train_wall=173, wall=39244
2022-07-11 01:42:58 | INFO | train_inner | epoch 020:   1013 / 1122 loss=5.277, nll_loss=2.097, mask_ins=0.799, word_ins_ml=3.736, word_reposition=0.742, ppl=38.78, wps=11824.6, ups=0.58, wpb=20556.6, bsz=256, num_updates=22300, lr=0.000236757, gnorm=1.875, clip=0, loss_scale=758, train_wall=173, wall=39418
2022-07-11 01:45:52 | INFO | train_inner | epoch 020:   1113 / 1122 loss=5.266, nll_loss=2.078, mask_ins=0.809, word_ins_ml=3.719, word_reposition=0.738, ppl=38.48, wps=11796, ups=0.58, wpb=20473.6, bsz=256, num_updates=22400, lr=0.000236228, gnorm=1.812, clip=0, loss_scale=1024, train_wall=173, wall=39591
2022-07-11 01:46:07 | INFO | train | epoch 020 | loss 5.273 | nll_loss 2.078 | mask_ins 0.809 | word_ins_ml 3.72 | word_reposition 0.744 | ppl 38.66 | wps 11592.1 | ups 0.56 | wpb 20521.5 | bsz 255.8 | num_updates 22409 | lr 0.00023618 | gnorm 1.858 | clip 0 | loss_scale 993 | train_wall 1942 | wall 39606
2022-07-11 01:46:33 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.826 | nll_loss 5.538 | mask_ins 1.491 | word_ins_ml 6.896 | word_reposition 1.438 | ppl 907.71 | wps 38014.2 | wpb 2367.6 | bsz 32 | num_updates 22409 | best_loss 9.826
2022-07-11 01:46:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 20 @ 22409 updates, score 9.826) (writing took 9.84427397698164 seconds)
2022-07-11 01:49:22 | INFO | train_inner | epoch 021:     91 / 1122 loss=5.288, nll_loss=2.092, mask_ins=0.81, word_ins_ml=3.732, word_reposition=0.746, ppl=39.07, wps=9757.5, ups=0.48, wpb=20465.8, bsz=253.8, num_updates=22500, lr=0.000235702, gnorm=1.892, clip=0, loss_scale=1024, train_wall=173, wall=39801
2022-07-11 01:52:16 | INFO | train_inner | epoch 021:    191 / 1122 loss=5.258, nll_loss=2.069, mask_ins=0.804, word_ins_ml=3.711, word_reposition=0.743, ppl=38.27, wps=11729.5, ups=0.57, wpb=20460.8, bsz=256, num_updates=22600, lr=0.00023518, gnorm=1.825, clip=0, loss_scale=1024, train_wall=174, wall=39975
2022-07-11 01:55:11 | INFO | train_inner | epoch 021:    291 / 1122 loss=5.247, nll_loss=2.067, mask_ins=0.798, word_ins_ml=3.709, word_reposition=0.74, ppl=37.97, wps=11819.8, ups=0.57, wpb=20640.5, bsz=256, num_updates=22700, lr=0.000234662, gnorm=1.895, clip=0, loss_scale=1024, train_wall=174, wall=40150
2022-07-11 01:55:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-11 01:58:08 | INFO | train_inner | epoch 021:    392 / 1122 loss=5.253, nll_loss=2.059, mask_ins=0.808, word_ins_ml=3.703, word_reposition=0.743, ppl=38.14, wps=11570.3, ups=0.56, wpb=20508.8, bsz=256, num_updates=22800, lr=0.000234146, gnorm=2.184, clip=0, loss_scale=603, train_wall=176, wall=40327
2022-07-11 02:01:04 | INFO | train_inner | epoch 021:    492 / 1122 loss=5.218, nll_loss=2.037, mask_ins=0.8, word_ins_ml=3.683, word_reposition=0.735, ppl=37.23, wps=11699.5, ups=0.57, wpb=20638.4, bsz=256, num_updates=22900, lr=0.000233635, gnorm=1.937, clip=0, loss_scale=512, train_wall=175, wall=40504
2022-07-11 02:03:57 | INFO | train_inner | epoch 021:    592 / 1122 loss=5.201, nll_loss=2.018, mask_ins=0.798, word_ins_ml=3.666, word_reposition=0.737, ppl=36.78, wps=11894.1, ups=0.58, wpb=20503.2, bsz=256, num_updates=23000, lr=0.000233126, gnorm=1.827, clip=0, loss_scale=512, train_wall=172, wall=40676
2022-07-11 02:06:51 | INFO | train_inner | epoch 021:    692 / 1122 loss=5.274, nll_loss=2.086, mask_ins=0.801, word_ins_ml=3.726, word_reposition=0.747, ppl=38.68, wps=11711.6, ups=0.57, wpb=20432.8, bsz=256, num_updates=23100, lr=0.000232621, gnorm=1.859, clip=0, loss_scale=512, train_wall=174, wall=40851
2022-07-11 02:09:43 | INFO | train_inner | epoch 021:    792 / 1122 loss=5.244, nll_loss=2.053, mask_ins=0.804, word_ins_ml=3.696, word_reposition=0.743, ppl=37.89, wps=11931.1, ups=0.58, wpb=20552.8, bsz=256, num_updates=23200, lr=0.000232119, gnorm=1.971, clip=0, loss_scale=512, train_wall=171, wall=41023
2022-07-11 02:12:36 | INFO | train_inner | epoch 021:    892 / 1122 loss=5.216, nll_loss=2.03, mask_ins=0.802, word_ins_ml=3.676, word_reposition=0.738, ppl=37.16, wps=11855.9, ups=0.58, wpb=20409.5, bsz=256, num_updates=23300, lr=0.000231621, gnorm=1.846, clip=0, loss_scale=876, train_wall=171, wall=41195
2022-07-11 02:14:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-11 02:15:30 | INFO | train_inner | epoch 021:    993 / 1122 loss=5.246, nll_loss=2.068, mask_ins=0.797, word_ins_ml=3.709, word_reposition=0.739, ppl=37.94, wps=11827.8, ups=0.57, wpb=20583.2, bsz=256, num_updates=23400, lr=0.000231125, gnorm=1.845, clip=0, loss_scale=857, train_wall=173, wall=41369
2022-07-11 02:18:23 | INFO | train_inner | epoch 021:   1093 / 1122 loss=5.25, nll_loss=2.078, mask_ins=0.796, word_ins_ml=3.718, word_reposition=0.736, ppl=38.05, wps=11840.5, ups=0.58, wpb=20496.2, bsz=256, num_updates=23500, lr=0.000230633, gnorm=1.809, clip=0, loss_scale=512, train_wall=172, wall=41542
2022-07-11 02:19:12 | INFO | train | epoch 021 | loss 5.244 | nll_loss 2.059 | mask_ins 0.801 | word_ins_ml 3.702 | word_reposition 0.74 | ppl 37.89 | wps 11577.2 | ups 0.56 | wpb 20520.4 | bsz 255.8 | num_updates 23529 | lr 0.000230491 | gnorm 1.899 | clip 0 | loss_scale 716 | train_wall 1940 | wall 41592
2022-07-11 02:19:38 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.832 | nll_loss 5.545 | mask_ins 1.532 | word_ins_ml 6.903 | word_reposition 1.396 | ppl 911.15 | wps 37940.4 | wpb 2367.6 | bsz 32 | num_updates 23529 | best_loss 9.826
2022-07-11 02:19:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 21 @ 23529 updates, score 9.832) (writing took 5.0732859978452325 seconds)
2022-07-11 02:21:45 | INFO | train_inner | epoch 022:     71 / 1122 loss=5.235, nll_loss=2.065, mask_ins=0.786, word_ins_ml=3.707, word_reposition=0.741, ppl=37.65, wps=10046.3, ups=0.49, wpb=20341.5, bsz=253.8, num_updates=23600, lr=0.000230144, gnorm=1.996, clip=0, loss_scale=512, train_wall=171, wall=41745
2022-07-11 02:24:37 | INFO | train_inner | epoch 022:    171 / 1122 loss=5.199, nll_loss=2.022, mask_ins=0.796, word_ins_ml=3.668, word_reposition=0.734, ppl=36.72, wps=11925.5, ups=0.58, wpb=20554.7, bsz=256, num_updates=23700, lr=0.000229658, gnorm=1.886, clip=0, loss_scale=512, train_wall=172, wall=41917
2022-07-11 02:27:29 | INFO | train_inner | epoch 022:    271 / 1122 loss=5.181, nll_loss=2.014, mask_ins=0.789, word_ins_ml=3.661, word_reposition=0.731, ppl=36.28, wps=12025.1, ups=0.58, wpb=20622.8, bsz=256, num_updates=23800, lr=0.000229175, gnorm=1.919, clip=0, loss_scale=512, train_wall=171, wall=42088
2022-07-11 02:30:20 | INFO | train_inner | epoch 022:    371 / 1122 loss=5.185, nll_loss=2.015, mask_ins=0.786, word_ins_ml=3.663, word_reposition=0.736, ppl=36.37, wps=12083.9, ups=0.59, wpb=20635.9, bsz=256, num_updates=23900, lr=0.000228695, gnorm=1.802, clip=0, loss_scale=620, train_wall=170, wall=42259
2022-07-11 02:33:13 | INFO | train_inner | epoch 022:    471 / 1122 loss=5.178, nll_loss=2.018, mask_ins=0.781, word_ins_ml=3.665, word_reposition=0.732, ppl=36.21, wps=11904, ups=0.58, wpb=20627.9, bsz=256, num_updates=24000, lr=0.000228218, gnorm=1.908, clip=0, loss_scale=1024, train_wall=173, wall=42432
2022-07-11 02:36:06 | INFO | train_inner | epoch 022:    571 / 1122 loss=5.221, nll_loss=2.042, mask_ins=0.795, word_ins_ml=3.686, word_reposition=0.739, ppl=37.29, wps=11848.2, ups=0.58, wpb=20497.5, bsz=256, num_updates=24100, lr=0.000227744, gnorm=1.876, clip=0, loss_scale=1024, train_wall=172, wall=42605
2022-07-11 02:38:59 | INFO | train_inner | epoch 022:    671 / 1122 loss=5.169, nll_loss=2.001, mask_ins=0.789, word_ins_ml=3.649, word_reposition=0.73, ppl=35.96, wps=11936.9, ups=0.58, wpb=20592.5, bsz=256, num_updates=24200, lr=0.000227273, gnorm=1.77, clip=0, loss_scale=1024, train_wall=172, wall=42778
2022-07-11 02:41:51 | INFO | train_inner | epoch 022:    771 / 1122 loss=5.191, nll_loss=2.032, mask_ins=0.783, word_ins_ml=3.676, word_reposition=0.732, ppl=36.52, wps=11879.8, ups=0.58, wpb=20515.3, bsz=256, num_updates=24300, lr=0.000226805, gnorm=1.817, clip=0, loss_scale=1024, train_wall=172, wall=42951
2022-07-11 02:44:44 | INFO | train_inner | epoch 022:    871 / 1122 loss=5.183, nll_loss=2.021, mask_ins=0.788, word_ins_ml=3.666, word_reposition=0.729, ppl=36.33, wps=11833, ups=0.58, wpb=20426.5, bsz=256, num_updates=24400, lr=0.000226339, gnorm=1.786, clip=0, loss_scale=1116, train_wall=172, wall=43123
2022-07-11 02:47:36 | INFO | train_inner | epoch 022:    971 / 1122 loss=5.184, nll_loss=2.022, mask_ins=0.787, word_ins_ml=3.668, word_reposition=0.73, ppl=36.36, wps=11953.1, ups=0.58, wpb=20623.9, bsz=256, num_updates=24500, lr=0.000225877, gnorm=1.784, clip=0, loss_scale=2048, train_wall=172, wall=43296
2022-07-11 02:50:29 | INFO | train_inner | epoch 022:   1071 / 1122 loss=5.194, nll_loss=2.034, mask_ins=0.787, word_ins_ml=3.678, word_reposition=0.729, ppl=36.6, wps=11862.9, ups=0.58, wpb=20421.5, bsz=256, num_updates=24600, lr=0.000225417, gnorm=1.799, clip=0, loss_scale=2048, train_wall=171, wall=43468
2022-07-11 02:51:56 | INFO | train | epoch 022 | loss 5.193 | nll_loss 2.026 | mask_ins 0.788 | word_ins_ml 3.671 | word_reposition 0.733 | ppl 36.57 | wps 11724.4 | ups 0.57 | wpb 20521 | bsz 255.8 | num_updates 24651 | lr 0.000225184 | gnorm 1.851 | clip 0 | loss_scale 1102 | train_wall 1924 | wall 43555
2022-07-11 02:52:22 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.788 | nll_loss 5.482 | mask_ins 1.475 | word_ins_ml 6.843 | word_reposition 1.47 | ppl 883.9 | wps 38015.6 | wpb 2367.6 | bsz 32 | num_updates 24651 | best_loss 9.788
2022-07-11 02:52:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 22 @ 24651 updates, score 9.788) (writing took 9.257974231615663 seconds)
2022-07-11 02:53:57 | INFO | train_inner | epoch 023:     49 / 1122 loss=5.188, nll_loss=2.027, mask_ins=0.789, word_ins_ml=3.672, word_reposition=0.727, ppl=36.46, wps=9730.2, ups=0.48, wpb=20241.4, bsz=253.8, num_updates=24700, lr=0.000224961, gnorm=1.879, clip=0, loss_scale=2048, train_wall=172, wall=43676
2022-07-11 02:56:51 | INFO | train_inner | epoch 023:    149 / 1122 loss=5.149, nll_loss=1.992, mask_ins=0.775, word_ins_ml=3.641, word_reposition=0.733, ppl=35.48, wps=11826.8, ups=0.57, wpb=20678.2, bsz=256, num_updates=24800, lr=0.000224507, gnorm=1.8, clip=0, loss_scale=2048, train_wall=174, wall=43851
2022-07-11 02:59:47 | INFO | train_inner | epoch 023:    249 / 1122 loss=5.171, nll_loss=2.002, mask_ins=0.785, word_ins_ml=3.65, word_reposition=0.736, ppl=36.03, wps=11691.2, ups=0.57, wpb=20529.7, bsz=256, num_updates=24900, lr=0.000224055, gnorm=1.743, clip=0, loss_scale=2048, train_wall=175, wall=44026
2022-07-11 03:00:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 03:02:44 | INFO | train_inner | epoch 023:    350 / 1122 loss=5.22, nll_loss=2.042, mask_ins=0.789, word_ins_ml=3.685, word_reposition=0.746, ppl=37.27, wps=11638, ups=0.57, wpb=20550.7, bsz=256, num_updates=25000, lr=0.000223607, gnorm=1.791, clip=0, loss_scale=2210, train_wall=176, wall=44203
2022-07-11 03:05:37 | INFO | train_inner | epoch 023:    450 / 1122 loss=5.157, nll_loss=2.006, mask_ins=0.773, word_ins_ml=3.652, word_reposition=0.732, ppl=35.69, wps=11805.7, ups=0.58, wpb=20501.2, bsz=256, num_updates=25100, lr=0.000223161, gnorm=1.784, clip=0, loss_scale=2048, train_wall=173, wall=44377
2022-07-11 03:08:30 | INFO | train_inner | epoch 023:    550 / 1122 loss=5.136, nll_loss=1.974, mask_ins=0.778, word_ins_ml=3.624, word_reposition=0.735, ppl=35.17, wps=11887.5, ups=0.58, wpb=20538.3, bsz=256, num_updates=25200, lr=0.000222718, gnorm=1.754, clip=0, loss_scale=2048, train_wall=172, wall=44549
2022-07-11 03:11:22 | INFO | train_inner | epoch 023:    650 / 1122 loss=5.174, nll_loss=2.023, mask_ins=0.778, word_ins_ml=3.668, word_reposition=0.729, ppl=36.1, wps=11939.7, ups=0.58, wpb=20564.9, bsz=256, num_updates=25300, lr=0.000222277, gnorm=1.728, clip=0, loss_scale=2048, train_wall=171, wall=44722
2022-07-11 03:14:14 | INFO | train_inner | epoch 023:    750 / 1122 loss=5.149, nll_loss=2.006, mask_ins=0.771, word_ins_ml=3.652, word_reposition=0.726, ppl=35.48, wps=11979.5, ups=0.58, wpb=20542.6, bsz=256, num_updates=25400, lr=0.000221839, gnorm=1.823, clip=0, loss_scale=2048, train_wall=171, wall=44893
2022-07-11 03:17:05 | INFO | train_inner | epoch 023:    850 / 1122 loss=5.164, nll_loss=2, mask_ins=0.782, word_ins_ml=3.646, word_reposition=0.736, ppl=35.86, wps=11956.1, ups=0.58, wpb=20523.6, bsz=256, num_updates=25500, lr=0.000221404, gnorm=1.811, clip=0, loss_scale=3645, train_wall=171, wall=45065
2022-07-11 03:19:58 | INFO | train_inner | epoch 023:    950 / 1122 loss=5.145, nll_loss=1.998, mask_ins=0.775, word_ins_ml=3.645, word_reposition=0.725, ppl=35.37, wps=11968.7, ups=0.58, wpb=20620.7, bsz=256, num_updates=25600, lr=0.000220971, gnorm=1.751, clip=0, loss_scale=4096, train_wall=171, wall=45237
2022-07-11 03:22:49 | INFO | train_inner | epoch 023:   1050 / 1122 loss=5.137, nll_loss=1.998, mask_ins=0.771, word_ins_ml=3.645, word_reposition=0.721, ppl=35.18, wps=11908.1, ups=0.58, wpb=20416.5, bsz=256, num_updates=25700, lr=0.000220541, gnorm=1.83, clip=0, loss_scale=4096, train_wall=171, wall=45409
2022-07-11 03:24:52 | INFO | train | epoch 023 | loss 5.16 | nll_loss 2.005 | mask_ins 0.778 | word_ins_ml 3.651 | word_reposition 0.731 | ppl 35.75 | wps 11639.8 | ups 0.57 | wpb 20521.3 | bsz 255.8 | num_updates 25772 | lr 0.000220232 | gnorm 1.787 | clip 0 | loss_scale 2701 | train_wall 1932 | wall 45532
2022-07-11 03:25:18 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.677 | nll_loss 5.457 | mask_ins 1.514 | word_ins_ml 6.817 | word_reposition 1.345 | ppl 818.48 | wps 37987.3 | wpb 2367.6 | bsz 32 | num_updates 25772 | best_loss 9.677
2022-07-11 03:25:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 23 @ 25772 updates, score 9.677) (writing took 8.89487532991916 seconds)
2022-07-11 03:25:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 03:26:17 | INFO | train_inner | epoch 024:     29 / 1122 loss=5.141, nll_loss=1.998, mask_ins=0.78, word_ins_ml=3.645, word_reposition=0.716, ppl=35.27, wps=9714.1, ups=0.48, wpb=20232.7, bsz=253.8, num_updates=25800, lr=0.000220113, gnorm=1.868, clip=0, loss_scale=3711, train_wall=172, wall=45617
2022-07-11 03:29:11 | INFO | train_inner | epoch 024:    129 / 1122 loss=5.129, nll_loss=1.984, mask_ins=0.769, word_ins_ml=3.633, word_reposition=0.728, ppl=35, wps=11826.2, ups=0.58, wpb=20501.2, bsz=256, num_updates=25900, lr=0.000219687, gnorm=1.781, clip=0, loss_scale=2048, train_wall=172, wall=45790
2022-07-11 03:32:03 | INFO | train_inner | epoch 024:    229 / 1122 loss=5.128, nll_loss=1.981, mask_ins=0.769, word_ins_ml=3.63, word_reposition=0.73, ppl=34.96, wps=11890, ups=0.58, wpb=20491.8, bsz=256, num_updates=26000, lr=0.000219265, gnorm=1.801, clip=0, loss_scale=2048, train_wall=172, wall=45963
2022-07-11 03:34:56 | INFO | train_inner | epoch 024:    329 / 1122 loss=5.129, nll_loss=1.985, mask_ins=0.772, word_ins_ml=3.633, word_reposition=0.724, ppl=35, wps=11857, ups=0.58, wpb=20492.8, bsz=256, num_updates=26100, lr=0.000218844, gnorm=1.779, clip=0, loss_scale=2048, train_wall=172, wall=46135
2022-07-11 03:37:49 | INFO | train_inner | epoch 024:    429 / 1122 loss=5.175, nll_loss=2.012, mask_ins=0.784, word_ins_ml=3.657, word_reposition=0.735, ppl=36.12, wps=11893.1, ups=0.58, wpb=20593.1, bsz=256, num_updates=26200, lr=0.000218426, gnorm=1.794, clip=0, loss_scale=2048, train_wall=172, wall=46309
2022-07-11 03:40:42 | INFO | train_inner | epoch 024:    529 / 1122 loss=5.081, nll_loss=1.943, mask_ins=0.767, word_ins_ml=3.596, word_reposition=0.718, ppl=33.85, wps=11925.3, ups=0.58, wpb=20554.2, bsz=256, num_updates=26300, lr=0.00021801, gnorm=1.805, clip=0, loss_scale=2191, train_wall=172, wall=46481
2022-07-11 03:43:35 | INFO | train_inner | epoch 024:    629 / 1122 loss=5.117, nll_loss=1.982, mask_ins=0.773, word_ins_ml=3.63, word_reposition=0.714, ppl=34.71, wps=11793.9, ups=0.58, wpb=20472, bsz=256, num_updates=26400, lr=0.000217597, gnorm=1.804, clip=0, loss_scale=4096, train_wall=173, wall=46655
2022-07-11 03:46:30 | INFO | train_inner | epoch 024:    729 / 1122 loss=5.134, nll_loss=1.988, mask_ins=0.768, word_ins_ml=3.636, word_reposition=0.73, ppl=35.1, wps=11765.1, ups=0.57, wpb=20588.3, bsz=256, num_updates=26500, lr=0.000217186, gnorm=1.804, clip=0, loss_scale=4096, train_wall=174, wall=46830
2022-07-11 03:49:24 | INFO | train_inner | epoch 024:    829 / 1122 loss=5.111, nll_loss=1.959, mask_ins=0.771, word_ins_ml=3.61, word_reposition=0.731, ppl=34.57, wps=11814.3, ups=0.57, wpb=20573.2, bsz=256, num_updates=26600, lr=0.000216777, gnorm=1.785, clip=0, loss_scale=4096, train_wall=173, wall=47004
2022-07-11 03:52:18 | INFO | train_inner | epoch 024:    929 / 1122 loss=5.115, nll_loss=1.969, mask_ins=0.771, word_ins_ml=3.619, word_reposition=0.725, ppl=34.66, wps=11817.9, ups=0.57, wpb=20577.9, bsz=256, num_updates=26700, lr=0.000216371, gnorm=1.766, clip=0, loss_scale=4096, train_wall=173, wall=47178
2022-07-11 03:55:12 | INFO | train_inner | epoch 024:   1029 / 1122 loss=5.175, nll_loss=2.013, mask_ins=0.788, word_ins_ml=3.657, word_reposition=0.73, ppl=36.14, wps=11787.9, ups=0.57, wpb=20514.9, bsz=256, num_updates=26800, lr=0.000215967, gnorm=1.824, clip=0, loss_scale=4096, train_wall=173, wall=47352
2022-07-11 03:57:54 | INFO | train | epoch 024 | loss 5.132 | nll_loss 1.984 | mask_ins 0.774 | word_ins_ml 3.632 | word_reposition 0.726 | ppl 35.07 | wps 11605.8 | ups 0.57 | wpb 20520 | bsz 255.8 | num_updates 26893 | lr 0.000215593 | gnorm 1.803 | clip 0 | loss_scale 3483 | train_wall 1938 | wall 47514
2022-07-11 03:58:20 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.792 | nll_loss 5.489 | mask_ins 1.499 | word_ins_ml 6.849 | word_reposition 1.443 | ppl 886.28 | wps 38077.1 | wpb 2367.6 | bsz 32 | num_updates 26893 | best_loss 9.677
2022-07-11 03:58:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 24 @ 26893 updates, score 9.792) (writing took 5.15357593074441 seconds)
2022-07-11 03:58:38 | INFO | train_inner | epoch 025:      7 / 1122 loss=5.158, nll_loss=2.01, mask_ins=0.775, word_ins_ml=3.654, word_reposition=0.729, ppl=35.7, wps=9961.8, ups=0.49, wpb=20454.7, bsz=253.8, num_updates=26900, lr=0.000215565, gnorm=1.877, clip=0, loss_scale=7987, train_wall=173, wall=47557
2022-07-11 04:01:33 | INFO | train_inner | epoch 025:    107 / 1122 loss=5.115, nll_loss=1.969, mask_ins=0.77, word_ins_ml=3.619, word_reposition=0.726, ppl=34.65, wps=11734.2, ups=0.57, wpb=20603.5, bsz=256, num_updates=27000, lr=0.000215166, gnorm=1.786, clip=0, loss_scale=8192, train_wall=175, wall=47733
2022-07-11 04:04:27 | INFO | train_inner | epoch 025:    207 / 1122 loss=5.12, nll_loss=1.977, mask_ins=0.77, word_ins_ml=3.625, word_reposition=0.725, ppl=34.78, wps=11804.7, ups=0.58, wpb=20508.2, bsz=256, num_updates=27100, lr=0.000214768, gnorm=1.747, clip=0, loss_scale=8192, train_wall=173, wall=47906
2022-07-11 04:07:20 | INFO | train_inner | epoch 025:    307 / 1122 loss=5.119, nll_loss=1.98, mask_ins=0.769, word_ins_ml=3.628, word_reposition=0.721, ppl=34.74, wps=11947.9, ups=0.58, wpb=20640.1, bsz=256, num_updates=27200, lr=0.000214373, gnorm=1.754, clip=0, loss_scale=8192, train_wall=172, wall=48079
2022-07-11 04:10:12 | INFO | train_inner | epoch 025:    407 / 1122 loss=5.128, nll_loss=1.974, mask_ins=0.771, word_ins_ml=3.623, word_reposition=0.734, ppl=34.97, wps=11918.8, ups=0.58, wpb=20579.8, bsz=256, num_updates=27300, lr=0.00021398, gnorm=1.764, clip=0, loss_scale=8192, train_wall=172, wall=48252
2022-07-11 04:11:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-11 04:12:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 04:13:08 | INFO | train_inner | epoch 025:    509 / 1122 loss=5.102, nll_loss=1.964, mask_ins=0.767, word_ins_ml=3.614, word_reposition=0.72, ppl=34.34, wps=11679.3, ups=0.57, wpb=20503.5, bsz=256, num_updates=27400, lr=0.000213589, gnorm=1.755, clip=0, loss_scale=8553, train_wall=175, wall=48427
2022-07-11 04:16:00 | INFO | train_inner | epoch 025:    609 / 1122 loss=5.074, nll_loss=1.927, mask_ins=0.769, word_ins_ml=3.581, word_reposition=0.723, ppl=33.68, wps=11872.2, ups=0.58, wpb=20426.7, bsz=256, num_updates=27500, lr=0.000213201, gnorm=1.766, clip=0, loss_scale=4096, train_wall=171, wall=48600
2022-07-11 04:18:51 | INFO | train_inner | epoch 025:    709 / 1122 loss=5.098, nll_loss=1.972, mask_ins=0.76, word_ins_ml=3.62, word_reposition=0.718, ppl=34.26, wps=11994.3, ups=0.58, wpb=20520.1, bsz=256, num_updates=27600, lr=0.000212814, gnorm=1.773, clip=0, loss_scale=4096, train_wall=170, wall=48771
2022-07-11 04:21:42 | INFO | train_inner | epoch 025:    809 / 1122 loss=5.086, nll_loss=1.946, mask_ins=0.762, word_ins_ml=3.597, word_reposition=0.726, ppl=33.97, wps=12107.7, ups=0.58, wpb=20736.3, bsz=256, num_updates=27700, lr=0.00021243, gnorm=1.8, clip=0, loss_scale=4096, train_wall=171, wall=48942
2022-07-11 04:24:34 | INFO | train_inner | epoch 025:    909 / 1122 loss=5.092, nll_loss=1.963, mask_ins=0.765, word_ins_ml=3.612, word_reposition=0.715, ppl=34.11, wps=11877.1, ups=0.58, wpb=20400.9, bsz=256, num_updates=27800, lr=0.000212047, gnorm=1.761, clip=0, loss_scale=4096, train_wall=171, wall=49114
2022-07-11 04:27:26 | INFO | train_inner | epoch 025:   1009 / 1122 loss=5.094, nll_loss=1.961, mask_ins=0.762, word_ins_ml=3.61, word_reposition=0.722, ppl=34.16, wps=11868.3, ups=0.58, wpb=20447.2, bsz=256, num_updates=27900, lr=0.000211667, gnorm=1.787, clip=0, loss_scale=4792, train_wall=171, wall=49286
2022-07-11 04:30:19 | INFO | train_inner | epoch 025:   1109 / 1122 loss=5.096, nll_loss=1.961, mask_ins=0.768, word_ins_ml=3.61, word_reposition=0.718, ppl=34.21, wps=11821.2, ups=0.58, wpb=20439.2, bsz=256, num_updates=28000, lr=0.000211289, gnorm=1.721, clip=0, loss_scale=8192, train_wall=172, wall=49459
2022-07-11 04:30:41 | INFO | train | epoch 025 | loss 5.102 | nll_loss 1.963 | mask_ins 0.767 | word_ins_ml 3.613 | word_reposition 0.723 | ppl 34.35 | wps 11684.8 | ups 0.57 | wpb 20522.4 | bsz 255.8 | num_updates 28013 | lr 0.00021124 | gnorm 1.771 | clip 0 | loss_scale 6462 | train_wall 1927 | wall 49481
2022-07-11 04:31:07 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.902 | nll_loss 5.544 | mask_ins 1.545 | word_ins_ml 6.903 | word_reposition 1.454 | ppl 956.47 | wps 37965.4 | wpb 2367.6 | bsz 32 | num_updates 28013 | best_loss 9.677
2022-07-11 04:31:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 25 @ 28013 updates, score 9.902) (writing took 5.103360949084163 seconds)
2022-07-11 04:32:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 04:33:46 | INFO | train_inner | epoch 026:     88 / 1122 loss=5.075, nll_loss=1.943, mask_ins=0.764, word_ins_ml=3.595, word_reposition=0.716, ppl=33.7, wps=9922, ups=0.48, wpb=20469.2, bsz=253.8, num_updates=28100, lr=0.000210912, gnorm=1.845, clip=0, loss_scale=5718, train_wall=174, wall=49665
2022-07-11 04:36:39 | INFO | train_inner | epoch 026:    188 / 1122 loss=5.073, nll_loss=1.942, mask_ins=0.76, word_ins_ml=3.594, word_reposition=0.719, ppl=33.66, wps=11830.5, ups=0.58, wpb=20500.7, bsz=256, num_updates=28200, lr=0.000210538, gnorm=1.72, clip=0, loss_scale=4096, train_wall=173, wall=49838
2022-07-11 04:39:34 | INFO | train_inner | epoch 026:    288 / 1122 loss=5.06, nll_loss=1.923, mask_ins=0.761, word_ins_ml=3.577, word_reposition=0.722, ppl=33.36, wps=11650.2, ups=0.57, wpb=20419.3, bsz=256, num_updates=28300, lr=0.000210166, gnorm=1.753, clip=0, loss_scale=4096, train_wall=174, wall=50014
2022-07-11 04:42:28 | INFO | train_inner | epoch 026:    388 / 1122 loss=5.071, nll_loss=1.943, mask_ins=0.759, word_ins_ml=3.594, word_reposition=0.718, ppl=33.62, wps=11833.4, ups=0.57, wpb=20620, bsz=256, num_updates=28400, lr=0.000209795, gnorm=1.823, clip=0, loss_scale=4096, train_wall=173, wall=50188
2022-07-11 04:45:21 | INFO | train_inner | epoch 026:    488 / 1122 loss=5.049, nll_loss=1.918, mask_ins=0.755, word_ins_ml=3.572, word_reposition=0.723, ppl=33.11, wps=11894.9, ups=0.58, wpb=20539.7, bsz=256, num_updates=28500, lr=0.000209427, gnorm=1.733, clip=0, loss_scale=4096, train_wall=172, wall=50361
2022-07-11 04:47:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 04:48:15 | INFO | train_inner | epoch 026:    589 / 1122 loss=5.06, nll_loss=1.941, mask_ins=0.749, word_ins_ml=3.592, word_reposition=0.72, ppl=33.36, wps=11748.3, ups=0.58, wpb=20424.3, bsz=256, num_updates=28600, lr=0.000209061, gnorm=1.777, clip=0, loss_scale=4420, train_wall=173, wall=50534
2022-07-11 04:51:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 04:51:10 | INFO | train_inner | epoch 026:    690 / 1122 loss=5.052, nll_loss=1.931, mask_ins=0.751, word_ins_ml=3.583, word_reposition=0.718, ppl=33.18, wps=11750.7, ups=0.57, wpb=20559.3, bsz=256, num_updates=28700, lr=0.000208696, gnorm=1.72, clip=0, loss_scale=4035, train_wall=174, wall=50709
2022-07-11 04:54:04 | INFO | train_inner | epoch 026:    790 / 1122 loss=5.048, nll_loss=1.924, mask_ins=0.756, word_ins_ml=3.577, word_reposition=0.715, ppl=33.09, wps=11855.4, ups=0.58, wpb=20579.9, bsz=256, num_updates=28800, lr=0.000208333, gnorm=1.743, clip=0, loss_scale=2048, train_wall=173, wall=50883
2022-07-11 04:56:57 | INFO | train_inner | epoch 026:    890 / 1122 loss=5.071, nll_loss=1.943, mask_ins=0.762, word_ins_ml=3.594, word_reposition=0.716, ppl=33.62, wps=11901.6, ups=0.58, wpb=20635.1, bsz=256, num_updates=28900, lr=0.000207973, gnorm=1.712, clip=0, loss_scale=2048, train_wall=173, wall=51056
2022-07-11 04:59:50 | INFO | train_inner | epoch 026:    990 / 1122 loss=5.124, nll_loss=1.989, mask_ins=0.762, word_ins_ml=3.634, word_reposition=0.727, ppl=34.86, wps=11900.7, ups=0.58, wpb=20566.7, bsz=256, num_updates=29000, lr=0.000207614, gnorm=1.782, clip=0, loss_scale=2048, train_wall=172, wall=51229
2022-07-11 05:02:42 | INFO | train_inner | epoch 026:   1090 / 1122 loss=5.07, nll_loss=1.948, mask_ins=0.751, word_ins_ml=3.598, word_reposition=0.72, ppl=33.58, wps=11932.8, ups=0.58, wpb=20534, bsz=256, num_updates=29100, lr=0.000207257, gnorm=1.773, clip=0, loss_scale=2048, train_wall=171, wall=51401
2022-07-11 05:03:36 | INFO | train | epoch 026 | loss 5.069 | nll_loss 1.941 | mask_ins 0.757 | word_ins_ml 3.593 | word_reposition 0.72 | ppl 33.58 | wps 11628.3 | ups 0.57 | wpb 20521.7 | bsz 255.8 | num_updates 29132 | lr 0.000207143 | gnorm 1.76 | clip 0 | loss_scale 3430 | train_wall 1935 | wall 51456
2022-07-11 05:04:02 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.778 | nll_loss 5.549 | mask_ins 1.528 | word_ins_ml 6.91 | word_reposition 1.341 | ppl 878.21 | wps 37958.7 | wpb 2367.6 | bsz 32 | num_updates 29132 | best_loss 9.677
2022-07-11 05:04:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 26 @ 29132 updates, score 9.778) (writing took 5.122782204300165 seconds)
2022-07-11 05:06:04 | INFO | train_inner | epoch 027:     68 / 1122 loss=5.078, nll_loss=1.955, mask_ins=0.756, word_ins_ml=3.605, word_reposition=0.718, ppl=33.79, wps=10065.5, ups=0.49, wpb=20364.9, bsz=253.8, num_updates=29200, lr=0.000206901, gnorm=1.804, clip=0, loss_scale=2048, train_wall=170, wall=51604
2022-07-11 05:08:56 | INFO | train_inner | epoch 027:    168 / 1122 loss=5.02, nll_loss=1.899, mask_ins=0.753, word_ins_ml=3.555, word_reposition=0.712, ppl=32.45, wps=11992.6, ups=0.58, wpb=20666.6, bsz=256, num_updates=29300, lr=0.000206548, gnorm=1.812, clip=0, loss_scale=3912, train_wall=172, wall=51776
2022-07-11 05:11:49 | INFO | train_inner | epoch 027:    268 / 1122 loss=5.071, nll_loss=1.953, mask_ins=0.75, word_ins_ml=3.603, word_reposition=0.719, ppl=33.62, wps=11966.7, ups=0.58, wpb=20641.1, bsz=256, num_updates=29400, lr=0.000206197, gnorm=1.758, clip=0, loss_scale=4096, train_wall=172, wall=51948
2022-07-11 05:14:42 | INFO | train_inner | epoch 027:    368 / 1122 loss=5.035, nll_loss=1.919, mask_ins=0.75, word_ins_ml=3.573, word_reposition=0.712, ppl=32.78, wps=11770.7, ups=0.58, wpb=20344.4, bsz=256, num_updates=29500, lr=0.000205847, gnorm=1.761, clip=0, loss_scale=4096, train_wall=172, wall=52121
2022-07-11 05:17:35 | INFO | train_inner | epoch 027:    468 / 1122 loss=5.071, nll_loss=1.937, mask_ins=0.763, word_ins_ml=3.588, word_reposition=0.719, ppl=33.61, wps=11800.4, ups=0.58, wpb=20437.3, bsz=256, num_updates=29600, lr=0.000205499, gnorm=1.71, clip=0, loss_scale=4096, train_wall=172, wall=52294
2022-07-11 05:20:28 | INFO | train_inner | epoch 027:    568 / 1122 loss=5.077, nll_loss=1.951, mask_ins=0.756, word_ins_ml=3.6, word_reposition=0.721, ppl=33.75, wps=11853.1, ups=0.58, wpb=20526, bsz=256, num_updates=29700, lr=0.000205152, gnorm=1.803, clip=0, loss_scale=4096, train_wall=172, wall=52468
2022-07-11 05:22:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 05:23:24 | INFO | train_inner | epoch 027:    669 / 1122 loss=5.075, nll_loss=1.958, mask_ins=0.753, word_ins_ml=3.606, word_reposition=0.716, ppl=33.7, wps=11729.9, ups=0.57, wpb=20584.4, bsz=256, num_updates=29800, lr=0.000204808, gnorm=1.766, clip=0, loss_scale=6610, train_wall=175, wall=52643
2022-07-11 05:26:18 | INFO | train_inner | epoch 027:    769 / 1122 loss=5.052, nll_loss=1.926, mask_ins=0.751, word_ins_ml=3.578, word_reposition=0.723, ppl=33.16, wps=11802.1, ups=0.57, wpb=20544, bsz=256, num_updates=29900, lr=0.000204465, gnorm=1.716, clip=0, loss_scale=4096, train_wall=173, wall=52817
2022-07-11 05:29:11 | INFO | train_inner | epoch 027:    869 / 1122 loss=5.013, nll_loss=1.889, mask_ins=0.755, word_ins_ml=3.545, word_reposition=0.713, ppl=32.28, wps=11853.2, ups=0.58, wpb=20557.3, bsz=256, num_updates=30000, lr=0.000204124, gnorm=1.798, clip=0, loss_scale=4096, train_wall=173, wall=52991
2022-07-11 05:32:05 | INFO | train_inner | epoch 027:    969 / 1122 loss=5.042, nll_loss=1.925, mask_ins=0.751, word_ins_ml=3.576, word_reposition=0.715, ppl=32.95, wps=11816.1, ups=0.57, wpb=20589.5, bsz=256, num_updates=30100, lr=0.000203785, gnorm=1.675, clip=0, loss_scale=4096, train_wall=173, wall=53165
2022-07-11 05:33:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 05:35:02 | INFO | train_inner | epoch 027:   1070 / 1122 loss=5.049, nll_loss=1.931, mask_ins=0.752, word_ins_ml=3.582, word_reposition=0.715, ppl=33.1, wps=11637.4, ups=0.57, wpb=20518.5, bsz=256, num_updates=30200, lr=0.000203447, gnorm=1.692, clip=0, loss_scale=3163, train_wall=175, wall=53341
2022-07-11 05:36:31 | INFO | train | epoch 027 | loss 5.049 | nll_loss 1.929 | mask_ins 0.753 | word_ins_ml 3.58 | word_reposition 0.716 | ppl 33.11 | wps 11637.1 | ups 0.57 | wpb 20522.9 | bsz 255.8 | num_updates 30252 | lr 0.000203272 | gnorm 1.755 | clip 0 | loss_scale 4003 | train_wall 1935 | wall 53431
2022-07-11 05:36:58 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.789 | nll_loss 5.525 | mask_ins 1.49 | word_ins_ml 6.882 | word_reposition 1.417 | ppl 884.47 | wps 37950.4 | wpb 2367.6 | bsz 32 | num_updates 30252 | best_loss 9.677
2022-07-11 05:37:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 27 @ 30252 updates, score 9.789) (writing took 4.8975815223529935 seconds)
2022-07-11 05:38:26 | INFO | train_inner | epoch 028:     48 / 1122 loss=5.031, nll_loss=1.923, mask_ins=0.745, word_ins_ml=3.576, word_reposition=0.71, ppl=32.69, wps=9951, ups=0.49, wpb=20336.3, bsz=253.8, num_updates=30300, lr=0.000203111, gnorm=1.8, clip=0, loss_scale=2048, train_wall=173, wall=53546
2022-07-11 05:41:20 | INFO | train_inner | epoch 028:    148 / 1122 loss=5.033, nll_loss=1.923, mask_ins=0.745, word_ins_ml=3.575, word_reposition=0.713, ppl=32.75, wps=11781.7, ups=0.57, wpb=20527.9, bsz=256, num_updates=30400, lr=0.000202777, gnorm=1.756, clip=0, loss_scale=2048, train_wall=173, wall=53720
2022-07-11 05:44:14 | INFO | train_inner | epoch 028:    248 / 1122 loss=4.988, nll_loss=1.865, mask_ins=0.749, word_ins_ml=3.523, word_reposition=0.716, ppl=31.73, wps=11786.3, ups=0.57, wpb=20524.4, bsz=256, num_updates=30500, lr=0.000202444, gnorm=1.706, clip=0, loss_scale=2048, train_wall=173, wall=53894
2022-07-11 05:47:08 | INFO | train_inner | epoch 028:    348 / 1122 loss=5.035, nll_loss=1.926, mask_ins=0.747, word_ins_ml=3.577, word_reposition=0.711, ppl=32.8, wps=11818.3, ups=0.58, wpb=20488.2, bsz=256, num_updates=30600, lr=0.000202113, gnorm=1.781, clip=0, loss_scale=2048, train_wall=173, wall=54067
2022-07-11 05:50:03 | INFO | train_inner | epoch 028:    448 / 1122 loss=5.016, nll_loss=1.914, mask_ins=0.736, word_ins_ml=3.566, word_reposition=0.714, ppl=32.37, wps=11843.7, ups=0.57, wpb=20691.9, bsz=256, num_updates=30700, lr=0.000201784, gnorm=1.745, clip=0, loss_scale=2744, train_wall=174, wall=54242
2022-07-11 05:52:58 | INFO | train_inner | epoch 028:    548 / 1122 loss=4.982, nll_loss=1.868, mask_ins=0.746, word_ins_ml=3.526, word_reposition=0.71, ppl=31.61, wps=11716.3, ups=0.57, wpb=20507.7, bsz=256, num_updates=30800, lr=0.000201456, gnorm=1.709, clip=0, loss_scale=4096, train_wall=174, wall=54417
2022-07-11 05:55:52 | INFO | train_inner | epoch 028:    648 / 1122 loss=5.016, nll_loss=1.908, mask_ins=0.743, word_ins_ml=3.562, word_reposition=0.711, ppl=32.35, wps=11708.5, ups=0.57, wpb=20454, bsz=256, num_updates=30900, lr=0.000201129, gnorm=1.77, clip=0, loss_scale=4096, train_wall=174, wall=54592
2022-07-11 05:58:46 | INFO | train_inner | epoch 028:    748 / 1122 loss=5.015, nll_loss=1.906, mask_ins=0.742, word_ins_ml=3.56, word_reposition=0.713, ppl=32.33, wps=11788.6, ups=0.58, wpb=20473.9, bsz=256, num_updates=31000, lr=0.000200805, gnorm=1.775, clip=0, loss_scale=4096, train_wall=173, wall=54765
2022-07-11 06:00:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 06:00:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-11 06:00:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-11 06:01:44 | INFO | train_inner | epoch 028:    851 / 1122 loss=4.99, nll_loss=1.9, mask_ins=0.734, word_ins_ml=3.554, word_reposition=0.702, ppl=31.77, wps=11511.2, ups=0.56, wpb=20482, bsz=256, num_updates=31100, lr=0.000200482, gnorm=1.85, clip=0, loss_scale=2749, train_wall=177, wall=54943
2022-07-11 06:04:36 | INFO | train_inner | epoch 028:    951 / 1122 loss=5.029, nll_loss=1.918, mask_ins=0.745, word_ins_ml=3.571, word_reposition=0.713, ppl=32.65, wps=11902, ups=0.58, wpb=20488.3, bsz=256, num_updates=31200, lr=0.00020016, gnorm=2.014, clip=0, loss_scale=512, train_wall=171, wall=55115
2022-07-11 06:07:29 | INFO | train_inner | epoch 028:   1051 / 1122 loss=5.049, nll_loss=1.946, mask_ins=0.744, word_ins_ml=3.595, word_reposition=0.71, ppl=33.1, wps=11943.4, ups=0.58, wpb=20604.5, bsz=256, num_updates=31300, lr=0.00019984, gnorm=1.822, clip=0, loss_scale=512, train_wall=172, wall=55288
2022-07-11 06:09:31 | INFO | train | epoch 028 | loss 5.018 | nll_loss 1.91 | mask_ins 0.743 | word_ins_ml 3.563 | word_reposition 0.711 | ppl 32.4 | wps 11602.9 | ups 0.57 | wpb 20521.1 | bsz 255.8 | num_updates 31371 | lr 0.000199614 | gnorm 1.818 | clip 0 | loss_scale 2351 | train_wall 1940 | wall 55410
2022-07-11 06:09:57 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.847 | nll_loss 5.56 | mask_ins 1.511 | word_ins_ml 6.917 | word_reposition 1.419 | ppl 920.69 | wps 37931.7 | wpb 2367.6 | bsz 32 | num_updates 31371 | best_loss 9.677
2022-07-11 06:10:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 28 @ 31371 updates, score 9.847) (writing took 4.8617199175059795 seconds)
2022-07-11 06:10:51 | INFO | train_inner | epoch 029:     29 / 1122 loss=5.023, nll_loss=1.916, mask_ins=0.74, word_ins_ml=3.568, word_reposition=0.715, ppl=32.52, wps=10090.6, ups=0.49, wpb=20473.3, bsz=253.8, num_updates=31400, lr=0.000199522, gnorm=2.195, clip=0, loss_scale=512, train_wall=171, wall=55491
2022-07-11 06:13:44 | INFO | train_inner | epoch 029:    129 / 1122 loss=4.972, nll_loss=1.872, mask_ins=0.736, word_ins_ml=3.529, word_reposition=0.706, ppl=31.37, wps=11925.1, ups=0.58, wpb=20577.9, bsz=256, num_updates=31500, lr=0.000199205, gnorm=1.768, clip=0, loss_scale=512, train_wall=172, wall=55663
2022-07-11 06:16:37 | INFO | train_inner | epoch 029:    229 / 1122 loss=4.966, nll_loss=1.865, mask_ins=0.736, word_ins_ml=3.523, word_reposition=0.708, ppl=31.26, wps=11872.9, ups=0.58, wpb=20501.8, bsz=256, num_updates=31600, lr=0.000198889, gnorm=1.746, clip=0, loss_scale=609, train_wall=172, wall=55836
2022-07-11 06:19:29 | INFO | train_inner | epoch 029:    329 / 1122 loss=5.002, nll_loss=1.904, mask_ins=0.733, word_ins_ml=3.557, word_reposition=0.712, ppl=32.05, wps=11897.7, ups=0.58, wpb=20537.6, bsz=256, num_updates=31700, lr=0.000198575, gnorm=1.742, clip=0, loss_scale=1024, train_wall=172, wall=56009
2022-07-11 06:22:22 | INFO | train_inner | epoch 029:    429 / 1122 loss=5.016, nll_loss=1.91, mask_ins=0.744, word_ins_ml=3.563, word_reposition=0.71, ppl=32.36, wps=11870.7, ups=0.58, wpb=20501.5, bsz=256, num_updates=31800, lr=0.000198263, gnorm=1.847, clip=0, loss_scale=1024, train_wall=172, wall=56181
2022-07-11 06:25:15 | INFO | train_inner | epoch 029:    529 / 1122 loss=4.985, nll_loss=1.884, mask_ins=0.735, word_ins_ml=3.54, word_reposition=0.71, ppl=31.67, wps=11849.3, ups=0.58, wpb=20500.6, bsz=256, num_updates=31900, lr=0.000197952, gnorm=1.724, clip=0, loss_scale=1024, train_wall=172, wall=56354
2022-07-11 06:28:07 | INFO | train_inner | epoch 029:    629 / 1122 loss=4.997, nll_loss=1.901, mask_ins=0.736, word_ins_ml=3.555, word_reposition=0.706, ppl=31.92, wps=11946.3, ups=0.58, wpb=20576.6, bsz=256, num_updates=32000, lr=0.000197642, gnorm=1.781, clip=0, loss_scale=1024, train_wall=171, wall=56527
2022-07-11 06:31:00 | INFO | train_inner | epoch 029:    729 / 1122 loss=4.99, nll_loss=1.891, mask_ins=0.74, word_ins_ml=3.546, word_reposition=0.705, ppl=31.78, wps=11905.7, ups=0.58, wpb=20525.3, bsz=256, num_updates=32100, lr=0.000197334, gnorm=1.75, clip=0, loss_scale=1096, train_wall=172, wall=56699
2022-07-11 06:33:52 | INFO | train_inner | epoch 029:    829 / 1122 loss=5.006, nll_loss=1.905, mask_ins=0.737, word_ins_ml=3.558, word_reposition=0.711, ppl=32.14, wps=11886.6, ups=0.58, wpb=20527.1, bsz=256, num_updates=32200, lr=0.000197028, gnorm=1.78, clip=0, loss_scale=2048, train_wall=172, wall=56872
2022-07-11 06:36:45 | INFO | train_inner | epoch 029:    929 / 1122 loss=5.02, nll_loss=1.917, mask_ins=0.737, word_ins_ml=3.568, word_reposition=0.716, ppl=32.46, wps=11925.6, ups=0.58, wpb=20621.1, bsz=256, num_updates=32300, lr=0.000196722, gnorm=1.729, clip=0, loss_scale=2048, train_wall=172, wall=57045
2022-07-11 06:39:38 | INFO | train_inner | epoch 029:   1029 / 1122 loss=5.024, nll_loss=1.913, mask_ins=0.744, word_ins_ml=3.565, word_reposition=0.716, ppl=32.54, wps=11830.1, ups=0.58, wpb=20467.3, bsz=256, num_updates=32400, lr=0.000196419, gnorm=1.8, clip=0, loss_scale=2048, train_wall=172, wall=57218
2022-07-11 06:42:19 | INFO | train | epoch 029 | loss 4.998 | nll_loss 1.897 | mask_ins 0.737 | word_ins_ml 3.551 | word_reposition 0.71 | ppl 31.96 | wps 11695.2 | ups 0.57 | wpb 20520 | bsz 255.8 | num_updates 32493 | lr 0.000196137 | gnorm 1.777 | clip 0 | loss_scale 1293 | train_wall 1929 | wall 57379
2022-07-11 06:42:45 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.455 | nll_loss 5.352 | mask_ins 1.495 | word_ins_ml 6.719 | word_reposition 1.242 | ppl 701.78 | wps 37964.2 | wpb 2367.6 | bsz 32 | num_updates 32493 | best_loss 9.455
2022-07-11 06:42:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_best.pt (epoch 29 @ 32493 updates, score 9.455) (writing took 9.038952481001616 seconds)
2022-07-11 06:43:07 | INFO | train_inner | epoch 030:      7 / 1122 loss=4.983, nll_loss=1.893, mask_ins=0.731, word_ins_ml=3.547, word_reposition=0.705, ppl=31.63, wps=9760.3, ups=0.48, wpb=20348.7, bsz=253.8, num_updates=32500, lr=0.000196116, gnorm=1.78, clip=0, loss_scale=2048, train_wall=172, wall=57426
2022-07-11 06:46:02 | INFO | train_inner | epoch 030:    107 / 1122 loss=4.994, nll_loss=1.898, mask_ins=0.735, word_ins_ml=3.552, word_reposition=0.707, ppl=31.86, wps=11734.4, ups=0.57, wpb=20514.5, bsz=256, num_updates=32600, lr=0.000195815, gnorm=1.785, clip=0, loss_scale=2048, train_wall=174, wall=57601
2022-07-11 06:48:57 | INFO | train_inner | epoch 030:    207 / 1122 loss=5.014, nll_loss=1.903, mask_ins=0.747, word_ins_ml=3.556, word_reposition=0.711, ppl=32.31, wps=11666.1, ups=0.57, wpb=20511.3, bsz=256, num_updates=32700, lr=0.000195515, gnorm=1.813, clip=0, loss_scale=3994, train_wall=175, wall=57777
2022-07-11 06:51:53 | INFO | train_inner | epoch 030:    307 / 1122 loss=4.981, nll_loss=1.886, mask_ins=0.732, word_ins_ml=3.542, word_reposition=0.708, ppl=31.58, wps=11664.1, ups=0.57, wpb=20514.9, bsz=256, num_updates=32800, lr=0.000195217, gnorm=1.77, clip=0, loss_scale=4096, train_wall=175, wall=57953
2022-07-11 06:52:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 06:54:50 | INFO | train_inner | epoch 030:    408 / 1122 loss=4.971, nll_loss=1.878, mask_ins=0.735, word_ins_ml=3.534, word_reposition=0.703, ppl=31.36, wps=11532.1, ups=0.56, wpb=20413.5, bsz=256, num_updates=32900, lr=0.00019492, gnorm=1.948, clip=0, loss_scale=2778, train_wall=176, wall=58130
2022-07-11 06:57:46 | INFO | train_inner | epoch 030:    508 / 1122 loss=4.97, nll_loss=1.869, mask_ins=0.732, word_ins_ml=3.526, word_reposition=0.712, ppl=31.34, wps=11808.5, ups=0.57, wpb=20695.8, bsz=256, num_updates=33000, lr=0.000194625, gnorm=1.835, clip=0, loss_scale=2048, train_wall=174, wall=58305
2022-07-11 06:58:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-11 07:00:42 | INFO | train_inner | epoch 030:    609 / 1122 loss=4.979, nll_loss=1.883, mask_ins=0.737, word_ins_ml=3.538, word_reposition=0.703, ppl=31.54, wps=11614.6, ups=0.57, wpb=20463.5, bsz=256, num_updates=33100, lr=0.000194331, gnorm=1.835, clip=0, loss_scale=1156, train_wall=175, wall=58481
2022-07-11 07:03:36 | INFO | train_inner | epoch 030:    709 / 1122 loss=4.991, nll_loss=1.897, mask_ins=0.734, word_ins_ml=3.55, word_reposition=0.707, ppl=31.8, wps=11698.9, ups=0.57, wpb=20417.6, bsz=256, num_updates=33200, lr=0.000194038, gnorm=1.78, clip=0, loss_scale=1024, train_wall=174, wall=58656
2022-07-11 07:06:30 | INFO | train_inner | epoch 030:    809 / 1122 loss=4.988, nll_loss=1.89, mask_ins=0.731, word_ins_ml=3.544, word_reposition=0.712, ppl=31.73, wps=11851.6, ups=0.58, wpb=20588.2, bsz=256, num_updates=33300, lr=0.000193746, gnorm=1.776, clip=0, loss_scale=1024, train_wall=173, wall=58829
2022-07-11 07:09:23 | INFO | train_inner | epoch 030:    909 / 1122 loss=4.951, nll_loss=1.858, mask_ins=0.734, word_ins_ml=3.515, word_reposition=0.701, ppl=30.93, wps=11905.5, ups=0.58, wpb=20584.7, bsz=256, num_updates=33400, lr=0.000193456, gnorm=1.715, clip=0, loss_scale=1024, train_wall=172, wall=59002
2022-07-11 07:12:16 | INFO | train_inner | epoch 030:   1009 / 1122 loss=4.978, nll_loss=1.874, mask_ins=0.736, word_ins_ml=3.53, word_reposition=0.712, ppl=31.51, wps=11915.8, ups=0.58, wpb=20636.1, bsz=256, num_updates=33500, lr=0.000193167, gnorm=1.739, clip=0, loss_scale=1024, train_wall=172, wall=59176
2022-07-11 07:15:09 | INFO | train_inner | epoch 030:   1109 / 1122 loss=4.966, nll_loss=1.877, mask_ins=0.729, word_ins_ml=3.533, word_reposition=0.704, ppl=31.24, wps=11861.9, ups=0.58, wpb=20547.9, bsz=256, num_updates=33600, lr=0.000192879, gnorm=1.718, clip=0, loss_scale=1802, train_wall=172, wall=59349
2022-07-11 07:15:31 | INFO | train | epoch 030 | loss 4.98 | nll_loss 1.883 | mask_ins 0.735 | word_ins_ml 3.538 | word_reposition 0.707 | ppl 31.57 | wps 11536.5 | ups 0.56 | wpb 20520 | bsz 255.8 | num_updates 33613 | lr 0.000192842 | gnorm 1.796 | clip 0 | loss_scale 2002 | train_wall 1947 | wall 59371
2022-07-11 07:15:57 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.881 | nll_loss 5.539 | mask_ins 1.51 | word_ins_ml 6.889 | word_reposition 1.481 | ppl 942.84 | wps 37952.8 | wpb 2367.6 | bsz 32 | num_updates 33613 | best_loss 9.455
2022-07-11 07:16:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 30 @ 33613 updates, score 9.881) (writing took 5.2112662717700005 seconds)
2022-07-11 07:18:34 | INFO | train_inner | epoch 031:     87 / 1122 loss=4.962, nll_loss=1.866, mask_ins=0.729, word_ins_ml=3.523, word_reposition=0.711, ppl=31.18, wps=10041.4, ups=0.49, wpb=20505, bsz=253.8, num_updates=33700, lr=0.000192593, gnorm=1.794, clip=0, loss_scale=2048, train_wall=172, wall=59553
2022-07-11 07:21:29 | INFO | train_inner | epoch 031:    187 / 1122 loss=4.979, nll_loss=1.875, mask_ins=0.741, word_ins_ml=3.53, word_reposition=0.707, ppl=31.53, wps=11736.8, ups=0.57, wpb=20621.5, bsz=256, num_updates=33800, lr=0.000192308, gnorm=1.799, clip=0, loss_scale=2048, train_wall=175, wall=59729
2022-07-11 07:24:23 | INFO | train_inner | epoch 031:    287 / 1122 loss=4.977, nll_loss=1.873, mask_ins=0.744, word_ins_ml=3.529, word_reposition=0.704, ppl=31.5, wps=11727.7, ups=0.57, wpb=20406.4, bsz=256, num_updates=33900, lr=0.000192024, gnorm=1.792, clip=0, loss_scale=2048, train_wall=173, wall=59903
2022-07-11 07:27:18 | INFO | train_inner | epoch 031:    387 / 1122 loss=4.998, nll_loss=1.899, mask_ins=0.736, word_ins_ml=3.552, word_reposition=0.71, ppl=31.95, wps=11773.2, ups=0.57, wpb=20597.6, bsz=256, num_updates=34000, lr=0.000191741, gnorm=1.705, clip=0, loss_scale=2048, train_wall=174, wall=60078
2022-07-11 07:30:11 | INFO | train_inner | epoch 031:    487 / 1122 loss=4.978, nll_loss=1.879, mask_ins=0.73, word_ins_ml=3.534, word_reposition=0.714, ppl=31.51, wps=11807, ups=0.58, wpb=20434.2, bsz=256, num_updates=34100, lr=0.00019146, gnorm=1.781, clip=0, loss_scale=3359, train_wall=172, wall=60251
2022-07-11 07:33:05 | INFO | train_inner | epoch 031:    587 / 1122 loss=4.939, nll_loss=1.857, mask_ins=0.724, word_ins_ml=3.514, word_reposition=0.701, ppl=30.68, wps=11889.8, ups=0.58, wpb=20634.6, bsz=256, num_updates=34200, lr=0.00019118, gnorm=1.726, clip=0, loss_scale=4096, train_wall=173, wall=60424
2022-07-11 07:35:58 | INFO | train_inner | epoch 031:    687 / 1122 loss=4.942, nll_loss=1.857, mask_ins=0.725, word_ins_ml=3.515, word_reposition=0.702, ppl=30.74, wps=11840.5, ups=0.58, wpb=20553.4, bsz=256, num_updates=34300, lr=0.000190901, gnorm=1.733, clip=0, loss_scale=4096, train_wall=173, wall=60598
2022-07-11 07:38:51 | INFO | train_inner | epoch 031:    787 / 1122 loss=4.949, nll_loss=1.858, mask_ins=0.731, word_ins_ml=3.515, word_reposition=0.703, ppl=30.89, wps=11908.1, ups=0.58, wpb=20547.4, bsz=256, num_updates=34400, lr=0.000190623, gnorm=1.702, clip=0, loss_scale=4096, train_wall=172, wall=60770
2022-07-11 07:41:45 | INFO | train_inner | epoch 031:    887 / 1122 loss=4.911, nll_loss=1.832, mask_ins=0.721, word_ins_ml=3.492, word_reposition=0.698, ppl=30.08, wps=11766.8, ups=0.57, wpb=20485.6, bsz=256, num_updates=34500, lr=0.000190347, gnorm=1.76, clip=0, loss_scale=4096, train_wall=173, wall=60944
2022-07-11 07:43:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 07:44:39 | INFO | train_inner | epoch 031:    988 / 1122 loss=4.973, nll_loss=1.884, mask_ins=0.733, word_ins_ml=3.538, word_reposition=0.702, ppl=31.4, wps=11712.8, ups=0.57, wpb=20408.4, bsz=256, num_updates=34600, lr=0.000190071, gnorm=1.772, clip=0, loss_scale=4461, train_wall=173, wall=61119
2022-07-11 07:47:36 | INFO | train_inner | epoch 031:   1088 / 1122 loss=4.955, nll_loss=1.87, mask_ins=0.729, word_ins_ml=3.525, word_reposition=0.701, ppl=31.01, wps=11608.7, ups=0.57, wpb=20541.3, bsz=256, num_updates=34700, lr=0.000189797, gnorm=1.725, clip=0, loss_scale=4096, train_wall=176, wall=61296
2022-07-11 07:48:35 | INFO | train | epoch 031 | loss 4.962 | nll_loss 1.869 | mask_ins 0.732 | word_ins_ml 3.525 | word_reposition 0.705 | ppl 31.16 | wps 11593.4 | ups 0.56 | wpb 20520.5 | bsz 255.8 | num_updates 34734 | lr 0.000189704 | gnorm 1.757 | clip 0 | loss_scale 3357 | train_wall 1944 | wall 61355
2022-07-11 07:49:03 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.64 | nll_loss 5.429 | mask_ins 1.498 | word_ins_ml 6.788 | word_reposition 1.354 | ppl 798.13 | wps 35959.3 | wpb 2367.6 | bsz 32 | num_updates 34734 | best_loss 9.455
2022-07-11 07:49:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 31 @ 34734 updates, score 9.64) (writing took 5.634824910201132 seconds)
2022-07-11 07:51:05 | INFO | train_inner | epoch 032:     66 / 1122 loss=4.961, nll_loss=1.868, mask_ins=0.731, word_ins_ml=3.524, word_reposition=0.706, ppl=31.16, wps=9746.3, ups=0.48, wpb=20331.6, bsz=253.8, num_updates=34800, lr=0.000189525, gnorm=1.861, clip=0, loss_scale=4096, train_wall=175, wall=61504
2022-07-11 07:53:58 | INFO | train_inner | epoch 032:    166 / 1122 loss=4.931, nll_loss=1.848, mask_ins=0.725, word_ins_ml=3.507, word_reposition=0.699, ppl=30.51, wps=11807.2, ups=0.58, wpb=20451.1, bsz=256, num_updates=34900, lr=0.000189253, gnorm=1.742, clip=0, loss_scale=4096, train_wall=172, wall=61678
2022-07-11 07:56:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 07:56:55 | INFO | train_inner | epoch 032:    267 / 1122 loss=4.906, nll_loss=1.827, mask_ins=0.718, word_ins_ml=3.488, word_reposition=0.7, ppl=29.97, wps=11586.5, ups=0.56, wpb=20521.5, bsz=256, num_updates=35000, lr=0.000188982, gnorm=1.747, clip=0, loss_scale=3873, train_wall=176, wall=61855
2022-07-11 07:59:51 | INFO | train_inner | epoch 032:    367 / 1122 loss=4.938, nll_loss=1.846, mask_ins=0.73, word_ins_ml=3.504, word_reposition=0.703, ppl=30.65, wps=11786.1, ups=0.57, wpb=20693.8, bsz=256, num_updates=35100, lr=0.000188713, gnorm=1.76, clip=0, loss_scale=2048, train_wall=175, wall=62030
2022-07-11 08:02:43 | INFO | train_inner | epoch 032:    467 / 1122 loss=4.901, nll_loss=1.807, mask_ins=0.727, word_ins_ml=3.47, word_reposition=0.704, ppl=29.87, wps=12012.7, ups=0.58, wpb=20725.3, bsz=256, num_updates=35200, lr=0.000188445, gnorm=1.714, clip=0, loss_scale=2048, train_wall=172, wall=62203
2022-07-11 08:05:39 | INFO | train_inner | epoch 032:    567 / 1122 loss=4.941, nll_loss=1.86, mask_ins=0.726, word_ins_ml=3.517, word_reposition=0.698, ppl=30.71, wps=11717.8, ups=0.57, wpb=20548.6, bsz=256, num_updates=35300, lr=0.000188177, gnorm=1.865, clip=0, loss_scale=2048, train_wall=175, wall=62378
2022-07-11 08:08:36 | INFO | train_inner | epoch 032:    667 / 1122 loss=4.975, nll_loss=1.887, mask_ins=0.731, word_ins_ml=3.54, word_reposition=0.703, ppl=31.44, wps=11532.3, ups=0.56, wpb=20438.5, bsz=256, num_updates=35400, lr=0.000187912, gnorm=1.847, clip=0, loss_scale=2048, train_wall=176, wall=62555
2022-07-11 08:11:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-11 08:11:35 | INFO | train_inner | epoch 032:    768 / 1122 loss=4.938, nll_loss=1.853, mask_ins=0.728, word_ins_ml=3.511, word_reposition=0.699, ppl=30.65, wps=11363.2, ups=0.56, wpb=20375, bsz=256, num_updates=35500, lr=0.000187647, gnorm=1.816, clip=0, loss_scale=1845, train_wall=178, wall=62735
2022-07-11 08:14:28 | INFO | train_inner | epoch 032:    868 / 1122 loss=4.926, nll_loss=1.848, mask_ins=0.72, word_ins_ml=3.506, word_reposition=0.701, ppl=30.4, wps=11824.8, ups=0.58, wpb=20411.2, bsz=256, num_updates=35600, lr=0.000187383, gnorm=1.75, clip=0, loss_scale=1024, train_wall=172, wall=62907
2022-07-11 08:17:26 | INFO | train_inner | epoch 032:    968 / 1122 loss=4.93, nll_loss=1.854, mask_ins=0.722, word_ins_ml=3.511, word_reposition=0.698, ppl=30.49, wps=11591.6, ups=0.56, wpb=20638.5, bsz=256, num_updates=35700, lr=0.00018712, gnorm=1.767, clip=0, loss_scale=1024, train_wall=177, wall=63085
2022-07-11 08:20:25 | INFO | train_inner | epoch 032:   1068 / 1122 loss=4.947, nll_loss=1.854, mask_ins=0.728, word_ins_ml=3.511, word_reposition=0.707, ppl=30.84, wps=11509, ups=0.56, wpb=20572.4, bsz=256, num_updates=35800, lr=0.000186859, gnorm=1.792, clip=0, loss_scale=1024, train_wall=178, wall=63264
2022-07-11 08:21:59 | INFO | train | epoch 032 | loss 4.932 | nll_loss 1.847 | mask_ins 0.725 | word_ins_ml 3.505 | word_reposition 0.701 | ppl 30.52 | wps 11472.7 | ups 0.56 | wpb 20521.9 | bsz 255.8 | num_updates 35854 | lr 0.000186718 | gnorm 1.785 | clip 0 | loss_scale 2174 | train_wall 1961 | wall 63358
2022-07-11 08:22:25 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.748 | nll_loss 5.483 | mask_ins 1.497 | word_ins_ml 6.847 | word_reposition 1.404 | ppl 859.75 | wps 38011.4 | wpb 2367.6 | bsz 32 | num_updates 35854 | best_loss 9.455
2022-07-11 08:22:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 32 @ 35854 updates, score 9.748) (writing took 5.741395579650998 seconds)
2022-07-11 08:23:50 | INFO | train_inner | epoch 033:     46 / 1122 loss=4.936, nll_loss=1.858, mask_ins=0.721, word_ins_ml=3.515, word_reposition=0.701, ppl=30.62, wps=9926.3, ups=0.49, wpb=20418.2, bsz=253.8, num_updates=35900, lr=0.000186598, gnorm=1.877, clip=0, loss_scale=1024, train_wall=173, wall=63470
2022-07-11 08:26:44 | INFO | train_inner | epoch 033:    146 / 1122 loss=4.929, nll_loss=1.838, mask_ins=0.729, word_ins_ml=3.497, word_reposition=0.703, ppl=30.47, wps=11907.9, ups=0.58, wpb=20675, bsz=256, num_updates=36000, lr=0.000186339, gnorm=1.888, clip=0, loss_scale=1106, train_wall=173, wall=63643
2022-07-11 08:28:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-11 08:29:42 | INFO | train_inner | epoch 033:    247 / 1122 loss=4.942, nll_loss=1.85, mask_ins=0.727, word_ins_ml=3.508, word_reposition=0.707, ppl=30.73, wps=11512.3, ups=0.56, wpb=20535.1, bsz=256, num_updates=36100, lr=0.000186081, gnorm=1.836, clip=0, loss_scale=1703, train_wall=178, wall=63822
2022-07-11 08:32:41 | INFO | train_inner | epoch 033:    347 / 1122 loss=4.883, nll_loss=1.805, mask_ins=0.717, word_ins_ml=3.467, word_reposition=0.699, ppl=29.5, wps=11499.2, ups=0.56, wpb=20586.8, bsz=256, num_updates=36200, lr=0.000185824, gnorm=1.871, clip=0, loss_scale=1024, train_wall=178, wall=64001
2022-07-11 08:35:37 | INFO | train_inner | epoch 033:    447 / 1122 loss=4.902, nll_loss=1.818, mask_ins=0.72, word_ins_ml=3.479, word_reposition=0.702, ppl=29.89, wps=11625.1, ups=0.57, wpb=20441.4, bsz=256, num_updates=36300, lr=0.000185567, gnorm=1.787, clip=0, loss_scale=1024, train_wall=175, wall=64177
2022-07-11 08:38:31 | INFO | train_inner | epoch 033:    547 / 1122 loss=4.905, nll_loss=1.832, mask_ins=0.716, word_ins_ml=3.491, word_reposition=0.698, ppl=29.97, wps=11811.1, ups=0.58, wpb=20521.4, bsz=256, num_updates=36400, lr=0.000185312, gnorm=1.774, clip=0, loss_scale=1024, train_wall=173, wall=64350
2022-07-11 08:41:25 | INFO | train_inner | epoch 033:    647 / 1122 loss=4.956, nll_loss=1.872, mask_ins=0.726, word_ins_ml=3.527, word_reposition=0.703, ppl=31.03, wps=11762.8, ups=0.57, wpb=20485.9, bsz=256, num_updates=36500, lr=0.000185058, gnorm=1.814, clip=0, loss_scale=1024, train_wall=173, wall=64525
2022-07-11 08:44:25 | INFO | train_inner | epoch 033:    747 / 1122 loss=4.947, nll_loss=1.868, mask_ins=0.723, word_ins_ml=3.523, word_reposition=0.702, ppl=30.85, wps=11414, ups=0.56, wpb=20494.4, bsz=256, num_updates=36600, lr=0.000184805, gnorm=1.773, clip=0, loss_scale=1249, train_wall=179, wall=64704
2022-07-11 08:47:24 | INFO | train_inner | epoch 033:    847 / 1122 loss=4.891, nll_loss=1.825, mask_ins=0.712, word_ins_ml=3.485, word_reposition=0.694, ppl=29.66, wps=11428.5, ups=0.56, wpb=20487.5, bsz=256, num_updates=36700, lr=0.000184553, gnorm=1.757, clip=0, loss_scale=2048, train_wall=178, wall=64883
2022-07-11 08:50:18 | INFO | train_inner | epoch 033:    947 / 1122 loss=4.889, nll_loss=1.808, mask_ins=0.719, word_ins_ml=3.47, word_reposition=0.7, ppl=29.63, wps=11791.4, ups=0.57, wpb=20521.2, bsz=256, num_updates=36800, lr=0.000184302, gnorm=1.743, clip=0, loss_scale=2048, train_wall=173, wall=65057
2022-07-11 08:53:15 | INFO | train_inner | epoch 033:   1047 / 1122 loss=4.942, nll_loss=1.854, mask_ins=0.726, word_ins_ml=3.511, word_reposition=0.705, ppl=30.74, wps=11631.9, ups=0.56, wpb=20624.2, bsz=256, num_updates=36900, lr=0.000184053, gnorm=1.74, clip=0, loss_scale=2048, train_wall=176, wall=65235
2022-07-11 08:55:30 | INFO | train | epoch 033 | loss 4.919 | nll_loss 1.838 | mask_ins 0.722 | word_ins_ml 3.497 | word_reposition 0.701 | ppl 30.25 | wps 11440.6 | ups 0.56 | wpb 20520.8 | bsz 255.8 | num_updates 36975 | lr 0.000183866 | gnorm 1.801 | clip 0 | loss_scale 1455 | train_wall 1969 | wall 65369
2022-07-11 08:55:57 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 9.762 | nll_loss 5.487 | mask_ins 1.492 | word_ins_ml 6.843 | word_reposition 1.427 | ppl 868.13 | wps 36452.6 | wpb 2367.6 | bsz 32 | num_updates 36975 | best_loss 9.455
2022-07-11 08:56:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_cased/checkpoint_last.pt (epoch 33 @ 36975 updates, score 9.762) (writing took 5.102033361792564 seconds)
2022-07-11 08:56:46 | INFO | train_inner | epoch 034:     25 / 1122 loss=4.908, nll_loss=1.831, mask_ins=0.722, word_ins_ml=3.49, word_reposition=0.695, ppl=30.01, wps=9640.8, ups=0.47, wpb=20344.3, bsz=253.8, num_updates=37000, lr=0.000183804, gnorm=1.778, clip=0, loss_scale=2048, train_wall=178, wall=65446
2022-07-11 08:59:43 | INFO | train_inner | epoch 034:    125 / 1122 loss=4.908, nll_loss=1.839, mask_ins=0.716, word_ins_ml=3.498, word_reposition=0.694, ppl=30.02, wps=11604.8, ups=0.57, wpb=20452, bsz=256, num_updates=37100, lr=0.000183556, gnorm=1.79, clip=0, loss_scale=2253, train_wall=175, wall=65622
2022-07-11 09:02:37 | INFO | train_inner | epoch 034:    225 / 1122 loss=4.921, nll_loss=1.839, mask_ins=0.723, word_ins_ml=3.497, word_reposition=0.7, ppl=30.3, wps=11751.9, ups=0.57, wpb=20449.2, bsz=256, num_updates=37200, lr=0.000183309, gnorm=1.763, clip=0, loss_scale=4096, train_wall=173, wall=65796
2022-07-11 09:05:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 09:05:31 | INFO | train_inner | epoch 034:    326 / 1122 loss=4.883, nll_loss=1.812, mask_ins=0.713, word_ins_ml=3.474, word_reposition=0.696, ppl=29.5, wps=11765.4, ups=0.57, wpb=20505.2, bsz=256, num_updates=37300, lr=0.000183063, gnorm=1.777, clip=0, loss_scale=3853, train_wall=174, wall=65970
2022-07-11 09:08:24 | INFO | train_inner | epoch 034:    426 / 1122 loss=4.887, nll_loss=1.808, mask_ins=0.719, word_ins_ml=3.47, word_reposition=0.698, ppl=29.59, wps=11847.8, ups=0.58, wpb=20517.1, bsz=256, num_updates=37400, lr=0.000182818, gnorm=1.724, clip=0, loss_scale=2048, train_wall=172, wall=66143
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
train.sh: line 39: nts: command not found
