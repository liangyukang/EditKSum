nohup: ignoring input
2022-08-10 15:15:58 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:14995
2022-08-10 15:15:58 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:14995
2022-08-10 15:15:58 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:14995
2022-08-10 15:15:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-08-10 15:15:58 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:14995
2022-08-10 15:15:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-08-10 15:15:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-08-10 15:15:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-08-10 15:15:59 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-10 15:15:59 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-08-10 15:15:59 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-10 15:15:59 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-08-10 15:15:59 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-10 15:15:59 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-08-10 15:15:59 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-10 15:15:59 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-08-10 15:16:03 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=4, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=4, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:14995', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_kpe_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-08-10 15:16:03 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-08-10 15:16:03 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-08-10 15:16:03 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.source
2022-08-10 15:16:03 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.target
2022-08-10 15:16:03 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]365it [00:00, 3647.90it/s]386it [00:00, 3856.74it/s]383it [00:00, 3822.69it/s]388it [00:00, 3871.01it/s]730it [00:00, 3431.62it/s]772it [00:00, 3493.09it/s]766it [00:00, 3455.02it/s]776it [00:00, 3460.65it/s]1095it [00:00, 3524.30it/s]1138it [00:00, 3564.12it/s]1114it [00:00, 3457.15it/s]1125it [00:00, 3442.55it/s]1449it [00:00, 3393.91it/s]1497it [00:00, 3429.29it/s]1461it [00:00, 3330.08it/s]1471it [00:00, 3307.08it/s]1819it [00:00, 3497.70it/s]1886it [00:00, 3587.41it/s]1841it [00:00, 3488.17it/s]1854it [00:00, 3483.29it/s]2209it [00:00, 3629.82it/s]2223it [00:00, 3597.15it/s]2247it [00:00, 3504.36it/s]2236it [00:00, 3399.08it/s]2573it [00:00, 3549.51it/s]2643it [00:00, 3646.23it/s]2585it [00:00, 3455.13it/s]2605it [00:00, 3485.96it/s]2950it [00:00, 3616.86it/s]3041it [00:00, 3746.95it/s]2972it [00:00, 3581.27it/s]2995it [00:00, 3610.29it/s]3313it [00:00, 3533.16it/s]3417it [00:00, 3616.23it/s]3332it [00:00, 3482.43it/s]3358it [00:00, 3481.31it/s]3701it [00:01, 3635.19it/s]3807it [00:01, 3699.91it/s]3710it [00:01, 3568.78it/s]3718it [00:01, 3514.72it/s]4066it [00:01, 3533.10it/s]4179it [00:01, 3590.09it/s]4069it [00:01, 3425.87it/s]4071it [00:01, 3423.80it/s]4429it [00:01, 3561.33it/s]4556it [00:01, 3641.42it/s]4440it [00:01, 3506.57it/s]4442it [00:01, 3505.80it/s]4786it [00:01, 3464.91it/s]4922it [00:01, 3530.37it/s]4793it [00:01, 3406.29it/s]4794it [00:01, 3391.22it/s]5167it [00:01, 3562.55it/s]5299it [00:01, 3597.19it/s]5150it [00:01, 3451.36it/s]5148it [00:01, 3433.14it/s]5538it [00:01, 3603.28it/s]5519it [00:01, 3518.10it/s]5660it [00:01, 3456.58it/s]5515it [00:01, 3501.88it/s]5900it [00:01, 3374.31it/s]6009it [00:01, 3464.74it/s]5872it [00:01, 3351.63it/s]5867it [00:01, 3334.15it/s]6250it [00:01, 3409.51it/s]6357it [00:01, 3437.27it/s]6221it [00:01, 3390.55it/s]6213it [00:01, 3329.42it/s]6594it [00:02, 2104.34it/s]6702it [00:02, 2056.47it/s]6562it [00:02, 2001.33it/s]6548it [00:02, 1983.33it/s]6946it [00:02, 2389.21it/s]7041it [00:02, 2321.41it/s]6893it [00:02, 2257.43it/s]6877it [00:02, 2238.98it/s]7299it [00:02, 2643.21it/s]7334it [00:02, 2452.37it/s]7241it [00:02, 2524.34it/s]7224it [00:02, 2507.73it/s]7613it [00:02, 2665.88it/s]7691it [00:02, 2719.84it/s]7547it [00:02, 2620.56it/s]7527it [00:02, 2600.12it/s]7965it [00:02, 2878.33it/s]8046it [00:02, 2930.73it/s]7898it [00:02, 2843.69it/s]7878it [00:02, 2827.29it/s]8282it [00:02, 2871.48it/s]8371it [00:02, 2920.74it/s]8215it [00:02, 2778.44it/s]8192it [00:02, 2760.31it/s]8637it [00:02, 3051.22it/s]8729it [00:02, 3097.64it/s]8563it [00:02, 2962.61it/s]8539it [00:02, 2946.12it/s]8990it [00:02, 3182.32it/s]9056it [00:02, 3072.59it/s]8912it [00:02, 3106.50it/s]8887it [00:02, 3091.82it/s]9321it [00:02, 3122.14it/s]9394it [00:02, 3158.13it/s]9237it [00:02, 3017.74it/s]9211it [00:02, 3006.23it/s]9662it [00:03, 3201.88it/s]9751it [00:03, 3273.33it/s]9586it [00:03, 3146.92it/s]9557it [00:03, 3132.16it/s]9989it [00:03, 3135.20it/s]10086it [00:03, 3189.43it/s]9909it [00:03, 3095.09it/s]9879it [00:03, 3066.49it/s]10328it [00:03, 3205.56it/s]10440it [00:03, 3288.00it/s]10256it [00:03, 3199.34it/s]10226it [00:03, 3179.33it/s]10680it [00:03, 3296.03it/s]10773it [00:03, 3164.60it/s]10591it [00:03, 3240.28it/s]10561it [00:03, 3227.70it/s]11013it [00:03, 3176.41it/s]11125it [00:03, 3265.23it/s]10919it [00:03, 3151.33it/s]10888it [00:03, 3132.43it/s]11365it [00:03, 3273.32it/s]11480it [00:03, 3346.39it/s]11270it [00:03, 3253.08it/s]11237it [00:03, 3232.78it/s]11695it [00:03, 3182.03it/s]11818it [00:03, 3204.26it/s]11598it [00:03, 3125.34it/s]11563it [00:03, 3093.77it/s]12037it [00:03, 3247.86it/s]12174it [00:03, 3303.82it/s]11946it [00:03, 3224.48it/s]11911it [00:03, 3203.17it/s]12386it [00:03, 3174.64it/s]12507it [00:03, 3222.60it/s]12298it [00:03, 3307.47it/s]12264it [00:03, 3295.48it/s]12737it [00:03, 3268.68it/s]12860it [00:03, 3309.07it/s]12631it [00:04, 3197.76it/s]12596it [00:04, 3178.95it/s]13079it [00:04, 3310.63it/s]13196it [00:04, 3322.35it/s]12967it [00:04, 3242.10it/s]12933it [00:04, 3233.18it/s]13368it [00:04, 3212.98it/s]
2022-08-10 15:16:07 | INFO | root | success load 13368 data
2022-08-10 15:16:07 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-08-10 15:16:07 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-08-10 15:16:07 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-08-10 15:16:07 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-08-10 15:16:07 | INFO | transformer.tokenization_utils | loading file None
2022-08-10 15:16:07 | INFO | transformer.tokenization_utils | loading file None
2022-08-10 15:16:07 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
13368it [00:04, 3193.16it/s]
13293it [00:04, 3158.48it/s]13259it [00:04, 3133.77it/s]13368it [00:04, 3139.21it/s]
13368it [00:04, 3121.47it/s]
2022-08-10 15:16:08 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-10 15:16:08 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-10 15:16:08 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-08-10 15:16:11 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-08-10 15:16:11 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-08-10 15:16:11 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-10 15:16:11 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-10 15:16:11 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
2022-08-10 15:16:13 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-08-10 15:16:13 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 668
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 404
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']Trained parameters: len 668
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 404
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']
2022-08-10 15:16:13 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-08-10 15:16:13 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-08-10 15:16:13 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 380755715)
2022-08-10 15:16:13 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 146472707)
2022-08-10 15:16:13 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 234283008)
Trained parameters: len 668
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 404
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']2022-08-10 15:16:14 | INFO | fairseq_cli.train | training on 4 GPUs
2022-08-10 15:16:14 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 4
Trained parameters: len 668
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 404
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']2022-08-10 15:16:17 | INFO | fairseq.trainer | loaded checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_last.pt (epoch 1 @ 1122 updates)
2022-08-10 15:16:17 | INFO | fairseq.trainer | loading train data for epoch 1
2022-08-10 15:16:17 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.source
2022-08-10 15:16:17 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.target
2022-08-10 15:16:17 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]368it [00:00, 3675.78it/s]743it [00:00, 3717.10it/s]1115it [00:00, 3401.21it/s]1483it [00:00, 3497.69it/s]1836it [00:00, 3403.04it/s]2196it [00:00, 3465.09it/s]2570it [00:00, 3550.91it/s]2927it [00:00, 3432.47it/s]3303it [00:00, 3520.59it/s]3657it [00:01, 3357.28it/s]4023it [00:01, 3444.11it/s]4370it [00:01, 3356.13it/s]4738it [00:01, 3448.06it/s]5085it [00:01, 3440.17it/s]5431it [00:01, 3280.31it/s]5798it [00:01, 3390.95it/s]6140it [00:01, 3310.34it/s]6491it [00:01, 3365.43it/s]6829it [00:02, 3266.72it/s]
start load cached examples train ...
0it [00:00, ?it/s]358it [00:00, 3579.06it/s]716it [00:00, 3574.50it/s]1074it [00:00, 3343.02it/s]
start load cached examples train ...
0it [00:00, ?it/s]1430it [00:00, 3392.05it/s]
start load cached examples train ...
0it [00:00, ?it/s]366it [00:00, 3655.26it/s]1771it [00:00, 3284.97it/s]364it [00:00, 3624.15it/s]737it [00:00, 3686.86it/s]2139it [00:00, 3410.81it/s]727it [00:00, 3615.64it/s]1106it [00:00, 3415.71it/s]2505it [00:00, 3489.07it/s]7158it [00:02, 1095.18it/s]1089it [00:00, 3388.49it/s]1461it [00:00, 3454.07it/s]7527it [00:02, 1408.23it/s]2855it [00:00, 3344.07it/s]1447it [00:00, 3459.40it/s]1808it [00:00, 3365.48it/s]7838it [00:03, 1658.14it/s]3225it [00:00, 3448.44it/s]1795it [00:00, 3360.75it/s]2181it [00:00, 3482.68it/s]8191it [00:03, 1982.41it/s]3572it [00:01, 3331.93it/s]2160it [00:00, 3452.60it/s]2540it [00:00, 3514.02it/s]8553it [00:03, 2245.05it/s]3916it [00:01, 3363.05it/s]2531it [00:00, 3533.36it/s]2893it [00:00, 3337.39it/s]8926it [00:03, 2565.12it/s]4282it [00:01, 3450.10it/s]2886it [00:00, 3360.89it/s]3263it [00:00, 3444.06it/s]9282it [00:03, 2799.26it/s]4629it [00:01, 3313.75it/s]3257it [00:00, 3464.65it/s]3610it [00:01, 3325.56it/s]9620it [00:03, 2895.38it/s]4988it [00:01, 3391.31it/s]3606it [00:01, 3323.61it/s]3963it [00:01, 3384.45it/s]9996it [00:03, 3119.33it/s]5329it [00:01, 3302.61it/s]3972it [00:01, 3420.38it/s]10341it [00:03, 3141.37it/s]4304it [00:01, 3279.18it/s]5688it [00:01, 3383.21it/s]4317it [00:01, 3294.89it/s]10703it [00:03, 3271.10it/s]4662it [00:01, 3364.72it/s]6028it [00:01, 3299.70it/s]4689it [00:01, 3415.21it/s]5020it [00:01, 3426.63it/s]11073it [00:03, 3248.07it/s]6387it [00:01, 3382.79it/s]5046it [00:01, 3459.99it/s]5364it [00:01, 3336.80it/s]11447it [00:04, 3384.89it/s]6751it [00:01, 3456.06it/s]5394it [00:01, 3349.28it/s]5720it [00:01, 3399.79it/s]11818it [00:04, 3474.87it/s]5750it [00:01, 3408.80it/s]6062it [00:01, 3299.75it/s]12173it [00:04, 3390.48it/s]6093it [00:01, 3321.09it/s]6423it [00:01, 3386.81it/s]12550it [00:04, 3497.44it/s]6449it [00:01, 3388.16it/s]6784it [00:01, 3450.72it/s]12904it [00:04, 3417.94it/s]6804it [00:02, 3263.76it/s]13294it [00:04, 3554.14it/s]13653it [00:04, 3424.66it/s]14035it [00:04, 3535.09it/s]14423it [00:04, 3634.64it/s]7098it [00:02, 1054.61it/s]14789it [00:04, 3480.45it/s]7449it [00:02, 1333.00it/s]15174it [00:05, 3585.14it/s]7753it [00:03, 1571.09it/s]15535it [00:05, 3472.30it/s]8120it [00:03, 1919.55it/s]15896it [00:05, 3510.95it/s]7131it [00:02, 1062.12it/s]8468it [00:03, 2218.42it/s]7133it [00:02, 1036.85it/s]7488it [00:02, 1348.69it/s]8792it [00:03, 2406.82it/s]7484it [00:02, 1316.90it/s]7780it [00:03, 1569.08it/s]9159it [00:03, 2700.36it/s]7782it [00:03, 1548.95it/s]8135it [00:03, 1899.14it/s]9492it [00:03, 2804.71it/s]8151it [00:03, 1904.39it/s]8491it [00:03, 2218.34it/s]9862it [00:03, 3035.27it/s]8523it [00:03, 2251.93it/s]8813it [00:03, 2417.79it/s]10216it [00:03, 3170.77it/s]8850it [00:03, 2412.41it/s]9167it [00:03, 2679.45it/s]10560it [00:03, 3156.57it/s]9219it [00:03, 2706.59it/s]9497it [00:03, 2766.57it/s]10924it [00:03, 3289.71it/s]9552it [00:03, 2807.20it/s]9858it [00:03, 2983.24it/s]11267it [00:04, 3225.18it/s]9913it [00:03, 3012.59it/s]10213it [00:03, 3136.14it/s]11633it [00:04, 3340.92it/s]10249it [00:03, 3035.09it/s]16249it [00:06, 957.22it/s] 10553it [00:03, 3065.25it/s]11975it [00:04, 3281.90it/s]10599it [00:03, 3161.76it/s]16635it [00:06, 1250.97it/s]10917it [00:03, 3223.17it/s]12348it [00:04, 3409.62it/s]10965it [00:04, 3299.44it/s]17023it [00:06, 1551.23it/s]11254it [00:04, 3157.56it/s]12709it [00:04, 3466.41it/s]11309it [00:04, 3237.97it/s]17389it [00:06, 1868.00it/s]11623it [00:04, 3304.76it/s]13059it [00:04, 3386.01it/s]11657it [00:04, 3305.34it/s]17770it [00:06, 2210.65it/s]11962it [00:04, 3255.85it/s]13438it [00:04, 3500.45it/s]11995it [00:04, 3249.12it/s]18112it [00:06, 2427.45it/s]12321it [00:04, 3349.92it/s]13791it [00:04, 3419.58it/s]12372it [00:04, 3396.20it/s]18489it [00:06, 2723.38it/s]12696it [00:04, 3464.05it/s]14154it [00:04, 3477.58it/s]12741it [00:04, 3469.19it/s]18838it [00:07, 2847.82it/s]13046it [00:04, 3358.43it/s]14504it [00:05, 3398.03it/s]13092it [00:04, 3396.85it/s]19212it [00:07, 3070.97it/s]13428it [00:04, 3489.62it/s]14881it [00:05, 3504.70it/s]13468it [00:04, 3500.30it/s]19562it [00:07, 3120.66it/s]13780it [00:04, 3377.32it/s]15258it [00:05, 3579.97it/s]13821it [00:04, 3407.88it/s]19925it [00:07, 3257.18it/s]14156it [00:04, 3486.30it/s]15618it [00:05, 3409.01it/s]14191it [00:04, 3481.98it/s]20302it [00:07, 3398.06it/s]14507it [00:05, 3409.51it/s]15981it [00:05, 3470.82it/s]14541it [00:05, 3389.65it/s]20659it [00:07, 3333.06it/s]14865it [00:05, 3457.97it/s]14904it [00:05, 3455.89it/s]21022it [00:07, 3416.29it/s]15250it [00:05, 3571.04it/s]15273it [00:05, 3388.76it/s]21373it [00:07, 3343.84it/s]15609it [00:05, 3420.68it/s]15645it [00:05, 3482.02it/s]21752it [00:07, 3470.50it/s]15982it [00:05, 3508.39it/s]16018it [00:05, 3553.45it/s]22105it [00:08, 3381.23it/s]22476it [00:08, 3474.94it/s]22857it [00:08, 3569.34it/s]23217it [00:08, 3448.45it/s]23574it [00:08, 3481.93it/s]16330it [00:06, 1028.14it/s]23925it [00:08, 3387.29it/s]16701it [00:06, 1320.39it/s]24302it [00:08, 3496.01it/s]17023it [00:06, 1574.07it/s]17394it [00:06, 1918.40it/s]24654it [00:08, 3366.60it/s]17769it [00:06, 2261.05it/s]25024it [00:08, 3460.33it/s]25395it [00:08, 3530.54it/s]18106it [00:06, 2449.72it/s]16335it [00:06, 894.06it/s] 18486it [00:06, 2757.78it/s]25750it [00:09, 3433.01it/s]16694it [00:06, 1151.39it/s]16375it [00:06, 869.96it/s] 26108it [00:09, 3473.00it/s]18830it [00:07, 2854.14it/s]17023it [00:06, 1408.00it/s]16760it [00:06, 1146.32it/s]19196it [00:07, 3037.33it/s]26457it [00:09, 3372.80it/s]17402it [00:06, 1756.50it/s]17083it [00:06, 1389.73it/s]26827it [00:09, 3465.83it/s]19543it [00:07, 3081.99it/s]17786it [00:06, 2109.20it/s]17463it [00:06, 1734.43it/s]19911it [00:07, 3243.18it/s]27175it [00:09, 3374.34it/s]18126it [00:07, 2342.18it/s]17844it [00:07, 2086.73it/s]20282it [00:07, 3373.10it/s]27555it [00:09, 3495.99it/s]18509it [00:07, 2665.95it/s]18188it [00:07, 2320.63it/s]27933it [00:09, 3578.64it/s]20635it [00:07, 3275.97it/s]18859it [00:07, 2782.09it/s]18569it [00:07, 2640.90it/s]21005it [00:07, 3393.76it/s]19237it [00:07, 3028.28it/s]18919it [00:07, 2777.20it/s]21353it [00:07, 3313.73it/s]19587it [00:07, 3091.93it/s]19299it [00:07, 3030.04it/s]21722it [00:07, 3420.21it/s]19954it [00:07, 3244.98it/s]19652it [00:07, 3075.27it/s]22069it [00:08, 3296.52it/s]20326it [00:07, 3374.84it/s]20031it [00:07, 3263.45it/s]22446it [00:08, 3429.25it/s]20683it [00:07, 3301.90it/s]20384it [00:07, 3228.27it/s]22823it [00:08, 3526.55it/s]21062it [00:07, 3437.25it/s]20759it [00:07, 3372.01it/s]23179it [00:08, 3405.49it/s]21416it [00:08, 3345.66it/s]21137it [00:07, 3485.54it/s]23532it [00:08, 3439.03it/s]21792it [00:08, 3462.39it/s]21497it [00:08, 3397.84it/s]23878it [00:08, 3338.33it/s]22145it [00:08, 3382.95it/s]21873it [00:08, 3498.13it/s]24245it [00:08, 3433.27it/s]28293it [00:10, 911.25it/s] 22517it [00:08, 3477.95it/s]22229it [00:08, 3399.36it/s]24591it [00:08, 3313.92it/s]28660it [00:10, 1177.26it/s]22899it [00:08, 3575.48it/s]22615it [00:08, 3529.69it/s]24954it [00:08, 3403.40it/s]28962it [00:10, 1399.15it/s]23260it [00:08, 3419.85it/s]22972it [00:08, 3431.79it/s]25320it [00:08, 3475.87it/s]29335it [00:11, 1742.28it/s]23633it [00:08, 3505.83it/s]23325it [00:08, 3459.72it/s]25670it [00:09, 3367.07it/s]29692it [00:11, 2012.74it/s]23698it [00:08, 3536.62it/s]23987it [00:08, 3380.48it/s]26019it [00:09, 3401.65it/s]30066it [00:11, 2348.79it/s]24365it [00:08, 3493.95it/s]24054it [00:08, 3428.72it/s]26361it [00:09, 3308.27it/s]30428it [00:11, 2625.34it/s]24429it [00:08, 3519.47it/s]24717it [00:08, 3370.47it/s]26727it [00:09, 3407.27it/s]30771it [00:11, 2754.78it/s]25087it [00:09, 3462.81it/s]24783it [00:09, 3402.78it/s]27100it [00:09, 3499.10it/s]31142it [00:11, 2992.42it/s]25150it [00:09, 3478.70it/s]25436it [00:09, 3351.69it/s]27452it [00:09, 3375.71it/s]31487it [00:11, 3019.60it/s]25813it [00:09, 3470.03it/s]25500it [00:09, 3365.82it/s]27819it [00:09, 3458.71it/s]31860it [00:11, 3208.19it/s]26170it [00:09, 3497.94it/s]25876it [00:09, 3476.23it/s]32213it [00:11, 3195.25it/s]26244it [00:09, 3533.81it/s]26522it [00:09, 3345.47it/s]32582it [00:12, 3329.78it/s]26893it [00:09, 3442.00it/s]26599it [00:09, 3410.91it/s]32935it [00:12, 3386.03it/s]26964it [00:09, 3479.23it/s]27240it [00:09, 3342.84it/s]33284it [00:12, 3296.31it/s]27620it [00:09, 3471.40it/s]27314it [00:09, 3385.63it/s]33655it [00:12, 3412.02it/s]27693it [00:09, 3499.12it/s]27969it [00:09, 3353.72it/s]34002it [00:12, 3311.38it/s]34375it [00:12, 3430.40it/s]34733it [00:12, 3329.71it/s]35088it [00:12, 3391.52it/s]35462it [00:12, 3490.32it/s]35814it [00:12, 3389.33it/s]28167it [00:10, 826.89it/s] 36185it [00:13, 3480.40it/s]28528it [00:11, 1078.14it/s]36535it [00:13, 3340.42it/s]28853it [00:11, 1312.49it/s]36903it [00:13, 3431.28it/s]29220it [00:11, 1640.11it/s]29581it [00:11, 1965.93it/s]37253it [00:13, 3343.12it/s]29908it [00:11, 2191.37it/s]37607it [00:13, 3398.84it/s]28045it [00:11, 832.57it/s] 30261it [00:11, 2475.03it/s]37976it [00:13, 3481.88it/s]28416it [00:11, 1091.23it/s]30592it [00:11, 2626.43it/s]38326it [00:13, 3381.67it/s]28307it [00:11, 725.26it/s] 28785it [00:11, 1386.81it/s]30953it [00:11, 2866.12it/s]38672it [00:13, 3392.62it/s]28675it [00:11, 964.10it/s]29099it [00:11, 1628.44it/s]31318it [00:11, 3069.63it/s]39013it [00:13, 3322.10it/s]28967it [00:11, 1166.45it/s]29464it [00:11, 1963.85it/s]39391it [00:14, 3452.69it/s]31662it [00:11, 3043.74it/s]29340it [00:11, 1496.50it/s]29792it [00:11, 2192.38it/s]39759it [00:14, 3517.99it/s]32028it [00:12, 3210.39it/s]29693it [00:11, 1777.60it/s]30165it [00:11, 2519.29it/s]32369it [00:12, 3180.76it/s]40112it [00:14, 3367.71it/s]30065it [00:11, 2124.27it/s]30503it [00:11, 2711.06it/s]32718it [00:12, 3265.45it/s]40481it [00:14, 3457.98it/s]30421it [00:11, 2415.84it/s]30840it [00:11, 2807.71it/s]33055it [00:12, 3200.78it/s]40829it [00:14, 3364.89it/s]30759it [00:12, 2592.32it/s]31214it [00:11, 3046.60it/s]33417it [00:12, 3318.06it/s]41188it [00:14, 3428.89it/s]31131it [00:12, 2861.99it/s]31556it [00:12, 3074.70it/s]33784it [00:12, 3419.28it/s]41533it [00:14, 3343.64it/s]31474it [00:12, 2912.72it/s]31929it [00:12, 3253.26it/s]34131it [00:12, 3302.59it/s]41904it [00:14, 3448.74it/s]31851it [00:12, 3134.67it/s]32275it [00:12, 3236.31it/s]34500it [00:12, 3411.67it/s]42279it [00:14, 3535.80it/s]32213it [00:12, 3162.03it/s]32624it [00:12, 3306.24it/s]34845it [00:12, 3305.64it/s]42634it [00:14, 3397.34it/s]32582it [00:12, 3305.64it/s]32986it [00:12, 3394.15it/s]35211it [00:13, 3404.69it/s]42995it [00:15, 3458.40it/s]32930it [00:12, 3342.61it/s]33333it [00:12, 3304.85it/s]35562it [00:13, 3434.02it/s]33277it [00:12, 3285.90it/s]33705it [00:12, 3423.27it/s]35908it [00:13, 3341.35it/s]33650it [00:12, 3411.36it/s]34052it [00:12, 3345.34it/s]36269it [00:13, 3417.92it/s]33998it [00:12, 3321.24it/s]34416it [00:12, 3427.86it/s]36613it [00:13, 3264.67it/s]34374it [00:13, 3445.82it/s]34762it [00:13, 3326.44it/s]36976it [00:13, 3366.32it/s]34733it [00:13, 3350.67it/s]35133it [00:13, 3435.58it/s]37315it [00:13, 3288.46it/s]35102it [00:13, 3445.29it/s]35502it [00:13, 3509.32it/s]37673it [00:13, 3368.93it/s]35464it [00:13, 3493.84it/s]35855it [00:13, 3365.32it/s]38024it [00:13, 3408.17it/s]35816it [00:13, 3395.62it/s]36225it [00:13, 3458.64it/s]38366it [00:13, 3318.07it/s]36187it [00:13, 3485.82it/s]36573it [00:13, 3347.00it/s]38721it [00:14, 3383.66it/s]36538it [00:13, 3374.03it/s]36944it [00:13, 3449.96it/s]39061it [00:14, 3287.81it/s]36901it [00:13, 3420.02it/s]37291it [00:13, 3314.44it/s]39418it [00:14, 3368.49it/s]37254it [00:13, 3330.51it/s]37657it [00:13, 3411.35it/s]39774it [00:14, 3270.94it/s]37619it [00:14, 3420.95it/s]38026it [00:13, 3489.43it/s]40138it [00:14, 3373.38it/s]37990it [00:14, 3502.32it/s]38377it [00:14, 3376.60it/s]40504it [00:14, 3455.30it/s]38342it [00:14, 3403.73it/s]38725it [00:14, 3405.51it/s]40851it [00:14, 3305.05it/s]38703it [00:14, 3461.45it/s]39067it [00:14, 3323.30it/s]43343it [00:16, 592.55it/s] 41219it [00:14, 3410.44it/s]39439it [00:14, 3436.37it/s]39051it [00:14, 3258.48it/s]43711it [00:16, 797.43it/s]41563it [00:14, 3304.17it/s]39425it [00:14, 3391.82it/s]39784it [00:14, 3338.69it/s]44041it [00:17, 1010.36it/s]41913it [00:15, 3359.08it/s]40137it [00:14, 3391.84it/s]39774it [00:14, 3320.41it/s]44403it [00:17, 1296.45it/s]42287it [00:15, 3467.09it/s]40509it [00:14, 3486.14it/s]40145it [00:14, 3430.30it/s]44774it [00:17, 1623.22it/s]42636it [00:15, 3365.74it/s]40515it [00:14, 3506.16it/s]40859it [00:14, 3363.59it/s]45106it [00:17, 1884.50it/s]42994it [00:15, 3418.30it/s]41234it [00:14, 3472.28it/s]40868it [00:14, 3395.45it/s]45474it [00:17, 2219.28it/s]41243it [00:15, 3497.05it/s]45814it [00:17, 2421.45it/s]41583it [00:15, 3359.59it/s]46163it [00:17, 2663.98it/s]41936it [00:15, 3406.79it/s]41595it [00:15, 3317.54it/s]46531it [00:17, 2913.06it/s]41969it [00:15, 3434.44it/s]42292it [00:15, 3332.31it/s]46877it [00:17, 2964.97it/s]42665it [00:15, 3445.22it/s]42316it [00:15, 3361.76it/s]47240it [00:17, 3139.11it/s]43031it [00:15, 3505.56it/s]42692it [00:15, 3473.51it/s]43056it [00:15, 3520.49it/s]47584it [00:18, 3140.28it/s]47954it [00:18, 3292.72it/s]48299it [00:18, 3251.50it/s]48659it [00:18, 3348.83it/s]49033it [00:18, 3460.34it/s]49386it [00:18, 3363.05it/s]49756it [00:18, 3458.19it/s]50106it [00:18, 3308.95it/s]50475it [00:18, 3414.76it/s]50820it [00:19, 3319.58it/s]51193it [00:19, 3435.16it/s]51562it [00:19, 3507.87it/s]43338it [00:17, 562.05it/s] 43700it [00:17, 757.29it/s]51915it [00:19, 3390.77it/s]52286it [00:19, 3480.87it/s]44043it [00:17, 974.40it/s]44400it [00:17, 1249.55it/s]52636it [00:19, 3376.87it/s]44769it [00:17, 1570.43it/s]53006it [00:19, 3467.04it/s]43410it [00:17, 633.48it/s] 45098it [00:17, 1829.86it/s]43383it [00:17, 593.36it/s] 53355it [00:19, 3340.99it/s]43778it [00:17, 846.32it/s]45464it [00:17, 2164.75it/s]43752it [00:17, 797.83it/s]53720it [00:19, 3428.35it/s]44097it [00:17, 1058.81it/s]44062it [00:17, 996.30it/s]45801it [00:17, 2371.30it/s]54096it [00:19, 3522.97it/s]44473it [00:17, 1368.42it/s]44441it [00:17, 1302.26it/s]46155it [00:17, 2633.28it/s]54450it [00:20, 3419.63it/s]44832it [00:17, 1681.22it/s]44812it [00:17, 1627.11it/s]46519it [00:18, 2877.32it/s]54808it [00:20, 3464.29it/s]45164it [00:17, 1940.14it/s]45147it [00:17, 1875.68it/s]46864it [00:18, 2934.10it/s]55156it [00:20, 3356.76it/s]45537it [00:17, 2282.39it/s]45515it [00:17, 2209.94it/s]47238it [00:18, 3145.68it/s]55517it [00:20, 3428.69it/s]45878it [00:17, 2478.71it/s]45853it [00:17, 2411.25it/s]47584it [00:18, 3135.34it/s]55862it [00:20, 3332.23it/s]46229it [00:18, 2716.00it/s]46216it [00:18, 2686.74it/s]47951it [00:18, 3280.18it/s]56217it [00:20, 3394.09it/s]46567it [00:18, 2821.06it/s]46562it [00:18, 2797.42it/s]48296it [00:18, 3233.67it/s]56584it [00:20, 3474.29it/s]46937it [00:18, 3046.14it/s]46912it [00:18, 2975.68it/s]48666it [00:18, 3362.67it/s]56933it [00:20, 3376.80it/s]47315it [00:18, 3242.93it/s]47288it [00:18, 3183.10it/s]49029it [00:18, 3438.27it/s]57309it [00:20, 3486.75it/s]47668it [00:18, 3181.59it/s]47637it [00:18, 3169.20it/s]49380it [00:18, 3339.67it/s]57659it [00:21, 3382.36it/s]48035it [00:18, 3313.61it/s]48008it [00:18, 3316.63it/s]49752it [00:19, 3447.62it/s]58012it [00:21, 3422.72it/s]48382it [00:18, 3280.94it/s]48356it [00:18, 3266.52it/s]50101it [00:19, 3343.03it/s]58356it [00:21, 3320.70it/s]48756it [00:18, 3409.52it/s]48713it [00:18, 3350.36it/s]50465it [00:19, 3426.53it/s]58725it [00:21, 3425.03it/s]49106it [00:18, 3336.47it/s]49082it [00:18, 3300.97it/s]50811it [00:19, 3327.34it/s]59089it [00:21, 3484.99it/s]49477it [00:19, 3442.57it/s]49450it [00:18, 3405.16it/s]51181it [00:19, 3431.79it/s]59439it [00:21, 3363.23it/s]49833it [00:19, 3475.42it/s]49824it [00:19, 3500.12it/s]51534it [00:19, 3455.02it/s]59800it [00:21, 3432.43it/s]50184it [00:19, 3373.51it/s]50178it [00:19, 3379.04it/s]51882it [00:19, 3351.65it/s]60145it [00:21, 3268.24it/s]50553it [00:19, 3463.24it/s]50528it [00:19, 3411.95it/s]52245it [00:19, 3430.93it/s]60511it [00:21, 3379.05it/s]50902it [00:19, 3367.13it/s]50872it [00:19, 3317.36it/s]52590it [00:19, 3335.52it/s]60852it [00:21, 3302.19it/s]51273it [00:19, 3464.53it/s]51248it [00:19, 3442.28it/s]52957it [00:19, 3429.91it/s]61225it [00:22, 3423.96it/s]51622it [00:19, 3327.53it/s]51602it [00:19, 3338.80it/s]53302it [00:20, 3326.73it/s]61590it [00:22, 3489.32it/s]51993it [00:19, 3435.82it/s]51972it [00:19, 3441.66it/s]53661it [00:20, 3402.30it/s]52365it [00:19, 3516.99it/s]52329it [00:19, 3476.13it/s]54015it [00:20, 3440.69it/s]52719it [00:19, 3403.05it/s]52679it [00:19, 3355.98it/s]54361it [00:20, 3345.62it/s]53088it [00:20, 3484.61it/s]53052it [00:20, 3461.24it/s]54724it [00:20, 3427.78it/s]53439it [00:20, 3355.53it/s]53400it [00:20, 3349.27it/s]55068it [00:20, 3310.45it/s]53807it [00:20, 3447.61it/s]53763it [00:20, 3429.47it/s]55425it [00:20, 3383.52it/s]54154it [00:20, 3370.19it/s]55787it [00:20, 3450.84it/s]54122it [00:20, 3301.80it/s]54529it [00:20, 3476.61it/s]56134it [00:20, 3346.25it/s]54496it [00:20, 3423.31it/s]54883it [00:20, 3493.77it/s]56501it [00:21, 3438.00it/s]54865it [00:20, 3499.73it/s]55234it [00:20, 3334.48it/s]56847it [00:21, 3334.83it/s]55217it [00:20, 3372.98it/s]55597it [00:20, 3417.07it/s]57201it [00:21, 3391.50it/s]55581it [00:20, 3448.73it/s]55941it [00:20, 3332.07it/s]57542it [00:21, 3297.94it/s]55928it [00:20, 3292.18it/s]56301it [00:21, 3406.60it/s]57906it [00:21, 3395.63it/s]56304it [00:21, 3422.24it/s]56644it [00:21, 3324.05it/s]58265it [00:21, 3452.18it/s]56649it [00:21, 3327.06it/s]57019it [00:21, 3444.91it/s]58612it [00:21, 3333.72it/s]57024it [00:21, 3445.42it/s]57390it [00:21, 3521.56it/s]58973it [00:21, 3411.78it/s]57396it [00:21, 3523.29it/s]57744it [00:21, 3374.54it/s]59316it [00:21, 3300.62it/s]57751it [00:21, 3359.47it/s]58114it [00:21, 3457.62it/s]59675it [00:21, 3382.20it/s]58116it [00:21, 3439.78it/s]58462it [00:21, 3357.85it/s]60015it [00:22, 3127.39it/s]58463it [00:21, 3329.11it/s]58826it [00:21, 3436.50it/s]61941it [00:24, 517.48it/s] 60379it [00:22, 3268.65it/s]58829it [00:21, 3421.72it/s]62311it [00:24, 702.83it/s]59172it [00:21, 3338.17it/s]60744it [00:22, 3374.91it/s]59174it [00:21, 3318.20it/s]62623it [00:24, 888.74it/s]59521it [00:21, 3380.61it/s]61086it [00:22, 3296.14it/s]59518it [00:21, 3352.19it/s]62974it [00:24, 1148.21it/s]59888it [00:22, 3464.35it/s]61451it [00:22, 3394.78it/s]59885it [00:22, 3443.32it/s]63346it [00:24, 1466.81it/s]60236it [00:22, 3353.31it/s]60231it [00:22, 3331.03it/s]63675it [00:24, 1723.44it/s]60607it [00:22, 3454.16it/s]60600it [00:22, 3433.63it/s]64044it [00:24, 2067.86it/s]60954it [00:22, 3316.09it/s]60945it [00:22, 3328.55it/s]64381it [00:24, 2267.74it/s]61328it [00:22, 3435.83it/s]61302it [00:22, 3395.82it/s]64749it [00:25, 2574.54it/s]61684it [00:22, 3320.98it/s]61667it [00:22, 3469.25it/s]65111it [00:25, 2712.33it/s]65469it [00:25, 2924.41it/s]65819it [00:25, 3072.75it/s]66161it [00:25, 3075.52it/s]66524it [00:25, 3225.06it/s]66865it [00:25, 3188.55it/s]67219it [00:25, 3286.66it/s]67584it [00:25, 3388.68it/s]67931it [00:26, 3311.78it/s]68294it [00:26, 3402.30it/s]68639it [00:26, 3264.72it/s]69004it [00:26, 3373.39it/s]69345it [00:26, 3296.86it/s]61793it [00:24, 543.34it/s] 69708it [00:26, 3391.89it/s]62163it [00:24, 739.12it/s]70064it [00:26, 3439.61it/s]62531it [00:24, 978.49it/s]70410it [00:26, 3342.58it/s]62839it [00:24, 1197.16it/s]70775it [00:26, 3430.59it/s]63202it [00:24, 1512.07it/s]71120it [00:26, 3323.34it/s]63527it [00:24, 1767.62it/s]71474it [00:27, 3384.62it/s]63897it [00:25, 2116.64it/s]71831it [00:27, 3309.99it/s]64263it [00:25, 2433.17it/s]72195it [00:27, 3402.96it/s]64607it [00:25, 2598.11it/s]62019it [00:24, 472.66it/s] 72559it [00:27, 3471.07it/s]64973it [00:25, 2849.60it/s]62387it [00:24, 647.04it/s]72908it [00:27, 3368.09it/s]62683it [00:25, 813.71it/s]65316it [00:25, 2924.88it/s]62016it [00:25, 443.23it/s] 73271it [00:27, 3442.18it/s]63048it [00:25, 1078.34it/s]65661it [00:25, 3061.84it/s]62386it [00:25, 608.42it/s]73617it [00:27, 3361.94it/s]63418it [00:25, 1386.53it/s]65998it [00:25, 3079.95it/s]62688it [00:25, 771.34it/s]73995it [00:27, 3481.19it/s]63744it [00:25, 1642.73it/s]66362it [00:25, 3233.08it/s]63055it [00:25, 1026.37it/s]74349it [00:27, 3495.84it/s]64098it [00:25, 1963.97it/s]66725it [00:25, 3344.31it/s]63425it [00:25, 1324.70it/s]74700it [00:27, 3387.56it/s]64429it [00:25, 2194.94it/s]67072it [00:25, 3286.01it/s]63755it [00:25, 1579.63it/s]75063it [00:28, 3454.93it/s]64800it [00:25, 2520.98it/s]67436it [00:26, 3384.33it/s]64124it [00:25, 1922.98it/s]75410it [00:28, 3346.61it/s]65139it [00:25, 2669.91it/s]67781it [00:26, 3313.29it/s]64461it [00:25, 2156.91it/s]75761it [00:28, 3393.55it/s]65503it [00:25, 2909.05it/s]68143it [00:26, 3400.64it/s]64831it [00:25, 2475.41it/s]76102it [00:28, 3290.63it/s]65870it [00:25, 3106.08it/s]68487it [00:26, 3268.87it/s]65171it [00:25, 2618.13it/s]76466it [00:28, 3390.43it/s]66218it [00:26, 3075.09it/s]68854it [00:26, 3382.48it/s]65532it [00:26, 2857.20it/s]76831it [00:28, 3464.21it/s]66583it [00:26, 3229.02it/s]69218it [00:26, 3456.28it/s]65898it [00:26, 3063.43it/s]77179it [00:28, 3310.21it/s]66926it [00:26, 3206.25it/s]69567it [00:26, 3347.50it/s]66245it [00:26, 3063.16it/s]77548it [00:28, 3417.98it/s]67294it [00:26, 3336.50it/s]69935it [00:26, 3442.64it/s]66613it [00:26, 3229.62it/s]77892it [00:28, 3325.06it/s]67639it [00:26, 3270.85it/s]70282it [00:26, 3352.98it/s]66958it [00:26, 3193.62it/s]78258it [00:29, 3420.39it/s]67995it [00:26, 3350.32it/s]70644it [00:27, 3429.65it/s]67325it [00:26, 3326.15it/s]78602it [00:29, 3326.72it/s]68360it [00:26, 3434.37it/s]70993it [00:27, 3304.53it/s]67670it [00:26, 3254.02it/s]78952it [00:29, 3376.32it/s]68708it [00:26, 3340.71it/s]71339it [00:27, 3347.41it/s]68026it [00:26, 3338.78it/s]79315it [00:29, 3449.50it/s]69073it [00:26, 3427.19it/s]71700it [00:27, 3422.84it/s]68391it [00:26, 3427.29it/s]79662it [00:29, 3338.53it/s]69419it [00:27, 3333.66it/s]72044it [00:27, 3321.74it/s]68739it [00:26, 3329.44it/s]80030it [00:29, 3435.97it/s]69776it [00:27, 3379.31it/s]72408it [00:27, 3412.78it/s]69101it [00:27, 3411.76it/s]80375it [00:29, 3306.55it/s]70150it [00:27, 3482.63it/s]72751it [00:27, 3324.79it/s]69446it [00:27, 3317.22it/s]80747it [00:29, 3422.28it/s]70500it [00:27, 3372.63it/s]73125it [00:27, 3443.60it/s]69811it [00:27, 3411.61it/s]81091it [00:29, 3323.45it/s]70862it [00:27, 3443.69it/s]73494it [00:27, 3515.02it/s]70155it [00:27, 3327.51it/s]81454it [00:29, 3409.19it/s]71208it [00:27, 3339.85it/s]73847it [00:27, 3412.71it/s]70523it [00:27, 3427.24it/s]81821it [00:30, 3482.40it/s]71574it [00:27, 3431.69it/s]74215it [00:28, 3487.34it/s]70877it [00:27, 3460.04it/s]82171it [00:30, 3337.24it/s]71919it [00:27, 3296.44it/s]74565it [00:28, 3341.58it/s]71225it [00:27, 3336.68it/s]82537it [00:30, 3426.82it/s]72282it [00:27, 3389.65it/s]74928it [00:28, 3422.80it/s]71592it [00:27, 3430.16it/s]82882it [00:30, 3335.13it/s]72648it [00:27, 3467.65it/s]75272it [00:28, 3322.35it/s]71937it [00:27, 3329.98it/s]83245it [00:30, 3417.79it/s]72997it [00:28, 3390.18it/s]75636it [00:28, 3411.84it/s]72300it [00:28, 3415.29it/s]83592it [00:30, 3287.62it/s]73362it [00:28, 3463.70it/s]75995it [00:28, 3462.97it/s]72668it [00:28, 3490.84it/s]83961it [00:30, 3399.61it/s]73710it [00:28, 3342.33it/s]73019it [00:28, 3393.55it/s]76343it [00:28, 3333.59it/s]84326it [00:30, 3471.86it/s]74082it [00:28, 3448.53it/s]73379it [00:28, 3452.00it/s]76708it [00:28, 3423.66it/s]84675it [00:30, 3324.77it/s]74429it [00:28, 3345.80it/s]77052it [00:28, 3266.83it/s]73726it [00:28, 3216.20it/s]85041it [00:31, 3418.19it/s]74795it [00:28, 3434.18it/s]77418it [00:29, 3378.09it/s]74101it [00:28, 3363.67it/s]75157it [00:28, 3487.17it/s]77759it [00:29, 3286.23it/s]74441it [00:28, 3276.98it/s]75507it [00:28, 3366.26it/s]78125it [00:29, 3391.72it/s]74807it [00:28, 3385.19it/s]75858it [00:28, 3405.73it/s]78488it [00:29, 3460.43it/s]75170it [00:28, 3455.09it/s]76200it [00:29, 3296.83it/s]78836it [00:29, 3338.86it/s]75518it [00:28, 3330.96it/s]76563it [00:29, 3390.93it/s]79200it [00:29, 3423.11it/s]75885it [00:29, 3426.09it/s]76904it [00:29, 3294.74it/s]79544it [00:29, 3275.33it/s]76230it [00:29, 3297.04it/s]77270it [00:29, 3398.35it/s]79913it [00:29, 3390.85it/s]76592it [00:29, 3382.46it/s]77619it [00:29, 3423.34it/s]80255it [00:29, 3293.47it/s]76933it [00:29, 3279.60it/s]77963it [00:29, 3322.33it/s]80625it [00:29, 3407.83it/s]77297it [00:29, 3382.34it/s]78330it [00:29, 3421.09it/s]80988it [00:30, 3470.92it/s]77637it [00:29, 3357.86it/s]78674it [00:29, 3320.35it/s]81337it [00:30, 3351.75it/s]77974it [00:29, 3257.58it/s]79037it [00:29, 3406.79it/s]81697it [00:30, 3421.87it/s]78342it [00:29, 3369.00it/s]79393it [00:29, 3305.24it/s]82041it [00:30, 3320.89it/s]78681it [00:29, 3280.79it/s]79743it [00:30, 3353.35it/s]82390it [00:30, 3369.35it/s]79042it [00:30, 3368.03it/s]80108it [00:30, 3436.33it/s]82753it [00:30, 3266.22it/s]79392it [00:30, 3273.20it/s]80453it [00:30, 3341.14it/s]83113it [00:30, 3359.30it/s]79758it [00:30, 3382.04it/s]80822it [00:30, 3440.95it/s]83475it [00:30, 3433.60it/s]80123it [00:30, 3458.33it/s]81168it [00:30, 3321.16it/s]83820it [00:30, 3329.31it/s]80471it [00:30, 3347.77it/s]81527it [00:30, 3396.12it/s]84183it [00:31, 3415.28it/s]80839it [00:30, 3442.03it/s]81894it [00:30, 3475.37it/s]84526it [00:31, 3309.47it/s]81185it [00:30, 3264.89it/s]82243it [00:30, 3354.99it/s]84879it [00:31, 3371.69it/s]81548it [00:30, 3366.62it/s]82606it [00:30, 3432.36it/s]85240it [00:31, 3438.84it/s]81912it [00:30, 3274.77it/s]82951it [00:31, 3329.66it/s]82277it [00:31, 3378.89it/s]83313it [00:31, 3410.08it/s]82642it [00:31, 3453.89it/s]83656it [00:31, 3315.09it/s]82990it [00:31, 3334.08it/s]84021it [00:31, 3409.31it/s]83354it [00:31, 3418.76it/s]84386it [00:31, 3479.14it/s]83698it [00:31, 3310.30it/s]84736it [00:31, 3331.01it/s]85385it [00:34, 366.76it/s] 84063it [00:31, 3405.92it/s]85099it [00:31, 3416.50it/s]85752it [00:34, 506.75it/s]84408it [00:31, 3417.25it/s]86122it [00:34, 688.98it/s]84751it [00:31, 3290.87it/s]86430it [00:34, 868.97it/s]85108it [00:31, 3369.17it/s]86795it [00:34, 1139.10it/s]87120it [00:34, 1378.81it/s]87494it [00:34, 1724.73it/s]87861it [00:34, 2007.05it/s]88228it [00:34, 2329.78it/s]88588it [00:34, 2605.15it/s]88934it [00:35, 2738.37it/s]89296it [00:35, 2956.81it/s]89640it [00:35, 2994.68it/s]89993it [00:35, 3136.07it/s]90347it [00:35, 3246.93it/s]90691it [00:35, 3174.27it/s]91047it [00:35, 3281.28it/s]91386it [00:35, 3193.00it/s]91723it [00:35, 3241.00it/s]92062it [00:36, 3170.71it/s]85586it [00:34, 402.45it/s] 92416it [00:36, 3274.59it/s]85948it [00:34, 552.50it/s]92771it [00:36, 3353.24it/s]86257it [00:34, 711.01it/s]93110it [00:36, 3253.75it/s]86608it [00:34, 940.18it/s]93462it [00:36, 3320.07it/s]86967it [00:34, 1217.65it/s]93796it [00:36, 3228.82it/s]87290it [00:34, 1471.29it/s]94150it [00:36, 3316.57it/s]87654it [00:34, 1809.67it/s]94506it [00:36, 3385.38it/s]87987it [00:34, 2062.58it/s]85443it [00:34, 400.46it/s] 94846it [00:36, 3258.22it/s]88352it [00:34, 2383.43it/s]85810it [00:34, 551.64it/s]95189it [00:36, 3307.05it/s]88702it [00:34, 2576.55it/s]86156it [00:34, 731.77it/s]85447it [00:34, 397.38it/s] 95522it [00:37, 3210.19it/s]89054it [00:35, 2802.11it/s]86462it [00:34, 920.92it/s]85812it [00:34, 548.88it/s]95875it [00:37, 3301.45it/s]89416it [00:35, 3009.40it/s]86824it [00:34, 1201.48it/s]96229it [00:37, 3368.58it/s]86181it [00:34, 736.75it/s]89761it [00:35, 3040.16it/s]87144it [00:34, 1452.51it/s]86546it [00:34, 972.76it/s]96568it [00:37, 3250.74it/s]90120it [00:35, 3188.08it/s]87512it [00:34, 1797.35it/s]86905it [00:34, 1244.05it/s]96911it [00:37, 3300.05it/s]90462it [00:35, 3132.79it/s]87862it [00:35, 2062.33it/s]87231it [00:35, 1489.65it/s]97243it [00:37, 3199.40it/s]90811it [00:35, 3230.48it/s]88226it [00:35, 2381.21it/s]87599it [00:35, 1827.72it/s]97595it [00:37, 3290.76it/s]91161it [00:35, 3304.63it/s]88598it [00:35, 2681.14it/s]87933it [00:35, 2069.58it/s]97941it [00:37, 3200.55it/s]88946it [00:35, 2796.88it/s]91501it [00:35, 3206.53it/s]88281it [00:35, 2350.98it/s]98292it [00:37, 3287.36it/s]89308it [00:35, 3004.35it/s]91836it [00:35, 3247.07it/s]88654it [00:35, 2659.66it/s]98631it [00:38, 3315.71it/s]92166it [00:35, 3162.55it/s]89653it [00:35, 3012.67it/s]88998it [00:35, 2769.73it/s]98964it [00:38, 3214.90it/s]92517it [00:36, 3260.28it/s]90017it [00:35, 3179.43it/s]89362it [00:35, 2988.69it/s]99319it [00:38, 3310.08it/s]92870it [00:36, 3336.87it/s]90374it [00:35, 3285.27it/s]89705it [00:35, 3011.55it/s]99652it [00:38, 3210.08it/s]93207it [00:36, 3227.65it/s]90720it [00:35, 3197.76it/s]90066it [00:35, 3171.93it/s]100005it [00:38, 3301.51it/s]93559it [00:36, 3309.52it/s]91072it [00:36, 3286.80it/s]90407it [00:36, 3115.26it/s]100358it [00:38, 3366.71it/s]93892it [00:36, 3206.22it/s]91411it [00:36, 3191.49it/s]90758it [00:36, 3222.35it/s]100696it [00:38, 3211.22it/s]94232it [00:36, 3261.68it/s]91763it [00:36, 3281.48it/s]91109it [00:36, 3302.66it/s]101050it [00:38, 3304.14it/s]94583it [00:36, 3177.22it/s]92097it [00:36, 3183.10it/s]91449it [00:36, 3148.39it/s]101383it [00:38, 3207.89it/s]94934it [00:36, 3270.20it/s]92435it [00:36, 3238.22it/s]91799it [00:36, 3246.09it/s]101732it [00:38, 3287.31it/s]95285it [00:36, 3338.56it/s]92788it [00:36, 3321.45it/s]92130it [00:36, 3152.71it/s]102086it [00:39, 3359.26it/s]95621it [00:37, 3227.78it/s]93123it [00:36, 3216.30it/s]92481it [00:36, 3252.65it/s]102424it [00:39, 3241.95it/s]95971it [00:37, 3304.53it/s]93476it [00:36, 3304.17it/s]92830it [00:36, 3320.04it/s]102753it [00:39, 3255.50it/s]96303it [00:37, 3202.35it/s]93809it [00:36, 3206.67it/s]93165it [00:36, 3200.75it/s]103080it [00:39, 3173.53it/s]96640it [00:37, 3249.10it/s]94161it [00:36, 3294.96it/s]93517it [00:36, 3290.64it/s]103433it [00:39, 3274.82it/s]96992it [00:37, 3325.51it/s]94513it [00:37, 3358.33it/s]103787it [00:39, 3349.46it/s]93849it [00:37, 3178.27it/s]97326it [00:37, 3218.06it/s]94851it [00:37, 3243.21it/s]94199it [00:37, 3269.70it/s]104123it [00:39, 3239.41it/s]97677it [00:37, 3300.90it/s]95202it [00:37, 3318.23it/s]94535it [00:37, 3294.45it/s]104474it [00:39, 3315.96it/s]98009it [00:37, 3205.94it/s]95536it [00:37, 3203.62it/s]94866it [00:37, 3178.05it/s]104807it [00:39, 3155.61it/s]98360it [00:37, 3292.33it/s]95871it [00:37, 3243.38it/s]95216it [00:37, 3269.00it/s]105159it [00:40, 3257.25it/s]98712it [00:37, 3356.98it/s]96223it [00:37, 3321.19it/s]95545it [00:37, 3162.73it/s]105501it [00:40, 3174.99it/s]99049it [00:38, 3239.17it/s]96557it [00:37, 3209.74it/s]95895it [00:37, 3256.85it/s]105855it [00:40, 3277.46it/s]99387it [00:38, 3279.47it/s]96908it [00:37, 3295.06it/s]96246it [00:37, 3328.28it/s]106208it [00:40, 3348.39it/s]99717it [00:38, 3181.35it/s]97239it [00:37, 3194.14it/s]96581it [00:37, 3198.40it/s]106545it [00:40, 3245.54it/s]100068it [00:38, 3274.84it/s]97591it [00:38, 3287.10it/s]96931it [00:38, 3283.46it/s]106885it [00:40, 3287.43it/s]100419it [00:38, 3341.99it/s]97943it [00:38, 3184.01it/s]97262it [00:38, 3128.55it/s]107216it [00:40, 3195.18it/s]100755it [00:38, 3228.88it/s]98295it [00:38, 3276.86it/s]97611it [00:38, 3230.60it/s]107569it [00:40, 3290.28it/s]101106it [00:38, 3307.86it/s]98645it [00:38, 3340.33it/s]107925it [00:40, 3366.09it/s]97942it [00:38, 3135.46it/s]101439it [00:38, 3203.89it/s]98981it [00:38, 3230.27it/s]98292it [00:38, 3235.29it/s]108263it [00:40, 3253.52it/s]101771it [00:38, 3236.16it/s]99317it [00:38, 3266.81it/s]98642it [00:38, 3307.07it/s]108603it [00:41, 3295.09it/s]102121it [00:39, 3312.13it/s]99645it [00:38, 3168.25it/s]98975it [00:38, 3197.29it/s]108934it [00:41, 3208.65it/s]102454it [00:39, 3204.33it/s]99997it [00:38, 3267.12it/s]99327it [00:38, 3287.56it/s]109288it [00:41, 3303.51it/s]102804it [00:39, 3289.34it/s]100348it [00:38, 3336.57it/s]109643it [00:41, 3373.71it/s]99658it [00:38, 3173.52it/s]103135it [00:39, 3191.24it/s]100683it [00:38, 3220.28it/s]100009it [00:38, 3267.19it/s]109982it [00:41, 3255.20it/s]103486it [00:39, 3280.38it/s]101034it [00:39, 3303.05it/s]100343it [00:39, 3287.24it/s]110325it [00:41, 3305.24it/s]103823it [00:39, 3182.61it/s]101366it [00:39, 3203.57it/s]110657it [00:41, 3208.79it/s]100673it [00:39, 3174.51it/s]104163it [00:39, 3242.22it/s]101713it [00:39, 3279.22it/s]111012it [00:41, 3304.59it/s]101023it [00:39, 3267.30it/s]104511it [00:39, 3309.06it/s]102066it [00:39, 3349.53it/s]111368it [00:41, 3377.29it/s]101352it [00:39, 3170.16it/s]104844it [00:39, 3203.47it/s]102403it [00:39, 3188.56it/s]101698it [00:39, 3252.31it/s]111707it [00:42, 3259.74it/s]105193it [00:39, 3284.79it/s]102755it [00:39, 3280.97it/s]102050it [00:39, 3328.49it/s]112046it [00:42, 3295.54it/s]105523it [00:40, 3181.42it/s]103086it [00:39, 3185.55it/s]112377it [00:42, 3202.31it/s]102385it [00:39, 3202.66it/s]105874it [00:40, 3273.76it/s]103437it [00:39, 3278.07it/s]112730it [00:42, 3296.31it/s]102735it [00:39, 3286.04it/s]106224it [00:40, 3337.00it/s]103787it [00:39, 3340.19it/s]113061it [00:42, 3206.63it/s]103066it [00:39, 3179.48it/s]106559it [00:40, 3222.96it/s]104123it [00:40, 3222.12it/s]113403it [00:42, 3266.25it/s]103402it [00:40, 3223.16it/s]106898it [00:40, 3269.95it/s]104472it [00:40, 3298.75it/s]113756it [00:42, 3340.80it/s]103752it [00:40, 3296.29it/s]107227it [00:40, 3172.82it/s]104804it [00:40, 3196.87it/s]114092it [00:42, 3235.49it/s]104083it [00:40, 3161.82it/s]107577it [00:40, 3265.11it/s]105153it [00:40, 3277.73it/s]114448it [00:42, 3328.13it/s]104430it [00:40, 3248.20it/s]107929it [00:40, 3338.12it/s]105503it [00:40, 3180.62it/s]114783it [00:42, 3220.36it/s]104757it [00:40, 3150.28it/s]108264it [00:40, 3224.61it/s]105840it [00:40, 3232.34it/s]105106it [00:40, 3246.40it/s]108617it [00:41, 3311.20it/s]106191it [00:40, 3312.02it/s]105455it [00:40, 3316.33it/s]108950it [00:41, 3208.29it/s]106524it [00:40, 3208.51it/s]105788it [00:40, 3191.91it/s]109287it [00:41, 3252.49it/s]106876it [00:40, 3297.15it/s]106138it [00:40, 3278.68it/s]109639it [00:41, 3328.80it/s]107208it [00:40, 3198.03it/s]106468it [00:40, 3140.73it/s]109973it [00:41, 3222.83it/s]107560it [00:41, 3287.71it/s]106821it [00:41, 3249.27it/s]110326it [00:41, 3310.59it/s]107913it [00:41, 3355.84it/s]107171it [00:41, 3320.77it/s]110659it [00:41, 3203.02it/s]108250it [00:41, 3239.70it/s]107505it [00:41, 3204.63it/s]111011it [00:41, 3292.97it/s]108604it [00:41, 3325.82it/s]107857it [00:41, 3294.62it/s]111364it [00:41, 3360.29it/s]108939it [00:41, 3224.20it/s]108189it [00:41, 3182.33it/s]111702it [00:41, 3192.03it/s]109277it [00:41, 3268.29it/s]108540it [00:41, 3273.81it/s]112053it [00:42, 3280.95it/s]109629it [00:41, 3340.14it/s]108870it [00:41, 3177.26it/s]112384it [00:42, 3181.27it/s]109965it [00:41, 3225.40it/s]109220it [00:41, 3267.31it/s]112735it [00:42, 3274.48it/s]110317it [00:41, 3308.26it/s]109559it [00:41, 3302.34it/s]113065it [00:42, 3177.42it/s]110650it [00:42, 3208.67it/s]109891it [00:42, 3188.70it/s]113418it [00:42, 3276.43it/s]111000it [00:42, 3291.79it/s]110243it [00:42, 3281.88it/s]113771it [00:42, 3348.62it/s]111352it [00:42, 3357.53it/s]110573it [00:42, 3176.27it/s]114108it [00:42, 3242.41it/s]111689it [00:42, 3237.51it/s]110927it [00:42, 3278.71it/s]114463it [00:42, 3322.57it/s]112041it [00:42, 3318.66it/s]111279it [00:42, 3347.10it/s]114797it [00:42, 3181.38it/s]112375it [00:42, 3177.45it/s]111615it [00:42, 3214.41it/s]112727it [00:42, 3273.15it/s]111966it [00:42, 3298.13it/s]113063it [00:42, 3188.20it/s]112298it [00:42, 3189.66it/s]113416it [00:42, 3283.85it/s]112637it [00:42, 3240.29it/s]113769it [00:42, 3352.74it/s]112988it [00:42, 3317.01it/s]114106it [00:43, 3254.90it/s]113321it [00:43, 3203.85it/s]114462it [00:43, 3340.37it/s]113672it [00:43, 3290.59it/s]114798it [00:43, 3229.71it/s]114003it [00:43, 3185.04it/s]114358it [00:43, 3287.85it/s]114711it [00:43, 3356.44it/s]115107it [00:46, 298.19it/s] 115462it [00:46, 416.95it/s]115745it [00:46, 536.61it/s]116103it [00:46, 737.92it/s]116460it [00:46, 982.38it/s]116776it [00:46, 1211.67it/s]117130it [00:47, 1524.64it/s]117453it [00:47, 1769.01it/s]117812it [00:47, 2104.50it/s]118167it [00:47, 2405.16it/s]118502it [00:47, 2551.52it/s]118857it [00:47, 2791.57it/s]119191it [00:47, 2816.87it/s]119544it [00:47, 3002.08it/s]119874it [00:47, 2988.37it/s]120225it [00:48, 3129.57it/s]120571it [00:48, 3221.96it/s]120905it [00:48, 3155.55it/s]121259it [00:48, 3263.30it/s]121592it [00:48, 3184.84it/s]115118it [00:46, 301.97it/s] 121945it [00:48, 3282.37it/s]115468it [00:46, 420.38it/s]122288it [00:48, 3323.20it/s]115763it [00:46, 547.42it/s]116118it [00:46, 748.39it/s]122624it [00:48, 3227.34it/s]116474it [00:46, 993.76it/s]122980it [00:48, 3321.41it/s]116789it [00:46, 1225.26it/s]123315it [00:48, 3228.15it/s]117129it [00:47, 1520.90it/s]123669it [00:49, 3317.09it/s]115123it [00:46, 313.29it/s] 117448it [00:47, 1769.96it/s]124012it [00:49, 3349.30it/s]115474it [00:46, 435.20it/s]117802it [00:47, 2099.91it/s]124349it [00:49, 3239.78it/s]115766it [00:46, 563.54it/s]118153it [00:47, 2396.55it/s]124704it [00:49, 3328.64it/s]116119it [00:46, 767.30it/s]118486it [00:47, 2548.04it/s]125039it [00:49, 3228.40it/s]116475it [00:47, 1016.76it/s]118840it [00:47, 2789.01it/s]125394it [00:49, 3318.98it/s]116790it [00:47, 1249.96it/s]119173it [00:47, 2844.12it/s]115048it [00:47, 291.62it/s] 117146it [00:47, 1569.18it/s]125731it [00:49, 3202.00it/s]119524it [00:47, 3018.83it/s]115399it [00:47, 404.44it/s]126088it [00:49, 3305.41it/s]117470it [00:47, 1809.77it/s]115690it [00:47, 523.87it/s]119855it [00:47, 2973.14it/s]126443it [00:49, 3375.51it/s]117822it [00:47, 2128.64it/s]116028it [00:47, 706.01it/s]120208it [00:47, 3124.80it/s]126782it [00:50, 3255.27it/s]118172it [00:47, 2333.80it/s]116384it [00:47, 944.24it/s]120560it [00:48, 3235.28it/s]127137it [00:50, 3337.80it/s]118526it [00:47, 2605.21it/s]116698it [00:47, 1170.97it/s]120896it [00:48, 3157.34it/s]118879it [00:47, 2829.77it/s]127473it [00:50, 3220.21it/s]117052it [00:47, 1481.97it/s]121248it [00:48, 3258.09it/s]127828it [00:50, 3312.35it/s]119215it [00:47, 2874.77it/s]117375it [00:47, 1732.36it/s]121581it [00:48, 3168.85it/s]128181it [00:50, 3369.86it/s]119563it [00:47, 3032.93it/s]117728it [00:47, 2059.05it/s]121934it [00:48, 3271.38it/s]128520it [00:50, 3269.04it/s]119895it [00:48, 3015.82it/s]118082it [00:48, 2363.61it/s]122271it [00:48, 3297.81it/s]128877it [00:50, 3353.86it/s]120246it [00:48, 3151.25it/s]118416it [00:48, 2510.96it/s]122604it [00:48, 3206.07it/s]120586it [00:48, 3218.94it/s]129214it [00:50, 3227.11it/s]118769it [00:48, 2754.83it/s]122957it [00:48, 3298.57it/s]129568it [00:50, 3315.22it/s]120919it [00:48, 3145.29it/s]119101it [00:48, 2776.27it/s]123289it [00:48, 3203.68it/s]129924it [00:50, 3384.65it/s]121272it [00:48, 3253.00it/s]119451it [00:48, 2962.63it/s]123639it [00:49, 3287.77it/s]130264it [00:51, 3269.94it/s]121604it [00:48, 3166.37it/s]119802it [00:48, 3108.89it/s]123988it [00:49, 3344.81it/s]130608it [00:51, 3316.55it/s]121957it [00:48, 3267.77it/s]120136it [00:48, 3059.30it/s]124324it [00:49, 3232.01it/s]122311it [00:48, 3345.93it/s]130941it [00:51, 3219.93it/s]120488it [00:48, 3185.88it/s]124658it [00:49, 3261.85it/s]131297it [00:51, 3316.20it/s]122649it [00:48, 3247.49it/s]120819it [00:48, 3119.19it/s]124986it [00:49, 3171.10it/s]123001it [00:49, 3323.90it/s]131630it [00:51, 3219.53it/s]121170it [00:49, 3228.16it/s]125338it [00:49, 3270.75it/s]131985it [00:51, 3313.42it/s]123336it [00:49, 3178.02it/s]121520it [00:49, 3304.93it/s]125691it [00:49, 3346.09it/s]132329it [00:51, 3349.96it/s]123686it [00:49, 3268.54it/s]121856it [00:49, 3193.00it/s]126027it [00:49, 3240.14it/s]132666it [00:51, 3243.96it/s]124038it [00:49, 3339.71it/s]122190it [00:49, 3234.82it/s]126377it [00:49, 3314.73it/s]133023it [00:51, 3335.38it/s]124374it [00:49, 3226.53it/s]122517it [00:49, 3149.42it/s]126710it [00:49, 3213.00it/s]133358it [00:52, 3238.86it/s]124726it [00:49, 3310.14it/s]122870it [00:49, 3256.46it/s]127059it [00:50, 3291.76it/s]133712it [00:52, 3324.98it/s]125059it [00:49, 3205.11it/s]123211it [00:49, 3164.22it/s]127412it [00:50, 3190.27it/s]134059it [00:52, 3366.33it/s]125410it [00:49, 3291.24it/s]123563it [00:49, 3265.09it/s]127750it [00:50, 3243.40it/s]134397it [00:52, 3256.26it/s]125741it [00:49, 3197.85it/s]123914it [00:49, 3334.23it/s]128103it [00:50, 3324.01it/s]134755it [00:52, 3348.15it/s]126094it [00:49, 3292.83it/s]124250it [00:50, 3215.82it/s]128437it [00:50, 3219.59it/s]135092it [00:52, 3247.41it/s]126431it [00:50, 3313.87it/s]124600it [00:50, 3297.09it/s]128791it [00:50, 3308.85it/s]135452it [00:52, 3347.58it/s]126764it [00:50, 3197.81it/s]124932it [00:50, 3186.63it/s]129124it [00:50, 3208.72it/s]135795it [00:52, 3369.83it/s]127114it [00:50, 3284.03it/s]125266it [00:50, 3228.05it/s]129478it [00:50, 3301.54it/s]136134it [00:52, 3267.00it/s]127444it [00:50, 3191.63it/s]125618it [00:50, 3310.10it/s]129830it [00:50, 3363.41it/s]136490it [00:52, 3350.73it/s]127796it [00:50, 3284.19it/s]125951it [00:50, 3200.52it/s]130168it [00:51, 3235.31it/s]136827it [00:53, 3248.44it/s]128146it [00:50, 3345.77it/s]126304it [00:50, 3293.89it/s]130518it [00:51, 3310.62it/s]137184it [00:53, 3340.28it/s]128482it [00:50, 3229.81it/s]126635it [00:50, 3187.72it/s]130851it [00:51, 3204.20it/s]137520it [00:53, 3211.29it/s]128836it [00:50, 3317.82it/s]126984it [00:50, 3271.95it/s]131204it [00:51, 3296.36it/s]137878it [00:53, 3315.47it/s]129170it [00:50, 3179.21it/s]127336it [00:50, 3343.56it/s]131556it [00:51, 3360.58it/s]138238it [00:53, 3396.91it/s]129522it [00:51, 3274.08it/s]127672it [00:51, 3216.33it/s]131894it [00:51, 3241.93it/s]138580it [00:53, 3283.47it/s]129874it [00:51, 3343.39it/s]128023it [00:51, 3298.05it/s]132247it [00:51, 3323.60it/s]138939it [00:53, 3369.48it/s]130210it [00:51, 3238.77it/s]128355it [00:51, 3144.21it/s]132581it [00:51, 3224.10it/s]139278it [00:53, 3234.81it/s]130562it [00:51, 3313.49it/s]128709it [00:51, 3253.85it/s]132934it [00:51, 3311.42it/s]139636it [00:53, 3332.46it/s]130895it [00:51, 3217.15it/s]129061it [00:51, 3329.38it/s]133287it [00:51, 3374.59it/s]139997it [00:54, 3409.99it/s]131247it [00:51, 3303.22it/s]129396it [00:51, 3213.13it/s]133626it [00:52, 3251.12it/s]131598it [00:51, 3362.44it/s]140340it [00:54, 3290.86it/s]129747it [00:51, 3295.63it/s]133978it [00:52, 3326.52it/s]140683it [00:54, 3329.42it/s]131936it [00:51, 3237.29it/s]130079it [00:51, 3187.54it/s]134313it [00:52, 3222.92it/s]141018it [00:54, 3238.71it/s]132274it [00:51, 3276.51it/s]130429it [00:51, 3276.23it/s]134668it [00:52, 3316.37it/s]141377it [00:54, 3339.54it/s]132603it [00:51, 3177.61it/s]135002it [00:52, 3218.00it/s]130771it [00:52, 3174.38it/s]141713it [00:54, 3242.13it/s]132954it [00:52, 3272.81it/s]135347it [00:52, 3283.26it/s]131124it [00:52, 3273.42it/s]142073it [00:54, 3343.91it/s]133292it [00:52, 3181.37it/s]135705it [00:52, 3369.02it/s]131461it [00:52, 3300.90it/s]142417it [00:54, 3371.14it/s]133645it [00:52, 3280.66it/s]136044it [00:52, 3262.97it/s]131793it [00:52, 3189.15it/s]142756it [00:54, 3266.72it/s]133998it [00:52, 3351.82it/s]136398it [00:52, 3342.42it/s]132143it [00:52, 3276.32it/s]143115it [00:54, 3357.85it/s]134335it [00:52, 3247.34it/s]136734it [00:53, 3237.52it/s]132473it [00:52, 3169.11it/s]143453it [00:55, 3258.75it/s]134687it [00:52, 3325.09it/s]137088it [00:53, 3323.02it/s]132825it [00:52, 3268.15it/s]143814it [00:55, 3357.79it/s]135021it [00:52, 3187.14it/s]137441it [00:53, 3382.96it/s]133177it [00:52, 3340.47it/s]144157it [00:55, 3377.88it/s]135378it [00:52, 3295.97it/s]137781it [00:53, 3268.00it/s]133513it [00:52, 3221.20it/s]144496it [00:55, 3276.31it/s]135734it [00:52, 3371.24it/s]138139it [00:53, 3356.27it/s]133866it [00:52, 3308.29it/s]144859it [00:55, 3376.84it/s]136073it [00:53, 3260.56it/s]138477it [00:53, 3244.14it/s]134199it [00:53, 3205.71it/s]145198it [00:55, 3278.37it/s]136427it [00:53, 3338.42it/s]138833it [00:53, 3333.19it/s]134535it [00:53, 3248.49it/s]145559it [00:55, 3372.87it/s]136763it [00:53, 3233.92it/s]134887it [00:53, 3325.12it/s]139172it [00:53, 3232.53it/s]145898it [00:55, 3238.18it/s]137114it [00:53, 3312.47it/s]139528it [00:53, 3323.73it/s]135221it [00:53, 3211.77it/s]146256it [00:55, 3333.30it/s]137467it [00:53, 3373.15it/s]139884it [00:53, 3390.61it/s]135578it [00:53, 3312.47it/s]146615it [00:56, 3405.31it/s]137806it [00:53, 3258.95it/s]135911it [00:53, 3205.48it/s]140225it [00:54, 3233.41it/s]146957it [00:56, 3295.03it/s]138147it [00:53, 3301.54it/s]136266it [00:53, 3302.53it/s]140577it [00:54, 3312.61it/s]147318it [00:56, 3385.11it/s]138479it [00:53, 3205.36it/s]136620it [00:53, 3370.78it/s]140911it [00:54, 3216.83it/s]147658it [00:56, 3244.04it/s]138833it [00:53, 3300.12it/s]141269it [00:54, 3318.56it/s]136959it [00:53, 3245.74it/s]148019it [00:56, 3347.46it/s]139172it [00:53, 3210.75it/s]141622it [00:54, 3376.58it/s]137313it [00:54, 3329.88it/s]148379it [00:56, 3417.88it/s]139527it [00:54, 3306.72it/s]141962it [00:54, 3265.87it/s]137648it [00:54, 3181.31it/s]148723it [00:56, 3292.02it/s]139883it [00:54, 3378.17it/s]142319it [00:54, 3351.69it/s]138004it [00:54, 3286.46it/s]149084it [00:56, 3381.98it/s]140223it [00:54, 3262.15it/s]142656it [00:54, 3243.10it/s]138335it [00:54, 3187.97it/s]140576it [00:54, 3337.45it/s]149424it [00:56, 3231.69it/s]143012it [00:54, 3331.32it/s]138688it [00:54, 3284.33it/s]149786it [00:56, 3341.14it/s]140912it [00:54, 3205.27it/s]143368it [00:54, 3396.90it/s]139044it [00:54, 3363.26it/s]150123it [00:57, 3240.27it/s]141268it [00:54, 3305.33it/s]143709it [00:55, 3262.62it/s]139382it [00:54, 3245.51it/s]150484it [00:57, 3343.92it/s]141622it [00:54, 3363.46it/s]144061it [00:55, 3335.71it/s]139734it [00:54, 3322.13it/s]150828it [00:57, 3369.48it/s]141960it [00:54, 3259.36it/s]144397it [00:55, 3223.72it/s]140068it [00:54, 3217.87it/s]142313it [00:54, 3336.06it/s]144759it [00:55, 3335.70it/s]140421it [00:54, 3306.96it/s]142648it [00:55, 3234.24it/s]140759it [00:55, 3325.91it/s]145095it [00:55, 3238.96it/s]143002it [00:55, 3320.38it/s]145436it [00:55, 3286.65it/s]141093it [00:55, 3211.38it/s]143357it [00:55, 3386.73it/s]145795it [00:55, 3373.51it/s]141447it [00:55, 3305.32it/s]143697it [00:55, 3237.27it/s]146134it [00:55, 3268.95it/s]141779it [00:55, 3196.58it/s]144051it [00:55, 3323.24it/s]146489it [00:55, 3348.84it/s]142135it [00:55, 3298.69it/s]144386it [00:55, 3225.19it/s]142489it [00:55, 3368.46it/s]146826it [00:56, 3239.19it/s]144747it [00:55, 3334.11it/s]147177it [00:56, 3315.51it/s]142828it [00:55, 3244.43it/s]145083it [00:55, 3237.14it/s]147537it [00:56, 3396.81it/s]143182it [00:55, 3329.01it/s]145442it [00:55, 3335.84it/s]147878it [00:56, 3279.07it/s]143517it [00:55, 3178.39it/s]145799it [00:55, 3402.65it/s]148235it [00:56, 3360.84it/s]143873it [00:56, 3286.58it/s]146141it [00:56, 3290.05it/s]148573it [00:56, 3253.86it/s]144211it [00:56, 3185.02it/s]146494it [00:56, 3358.52it/s]148930it [00:56, 3343.20it/s]144568it [00:56, 3293.73it/s]146832it [00:56, 3219.49it/s]149266it [00:56, 3241.20it/s]144925it [00:56, 3373.13it/s]147187it [00:56, 3312.51it/s]149626it [00:56, 3342.26it/s]145265it [00:56, 3254.74it/s]147546it [00:56, 3391.56it/s]149983it [00:56, 3407.87it/s]145624it [00:56, 3349.57it/s]147887it [00:56, 3271.54it/s]150326it [00:57, 3273.11it/s]145961it [00:56, 3242.42it/s]148242it [00:56, 3349.10it/s]150681it [00:57, 3350.42it/s]146316it [00:56, 3329.03it/s]148579it [00:56, 3245.08it/s]146657it [00:56, 3350.69it/s]148935it [00:56, 3334.90it/s]146994it [00:56, 3234.80it/s]149271it [00:57, 3231.12it/s]147350it [00:57, 3325.68it/s]149615it [00:57, 3289.21it/s]147685it [00:57, 3222.09it/s]149972it [00:57, 3368.17it/s]148042it [00:57, 3320.95it/s]150311it [00:57, 3258.54it/s]148397it [00:57, 3385.96it/s]150668it [00:57, 3347.38it/s]148737it [00:57, 3254.45it/s]149095it [00:57, 3345.90it/s]149432it [00:57, 3229.96it/s]149772it [00:57, 3278.43it/s]150102it [00:57, 3174.42it/s]150460it [00:58, 3288.89it/s]150818it [00:58, 3372.44it/s]151167it [01:02, 230.44it/s] 151513it [01:02, 319.90it/s]151841it [01:02, 430.98it/s]152201it [01:02, 593.79it/s]152561it [01:02, 799.29it/s]152882it [01:02, 1008.15it/s]153228it [01:02, 1283.03it/s]153552it [01:02, 1537.99it/s]153917it [01:02, 1882.65it/s]154283it [01:02, 2218.00it/s]154625it [01:03, 2413.68it/s]154974it [01:03, 2658.77it/s]155311it [01:03, 2763.41it/s]155674it [01:03, 2983.87it/s]151018it [01:01, 263.33it/s] 156036it [01:03, 3152.65it/s]151371it [01:01, 366.27it/s]156381it [01:03, 3131.05it/s]151723it [01:01, 502.11it/s]156741it [01:03, 3253.27it/s]152020it [01:01, 644.77it/s]152372it [01:01, 864.54it/s]157082it [01:03, 3167.77it/s]152685it [01:01, 1081.08it/s]157446it [01:03, 3298.54it/s]153041it [01:01, 1384.86it/s]157785it [01:04, 3238.04it/s]153396it [01:02, 1707.24it/s]158145it [01:04, 3338.44it/s]153728it [01:02, 1957.64it/s]158512it [01:04, 3433.93it/s]151005it [01:01, 249.34it/s] 154084it [01:02, 2273.53it/s]158860it [01:04, 3334.13it/s]151359it [01:01, 348.06it/s]154417it [01:02, 2452.27it/s]159227it [01:04, 3430.36it/s]151716it [01:02, 480.44it/s]154760it [01:02, 2681.89it/s]159573it [01:04, 3328.48it/s]152014it [01:02, 618.67it/s]155119it [01:02, 2908.45it/s]159938it [01:04, 3419.63it/s]152371it [01:02, 835.08it/s]155457it [01:02, 2939.72it/s]160282it [01:04, 3318.24it/s]152686it [01:02, 1048.45it/s]155814it [01:02, 3108.01it/s]160649it [01:04, 3418.89it/s]153044it [01:02, 1348.89it/s]156151it [01:02, 3078.29it/s]161011it [01:04, 3475.01it/s]153403it [01:02, 1673.25it/s]156510it [01:03, 3218.25it/s]161360it [01:05, 3349.57it/s]153738it [01:02, 1914.08it/s]156868it [01:03, 3320.17it/s]161724it [01:05, 3432.71it/s]154100it [01:02, 2242.49it/s]157211it [01:03, 3230.59it/s]162069it [01:05, 3345.85it/s]151157it [01:02, 235.57it/s] 154434it [01:02, 2428.38it/s]157572it [01:03, 3337.32it/s]162437it [01:05, 3440.20it/s]151512it [01:02, 329.48it/s]154794it [01:02, 2699.40it/s]157912it [01:03, 3240.35it/s]162783it [01:05, 3348.83it/s]151840it [01:02, 442.59it/s]155155it [01:03, 2925.57it/s]158271it [01:03, 3337.62it/s]163145it [01:05, 3425.23it/s]152195it [01:03, 605.60it/s]155498it [01:03, 2961.26it/s]158609it [01:03, 3241.63it/s]163515it [01:05, 3504.47it/s]152550it [01:03, 810.81it/s]155857it [01:03, 3127.77it/s]158971it [01:03, 3348.22it/s]152869it [01:03, 1019.92it/s]163867it [01:05, 3390.67it/s]156197it [01:03, 3104.67it/s]159332it [01:03, 3421.64it/s]164236it [01:05, 3474.55it/s]153226it [01:03, 1303.33it/s]156543it [01:03, 3201.13it/s]159677it [01:03, 3312.45it/s]164585it [01:06, 3364.05it/s]153548it [01:03, 1551.21it/s]156881it [01:03, 3157.45it/s]160022it [01:04, 3351.18it/s]164962it [01:06, 3478.51it/s]153908it [01:03, 1887.11it/s]157241it [01:03, 3281.49it/s]160359it [01:04, 3251.07it/s]154264it [01:03, 2204.94it/s]165312it [01:06, 3343.28it/s]157600it [01:03, 3368.43it/s]160718it [01:04, 3347.51it/s]154600it [01:03, 2386.82it/s]165684it [01:06, 3449.71it/s]157943it [01:03, 3269.71it/s]161078it [01:04, 3419.88it/s]154959it [01:03, 2661.22it/s]166058it [01:06, 3532.89it/s]158305it [01:04, 3367.56it/s]161422it [01:04, 3306.79it/s]155294it [01:04, 2753.31it/s]166413it [01:06, 3424.54it/s]158646it [01:04, 3273.14it/s]161783it [01:04, 3392.66it/s]155652it [01:04, 2962.13it/s]166795it [01:06, 3536.17it/s]159011it [01:04, 3378.68it/s]162124it [01:04, 3304.68it/s]156010it [01:04, 3124.73it/s]167151it [01:06, 3417.03it/s]159374it [01:04, 3449.74it/s]162482it [01:04, 3382.68it/s]167521it [01:06, 3496.57it/s]156352it [01:04, 3051.81it/s]159721it [01:04, 3294.32it/s]162822it [01:04, 3292.80it/s]156708it [01:04, 3189.03it/s]167873it [01:06, 3352.04it/s]160078it [01:04, 3372.84it/s]163182it [01:05, 3380.59it/s]168248it [01:07, 3464.77it/s]157043it [01:04, 3132.55it/s]160418it [01:04, 3272.31it/s]163546it [01:05, 3455.33it/s]168623it [01:07, 3545.27it/s]157403it [01:04, 3261.28it/s]160778it [01:04, 3365.38it/s]163893it [01:05, 3334.13it/s]168980it [01:07, 3426.44it/s]157738it [01:04, 3184.35it/s]161117it [01:04, 3267.99it/s]164259it [01:05, 3427.95it/s]169346it [01:07, 3491.67it/s]158095it [01:04, 3291.45it/s]161476it [01:04, 3358.38it/s]164604it [01:05, 3307.00it/s]158452it [01:04, 3370.23it/s]169697it [01:07, 3380.67it/s]161841it [01:05, 3440.38it/s]164957it [01:05, 3368.97it/s]170065it [01:07, 3465.51it/s]158793it [01:05, 3258.72it/s]162187it [01:05, 3327.55it/s]165296it [01:05, 3276.59it/s]159157it [01:05, 3366.11it/s]170414it [01:07, 3301.30it/s]162534it [01:05, 3367.21it/s]165662it [01:05, 3386.22it/s]170784it [01:07, 3411.54it/s]159497it [01:05, 3216.24it/s]162872it [01:05, 3274.58it/s]166030it [01:05, 3470.37it/s]171151it [01:07, 3485.15it/s]159857it [01:05, 3323.02it/s]163238it [01:05, 3383.86it/s]166379it [01:05, 3356.14it/s]160215it [01:05, 3395.36it/s]171502it [01:08, 3375.84it/s]163601it [01:05, 3293.85it/s]166751it [01:06, 3457.23it/s]171868it [01:08, 3455.50it/s]160557it [01:05, 3282.35it/s]163964it [01:05, 3387.74it/s]167099it [01:06, 3346.25it/s]160917it [01:05, 3373.29it/s]172216it [01:08, 3360.02it/s]164332it [01:05, 3471.36it/s]167461it [01:06, 3424.51it/s]172587it [01:08, 3459.67it/s]161257it [01:05, 3269.35it/s]164681it [01:05, 3351.24it/s]167805it [01:06, 3307.77it/s]161618it [01:05, 3365.43it/s]172935it [01:08, 3299.76it/s]165054it [01:06, 3459.56it/s]168176it [01:06, 3421.16it/s]161957it [01:06, 3267.54it/s]173307it [01:08, 3417.16it/s]165402it [01:06, 3349.21it/s]168547it [01:06, 3502.46it/s]162322it [01:06, 3375.64it/s]173678it [01:08, 3501.09it/s]165758it [01:06, 3407.53it/s]168899it [01:06, 3368.58it/s]162668it [01:06, 3399.80it/s]174031it [01:08, 3374.79it/s]166121it [01:06, 3323.33it/s]169264it [01:06, 3447.30it/s]163010it [01:06, 3284.77it/s]174406it [01:08, 3481.56it/s]166491it [01:06, 3429.39it/s]169611it [01:06, 3324.48it/s]163375it [01:06, 3388.92it/s]174757it [01:08, 3384.81it/s]166864it [01:06, 3515.95it/s]169971it [01:07, 3400.21it/s]163716it [01:06, 3279.56it/s]175129it [01:09, 3479.02it/s]167218it [01:06, 3380.39it/s]170320it [01:07, 3304.54it/s]164080it [01:06, 3382.28it/s]175479it [01:09, 3331.20it/s]167587it [01:06, 3467.90it/s]170670it [01:07, 3352.78it/s]164440it [01:06, 3275.85it/s]175851it [01:09, 3432.96it/s]167936it [01:06, 3359.19it/s]171034it [01:07, 3434.61it/s]164805it [01:06, 3381.28it/s]176201it [01:09, 3328.19it/s]168304it [01:06, 3448.70it/s]171379it [01:07, 3319.03it/s]165175it [01:06, 3471.69it/s]176569it [01:09, 3427.69it/s]168651it [01:07, 3309.19it/s]171740it [01:07, 3401.03it/s]165524it [01:07, 3350.64it/s]176943it [01:09, 3517.64it/s]169015it [01:07, 3402.46it/s]172082it [01:07, 3305.73it/s]165868it [01:07, 3376.06it/s]177297it [01:09, 3397.53it/s]169378it [01:07, 3467.85it/s]172452it [01:07, 3417.83it/s]166208it [01:07, 3281.15it/s]177649it [01:09, 3431.62it/s]169727it [01:07, 3348.05it/s]172822it [01:07, 3497.25it/s]166576it [01:07, 3393.76it/s]177994it [01:09, 3351.25it/s]170095it [01:07, 3441.15it/s]173173it [01:07, 3376.28it/s]166945it [01:07, 3478.09it/s]178371it [01:10, 3462.42it/s]170441it [01:07, 3330.86it/s]173525it [01:08, 3416.79it/s]167295it [01:07, 3342.88it/s]178721it [01:10, 3297.84it/s]170803it [01:07, 3412.53it/s]173868it [01:08, 3225.28it/s]167660it [01:07, 3427.48it/s]179071it [01:10, 3345.79it/s]171161it [01:07, 3288.75it/s]174203it [01:08, 3258.61it/s]168005it [01:07, 3290.87it/s]179408it [01:10, 3347.52it/s]171500it [01:07, 3316.35it/s]174531it [01:08, 3055.14it/s]168366it [01:07, 3381.47it/s]179745it [01:10, 3101.33it/s]171861it [01:08, 3392.36it/s]174868it [01:08, 3140.12it/s]168707it [01:08, 3269.02it/s]180080it [01:10, 3167.81it/s]172202it [01:08, 3276.39it/s]175206it [01:08, 3205.83it/s]169042it [01:08, 3289.65it/s]172562it [01:08, 3368.05it/s]180401it [01:10, 3019.11it/s]169401it [01:08, 3374.72it/s]175530it [01:08, 2973.95it/s]180749it [01:10, 3145.50it/s]172901it [01:08, 3290.23it/s]175865it [01:08, 3076.81it/s]169740it [01:08, 3245.59it/s]181095it [01:10, 3234.75it/s]173260it [01:08, 3373.84it/s]176199it [01:08, 3148.91it/s]170099it [01:08, 3344.08it/s]173615it [01:08, 3424.40it/s]181422it [01:11, 3046.11it/s]170436it [01:08, 3237.20it/s]176518it [01:09, 2966.86it/s]173959it [01:08, 3294.87it/s]181747it [01:11, 3101.20it/s]170789it [01:08, 3320.78it/s]176851it [01:09, 3066.26it/s]174312it [01:08, 3356.81it/s]182082it [01:11, 2963.04it/s]171146it [01:08, 3392.84it/s]177162it [01:09, 2915.63it/s]174650it [01:08, 3301.99it/s]182435it [01:11, 3118.37it/s]171487it [01:08, 3268.97it/s]177494it [01:09, 3027.38it/s]174987it [01:08, 3320.96it/s]182784it [01:11, 3221.55it/s]171838it [01:08, 3336.82it/s]177826it [01:09, 3109.51it/s]175347it [01:09, 3400.72it/s]183110it [01:11, 3055.37it/s]172174it [01:09, 3206.96it/s]178140it [01:09, 2963.99it/s]175688it [01:09, 3293.50it/s]183454it [01:11, 3162.34it/s]172528it [01:09, 3300.92it/s]178467it [01:09, 3048.60it/s]176038it [01:09, 3352.13it/s]183774it [01:11, 2991.69it/s]172860it [01:09, 3217.78it/s]178775it [01:09, 2923.51it/s]176375it [01:09, 3251.56it/s]184112it [01:11, 3095.57it/s]173221it [01:09, 3329.48it/s]179108it [01:09, 3035.82it/s]176734it [01:09, 3347.47it/s]184458it [01:12, 3197.68it/s]173570it [01:09, 3375.39it/s]179434it [01:10, 3097.83it/s]177070it [01:09, 3226.64it/s]184781it [01:12, 3030.28it/s]173909it [01:09, 3262.92it/s]177432it [01:09, 3338.24it/s]179746it [01:10, 2958.20it/s]185134it [01:12, 3168.07it/s]174271it [01:09, 3364.68it/s]177793it [01:09, 3414.21it/s]180085it [01:10, 3078.08it/s]185455it [01:12, 2985.16it/s]174609it [01:09, 3273.26it/s]178136it [01:09, 3281.48it/s]180401it [01:10, 2919.55it/s]185808it [01:12, 3135.44it/s]174966it [01:09, 3357.10it/s]178494it [01:10, 3366.81it/s]180740it [01:10, 3050.19it/s]186153it [01:12, 3223.63it/s]175326it [01:10, 3426.13it/s]178833it [01:10, 3279.40it/s]181084it [01:10, 3160.79it/s]186479it [01:12, 3052.09it/s]175670it [01:10, 3294.11it/s]179193it [01:10, 3371.49it/s]181403it [01:10, 2974.72it/s]186841it [01:12, 3209.45it/s]176024it [01:10, 3364.33it/s]179552it [01:10, 3433.14it/s]181736it [01:10, 3073.97it/s]187166it [01:12, 3030.92it/s]176362it [01:10, 3240.87it/s]179897it [01:10, 3305.16it/s]182081it [01:10, 2952.03it/s]187521it [01:12, 3173.43it/s]176727it [01:10, 3356.45it/s]180267it [01:10, 3417.80it/s]182431it [01:10, 3099.31it/s]187877it [01:13, 3281.16it/s]177065it [01:10, 3242.93it/s]180611it [01:10, 3297.55it/s]182781it [01:11, 3202.31it/s]177428it [01:10, 3352.56it/s]188209it [01:13, 3070.85it/s]180979it [01:10, 3404.73it/s]183105it [01:11, 3049.94it/s]177789it [01:10, 3425.21it/s]188568it [01:13, 3214.74it/s]181322it [01:10, 3256.55it/s]183445it [01:11, 3146.94it/s]178134it [01:10, 3308.70it/s]188894it [01:13, 3038.01it/s]181681it [01:10, 3348.93it/s]183763it [01:11, 2987.53it/s]178495it [01:10, 3394.70it/s]189247it [01:13, 3171.61it/s]182050it [01:11, 3446.04it/s]184111it [01:11, 3120.29it/s]178837it [01:11, 3294.86it/s]189588it [01:13, 3238.51it/s]182397it [01:11, 3326.06it/s]184458it [01:11, 3218.16it/s]179194it [01:11, 3373.33it/s]189916it [01:13, 3084.25it/s]182773it [01:11, 3449.22it/s]184783it [01:11, 3048.86it/s]179557it [01:11, 3447.67it/s]190263it [01:13, 3191.65it/s]183120it [01:11, 3336.04it/s]185132it [01:11, 3172.42it/s]179904it [01:11, 3290.32it/s]183485it [01:11, 3423.70it/s]190586it [01:13, 3029.52it/s]185453it [01:11, 2950.97it/s]180270it [01:11, 3394.67it/s]190935it [01:14, 3156.52it/s]183830it [01:11, 3329.85it/s]185803it [01:12, 3100.64it/s]180612it [01:11, 3272.83it/s]191283it [01:14, 3247.10it/s]184193it [01:11, 3414.29it/s]186161it [01:12, 3233.59it/s]180982it [01:11, 3394.35it/s]184548it [01:11, 3452.31it/s]191611it [01:14, 3083.72it/s]186489it [01:12, 3076.54it/s]181324it [01:11, 3287.30it/s]184895it [01:11, 3362.16it/s]191979it [01:14, 3251.05it/s]186854it [01:12, 3236.29it/s]181688it [01:11, 3387.85it/s]185262it [01:12, 3450.23it/s]192308it [01:14, 3055.53it/s]182046it [01:12, 3443.04it/s]187182it [01:12, 3060.92it/s]185609it [01:12, 3318.17it/s]192672it [01:14, 3217.23it/s]182392it [01:12, 3332.78it/s]187534it [01:12, 3188.16it/s]185982it [01:12, 3435.63it/s]193002it [01:14, 3086.90it/s]182770it [01:12, 3459.35it/s]187891it [01:12, 3291.66it/s]186328it [01:12, 3349.90it/s]193359it [01:14, 3219.95it/s]183118it [01:12, 3333.10it/s]188224it [01:12, 3104.69it/s]186704it [01:12, 3466.97it/s]193705it [01:14, 3286.40it/s]183480it [01:12, 3411.79it/s]188571it [01:12, 3205.08it/s]187074it [01:12, 3534.59it/s]194037it [01:15, 3139.24it/s]183823it [01:12, 3315.69it/s]188896it [01:13, 3051.68it/s]187429it [01:12, 3425.30it/s]194385it [01:15, 3234.71it/s]184189it [01:12, 3412.76it/s]189245it [01:13, 3172.05it/s]187780it [01:12, 3449.38it/s]194712it [01:15, 3073.44it/s]184532it [01:12, 3335.00it/s]189592it [01:13, 3254.27it/s]188127it [01:12, 3370.65it/s]195078it [01:15, 3237.54it/s]184867it [01:12, 3271.79it/s]188496it [01:12, 3461.05it/s]189921it [01:13, 3081.84it/s]195440it [01:15, 3345.38it/s]185230it [01:13, 3366.22it/s]190271it [01:13, 3196.05it/s]188844it [01:13, 3348.71it/s]195778it [01:15, 3163.31it/s]185568it [01:13, 3261.52it/s]189212it [01:13, 3441.79it/s]190594it [01:13, 3011.07it/s]196128it [01:15, 3256.93it/s]185936it [01:13, 3381.17it/s]189572it [01:13, 3484.43it/s]190936it [01:13, 3122.57it/s]186280it [01:13, 3308.18it/s]189922it [01:13, 3376.82it/s]191285it [01:13, 3226.64it/s]186655it [01:13, 3435.02it/s]190300it [01:13, 3492.40it/s]191611it [01:13, 3067.50it/s]187038it [01:13, 3549.45it/s]190651it [01:13, 3379.21it/s]191975it [01:14, 3227.12it/s]187395it [01:13, 3421.15it/s]191020it [01:13, 3466.92it/s]192302it [01:14, 3065.12it/s]187767it [01:13, 3505.18it/s]191369it [01:13, 3325.08it/s]192660it [01:14, 3208.14it/s]188120it [01:13, 3306.53it/s]191742it [01:13, 3433.98it/s]193001it [01:14, 3053.31it/s]188490it [01:13, 3415.64it/s]192123it [01:14, 3542.28it/s]193352it [01:14, 3177.42it/s]188835it [01:14, 3326.39it/s]192479it [01:14, 3435.58it/s]193701it [01:14, 3255.64it/s]189203it [01:14, 3426.55it/s]192857it [01:14, 3533.24it/s]194030it [01:14, 3103.58it/s]189569it [01:14, 3492.93it/s]193212it [01:14, 3422.45it/s]194381it [01:14, 3216.78it/s]189920it [01:14, 3375.63it/s]193574it [01:14, 3476.75it/s]194706it [01:14, 3039.59it/s]190297it [01:14, 3487.52it/s]193924it [01:14, 3396.30it/s]195058it [01:15, 3172.15it/s]190648it [01:14, 3365.00it/s]194302it [01:14, 3504.81it/s]195412it [01:15, 3274.08it/s]191016it [01:14, 3452.70it/s]194677it [01:14, 3574.72it/s]195743it [01:15, 3108.60it/s]191364it [01:14, 3299.36it/s]195036it [01:14, 3390.28it/s]196082it [01:15, 3187.28it/s]191736it [01:14, 3417.71it/s]195425it [01:14, 3531.65it/s]196404it [01:15, 3027.05it/s]192114it [01:15, 3519.71it/s]195781it [01:15, 3423.29it/s]192469it [01:15, 3407.00it/s]196147it [01:15, 3489.17it/s]192849it [01:15, 3518.37it/s]193203it [01:15, 3417.24it/s]193565it [01:15, 3474.49it/s]193914it [01:15, 3342.80it/s]194295it [01:15, 3473.50it/s]194672it [01:15, 3556.93it/s]195030it [01:15, 3418.28it/s]195420it [01:15, 3555.00it/s]195778it [01:16, 3428.17it/s]196145it [01:16, 3494.60it/s]196457it [01:21, 183.25it/s] 196829it [01:21, 262.54it/s]197186it [01:21, 365.17it/s]197485it [01:21, 474.65it/s]197850it [01:22, 655.42it/s]198163it [01:22, 833.37it/s]198500it [01:22, 1077.24it/s]198864it [01:22, 1386.77it/s]199192it [01:22, 1616.11it/s]199558it [01:22, 1962.02it/s]199886it [01:22, 2131.47it/s]200241it [01:22, 2429.65it/s]200598it [01:22, 2692.35it/s]200932it [01:23, 2654.30it/s]201288it [01:23, 2878.00it/s]196498it [01:20, 208.67it/s] 196710it [01:21, 181.87it/s] 201612it [01:23, 2844.45it/s]196882it [01:20, 296.15it/s]197058it [01:21, 258.79it/s]201966it [01:23, 3026.50it/s]197236it [01:20, 404.07it/s]197314it [01:21, 332.76it/s]197548it [01:21, 527.24it/s]202311it [01:23, 2970.43it/s]197662it [01:21, 471.25it/s]197926it [01:21, 725.13it/s]202678it [01:23, 3157.46it/s]198021it [01:21, 655.69it/s]198256it [01:21, 925.71it/s]203011it [01:23, 3202.54it/s]198327it [01:21, 833.32it/s]198626it [01:21, 1208.62it/s]203341it [01:23, 3076.79it/s]198679it [01:21, 1099.34it/s]198963it [01:21, 1466.91it/s]203690it [01:23, 3189.65it/s]198990it [01:21, 1323.32it/s]199325it [01:21, 1794.61it/s]204015it [01:24, 3037.21it/s]199348it [01:22, 1657.25it/s]199700it [01:21, 2143.94it/s]204372it [01:24, 3184.63it/s]199705it [01:22, 1990.22it/s]200050it [01:21, 2343.56it/s]204722it [01:24, 3271.65it/s]200034it [01:22, 2142.85it/s]200415it [01:21, 2628.93it/s]205053it [01:24, 3112.02it/s]200382it [01:22, 2427.18it/s]200759it [01:21, 2746.03it/s]196497it [01:21, 201.47it/s] 205389it [01:24, 3180.95it/s]200703it [01:22, 2487.97it/s]201124it [01:22, 2971.12it/s]196879it [01:22, 285.48it/s]205711it [01:24, 3040.11it/s]201051it [01:22, 2727.10it/s]201471it [01:22, 2971.18it/s]197233it [01:22, 389.94it/s]206071it [01:24, 3196.70it/s]201392it [01:22, 2901.96it/s]201836it [01:22, 3149.97it/s]197541it [01:22, 507.69it/s]206427it [01:24, 3298.15it/s]201717it [01:22, 2809.75it/s]202209it [01:22, 3306.80it/s]197915it [01:22, 698.09it/s]206760it [01:24, 3114.80it/s]202066it [01:22, 2987.75it/s]202559it [01:22, 3278.52it/s]198242it [01:22, 893.30it/s]207106it [01:25, 3210.64it/s]202385it [01:22, 2902.92it/s]202936it [01:22, 3414.95it/s]198607it [01:22, 1167.28it/s]207431it [01:25, 3065.74it/s]202747it [01:23, 3094.65it/s]203288it [01:22, 3329.13it/s]198949it [01:22, 1428.01it/s]207778it [01:25, 3177.10it/s]203109it [01:23, 3240.30it/s]203652it [01:22, 3415.90it/s]199322it [01:22, 1771.08it/s]208146it [01:25, 3320.17it/s]203443it [01:23, 3067.62it/s]199698it [01:22, 2119.91it/s]204000it [01:22, 3211.17it/s]208481it [01:25, 3143.31it/s]203780it [01:23, 3150.15it/s]204360it [01:23, 3318.23it/s]200049it [01:22, 2295.07it/s]208830it [01:25, 3239.64it/s]204725it [01:23, 3410.33it/s]204102it [01:23, 2988.23it/s]200409it [01:23, 2575.62it/s]209158it [01:25, 3111.50it/s]204444it [01:23, 3107.35it/s]205071it [01:23, 3309.72it/s]200750it [01:23, 2688.46it/s]209496it [01:25, 3186.75it/s]204786it [01:23, 3195.71it/s]205438it [01:23, 3410.55it/s]201112it [01:23, 2915.59it/s]209841it [01:25, 3261.75it/s]205110it [01:23, 3039.17it/s]205782it [01:23, 3320.68it/s]201469it [01:23, 2961.36it/s]210170it [01:25, 3082.22it/s]205460it [01:23, 3167.37it/s]206155it [01:23, 3436.35it/s]201839it [01:23, 3153.91it/s]210514it [01:26, 3182.09it/s]202207it [01:23, 3295.94it/s]206511it [01:23, 3339.83it/s]205781it [01:24, 3015.26it/s]210836it [01:26, 3041.24it/s]206878it [01:23, 3431.69it/s]206136it [01:24, 3163.63it/s]202556it [01:23, 3232.72it/s]211176it [01:26, 3140.97it/s]207224it [01:23, 3415.95it/s]206477it [01:24, 3232.29it/s]202926it [01:23, 3362.53it/s]211515it [01:26, 3211.43it/s]207567it [01:23, 3319.77it/s]203273it [01:23, 3284.70it/s]206804it [01:24, 3036.80it/s]211839it [01:26, 3058.94it/s]207937it [01:24, 3426.32it/s]203632it [01:24, 3370.28it/s]207148it [01:24, 3148.36it/s]212181it [01:26, 3158.62it/s]208281it [01:24, 3348.33it/s]203989it [01:24, 3259.69it/s]207467it [01:24, 2993.65it/s]212500it [01:26, 3029.72it/s]208652it [01:24, 3450.63it/s]204355it [01:24, 3369.74it/s]207814it [01:24, 3125.64it/s]212846it [01:26, 3149.14it/s]209018it [01:24, 3510.03it/s]204714it [01:24, 3431.18it/s]208163it [01:24, 3229.03it/s]213191it [01:26, 3234.57it/s]209370it [01:24, 3391.28it/s]205061it [01:24, 3323.49it/s]208489it [01:24, 3068.05it/s]213517it [01:27, 3041.67it/s]209720it [01:24, 3421.60it/s]205410it [01:24, 3371.03it/s]208837it [01:25, 3182.39it/s]213862it [01:27, 3153.65it/s]210064it [01:24, 3304.69it/s]205750it [01:24, 3265.47it/s]209159it [01:25, 3037.07it/s]214181it [01:27, 3019.27it/s]210430it [01:24, 3405.13it/s]206124it [01:24, 3400.56it/s]209499it [01:25, 3138.14it/s]214520it [01:27, 3121.82it/s]210772it [01:24, 3290.71it/s]206490it [01:24, 3474.78it/s]209837it [01:25, 3206.59it/s]214866it [01:27, 3216.70it/s]211131it [01:25, 3375.90it/s]206840it [01:24, 3354.82it/s]210161it [01:25, 3029.94it/s]215191it [01:27, 3057.54it/s]211494it [01:25, 3449.54it/s]207209it [01:25, 3444.73it/s]210507it [01:25, 3149.99it/s]215538it [01:27, 3173.17it/s]211841it [01:25, 3306.87it/s]207556it [01:25, 3328.63it/s]210826it [01:25, 2989.62it/s]215859it [01:27, 3036.88it/s]212207it [01:25, 3405.53it/s]207929it [01:25, 3441.76it/s]211168it [01:25, 3107.04it/s]216207it [01:27, 3158.95it/s]212550it [01:25, 3299.28it/s]208275it [01:25, 3308.98it/s]211505it [01:25, 3181.64it/s]216568it [01:28, 3285.40it/s]212920it [01:25, 3413.52it/s]208648it [01:25, 3426.65it/s]211826it [01:26, 3041.18it/s]216900it [01:28, 3132.48it/s]213264it [01:25, 3309.96it/s]209020it [01:25, 3509.82it/s]212166it [01:26, 3141.98it/s]217242it [01:28, 3211.45it/s]213623it [01:25, 3389.71it/s]209373it [01:25, 3378.88it/s]212483it [01:26, 2988.82it/s]213996it [01:25, 3486.93it/s]217566it [01:28, 3097.73it/s]209733it [01:25, 3440.71it/s]212833it [01:26, 3131.05it/s]217931it [01:28, 3252.46it/s]214347it [01:25, 3320.89it/s]210079it [01:25, 3311.02it/s]213173it [01:26, 3205.73it/s]214702it [01:26, 3383.87it/s]218271it [01:28, 3103.14it/s]210443it [01:26, 3403.26it/s]213497it [01:26, 3032.66it/s]218635it [01:28, 3251.54it/s]215043it [01:26, 3300.48it/s]210786it [01:26, 3298.18it/s]213848it [01:26, 3164.83it/s]218995it [01:28, 3350.52it/s]215410it [01:26, 3406.17it/s]211132it [01:26, 3341.94it/s]214168it [01:26, 2980.46it/s]215753it [01:26, 3308.78it/s]219333it [01:28, 3178.39it/s]211499it [01:26, 3435.38it/s]214506it [01:26, 3090.50it/s]216122it [01:26, 3417.74it/s]219683it [01:28, 3267.66it/s]211844it [01:26, 3311.41it/s]214840it [01:26, 3156.61it/s]216480it [01:26, 3462.82it/s]212199it [01:26, 3377.41it/s]220013it [01:29, 3112.68it/s]215159it [01:27, 3004.74it/s]216828it [01:26, 3331.05it/s]220380it [01:29, 3267.97it/s]212539it [01:26, 3273.77it/s]215498it [01:27, 3111.65it/s]217197it [01:26, 3431.65it/s]220737it [01:29, 3351.93it/s]212899it [01:26, 3365.47it/s]215813it [01:27, 2958.64it/s]217542it [01:26, 3346.35it/s]213237it [01:26, 3270.48it/s]221075it [01:29, 3185.02it/s]216144it [01:27, 3055.59it/s]217927it [01:27, 3489.38it/s]213585it [01:26, 3327.96it/s]221442it [01:29, 3320.63it/s]216487it [01:27, 3161.94it/s]218278it [01:27, 3389.07it/s]213949it [01:27, 3418.36it/s]221778it [01:29, 3175.16it/s]216806it [01:27, 3021.31it/s]218658it [01:27, 3506.09it/s]214292it [01:27, 3304.67it/s]222134it [01:29, 3281.19it/s]217167it [01:27, 3185.21it/s]219035it [01:27, 3581.03it/s]214649it [01:27, 3379.73it/s]222471it [01:29, 3090.82it/s]217489it [01:27, 3026.83it/s]219395it [01:27, 3456.03it/s]214989it [01:27, 3263.94it/s]222829it [01:29, 3223.63it/s]217847it [01:27, 3181.73it/s]219751it [01:27, 3484.45it/s]215353it [01:27, 3371.19it/s]223169it [01:30, 3271.23it/s]218209it [01:28, 3304.51it/s]220101it [01:27, 3406.24it/s]215717it [01:27, 3447.18it/s]223500it [01:30, 3101.29it/s]220492it [01:27, 3550.01it/s]218543it [01:28, 3140.20it/s]216064it [01:27, 3328.93it/s]223853it [01:30, 3219.50it/s]218898it [01:28, 3253.10it/s]220849it [01:27, 3434.92it/s]216415it [01:27, 3378.41it/s]224179it [01:30, 3060.18it/s]221220it [01:27, 3512.86it/s]219227it [01:28, 3097.08it/s]216755it [01:27, 3284.49it/s]224529it [01:30, 3180.78it/s]221605it [01:28, 3609.62it/s]219578it [01:28, 3212.35it/s]217125it [01:28, 3402.09it/s]224870it [01:30, 3245.07it/s]221968it [01:28, 3473.67it/s]219932it [01:28, 3304.71it/s]217467it [01:28, 3288.47it/s]225198it [01:30, 3065.63it/s]222318it [01:28, 3478.83it/s]220266it [01:28, 3144.23it/s]217849it [01:28, 3430.34it/s]225538it [01:30, 3157.50it/s]222668it [01:28, 3345.30it/s]220622it [01:28, 3257.18it/s]218219it [01:28, 3504.52it/s]225857it [01:30, 2965.90it/s]223028it [01:28, 3416.32it/s]218571it [01:28, 3379.78it/s]220951it [01:28, 3086.05it/s]226202it [01:31, 3099.57it/s]223372it [01:28, 3304.63it/s]218939it [01:28, 3464.50it/s]221277it [01:29, 3133.01it/s]226546it [01:31, 3195.76it/s]223736it [01:28, 3399.46it/s]219288it [01:28, 3359.29it/s]221630it [01:29, 3058.01it/s]226869it [01:31, 2992.65it/s]224112it [01:28, 3502.62it/s]219662it [01:28, 3467.96it/s]221985it [01:29, 3194.25it/s]227209it [01:31, 3104.44it/s]224464it [01:28, 3356.20it/s]220011it [01:28, 3374.70it/s]222323it [01:29, 3246.60it/s]227524it [01:31, 2925.75it/s]224807it [01:29, 3375.41it/s]220393it [01:28, 3501.46it/s]222650it [01:29, 3050.47it/s]227872it [01:31, 3075.04it/s]225147it [01:29, 3288.56it/s]220771it [01:29, 3580.22it/s]222992it [01:29, 3152.50it/s]228211it [01:31, 3157.54it/s]225505it [01:29, 3372.12it/s]221131it [01:29, 3441.08it/s]223311it [01:29, 2965.96it/s]228531it [01:31, 3001.71it/s]225844it [01:29, 3278.66it/s]221496it [01:29, 3499.47it/s]223660it [01:29, 3102.17it/s]228870it [01:31, 3108.29it/s]226204it [01:29, 3370.25it/s]221848it [01:29, 3419.46it/s]224018it [01:29, 3236.70it/s]226564it [01:29, 3434.70it/s]229191it [01:32, 2952.72it/s]222209it [01:29, 3472.30it/s]224346it [01:30, 3050.95it/s]226909it [01:29, 3304.14it/s]229551it [01:32, 3130.60it/s]222558it [01:29, 3358.12it/s]224687it [01:30, 3148.97it/s]227250it [01:29, 3333.10it/s]229897it [01:32, 3223.91it/s]222920it [01:29, 3431.13it/s]225006it [01:30, 2981.86it/s]227585it [01:29, 3246.90it/s]223288it [01:29, 3500.86it/s]230223it [01:32, 3049.52it/s]225352it [01:30, 3114.16it/s]227944it [01:29, 3343.49it/s]230562it [01:32, 3140.68it/s]223640it [01:29, 3344.74it/s]225693it [01:30, 3197.24it/s]228303it [01:30, 3413.70it/s]223998it [01:30, 3411.44it/s]230880it [01:32, 2962.56it/s]226016it [01:30, 3002.84it/s]228646it [01:30, 3300.13it/s]224341it [01:30, 3313.29it/s]231238it [01:32, 3131.45it/s]226350it [01:30, 3095.37it/s]229000it [01:30, 3368.63it/s]224700it [01:30, 3391.80it/s]231581it [01:32, 3215.39it/s]226670it [01:30, 2890.93it/s]229339it [01:30, 3279.79it/s]225041it [01:30, 3286.37it/s]231906it [01:32, 3063.66it/s]227008it [01:30, 3022.85it/s]229709it [01:30, 3399.85it/s]225403it [01:30, 3380.45it/s]232250it [01:33, 3167.03it/s]227339it [01:31, 3101.27it/s]230051it [01:30, 3235.60it/s]225759it [01:30, 3428.47it/s]232570it [01:33, 2979.55it/s]230411it [01:30, 3336.53it/s]227653it [01:31, 2918.37it/s]226104it [01:30, 3271.68it/s]232918it [01:33, 3117.76it/s]230769it [01:30, 3405.11it/s]227992it [01:31, 3047.01it/s]226458it [01:30, 3346.58it/s]233264it [01:33, 3213.30it/s]228328it [01:31, 3134.06it/s]231112it [01:30, 3309.31it/s]226795it [01:30, 3213.56it/s]233589it [01:33, 3053.40it/s]231474it [01:31, 3398.03it/s]228645it [01:31, 2958.76it/s]227150it [01:31, 3308.62it/s]233933it [01:33, 3161.73it/s]231816it [01:31, 3279.89it/s]228981it [01:31, 3069.54it/s]227502it [01:31, 3367.35it/s]234253it [01:33, 3001.77it/s]232178it [01:31, 3375.93it/s]229292it [01:31, 2933.29it/s]227841it [01:31, 3251.29it/s]234585it [01:33, 3090.47it/s]232522it [01:31, 3392.88it/s]229637it [01:31, 3075.80it/s]228195it [01:31, 3333.66it/s]234929it [01:33, 3189.09it/s]232863it [01:31, 3266.60it/s]229970it [01:31, 3145.87it/s]228530it [01:31, 3125.41it/s]235251it [01:33, 3020.98it/s]233230it [01:31, 3380.79it/s]230288it [01:31, 2984.74it/s]228890it [01:31, 3256.12it/s]235595it [01:34, 3138.32it/s]233570it [01:31, 3292.71it/s]230629it [01:32, 3104.00it/s]229219it [01:31, 3173.17it/s]235912it [01:34, 2978.14it/s]233935it [01:31, 3393.63it/s]230943it [01:32, 2958.88it/s]229593it [01:31, 3333.29it/s]236253it [01:34, 3098.57it/s]234276it [01:31, 3304.72it/s]231296it [01:32, 3117.21it/s]229954it [01:31, 3411.03it/s]236599it [01:34, 3201.25it/s]234634it [01:31, 3382.62it/s]231617it [01:32, 3143.47it/s]230298it [01:31, 3287.28it/s]236922it [01:34, 3042.50it/s]234985it [01:32, 3417.81it/s]231934it [01:32, 2991.84it/s]230654it [01:32, 3347.69it/s]237270it [01:34, 3163.34it/s]235328it [01:32, 3299.84it/s]232269it [01:32, 3085.40it/s]230991it [01:32, 3239.94it/s]237591it [01:34, 2972.24it/s]235677it [01:32, 3352.13it/s]231358it [01:32, 3362.19it/s]232581it [01:32, 2938.78it/s]237941it [01:34, 3109.70it/s]236014it [01:32, 3256.78it/s]232918it [01:32, 3057.36it/s]231709it [01:32, 3245.99it/s]238276it [01:34, 3176.90it/s]236371it [01:32, 3345.01it/s]233270it [01:32, 3187.64it/s]232068it [01:32, 3342.73it/s]236732it [01:32, 3421.62it/s]238597it [01:35, 3013.22it/s]232432it [01:32, 3426.96it/s]233592it [01:33, 3028.32it/s]238936it [01:35, 3116.82it/s]237076it [01:32, 3260.52it/s]233932it [01:33, 3132.74it/s]232777it [01:32, 3283.15it/s]237430it [01:32, 3337.93it/s]239271it [01:35, 2957.70it/s]233131it [01:32, 3353.46it/s]234249it [01:33, 2971.11it/s]237766it [01:32, 3241.86it/s]239620it [01:35, 3103.24it/s]234585it [01:33, 3077.24it/s]233469it [01:32, 3249.46it/s]238122it [01:33, 3331.88it/s]239960it [01:35, 3186.20it/s]234929it [01:33, 3170.36it/s]233831it [01:33, 3352.76it/s]238457it [01:33, 3232.50it/s]240283it [01:35, 2997.00it/s]234198it [01:33, 3442.65it/s]235249it [01:33, 3010.61it/s]238818it [01:33, 3338.36it/s]240632it [01:35, 3132.13it/s]234544it [01:33, 3299.25it/s]235583it [01:33, 3102.91it/s]239155it [01:33, 3345.97it/s]240951it [01:35, 2963.61it/s]234902it [01:33, 3377.59it/s]235909it [01:33, 2957.19it/s]239491it [01:33, 3250.79it/s]241304it [01:35, 3118.66it/s]235242it [01:33, 3243.21it/s]236251it [01:33, 3084.69it/s]239849it [01:33, 3343.22it/s]241647it [01:36, 3205.36it/s]235589it [01:33, 3307.07it/s]236563it [01:34, 3078.75it/s]240185it [01:33, 3229.42it/s]241972it [01:36, 3019.83it/s]235922it [01:33, 3216.39it/s]236874it [01:34, 2954.30it/s]240546it [01:33, 3337.18it/s]242324it [01:36, 3156.81it/s]236284it [01:33, 3329.38it/s]237216it [01:34, 3083.58it/s]240904it [01:33, 3406.51it/s]236635it [01:33, 3381.45it/s]242644it [01:36, 2980.70it/s]237553it [01:34, 3163.43it/s]241246it [01:33, 3264.64it/s]242992it [01:36, 3117.40it/s]236975it [01:33, 3273.78it/s]237872it [01:34, 2974.86it/s]241605it [01:34, 3356.76it/s]243334it [01:36, 3202.70it/s]237327it [01:34, 3343.49it/s]238209it [01:34, 3084.60it/s]241943it [01:34, 3266.54it/s]243658it [01:36, 3048.03it/s]237663it [01:34, 3203.90it/s]238521it [01:34, 2927.81it/s]242304it [01:34, 3363.05it/s]243988it [01:36, 3118.24it/s]238018it [01:34, 3302.41it/s]238855it [01:34, 3041.81it/s]242642it [01:34, 3252.35it/s]238369it [01:34, 3360.49it/s]244311it [01:36, 2957.58it/s]239193it [01:34, 3135.62it/s]243001it [01:34, 3346.70it/s]238707it [01:34, 3257.13it/s]244660it [01:37, 3104.75it/s]243348it [01:34, 3380.86it/s]239510it [01:35, 2951.46it/s]239055it [01:34, 3320.18it/s]244995it [01:37, 3173.80it/s]239844it [01:35, 3059.25it/s]243688it [01:34, 3253.30it/s]239389it [01:34, 3197.74it/s]245316it [01:37, 2993.22it/s]244049it [01:34, 3355.12it/s]240154it [01:35, 2920.55it/s]239752it [01:34, 3320.39it/s]245657it [01:37, 3108.78it/s]240497it [01:35, 3062.15it/s]244387it [01:34, 3253.60it/s]240093it [01:34, 3344.66it/s]245991it [01:37, 2955.55it/s]240832it [01:35, 3142.31it/s]244743it [01:35, 3341.20it/s]240429it [01:35, 3229.28it/s]246341it [01:37, 3098.78it/s]245105it [01:35, 3421.12it/s]241149it [01:35, 2990.06it/s]240791it [01:35, 3339.05it/s]246691it [01:37, 3200.85it/s]241488it [01:35, 3102.09it/s]245449it [01:35, 3255.42it/s]241127it [01:35, 3232.12it/s]247015it [01:37, 3041.13it/s]245812it [01:35, 3360.11it/s]241802it [01:35, 2910.97it/s]241491it [01:35, 3346.62it/s]247366it [01:37, 3169.83it/s]246151it [01:35, 3265.15it/s]242147it [01:35, 3060.27it/s]241828it [01:35, 3261.65it/s]247687it [01:38, 2994.76it/s]246513it [01:35, 3365.09it/s]242493it [01:35, 3172.60it/s]242176it [01:35, 3323.19it/s]248034it [01:38, 3124.77it/s]246852it [01:35, 3271.96it/s]242814it [01:36, 3005.33it/s]242538it [01:35, 3409.51it/s]248374it [01:38, 3202.34it/s]247201it [01:35, 3332.93it/s]243149it [01:36, 3100.30it/s]242881it [01:35, 3254.95it/s]247539it [01:35, 3345.81it/s]248698it [01:38, 3032.25it/s]243469it [01:36, 2952.19it/s]243243it [01:35, 3359.10it/s]249050it [01:38, 3167.47it/s]247875it [01:35, 3264.04it/s]243819it [01:36, 3095.59it/s]243581it [01:35, 3249.12it/s]248227it [01:36, 3337.07it/s]249371it [01:38, 2971.31it/s]244157it [01:36, 3175.96it/s]243942it [01:36, 3351.78it/s]248562it [01:36, 3242.83it/s]249721it [01:38, 3115.15it/s]244478it [01:36, 2995.82it/s]244301it [01:36, 3420.28it/s]248925it [01:36, 3354.00it/s]250069it [01:38, 3218.17it/s]244815it [01:36, 3099.34it/s]244645it [01:36, 3250.40it/s]249283it [01:36, 3419.22it/s]250395it [01:38, 3064.09it/s]245005it [01:36, 3347.13it/s]245149it [01:36, 2943.29it/s]249626it [01:36, 3272.97it/s]250749it [01:38, 3195.81it/s]245493it [01:36, 3077.99it/s]245343it [01:36, 3220.11it/s]249980it [01:36, 3346.95it/s]251073it [01:39, 3013.17it/s]245830it [01:37, 3159.71it/s]245704it [01:36, 3328.66it/s]250317it [01:36, 3257.81it/s]251409it [01:39, 3109.27it/s]246150it [01:37, 3001.27it/s]246040it [01:36, 3231.40it/s]250681it [01:36, 3365.46it/s]251750it [01:39, 3193.79it/s]246485it [01:37, 3096.32it/s]246392it [01:36, 3313.19it/s]251031it [01:36, 3271.83it/s]252073it [01:39, 3046.29it/s]246815it [01:37, 3153.69it/s]246742it [01:36, 3366.06it/s]251393it [01:37, 3370.58it/s]252422it [01:39, 3170.40it/s]247081it [01:37, 3252.61it/s]247133it [01:37, 2902.81it/s]251735it [01:37, 3384.34it/s]252743it [01:39, 2992.03it/s]247435it [01:37, 3333.28it/s]247472it [01:37, 3036.44it/s]252075it [01:37, 3296.80it/s]253093it [01:39, 3131.15it/s]247770it [01:37, 3231.19it/s]247781it [01:37, 2904.57it/s]252433it [01:37, 3376.88it/s]253434it [01:39, 3208.28it/s]248128it [01:37, 3330.75it/s]248121it [01:37, 3041.01it/s]252772it [01:37, 3264.65it/s]248488it [01:37, 3408.25it/s]248465it [01:37, 3152.10it/s]253132it [01:37, 3358.98it/s]248831it [01:37, 3292.64it/s]248784it [01:38, 2996.61it/s]253497it [01:37, 3442.23it/s]249184it [01:37, 3359.98it/s]249132it [01:38, 3131.31it/s]249522it [01:37, 3237.85it/s]249449it [01:38, 2951.48it/s]249882it [01:37, 3338.80it/s]249791it [01:38, 3080.56it/s]250218it [01:37, 3245.40it/s]250139it [01:38, 3192.75it/s]250588it [01:38, 3374.00it/s]250462it [01:38, 3029.67it/s]250953it [01:38, 3452.08it/s]250808it [01:38, 3148.67it/s]251300it [01:38, 3294.64it/s]251127it [01:38, 2974.15it/s]251659it [01:38, 3373.20it/s]251470it [01:38, 3098.62it/s]251999it [01:38, 3270.95it/s]251794it [01:39, 3136.62it/s]252363it [01:38, 3373.85it/s]252111it [01:39, 2994.03it/s]252709it [01:38, 3259.74it/s]252451it [01:39, 3106.97it/s]253073it [01:38, 3365.84it/s]252765it [01:39, 2945.78it/s]253437it [01:38, 3444.38it/s]253112it [01:39, 3089.70it/s]253460it [01:39, 3199.07it/s]253843it [01:44, 157.34it/s] 254209it [01:44, 223.10it/s]254511it [01:45, 296.15it/s]254877it [01:45, 417.11it/s]255239it [01:45, 573.60it/s]255565it [01:45, 744.55it/s]255930it [01:45, 990.78it/s]256262it [01:45, 1232.72it/s]256630it [01:45, 1557.20it/s]256980it [01:45, 1828.75it/s]257343it [01:45, 2156.52it/s]253758it [01:48, 126.93it/s] 257694it [01:46, 2435.09it/s]254108it [01:48, 180.91it/s]258037it [01:46, 2592.67it/s]254457it [01:48, 254.87it/s]258403it [01:46, 2849.27it/s]254747it [01:48, 335.63it/s]258746it [01:46, 2915.60it/s]255086it [01:48, 464.51it/s]259114it [01:46, 3115.75it/s]255385it [01:48, 603.47it/s]259475it [01:46, 3249.81it/s]255737it [01:49, 820.90it/s]259824it [01:46, 3182.38it/s]256089it [01:49, 1080.51it/s]260193it [01:46, 3322.52it/s]256410it [01:49, 1311.51it/s]253784it [01:46, 146.38it/s] 260538it [01:46, 3247.30it/s]256763it [01:49, 1632.11it/s]254145it [01:46, 206.55it/s]260903it [01:46, 3360.53it/s]257084it [01:49, 1833.70it/s]254457it [01:46, 276.82it/s]261247it [01:47, 3293.64it/s]257429it [01:49, 2141.50it/s]253783it [01:47, 135.02it/s] 254816it [01:47, 387.47it/s]261608it [01:47, 3382.20it/s]257771it [01:49, 2415.12it/s]254120it [01:47, 190.30it/s]255173it [01:47, 532.39it/s]261965it [01:47, 3433.95it/s]258095it [01:49, 2503.98it/s]255495it [01:47, 693.32it/s]254457it [01:47, 264.58it/s]262312it [01:47, 3321.79it/s]258445it [01:49, 2744.79it/s]255857it [01:47, 927.16it/s]254804it [01:47, 369.53it/s]262673it [01:47, 3404.22it/s]258767it [01:49, 2733.58it/s]255150it [01:47, 507.57it/s]256186it [01:47, 1158.67it/s]263016it [01:47, 3309.59it/s]259106it [01:50, 2902.43it/s]256533it [01:47, 1451.25it/s]255456it [01:48, 654.41it/s]263380it [01:47, 3401.81it/s]259453it [01:50, 3054.11it/s]256891it [01:47, 1777.02it/s]255801it [01:48, 873.04it/s]263722it [01:47, 3291.77it/s]259778it [01:50, 2956.77it/s]257229it [01:47, 2019.29it/s]256137it [01:48, 1096.50it/s]264075it [01:47, 3358.23it/s]260128it [01:50, 3105.56it/s]257589it [01:47, 2336.18it/s]256487it [01:48, 1390.24it/s]264444it [01:48, 3451.86it/s]260450it [01:50, 2965.19it/s]257926it [01:48, 2495.10it/s]256837it [01:48, 1703.02it/s]264791it [01:48, 3331.37it/s]260788it [01:50, 3078.31it/s]258287it [01:48, 2757.70it/s]257160it [01:48, 1906.68it/s]265159it [01:48, 3430.41it/s]261138it [01:50, 3196.72it/s]258648it [01:48, 2971.12it/s]257497it [01:48, 2191.77it/s]265504it [01:48, 3314.62it/s]261464it [01:50, 3036.31it/s]258992it [01:48, 2997.91it/s]257817it [01:48, 2308.97it/s]265868it [01:48, 3406.63it/s]261816it [01:50, 3170.77it/s]259337it [01:48, 3116.80it/s]258167it [01:48, 2580.78it/s]266220it [01:48, 3261.05it/s]259673it [01:48, 3084.11it/s]262138it [01:51, 3020.54it/s]258500it [01:49, 2765.50it/s]266590it [01:48, 3384.42it/s]260034it [01:48, 3229.08it/s]262485it [01:51, 3144.29it/s]258821it [01:49, 2744.13it/s]266955it [01:48, 3458.55it/s]262823it [01:51, 3210.05it/s]260370it [01:48, 3161.91it/s]259168it [01:49, 2933.29it/s]267303it [01:48, 3346.06it/s]260736it [01:48, 3300.86it/s]263148it [01:51, 3052.41it/s]267668it [01:48, 3432.60it/s]259497it [01:49, 2856.68it/s]261100it [01:48, 3396.85it/s]263495it [01:51, 3169.47it/s]259847it [01:49, 3023.93it/s]268014it [01:49, 3335.25it/s]261446it [01:49, 3286.84it/s]263816it [01:51, 3030.06it/s]260199it [01:49, 3159.86it/s]268371it [01:49, 3400.13it/s]261808it [01:49, 3380.87it/s]264166it [01:51, 3160.29it/s]268732it [01:49, 3460.64it/s]260526it [01:49, 3011.03it/s]264519it [01:51, 3264.47it/s]262150it [01:49, 3239.49it/s]269080it [01:49, 3333.85it/s]260873it [01:49, 3136.68it/s]262509it [01:49, 3336.27it/s]264849it [01:51, 3049.41it/s]269440it [01:49, 3406.10it/s]261194it [01:49, 2996.61it/s]262857it [01:49, 3243.81it/s]265204it [01:52, 3186.62it/s]269783it [01:49, 3315.47it/s]261543it [01:50, 3132.67it/s]263217it [01:49, 3342.76it/s]265527it [01:52, 3037.88it/s]270151it [01:49, 3418.58it/s]261896it [01:50, 3243.69it/s]263579it [01:49, 3422.28it/s]265880it [01:52, 3172.43it/s]270495it [01:49, 3289.08it/s]262225it [01:50, 3074.57it/s]263924it [01:49, 3307.94it/s]266220it [01:52, 3017.61it/s]270855it [01:49, 3376.31it/s]262571it [01:50, 3180.81it/s]264290it [01:49, 3407.57it/s]266565it [01:52, 3135.81it/s]271216it [01:50, 3442.05it/s]262893it [01:50, 3026.80it/s]264633it [01:50, 3254.69it/s]266918it [01:52, 3244.84it/s]271562it [01:50, 3312.39it/s]263229it [01:50, 3117.97it/s]264984it [01:50, 3326.47it/s]271924it [01:50, 3400.34it/s]267246it [01:52, 3061.59it/s]263558it [01:50, 3164.29it/s]265352it [01:50, 3426.87it/s]267598it [01:52, 3188.50it/s]272266it [01:50, 3295.38it/s]263877it [01:50, 3020.13it/s]265697it [01:50, 3319.24it/s]272614it [01:50, 3347.54it/s]267921it [01:52, 3047.11it/s]264229it [01:50, 3160.94it/s]266062it [01:50, 3412.87it/s]272951it [01:50, 3258.35it/s]268281it [01:53, 3199.01it/s]264548it [01:50, 3008.12it/s]266405it [01:50, 3297.41it/s]273311it [01:50, 3356.37it/s]268623it [01:53, 3261.19it/s]264896it [01:51, 3140.29it/s]266771it [01:50, 3400.55it/s]273676it [01:50, 3439.96it/s]268952it [01:53, 3088.66it/s]265251it [01:51, 3255.63it/s]267113it [01:50, 3297.13it/s]274022it [01:50, 3323.81it/s]269300it [01:53, 3196.98it/s]265580it [01:51, 3090.56it/s]267464it [01:50, 3357.97it/s]274385it [01:50, 3411.94it/s]269623it [01:53, 3051.51it/s]265929it [01:51, 3201.94it/s]267827it [01:50, 3436.29it/s]274728it [01:51, 3270.24it/s]269976it [01:53, 3184.98it/s]266253it [01:51, 3030.10it/s]268172it [01:51, 3327.17it/s]275093it [01:51, 3377.82it/s]270330it [01:53, 3284.64it/s]266608it [01:51, 3174.33it/s]268533it [01:51, 3408.47it/s]275457it [01:51, 3451.85it/s]270662it [01:53, 3071.49it/s]266958it [01:51, 3264.53it/s]268876it [01:51, 3299.87it/s]275804it [01:51, 3322.97it/s]271009it [01:53, 3181.00it/s]269235it [01:51, 3381.55it/s]267288it [01:51, 3094.55it/s]276166it [01:51, 3404.82it/s]271331it [01:54, 3028.52it/s]267638it [01:51, 3207.20it/s]269578it [01:51, 3268.93it/s]276509it [01:51, 3293.21it/s]271671it [01:54, 3130.00it/s]269940it [01:51, 3368.71it/s]267962it [01:52, 3052.47it/s]276860it [01:51, 3352.18it/s]272020it [01:54, 3232.06it/s]270284it [01:51, 3388.93it/s]268314it [01:52, 3182.57it/s]277197it [01:51, 3272.07it/s]272347it [01:54, 3055.68it/s]268668it [01:52, 3272.14it/s]270625it [01:51, 3289.95it/s]277557it [01:51, 3366.16it/s]272680it [01:54, 3131.80it/s]270986it [01:51, 3379.89it/s]268998it [01:52, 3051.52it/s]277916it [01:52, 3430.56it/s]272997it [01:54, 2978.06it/s]271326it [01:52, 3266.93it/s]269343it [01:52, 3161.84it/s]278261it [01:52, 3301.93it/s]273342it [01:54, 3107.81it/s]271681it [01:52, 3345.91it/s]269664it [01:52, 3012.17it/s]278624it [01:52, 3393.55it/s]273696it [01:54, 3230.35it/s]272046it [01:52, 3433.28it/s]270013it [01:52, 3143.88it/s]278965it [01:52, 3261.80it/s]274022it [01:54, 3056.17it/s]272391it [01:52, 3299.22it/s]270364it [01:52, 3247.21it/s]279324it [01:52, 3353.20it/s]274370it [01:54, 3173.80it/s]272748it [01:52, 3376.67it/s]270692it [01:52, 3080.76it/s]279662it [01:52, 3265.19it/s]273088it [01:52, 3229.63it/s]274691it [01:55, 2989.84it/s]271041it [01:53, 3193.49it/s]280024it [01:52, 3366.27it/s]273444it [01:52, 3321.22it/s]275039it [01:55, 3125.38it/s]280380it [01:52, 3421.42it/s]271364it [01:53, 3022.88it/s]275390it [01:55, 3224.58it/s]273779it [01:52, 3232.90it/s]271709it [01:53, 3141.28it/s]280724it [01:52, 3310.74it/s]274143it [01:52, 3348.90it/s]275716it [01:55, 3065.25it/s]272058it [01:53, 3238.18it/s]281073it [01:52, 3360.75it/s]274501it [01:52, 3415.65it/s]276065it [01:55, 3184.01it/s]281411it [01:53, 3267.49it/s]272385it [01:53, 3057.29it/s]274845it [01:53, 3283.63it/s]276387it [01:55, 3018.10it/s]281769it [01:53, 3357.12it/s]272728it [01:53, 3159.36it/s]275206it [01:53, 3376.11it/s]276731it [01:55, 3134.49it/s]282127it [01:53, 3420.78it/s]273048it [01:53, 3011.37it/s]275546it [01:53, 3233.90it/s]277072it [01:55, 3210.74it/s]282471it [01:53, 3302.27it/s]273390it [01:53, 3123.68it/s]275908it [01:53, 3341.89it/s]277396it [01:55, 3048.98it/s]282832it [01:53, 3390.21it/s]273741it [01:53, 3232.90it/s]276270it [01:53, 3420.24it/s]277746it [01:56, 3174.19it/s]283173it [01:53, 3245.78it/s]274068it [01:54, 3023.79it/s]276614it [01:53, 3286.42it/s]278067it [01:56, 3004.56it/s]283528it [01:53, 3332.32it/s]274412it [01:54, 3138.97it/s]276976it [01:53, 3380.49it/s]278411it [01:56, 3125.19it/s]283864it [01:53, 3231.58it/s]274730it [01:54, 2985.55it/s]277316it [01:53, 3275.96it/s]278747it [01:56, 3191.91it/s]284226it [01:53, 3340.33it/s]275080it [01:54, 3127.85it/s]277677it [01:53, 3369.09it/s]284584it [01:54, 3408.56it/s]279069it [01:56, 3033.18it/s]275424it [01:54, 3215.86it/s]278016it [01:54, 3244.08it/s]279412it [01:56, 3144.44it/s]284927it [01:54, 3283.05it/s]275749it [01:54, 3036.87it/s]278354it [01:54, 3282.30it/s]285267it [01:54, 3316.41it/s]279730it [01:56, 2989.91it/s]276095it [01:54, 3153.45it/s]278719it [01:54, 3388.15it/s]280077it [01:56, 3124.08it/s]285601it [01:54, 3231.08it/s]276414it [01:54, 3003.59it/s]279060it [01:54, 3266.53it/s]280426it [01:56, 3226.63it/s]285960it [01:54, 3331.99it/s]276754it [01:54, 3113.70it/s]279415it [01:54, 3347.34it/s]286319it [01:54, 3405.02it/s]280752it [01:57, 3056.71it/s]277105it [01:54, 3225.48it/s]279752it [01:54, 3234.84it/s]286661it [01:54, 3295.53it/s]281086it [01:57, 3134.62it/s]277431it [01:55, 3041.11it/s]280110it [01:54, 3331.17it/s]287028it [01:54, 3402.19it/s]287112it [01:54, 2501.51it/s]
281403it [01:57, 3000.07it/s]277780it [01:55, 3165.95it/s]280471it [01:54, 3409.75it/s]281743it [01:57, 3112.22it/s]278101it [01:55, 3008.37it/s]280814it [01:54, 3280.01it/s]282095it [01:57, 3227.01it/s]278447it [01:55, 3133.76it/s]281154it [01:54, 3312.88it/s]282421it [01:57, 3056.17it/s]278794it [01:55, 3229.19it/s]281487it [01:55, 3217.68it/s]282767it [01:57, 3169.49it/s]281846it [01:55, 3321.95it/s]279120it [01:55, 3012.48it/s]283087it [01:57, 3000.93it/s]279468it [01:55, 3140.73it/s]282180it [01:55, 3222.23it/s]283432it [01:57, 3125.86it/s]282538it [01:55, 3322.47it/s]279787it [01:55, 2997.23it/s]283784it [01:57, 3237.81it/s]282896it [01:55, 3395.69it/s]280130it [01:55, 3116.34it/s]284111it [01:58, 3067.46it/s]280479it [01:56, 3221.86it/s]283237it [01:55, 3273.97it/s]284453it [01:58, 3165.75it/s]283596it [01:55, 3364.13it/s]280805it [01:56, 3056.97it/s]284773it [01:58, 3005.88it/s]281152it [01:56, 3170.45it/s]283934it [01:55, 3205.20it/s]285121it [01:58, 3136.01it/s]284293it [01:55, 3313.70it/s]281473it [01:56, 3016.82it/s]285475it [01:58, 3249.75it/s]284647it [01:56, 3376.85it/s]281815it [01:56, 3127.91it/s]285803it [01:58, 3084.72it/s]284987it [01:56, 3264.26it/s]282162it [01:56, 3223.63it/s]286153it [01:58, 3199.74it/s]285348it [01:56, 3361.17it/s]282488it [01:56, 3049.45it/s]286477it [01:58, 3042.28it/s]285686it [01:56, 3249.20it/s]282834it [01:56, 3163.75it/s]286829it [01:58, 3173.37it/s]286041it [01:56, 3332.71it/s]283154it [01:56, 3002.58it/s]287112it [01:59, 2411.86it/s]
2022-08-10 15:18:16 | INFO | root | success load 287112 data
2022-08-10 15:18:16 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-08-10 15:18:16 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-08-10 15:18:16 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-08-10 15:18:16 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-08-10 15:18:16 | INFO | transformer.tokenization_utils | loading file None
2022-08-10 15:18:16 | INFO | transformer.tokenization_utils | loading file None
2022-08-10 15:18:16 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
286378it [01:56, 3198.42it/s]283499it [01:57, 3126.02it/s]286741it [01:56, 3320.04it/s]283844it [01:57, 3217.40it/s]287110it [01:56, 3424.81it/s]287112it [01:56, 2458.64it/s]
284169it [01:57, 3010.55it/s]284515it [01:57, 3134.94it/s]284833it [01:57, 3006.86it/s]285185it [01:57, 3148.24it/s]285537it [01:57, 3252.17it/s]285866it [01:57, 3086.26it/s]286218it [01:57, 3206.22it/s]286542it [01:58, 3060.61it/s]286898it [01:58, 3199.12it/s]287112it [01:58, 2429.10it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-08-10 15:22:11 | INFO | train_inner | epoch 001:   1200 / 2244 loss=13.334, nll_loss=8.279, mask_ins=1.711, word_ins_ml=9.239, word_reposition=1.401, kpe=0.984, ppl=10328.3, wps=2403.4, ups=0.2, wpb=12300.8, bsz=154, num_updates=1200, lr=0.000120076, gnorm=3.194, clip=0, loss_scale=512, train_wall=272, wall=0
2022-08-10 15:27:01 | INFO | train_inner | epoch 001:   1300 / 2244 loss=13.136, nll_loss=8.065, mask_ins=1.698, word_ins_ml=9.051, word_reposition=1.406, kpe=0.982, ppl=9003.62, wps=3544.7, ups=0.34, wpb=10299, bsz=128, num_updates=1300, lr=0.000130074, gnorm=3.039, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-10 15:32:33 | INFO | train_inner | epoch 001:   1400 / 2244 loss=13.003, nll_loss=7.874, mask_ins=1.723, word_ins_ml=8.882, word_reposition=1.414, kpe=0.984, ppl=8211.08, wps=3114.6, ups=0.3, wpb=10319.7, bsz=128, num_updates=1400, lr=0.000140072, gnorm=3.151, clip=0, loss_scale=512, train_wall=294, wall=0
2022-08-10 15:37:25 | INFO | train_inner | epoch 001:   1500 / 2244 loss=12.67, nll_loss=7.564, mask_ins=1.697, word_ins_ml=8.607, word_reposition=1.378, kpe=0.988, ppl=6515.1, wps=3503, ups=0.34, wpb=10233.9, bsz=128, num_updates=1500, lr=0.00015007, gnorm=3.323, clip=0, loss_scale=512, train_wall=253, wall=0
2022-08-10 15:42:18 | INFO | train_inner | epoch 001:   1600 / 2244 loss=12.368, nll_loss=7.19, mask_ins=1.693, word_ins_ml=8.28, word_reposition=1.393, kpe=1.001, ppl=5286.54, wps=3534.5, ups=0.34, wpb=10359.6, bsz=128, num_updates=1600, lr=0.000160068, gnorm=3.542, clip=0, loss_scale=512, train_wall=254, wall=0
2022-08-10 15:47:11 | INFO | train_inner | epoch 001:   1700 / 2244 loss=12.005, nll_loss=6.802, mask_ins=1.7, word_ins_ml=7.942, word_reposition=1.363, kpe=1.001, ppl=4110.08, wps=3511.5, ups=0.34, wpb=10290.1, bsz=128, num_updates=1700, lr=0.000170066, gnorm=3.809, clip=0, loss_scale=855, train_wall=254, wall=0
2022-08-10 15:52:03 | INFO | train_inner | epoch 001:   1800 / 2244 loss=11.677, nll_loss=6.487, mask_ins=1.676, word_ins_ml=7.667, word_reposition=1.334, kpe=0.999, ppl=3273.29, wps=3493.6, ups=0.34, wpb=10216.9, bsz=128, num_updates=1800, lr=0.000180064, gnorm=3.817, clip=0, loss_scale=1024, train_wall=253, wall=0
2022-08-10 15:56:56 | INFO | train_inner | epoch 001:   1900 / 2244 loss=11.357, nll_loss=6.139, mask_ins=1.685, word_ins_ml=7.366, word_reposition=1.304, kpe=1.002, ppl=2622.67, wps=3515.7, ups=0.34, wpb=10274.7, bsz=128, num_updates=1900, lr=0.000190062, gnorm=3.885, clip=0, loss_scale=1024, train_wall=253, wall=0
2022-08-10 16:02:33 | INFO | train_inner | epoch 001:   2000 / 2244 loss=11.081, nll_loss=5.863, mask_ins=1.672, word_ins_ml=7.125, word_reposition=1.283, kpe=1, ppl=2165.92, wps=3050.6, ups=0.3, wpb=10298.3, bsz=128, num_updates=2000, lr=0.00020006, gnorm=3.889, clip=0, loss_scale=1024, train_wall=298, wall=0
2022-08-10 16:07:26 | INFO | train_inner | epoch 001:   2100 / 2244 loss=10.829, nll_loss=5.624, mask_ins=1.65, word_ins_ml=6.918, word_reposition=1.258, kpe=1.002, ppl=1819.37, wps=3505.2, ups=0.34, wpb=10260.9, bsz=128, num_updates=2100, lr=0.000210058, gnorm=3.888, clip=0, loss_scale=1024, train_wall=253, wall=0
2022-08-10 16:12:19 | INFO | train_inner | epoch 001:   2200 / 2244 loss=10.632, nll_loss=5.371, mask_ins=1.678, word_ins_ml=6.698, word_reposition=1.253, kpe=1.003, ppl=1587.28, wps=3494.2, ups=0.34, wpb=10244.6, bsz=128, num_updates=2200, lr=0.000220056, gnorm=3.994, clip=0, loss_scale=1587, train_wall=254, wall=0
2022-08-10 16:14:27 | INFO | train | epoch 001 | loss 11.921 | nll_loss 6.748 | mask_ins 1.689 | word_ins_ml 7.898 | word_reposition 1.338 | kpe 0.995 | ppl 3877.74 | wps 3226.4 | ups 0.31 | wpb 10268.1 | bsz 127.9 | num_updates 2244 | lr 0.000224455 | gnorm 3.629 | clip 0 | loss_scale 881 | train_wall 2931 | wall 0
2022-08-10 16:16:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.113 | nll_loss 7.308 | mask_ins 1.756 | word_ins_ml 8.526 | word_reposition 1.44 | kpe 1.391 | ppl 8857.44 | wps 7400.8 | wpb 1183.8 | bsz 16 | num_updates 2244 | best_loss 13.113
2022-08-10 16:17:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_best.pt (epoch 1 @ 2244 updates, score 13.113) (writing took 66.35935029946268 seconds)
2022-08-10 16:20:32 | INFO | train_inner | epoch 002:     56 / 2244 loss=10.325, nll_loss=5.118, mask_ins=1.653, word_ins_ml=6.477, word_reposition=1.205, kpe=0.99, ppl=1282.49, wps=2066.3, ups=0.2, wpb=10174.7, bsz=127, num_updates=2300, lr=0.000230054, gnorm=4.055, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-08-10 16:25:25 | INFO | train_inner | epoch 002:    156 / 2244 loss=10.137, nll_loss=4.918, mask_ins=1.66, word_ins_ml=6.303, word_reposition=1.184, kpe=0.991, ppl=1126.17, wps=3506.2, ups=0.34, wpb=10276.8, bsz=128, num_updates=2400, lr=0.000240052, gnorm=4.059, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-08-10 16:30:17 | INFO | train_inner | epoch 002:    256 / 2244 loss=10.039, nll_loss=4.826, mask_ins=1.658, word_ins_ml=6.222, word_reposition=1.171, kpe=0.989, ppl=1052.17, wps=3525, ups=0.34, wpb=10289.7, bsz=128, num_updates=2500, lr=0.00025005, gnorm=4.028, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-08-10 16:35:58 | INFO | train_inner | epoch 002:    356 / 2244 loss=9.928, nll_loss=4.722, mask_ins=1.646, word_ins_ml=6.129, word_reposition=1.16, kpe=0.992, ppl=974.25, wps=3027, ups=0.29, wpb=10346.9, bsz=128, num_updates=2600, lr=0.000260048, gnorm=4.173, clip=0, loss_scale=2048, train_wall=303, wall=0
2022-08-10 16:40:55 | INFO | train_inner | epoch 002:    456 / 2244 loss=9.831, nll_loss=4.617, mask_ins=1.648, word_ins_ml=6.036, word_reposition=1.147, kpe=1.001, ppl=910.9, wps=3448.2, ups=0.34, wpb=10235.3, bsz=128, num_updates=2700, lr=0.000270046, gnorm=4.191, clip=0, loss_scale=2929, train_wall=256, wall=0
2022-08-10 16:45:47 | INFO | train_inner | epoch 002:    556 / 2244 loss=9.66, nll_loss=4.445, mask_ins=1.644, word_ins_ml=5.884, word_reposition=1.122, kpe=1.01, ppl=808.86, wps=3524.5, ups=0.34, wpb=10284.9, bsz=128, num_updates=2800, lr=0.000280044, gnorm=4.357, clip=0, loss_scale=4096, train_wall=253, wall=0
2022-08-10 16:50:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-10 16:50:42 | INFO | train_inner | epoch 002:    657 / 2244 loss=9.522, nll_loss=4.338, mask_ins=1.614, word_ins_ml=5.789, word_reposition=1.119, kpe=1, ppl=734.99, wps=3473.3, ups=0.34, wpb=10256.1, bsz=128, num_updates=2900, lr=0.000290042, gnorm=4.227, clip=0, loss_scale=3792, train_wall=255, wall=0
2022-08-10 16:55:35 | INFO | train_inner | epoch 002:    757 / 2244 loss=9.367, nll_loss=4.206, mask_ins=1.58, word_ins_ml=5.672, word_reposition=1.101, kpe=1.014, ppl=660.2, wps=3523.8, ups=0.34, wpb=10294.7, bsz=128, num_updates=3000, lr=0.00030004, gnorm=4.558, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-08-10 16:58:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-10 17:00:30 | INFO | train_inner | epoch 002:    858 / 2244 loss=nan, nll_loss=3.867, mask_ins=1.456, word_ins_ml=5.372, word_reposition=1.036, kpe=nan, ppl=nan, wps=3462.1, ups=0.34, wpb=10239.7, bsz=128, num_updates=3100, lr=0.000310038, gnorm=4.505, clip=0, loss_scale=1653, train_wall=256, wall=0
2022-08-10 17:06:11 | INFO | train_inner | epoch 002:    958 / 2244 loss=8.632, nll_loss=3.703, mask_ins=1.374, word_ins_ml=5.226, word_reposition=1.019, kpe=1.013, ppl=396.79, wps=3009.8, ups=0.29, wpb=10258.1, bsz=128, num_updates=3200, lr=0.000320036, gnorm=4.599, clip=0, loss_scale=1024, train_wall=302, wall=0
2022-08-10 17:10:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-10 17:11:08 | INFO | train_inner | epoch 002:   1059 / 2244 loss=8.525, nll_loss=3.624, mask_ins=1.338, word_ins_ml=5.155, word_reposition=1.011, kpe=1.02, ppl=368.24, wps=3470.3, ups=0.34, wpb=10304.2, bsz=128, num_updates=3300, lr=0.000330034, gnorm=4.982, clip=0, loss_scale=973, train_wall=257, wall=0
2022-08-10 17:16:01 | INFO | train_inner | epoch 002:   1159 / 2244 loss=8.516, nll_loss=3.606, mask_ins=1.346, word_ins_ml=5.138, word_reposition=1.003, kpe=1.029, ppl=366.06, wps=3492.8, ups=0.34, wpb=10242.1, bsz=128, num_updates=3400, lr=0.000340032, gnorm=5.171, clip=0, loss_scale=512, train_wall=253, wall=0
2022-08-10 17:20:53 | INFO | train_inner | epoch 002:   1259 / 2244 loss=8.367, nll_loss=3.48, mask_ins=1.323, word_ins_ml=5.025, word_reposition=0.996, kpe=1.023, ppl=330.23, wps=3535.4, ups=0.34, wpb=10294.9, bsz=128, num_updates=3500, lr=0.00035003, gnorm=4.613, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-10 17:25:44 | INFO | train_inner | epoch 002:   1359 / 2244 loss=nan, nll_loss=3.541, mask_ins=1.309, word_ins_ml=5.077, word_reposition=0.987, kpe=nan, ppl=nan, wps=3515.5, ups=0.34, wpb=10263.2, bsz=128, num_updates=3600, lr=0.000360028, gnorm=4.47, clip=0, loss_scale=512, train_wall=253, wall=0
2022-08-10 17:30:37 | INFO | train_inner | epoch 002:   1459 / 2244 loss=8.293, nll_loss=3.464, mask_ins=1.281, word_ins_ml=5.009, word_reposition=0.979, kpe=1.023, ppl=313.58, wps=3500, ups=0.34, wpb=10244.9, bsz=128, num_updates=3700, lr=0.000370026, gnorm=4.689, clip=0, loss_scale=512, train_wall=253, wall=0
2022-08-10 17:34:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-10 17:36:21 | INFO | train_inner | epoch 002:   1560 / 2244 loss=8.308, nll_loss=3.489, mask_ins=1.269, word_ins_ml=5.028, word_reposition=0.981, kpe=1.029, ppl=316.87, wps=2980.6, ups=0.29, wpb=10254.6, bsz=128, num_updates=3800, lr=0.000380024, gnorm=5.665, clip=1, loss_scale=451, train_wall=304, wall=0
2022-08-10 17:41:16 | INFO | train_inner | epoch 002:   1660 / 2244 loss=8.281, nll_loss=3.454, mask_ins=1.253, word_ins_ml=4.997, word_reposition=0.98, kpe=1.051, ppl=311.15, wps=3488.6, ups=0.34, wpb=10269.5, bsz=128, num_updates=3900, lr=0.000390022, gnorm=4.696, clip=0, loss_scale=256, train_wall=255, wall=0
2022-08-10 17:46:11 | INFO | train_inner | epoch 002:   1760 / 2244 loss=8.414, nll_loss=3.53, mask_ins=1.282, word_ins_ml=5.062, word_reposition=0.994, kpe=1.077, ppl=341.2, wps=3455, ups=0.34, wpb=10191.6, bsz=128, num_updates=4000, lr=0.00040002, gnorm=6.145, clip=1, loss_scale=256, train_wall=255, wall=0
2022-08-10 17:51:03 | INFO | train_inner | epoch 002:   1860 / 2244 loss=8.39, nll_loss=3.533, mask_ins=1.272, word_ins_ml=5.065, word_reposition=0.977, kpe=1.077, ppl=335.51, wps=3518.1, ups=0.34, wpb=10280.9, bsz=128, num_updates=4100, lr=0.000410018, gnorm=5.761, clip=0, loss_scale=256, train_wall=253, wall=0
2022-08-10 17:55:55 | INFO | train_inner | epoch 002:   1960 / 2244 loss=8.211, nll_loss=3.39, mask_ins=1.237, word_ins_ml=4.938, word_reposition=0.965, kpe=1.071, ppl=296.39, wps=3510.2, ups=0.34, wpb=10245.4, bsz=128, num_updates=4200, lr=0.000420016, gnorm=5.421, clip=1, loss_scale=256, train_wall=253, wall=0
2022-08-10 18:00:47 | INFO | train_inner | epoch 002:   2060 / 2244 loss=8.27, nll_loss=3.447, mask_ins=1.233, word_ins_ml=4.987, word_reposition=0.981, kpe=1.07, ppl=308.74, wps=3517.1, ups=0.34, wpb=10273.5, bsz=128, num_updates=4300, lr=0.000430014, gnorm=6.199, clip=1, loss_scale=287, train_wall=253, wall=0
2022-08-10 18:05:39 | INFO | train_inner | epoch 002:   2160 / 2244 loss=8.256, nll_loss=3.464, mask_ins=1.215, word_ins_ml=5, word_reposition=0.973, kpe=1.068, ppl=305.8, wps=3524.1, ups=0.34, wpb=10301, bsz=128, num_updates=4400, lr=0.000440012, gnorm=5.567, clip=0, loss_scale=512, train_wall=253, wall=0
2022-08-10 18:10:33 | INFO | train | epoch 002 | loss nan | nll_loss 3.902 | mask_ins 1.41 | word_ins_ml 5.398 | word_reposition 1.044 | kpe nan | ppl nan | wps 3300.2 | ups 0.32 | wpb 10264.1 | bsz 127.9 | num_updates 4484 | lr 0.00044841 | gnorm 4.9 | clip 0.2 | loss_scale 1276 | train_wall 5880 | wall 0
2022-08-10 18:12:47 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.995 | nll_loss 6.822 | mask_ins 1.907 | word_ins_ml 8.099 | word_reposition 1.421 | kpe 1.567 | ppl 8162.08 | wps 7415.9 | wpb 1183.8 | bsz 16 | num_updates 4484 | best_loss 12.995
2022-08-10 18:13:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_best.pt (epoch 2 @ 4484 updates, score 12.995) (writing took 55.18426292389631 seconds)
2022-08-10 18:14:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-10 18:14:32 | INFO | train_inner | epoch 003:     17 / 2244 loss=8.258, nll_loss=3.46, mask_ins=1.22, word_ins_ml=4.997, word_reposition=0.974, kpe=1.067, ppl=306.18, wps=1911.2, ups=0.19, wpb=10176.1, bsz=126.8, num_updates=4500, lr=0.00045001, gnorm=6.43, clip=0, loss_scale=492, train_wall=305, wall=0
2022-08-10 18:15:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-10 18:19:26 | INFO | train_inner | epoch 003:    118 / 2244 loss=8.189, nll_loss=3.4, mask_ins=1.223, word_ins_ml=4.943, word_reposition=0.975, kpe=1.048, ppl=291.79, wps=3494.6, ups=0.34, wpb=10296.5, bsz=128, num_updates=4600, lr=0.000460008, gnorm=6.651, clip=2, loss_scale=157, train_wall=255, wall=0
2022-08-10 18:24:18 | INFO | train_inner | epoch 003:    218 / 2244 loss=7.993, nll_loss=3.253, mask_ins=1.189, word_ins_ml=4.813, word_reposition=0.958, kpe=1.033, ppl=254.74, wps=3494, ups=0.34, wpb=10206.7, bsz=128, num_updates=4700, lr=0.000470006, gnorm=4.588, clip=0, loss_scale=128, train_wall=252, wall=0
2022-08-10 18:29:11 | INFO | train_inner | epoch 003:    318 / 2244 loss=8.152, nll_loss=3.374, mask_ins=1.213, word_ins_ml=4.918, word_reposition=0.968, kpe=1.053, ppl=284.47, wps=3514, ups=0.34, wpb=10269.3, bsz=128, num_updates=4800, lr=0.000480004, gnorm=5.869, clip=1, loss_scale=128, train_wall=253, wall=0
2022-08-10 18:34:03 | INFO | train_inner | epoch 003:    418 / 2244 loss=8.205, nll_loss=3.439, mask_ins=1.199, word_ins_ml=4.975, word_reposition=0.968, kpe=1.063, ppl=295.09, wps=3501.2, ups=0.34, wpb=10233.9, bsz=128, num_updates=4900, lr=0.000490002, gnorm=5.611, clip=0, loss_scale=128, train_wall=253, wall=0
2022-08-10 18:38:55 | INFO | train_inner | epoch 003:    518 / 2244 loss=8.27, nll_loss=3.418, mask_ins=1.248, word_ins_ml=4.956, word_reposition=0.984, kpe=1.082, ppl=308.74, wps=3525.7, ups=0.34, wpb=10315.6, bsz=128, num_updates=5000, lr=0.0005, gnorm=6.943, clip=2, loss_scale=128, train_wall=252, wall=0
2022-08-10 18:44:37 | INFO | train_inner | epoch 003:    618 / 2244 loss=8.281, nll_loss=3.492, mask_ins=1.203, word_ins_ml=5.021, word_reposition=0.965, kpe=1.092, ppl=311.08, wps=2993.4, ups=0.29, wpb=10223.1, bsz=128, num_updates=5100, lr=0.000495074, gnorm=6.367, clip=1, loss_scale=212, train_wall=302, wall=0
2022-08-10 18:49:33 | INFO | train_inner | epoch 003:    718 / 2244 loss=8.222, nll_loss=3.409, mask_ins=1.227, word_ins_ml=4.947, word_reposition=0.987, kpe=1.061, ppl=298.53, wps=3467.7, ups=0.34, wpb=10260.5, bsz=128, num_updates=5200, lr=0.00049029, gnorm=5.924, clip=1, loss_scale=256, train_wall=256, wall=0
2022-08-10 18:54:26 | INFO | train_inner | epoch 003:    818 / 2244 loss=nan, nll_loss=3.364, mask_ins=1.194, word_ins_ml=4.906, word_reposition=0.968, kpe=nan, ppl=nan, wps=3519.4, ups=0.34, wpb=10300.7, bsz=128, num_updates=5300, lr=0.000485643, gnorm=4.975, clip=0, loss_scale=256, train_wall=253, wall=0
2022-08-10 18:59:18 | INFO | train_inner | epoch 003:    918 / 2244 loss=8.054, nll_loss=3.259, mask_ins=1.208, word_ins_ml=4.814, word_reposition=0.966, kpe=1.066, ppl=265.71, wps=3505.7, ups=0.34, wpb=10266.5, bsz=128, num_updates=5400, lr=0.000481125, gnorm=5.701, clip=1, loss_scale=256, train_wall=252, wall=0
2022-08-10 19:04:11 | INFO | train_inner | epoch 003:   1018 / 2244 loss=8.047, nll_loss=3.28, mask_ins=1.199, word_ins_ml=4.832, word_reposition=0.96, kpe=1.057, ppl=264.46, wps=3490.7, ups=0.34, wpb=10208.3, bsz=128, num_updates=5500, lr=0.000476731, gnorm=5.604, clip=0, loss_scale=256, train_wall=253, wall=0
2022-08-10 19:06:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-10 19:09:07 | INFO | train_inner | epoch 003:   1119 / 2244 loss=8.127, nll_loss=3.368, mask_ins=1.199, word_ins_ml=4.905, word_reposition=0.959, kpe=1.063, ppl=279.52, wps=3480.8, ups=0.34, wpb=10294.8, bsz=128, num_updates=5600, lr=0.000472456, gnorm=5.124, clip=0, loss_scale=276, train_wall=256, wall=0
2022-08-10 19:14:47 | INFO | train_inner | epoch 003:   1219 / 2244 loss=8.084, nll_loss=3.304, mask_ins=1.191, word_ins_ml=4.851, word_reposition=0.961, kpe=1.08, ppl=271.27, wps=3003.7, ups=0.29, wpb=10230.5, bsz=128, num_updates=5700, lr=0.000468293, gnorm=6.49, clip=2, loss_scale=256, train_wall=289, wall=0
2022-08-10 19:19:39 | INFO | train_inner | epoch 003:   1319 / 2244 loss=nan, nll_loss=3.133, mask_ins=1.163, word_ins_ml=4.698, word_reposition=0.94, kpe=nan, ppl=nan, wps=3508.6, ups=0.34, wpb=10250.6, bsz=128, num_updates=5800, lr=0.000464238, gnorm=4.582, clip=0, loss_scale=256, train_wall=253, wall=0
2022-08-10 19:24:32 | INFO | train_inner | epoch 003:   1419 / 2244 loss=7.899, nll_loss=3.196, mask_ins=1.173, word_ins_ml=4.752, word_reposition=0.933, kpe=1.041, ppl=238.73, wps=3494.9, ups=0.34, wpb=10243.5, bsz=128, num_updates=5900, lr=0.000460287, gnorm=4.373, clip=0, loss_scale=256, train_wall=253, wall=0
2022-08-10 19:29:24 | INFO | train_inner | epoch 003:   1519 / 2244 loss=7.885, nll_loss=3.175, mask_ins=1.164, word_ins_ml=4.732, word_reposition=0.935, kpe=1.054, ppl=236.46, wps=3553.5, ups=0.34, wpb=10342.4, bsz=128, num_updates=6000, lr=0.000456435, gnorm=6.002, clip=2, loss_scale=256, train_wall=252, wall=0
2022-08-10 19:34:16 | INFO | train_inner | epoch 003:   1619 / 2244 loss=7.842, nll_loss=3.143, mask_ins=1.163, word_ins_ml=4.704, word_reposition=0.927, kpe=1.049, ppl=229.47, wps=3505.8, ups=0.34, wpb=10244.5, bsz=128, num_updates=6100, lr=0.000452679, gnorm=4.82, clip=1, loss_scale=346, train_wall=253, wall=0
2022-08-10 19:39:07 | INFO | train_inner | epoch 003:   1719 / 2244 loss=7.75, nll_loss=3.072, mask_ins=1.155, word_ins_ml=4.639, word_reposition=0.921, kpe=1.035, ppl=215.21, wps=3538.1, ups=0.34, wpb=10315, bsz=128, num_updates=6200, lr=0.000449013, gnorm=3.911, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-10 19:44:51 | INFO | train_inner | epoch 003:   1819 / 2244 loss=7.742, nll_loss=3.067, mask_ins=1.153, word_ins_ml=4.634, word_reposition=0.921, kpe=1.034, ppl=214.07, wps=2994.3, ups=0.29, wpb=10289.7, bsz=128, num_updates=6300, lr=0.000445435, gnorm=4.093, clip=0, loss_scale=512, train_wall=304, wall=0
2022-08-10 19:49:43 | INFO | train_inner | epoch 003:   1919 / 2244 loss=7.6, nll_loss=2.981, mask_ins=1.113, word_ins_ml=4.557, word_reposition=0.902, kpe=1.028, ppl=194.02, wps=3524.7, ups=0.34, wpb=10295.1, bsz=128, num_updates=6400, lr=0.000441942, gnorm=3.932, clip=0, loss_scale=512, train_wall=253, wall=0
2022-08-10 19:52:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-10 19:54:42 | INFO | train_inner | epoch 003:   2020 / 2244 loss=7.695, nll_loss=3.038, mask_ins=1.136, word_ins_ml=4.607, word_reposition=0.92, kpe=1.033, ppl=207.28, wps=3432.4, ups=0.33, wpb=10255.8, bsz=128, num_updates=6500, lr=0.000438529, gnorm=4.209, clip=0, loss_scale=370, train_wall=258, wall=0
2022-08-10 19:59:34 | INFO | train_inner | epoch 003:   2120 / 2244 loss=7.649, nll_loss=3.029, mask_ins=1.114, word_ins_ml=4.598, word_reposition=0.9, kpe=1.037, ppl=200.76, wps=3532.7, ups=0.34, wpb=10320.3, bsz=128, num_updates=6600, lr=0.000435194, gnorm=3.984, clip=0, loss_scale=256, train_wall=252, wall=0
2022-08-10 20:04:25 | INFO | train_inner | epoch 003:   2220 / 2244 loss=7.662, nll_loss=3.051, mask_ins=1.113, word_ins_ml=4.615, word_reposition=0.905, kpe=1.029, ppl=202.58, wps=3498.9, ups=0.34, wpb=10197.8, bsz=128, num_updates=6700, lr=0.000431934, gnorm=3.997, clip=0, loss_scale=256, train_wall=252, wall=0
2022-08-10 20:05:33 | INFO | train | epoch 003 | loss nan | nll_loss 3.237 | mask_ins 1.179 | word_ins_ml 4.791 | word_reposition 0.946 | kpe nan | ppl nan | wps 3331.9 | ups 0.32 | wpb 10263.4 | bsz 127.9 | num_updates 6724 | lr 0.000431163 | gnorm 5.172 | clip 0.6 | loss_scale 272 | train_wall 5810 | wall 0
2022-08-10 20:07:47 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.417 | nll_loss 6.633 | mask_ins 1.651 | word_ins_ml 7.95 | word_reposition 1.401 | kpe 1.415 | ppl 5467.75 | wps 7413.9 | wpb 1183.8 | bsz 16 | num_updates 6724 | best_loss 12.417
2022-08-10 20:08:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_best.pt (epoch 3 @ 6724 updates, score 12.417) (writing took 51.68267538771033 seconds)
2022-08-10 20:12:21 | INFO | train_inner | epoch 004:     76 / 2244 loss=7.6, nll_loss=2.985, mask_ins=1.127, word_ins_ml=4.558, word_reposition=0.893, kpe=1.021, ppl=193.96, wps=2133.6, ups=0.21, wpb=10149, bsz=126.8, num_updates=6800, lr=0.000428746, gnorm=4.139, clip=0, loss_scale=256, train_wall=251, wall=0
2022-08-10 20:18:03 | INFO | train_inner | epoch 004:    176 / 2244 loss=7.473, nll_loss=2.908, mask_ins=1.089, word_ins_ml=4.489, word_reposition=0.882, kpe=1.013, ppl=177.63, wps=2993.7, ups=0.29, wpb=10223.4, bsz=128, num_updates=6900, lr=0.000425628, gnorm=4.085, clip=0, loss_scale=256, train_wall=302, wall=0
2022-08-10 20:22:55 | INFO | train_inner | epoch 004:    276 / 2244 loss=7.485, nll_loss=2.922, mask_ins=1.091, word_ins_ml=4.5, word_reposition=0.878, kpe=1.016, ppl=179.17, wps=3509.1, ups=0.34, wpb=10265.7, bsz=128, num_updates=7000, lr=0.000422577, gnorm=3.914, clip=0, loss_scale=369, train_wall=253, wall=0
2022-08-10 20:27:48 | INFO | train_inner | epoch 004:    376 / 2244 loss=7.511, nll_loss=2.947, mask_ins=1.092, word_ins_ml=4.521, word_reposition=0.889, kpe=1.009, ppl=182.35, wps=3513.3, ups=0.34, wpb=10276.6, bsz=128, num_updates=7100, lr=0.000419591, gnorm=3.896, clip=0, loss_scale=512, train_wall=253, wall=0
2022-08-10 20:32:40 | INFO | train_inner | epoch 004:    476 / 2244 loss=7.376, nll_loss=2.841, mask_ins=1.069, word_ins_ml=4.427, word_reposition=0.876, kpe=1.004, ppl=166.16, wps=3503.2, ups=0.34, wpb=10239.9, bsz=128, num_updates=7200, lr=0.000416667, gnorm=3.883, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-10 20:37:31 | INFO | train_inner | epoch 004:    576 / 2244 loss=7.332, nll_loss=2.801, mask_ins=1.07, word_ins_ml=4.392, word_reposition=0.875, kpe=0.995, ppl=161.09, wps=3540.4, ups=0.34, wpb=10290, bsz=128, num_updates=7300, lr=0.000413803, gnorm=3.649, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-10 20:42:23 | INFO | train_inner | epoch 004:    676 / 2244 loss=7.454, nll_loss=2.901, mask_ins=1.103, word_ins_ml=4.478, word_reposition=0.874, kpe=0.999, ppl=175.29, wps=3533.9, ups=0.34, wpb=10322.9, bsz=128, num_updates=7400, lr=0.000410997, gnorm=3.541, clip=0, loss_scale=512, train_wall=253, wall=0
2022-08-10 20:48:05 | INFO | train_inner | epoch 004:    776 / 2244 loss=7.397, nll_loss=2.825, mask_ins=1.105, word_ins_ml=4.412, word_reposition=0.866, kpe=1.014, ppl=168.54, wps=3005.4, ups=0.29, wpb=10283, bsz=128, num_updates=7500, lr=0.000408248, gnorm=4.21, clip=0, loss_scale=676, train_wall=290, wall=0
2022-08-10 20:52:58 | INFO | train_inner | epoch 004:    876 / 2244 loss=7.33, nll_loss=2.816, mask_ins=1.068, word_ins_ml=4.402, word_reposition=0.863, kpe=0.996, ppl=160.9, wps=3504, ups=0.34, wpb=10257.1, bsz=128, num_updates=7600, lr=0.000405554, gnorm=3.768, clip=0, loss_scale=1024, train_wall=253, wall=0
2022-08-10 20:57:52 | INFO | train_inner | epoch 004:    976 / 2244 loss=7.363, nll_loss=2.826, mask_ins=1.086, word_ins_ml=4.411, word_reposition=0.868, kpe=0.999, ppl=164.67, wps=3464.7, ups=0.34, wpb=10211.6, bsz=128, num_updates=7700, lr=0.000402911, gnorm=3.506, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-08-10 21:02:44 | INFO | train_inner | epoch 004:   1076 / 2244 loss=7.333, nll_loss=2.81, mask_ins=1.076, word_ins_ml=4.394, word_reposition=0.868, kpe=0.995, ppl=161.26, wps=3525.7, ups=0.34, wpb=10285.9, bsz=128, num_updates=7800, lr=0.00040032, gnorm=3.674, clip=0, loss_scale=1024, train_wall=252, wall=0
2022-08-10 21:05:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-10 21:07:40 | INFO | train_inner | epoch 004:   1177 / 2244 loss=7.36, nll_loss=2.809, mask_ins=1.1, word_ins_ml=4.394, word_reposition=0.872, kpe=0.993, ppl=164.23, wps=3472.2, ups=0.34, wpb=10258.2, bsz=128, num_updates=7900, lr=0.000397779, gnorm=3.829, clip=0, loss_scale=821, train_wall=255, wall=0
2022-08-10 21:12:31 | INFO | train_inner | epoch 004:   1277 / 2244 loss=7.315, nll_loss=2.779, mask_ins=1.089, word_ins_ml=4.367, word_reposition=0.861, kpe=0.997, ppl=159.18, wps=3529.7, ups=0.34, wpb=10279.9, bsz=128, num_updates=8000, lr=0.000395285, gnorm=3.709, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-10 21:18:14 | INFO | train_inner | epoch 004:   1377 / 2244 loss=7.288, nll_loss=2.779, mask_ins=1.065, word_ins_ml=4.367, word_reposition=0.862, kpe=0.994, ppl=156.27, wps=2995.4, ups=0.29, wpb=10289.9, bsz=128, num_updates=8100, lr=0.000392837, gnorm=3.614, clip=0, loss_scale=512, train_wall=304, wall=0
2022-08-10 21:23:07 | INFO | train_inner | epoch 004:   1477 / 2244 loss=7.177, nll_loss=2.7, mask_ins=1.046, word_ins_ml=4.296, word_reposition=0.851, kpe=0.985, ppl=144.66, wps=3499.5, ups=0.34, wpb=10246.8, bsz=128, num_updates=8200, lr=0.000390434, gnorm=3.613, clip=0, loss_scale=512, train_wall=253, wall=0
2022-08-10 21:27:59 | INFO | train_inner | epoch 004:   1577 / 2244 loss=7.201, nll_loss=2.723, mask_ins=1.046, word_ins_ml=4.316, word_reposition=0.853, kpe=0.986, ppl=147.15, wps=3522.5, ups=0.34, wpb=10288.8, bsz=128, num_updates=8300, lr=0.000388075, gnorm=3.55, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-10 21:32:51 | INFO | train_inner | epoch 004:   1677 / 2244 loss=7.186, nll_loss=2.712, mask_ins=1.053, word_ins_ml=4.306, word_reposition=0.842, kpe=0.985, ppl=145.63, wps=3509.9, ups=0.34, wpb=10232.5, bsz=128, num_updates=8400, lr=0.000385758, gnorm=3.36, clip=0, loss_scale=655, train_wall=252, wall=0
2022-08-10 21:37:43 | INFO | train_inner | epoch 004:   1777 / 2244 loss=nan, nll_loss=2.726, mask_ins=1.048, word_ins_ml=4.317, word_reposition=0.85, kpe=nan, ppl=nan, wps=3502.4, ups=0.34, wpb=10231, bsz=128, num_updates=8500, lr=0.000383482, gnorm=3.543, clip=0, loss_scale=1024, train_wall=253, wall=0
2022-08-10 21:42:35 | INFO | train_inner | epoch 004:   1877 / 2244 loss=7.115, nll_loss=2.655, mask_ins=1.034, word_ins_ml=4.254, word_reposition=0.848, kpe=0.978, ppl=138.61, wps=3522.3, ups=0.34, wpb=10275.2, bsz=128, num_updates=8600, lr=0.000381246, gnorm=3.268, clip=0, loss_scale=1024, train_wall=253, wall=0
2022-08-10 21:44:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-10 21:44:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-10 21:47:57 | INFO | train_inner | epoch 004:   1979 / 2244 loss=nan, nll_loss=2.666, mask_ins=1.037, word_ins_ml=4.263, word_reposition=0.839, kpe=nan, ppl=nan, wps=3172.7, ups=0.31, wpb=10214.8, bsz=128, num_updates=8700, lr=0.000379049, gnorm=3.587, clip=0, loss_scale=545, train_wall=282, wall=0
2022-08-10 21:53:13 | INFO | train_inner | epoch 004:   2079 / 2244 loss=7.066, nll_loss=2.614, mask_ins=1.034, word_ins_ml=4.217, word_reposition=0.839, kpe=0.976, ppl=134.03, wps=3255.9, ups=0.32, wpb=10312, bsz=128, num_updates=8800, lr=0.000376889, gnorm=3.36, clip=0, loss_scale=256, train_wall=277, wall=0
2022-08-10 21:58:06 | INFO | train_inner | epoch 004:   2179 / 2244 loss=7.142, nll_loss=2.693, mask_ins=1.035, word_ins_ml=4.285, word_reposition=0.846, kpe=0.977, ppl=141.23, wps=3534.2, ups=0.34, wpb=10354.1, bsz=128, num_updates=8900, lr=0.000374766, gnorm=3.336, clip=0, loss_scale=256, train_wall=253, wall=0
2022-08-10 22:01:19 | INFO | train | epoch 004 | loss nan | nll_loss 2.786 | mask_ins 1.069 | word_ins_ml 4.374 | word_reposition 0.862 | kpe nan | ppl nan | wps 3311.6 | ups 0.32 | wpb 10264 | bsz 127.9 | num_updates 8965 | lr 0.000373405 | gnorm 3.671 | clip 0 | loss_scale 598 | train_wall 5860 | wall 0
2022-08-10 22:03:33 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.724 | nll_loss 6.102 | mask_ins 1.581 | word_ins_ml 7.435 | word_reposition 1.366 | kpe 1.342 | ppl 3382.76 | wps 7416.6 | wpb 1183.8 | bsz 16 | num_updates 8965 | best_loss 11.724
2022-08-10 22:04:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_best.pt (epoch 4 @ 8965 updates, score 11.724) (writing took 54.78390655294061 seconds)
2022-08-10 22:06:10 | INFO | train_inner | epoch 005:     35 / 2244 loss=7.111, nll_loss=2.669, mask_ins=1.037, word_ins_ml=4.265, word_reposition=0.836, kpe=0.974, ppl=138.28, wps=2108, ups=0.21, wpb=10186.4, bsz=126.8, num_updates=9000, lr=0.000372678, gnorm=3.591, clip=0, loss_scale=256, train_wall=255, wall=0
2022-08-10 22:11:01 | INFO | train_inner | epoch 005:    135 / 2244 loss=6.948, nll_loss=2.546, mask_ins=1.008, word_ins_ml=4.156, word_reposition=0.834, kpe=0.95, ppl=123.46, wps=3534.3, ups=0.34, wpb=10310.5, bsz=128, num_updates=9100, lr=0.000370625, gnorm=3.355, clip=0, loss_scale=256, train_wall=253, wall=0
2022-08-10 22:11:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-10 22:15:57 | INFO | train_inner | epoch 005:    236 / 2244 loss=7.069, nll_loss=2.638, mask_ins=1.025, word_ins_ml=4.236, word_reposition=0.84, kpe=0.968, ppl=134.29, wps=3467.4, ups=0.34, wpb=10256.1, bsz=128, num_updates=9200, lr=0.000368605, gnorm=4.036, clip=1, loss_scale=131, train_wall=256, wall=0
2022-08-10 22:21:14 | INFO | train_inner | epoch 005:    336 / 2244 loss=nan, nll_loss=2.632, mask_ins=1.001, word_ins_ml=4.23, word_reposition=0.83, kpe=nan, ppl=nan, wps=3225.2, ups=0.32, wpb=10235.9, bsz=128, num_updates=9300, lr=0.000366618, gnorm=3.469, clip=0, loss_scale=128, train_wall=278, wall=0
2022-08-10 22:26:31 | INFO | train_inner | epoch 005:    436 / 2244 loss=6.886, nll_loss=2.492, mask_ins=1.006, word_ins_ml=4.108, word_reposition=0.809, kpe=0.963, ppl=118.3, wps=3221.7, ups=0.32, wpb=10197.7, bsz=128, num_updates=9400, lr=0.000364662, gnorm=4.356, clip=1, loss_scale=128, train_wall=277, wall=0
2022-08-10 22:31:24 | INFO | train_inner | epoch 005:    536 / 2244 loss=6.985, nll_loss=2.586, mask_ins=1.016, word_ins_ml=4.189, word_reposition=0.82, kpe=0.96, ppl=126.68, wps=3496.1, ups=0.34, wpb=10235.3, bsz=128, num_updates=9500, lr=0.000362738, gnorm=3.829, clip=1, loss_scale=128, train_wall=254, wall=0
2022-08-10 22:36:16 | INFO | train_inner | epoch 005:    636 / 2244 loss=7.012, nll_loss=2.598, mask_ins=1.022, word_ins_ml=4.199, word_reposition=0.827, kpe=0.964, ppl=129.07, wps=3503.9, ups=0.34, wpb=10226.1, bsz=128, num_updates=9600, lr=0.000360844, gnorm=3.794, clip=0, loss_scale=128, train_wall=253, wall=0
2022-08-10 22:41:07 | INFO | train_inner | epoch 005:    736 / 2244 loss=6.919, nll_loss=2.517, mask_ins=1.008, word_ins_ml=4.128, word_reposition=0.829, kpe=0.954, ppl=120.99, wps=3524.4, ups=0.34, wpb=10259.4, bsz=128, num_updates=9700, lr=0.000358979, gnorm=3.476, clip=0, loss_scale=239, train_wall=253, wall=0
2022-08-10 22:46:00 | INFO | train_inner | epoch 005:    836 / 2244 loss=6.918, nll_loss=2.529, mask_ins=1.003, word_ins_ml=4.138, word_reposition=0.819, kpe=0.958, ppl=120.93, wps=3544.8, ups=0.34, wpb=10381.8, bsz=128, num_updates=9800, lr=0.000357143, gnorm=3.535, clip=0, loss_scale=256, train_wall=254, wall=0
2022-08-10 22:50:52 | INFO | train_inner | epoch 005:    936 / 2244 loss=6.898, nll_loss=2.521, mask_ins=0.999, word_ins_ml=4.13, word_reposition=0.806, kpe=0.963, ppl=119.26, wps=3518.3, ups=0.34, wpb=10278.7, bsz=128, num_updates=9900, lr=0.000355335, gnorm=3.435, clip=0, loss_scale=256, train_wall=253, wall=0
2022-08-10 22:56:33 | INFO | train_inner | epoch 005:   1036 / 2244 loss=6.867, nll_loss=2.495, mask_ins=0.992, word_ins_ml=4.106, word_reposition=0.808, kpe=0.96, ppl=116.75, wps=3018.3, ups=0.29, wpb=10307.3, bsz=128, num_updates=10000, lr=0.000353553, gnorm=3.394, clip=0, loss_scale=256, train_wall=303, wall=0
2022-08-10 23:01:25 | INFO | train_inner | epoch 005:   1136 / 2244 loss=nan, nll_loss=2.494, mask_ins=1.005, word_ins_ml=4.106, word_reposition=0.812, kpe=nan, ppl=nan, wps=3522.3, ups=0.34, wpb=10281.1, bsz=128, num_updates=10100, lr=0.000351799, gnorm=3.42, clip=0, loss_scale=256, train_wall=253, wall=0
2022-08-10 23:06:21 | INFO | train_inner | epoch 005:   1236 / 2244 loss=6.852, nll_loss=2.467, mask_ins=0.992, word_ins_ml=4.082, word_reposition=0.814, kpe=0.964, ppl=115.49, wps=3471.8, ups=0.34, wpb=10269.6, bsz=128, num_updates=10200, lr=0.00035007, gnorm=4.29, clip=1, loss_scale=448, train_wall=256, wall=0
2022-08-10 23:08:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-10 23:08:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-10 23:11:18 | INFO | train_inner | epoch 005:   1338 / 2244 loss=6.887, nll_loss=2.516, mask_ins=0.998, word_ins_ml=4.123, word_reposition=0.808, kpe=0.958, ppl=118.39, wps=3455.6, ups=0.34, wpb=10267.6, bsz=128, num_updates=10300, lr=0.000348367, gnorm=4.335, clip=2, loss_scale=284, train_wall=257, wall=0
2022-08-10 23:16:11 | INFO | train_inner | epoch 005:   1438 / 2244 loss=6.822, nll_loss=2.47, mask_ins=0.987, word_ins_ml=4.083, word_reposition=0.799, kpe=0.954, ppl=113.15, wps=3514.7, ups=0.34, wpb=10300.5, bsz=128, num_updates=10400, lr=0.000346688, gnorm=3.447, clip=0, loss_scale=128, train_wall=253, wall=0
2022-08-10 23:21:02 | INFO | train_inner | epoch 005:   1538 / 2244 loss=6.881, nll_loss=2.536, mask_ins=0.985, word_ins_ml=4.14, word_reposition=0.804, kpe=0.952, ppl=117.89, wps=3530.2, ups=0.34, wpb=10257.5, bsz=128, num_updates=10500, lr=0.000345033, gnorm=3.47, clip=0, loss_scale=128, train_wall=252, wall=0
2022-08-10 23:26:44 | INFO | train_inner | epoch 005:   1638 / 2244 loss=6.889, nll_loss=2.5, mask_ins=1.007, word_ins_ml=4.109, word_reposition=0.81, kpe=0.963, ppl=118.5, wps=2998.7, ups=0.29, wpb=10253, bsz=128, num_updates=10600, lr=0.000343401, gnorm=4.142, clip=2, loss_scale=128, train_wall=302, wall=0
2022-08-10 23:31:36 | INFO | train_inner | epoch 005:   1738 / 2244 loss=6.828, nll_loss=2.45, mask_ins=0.998, word_ins_ml=4.064, word_reposition=0.813, kpe=0.953, ppl=113.6, wps=3515.2, ups=0.34, wpb=10266.2, bsz=128, num_updates=10700, lr=0.000341793, gnorm=3.122, clip=0, loss_scale=128, train_wall=253, wall=0
2022-08-10 23:36:28 | INFO | train_inner | epoch 005:   1838 / 2244 loss=6.755, nll_loss=2.406, mask_ins=0.973, word_ins_ml=4.025, word_reposition=0.805, kpe=0.952, ppl=108.02, wps=3515.4, ups=0.34, wpb=10273.1, bsz=128, num_updates=10800, lr=0.000340207, gnorm=3.152, clip=0, loss_scale=177, train_wall=253, wall=0
2022-08-10 23:41:20 | INFO | train_inner | epoch 005:   1938 / 2244 loss=6.769, nll_loss=2.436, mask_ins=0.972, word_ins_ml=4.051, word_reposition=0.795, kpe=0.952, ppl=109.09, wps=3512.7, ups=0.34, wpb=10256.3, bsz=128, num_updates=10900, lr=0.000338643, gnorm=3.526, clip=0, loss_scale=256, train_wall=253, wall=0
2022-08-10 23:46:12 | INFO | train_inner | epoch 005:   2038 / 2244 loss=6.781, nll_loss=2.438, mask_ins=0.965, word_ins_ml=4.053, word_reposition=0.814, kpe=0.95, ppl=109.99, wps=3532.7, ups=0.34, wpb=10303.5, bsz=128, num_updates=11000, lr=0.0003371, gnorm=3.136, clip=0, loss_scale=256, train_wall=252, wall=0
2022-08-10 23:51:03 | INFO | train_inner | epoch 005:   2138 / 2244 loss=6.8, nll_loss=2.44, mask_ins=0.988, word_ins_ml=4.054, word_reposition=0.802, kpe=0.956, ppl=111.41, wps=3504, ups=0.34, wpb=10207, bsz=128, num_updates=11100, lr=0.000335578, gnorm=3.272, clip=0, loss_scale=256, train_wall=252, wall=0
2022-08-10 23:56:46 | INFO | train_inner | epoch 005:   2238 / 2244 loss=6.802, nll_loss=2.446, mask_ins=0.97, word_ins_ml=4.06, word_reposition=0.808, kpe=0.965, ppl=111.58, wps=3004.2, ups=0.29, wpb=10305.8, bsz=128, num_updates=11200, lr=0.000334077, gnorm=3.542, clip=0, loss_scale=256, train_wall=303, wall=0
2022-08-10 23:57:01 | INFO | train | epoch 005 | loss nan | nll_loss 2.511 | mask_ins 0.997 | word_ins_ml 4.12 | word_reposition 0.814 | kpe nan | ppl nan | wps 3313.3 | ups 0.32 | wpb 10264.1 | bsz 127.9 | num_updates 11206 | lr 0.000333987 | gnorm 3.617 | clip 0.4 | loss_scale 210 | train_wall 5873 | wall 0
2022-08-10 23:59:17 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.746 | nll_loss 6.086 | mask_ins 1.606 | word_ins_ml 7.427 | word_reposition 1.323 | kpe 1.391 | ppl 3435.24 | wps 7303.6 | wpb 1183.8 | bsz 16 | num_updates 11206 | best_loss 11.724
2022-08-10 23:59:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_last.pt (epoch 5 @ 11206 updates, score 11.746) (writing took 11.663719778880477 seconds)
2022-08-11 00:04:03 | INFO | train_inner | epoch 006:     94 / 2244 loss=6.683, nll_loss=2.383, mask_ins=0.962, word_ins_ml=4.004, word_reposition=0.795, kpe=0.923, ppl=102.77, wps=2329, ups=0.23, wpb=10176.2, bsz=126.8, num_updates=11300, lr=0.000332595, gnorm=3.359, clip=0, loss_scale=323, train_wall=251, wall=0
2022-08-11 00:08:55 | INFO | train_inner | epoch 006:    194 / 2244 loss=6.68, nll_loss=2.384, mask_ins=0.961, word_ins_ml=4.003, word_reposition=0.791, kpe=0.925, ppl=102.56, wps=3501.7, ups=0.34, wpb=10229, bsz=128, num_updates=11400, lr=0.000331133, gnorm=3.072, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-11 00:13:50 | INFO | train_inner | epoch 006:    294 / 2244 loss=6.64, nll_loss=2.34, mask_ins=0.962, word_ins_ml=3.964, word_reposition=0.785, kpe=0.929, ppl=99.75, wps=3482.6, ups=0.34, wpb=10288.4, bsz=128, num_updates=11500, lr=0.00032969, gnorm=3.028, clip=0, loss_scale=512, train_wall=255, wall=0
2022-08-11 00:18:42 | INFO | train_inner | epoch 006:    394 / 2244 loss=6.704, nll_loss=2.403, mask_ins=0.955, word_ins_ml=4.02, word_reposition=0.796, kpe=0.933, ppl=104.25, wps=3501.2, ups=0.34, wpb=10215.1, bsz=128, num_updates=11600, lr=0.000328266, gnorm=3.342, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-11 00:23:35 | INFO | train_inner | epoch 006:    494 / 2244 loss=6.721, nll_loss=2.422, mask_ins=0.97, word_ins_ml=4.035, word_reposition=0.785, kpe=0.93, ppl=105.46, wps=3505.2, ups=0.34, wpb=10249.6, bsz=128, num_updates=11700, lr=0.00032686, gnorm=3.314, clip=0, loss_scale=512, train_wall=253, wall=0
2022-08-11 00:24:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-11 00:29:20 | INFO | train_inner | epoch 006:    595 / 2244 loss=6.738, nll_loss=2.443, mask_ins=0.952, word_ins_ml=4.055, word_reposition=0.795, kpe=0.937, ppl=106.75, wps=2976, ups=0.29, wpb=10273.5, bsz=128, num_updates=11800, lr=0.000325472, gnorm=3.878, clip=0, loss_scale=294, train_wall=305, wall=0
2022-08-11 00:34:12 | INFO | train_inner | epoch 006:    695 / 2244 loss=6.599, nll_loss=2.339, mask_ins=0.936, word_ins_ml=3.962, word_reposition=0.774, kpe=0.928, ppl=96.95, wps=3516.7, ups=0.34, wpb=10284.8, bsz=128, num_updates=11900, lr=0.000324102, gnorm=3.17, clip=0, loss_scale=256, train_wall=253, wall=0
2022-08-11 00:39:05 | INFO | train_inner | epoch 006:    795 / 2244 loss=6.638, nll_loss=2.349, mask_ins=0.95, word_ins_ml=3.971, word_reposition=0.79, kpe=0.927, ppl=99.57, wps=3503.6, ups=0.34, wpb=10251.1, bsz=128, num_updates=12000, lr=0.000322749, gnorm=3.965, clip=1, loss_scale=256, train_wall=253, wall=0
2022-08-11 00:43:56 | INFO | train_inner | epoch 006:    895 / 2244 loss=6.728, nll_loss=2.432, mask_ins=0.969, word_ins_ml=4.044, word_reposition=0.784, kpe=0.932, ppl=106.01, wps=3523.5, ups=0.34, wpb=10262.8, bsz=128, num_updates=12100, lr=0.000321412, gnorm=3.988, clip=1, loss_scale=256, train_wall=252, wall=0
2022-08-11 00:48:48 | INFO | train_inner | epoch 006:    995 / 2244 loss=6.66, nll_loss=2.384, mask_ins=0.948, word_ins_ml=4.001, word_reposition=0.784, kpe=0.928, ppl=101.15, wps=3540.6, ups=0.34, wpb=10336.3, bsz=128, num_updates=12200, lr=0.000320092, gnorm=3.977, clip=1, loss_scale=256, train_wall=252, wall=0
2022-08-11 00:53:40 | INFO | train_inner | epoch 006:   1095 / 2244 loss=6.691, nll_loss=2.403, mask_ins=0.963, word_ins_ml=4.018, word_reposition=0.783, kpe=0.928, ppl=103.33, wps=3499.9, ups=0.34, wpb=10204.4, bsz=128, num_updates=12300, lr=0.000318788, gnorm=3.395, clip=0, loss_scale=445, train_wall=252, wall=0
2022-08-11 00:59:20 | INFO | train_inner | epoch 006:   1195 / 2244 loss=6.719, nll_loss=2.411, mask_ins=0.964, word_ins_ml=4.024, word_reposition=0.797, kpe=0.935, ppl=105.37, wps=3022.5, ups=0.29, wpb=10297.3, bsz=128, num_updates=12400, lr=0.0003175, gnorm=3.758, clip=0, loss_scale=512, train_wall=289, wall=0
2022-08-11 01:04:13 | INFO | train_inner | epoch 006:   1295 / 2244 loss=6.598, nll_loss=2.316, mask_ins=0.945, word_ins_ml=3.94, word_reposition=0.782, kpe=0.93, ppl=96.86, wps=3515.5, ups=0.34, wpb=10279.8, bsz=128, num_updates=12500, lr=0.000316228, gnorm=3.28, clip=0, loss_scale=512, train_wall=253, wall=0
2022-08-11 01:09:05 | INFO | train_inner | epoch 006:   1395 / 2244 loss=6.575, nll_loss=2.32, mask_ins=0.937, word_ins_ml=3.944, word_reposition=0.773, kpe=0.922, ppl=95.31, wps=3510.2, ups=0.34, wpb=10269.8, bsz=128, num_updates=12600, lr=0.00031497, gnorm=3.331, clip=0, loss_scale=512, train_wall=253, wall=0
2022-08-11 01:13:57 | INFO | train_inner | epoch 006:   1495 / 2244 loss=6.669, nll_loss=2.391, mask_ins=0.954, word_ins_ml=4.005, word_reposition=0.787, kpe=0.923, ppl=101.76, wps=3550.8, ups=0.34, wpb=10352.4, bsz=128, num_updates=12700, lr=0.000313728, gnorm=3.236, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-11 01:16:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 01:18:54 | INFO | train_inner | epoch 006:   1596 / 2244 loss=6.631, nll_loss=2.364, mask_ins=0.943, word_ins_ml=3.982, word_reposition=0.777, kpe=0.929, ppl=99.09, wps=3440.8, ups=0.34, wpb=10241.5, bsz=128, num_updates=12800, lr=0.0003125, gnorm=3.345, clip=0, loss_scale=583, train_wall=257, wall=0
2022-08-11 01:23:47 | INFO | train_inner | epoch 006:   1696 / 2244 loss=6.614, nll_loss=2.33, mask_ins=0.957, word_ins_ml=3.952, word_reposition=0.772, kpe=0.933, ppl=97.96, wps=3493.6, ups=0.34, wpb=10214.7, bsz=128, num_updates=12900, lr=0.000311286, gnorm=3.223, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-11 01:29:16 | INFO | train_inner | epoch 006:   1796 / 2244 loss=6.566, nll_loss=2.321, mask_ins=0.925, word_ins_ml=3.943, word_reposition=0.772, kpe=0.925, ppl=94.73, wps=3110.6, ups=0.3, wpb=10245.3, bsz=128, num_updates=13000, lr=0.000310087, gnorm=3.025, clip=0, loss_scale=512, train_wall=290, wall=0
2022-08-11 01:34:10 | INFO | train_inner | epoch 006:   1896 / 2244 loss=6.564, nll_loss=2.308, mask_ins=0.935, word_ins_ml=3.931, word_reposition=0.778, kpe=0.92, ppl=94.59, wps=3518.3, ups=0.34, wpb=10322.3, bsz=128, num_updates=13100, lr=0.000308901, gnorm=3.087, clip=0, loss_scale=512, train_wall=254, wall=0
2022-08-11 01:39:01 | INFO | train_inner | epoch 006:   1996 / 2244 loss=nan, nll_loss=2.304, mask_ins=0.941, word_ins_ml=3.928, word_reposition=0.78, kpe=nan, ppl=nan, wps=3528.8, ups=0.34, wpb=10286.5, bsz=128, num_updates=13200, lr=0.000307729, gnorm=3.107, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-11 01:42:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-11 01:43:55 | INFO | train_inner | epoch 006:   2097 / 2244 loss=nan, nll_loss=2.361, mask_ins=0.943, word_ins_ml=3.977, word_reposition=0.777, kpe=nan, ppl=nan, wps=3485.2, ups=0.34, wpb=10248.6, bsz=128, num_updates=13300, lr=0.00030657, gnorm=3.243, clip=0, loss_scale=598, train_wall=255, wall=0
2022-08-11 01:48:47 | INFO | train_inner | epoch 006:   2197 / 2244 loss=6.536, nll_loss=2.293, mask_ins=0.927, word_ins_ml=3.917, word_reposition=0.771, kpe=0.921, ppl=92.77, wps=3512.5, ups=0.34, wpb=10253.9, bsz=128, num_updates=13400, lr=0.000305424, gnorm=3.196, clip=0, loss_scale=512, train_wall=252, wall=0
2022-08-11 01:51:02 | INFO | train | epoch 006 | loss nan | nll_loss 2.363 | mask_ins 0.949 | word_ins_ml 3.982 | word_reposition 0.783 | kpe nan | ppl nan | wps 3362.3 | ups 0.33 | wpb 10264 | bsz 127.9 | num_updates 13447 | lr 0.000304889 | gnorm 3.372 | clip 0.1 | loss_scale 453 | train_wall 5794 | wall 0
2022-08-11 01:53:16 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.568 | nll_loss 5.905 | mask_ins 1.604 | word_ins_ml 7.255 | word_reposition 1.299 | kpe 1.41 | ppl 3035.6 | wps 7424.1 | wpb 1183.8 | bsz 16 | num_updates 13447 | best_loss 11.568
2022-08-11 01:54:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_best.pt (epoch 6 @ 13447 updates, score 11.568) (writing took 65.31664147228003 seconds)
2022-08-11 01:56:56 | INFO | train_inner | epoch 007:     53 / 2244 loss=6.567, nll_loss=2.326, mask_ins=0.936, word_ins_ml=3.947, word_reposition=0.779, kpe=0.906, ppl=94.82, wps=2077.2, ups=0.2, wpb=10145.3, bsz=126.8, num_updates=13500, lr=0.00030429, gnorm=3.095, clip=0, loss_scale=512, train_wall=251, wall=0
2022-08-11 02:02:26 | INFO | train_inner | epoch 007:    153 / 2244 loss=6.524, nll_loss=2.285, mask_ins=0.94, word_ins_ml=3.91, word_reposition=0.779, kpe=0.895, ppl=92.06, wps=3102.2, ups=0.3, wpb=10238.4, bsz=128, num_updates=13600, lr=0.00030317, gnorm=3.091, clip=0, loss_scale=512, train_wall=290, wall=0
2022-08-11 02:07:31 | INFO | train_inner | epoch 007:    253 / 2244 loss=6.421, nll_loss=2.23, mask_ins=0.915, word_ins_ml=3.861, word_reposition=0.762, kpe=0.882, ppl=85.67, wps=3378.8, ups=0.33, wpb=10301.7, bsz=128, num_updates=13700, lr=0.000302061, gnorm=3.038, clip=0, loss_scale=512, train_wall=265, wall=0
2022-08-11 02:12:23 | INFO | train_inner | epoch 007:    353 / 2244 loss=6.481, nll_loss=2.282, mask_ins=0.914, word_ins_ml=3.907, word_reposition=0.768, kpe=0.892, ppl=89.32, wps=3516.9, ups=0.34, wpb=10279.8, bsz=128, num_updates=13800, lr=0.000300965, gnorm=3.18, clip=0, loss_scale=558, train_wall=253, wall=0
2022-08-11 02:17:14 | INFO | train_inner | epoch 007:    453 / 2244 loss=6.476, nll_loss=2.271, mask_ins=0.924, word_ins_ml=3.897, word_reposition=0.764, kpe=0.891, ppl=88.99, wps=3539.3, ups=0.34, wpb=10322.6, bsz=128, num_updates=13900, lr=0.00029988, gnorm=3.094, clip=0, loss_scale=1024, train_wall=252, wall=0
2022-08-11 02:22:08 | INFO | train_inner | epoch 007:    553 / 2244 loss=6.476, nll_loss=2.29, mask_ins=0.907, word_ins_ml=3.913, word_reposition=0.763, kpe=0.893, ppl=89.04, wps=3468.7, ups=0.34, wpb=10195, bsz=128, num_updates=14000, lr=0.000298807, gnorm=3.033, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-08-11 02:27:01 | INFO | train_inner | epoch 007:    653 / 2244 loss=6.471, nll_loss=2.264, mask_ins=0.924, word_ins_ml=3.891, word_reposition=0.765, kpe=0.891, ppl=88.72, wps=3524.8, ups=0.34, wpb=10313.2, bsz=128, num_updates=14100, lr=0.000297746, gnorm=2.929, clip=0, loss_scale=1024, train_wall=254, wall=0
2022-08-11 02:31:54 | INFO | train_inner | epoch 007:    753 / 2244 loss=6.451, nll_loss=2.26, mask_ins=0.909, word_ins_ml=3.886, word_reposition=0.766, kpe=0.89, ppl=87.47, wps=3510.5, ups=0.34, wpb=10270.1, bsz=128, num_updates=14200, lr=0.000296695, gnorm=2.934, clip=0, loss_scale=1024, train_wall=254, wall=0
2022-08-11 02:37:37 | INFO | train_inner | epoch 007:    853 / 2244 loss=6.477, nll_loss=2.279, mask_ins=0.915, word_ins_ml=3.903, word_reposition=0.77, kpe=0.889, ppl=89.06, wps=3000.9, ups=0.29, wpb=10307.9, bsz=128, num_updates=14300, lr=0.000295656, gnorm=2.965, clip=0, loss_scale=1024, train_wall=304, wall=0
2022-08-11 02:42:29 | INFO | train_inner | epoch 007:    953 / 2244 loss=nan, nll_loss=2.228, mask_ins=0.888, word_ins_ml=3.858, word_reposition=0.754, kpe=nan, ppl=nan, wps=3511.2, ups=0.34, wpb=10261.1, bsz=128, num_updates=14400, lr=0.000294628, gnorm=2.973, clip=0, loss_scale=2017, train_wall=253, wall=0
2022-08-11 02:47:22 | INFO | train_inner | epoch 007:   1053 / 2244 loss=6.52, nll_loss=2.304, mask_ins=0.932, word_ins_ml=3.924, word_reposition=0.766, kpe=0.899, ppl=91.79, wps=3506.6, ups=0.34, wpb=10248.9, bsz=128, num_updates=14500, lr=0.00029361, gnorm=2.947, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-08-11 02:47:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-11 02:52:16 | INFO | train_inner | epoch 007:   1154 / 2244 loss=6.416, nll_loss=2.225, mask_ins=0.907, word_ins_ml=3.854, word_reposition=0.759, kpe=0.896, ppl=85.37, wps=3483.1, ups=0.34, wpb=10267.8, bsz=128, num_updates=14600, lr=0.000292603, gnorm=2.824, clip=0, loss_scale=1085, train_wall=255, wall=0
2022-08-11 02:57:09 | INFO | train_inner | epoch 007:   1254 / 2244 loss=nan, nll_loss=2.3, mask_ins=0.951, word_ins_ml=3.92, word_reposition=0.771, kpe=nan, ppl=nan, wps=3526.9, ups=0.34, wpb=10318, bsz=128, num_updates=14700, lr=0.000291606, gnorm=3.088, clip=0, loss_scale=1024, train_wall=253, wall=0
2022-08-11 03:02:04 | INFO | train_inner | epoch 007:   1354 / 2244 loss=6.473, nll_loss=2.29, mask_ins=0.914, word_ins_ml=3.911, word_reposition=0.757, kpe=0.89, ppl=88.8, wps=3454.7, ups=0.34, wpb=10188.7, bsz=128, num_updates=14800, lr=0.000290619, gnorm=2.968, clip=0, loss_scale=1024, train_wall=255, wall=0
2022-08-11 03:07:35 | INFO | train_inner | epoch 007:   1454 / 2244 loss=6.431, nll_loss=2.253, mask_ins=0.9, word_ins_ml=3.878, word_reposition=0.76, kpe=0.893, ppl=86.31, wps=3125.2, ups=0.3, wpb=10351.4, bsz=128, num_updates=14900, lr=0.000289642, gnorm=2.901, clip=0, loss_scale=1024, train_wall=291, wall=0
2022-08-11 03:12:28 | INFO | train_inner | epoch 007:   1554 / 2244 loss=6.431, nll_loss=2.254, mask_ins=0.905, word_ins_ml=3.879, word_reposition=0.759, kpe=0.887, ppl=86.27, wps=3502.3, ups=0.34, wpb=10243.1, bsz=128, num_updates=15000, lr=0.000288675, gnorm=2.962, clip=0, loss_scale=1024, train_wall=253, wall=0
2022-08-11 03:17:20 | INFO | train_inner | epoch 007:   1654 / 2244 loss=6.477, nll_loss=2.266, mask_ins=0.923, word_ins_ml=3.889, word_reposition=0.767, kpe=0.898, ppl=89.06, wps=3512.2, ups=0.34, wpb=10261, bsz=128, num_updates=15100, lr=0.000287718, gnorm=2.947, clip=0, loss_scale=1874, train_wall=253, wall=0
2022-08-11 03:22:12 | INFO | train_inner | epoch 007:   1754 / 2244 loss=6.466, nll_loss=2.253, mask_ins=0.918, word_ins_ml=3.878, word_reposition=0.772, kpe=0.898, ppl=88.4, wps=3495.7, ups=0.34, wpb=10209.5, bsz=128, num_updates=15200, lr=0.00028677, gnorm=2.945, clip=0, loss_scale=2048, train_wall=253, wall=0
2022-08-11 03:23:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-11 03:27:11 | INFO | train_inner | epoch 007:   1855 / 2244 loss=6.389, nll_loss=2.214, mask_ins=0.898, word_ins_ml=3.844, word_reposition=0.75, kpe=0.898, ppl=83.81, wps=3422.7, ups=0.33, wpb=10230, bsz=128, num_updates=15300, lr=0.000285831, gnorm=3.177, clip=0, loss_scale=1186, train_wall=258, wall=0
2022-08-11 03:32:01 | INFO | train_inner | epoch 007:   1955 / 2244 loss=6.408, nll_loss=2.226, mask_ins=0.909, word_ins_ml=3.853, word_reposition=0.752, kpe=0.894, ppl=84.91, wps=3546.5, ups=0.34, wpb=10305.4, bsz=128, num_updates=15400, lr=0.000284901, gnorm=3.042, clip=0, loss_scale=1024, train_wall=252, wall=0
2022-08-11 03:37:44 | INFO | train_inner | epoch 007:   2055 / 2244 loss=6.413, nll_loss=2.219, mask_ins=0.91, word_ins_ml=3.846, word_reposition=0.764, kpe=0.892, ppl=85.2, wps=2993.3, ups=0.29, wpb=10258.5, bsz=128, num_updates=15500, lr=0.000283981, gnorm=2.92, clip=0, loss_scale=1024, train_wall=303, wall=0
2022-08-11 03:42:36 | INFO | train_inner | epoch 007:   2155 / 2244 loss=6.42, nll_loss=2.232, mask_ins=0.9, word_ins_ml=3.858, word_reposition=0.76, kpe=0.902, ppl=85.65, wps=3538.2, ups=0.34, wpb=10329.6, bsz=128, num_updates=15600, lr=0.000283069, gnorm=2.948, clip=0, loss_scale=1024, train_wall=253, wall=0
2022-08-11 03:46:54 | INFO | train | epoch 007 | loss nan | nll_loss 2.257 | mask_ins 0.914 | word_ins_ml 3.883 | word_reposition 0.763 | kpe nan | ppl nan | wps 3310.4 | ups 0.32 | wpb 10264 | bsz 127.9 | num_updates 15689 | lr 0.000282265 | gnorm 2.999 | clip 0 | loss_scale 1129 | train_wall 5867 | wall 0
2022-08-11 03:49:07 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.292 | nll_loss 5.814 | mask_ins 1.544 | word_ins_ml 7.166 | word_reposition 1.209 | kpe 1.373 | ppl 2507.84 | wps 7435.8 | wpb 1183.8 | bsz 16 | num_updates 15689 | best_loss 11.292
2022-08-11 03:49:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_kpe_cased/checkpoint_best.pt (epoch 7 @ 15689 updates, score 11.292) (writing took 15.719538524746895 seconds)
2022-08-11 03:49:54 | INFO | train_inner | epoch 008:     11 / 2244 loss=6.359, nll_loss=2.197, mask_ins=0.888, word_ins_ml=3.827, word_reposition=0.755, kpe=0.889, ppl=82.08, wps=2314, ups=0.23, wpb=10145.9, bsz=126.8, num_updates=15700, lr=0.000282166, gnorm=3.065, clip=0, loss_scale=1024, train_wall=251, wall=0
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
train.sh: line 42: 2,12: command not found
