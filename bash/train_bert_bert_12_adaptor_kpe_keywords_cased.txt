nohup: ignoring input
2022-07-12 14:39:16 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:17721
2022-07-12 14:39:16 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:17721
2022-07-12 14:39:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-12 14:39:16 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:17721
2022-07-12 14:39:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-12 14:39:16 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:17721
2022-07-12 14:39:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-12 14:39:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-12 14:39:16 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-12 14:39:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-12 14:39:16 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-12 14:39:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-12 14:39:16 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-12 14:39:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-12 14:39:16 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-12 14:39:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-12 14:39:20 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:17721', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_keywords_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='/data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='keywords', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-12 14:39:20 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-12 14:39:20 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-07-12 14:39:20 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.source
2022-07-12 14:39:20 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.target
2022-07-12 14:39:20 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]388it [00:00, 3877.05it/s]389it [00:00, 3882.55it/s]389it [00:00, 3888.68it/s]392it [00:00, 3903.40it/s]776it [00:00, 3525.57it/s]778it [00:00, 3513.07it/s]778it [00:00, 3541.55it/s]783it [00:00, 3468.59it/s]1145it [00:00, 3595.32it/s]1144it [00:00, 3575.24it/s]1137it [00:00, 3562.06it/s]1150it [00:00, 3550.54it/s]1507it [00:00, 3423.62it/s]1504it [00:00, 3423.35it/s]1495it [00:00, 3437.08it/s]1508it [00:00, 3417.55it/s]1898it [00:00, 3587.61it/s]1892it [00:00, 3578.69it/s]1878it [00:00, 3570.82it/s]1895it [00:00, 3571.11it/s]2252it [00:00, 3493.86it/s]2237it [00:00, 3492.82it/s]2259it [00:00, 3482.08it/s]2255it [00:00, 3455.91it/s]2648it [00:00, 3637.93it/s]2639it [00:00, 3658.38it/s]2659it [00:00, 3642.40it/s]2654it [00:00, 3615.14it/s]3042it [00:00, 3731.04it/s]3025it [00:00, 3719.33it/s]3060it [00:00, 3755.19it/s]3050it [00:00, 3719.45it/s]3417it [00:00, 3587.74it/s]3438it [00:00, 3582.60it/s]3398it [00:00, 3458.47it/s]3424it [00:00, 3584.31it/s]3807it [00:01, 3679.85it/s]3799it [00:01, 3551.37it/s]3802it [00:01, 3640.68it/s]3748it [00:01, 3442.00it/s]4177it [00:01, 3574.29it/s]4095it [00:01, 3340.25it/s]4156it [00:01, 3374.79it/s]4168it [00:01, 3443.14it/s]4557it [00:01, 3639.96it/s]4464it [00:01, 3439.57it/s]4534it [00:01, 3489.87it/s]4544it [00:01, 3527.57it/s]4923it [00:01, 3518.31it/s]4810it [00:01, 3385.43it/s]4886it [00:01, 3318.67it/s]4900it [00:01, 3410.96it/s]5299it [00:01, 3579.76it/s]5194it [00:01, 3515.70it/s]5263it [00:01, 3443.40it/s]5278it [00:01, 3514.30it/s]5560it [00:01, 3555.63it/s]5659it [00:01, 3447.54it/s]5611it [00:01, 3343.55it/s]5632it [00:01, 3403.79it/s]6012it [00:01, 3469.80it/s]5917it [00:01, 3364.42it/s]5969it [00:01, 3409.81it/s]5990it [00:01, 3451.88it/s]6361it [00:01, 3443.12it/s]6266it [00:01, 3397.41it/s]6325it [00:01, 3451.74it/s]6337it [00:01, 3455.83it/s]6707it [00:02, 2070.10it/s]6684it [00:02, 1942.89it/s]6672it [00:02, 1937.91it/s]6608it [00:02, 1876.52it/s]7048it [00:02, 2332.00it/s]6989it [00:02, 2152.92it/s]6979it [00:02, 2151.21it/s]6921it [00:02, 2108.65it/s]7341it [00:02, 2334.27it/s]7309it [00:02, 2376.39it/s]7290it [00:02, 2354.71it/s]7273it [00:02, 2403.47it/s]7693it [00:02, 2607.54it/s]7602it [00:02, 2504.74it/s]7584it [00:02, 2490.94it/s]7575it [00:02, 2523.82it/s]8047it [00:02, 2837.48it/s]7955it [00:02, 2761.24it/s]7938it [00:02, 2751.71it/s]7929it [00:02, 2771.81it/s]8362it [00:02, 2838.33it/s]8248it [00:02, 2736.45it/s]8266it [00:02, 2700.18it/s]8244it [00:02, 2760.20it/s]8714it [00:02, 3019.59it/s]8598it [00:02, 2939.00it/s]8616it [00:02, 2909.84it/s]8598it [00:02, 2964.53it/s]9034it [00:02, 2998.15it/s]8942it [00:02, 3075.61it/s]8969it [00:02, 3078.20it/s]8955it [00:02, 3121.57it/s]9371it [00:02, 3100.02it/s]9265it [00:03, 3041.19it/s]9293it [00:03, 3042.24it/s]9284it [00:03, 3074.15it/s]9726it [00:03, 3225.30it/s]9621it [00:03, 3185.32it/s]9648it [00:03, 3183.39it/s]9642it [00:03, 3215.45it/s]10056it [00:03, 3107.83it/s]9948it [00:03, 3123.35it/s]9973it [00:03, 3138.57it/s]9975it [00:03, 3025.30it/s]10406it [00:03, 3218.20it/s]10295it [00:03, 3220.52it/s]10327it [00:03, 3251.81it/s]10326it [00:03, 3159.59it/s]10733it [00:03, 3136.70it/s]10649it [00:03, 3310.53it/s]10683it [00:03, 3338.58it/s]10678it [00:03, 3259.84it/s]11082it [00:03, 3235.29it/s]10984it [00:03, 3207.65it/s]11009it [00:03, 3166.28it/s]11021it [00:03, 3214.90it/s]11419it [00:03, 3273.50it/s]11337it [00:03, 3299.15it/s]11349it [00:03, 3232.44it/s]11376it [00:03, 3310.23it/s]11749it [00:03, 3172.54it/s]11670it [00:03, 3209.43it/s]11676it [00:03, 3154.12it/s]11711it [00:03, 3215.00it/s]12089it [00:03, 3228.43it/s]12018it [00:03, 3285.55it/s]12030it [00:03, 3262.51it/s]12062it [00:03, 3297.40it/s]12414it [00:03, 3151.96it/s]12376it [00:03, 3369.70it/s]12383it [00:03, 3310.71it/s]12394it [00:03, 3209.94it/s]12765it [00:04, 3254.65it/s]12715it [00:04, 3248.79it/s]12746it [00:04, 3298.75it/s]12716it [00:04, 3196.24it/s]13101it [00:04, 3284.30it/s]13059it [00:04, 3301.25it/s]13085it [00:04, 3319.27it/s]13058it [00:04, 3259.55it/s]13368it [00:04, 3171.16it/s]
2022-07-12 14:39:24 | INFO | root | success load 13368 data
2022-07-12 14:39:24 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-12 14:39:24 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-12 14:39:24 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-12 14:39:24 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-12 14:39:24 | INFO | transformer.tokenization_utils | loading file None
2022-07-12 14:39:24 | INFO | transformer.tokenization_utils | loading file None
2022-07-12 14:39:24 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
13368it [00:04, 3141.55it/s]
13368it [00:04, 3139.62it/s]
13368it [00:04, 3132.87it/s]
2022-07-12 14:39:25 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-12 14:39:25 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-12 14:39:25 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-12 14:39:28 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-12 14:39:28 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-12 14:39:28 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-12 14:39:28 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-12 14:39:28 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-12 14:39:30 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-12 14:39:30 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-07-12 14:39:30 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-07-12 14:39:30 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-12 14:39:30 | INFO | fairseq_cli.train | num. model params: 380755715 (num. trained: 142456835)
Trained parameters: len 271
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-12 14:39:30 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-07-12 14:39:30 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 104294400)

Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]353it [00:00, 3528.36it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]706it [00:00, 3350.94it/s]2022-07-12 14:39:34 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-12 14:39:34 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-12 14:39:34 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt
2022-07-12 14:39:34 | INFO | fairseq.trainer | loading train data for epoch 1
365it [00:00, 3641.72it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]1066it [00:00, 3459.19it/s]363it [00:00, 3627.61it/s]2022-07-12 14:39:34 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.source
730it [00:00, 3274.11it/s]2022-07-12 14:39:34 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.target
2022-07-12 14:39:34 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]1425it [00:00, 3507.69it/s]726it [00:00, 3445.84it/s]1105it [00:00, 3477.22it/s]369it [00:00, 3689.41it/s]1777it [00:00, 3342.81it/s]1110it [00:00, 3616.93it/s]1463it [00:00, 3514.04it/s]738it [00:00, 3410.68it/s]2133it [00:00, 3412.50it/s]1480it [00:00, 3454.89it/s]1817it [00:00, 3400.48it/s]1105it [00:00, 3520.08it/s]2476it [00:00, 3328.36it/s]1864it [00:00, 3584.87it/s]2180it [00:00, 3469.22it/s]1472it [00:00, 3576.26it/s]2831it [00:00, 3394.38it/s]2249it [00:00, 3669.23it/s]2529it [00:00, 3383.19it/s]1831it [00:00, 3375.74it/s]3172it [00:00, 3320.90it/s]2888it [00:00, 3445.14it/s]2618it [00:00, 3513.15it/s]2196it [00:00, 3461.96it/s]3544it [00:01, 3439.64it/s]2998it [00:00, 3599.89it/s]3234it [00:00, 3364.83it/s]2545it [00:00, 3285.01it/s]3909it [00:01, 3500.96it/s]3587it [00:01, 3413.48it/s]3360it [00:00, 3478.41it/s]2919it [00:00, 3420.60it/s]4260it [00:01, 3389.47it/s]3954it [00:01, 3487.33it/s]3732it [00:01, 3548.37it/s]3264it [00:00, 3316.40it/s]4624it [00:01, 3460.32it/s]4304it [00:01, 3367.62it/s]4089it [00:01, 3447.92it/s]3633it [00:01, 3424.69it/s]4972it [00:01, 3355.30it/s]4674it [00:01, 3464.14it/s]4465it [00:01, 3537.00it/s]3990it [00:01, 3466.44it/s]5334it [00:01, 3431.83it/s]5022it [00:01, 3355.39it/s]4840it [00:01, 3389.09it/s]4339it [00:01, 3343.60it/s]5680it [00:01, 3271.46it/s]5380it [00:01, 3418.27it/s]5205it [00:01, 3460.43it/s]4699it [00:01, 3415.70it/s]6029it [00:01, 3332.70it/s]5580it [00:01, 3541.43it/s]5724it [00:01, 3286.24it/s]5043it [00:01, 3309.93it/s]6401it [00:01, 3442.81it/s]6096it [00:01, 3407.96it/s]5936it [00:01, 3442.01it/s]5415it [00:01, 3426.60it/s]6747it [00:01, 3354.78it/s]6457it [00:01, 3464.43it/s]6311it [00:01, 3530.11it/s]5760it [00:01, 3311.05it/s]7111it [00:02, 3434.93it/s]6805it [00:01, 3371.58it/s]6666it [00:01, 3443.74it/s]6129it [00:01, 3417.91it/s]7154it [00:02, 3404.77it/s]7043it [00:02, 3535.71it/s]6493it [00:01, 3480.87it/s]7398it [00:02, 3415.05it/s]6843it [00:02, 3354.32it/s]7208it [00:02, 3437.11it/s]7456it [00:02, 1064.33it/s]7831it [00:03, 1370.91it/s]7496it [00:02, 1088.75it/s]7742it [00:02, 1222.30it/s]8190it [00:03, 1682.04it/s]7855it [00:03, 1382.86it/s]8089it [00:02, 1508.71it/s]8501it [00:03, 1898.88it/s]8224it [00:03, 1714.46it/s]8392it [00:03, 1742.60it/s]8873it [00:03, 2245.68it/s]8536it [00:03, 1953.44it/s]7554it [00:02, 1050.64it/s]8759it [00:03, 2088.16it/s]9198it [00:03, 2440.35it/s]8907it [00:03, 2296.08it/s]7913it [00:03, 1335.92it/s]9109it [00:03, 2324.54it/s]9569it [00:03, 2734.61it/s]9235it [00:03, 2492.20it/s]8268it [00:03, 1606.06it/s]9483it [00:03, 2639.26it/s]9609it [00:03, 2786.11it/s]9949it [00:03, 2879.74it/s]8628it [00:03, 1928.53it/s]9860it [00:03, 2910.72it/s]10313it [00:03, 3073.44it/s]9949it [00:03, 2890.37it/s]8990it [00:03, 2246.02it/s]10205it [00:03, 2986.96it/s]10695it [00:03, 3270.43it/s]10328it [00:03, 3125.19it/s]9317it [00:03, 2426.02it/s]10583it [00:03, 3193.45it/s]10707it [00:03, 3305.55it/s]11049it [00:03, 3238.02it/s]9691it [00:03, 2726.24it/s]10932it [00:03, 3132.26it/s]11064it [00:03, 3285.82it/s]11429it [00:04, 3390.90it/s]10028it [00:03, 2819.39it/s]11313it [00:03, 3316.00it/s]11435it [00:04, 3403.04it/s]11783it [00:04, 3323.19it/s]10386it [00:03, 3013.38it/s]11661it [00:03, 3284.07it/s]11789it [00:04, 3347.37it/s]12168it [00:04, 3470.97it/s]10744it [00:03, 3163.27it/s]12046it [00:04, 3441.06it/s]12171it [00:04, 3479.30it/s]12524it [00:04, 3392.70it/s]11087it [00:04, 3146.42it/s]12434it [00:04, 3564.90it/s]12527it [00:04, 3427.18it/s]12904it [00:04, 3491.75it/s]11463it [00:04, 3316.12it/s]12798it [00:04, 3467.99it/s]12888it [00:04, 3477.67it/s]13285it [00:04, 3582.24it/s]11809it [00:04, 3224.12it/s]13175it [00:04, 3552.84it/s]13268it [00:04, 3570.10it/s]12187it [00:04, 3377.53it/s]13647it [00:04, 3395.00it/s]13535it [00:04, 3445.83it/s]13628it [00:04, 3497.39it/s]14034it [00:04, 3528.02it/s]12534it [00:04, 3295.01it/s]13926it [00:04, 3576.51it/s]14015it [00:04, 3602.92it/s]14391it [00:04, 3450.52it/s]12910it [00:04, 3425.74it/s]14287it [00:04, 3501.93it/s]14378it [00:04, 3517.93it/s]14779it [00:05, 3564.10it/s]13299it [00:04, 3520.21it/s]14673it [00:04, 3604.70it/s]14762it [00:04, 3608.97it/s]15138it [00:05, 3470.95it/s]13655it [00:04, 3431.71it/s]15036it [00:04, 3516.33it/s]15125it [00:05, 3536.69it/s]15512it [00:05, 3547.81it/s]14036it [00:04, 3537.58it/s]15413it [00:05, 3587.69it/s]15501it [00:05, 3599.68it/s]15869it [00:05, 3431.45it/s]14393it [00:04, 3430.15it/s]15794it [00:05, 3650.71it/s]15863it [00:05, 3480.10it/s]16242it [00:05, 3515.22it/s]14757it [00:05, 3489.27it/s]16161it [00:05, 3514.55it/s]16234it [00:05, 3544.87it/s]16619it [00:05, 3588.19it/s]15108it [00:05, 3422.87it/s]16544it [00:05, 3604.09it/s]16622it [00:05, 3641.56it/s]15478it [00:05, 3501.70it/s]15830it [00:05, 3386.63it/s]16171it [00:05, 3373.34it/s]16548it [00:05, 3486.53it/s]16988it [00:06, 1047.75it/s]16907it [00:06, 930.11it/s] 17370it [00:06, 1345.93it/s]16980it [00:06, 845.76it/s] 17297it [00:06, 1217.15it/s]17703it [00:06, 1609.78it/s]17323it [00:06, 1077.06it/s]18088it [00:06, 1967.30it/s]17599it [00:06, 1412.40it/s]17609it [00:06, 1277.97it/s]18421it [00:06, 2207.22it/s]17970it [00:06, 1747.18it/s]17997it [00:07, 1640.20it/s]18800it [00:06, 2537.69it/s]18356it [00:06, 2110.48it/s]16898it [00:06, 856.15it/s] 18324it [00:07, 1910.17it/s]19145it [00:07, 2740.98it/s]18692it [00:06, 2341.33it/s]17262it [00:06, 1114.92it/s]18648it [00:07, 2164.90it/s]19075it [00:07, 2666.41it/s]19489it [00:07, 2879.00it/s]17578it [00:06, 1354.58it/s]19022it [00:07, 2503.21it/s]19858it [00:07, 3086.60it/s]19423it [00:07, 2802.99it/s]17952it [00:07, 1696.15it/s]19798it [00:07, 3039.75it/s]19359it [00:07, 2537.27it/s]20206it [00:07, 2961.85it/s]18328it [00:07, 2046.34it/s]19711it [00:07, 2771.79it/s]20149it [00:07, 3022.75it/s]20568it [00:07, 3133.11it/s]18664it [00:07, 2284.68it/s]20082it [00:07, 3010.06it/s]20519it [00:07, 3201.47it/s]20938it [00:07, 3156.46it/s]19028it [00:07, 2577.77it/s]20421it [00:07, 3050.56it/s]20898it [00:07, 3362.09it/s]21307it [00:07, 3300.80it/s]19370it [00:07, 2703.13it/s]20753it [00:07, 3073.52it/s]21687it [00:07, 3439.21it/s]21254it [00:07, 3139.30it/s]19708it [00:07, 2870.35it/s]21080it [00:08, 3055.90it/s]22041it [00:07, 3384.70it/s]21631it [00:07, 3308.22it/s]20078it [00:07, 3086.28it/s]21461it [00:08, 3264.93it/s]22424it [00:08, 3511.23it/s]21975it [00:07, 3282.17it/s]20422it [00:07, 3085.67it/s]21799it [00:08, 3217.09it/s]22340it [00:08, 3379.12it/s]22781it [00:08, 3405.46it/s]20768it [00:07, 3186.66it/s]22132it [00:08, 3246.82it/s]23163it [00:08, 3523.49it/s]22685it [00:08, 3319.07it/s]21105it [00:08, 3119.21it/s]22513it [00:08, 3408.85it/s]23067it [00:08, 3460.05it/s]23519it [00:08, 3421.52it/s]21477it [00:08, 3285.01it/s]22859it [00:08, 3361.02it/s]23443it [00:08, 3545.22it/s]23896it [00:08, 3519.96it/s]21816it [00:08, 3250.74it/s]23238it [00:08, 3484.75it/s]24272it [00:08, 3587.57it/s]23801it [00:08, 3442.88it/s]22176it [00:08, 3348.36it/s]23590it [00:08, 3392.10it/s]24169it [00:08, 3510.86it/s]24633it [00:08, 3485.52it/s]22545it [00:08, 3446.23it/s]23964it [00:08, 3491.38it/s]25006it [00:08, 3555.40it/s]24523it [00:08, 3421.02it/s]22894it [00:08, 3371.48it/s]24316it [00:08, 3396.31it/s]24900it [00:08, 3520.86it/s]25364it [00:08, 3454.13it/s]23262it [00:08, 3459.69it/s]24685it [00:09, 3479.96it/s]25725it [00:08, 3498.93it/s]25254it [00:08, 3338.01it/s]23611it [00:08, 3322.89it/s]25035it [00:09, 3430.90it/s]26077it [00:09, 3393.13it/s]25630it [00:08, 3454.29it/s]23986it [00:08, 3443.13it/s]25380it [00:09, 3362.66it/s]26454it [00:09, 3499.65it/s]25980it [00:09, 3372.54it/s]24333it [00:08, 3324.77it/s]25762it [00:09, 3494.08it/s]26359it [00:09, 3489.38it/s]26819it [00:09, 3413.83it/s]24702it [00:09, 3428.51it/s]26113it [00:09, 3386.04it/s]26736it [00:09, 3569.96it/s]27195it [00:09, 3510.68it/s]25069it [00:09, 3497.60it/s]26481it [00:09, 3468.21it/s]27573it [00:09, 3588.16it/s]27095it [00:09, 3438.34it/s]25421it [00:09, 3328.44it/s]26830it [00:09, 3374.09it/s]27934it [00:09, 3497.03it/s]27483it [00:09, 3563.76it/s]25796it [00:09, 3447.76it/s]27211it [00:09, 3498.66it/s]28305it [00:09, 3544.66it/s]27842it [00:09, 3483.90it/s]26144it [00:09, 3342.69it/s]27563it [00:09, 3489.98it/s]28228it [00:09, 3589.46it/s]26498it [00:09, 3396.34it/s]27913it [00:10, 3333.76it/s]26840it [00:09, 3305.97it/s]28285it [00:10, 3442.24it/s]27205it [00:09, 3403.85it/s]27585it [00:09, 3517.06it/s]27939it [00:10, 3407.87it/s]28296it [00:10, 3452.49it/s]28661it [00:10, 800.05it/s] 29039it [00:11, 1055.69it/s]28589it [00:11, 760.46it/s] 29408it [00:11, 1322.42it/s]28971it [00:11, 1007.88it/s]29788it [00:11, 1651.76it/s]28632it [00:11, 723.64it/s] 29349it [00:11, 1295.10it/s]30158it [00:11, 1978.76it/s]28975it [00:11, 939.49it/s]29670it [00:11, 1535.48it/s]30497it [00:11, 2216.29it/s]29348it [00:11, 1224.95it/s]30048it [00:11, 1883.06it/s]30869it [00:11, 2528.45it/s]29663it [00:11, 1470.44it/s]30384it [00:11, 2127.36it/s]31214it [00:11, 2685.61it/s]29991it [00:11, 1748.99it/s]30751it [00:11, 2440.73it/s]31580it [00:11, 2919.65it/s]30310it [00:12, 2008.87it/s]28643it [00:11, 663.98it/s] 31092it [00:11, 2603.52it/s]31928it [00:11, 2961.52it/s]30676it [00:12, 2345.90it/s]28987it [00:11, 869.52it/s]31456it [00:11, 2851.33it/s]32301it [00:12, 3161.18it/s]31041it [00:12, 2640.52it/s]29357it [00:11, 1140.05it/s]31830it [00:12, 3077.88it/s]32665it [00:12, 3289.84it/s]29665it [00:11, 1373.03it/s]31382it [00:12, 2772.57it/s]32182it [00:12, 3112.28it/s]33016it [00:12, 3237.75it/s]30039it [00:12, 1718.28it/s]31759it [00:12, 3026.75it/s]32556it [00:12, 3281.38it/s]33388it [00:12, 3370.68it/s]32106it [00:12, 3085.12it/s]30365it [00:12, 1918.42it/s]32908it [00:12, 3242.46it/s]33737it [00:12, 3329.48it/s]32482it [00:12, 3267.93it/s]30737it [00:12, 2265.64it/s]33279it [00:12, 3371.79it/s]34113it [00:12, 3450.98it/s]32833it [00:12, 3198.30it/s]31089it [00:12, 2465.56it/s]33629it [00:12, 3305.98it/s]34465it [00:12, 3369.17it/s]33200it [00:12, 3327.55it/s]31461it [00:12, 2754.77it/s]34006it [00:12, 3436.93it/s]34838it [00:12, 3470.70it/s]33571it [00:12, 3435.04it/s]31836it [00:12, 3000.53it/s]34387it [00:12, 3542.59it/s]35215it [00:12, 3556.22it/s]33925it [00:13, 3383.40it/s]32185it [00:12, 3049.86it/s]34747it [00:12, 3435.08it/s]35574it [00:12, 3423.00it/s]34299it [00:13, 3484.23it/s]32540it [00:12, 3182.63it/s]35121it [00:12, 3521.56it/s]35951it [00:13, 3522.25it/s]34653it [00:13, 3393.84it/s]32884it [00:12, 3157.31it/s]35477it [00:13, 3422.57it/s]36306it [00:13, 3421.75it/s]35014it [00:13, 3455.76it/s]33253it [00:13, 3303.42it/s]35847it [00:13, 3500.80it/s]36671it [00:13, 3484.43it/s]35363it [00:13, 3301.80it/s]33609it [00:13, 3219.46it/s]36200it [00:13, 3299.89it/s]37022it [00:13, 3277.80it/s]35728it [00:13, 3398.47it/s]33989it [00:13, 3380.69it/s]36562it [00:13, 3388.89it/s]37388it [00:13, 3383.95it/s]36104it [00:13, 3501.20it/s]34362it [00:13, 3480.01it/s]36938it [00:13, 3493.26it/s]37758it [00:13, 3473.15it/s]36457it [00:13, 3394.78it/s]34717it [00:13, 3353.93it/s]37290it [00:13, 3396.51it/s]38108it [00:13, 3400.53it/s]36823it [00:13, 3469.49it/s]35084it [00:13, 3441.78it/s]37661it [00:13, 3483.99it/s]38464it [00:13, 3445.61it/s]37172it [00:14, 3368.48it/s]35433it [00:13, 3354.48it/s]38012it [00:13, 3387.43it/s]38811it [00:13, 3357.41it/s]37533it [00:14, 3436.22it/s]35789it [00:13, 3411.10it/s]38383it [00:13, 3479.14it/s]39184it [00:14, 3462.95it/s]36133it [00:13, 3325.65it/s]37879it [00:14, 3210.95it/s]38733it [00:14, 3369.03it/s]39532it [00:14, 3387.57it/s]36479it [00:13, 3357.86it/s]38252it [00:14, 3354.28it/s]39098it [00:14, 3448.69it/s]39895it [00:14, 3457.13it/s]36847it [00:14, 3448.73it/s]38608it [00:14, 3412.76it/s]39480it [00:14, 3554.67it/s]40270it [00:14, 3542.08it/s]38953it [00:14, 3362.57it/s]37194it [00:14, 3346.40it/s]39837it [00:14, 3442.67it/s]40626it [00:14, 3428.48it/s]39336it [00:14, 3496.84it/s]37546it [00:14, 3396.12it/s]40216it [00:14, 3540.57it/s]41004it [00:14, 3527.27it/s]39688it [00:14, 3382.12it/s]37887it [00:14, 3298.02it/s]40572it [00:14, 3410.11it/s]41359it [00:14, 3402.09it/s]40049it [00:14, 3435.27it/s]38263it [00:14, 3428.73it/s]40915it [00:14, 3397.91it/s]41728it [00:14, 3482.40it/s]38608it [00:14, 3257.51it/s]40395it [00:14, 3143.91it/s]42078it [00:14, 3305.88it/s]41257it [00:14, 3129.95it/s]38937it [00:14, 3209.78it/s]40749it [00:15, 3245.85it/s]42449it [00:14, 3419.71it/s]41627it [00:14, 3285.79it/s]39317it [00:14, 3375.87it/s]41111it [00:15, 3348.71it/s]42813it [00:15, 3481.71it/s]42003it [00:14, 3417.14it/s]39657it [00:14, 3245.78it/s]41450it [00:15, 3262.04it/s]43164it [00:15, 3364.92it/s]42349it [00:15, 3341.73it/s]40026it [00:15, 3370.14it/s]41799it [00:15, 3324.82it/s]43503it [00:15, 3367.19it/s]42709it [00:15, 3414.06it/s]42134it [00:15, 3291.81it/s]40366it [00:15, 3299.81it/s]43053it [00:15, 3310.11it/s]42503it [00:15, 3406.45it/s]40724it [00:15, 3378.80it/s]43422it [00:15, 3417.25it/s]41094it [00:15, 3471.01it/s]42848it [00:15, 3345.18it/s]43222it [00:15, 3459.28it/s]41443it [00:15, 3340.61it/s]43595it [00:15, 3536.87it/s]41811it [00:15, 3435.82it/s]42157it [00:15, 3333.30it/s]42516it [00:15, 3402.89it/s]42858it [00:15, 3314.89it/s]43225it [00:15, 3415.65it/s]43591it [00:16, 3484.68it/s]43766it [00:16, 642.80it/s] 43842it [00:17, 548.32it/s] 44152it [00:17, 874.79it/s]44224it [00:17, 754.51it/s]44530it [00:17, 1146.08it/s]44560it [00:17, 969.57it/s]44846it [00:17, 1346.98it/s]44877it [00:17, 1201.80it/s]45206it [00:17, 1663.91it/s]45242it [00:17, 1522.44it/s]43950it [00:17, 582.85it/s] 45523it [00:17, 1912.39it/s]45565it [00:17, 1786.13it/s]44330it [00:17, 791.42it/s]45900it [00:17, 2269.57it/s]45891it [00:17, 2056.34it/s]44622it [00:17, 971.00it/s]46265it [00:17, 2567.32it/s]46214it [00:17, 2290.73it/s]44984it [00:18, 1256.86it/s]46609it [00:17, 2672.91it/s]46535it [00:17, 2488.17it/s]45358it [00:18, 1588.09it/s]46983it [00:17, 2932.64it/s]46893it [00:18, 2742.18it/s]45689it [00:18, 1851.86it/s]47326it [00:18, 2999.18it/s]43941it [00:17, 558.77it/s] 47222it [00:18, 2685.91it/s]46053it [00:18, 2181.36it/s]47696it [00:18, 3184.84it/s]44318it [00:18, 760.49it/s]47587it [00:18, 2931.53it/s]46391it [00:18, 2383.23it/s]48042it [00:18, 3185.21it/s]44628it [00:18, 953.20it/s]46765it [00:18, 2690.10it/s]47957it [00:18, 3013.57it/s]48419it [00:18, 3347.14it/s]45005it [00:18, 1249.76it/s]48308it [00:18, 3146.81it/s]47118it [00:18, 2770.91it/s]48769it [00:18, 3327.08it/s]45358it [00:18, 1547.71it/s]48640it [00:18, 3181.17it/s]47447it [00:18, 2873.99it/s]49112it [00:18, 3283.38it/s]45687it [00:18, 1804.36it/s]48971it [00:18, 3111.42it/s]47813it [00:18, 3079.04it/s]49489it [00:18, 3420.76it/s]46053it [00:18, 2141.48it/s]49342it [00:18, 3279.40it/s]48151it [00:18, 3107.87it/s]49837it [00:18, 3354.17it/s]46389it [00:18, 2347.14it/s]49678it [00:18, 3254.55it/s]48526it [00:19, 3284.46it/s]50200it [00:18, 3422.61it/s]46717it [00:18, 2543.53it/s]50009it [00:19, 3225.09it/s]48871it [00:19, 3108.10it/s]50546it [00:18, 3285.73it/s]47090it [00:18, 2829.26it/s]50383it [00:19, 3372.61it/s]49246it [00:19, 3282.80it/s]50920it [00:19, 3412.48it/s]47430it [00:18, 2913.13it/s]50724it [00:19, 3273.63it/s]49617it [00:19, 3402.29it/s]51294it [00:19, 3504.28it/s]47799it [00:19, 3117.57it/s]51093it [00:19, 3392.83it/s]49966it [00:19, 3265.63it/s]51647it [00:19, 3340.25it/s]48142it [00:19, 3084.08it/s]51435it [00:19, 3307.13it/s]50329it [00:19, 3366.58it/s]52026it [00:19, 3465.55it/s]48517it [00:19, 3265.19it/s]51807it [00:19, 3422.39it/s]50672it [00:19, 3305.57it/s]52376it [00:19, 3387.71it/s]48861it [00:19, 3234.14it/s]52157it [00:19, 3336.55it/s]51050it [00:19, 3439.88it/s]52750it [00:19, 3488.50it/s]49229it [00:19, 3357.18it/s]52532it [00:19, 3454.62it/s]51398it [00:19, 3362.12it/s]53101it [00:19, 3398.40it/s]49585it [00:19, 3414.84it/s]52901it [00:19, 3521.89it/s]51761it [00:20, 3438.47it/s]53476it [00:19, 3497.80it/s]49934it [00:19, 3323.92it/s]53255it [00:19, 3407.12it/s]52136it [00:20, 3528.88it/s]53840it [00:19, 3372.94it/s]50302it [00:19, 3424.48it/s]53614it [00:20, 3458.24it/s]52491it [00:20, 3431.00it/s]54223it [00:20, 3501.86it/s]50649it [00:19, 3322.43it/s]53962it [00:20, 3382.28it/s]52864it [00:20, 3515.11it/s]54598it [00:20, 3571.80it/s]51006it [00:20, 3391.60it/s]54331it [00:20, 3470.44it/s]53217it [00:20, 3389.17it/s]54957it [00:20, 3447.53it/s]51348it [00:20, 3302.75it/s]54680it [00:20, 3385.36it/s]53590it [00:20, 3484.96it/s]55322it [00:20, 3504.58it/s]51720it [00:20, 3420.80it/s]55043it [00:20, 3454.40it/s]53941it [00:20, 3404.24it/s]55675it [00:20, 3387.90it/s]52089it [00:20, 3495.99it/s]55410it [00:20, 3515.75it/s]54322it [00:20, 3520.12it/s]56053it [00:20, 3497.20it/s]52441it [00:20, 3346.38it/s]55763it [00:20, 3363.39it/s]54676it [00:20, 3518.76it/s]56405it [00:20, 3303.29it/s]52810it [00:20, 3443.95it/s]56126it [00:20, 3438.86it/s]55029it [00:20, 3327.97it/s]56778it [00:20, 3422.35it/s]53157it [00:20, 3341.64it/s]56472it [00:20, 3336.46it/s]55395it [00:21, 3419.72it/s]57156it [00:20, 3524.61it/s]53527it [00:20, 3442.36it/s]56847it [00:21, 3454.34it/s]55740it [00:21, 3332.39it/s]57511it [00:20, 3425.48it/s]53874it [00:20, 3299.87it/s]57197it [00:21, 3365.22it/s]56119it [00:21, 3461.69it/s]57888it [00:21, 3522.04it/s]54254it [00:20, 3439.85it/s]57572it [00:21, 3474.55it/s]56468it [00:21, 3360.62it/s]58243it [00:21, 3404.64it/s]54622it [00:21, 3507.33it/s]57935it [00:21, 3517.50it/s]56846it [00:21, 3480.31it/s]58613it [00:21, 3486.70it/s]54975it [00:21, 3344.00it/s]58288it [00:21, 3401.64it/s]57199it [00:21, 3401.77it/s]58964it [00:21, 3363.91it/s]55339it [00:21, 3426.86it/s]58650it [00:21, 3464.03it/s]57574it [00:21, 3501.59it/s]59333it [00:21, 3455.60it/s]55684it [00:21, 3315.06it/s]58998it [00:21, 3355.79it/s]57947it [00:21, 3567.64it/s]59700it [00:21, 3516.35it/s]56055it [00:21, 3426.08it/s]59356it [00:21, 3418.41it/s]58305it [00:21, 3440.90it/s]60054it [00:21, 3408.25it/s]56400it [00:21, 3310.38it/s]59717it [00:21, 3328.58it/s]58676it [00:22, 3517.62it/s]60422it [00:21, 3486.03it/s]56773it [00:21, 3428.51it/s]60078it [00:21, 3406.80it/s]59030it [00:22, 3302.86it/s]57147it [00:21, 3516.07it/s]60773it [00:21, 3203.09it/s]60421it [00:22, 3262.91it/s]59364it [00:22, 3248.27it/s]61141it [00:22, 3333.14it/s]57501it [00:21, 3402.51it/s]60750it [00:22, 3184.52it/s]59719it [00:22, 3143.04it/s]57866it [00:22, 3471.59it/s]61479it [00:22, 3187.94it/s]61113it [00:22, 3308.06it/s]60080it [00:22, 3271.34it/s]61847it [00:22, 3323.55it/s]58215it [00:22, 3323.24it/s]61446it [00:22, 3210.95it/s]60450it [00:22, 3390.11it/s]62223it [00:22, 3446.32it/s]58581it [00:22, 3417.39it/s]61770it [00:22, 3218.48it/s]60792it [00:22, 3121.89it/s]62093it [00:22, 3221.66it/s]58925it [00:22, 3197.07it/s]61146it [00:22, 3235.82it/s]59295it [00:22, 3335.99it/s]61475it [00:22, 3188.28it/s]59638it [00:22, 3361.42it/s]61798it [00:23, 3167.90it/s]59977it [00:22, 3195.77it/s]62170it [00:23, 3323.16it/s]60333it [00:22, 3296.24it/s]60666it [00:22, 3241.47it/s]61037it [00:23, 3374.91it/s]61399it [00:23, 3295.44it/s]61769it [00:23, 3409.83it/s]62117it [00:23, 3428.81it/s]62572it [00:24, 511.96it/s] 62916it [00:24, 680.35it/s]63224it [00:24, 862.42it/s]63553it [00:24, 1099.55it/s]63914it [00:24, 1407.07it/s]64232it [00:24, 1627.13it/s]64599it [00:25, 1977.06it/s]64921it [00:25, 2139.91it/s]62505it [00:25, 439.94it/s] 65281it [00:25, 2449.95it/s]62416it [00:25, 361.76it/s] 62807it [00:25, 573.53it/s]65632it [00:25, 2696.39it/s]62747it [00:25, 493.64it/s]63148it [00:25, 764.38it/s]63097it [00:25, 674.21it/s]65963it [00:25, 2729.11it/s]63492it [00:25, 1002.61it/s]63384it [00:25, 846.45it/s]66315it [00:25, 2930.49it/s]63860it [00:25, 1304.75it/s]63753it [00:25, 1134.35it/s]62462it [00:25, 465.35it/s] 66642it [00:25, 2952.39it/s]64177it [00:26, 1560.33it/s]64064it [00:25, 1366.99it/s]62790it [00:25, 615.78it/s]66963it [00:25, 3021.71it/s]64545it [00:26, 1910.29it/s]64432it [00:26, 1717.07it/s]67327it [00:25, 3193.44it/s]63147it [00:25, 814.87it/s]64877it [00:26, 2134.68it/s]64770it [00:26, 2012.24it/s]67660it [00:26, 3150.96it/s]63496it [00:25, 1058.09it/s]65241it [00:26, 2451.47it/s]65097it [00:26, 2245.55it/s]68000it [00:26, 3219.03it/s]63831it [00:26, 1321.11it/s]65576it [00:26, 2645.40it/s]65429it [00:26, 2484.30it/s]68329it [00:26, 3124.49it/s]64142it [00:26, 1562.06it/s]65909it [00:26, 2770.19it/s]65755it [00:26, 2626.15it/s]68659it [00:26, 3173.47it/s]64474it [00:26, 1854.22it/s]66236it [00:26, 2842.46it/s]66130it [00:26, 2909.18it/s]69017it [00:26, 3289.17it/s]64828it [00:26, 2102.15it/s]66556it [00:26, 2859.70it/s]66498it [00:26, 3113.34it/s]69350it [00:26, 3221.99it/s]65190it [00:26, 2420.04it/s]66884it [00:26, 2971.31it/s]66844it [00:26, 3020.78it/s]69713it [00:26, 3339.85it/s]65555it [00:26, 2702.35it/s]67243it [00:26, 3142.84it/s]67206it [00:26, 3181.84it/s]70050it [00:26, 3186.29it/s]65892it [00:26, 2806.44it/s]67572it [00:27, 3058.14it/s]67543it [00:26, 3114.88it/s]70422it [00:26, 3336.72it/s]66260it [00:26, 3029.85it/s]67954it [00:27, 3270.72it/s]67893it [00:27, 3220.97it/s]70759it [00:26, 3257.28it/s]66601it [00:26, 2905.11it/s]68290it [00:27, 3079.40it/s]68226it [00:27, 3082.18it/s]71127it [00:27, 3375.75it/s]66952it [00:26, 3062.63it/s]68652it [00:27, 3228.38it/s]68597it [00:27, 3254.02it/s]71498it [00:27, 3471.33it/s]67321it [00:27, 3232.73it/s]69024it [00:27, 3366.40it/s]68968it [00:27, 3381.90it/s]71847it [00:27, 3352.26it/s]67661it [00:27, 3194.22it/s]69367it [00:27, 3259.52it/s]72185it [00:27, 3344.93it/s]69312it [00:27, 3184.55it/s]67992it [00:27, 3188.50it/s]69698it [00:27, 3210.18it/s]69654it [00:27, 3249.47it/s]72521it [00:27, 3235.82it/s]68319it [00:27, 3156.54it/s]70023it [00:27, 3203.34it/s]69984it [00:27, 3215.66it/s]72895it [00:27, 3379.86it/s]68685it [00:27, 3297.32it/s]70391it [00:27, 3340.39it/s]70309it [00:27, 3199.12it/s]69023it [00:27, 3320.02it/s]73235it [00:27, 3183.60it/s]70728it [00:28, 3197.65it/s]70682it [00:27, 3351.29it/s]73607it [00:27, 3332.63it/s]69359it [00:27, 3253.43it/s]71091it [00:28, 3320.40it/s]71020it [00:28, 3268.86it/s]73978it [00:27, 3440.34it/s]69687it [00:27, 3244.75it/s]71426it [00:28, 3314.80it/s]71349it [00:28, 3242.26it/s]74325it [00:28, 3345.82it/s]70014it [00:27, 3122.29it/s]71760it [00:28, 3201.19it/s]71675it [00:28, 3208.19it/s]74687it [00:28, 3423.50it/s]70371it [00:28, 3249.25it/s]72121it [00:28, 3318.10it/s]72022it [00:28, 3283.67it/s]70708it [00:28, 3160.57it/s]75032it [00:28, 3209.72it/s]72455it [00:28, 3182.33it/s]72352it [00:28, 3280.35it/s]71051it [00:28, 3236.33it/s]75390it [00:28, 3311.98it/s]72824it [00:28, 3324.02it/s]72681it [00:28, 3237.01it/s]71400it [00:28, 3308.03it/s]75750it [00:28, 3217.78it/s]73193it [00:28, 3428.98it/s]73022it [00:28, 3287.24it/s]71733it [00:28, 3141.26it/s]76075it [00:28, 3197.70it/s]73538it [00:28, 3339.15it/s]73352it [00:28, 3217.78it/s]72099it [00:28, 3288.03it/s]76438it [00:28, 3318.51it/s]73921it [00:28, 3479.53it/s]73716it [00:28, 3337.76it/s]72431it [00:28, 3201.26it/s]76772it [00:28, 3094.37it/s]74271it [00:29, 3257.91it/s]74068it [00:28, 3240.19it/s]72799it [00:28, 3335.48it/s]77132it [00:28, 3233.58it/s]74630it [00:29, 3349.03it/s]74436it [00:29, 3364.72it/s]73158it [00:28, 3395.50it/s]77460it [00:29, 3168.12it/s]74969it [00:29, 3124.95it/s]74774it [00:29, 3252.56it/s]77814it [00:29, 3272.48it/s]73500it [00:29, 3134.36it/s]75318it [00:29, 3225.25it/s]75101it [00:29, 3200.58it/s]73873it [00:29, 3299.58it/s]78186it [00:29, 3399.57it/s]75683it [00:29, 3343.66it/s]75465it [00:29, 3326.26it/s]78529it [00:29, 3231.53it/s]74208it [00:29, 3134.80it/s]76022it [00:29, 3269.34it/s]75799it [00:29, 3260.79it/s]78895it [00:29, 3351.03it/s]74573it [00:29, 3277.49it/s]76383it [00:29, 3364.36it/s]76153it [00:29, 3339.66it/s]74906it [00:29, 3289.31it/s]79233it [00:29, 3142.66it/s]76722it [00:29, 3283.59it/s]76493it [00:29, 3355.75it/s]75238it [00:29, 3181.93it/s]79593it [00:29, 3269.47it/s]77078it [00:29, 3358.03it/s]76830it [00:29, 3248.52it/s]75602it [00:29, 3309.85it/s]79950it [00:29, 3132.44it/s]77428it [00:30, 3230.85it/s]77157it [00:29, 3252.72it/s]75936it [00:29, 3095.49it/s]80301it [00:29, 3236.04it/s]77771it [00:30, 3285.42it/s]77484it [00:30, 3165.80it/s]76294it [00:29, 3228.63it/s]80673it [00:30, 3372.04it/s]78126it [00:30, 3361.49it/s]77850it [00:30, 3305.73it/s]76621it [00:29, 3084.03it/s]81014it [00:30, 3199.29it/s]78182it [00:30, 3240.81it/s]78464it [00:30, 3122.12it/s]76975it [00:30, 3210.33it/s]81377it [00:30, 3317.46it/s]78508it [00:30, 3207.25it/s]78827it [00:30, 3262.01it/s]77341it [00:30, 3336.20it/s]78864it [00:30, 3309.12it/s]81713it [00:30, 3145.61it/s]79158it [00:30, 3156.67it/s]77678it [00:30, 3178.13it/s]82048it [00:30, 3200.24it/s]79196it [00:30, 3167.35it/s]79489it [00:30, 3199.00it/s]78042it [00:30, 3308.07it/s]82411it [00:30, 3320.72it/s]79562it [00:30, 3307.30it/s]79852it [00:30, 3320.05it/s]78377it [00:30, 3203.36it/s]79927it [00:30, 3403.48it/s]82746it [00:30, 3260.62it/s]80187it [00:30, 3247.40it/s]78725it [00:30, 3281.13it/s]83083it [00:30, 3290.42it/s]80557it [00:31, 3375.54it/s]80269it [00:30, 3201.51it/s]79088it [00:30, 3379.31it/s]83414it [00:30, 3221.32it/s]80897it [00:31, 3294.86it/s]80643it [00:30, 3351.02it/s]79428it [00:30, 3263.10it/s]83786it [00:30, 3362.76it/s]81262it [00:31, 3395.19it/s]80982it [00:31, 3282.60it/s]79757it [00:30, 3237.56it/s]84124it [00:31, 3352.50it/s]81613it [00:31, 3427.56it/s]81341it [00:31, 3369.29it/s]80083it [00:31, 3135.66it/s]84461it [00:31, 3201.55it/s]81957it [00:31, 3254.75it/s]81680it [00:31, 3200.75it/s]80450it [00:31, 3287.26it/s]84824it [00:31, 3323.25it/s]82323it [00:31, 3369.18it/s]82048it [00:31, 3327.20it/s]80788it [00:31, 3175.18it/s]85159it [00:31, 3240.17it/s]82404it [00:31, 3393.46it/s]82663it [00:31, 3225.13it/s]81108it [00:31, 3131.52it/s]85485it [00:31, 3189.13it/s]82989it [00:31, 3211.52it/s]82746it [00:31, 3170.66it/s]81468it [00:31, 3264.85it/s]85830it [00:31, 3152.02it/s]83312it [00:31, 3179.66it/s]83113it [00:31, 3309.32it/s]83637it [00:31, 3198.94it/s]81797it [00:31, 3091.85it/s]83448it [00:31, 3094.70it/s]83990it [00:32, 3293.96it/s]82138it [00:31, 3179.30it/s]83818it [00:31, 3260.08it/s]84321it [00:32, 3244.20it/s]82468it [00:31, 3137.81it/s]84149it [00:32, 3196.82it/s]84693it [00:32, 3381.20it/s]82835it [00:31, 3288.70it/s]84521it [00:32, 3343.44it/s]85032it [00:32, 3308.38it/s]83198it [00:31, 3384.55it/s]84859it [00:32, 3321.64it/s]85364it [00:32, 3216.07it/s]83539it [00:32, 3140.62it/s]85194it [00:32, 3199.56it/s]85729it [00:32, 3339.34it/s]83907it [00:32, 3288.75it/s]85554it [00:32, 3311.26it/s]84241it [00:32, 3188.86it/s]85888it [00:32, 3254.49it/s]84608it [00:32, 3323.68it/s]84960it [00:32, 3377.51it/s]85301it [00:32, 3209.16it/s]85670it [00:32, 3344.24it/s]86147it [00:34, 377.95it/s] 86485it [00:34, 516.86it/s]86787it [00:34, 672.01it/s]87140it [00:34, 904.51it/s]87468it [00:34, 1151.79it/s]87777it [00:34, 1402.69it/s]88137it [00:34, 1744.93it/s]88460it [00:35, 1971.10it/s]88835it [00:35, 2331.88it/s]89201it [00:35, 2630.19it/s]86065it [00:35, 363.57it/s] 89543it [00:35, 2613.63it/s]86391it [00:35, 488.87it/s]89915it [00:35, 2882.25it/s]86738it [00:35, 657.93it/s]90248it [00:35, 2933.61it/s]87103it [00:35, 886.06it/s]90574it [00:35, 2948.80it/s]87408it [00:35, 1091.11it/s]90925it [00:35, 3098.29it/s]87719it [00:36, 1338.10it/s]86216it [00:35, 317.72it/s] 88093it [00:36, 1695.38it/s]91253it [00:35, 3066.51it/s]86589it [00:36, 449.07it/s]86008it [00:35, 352.94it/s] 88418it [00:36, 1956.36it/s]91572it [00:35, 3050.67it/s]86866it [00:36, 570.02it/s]86324it [00:35, 469.18it/s]88791it [00:36, 2309.37it/s]91886it [00:36, 2959.22it/s]87232it [00:36, 784.15it/s]86689it [00:36, 646.81it/s]89163it [00:36, 2621.45it/s]92233it [00:36, 3099.67it/s]87577it [00:36, 1015.92it/s]86983it [00:36, 816.19it/s]89510it [00:36, 2742.22it/s]92567it [00:36, 3166.74it/s]87913it [00:36, 1281.25it/s]87300it [00:36, 1040.82it/s]89864it [00:36, 2941.07it/s]92888it [00:36, 3052.60it/s]88259it [00:36, 1584.52it/s]87606it [00:36, 1282.80it/s]90205it [00:36, 2986.84it/s]93246it [00:36, 3200.52it/s]88584it [00:36, 1852.46it/s]87953it [00:36, 1602.70it/s]90537it [00:36, 3011.32it/s]88907it [00:36, 2103.82it/s]93570it [00:36, 2999.46it/s]88269it [00:36, 1869.75it/s]90894it [00:36, 3163.49it/s]89257it [00:36, 2341.23it/s]93923it [00:36, 3146.02it/s]88584it [00:36, 2113.05it/s]91229it [00:37, 3119.99it/s]89617it [00:36, 2629.24it/s]94283it [00:36, 3273.51it/s]88949it [00:36, 2449.08it/s]91554it [00:37, 3119.05it/s]89948it [00:37, 2784.72it/s]94615it [00:36, 3051.23it/s]89278it [00:36, 2495.73it/s]91875it [00:37, 3062.80it/s]90277it [00:37, 2856.90it/s]94967it [00:37, 3179.33it/s]89644it [00:36, 2777.75it/s]92225it [00:37, 3184.81it/s]90620it [00:37, 3008.66it/s]89995it [00:37, 2965.46it/s]95290it [00:37, 3031.34it/s]92578it [00:37, 3282.05it/s]90948it [00:37, 2903.34it/s]95604it [00:37, 3059.84it/s]90328it [00:37, 2917.39it/s]92911it [00:37, 3182.11it/s]91305it [00:37, 3082.53it/s]95955it [00:37, 3186.65it/s]90681it [00:37, 3081.38it/s]93272it [00:37, 3302.55it/s]91663it [00:37, 3219.08it/s]96277it [00:37, 3118.73it/s]91009it [00:37, 3045.43it/s]93606it [00:37, 3096.33it/s]91997it [00:37, 3019.82it/s]96592it [00:37, 3099.09it/s]91328it [00:37, 2998.32it/s]93934it [00:37, 3145.40it/s]92316it [00:37, 3064.16it/s]96904it [00:37, 3052.68it/s]91673it [00:37, 3122.97it/s]94295it [00:38, 3275.53it/s]92631it [00:37, 3041.38it/s]97262it [00:37, 3204.28it/s]91994it [00:37, 3068.36it/s]94626it [00:38, 3182.61it/s]92990it [00:38, 3195.65it/s]97618it [00:37, 3307.10it/s]92333it [00:37, 3158.77it/s]94981it [00:38, 3286.25it/s]93315it [00:38, 3182.29it/s]97950it [00:38, 3201.20it/s]92654it [00:37, 3012.59it/s]95312it [00:38, 3114.91it/s]98305it [00:38, 3301.54it/s]93637it [00:38, 3000.05it/s]92996it [00:38, 3126.59it/s]95669it [00:38, 3241.84it/s]98637it [00:38, 3209.21it/s]93987it [00:38, 3137.85it/s]93333it [00:38, 3195.85it/s]95997it [00:38, 3167.83it/s]98990it [00:38, 3298.87it/s]94305it [00:38, 3077.71it/s]93656it [00:38, 3119.03it/s]96353it [00:38, 3278.27it/s]94642it [00:38, 3160.42it/s]99340it [00:38, 3189.82it/s]93982it [00:38, 3156.84it/s]96683it [00:38, 3208.39it/s]94961it [00:38, 3134.49it/s]99692it [00:38, 3282.47it/s]94300it [00:38, 3010.65it/s]97006it [00:38, 3044.45it/s]100039it [00:38, 3335.54it/s]95277it [00:38, 3002.31it/s]94635it [00:38, 3105.75it/s]97362it [00:38, 3187.24it/s]100375it [00:38, 3222.40it/s]95628it [00:38, 3140.00it/s]94981it [00:38, 3206.54it/s]97684it [00:39, 3136.45it/s]100733it [00:38, 3322.43it/s]95975it [00:38, 3232.89it/s]95304it [00:38, 3106.60it/s]98043it [00:39, 3264.06it/s]96301it [00:39, 3157.81it/s]101067it [00:38, 3221.22it/s]95650it [00:38, 3205.57it/s]98394it [00:39, 3332.63it/s]96655it [00:39, 3267.10it/s]101415it [00:39, 3289.19it/s]95974it [00:38, 3212.48it/s]101767it [00:39, 3354.98it/s]98729it [00:39, 3112.72it/s]96984it [00:39, 3048.06it/s]96297it [00:39, 3037.10it/s]99085it [00:39, 3235.92it/s]102104it [00:39, 3238.67it/s]97331it [00:39, 3164.85it/s]96651it [00:39, 3177.25it/s]99413it [00:39, 3175.91it/s]102459it [00:39, 3326.68it/s]97658it [00:39, 3110.00it/s]96972it [00:39, 3061.19it/s]99763it [00:39, 3266.91it/s]102794it [00:39, 3221.89it/s]98015it [00:39, 3238.95it/s]97326it [00:39, 3194.73it/s]100119it [00:39, 3350.51it/s]103149it [00:39, 3315.31it/s]98371it [00:39, 3329.77it/s]97657it [00:39, 3115.42it/s]100456it [00:39, 3271.11it/s]103505it [00:39, 3385.96it/s]98707it [00:39, 3221.98it/s]98008it [00:39, 3226.55it/s]100813it [00:40, 3356.53it/s]103845it [00:39, 3263.77it/s]99062it [00:39, 3314.91it/s]98360it [00:39, 3310.22it/s]101151it [00:40, 3271.95it/s]104201it [00:39, 3348.23it/s]99396it [00:40, 3240.36it/s]98693it [00:39, 3206.43it/s]101505it [00:40, 3347.68it/s]104538it [00:40, 3216.43it/s]99746it [00:40, 3314.82it/s]99030it [00:39, 3251.06it/s]101850it [00:40, 3376.27it/s]104882it [00:40, 3277.69it/s]100079it [00:40, 3300.57it/s]99357it [00:40, 3152.95it/s]102189it [00:40, 3227.78it/s]100410it [00:40, 3212.70it/s]105220it [00:40, 3189.68it/s]99710it [00:40, 3259.45it/s]102539it [00:40, 3305.26it/s]100767it [00:40, 3313.93it/s]105573it [00:40, 3285.10it/s]100063it [00:40, 3336.66it/s]102872it [00:40, 3221.33it/s]105929it [00:40, 3362.60it/s]101100it [00:40, 3224.48it/s]100398it [00:40, 3214.01it/s]103232it [00:40, 3329.65it/s]101455it [00:40, 3317.78it/s]106267it [00:40, 3235.44it/s]100751it [00:40, 3303.48it/s]103567it [00:40, 3236.01it/s]101803it [00:40, 3363.64it/s]106620it [00:40, 3318.66it/s]101083it [00:40, 3201.38it/s]103927it [00:40, 3339.23it/s]102141it [00:40, 3264.42it/s]106954it [00:40, 3227.01it/s]101419it [00:40, 3244.49it/s]104286it [00:41, 3411.69it/s]102498it [00:40, 3352.60it/s]107312it [00:40, 3327.65it/s]101767it [00:40, 3312.28it/s]104629it [00:41, 3300.28it/s]107670it [00:40, 3396.93it/s]102835it [00:41, 3257.65it/s]102100it [00:40, 3199.34it/s]104979it [00:41, 3355.91it/s]103189it [00:41, 3336.97it/s]108011it [00:41, 3283.58it/s]102452it [00:40, 3290.75it/s]105316it [00:41, 3252.75it/s]108369it [00:41, 3366.44it/s]103538it [00:41, 3240.49it/s]102783it [00:41, 3182.33it/s]105674it [00:41, 3345.31it/s]103894it [00:41, 3330.31it/s]108708it [00:41, 3254.81it/s]103136it [00:41, 3280.83it/s]106033it [00:41, 3415.24it/s]104251it [00:41, 3399.01it/s]109067it [00:41, 3350.08it/s]103486it [00:41, 3343.65it/s]106376it [00:41, 3289.24it/s]104593it [00:41, 3224.25it/s]109420it [00:41, 3170.30it/s]103822it [00:41, 3185.46it/s]106726it [00:41, 3348.62it/s]104941it [00:41, 3295.62it/s]109771it [00:41, 3263.37it/s]104174it [00:41, 3279.29it/s]107063it [00:41, 3252.25it/s]110128it [00:41, 3348.84it/s]105273it [00:41, 3210.32it/s]104504it [00:41, 3178.26it/s]107422it [00:42, 3347.84it/s]105629it [00:41, 3308.76it/s]110466it [00:41, 3249.67it/s]104856it [00:41, 3274.55it/s]107759it [00:42, 3238.81it/s]105984it [00:42, 3378.34it/s]110820it [00:41, 3325.22it/s]105206it [00:41, 3337.50it/s]108116it [00:42, 3331.51it/s]106324it [00:42, 3260.06it/s]111155it [00:42, 3203.28it/s]105542it [00:41, 3210.66it/s]108466it [00:42, 3378.53it/s]106652it [00:42, 3208.15it/s]111512it [00:42, 3305.28it/s]105865it [00:42, 3126.97it/s]108806it [00:42, 3217.86it/s]111868it [00:42, 3376.08it/s]106974it [00:42, 3131.35it/s]106180it [00:42, 3071.35it/s]109165it [00:42, 3322.71it/s]107330it [00:42, 3252.22it/s]112208it [00:42, 3264.09it/s]106532it [00:42, 3196.79it/s]109500it [00:42, 3240.53it/s]107689it [00:42, 3348.20it/s]112567it [00:42, 3356.33it/s]106884it [00:42, 3288.33it/s]109826it [00:42, 3233.85it/s]108026it [00:42, 3247.54it/s]112905it [00:42, 3237.88it/s]107215it [00:42, 3061.93it/s]110172it [00:42, 3296.98it/s]108383it [00:42, 3339.53it/s]113262it [00:42, 3331.58it/s]107569it [00:42, 3194.29it/s]110503it [00:42, 3196.40it/s]108719it [00:42, 3106.91it/s]113620it [00:42, 3231.30it/s]107892it [00:42, 3118.64it/s]110862it [00:43, 3308.89it/s]109056it [00:42, 3180.08it/s]113974it [00:42, 3316.14it/s]108207it [00:42, 3086.34it/s]111195it [00:43, 3053.38it/s]114320it [00:42, 3348.09it/s]109378it [00:43, 3132.97it/s]108558it [00:42, 3206.37it/s]111544it [00:43, 3172.37it/s]114657it [00:43, 3236.57it/s]109694it [00:43, 3002.03it/s]108881it [00:42, 3131.43it/s]111902it [00:43, 3286.74it/s]115016it [00:43, 3337.31it/s]110046it [00:43, 3147.10it/s]109235it [00:43, 3248.41it/s]112235it [00:43, 3182.76it/s]115352it [00:43, 3222.40it/s]110364it [00:43, 3065.84it/s]109562it [00:43, 3054.39it/s]112572it [00:43, 3235.84it/s]110714it [00:43, 3187.53it/s]109914it [00:43, 3184.38it/s]112898it [00:43, 3153.63it/s]111069it [00:43, 3289.65it/s]110257it [00:43, 3122.59it/s]113260it [00:43, 3286.22it/s]111400it [00:43, 3181.14it/s]110598it [00:43, 3202.47it/s]113616it [00:43, 3364.21it/s]111751it [00:43, 3274.19it/s]110921it [00:43, 3169.88it/s]113955it [00:44, 3149.79it/s]112081it [00:43, 3076.85it/s]111240it [00:43, 3071.75it/s]114307it [00:44, 3252.75it/s]112436it [00:44, 3208.80it/s]111592it [00:43, 3198.06it/s]114636it [00:44, 3176.37it/s]112778it [00:44, 3132.79it/s]111937it [00:43, 3132.29it/s]114997it [00:44, 3299.17it/s]113136it [00:44, 3257.05it/s]112292it [00:44, 3249.00it/s]115330it [00:44, 3211.81it/s]113491it [00:44, 3338.80it/s]112637it [00:44, 3272.22it/s]113828it [00:44, 3086.35it/s]112966it [00:44, 3105.46it/s]114173it [00:44, 3185.41it/s]113320it [00:44, 3227.24it/s]114496it [00:44, 3104.91it/s]113646it [00:44, 3143.10it/s]114824it [00:44, 3153.69it/s]114002it [00:44, 3259.47it/s]115182it [00:44, 3274.82it/s]114356it [00:44, 3339.67it/s]114692it [00:44, 3229.80it/s]115032it [00:44, 3278.55it/s]115362it [00:45, 3186.32it/s]115676it [00:46, 298.19it/s] 116038it [00:46, 418.68it/s]116331it [00:47, 542.38it/s]116685it [00:47, 739.46it/s]117039it [00:47, 979.79it/s]117355it [00:47, 1192.81it/s]117703it [00:47, 1495.60it/s]118018it [00:47, 1746.59it/s]118378it [00:47, 2085.75it/s]118703it [00:47, 2305.71it/s]115654it [00:48, 289.89it/s] 119024it [00:47, 2465.71it/s]116017it [00:48, 408.48it/s]119372it [00:47, 2710.80it/s]116322it [00:48, 535.95it/s]119696it [00:48, 2743.44it/s]116635it [00:48, 703.23it/s]120022it [00:48, 2876.67it/s]116997it [00:48, 949.33it/s]120382it [00:48, 3071.98it/s]117309it [00:48, 1179.11it/s]120711it [00:48, 3049.97it/s]117662it [00:48, 1491.07it/s]121066it [00:48, 3189.07it/s]117984it [00:48, 1748.95it/s]115512it [00:48, 280.89it/s] 121397it [00:48, 2966.57it/s]118342it [00:48, 2085.08it/s]115820it [00:48, 376.98it/s]121733it [00:48, 3072.16it/s]118702it [00:49, 2400.17it/s]116168it [00:48, 521.99it/s]115682it [00:48, 282.58it/s] 122087it [00:48, 3202.54it/s]116452it [00:48, 666.24it/s]119040it [00:49, 2544.44it/s]116037it [00:48, 397.18it/s]122415it [00:48, 3062.22it/s]116794it [00:49, 891.94it/s]119390it [00:49, 2773.21it/s]116328it [00:48, 517.07it/s]122772it [00:49, 3203.94it/s]117094it [00:49, 1109.63it/s]119723it [00:49, 2837.24it/s]116682it [00:48, 709.98it/s]123098it [00:49, 3136.10it/s]117398it [00:49, 1360.01it/s]120057it [00:49, 2968.53it/s]117009it [00:49, 923.82it/s]123452it [00:49, 3250.19it/s]117746it [00:49, 1689.28it/s]120407it [00:49, 2983.88it/s]117316it [00:49, 1147.80it/s]123781it [00:49, 3146.14it/s]118059it [00:49, 1931.24it/s]120766it [00:49, 3147.41it/s]117671it [00:49, 1463.76it/s]124135it [00:49, 3257.68it/s]118409it [00:49, 2252.72it/s]121124it [00:49, 3267.26it/s]117990it [00:49, 1714.87it/s]124493it [00:49, 3348.45it/s]118730it [00:49, 2418.96it/s]121463it [00:49, 3201.36it/s]118335it [00:49, 2030.69it/s]119075it [00:49, 2665.62it/s]124830it [00:49, 3244.05it/s]121821it [00:49, 3306.49it/s]118687it [00:49, 2337.93it/s]119424it [00:49, 2876.38it/s]125187it [00:49, 3336.72it/s]122159it [00:50, 3225.07it/s]119018it [00:49, 2490.47it/s]125523it [00:49, 3236.21it/s]119755it [00:50, 2891.68it/s]122522it [00:50, 3338.84it/s]119367it [00:49, 2730.61it/s]125883it [00:50, 3340.40it/s]120108it [00:50, 3063.06it/s]122882it [00:50, 3412.70it/s]119696it [00:49, 2791.68it/s]126241it [00:50, 3408.98it/s]120438it [00:50, 3034.97it/s]123227it [00:50, 3283.54it/s]120047it [00:50, 2978.51it/s]120790it [00:50, 3169.62it/s]126584it [00:50, 3277.71it/s]123585it [00:50, 3367.01it/s]120399it [00:50, 3125.32it/s]121136it [00:50, 3251.44it/s]126933it [00:50, 3338.36it/s]123925it [00:50, 3184.80it/s]120735it [00:50, 3081.22it/s]127269it [00:50, 3153.54it/s]121471it [00:50, 3042.84it/s]124276it [00:50, 3275.22it/s]121087it [00:50, 3201.10it/s]127627it [00:50, 3271.80it/s]121820it [00:50, 3166.31it/s]124607it [00:50, 3203.44it/s]121420it [00:50, 3126.17it/s]127969it [00:50, 3197.17it/s]122144it [00:50, 3099.65it/s]124967it [00:50, 3314.20it/s]121772it [00:50, 3237.00it/s]128325it [00:50, 3299.06it/s]122498it [00:50, 3222.77it/s]125325it [00:51, 3390.86it/s]122103it [00:50, 3128.18it/s]128676it [00:50, 3357.98it/s]122852it [00:50, 3311.82it/s]125666it [00:51, 3265.67it/s]122459it [00:50, 3248.18it/s]129014it [00:50, 3264.42it/s]123187it [00:51, 3214.59it/s]126028it [00:51, 3366.29it/s]122813it [00:50, 3331.04it/s]129369it [00:51, 3342.28it/s]123541it [00:51, 3306.20it/s]126367it [00:51, 3270.70it/s]123150it [00:50, 3213.68it/s]129705it [00:51, 3242.15it/s]123874it [00:51, 3206.19it/s]126723it [00:51, 3352.99it/s]123502it [00:51, 3299.31it/s]130064it [00:51, 3340.96it/s]124216it [00:51, 3266.95it/s]127081it [00:51, 3418.90it/s]123835it [00:51, 3188.00it/s]130421it [00:51, 3405.63it/s]124570it [00:51, 3344.04it/s]127425it [00:51, 3311.16it/s]124187it [00:51, 3281.50it/s]130763it [00:51, 3289.98it/s]124906it [00:51, 3229.32it/s]127785it [00:51, 3392.11it/s]124540it [00:51, 3352.84it/s]131119it [00:51, 3365.05it/s]125259it [00:51, 3313.58it/s]128126it [00:51, 3286.67it/s]124878it [00:51, 3241.59it/s]131457it [00:51, 3256.50it/s]125592it [00:51, 3204.42it/s]128475it [00:51, 3342.66it/s]125229it [00:51, 3318.05it/s]131803it [00:51, 3314.81it/s]125939it [00:51, 3279.59it/s]128811it [00:52, 3171.43it/s]125563it [00:51, 3205.84it/s]132151it [00:51, 3361.07it/s]126287it [00:52, 3176.34it/s]129169it [00:52, 3286.12it/s]125893it [00:51, 3230.48it/s]132489it [00:52, 3244.23it/s]126638it [00:52, 3270.53it/s]129526it [00:52, 3366.33it/s]126243it [00:51, 3308.00it/s]132847it [00:52, 3339.67it/s]126987it [00:52, 3326.12it/s]129865it [00:52, 3259.69it/s]126575it [00:52, 3188.74it/s]133183it [00:52, 3221.02it/s]127322it [00:52, 3215.40it/s]130217it [00:52, 3331.69it/s]126908it [00:52, 3227.89it/s]133517it [00:52, 3254.37it/s]127655it [00:52, 3247.56it/s]130552it [00:52, 3016.99it/s]127233it [00:52, 2974.24it/s]133849it [00:52, 3161.24it/s]127982it [00:52, 3111.72it/s]130890it [00:52, 3114.52it/s]127591it [00:52, 3139.36it/s]134208it [00:52, 3282.93it/s]128336it [00:52, 3230.95it/s]131232it [00:52, 3200.05it/s]127946it [00:52, 3254.11it/s]134560it [00:52, 3350.23it/s]128691it [00:52, 3321.76it/s]128276it [00:52, 3037.30it/s]131557it [00:52, 2950.01it/s]134897it [00:52, 3131.32it/s]129025it [00:52, 3123.45it/s]128632it [00:52, 3180.44it/s]131915it [00:53, 3121.70it/s]135253it [00:52, 3251.18it/s]129342it [00:52, 3135.79it/s]128955it [00:52, 3114.97it/s]132234it [00:53, 3091.69it/s]135582it [00:52, 3189.55it/s]129658it [00:53, 3085.04it/s]129307it [00:52, 3228.03it/s]132590it [00:53, 3221.72it/s]135916it [00:53, 3229.46it/s]130006it [00:53, 3195.92it/s]132916it [00:53, 3218.04it/s]129645it [00:53, 3140.98it/s]136256it [00:53, 3278.77it/s]130347it [00:53, 3256.27it/s]133241it [00:53, 3158.92it/s]129962it [00:53, 3103.53it/s]136586it [00:53, 3186.34it/s]130674it [00:53, 3160.05it/s]133567it [00:53, 3177.78it/s]130312it [00:53, 3214.27it/s]136908it [00:53, 3195.22it/s]130992it [00:53, 3031.68it/s]133887it [00:53, 2981.72it/s]130636it [00:53, 3034.46it/s]137229it [00:53, 3047.24it/s]131327it [00:53, 3021.93it/s]134249it [00:53, 3160.25it/s]130990it [00:53, 3175.61it/s]137590it [00:53, 3205.68it/s]131681it [00:53, 3167.53it/s]134608it [00:53, 3282.50it/s]131325it [00:53, 3101.09it/s]137941it [00:53, 3292.18it/s]132017it [00:53, 3222.08it/s]131650it [00:53, 3142.82it/s]134940it [00:54, 3045.27it/s]138273it [00:53, 3018.86it/s]132341it [00:53, 3076.37it/s]132002it [00:53, 3249.86it/s]135302it [00:54, 3204.04it/s]138633it [00:53, 3177.04it/s]132700it [00:54, 3219.71it/s]132329it [00:53, 3154.94it/s]135628it [00:54, 3165.22it/s]138956it [00:54, 3075.03it/s]133025it [00:54, 3019.45it/s]132680it [00:53, 3256.65it/s]135948it [00:54, 3115.91it/s]139292it [00:54, 3154.00it/s]133371it [00:54, 3141.11it/s]136309it [00:54, 3254.86it/s]133008it [00:54, 3163.16it/s]139649it [00:54, 3270.49it/s]133725it [00:54, 3252.06it/s]133358it [00:54, 3259.75it/s]136637it [00:54, 3169.88it/s]139979it [00:54, 3070.57it/s]134054it [00:54, 3037.99it/s]133710it [00:54, 3334.45it/s]136956it [00:54, 3164.78it/s]140337it [00:54, 3212.06it/s]134397it [00:54, 3144.62it/s]134045it [00:54, 3230.43it/s]137274it [00:54, 3131.48it/s]140663it [00:54, 3150.07it/s]134716it [00:54, 3079.73it/s]134397it [00:54, 3313.73it/s]137624it [00:54, 3236.94it/s]140981it [00:54, 3123.04it/s]135037it [00:54, 3110.23it/s]134730it [00:54, 3203.00it/s]137949it [00:54, 3117.55it/s]141330it [00:54, 3227.43it/s]135387it [00:54, 3219.86it/s]135073it [00:54, 3267.03it/s]138263it [00:55, 2990.66it/s]141655it [00:54, 3081.43it/s]135711it [00:55, 3089.16it/s]135435it [00:54, 3368.27it/s]138607it [00:55, 3112.76it/s]142015it [00:55, 3227.23it/s]136068it [00:55, 3225.39it/s]135774it [00:54, 3252.84it/s]138921it [00:55, 3093.10it/s]142341it [00:55, 3157.60it/s]136393it [00:55, 3148.57it/s]136131it [00:55, 3342.98it/s]139256it [00:55, 3166.93it/s]142695it [00:55, 3266.30it/s]136721it [00:55, 3184.91it/s]139579it [00:55, 3184.36it/s]136467it [00:55, 3226.43it/s]143044it [00:55, 3330.77it/s]137055it [00:55, 3228.79it/s]136827it [00:55, 3331.58it/s]139899it [00:55, 3143.83it/s]143379it [00:55, 3231.82it/s]137380it [00:55, 3144.58it/s]137186it [00:55, 3405.25it/s]140260it [00:55, 3279.52it/s]143726it [00:55, 3297.69it/s]137696it [00:55, 3098.55it/s]137528it [00:55, 3230.60it/s]140589it [00:55, 3023.66it/s]144058it [00:55, 3167.02it/s]138016it [00:55, 3126.40it/s]137885it [00:55, 3326.74it/s]140945it [00:55, 3171.83it/s]144418it [00:55, 3288.80it/s]138330it [00:55, 3072.24it/s]138221it [00:55, 3231.09it/s]141305it [00:56, 3293.53it/s]144762it [00:55, 3331.87it/s]138677it [00:55, 3183.22it/s]138547it [00:55, 3231.38it/s]141638it [00:56, 3075.36it/s]138997it [00:56, 3053.41it/s]145097it [00:55, 3129.11it/s]138885it [00:55, 3127.08it/s]141998it [00:56, 3220.92it/s]139351it [00:56, 3191.68it/s]145457it [00:56, 3261.63it/s]139233it [00:55, 3224.51it/s]142325it [00:56, 3087.10it/s]139672it [00:56, 3095.82it/s]145787it [00:56, 2999.53it/s]139558it [00:56, 3206.79it/s]142666it [00:56, 3176.66it/s]139984it [00:56, 2996.69it/s]146135it [00:56, 3129.67it/s]139880it [00:56, 3137.29it/s]143015it [00:56, 3263.96it/s]140324it [00:56, 3109.98it/s]146454it [00:56, 3000.93it/s]140237it [00:56, 3259.59it/s]143345it [00:56, 3124.61it/s]140637it [00:56, 2931.38it/s]146768it [00:56, 3039.19it/s]140566it [00:56, 3105.06it/s]143672it [00:56, 3163.18it/s]140950it [00:56, 2986.25it/s]147121it [00:56, 3176.77it/s]140916it [00:56, 3213.95it/s]143991it [00:56, 3103.14it/s]141306it [00:56, 3149.17it/s]147442it [00:56, 3134.13it/s]141273it [00:56, 3316.40it/s]144349it [00:57, 3237.51it/s]141624it [00:56, 3092.47it/s]147776it [00:56, 3192.71it/s]144694it [00:57, 3296.51it/s]141607it [00:56, 3066.86it/s]141936it [00:57, 3086.05it/s]148098it [00:56, 3163.36it/s]141931it [00:56, 3113.07it/s]145026it [00:57, 3124.78it/s]142247it [00:57, 3026.87it/s]148416it [00:57, 3099.56it/s]145389it [00:57, 3267.09it/s]142246it [00:56, 2965.71it/s]142603it [00:57, 3178.55it/s]148776it [00:57, 3244.14it/s]145719it [00:57, 3211.89it/s]142603it [00:57, 3131.62it/s]142923it [00:57, 3172.18it/s]149102it [00:57, 3054.85it/s]146043it [00:57, 3177.26it/s]142920it [00:57, 3072.38it/s]143242it [00:57, 2993.28it/s]149440it [00:57, 3144.60it/s]146404it [00:57, 3301.04it/s]143230it [00:57, 3008.29it/s]143599it [00:57, 3155.71it/s]149803it [00:57, 3283.03it/s]146736it [00:57, 3210.56it/s]143591it [00:57, 3176.83it/s]143927it [00:57, 3105.39it/s]150134it [00:57, 3200.82it/s]147100it [00:57, 3332.92it/s]143926it [00:57, 3116.58it/s]144283it [00:57, 3235.00it/s]150497it [00:57, 3323.19it/s]147435it [00:57, 3260.79it/s]144285it [00:57, 3249.09it/s]144632it [00:57, 3307.79it/s]150832it [00:57, 3126.54it/s]147763it [00:58, 3235.66it/s]144612it [00:57, 3215.15it/s]144965it [00:57, 3204.02it/s]151164it [00:57, 3178.65it/s]148114it [00:58, 3312.87it/s]144935it [00:57, 3065.46it/s]145312it [00:58, 3279.66it/s]151489it [00:57, 3100.73it/s]148447it [00:58, 3165.42it/s]145296it [00:57, 3218.42it/s]145642it [00:58, 3193.80it/s]151850it [00:58, 3244.86it/s]148809it [00:58, 3292.77it/s]145621it [00:58, 3147.36it/s]146004it [00:58, 3315.79it/s]152213it [00:58, 3353.51it/s]149141it [00:58, 3167.86it/s]145954it [00:58, 3197.07it/s]146357it [00:58, 3377.77it/s]149479it [00:58, 3227.61it/s]146288it [00:58, 3236.99it/s]146696it [00:58, 3260.15it/s]149808it [00:58, 3180.55it/s]146613it [00:58, 3108.66it/s]147057it [00:58, 3358.58it/s]150135it [00:58, 3203.81it/s]146936it [00:58, 3142.95it/s]147395it [00:58, 3127.42it/s]150484it [00:58, 3285.67it/s]147286it [00:58, 3086.22it/s]147755it [00:58, 3258.41it/s]150814it [00:59, 3214.47it/s]147648it [00:58, 3235.52it/s]148111it [00:58, 3343.75it/s]151158it [00:59, 3278.00it/s]148003it [00:58, 3324.66it/s]148449it [00:59, 3078.38it/s]151487it [00:59, 3177.49it/s]148338it [00:58, 3215.36it/s]148792it [00:59, 3173.74it/s]151806it [00:59, 3027.78it/s]148696it [00:58, 3319.14it/s]149115it [00:59, 3116.27it/s]152172it [00:59, 3203.60it/s]149030it [00:59, 3220.27it/s]149446it [00:59, 3167.70it/s]149354it [00:59, 3209.13it/s]149766it [00:59, 3164.69it/s]149716it [00:59, 3327.20it/s]150085it [00:59, 3043.27it/s]150050it [00:59, 3215.42it/s]150431it [00:59, 3161.14it/s]150373it [00:59, 3150.15it/s]150750it [00:59, 2912.62it/s]150690it [00:59, 2914.66it/s]151111it [00:59, 3102.70it/s]151050it [00:59, 3100.37it/s]151471it [01:00, 3241.62it/s]151405it [00:59, 3226.77it/s]151800it [01:00, 2999.97it/s]151732it [00:59, 3071.79it/s]152131it [01:00, 3083.37it/s]152089it [01:00, 3211.04it/s]152551it [01:02, 235.50it/s] 152918it [01:02, 332.90it/s]153238it [01:03, 443.55it/s]153604it [01:03, 612.64it/s]153974it [01:03, 828.32it/s]154302it [01:03, 1044.38it/s]154672it [01:03, 1347.28it/s]155008it [01:03, 1608.66it/s]152495it [01:03, 238.90it/s] 155377it [01:03, 1951.63it/s]152861it [01:03, 340.13it/s]155744it [01:03, 2278.52it/s]153227it [01:04, 474.78it/s]156092it [01:03, 2465.39it/s]153526it [01:04, 612.15it/s]156447it [01:03, 2712.78it/s]153884it [01:04, 827.31it/s]156789it [01:04, 2787.57it/s]154200it [01:04, 1041.18it/s]157156it [01:04, 3010.75it/s]154566it [01:04, 1349.27it/s]157496it [01:04, 3031.53it/s]154918it [01:04, 1629.19it/s]157858it [01:04, 3188.80it/s]155283it [01:04, 1968.19it/s]158223it [01:04, 3316.16it/s]155650it [01:04, 2296.65it/s]158571it [01:04, 3251.41it/s]155995it [01:04, 2485.87it/s]158940it [01:04, 3373.09it/s]152414it [01:04, 234.30it/s] 156363it [01:05, 2762.24it/s]159286it [01:04, 3293.04it/s]152756it [01:04, 326.06it/s]156708it [01:05, 2840.31it/s]159654it [01:04, 3401.40it/s]153115it [01:04, 455.07it/s]157077it [01:05, 3056.41it/s]153407it [01:04, 586.75it/s]160000it [01:05, 3300.07it/s]152445it [01:05, 211.77it/s] 157438it [01:05, 3082.75it/s]153767it [01:05, 800.34it/s]160367it [01:05, 3404.58it/s]152802it [01:05, 301.52it/s]157804it [01:05, 3237.07it/s]160729it [01:05, 3466.06it/s]154079it [01:05, 1008.24it/s]153159it [01:05, 421.74it/s]158170it [01:05, 3352.15it/s]154442it [01:05, 1311.08it/s]161079it [01:05, 3342.30it/s]153450it [01:05, 545.49it/s]158521it [01:05, 3255.05it/s]154797it [01:05, 1628.33it/s]161434it [01:05, 3399.89it/s]153798it [01:05, 740.65it/s]158879it [01:05, 3345.83it/s]155130it [01:05, 1876.70it/s]161777it [01:05, 3284.40it/s]154105it [01:05, 938.48it/s]159222it [01:05, 3279.15it/s]155488it [01:05, 2200.69it/s]162139it [01:05, 3379.47it/s]154462it [01:05, 1227.20it/s]159580it [01:05, 3363.28it/s]155821it [01:05, 2383.39it/s]162479it [01:05, 3292.85it/s]154812it [01:05, 1534.84it/s]159948it [01:06, 3453.48it/s]156179it [01:05, 2657.17it/s]162852it [01:05, 3416.21it/s]155140it [01:05, 1790.11it/s]160297it [01:06, 3346.94it/s]156540it [01:05, 2890.84it/s]163222it [01:05, 3498.46it/s]155497it [01:06, 2119.99it/s]160663it [01:06, 3434.91it/s]156882it [01:05, 2920.85it/s]163574it [01:06, 3384.55it/s]155828it [01:06, 2317.97it/s]161009it [01:06, 3338.07it/s]157225it [01:06, 3054.65it/s]163943it [01:06, 3471.24it/s]156185it [01:06, 2600.81it/s]161379it [01:06, 3441.23it/s]157558it [01:06, 3045.56it/s]164292it [01:06, 3370.68it/s]156544it [01:06, 2841.21it/s]161726it [01:06, 3351.63it/s]157919it [01:06, 3199.08it/s]164659it [01:06, 3454.54it/s]156884it [01:06, 2891.62it/s]162100it [01:06, 3462.06it/s]165006it [01:06, 3375.08it/s]158275it [01:06, 3139.11it/s]157245it [01:06, 3078.98it/s]162463it [01:06, 3509.02it/s]165377it [01:06, 3470.03it/s]158635it [01:06, 3266.59it/s]157583it [01:06, 3046.84it/s]162816it [01:06, 3404.98it/s]165752it [01:06, 3550.82it/s]159001it [01:06, 3376.83it/s]157940it [01:06, 3188.11it/s]163182it [01:07, 3477.41it/s]166109it [01:06, 3418.25it/s]159346it [01:06, 3268.37it/s]158276it [01:06, 3061.62it/s]163532it [01:07, 3331.14it/s]166468it [01:06, 3461.34it/s]159709it [01:06, 3370.12it/s]158626it [01:07, 3174.70it/s]163892it [01:07, 3406.03it/s]166816it [01:07, 3368.60it/s]160051it [01:06, 3251.80it/s]158982it [01:07, 3282.52it/s]164235it [01:07, 3323.27it/s]167169it [01:07, 3407.38it/s]160410it [01:07, 3346.86it/s]159318it [01:07, 3203.99it/s]164597it [01:07, 3408.00it/s]160772it [01:07, 3424.06it/s]167519it [01:07, 3320.72it/s]159675it [01:07, 3305.82it/s]164966it [01:07, 3488.37it/s]167893it [01:07, 3440.12it/s]161117it [01:07, 3177.72it/s]160010it [01:07, 3213.19it/s]165317it [01:07, 3396.53it/s]168271it [01:07, 3538.33it/s]161440it [01:07, 3180.38it/s]160361it [01:07, 3296.82it/s]165692it [01:07, 3496.37it/s]168627it [01:07, 3417.28it/s]160721it [01:07, 3384.49it/s]161762it [01:07, 3095.71it/s]166043it [01:07, 3409.61it/s]168997it [01:07, 3496.31it/s]162124it [01:07, 3241.99it/s]161062it [01:07, 3271.16it/s]166417it [01:07, 3503.06it/s]169349it [01:07, 3382.02it/s]161425it [01:07, 3371.86it/s]162476it [01:07, 3167.38it/s]166769it [01:08, 3426.23it/s]169719it [01:07, 3472.51it/s]162839it [01:07, 3296.31it/s]161765it [01:08, 3268.75it/s]167135it [01:08, 3493.68it/s]170068it [01:07, 3365.07it/s]163202it [01:07, 3389.91it/s]162128it [01:08, 3371.78it/s]167511it [01:08, 3570.83it/s]170444it [01:08, 3476.21it/s]163544it [01:07, 3269.87it/s]162477it [01:08, 3270.39it/s]167869it [01:08, 3422.28it/s]170816it [01:08, 3544.73it/s]163906it [01:08, 3368.46it/s]162833it [01:08, 3352.47it/s]168248it [01:08, 3526.75it/s]171172it [01:08, 3400.19it/s]163193it [01:08, 3423.77it/s]164245it [01:08, 3267.65it/s]168603it [01:08, 3351.47it/s]171535it [01:08, 3464.92it/s]164607it [01:08, 3367.02it/s]163537it [01:08, 3236.13it/s]168971it [01:08, 3442.99it/s]171884it [01:08, 3336.50it/s]164979it [01:08, 3467.42it/s]163900it [01:08, 3346.75it/s]169318it [01:08, 3357.40it/s]172255it [01:08, 3441.07it/s]165328it [01:08, 3328.17it/s]164238it [01:08, 3259.89it/s]169688it [01:08, 3448.42it/s]172601it [01:08, 3361.86it/s]165678it [01:08, 3376.36it/s]164602it [01:08, 3367.17it/s]170038it [01:09, 3358.09it/s]172979it [01:08, 3471.68it/s]164977it [01:08, 3477.60it/s]166018it [01:08, 3278.31it/s]170404it [01:09, 3442.15it/s]173353it [01:08, 3547.72it/s]166384it [01:08, 3387.30it/s]165327it [01:09, 3353.77it/s]170778it [01:09, 3527.88it/s]173710it [01:09, 3426.40it/s]166725it [01:08, 3294.92it/s]165687it [01:09, 3423.85it/s]171133it [01:09, 3417.38it/s]174084it [01:09, 3514.73it/s]167088it [01:09, 3390.31it/s]166032it [01:09, 3324.99it/s]171506it [01:09, 3505.04it/s]174437it [01:09, 3415.72it/s]167450it [01:09, 3456.85it/s]166397it [01:09, 3415.79it/s]171858it [01:09, 3389.77it/s]174806it [01:09, 3493.59it/s]167797it [01:09, 3324.31it/s]166741it [01:09, 3336.52it/s]172233it [01:09, 3490.53it/s]175157it [01:09, 3393.26it/s]168165it [01:09, 3426.07it/s]167104it [01:09, 3419.83it/s]172584it [01:09, 3404.96it/s]175528it [01:09, 3482.28it/s]167473it [01:09, 3496.28it/s]168510it [01:09, 3269.64it/s]172962it [01:09, 3511.63it/s]175902it [01:09, 3556.28it/s]168848it [01:09, 3299.26it/s]167824it [01:09, 3367.22it/s]173326it [01:09, 3548.16it/s]176259it [01:09, 3398.90it/s]168186it [01:09, 3438.14it/s]169195it [01:09, 3201.77it/s]173682it [01:10, 3326.83it/s]176617it [01:09, 3450.24it/s]169559it [01:09, 3323.29it/s]168532it [01:10, 3282.55it/s]174047it [01:10, 3417.34it/s]176964it [01:09, 3336.63it/s]169908it [01:09, 3369.01it/s]168898it [01:10, 3387.51it/s]174392it [01:10, 3346.33it/s]177333it [01:10, 3435.59it/s]170247it [01:09, 3268.83it/s]169239it [01:10, 3278.30it/s]174772it [01:10, 3475.10it/s]177679it [01:10, 3334.35it/s]170607it [01:10, 3363.77it/s]169605it [01:10, 3386.39it/s]175122it [01:10, 3386.15it/s]178058it [01:10, 3462.39it/s]169966it [01:10, 3449.83it/s]170945it [01:10, 3258.65it/s]175495it [01:10, 3484.79it/s]178432it [01:10, 3540.32it/s]171308it [01:10, 3364.64it/s]170313it [01:10, 3348.56it/s]175866it [01:10, 3549.88it/s]178788it [01:10, 3432.38it/s]171668it [01:10, 3430.36it/s]170674it [01:10, 3423.37it/s]176223it [01:10, 3400.85it/s]179156it [01:10, 3501.37it/s]172013it [01:10, 3308.56it/s]171018it [01:10, 3287.44it/s]176593it [01:10, 3484.82it/s]179508it [01:10, 3380.25it/s]172376it [01:10, 3399.16it/s]171383it [01:10, 3390.33it/s]176944it [01:11, 3399.25it/s]179887it [01:10, 3495.88it/s]172718it [01:10, 3287.08it/s]171724it [01:10, 3274.39it/s]177316it [01:11, 3490.71it/s]180239it [01:10, 3407.55it/s]173090it [01:10, 3410.01it/s]172095it [01:11, 3396.17it/s]177667it [01:11, 3385.11it/s]180609it [01:11, 3482.85it/s]172463it [01:11, 3476.86it/s]173433it [01:10, 3281.32it/s]178045it [01:11, 3497.42it/s]180959it [01:11, 3393.19it/s]173798it [01:11, 3384.34it/s]178420it [01:11, 3568.34it/s]172813it [01:11, 3351.87it/s]181321it [01:11, 3455.77it/s]174152it [01:11, 3428.96it/s]173175it [01:11, 3427.90it/s]178779it [01:11, 3322.17it/s]181684it [01:11, 3504.56it/s]174497it [01:11, 3329.89it/s]173520it [01:11, 3291.56it/s]179145it [01:11, 3416.09it/s]182036it [01:11, 3385.46it/s]174860it [01:11, 3415.69it/s]173877it [01:11, 3369.10it/s]179491it [01:11, 3330.68it/s]182410it [01:11, 3485.37it/s]175203it [01:11, 3314.08it/s]174236it [01:11, 3277.06it/s]179871it [01:11, 3461.27it/s]182760it [01:11, 3410.76it/s]175565it [01:11, 3394.60it/s]174612it [01:11, 3412.07it/s]180220it [01:11, 3392.66it/s]183129it [01:11, 3490.03it/s]175915it [01:11, 3296.95it/s]174975it [01:11, 3472.42it/s]180593it [01:12, 3487.56it/s]183480it [01:11, 3392.70it/s]176276it [01:11, 3385.87it/s]175324it [01:12, 3356.31it/s]180958it [01:12, 3408.27it/s]183854it [01:11, 3492.45it/s]176625it [01:11, 3415.93it/s]175691it [01:12, 3443.51it/s]181327it [01:12, 3486.30it/s]184229it [01:12, 3566.74it/s]176968it [01:11, 3305.41it/s]181700it [01:12, 3556.13it/s]176038it [01:12, 3317.04it/s]184587it [01:12, 3456.71it/s]177336it [01:12, 3412.02it/s]176402it [01:12, 3406.61it/s]182057it [01:12, 3426.64it/s]184961it [01:12, 3538.18it/s]177679it [01:12, 3294.22it/s]182432it [01:12, 3517.56it/s]176756it [01:12, 3275.49it/s]185317it [01:12, 3407.39it/s]178054it [01:12, 3423.13it/s]177119it [01:12, 3374.78it/s]182786it [01:12, 3448.14it/s]185689it [01:12, 3494.99it/s]178407it [01:12, 3452.72it/s]177483it [01:12, 3449.04it/s]183158it [01:12, 3524.03it/s]186041it [01:12, 3397.76it/s]178754it [01:12, 3336.76it/s]177830it [01:12, 3325.46it/s]183512it [01:12, 3355.16it/s]186413it [01:12, 3487.61it/s]179120it [01:12, 3427.42it/s]178195it [01:12, 3417.33it/s]183877it [01:13, 3438.32it/s]186800it [01:12, 3597.13it/s]179465it [01:12, 3290.79it/s]184252it [01:13, 3526.33it/s]178539it [01:12, 3302.48it/s]187162it [01:12, 3434.87it/s]179835it [01:12, 3406.41it/s]178911it [01:13, 3420.06it/s]184607it [01:13, 3403.26it/s]187539it [01:13, 3521.42it/s]180178it [01:12, 3318.62it/s]179266it [01:13, 3455.83it/s]184988it [01:13, 3519.14it/s]187894it [01:13, 3431.78it/s]180540it [01:13, 3404.92it/s]179614it [01:13, 3351.96it/s]185342it [01:13, 3407.55it/s]188269it [01:13, 3522.59it/s]180911it [01:13, 3492.04it/s]179971it [01:13, 3413.96it/s]185716it [01:13, 3501.63it/s]188623it [01:13, 3421.08it/s]181262it [01:13, 3345.09it/s]180314it [01:13, 3286.97it/s]186068it [01:13, 3414.97it/s]188997it [01:13, 3509.94it/s]181627it [01:13, 3429.95it/s]180673it [01:13, 3371.71it/s]186447it [01:13, 3520.32it/s]189359it [01:13, 3400.55it/s]181972it [01:13, 3305.41it/s]181012it [01:13, 3293.46it/s]186838it [01:13, 3489.10it/s]189734it [01:13, 3498.51it/s]182344it [01:13, 3421.50it/s]181374it [01:13, 3385.35it/s]187213it [01:13, 3561.86it/s]190113it [01:13, 3581.25it/s]182689it [01:13, 3327.87it/s]181734it [01:13, 3444.84it/s]187579it [01:14, 3589.15it/s]190473it [01:13, 3459.54it/s]183040it [01:13, 3379.16it/s]187939it [01:14, 3476.75it/s]182080it [01:14, 3174.95it/s]190844it [01:13, 3529.82it/s]183406it [01:13, 3460.22it/s]188311it [01:14, 3544.09it/s]182447it [01:14, 3311.43it/s]191199it [01:14, 3409.22it/s]183754it [01:13, 3345.82it/s]188667it [01:14, 3355.37it/s]182783it [01:14, 3171.30it/s]191564it [01:14, 3477.95it/s]184121it [01:14, 3438.15it/s]189027it [01:14, 3424.08it/s]183133it [01:14, 3262.19it/s]191914it [01:14, 3395.58it/s]184467it [01:14, 3336.22it/s]189372it [01:14, 3352.78it/s]183477it [01:14, 3219.06it/s]192281it [01:14, 3471.76it/s]184841it [01:14, 3451.57it/s]189742it [01:14, 3451.07it/s]183842it [01:14, 3339.76it/s]192663it [01:14, 3571.58it/s]185188it [01:14, 3341.74it/s]190108it [01:14, 3509.85it/s]184208it [01:14, 3431.71it/s]193022it [01:14, 3461.99it/s]185546it [01:14, 3409.33it/s]190461it [01:14, 3403.28it/s]184554it [01:14, 3338.01it/s]193396it [01:14, 3539.85it/s]185921it [01:14, 3507.59it/s]190829it [01:15, 3481.89it/s]184924it [01:14, 3440.19it/s]193752it [01:14, 3430.84it/s]186274it [01:14, 3390.41it/s]191179it [01:15, 3378.25it/s]185270it [01:14, 3333.91it/s]194132it [01:14, 3534.31it/s]186646it [01:14, 3484.02it/s]191554it [01:15, 3485.02it/s]185627it [01:15, 3399.94it/s]194487it [01:15, 3436.52it/s]186996it [01:14, 3400.43it/s]191904it [01:15, 3417.49it/s]185992it [01:15, 3471.52it/s]194863it [01:15, 3527.78it/s]187348it [01:15, 3433.10it/s]192277it [01:15, 3506.01it/s]186341it [01:15, 3366.71it/s]195239it [01:15, 3465.04it/s]187693it [01:15, 3329.94it/s]192655it [01:15, 3585.71it/s]186718it [01:15, 3481.91it/s]195616it [01:15, 3550.08it/s]188069it [01:15, 3452.12it/s]193015it [01:15, 3441.50it/s]187068it [01:15, 3370.62it/s]195985it [01:15, 3590.31it/s]188440it [01:15, 3527.04it/s]193386it [01:15, 3517.44it/s]187417it [01:15, 3404.54it/s]196346it [01:15, 3452.10it/s]188794it [01:15, 3393.55it/s]193740it [01:15, 3246.55it/s]187759it [01:15, 3221.40it/s]196717it [01:15, 3525.84it/s]189153it [01:15, 3447.56it/s]194118it [01:15, 3392.45it/s]188130it [01:15, 3357.63it/s]197072it [01:15, 3427.28it/s]189500it [01:15, 3338.75it/s]188495it [01:15, 3440.10it/s]194462it [01:16, 3345.23it/s]197444it [01:15, 3509.24it/s]189869it [01:15, 3437.72it/s]194835it [01:16, 3453.26it/s]188842it [01:16, 3353.55it/s]197797it [01:15, 3414.86it/s]190215it [01:15, 3347.18it/s]195231it [01:16, 3596.93it/s]189206it [01:16, 3435.42it/s]198179it [01:16, 3527.66it/s]190581it [01:15, 3434.74it/s]195594it [01:16, 3445.44it/s]189552it [01:16, 3327.43it/s]198561it [01:16, 3611.33it/s]190948it [01:16, 3500.75it/s]195963it [01:16, 3514.80it/s]189917it [01:16, 3414.70it/s]191300it [01:16, 3367.82it/s]190260it [01:16, 3344.45it/s]196317it [01:16, 3412.23it/s]191656it [01:16, 3421.96it/s]190619it [01:16, 3414.54it/s]196703it [01:16, 3539.78it/s]192000it [01:16, 3342.89it/s]190982it [01:16, 3475.10it/s]197060it [01:16, 3434.61it/s]192367it [01:16, 3436.48it/s]197442it [01:16, 3544.74it/s]191331it [01:16, 3340.67it/s]192716it [01:16, 3354.35it/s]191708it [01:16, 3462.11it/s]197799it [01:17, 3438.77it/s]193089it [01:16, 3461.53it/s]192056it [01:16, 3406.63it/s]198181it [01:17, 3545.86it/s]193457it [01:16, 3525.03it/s]192428it [01:17, 3495.20it/s]198555it [01:17, 3602.04it/s]193811it [01:16, 3397.35it/s]192779it [01:17, 3339.51it/s]194187it [01:17, 3499.62it/s]193137it [01:17, 3404.85it/s]194539it [01:17, 3392.71it/s]193512it [01:17, 3502.95it/s]194912it [01:17, 3487.44it/s]193864it [01:17, 3409.22it/s]195263it [01:17, 3406.41it/s]194252it [01:17, 3543.59it/s]195635it [01:17, 3494.32it/s]194608it [01:17, 3428.96it/s]195998it [01:17, 3533.44it/s]194985it [01:17, 3524.78it/s]196353it [01:17, 3353.47it/s]195340it [01:17, 3454.80it/s]196732it [01:17, 3476.67it/s]195722it [01:18, 3560.20it/s]197082it [01:17, 3373.56it/s]196080it [01:18, 3356.68it/s]197459it [01:17, 3485.43it/s]196460it [01:18, 3480.49it/s]197810it [01:18, 3381.07it/s]196852it [01:18, 3604.52it/s]198185it [01:18, 3484.22it/s]197215it [01:18, 3455.47it/s]198561it [01:18, 3563.65it/s]197599it [01:18, 3562.63it/s]197958it [01:18, 3319.19it/s]198337it [01:18, 3448.29it/s]198924it [01:21, 207.79it/s] 199305it [01:21, 292.52it/s]199629it [01:22, 388.31it/s]199997it [01:22, 534.20it/s]200347it [01:22, 705.97it/s]200724it [01:22, 944.70it/s]201100it [01:22, 1226.65it/s]201448it [01:22, 1489.74it/s]201831it [01:22, 1840.45it/s]202183it [01:22, 2103.88it/s]202568it [01:22, 2450.97it/s]202924it [01:23, 2640.55it/s]203303it [01:23, 2911.09it/s]198917it [01:23, 192.07it/s] 203678it [01:23, 3122.31it/s]199297it [01:23, 270.74it/s]204040it [01:23, 3119.63it/s]199613it [01:23, 358.02it/s]204415it [01:23, 3285.72it/s]199980it [01:23, 495.02it/s]204770it [01:23, 3259.86it/s]200346it [01:23, 665.63it/s]205151it [01:23, 3409.76it/s]200722it [01:23, 892.16it/s]205507it [01:23, 3345.87it/s]201098it [01:24, 1163.67it/s]205883it [01:23, 3462.32it/s]201446it [01:24, 1422.16it/s]206237it [01:23, 3392.91it/s]201827it [01:24, 1765.40it/s]206619it [01:24, 3514.14it/s]202178it [01:24, 2031.99it/s]206997it [01:24, 3583.83it/s]202565it [01:24, 2386.97it/s]207359it [01:24, 3467.78it/s]202921it [01:24, 2550.51it/s]207724it [01:24, 3517.49it/s]198919it [01:24, 196.41it/s] 203287it [01:24, 2804.49it/s]199294it [01:24, 276.38it/s]208079it [01:24, 3424.40it/s]203663it [01:24, 3039.81it/s]199609it [01:24, 365.88it/s]208467it [01:24, 3546.15it/s]204020it [01:24, 3076.41it/s]199970it [01:24, 504.05it/s]208824it [01:24, 3451.10it/s]204399it [01:24, 3263.88it/s]200298it [01:24, 663.30it/s]209198it [01:24, 3532.88it/s]204754it [01:25, 3243.37it/s]200618it [01:24, 851.64it/s]209567it [01:24, 3578.05it/s]205121it [01:25, 3358.99it/s]200979it [01:24, 1119.73it/s]209926it [01:25, 3450.74it/s]205472it [01:25, 3313.70it/s]201308it [01:24, 1371.60it/s]210296it [01:25, 3522.08it/s]205853it [01:25, 3451.78it/s]201679it [01:25, 1715.42it/s]210650it [01:25, 3413.34it/s]206227it [01:25, 3392.10it/s]202024it [01:25, 1980.98it/s]211021it [01:25, 3498.24it/s]206608it [01:25, 3509.37it/s]202410it [01:25, 2348.04it/s]211373it [01:25, 3395.39it/s]206986it [01:25, 3585.54it/s]202790it [01:25, 2664.61it/s]211748it [01:25, 3495.69it/s]207349it [01:25, 3469.57it/s]203146it [01:25, 2795.17it/s]198687it [01:25, 169.97it/s] 212107it [01:25, 3392.69it/s]207726it [01:25, 3553.16it/s]203516it [01:25, 3018.19it/s]199054it [01:25, 238.20it/s]212474it [01:25, 3470.89it/s]208084it [01:26, 3335.88it/s]199426it [01:25, 332.53it/s]203868it [01:25, 3040.67it/s]212840it [01:25, 3525.35it/s]208467it [01:26, 3465.31it/s]199737it [01:26, 435.98it/s]204229it [01:25, 3185.84it/s]213194it [01:25, 3409.03it/s]208818it [01:26, 3390.62it/s]200104it [01:26, 599.35it/s]204574it [01:25, 3157.05it/s]213564it [01:26, 3490.82it/s]209201it [01:26, 3515.09it/s]200431it [01:26, 775.62it/s]204945it [01:26, 3308.76it/s]213915it [01:26, 3393.85it/s]209571it [01:26, 3567.61it/s]200804it [01:26, 1033.59it/s]205315it [01:26, 3417.63it/s]214277it [01:26, 3449.26it/s]209930it [01:26, 3437.83it/s]201170it [01:26, 1324.61it/s]205668it [01:26, 3315.95it/s]214627it [01:26, 3346.50it/s]210300it [01:26, 3510.40it/s]201515it [01:26, 1596.37it/s]206045it [01:26, 3442.66it/s]214997it [01:26, 3446.69it/s]210653it [01:26, 3375.71it/s]201886it [01:26, 1936.81it/s]206396it [01:26, 3350.36it/s]215374it [01:26, 3539.81it/s]211024it [01:26, 3469.00it/s]202233it [01:26, 2200.04it/s]206767it [01:26, 3450.31it/s]215730it [01:26, 3418.82it/s]211373it [01:26, 3370.54it/s]202611it [01:26, 2530.57it/s]207116it [01:26, 3349.86it/s]216108it [01:26, 3520.69it/s]211747it [01:27, 3473.51it/s]202963it [01:26, 2720.47it/s]207484it [01:26, 3439.32it/s]216462it [01:26, 3414.10it/s]212107it [01:27, 3371.22it/s]203326it [01:27, 2943.02it/s]207858it [01:26, 3524.97it/s]216843it [01:27, 3525.84it/s]212482it [01:27, 3477.66it/s]203699it [01:27, 3146.95it/s]208213it [01:26, 3366.96it/s]217198it [01:27, 3432.10it/s]212857it [01:27, 3556.22it/s]204057it [01:27, 3132.02it/s]208589it [01:27, 3478.21it/s]217572it [01:27, 3519.03it/s]204422it [01:27, 3271.28it/s]213215it [01:27, 3341.58it/s]208940it [01:27, 3374.02it/s]217953it [01:27, 3601.44it/s]213573it [01:27, 3407.99it/s]204772it [01:27, 3227.89it/s]209313it [01:27, 3473.59it/s]218315it [01:27, 3489.53it/s]205150it [01:27, 3379.95it/s]213917it [01:27, 3332.91it/s]218703it [01:27, 3600.41it/s]209663it [01:27, 3335.93it/s]214285it [01:27, 3431.07it/s]205501it [01:27, 3304.30it/s]210027it [01:27, 3420.32it/s]219065it [01:27, 3491.27it/s]205876it [01:27, 3426.84it/s]214631it [01:27, 3330.87it/s]210394it [01:27, 3490.26it/s]219443it [01:27, 3572.55it/s]215003it [01:28, 3439.54it/s]206226it [01:27, 3348.72it/s]210745it [01:27, 3355.65it/s]219802it [01:27, 3485.10it/s]215381it [01:28, 3537.19it/s]206606it [01:28, 3476.61it/s]211111it [01:27, 3440.38it/s]220188it [01:27, 3591.56it/s]206974it [01:28, 3533.50it/s]215737it [01:28, 3416.03it/s]220549it [01:28, 3517.13it/s]211457it [01:27, 3319.49it/s]216105it [01:28, 3488.86it/s]207331it [01:28, 3442.67it/s]220931it [01:28, 3604.42it/s]211826it [01:28, 3423.41it/s]207696it [01:28, 3493.30it/s]216456it [01:28, 3386.39it/s]221314it [01:28, 3668.65it/s]212171it [01:28, 3314.16it/s]216838it [01:28, 3509.42it/s]208048it [01:28, 3419.61it/s]221682it [01:28, 3580.54it/s]212535it [01:28, 3406.38it/s]208426it [01:28, 3522.53it/s]217191it [01:28, 3420.63it/s]222055it [01:28, 3623.43it/s]212885it [01:28, 3432.81it/s]217573it [01:28, 3534.60it/s]208780it [01:28, 3436.62it/s]222419it [01:28, 3475.77it/s]213230it [01:28, 3313.67it/s]217956it [01:28, 3617.86it/s]209142it [01:28, 3488.40it/s]222788it [01:28, 3535.00it/s]213594it [01:28, 3407.28it/s]209504it [01:28, 3525.85it/s]218320it [01:29, 3414.59it/s]223143it [01:28, 3403.01it/s]213937it [01:28, 3307.08it/s]209858it [01:28, 3386.25it/s]218707it [01:29, 3542.80it/s]223511it [01:28, 3481.59it/s]214302it [01:28, 3404.70it/s]210224it [01:29, 3463.84it/s]219065it [01:29, 3428.22it/s]223867it [01:29, 3388.90it/s]214644it [01:28, 3284.95it/s]210572it [01:29, 3357.92it/s]219452it [01:29, 3553.55it/s]224244it [01:29, 3497.23it/s]215010it [01:28, 3391.84it/s]210944it [01:29, 3461.13it/s]219810it [01:29, 3464.18it/s]224605it [01:29, 3528.44it/s]215383it [01:29, 3487.66it/s]211292it [01:29, 3353.10it/s]220201it [01:29, 3591.25it/s]224960it [01:29, 3408.37it/s]215734it [01:29, 3354.78it/s]211665it [01:29, 3460.53it/s]220563it [01:29, 3518.44it/s]225332it [01:29, 3495.97it/s]216105it [01:29, 3455.62it/s]212032it [01:29, 3521.28it/s]220944it [01:29, 3601.50it/s]225684it [01:29, 3384.22it/s]216453it [01:29, 3337.52it/s]212386it [01:29, 3411.39it/s]221331it [01:29, 3677.61it/s]226046it [01:29, 3450.19it/s]216827it [01:29, 3451.96it/s]212755it [01:29, 3489.44it/s]221700it [01:29, 3571.56it/s]226393it [01:29, 3333.49it/s]217175it [01:29, 3302.04it/s]213106it [01:29, 3394.59it/s]222069it [01:30, 3603.35it/s]226761it [01:29, 3432.40it/s]217549it [01:29, 3423.56it/s]213469it [01:30, 3459.96it/s]222431it [01:30, 3461.46it/s]227123it [01:29, 3484.12it/s]217934it [01:29, 3545.69it/s]213817it [01:30, 3328.24it/s]222807it [01:30, 3546.19it/s]227473it [01:30, 3331.37it/s]218291it [01:29, 3422.06it/s]214169it [01:30, 3381.08it/s]223164it [01:30, 3336.19it/s]227832it [01:30, 3404.14it/s]218673it [01:30, 3533.39it/s]214535it [01:30, 3460.68it/s]223530it [01:30, 3426.72it/s]228175it [01:30, 3301.27it/s]219029it [01:30, 3422.88it/s]214883it [01:30, 3348.77it/s]223876it [01:30, 3351.30it/s]228544it [01:30, 3411.30it/s]219412it [01:30, 3538.58it/s]215255it [01:30, 3454.20it/s]224253it [01:30, 3468.35it/s]228907it [01:30, 3313.73it/s]219768it [01:30, 3431.88it/s]215602it [01:30, 3361.10it/s]224616it [01:30, 3512.95it/s]229265it [01:30, 3387.60it/s]220153it [01:30, 3551.42it/s]215970it [01:30, 3451.79it/s]224969it [01:30, 3398.94it/s]229645it [01:30, 3505.49it/s]220510it [01:30, 3461.23it/s]216317it [01:30, 3361.19it/s]225337it [01:31, 3479.26it/s]220890it [01:30, 3557.90it/s]229998it [01:30, 3383.72it/s]216694it [01:30, 3478.39it/s]225687it [01:31, 3374.35it/s]221267it [01:30, 3617.19it/s]230365it [01:30, 3463.02it/s]217072it [01:31, 3566.23it/s]226051it [01:31, 3448.75it/s]221630it [01:30, 3475.50it/s]230713it [01:31, 3339.00it/s]217430it [01:31, 3437.19it/s]226398it [01:31, 3339.85it/s]222004it [01:30, 3550.68it/s]231089it [01:31, 3457.80it/s]217814it [01:31, 3552.66it/s]226767it [01:31, 3438.69it/s]231437it [01:31, 3352.42it/s]222361it [01:31, 3396.75it/s]218171it [01:31, 3477.13it/s]227127it [01:31, 3484.51it/s]231807it [01:31, 3450.19it/s]222735it [01:31, 3491.80it/s]218542it [01:31, 3542.91it/s]227477it [01:31, 3333.73it/s]232177it [01:31, 3520.10it/s]223087it [01:31, 3356.70it/s]218898it [01:31, 3426.76it/s]227838it [01:31, 3410.83it/s]223451it [01:31, 3435.39it/s]232531it [01:31, 3307.16it/s]219265it [01:31, 3495.50it/s]228181it [01:31, 3230.01it/s]223818it [01:31, 3502.05it/s]232893it [01:31, 3394.54it/s]219636it [01:31, 3555.69it/s]228547it [01:31, 3348.69it/s]224170it [01:31, 3383.56it/s]233236it [01:31, 3319.47it/s]219993it [01:31, 3361.35it/s]228907it [01:32, 3272.91it/s]224534it [01:31, 3456.89it/s]233605it [01:31, 3423.00it/s]220371it [01:32, 3479.07it/s]229275it [01:32, 3387.41it/s]233950it [01:31, 3333.44it/s]224882it [01:31, 3330.41it/s]220722it [01:32, 3403.18it/s]229655it [01:32, 3504.40it/s]234311it [01:32, 3411.47it/s]225248it [01:31, 3424.21it/s]221096it [01:32, 3497.44it/s]230008it [01:32, 3361.28it/s]234677it [01:32, 3482.19it/s]225593it [01:32, 3277.01it/s]221448it [01:32, 3434.63it/s]230373it [01:32, 3442.13it/s]235027it [01:32, 3375.34it/s]225952it [01:32, 3364.02it/s]221832it [01:32, 3551.48it/s]230720it [01:32, 3329.58it/s]235397it [01:32, 3466.82it/s]226310it [01:32, 3426.10it/s]222189it [01:32, 3409.26it/s]231095it [01:32, 3449.01it/s]235746it [01:32, 3349.74it/s]226655it [01:32, 3297.36it/s]222558it [01:32, 3489.21it/s]231442it [01:32, 3344.65it/s]236113it [01:32, 3441.43it/s]227006it [01:32, 3357.58it/s]222923it [01:32, 3533.59it/s]231812it [01:32, 3443.90it/s]236467it [01:32, 3330.94it/s]227344it [01:32, 3241.48it/s]223278it [01:32, 3405.70it/s]232181it [01:33, 3513.27it/s]236835it [01:32, 3428.25it/s]227704it [01:32, 3342.76it/s]223639it [01:32, 3462.13it/s]232534it [01:33, 3391.88it/s]237203it [01:32, 3500.53it/s]228065it [01:32, 3237.30it/s]223987it [01:33, 3313.45it/s]232882it [01:33, 3415.54it/s]237555it [01:33, 3284.51it/s]228429it [01:32, 3350.33it/s]224339it [01:33, 3371.78it/s]233225it [01:33, 3313.36it/s]237918it [01:33, 3379.78it/s]228790it [01:33, 3424.56it/s]224698it [01:33, 3434.09it/s]233597it [01:33, 3420.29it/s]238259it [01:33, 3291.96it/s]229135it [01:33, 3298.36it/s]225043it [01:33, 3326.34it/s]233947it [01:33, 3326.42it/s]238623it [01:33, 3390.00it/s]229497it [01:33, 3387.98it/s]225405it [01:33, 3409.46it/s]234321it [01:33, 3444.24it/s]238987it [01:33, 3295.44it/s]229838it [01:33, 3284.77it/s]225748it [01:33, 3289.47it/s]234686it [01:33, 3502.32it/s]239338it [01:33, 3354.16it/s]230198it [01:33, 3374.60it/s]226108it [01:33, 3376.08it/s]235038it [01:33, 3384.78it/s]239708it [01:33, 3453.06it/s]230559it [01:33, 3441.27it/s]226448it [01:33, 3292.80it/s]235397it [01:33, 3441.13it/s]240056it [01:33, 3342.51it/s]230905it [01:33, 3320.72it/s]226806it [01:33, 3368.66it/s]235743it [01:34, 3322.71it/s]240421it [01:33, 3430.31it/s]231280it [01:33, 3442.98it/s]227167it [01:34, 3433.44it/s]236111it [01:34, 3423.09it/s]240766it [01:33, 3330.33it/s]231627it [01:33, 3316.95it/s]227512it [01:34, 3332.45it/s]236467it [01:34, 3312.64it/s]241137it [01:34, 3438.36it/s]231988it [01:33, 3399.50it/s]227880it [01:34, 3430.41it/s]236836it [01:34, 3417.84it/s]232330it [01:34, 3288.80it/s]241507it [01:34, 3342.10it/s]228225it [01:34, 3331.62it/s]237204it [01:34, 3493.00it/s]232691it [01:34, 3380.43it/s]241880it [01:34, 3450.84it/s]228594it [01:34, 3434.43it/s]237555it [01:34, 3361.63it/s]233059it [01:34, 3464.58it/s]242243it [01:34, 3500.19it/s]228939it [01:34, 3264.90it/s]237908it [01:34, 3409.49it/s]233407it [01:34, 3317.71it/s]242595it [01:34, 3300.92it/s]229304it [01:34, 3373.03it/s]238251it [01:34, 3279.51it/s]233777it [01:34, 3424.72it/s]242963it [01:34, 3405.20it/s]229679it [01:34, 3480.47it/s]238615it [01:34, 3380.11it/s]234122it [01:34, 3309.47it/s]243307it [01:34, 3319.78it/s]230030it [01:34, 3359.65it/s]238981it [01:35, 3459.49it/s]234486it [01:34, 3403.65it/s]243677it [01:34, 3420.50it/s]230395it [01:34, 3441.76it/s]239329it [01:35, 3339.86it/s]234829it [01:34, 3282.89it/s]244027it [01:34, 3329.59it/s]230742it [01:35, 3327.62it/s]239696it [01:35, 3432.25it/s]235195it [01:34, 3389.76it/s]244381it [01:35, 3389.10it/s]231121it [01:35, 3458.80it/s]240041it [01:35, 3322.46it/s]235553it [01:35, 3444.57it/s]244748it [01:35, 3469.76it/s]231469it [01:35, 3347.83it/s]240406it [01:35, 3413.92it/s]235899it [01:35, 3312.81it/s]245097it [01:35, 3355.66it/s]231840it [01:35, 3449.11it/s]240749it [01:35, 3292.28it/s]236260it [01:35, 3395.24it/s]245464it [01:35, 3445.31it/s]232187it [01:35, 3448.69it/s]241121it [01:35, 3412.51it/s]245811it [01:35, 3340.95it/s]236602it [01:35, 3274.91it/s]232534it [01:35, 3356.56it/s]241491it [01:35, 3494.99it/s]246179it [01:35, 3436.45it/s]236964it [01:35, 3372.81it/s]232899it [01:35, 3439.01it/s]241843it [01:35, 3388.31it/s]237305it [01:35, 3226.14it/s]246547it [01:35, 3334.37it/s]233245it [01:35, 3339.68it/s]242208it [01:35, 3461.29it/s]237666it [01:35, 3332.17it/s]246916it [01:35, 3433.32it/s]233614it [01:35, 3438.28it/s]242556it [01:36, 3347.81it/s]238029it [01:35, 3416.51it/s]247276it [01:35, 3480.97it/s]233960it [01:36, 3222.71it/s]242910it [01:36, 3401.36it/s]238373it [01:35, 3282.04it/s]247626it [01:36, 3309.18it/s]234330it [01:36, 3356.37it/s]243252it [01:36, 3306.62it/s]238735it [01:35, 3376.47it/s]247994it [01:36, 3414.11it/s]234694it [01:36, 3435.66it/s]243609it [01:36, 3382.19it/s]239075it [01:36, 3255.49it/s]248338it [01:36, 3314.97it/s]235041it [01:36, 3320.96it/s]243979it [01:36, 3474.49it/s]239434it [01:36, 3349.68it/s]248706it [01:36, 3416.80it/s]235409it [01:36, 3422.47it/s]244328it [01:36, 3355.37it/s]239798it [01:36, 3432.85it/s]249067it [01:36, 3326.10it/s]235754it [01:36, 3304.79it/s]244694it [01:36, 3440.97it/s]240143it [01:36, 3296.29it/s]249417it [01:36, 3371.64it/s]236119it [01:36, 3401.99it/s]245040it [01:36, 3333.03it/s]240503it [01:36, 3380.83it/s]249784it [01:36, 3455.06it/s]236466it [01:36, 3293.38it/s]245405it [01:36, 3422.74it/s]250131it [01:36, 3358.08it/s]240843it [01:36, 3264.47it/s]236834it [01:36, 3402.05it/s]245749it [01:37, 3323.98it/s]250505it [01:36, 3466.54it/s]241195it [01:36, 3336.04it/s]237194it [01:36, 3457.82it/s]246108it [01:37, 3398.37it/s]250854it [01:36, 3367.24it/s]241531it [01:36, 3239.94it/s]237542it [01:37, 3355.45it/s]246475it [01:37, 3476.54it/s]251223it [01:37, 3459.73it/s]241899it [01:36, 3364.20it/s]237900it [01:37, 3419.14it/s]246824it [01:37, 3360.73it/s]242259it [01:37, 3431.86it/s]251587it [01:37, 3344.23it/s]238244it [01:37, 3323.41it/s]247190it [01:37, 3446.52it/s]242604it [01:37, 3303.24it/s]251947it [01:37, 3415.53it/s]238591it [01:37, 3364.18it/s]247537it [01:37, 3248.86it/s]242966it [01:37, 3393.90it/s]252311it [01:37, 3478.39it/s]238947it [01:37, 3418.55it/s]247901it [01:37, 3357.79it/s]243308it [01:37, 3284.93it/s]252661it [01:37, 3348.01it/s]239290it [01:37, 3297.99it/s]248240it [01:37, 3261.12it/s]243675it [01:37, 3385.28it/s]253031it [01:37, 3448.34it/s]239658it [01:37, 3407.30it/s]248607it [01:37, 3376.97it/s]244025it [01:37, 3278.11it/s]253378it [01:37, 3339.70it/s]240001it [01:37, 3296.46it/s]248969it [01:37, 3446.83it/s]244385it [01:37, 3369.38it/s]253747it [01:37, 3437.79it/s]240366it [01:37, 3397.57it/s]249316it [01:38, 3341.80it/s]244748it [01:37, 3442.00it/s]254107it [01:37, 3326.17it/s]240708it [01:38, 3286.62it/s]249681it [01:38, 3429.36it/s]245094it [01:37, 3278.47it/s]254475it [01:38, 3424.47it/s]241078it [01:38, 3404.12it/s]250026it [01:38, 3333.04it/s]254830it [01:38, 3460.05it/s]245454it [01:37, 3367.78it/s]241436it [01:38, 3453.33it/s]250397it [01:38, 3436.03it/s]255178it [01:38, 3348.38it/s]245793it [01:38, 3254.97it/s]241783it [01:38, 3373.73it/s]250747it [01:38, 3344.92it/s]255545it [01:38, 3439.08it/s]246156it [01:38, 3360.19it/s]242136it [01:38, 3418.36it/s]251116it [01:38, 3443.71it/s]246518it [01:38, 3434.38it/s]242479it [01:38, 3331.83it/s]251483it [01:38, 3509.21it/s]242834it [01:38, 3394.82it/s]246864it [01:38, 3310.10it/s]251836it [01:38, 3359.97it/s]247228it [01:38, 3404.17it/s]243186it [01:38, 3268.09it/s]252209it [01:38, 3465.02it/s]247571it [01:38, 3287.19it/s]243548it [01:38, 3366.31it/s]252558it [01:39, 3252.56it/s]247935it [01:38, 3386.39it/s]243913it [01:38, 3446.40it/s]252920it [01:39, 3354.35it/s]244260it [01:39, 3337.88it/s]248276it [01:38, 3265.70it/s]253267it [01:39, 3275.14it/s]244627it [01:39, 3431.21it/s]248639it [01:38, 3367.57it/s]253636it [01:39, 3392.21it/s]248990it [01:39, 3407.64it/s]244972it [01:39, 3309.60it/s]254004it [01:39, 3474.78it/s]245339it [01:39, 3411.87it/s]249333it [01:39, 3287.66it/s]254354it [01:39, 3332.70it/s]245698it [01:39, 3462.31it/s]249694it [01:39, 3378.84it/s]254722it [01:39, 3429.88it/s]246046it [01:39, 3357.67it/s]250034it [01:39, 3268.98it/s]255068it [01:39, 3326.25it/s]246403it [01:39, 3418.27it/s]250399it [01:39, 3376.76it/s]255435it [01:39, 3423.99it/s]246747it [01:39, 3329.99it/s]250745it [01:39, 3277.79it/s]255787it [01:40, 3326.76it/s]247106it [01:39, 3400.43it/s]251110it [01:39, 3381.98it/s]251472it [01:39, 3449.40it/s]247448it [01:40, 3324.18it/s]247807it [01:40, 3398.69it/s]251819it [01:39, 3309.37it/s]248175it [01:40, 3479.47it/s]252187it [01:39, 3413.99it/s]252531it [01:40, 3280.97it/s]248524it [01:40, 3213.15it/s]252875it [01:40, 3324.70it/s]248893it [01:40, 3345.36it/s]253237it [01:40, 3409.26it/s]249232it [01:40, 3259.92it/s]249585it [01:40, 3336.23it/s]253580it [01:40, 3283.74it/s]253945it [01:40, 3387.88it/s]249922it [01:40, 3267.70it/s]250284it [01:40, 3366.86it/s]254286it [01:40, 3266.06it/s]250660it [01:40, 3480.58it/s]254649it [01:40, 3369.26it/s]251010it [01:41, 3353.81it/s]254988it [01:40, 3256.56it/s]251376it [01:41, 3440.10it/s]255350it [01:40, 3358.16it/s]255715it [01:41, 3438.88it/s]251722it [01:41, 3318.84it/s]252095it [01:41, 3435.37it/s]252441it [01:41, 3314.03it/s]252812it [01:41, 3425.21it/s]253174it [01:41, 3479.10it/s]253524it [01:41, 3273.34it/s]253883it [01:41, 3362.48it/s]254223it [01:42, 3258.26it/s]254587it [01:42, 3365.86it/s]254946it [01:42, 3275.49it/s]255317it [01:42, 3395.62it/s]255694it [01:42, 3500.99it/s]255891it [01:45, 158.04it/s] 256262it [01:45, 224.42it/s]256635it [01:45, 315.56it/s]256944it [01:45, 414.85it/s]257315it [01:45, 575.12it/s]257642it [01:46, 728.80it/s]257990it [01:46, 956.32it/s]258366it [01:46, 1252.28it/s]258697it [01:46, 1509.92it/s]259077it [01:46, 1866.81it/s]259418it [01:46, 2101.39it/s]259794it [01:46, 2437.59it/s]260138it [01:46, 2612.59it/s]260511it [01:46, 2879.68it/s]260890it [01:47, 3110.30it/s]261246it [01:47, 3111.63it/s]261622it [01:47, 3284.43it/s]261975it [01:47, 3260.81it/s]256122it [01:47, 147.82it/s] 262347it [01:47, 3386.85it/s]256485it [01:47, 209.58it/s]262699it [01:47, 3220.33it/s]256759it [01:47, 272.08it/s]263058it [01:47, 3321.10it/s]257129it [01:47, 388.95it/s]257497it [01:48, 541.80it/s]263417it [01:47, 3262.89it/s]263794it [01:47, 3404.02it/s]257819it [01:48, 706.51it/s]264173it [01:48, 3511.92it/s]258194it [01:48, 954.16it/s]264529it [01:48, 3411.81it/s]258529it [01:48, 1191.39it/s]264888it [01:48, 3460.08it/s]258907it [01:48, 1523.41it/s]265237it [01:48, 3374.83it/s]259249it [01:48, 1792.67it/s]265615it [01:48, 3488.76it/s]259621it [01:48, 2137.35it/s]259995it [01:48, 2463.92it/s]265966it [01:48, 3383.40it/s]266346it [01:48, 3500.75it/s]260349it [01:48, 2630.69it/s]256061it [01:48, 151.28it/s] 266707it [01:48, 3529.45it/s]260723it [01:49, 2894.02it/s]256427it [01:48, 214.43it/s]267062it [01:48, 3418.42it/s]261075it [01:49, 2950.78it/s]256726it [01:48, 284.02it/s]267435it [01:48, 3507.27it/s]261446it [01:49, 3147.77it/s]257095it [01:48, 401.77it/s]267788it [01:49, 3303.02it/s]261795it [01:49, 3060.01it/s]257454it [01:48, 551.83it/s]268167it [01:49, 3439.69it/s]262164it [01:49, 3226.18it/s]257779it [01:49, 718.50it/s]262537it [01:49, 3364.49it/s]268515it [01:49, 3326.98it/s]258130it [01:49, 948.01it/s]268888it [01:49, 3440.97it/s]262888it [01:49, 3302.59it/s]258459it [01:49, 1183.57it/s]269261it [01:49, 3522.66it/s]263260it [01:49, 3419.37it/s]258831it [01:49, 1511.67it/s]269616it [01:49, 3409.38it/s]263610it [01:49, 3330.95it/s]259200it [01:49, 1850.97it/s]269995it [01:49, 3515.91it/s]263975it [01:49, 3419.19it/s]259546it [01:49, 2094.45it/s]264322it [01:50, 3357.98it/s]270349it [01:49, 3370.86it/s]259914it [01:49, 2413.58it/s]264692it [01:50, 3455.28it/s]270724it [01:49, 3478.14it/s]260258it [01:49, 2580.11it/s]265067it [01:50, 3539.23it/s]271074it [01:50, 3377.09it/s]260619it [01:49, 2824.94it/s]265424it [01:50, 3427.65it/s]271447it [01:50, 3476.01it/s]260961it [01:50, 2899.30it/s]265798it [01:50, 3516.97it/s]271817it [01:50, 3371.82it/s]261331it [01:50, 3106.63it/s]266152it [01:50, 3408.52it/s]272181it [01:50, 3446.42it/s]261701it [01:50, 3265.43it/s]266511it [01:50, 3453.38it/s]272543it [01:50, 3495.19it/s]262052it [01:50, 3214.38it/s]266858it [01:50, 3299.31it/s]272895it [01:50, 3296.22it/s]262399it [01:50, 3283.89it/s]267232it [01:50, 3424.14it/s]273264it [01:50, 3405.50it/s]262740it [01:50, 3218.38it/s]267608it [01:51, 3520.31it/s]273608it [01:50, 3322.21it/s]263108it [01:50, 3348.66it/s]267962it [01:51, 3413.33it/s]273970it [01:50, 3404.90it/s]263450it [01:50, 3250.14it/s]268338it [01:51, 3510.42it/s]274337it [01:51, 3316.51it/s]263822it [01:50, 3381.89it/s]268691it [01:51, 3405.04it/s]274709it [01:51, 3430.11it/s]264189it [01:50, 3462.33it/s]269064it [01:51, 3496.29it/s]275083it [01:51, 3516.47it/s]264539it [01:51, 3342.48it/s]269416it [01:51, 3364.28it/s]275437it [01:51, 3398.44it/s]264906it [01:51, 3434.64it/s]269787it [01:51, 3461.60it/s]275797it [01:51, 3453.36it/s]265253it [01:51, 3324.32it/s]270135it [01:51, 3381.43it/s]276144it [01:51, 3352.25it/s]265625it [01:51, 3436.81it/s]270507it [01:51, 3477.38it/s]276513it [01:51, 3448.90it/s]265971it [01:51, 3317.84it/s]270879it [01:51, 3545.73it/s]276860it [01:51, 3345.69it/s]266344it [01:51, 3433.05it/s]271235it [01:52, 3413.56it/s]277234it [01:51, 3457.48it/s]266696it [01:51, 3455.60it/s]271595it [01:52, 3464.44it/s]277582it [01:51, 3433.58it/s]267044it [01:51, 3337.57it/s]271943it [01:52, 3345.14it/s]277927it [01:52, 3325.90it/s]267412it [01:51, 3434.94it/s]272301it [01:52, 3410.90it/s]256047it [01:52, 120.28it/s] 278292it [01:52, 3418.68it/s]267758it [01:52, 3326.03it/s]256410it [01:52, 169.55it/s]272655it [01:52, 3305.43it/s]278636it [01:52, 3323.01it/s]268132it [01:52, 3442.12it/s]256721it [01:52, 227.35it/s]273024it [01:52, 3412.23it/s]279008it [01:52, 3436.14it/s]268478it [01:52, 3331.55it/s]257085it [01:52, 320.57it/s]273391it [01:52, 3486.23it/s]279377it [01:52, 3328.85it/s]268846it [01:52, 3430.43it/s]257449it [01:52, 445.20it/s]273742it [01:52, 3376.85it/s]279736it [01:52, 3401.44it/s]269211it [01:52, 3493.92it/s]257777it [01:52, 587.44it/s]274114it [01:52, 3474.96it/s]280102it [01:52, 3474.75it/s]269562it [01:52, 3360.93it/s]258149it [01:52, 799.13it/s]274464it [01:53, 3355.68it/s]280452it [01:52, 3357.54it/s]269931it [01:52, 3455.23it/s]258486it [01:52, 1016.97it/s]274821it [01:53, 3416.89it/s]280823it [01:52, 3457.81it/s]270279it [01:52, 3334.65it/s]258861it [01:53, 1319.97it/s]275175it [01:53, 3317.55it/s]281171it [01:53, 3339.85it/s]270645it [01:52, 3426.84it/s]259214it [01:53, 1592.16it/s]275547it [01:53, 3432.11it/s]281528it [01:53, 3405.35it/s]270990it [01:53, 3267.01it/s]259583it [01:53, 1929.51it/s]275915it [01:53, 3501.76it/s]281894it [01:53, 3478.41it/s]271357it [01:53, 3380.66it/s]259947it [01:53, 2248.74it/s]276267it [01:53, 3369.00it/s]282244it [01:53, 3343.15it/s]271721it [01:53, 3454.04it/s]260297it [01:53, 2396.70it/s]276623it [01:53, 3422.27it/s]282600it [01:53, 3403.53it/s]272069it [01:53, 3332.03it/s]260660it [01:53, 2671.15it/s]276967it [01:53, 3309.11it/s]282942it [01:53, 3270.53it/s]272432it [01:53, 3415.21it/s]261000it [01:53, 2785.09it/s]277339it [01:53, 3424.82it/s]283304it [01:53, 3368.32it/s]272776it [01:53, 3292.00it/s]261374it [01:53, 3025.78it/s]277695it [01:53, 3288.82it/s]283643it [01:53, 3275.64it/s]273142it [01:53, 3396.70it/s]261734it [01:53, 3056.56it/s]278065it [01:54, 3403.80it/s]284009it [01:53, 3383.65it/s]273494it [01:53, 3279.09it/s]262105it [01:54, 3229.35it/s]278431it [01:54, 3477.19it/s]284378it [01:53, 3471.50it/s]273861it [01:53, 3389.27it/s]262468it [01:54, 3338.70it/s]278781it [01:54, 3360.73it/s]284727it [01:54, 3316.57it/s]274225it [01:53, 3460.98it/s]262819it [01:54, 3287.67it/s]279150it [01:54, 3454.22it/s]285094it [01:54, 3416.47it/s]274573it [01:54, 3329.62it/s]263182it [01:54, 3383.11it/s]279498it [01:54, 3339.72it/s]285438it [01:54, 3315.36it/s]274936it [01:54, 3414.68it/s]263530it [01:54, 3312.12it/s]279865it [01:54, 3426.08it/s]285806it [01:54, 3418.61it/s]275280it [01:54, 3308.01it/s]263897it [01:54, 3411.40it/s]280215it [01:54, 3290.89it/s]286150it [01:54, 3317.32it/s]275629it [01:54, 3359.31it/s]264254it [01:54, 3339.15it/s]280585it [01:54, 3404.38it/s]286509it [01:54, 3394.70it/s]275993it [01:54, 3438.12it/s]264617it [01:54, 3421.51it/s]280957it [01:54, 3493.12it/s]286880it [01:54, 3484.11it/s]276339it [01:54, 3303.51it/s]264984it [01:54, 3493.03it/s]287112it [01:54, 2501.34it/s]
281309it [01:55, 3287.21it/s]276701it [01:54, 3391.73it/s]265336it [01:54, 3287.93it/s]281671it [01:55, 3378.21it/s]277042it [01:54, 3289.12it/s]265710it [01:55, 3414.66it/s]282012it [01:55, 3286.04it/s]277403it [01:54, 3380.74it/s]266055it [01:55, 3320.08it/s]282380it [01:55, 3396.00it/s]277743it [01:55, 3262.76it/s]266424it [01:55, 3424.09it/s]282735it [01:55, 3270.52it/s]278109it [01:55, 3373.67it/s]266774it [01:55, 3334.82it/s]283103it [01:55, 3385.09it/s]278473it [01:55, 3448.96it/s]267148it [01:55, 3449.20it/s]283467it [01:55, 3456.09it/s]278820it [01:55, 3319.14it/s]267515it [01:55, 3510.69it/s]283815it [01:55, 3346.82it/s]279185it [01:55, 3413.39it/s]267868it [01:55, 3417.96it/s]284184it [01:55, 3442.47it/s]279529it [01:55, 3297.79it/s]268235it [01:55, 3490.54it/s]284531it [01:56, 3337.16it/s]279875it [01:55, 3343.07it/s]268586it [01:55, 3402.37it/s]284901it [01:56, 3438.28it/s]280214it [01:55, 3236.29it/s]268945it [01:56, 3455.22it/s]285255it [01:56, 3331.45it/s]280580it [01:55, 3356.00it/s]269294it [01:56, 3362.53it/s]285613it [01:56, 3401.26it/s]280945it [01:55, 3440.39it/s]269658it [01:56, 3441.90it/s]285981it [01:56, 3479.32it/s]281291it [01:56, 3309.73it/s]270025it [01:56, 3505.84it/s]286331it [01:56, 3279.08it/s]281655it [01:56, 3401.99it/s]270377it [01:56, 3307.84it/s]286701it [01:56, 3396.79it/s]281997it [01:56, 3284.82it/s]270749it [01:56, 3423.99it/s]287044it [01:56, 3320.28it/s]282360it [01:56, 3382.34it/s]287112it [01:56, 2458.71it/s]
271094it [01:56, 3315.18it/s]282719it [01:56, 3442.04it/s]271464it [01:56, 3420.92it/s]283065it [01:56, 3303.12it/s]271814it [01:56, 3302.41it/s]283425it [01:56, 3387.53it/s]272179it [01:56, 3398.19it/s]283766it [01:56, 3274.67it/s]272521it [01:57, 3395.88it/s]284112it [01:56, 3326.07it/s]272863it [01:57, 3278.73it/s]284447it [01:57, 3223.37it/s]273227it [01:57, 3381.45it/s]284812it [01:57, 3344.37it/s]273567it [01:57, 3313.70it/s]285176it [01:57, 3428.46it/s]273940it [01:57, 3432.75it/s]285521it [01:57, 3306.63it/s]274299it [01:57, 3476.62it/s]285884it [01:57, 3390.85it/s]274648it [01:57, 3378.48it/s]286225it [01:57, 3284.31it/s]275012it [01:57, 3452.67it/s]286591it [01:57, 3389.83it/s]275359it [01:57, 3357.71it/s]286934it [01:57, 3290.62it/s]275729it [01:58, 3454.24it/s]287112it [01:57, 2437.01it/s]
2022-07-12 14:41:32 | INFO | root | success load 287112 data
2022-07-12 14:41:32 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-12 14:41:32 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-12 14:41:32 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-12 14:41:32 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-12 14:41:32 | INFO | transformer.tokenization_utils | loading file None
2022-07-12 14:41:32 | INFO | transformer.tokenization_utils | loading file None
2022-07-12 14:41:32 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
276076it [01:58, 3358.66it/s]276444it [01:58, 3450.30it/s]276815it [01:58, 3524.16it/s]277169it [01:58, 3412.75it/s]277539it [01:58, 3494.87it/s]277890it [01:58, 3390.51it/s]278249it [01:58, 3446.16it/s]278595it [01:58, 3267.54it/s]278957it [01:58, 3365.76it/s]279324it [01:59, 3451.16it/s]279672it [01:59, 3357.66it/s]280018it [01:59, 3385.56it/s]280358it [01:59, 3302.83it/s]280735it [01:59, 3435.61it/s]281080it [01:59, 3293.76it/s]281451it [01:59, 3412.41it/s]281816it [01:59, 3480.17it/s]282166it [01:59, 3378.04it/s]282532it [02:00, 3458.88it/s]282880it [02:00, 3328.22it/s]283244it [02:00, 3414.97it/s]283588it [02:00, 3326.21it/s]283959it [02:00, 3434.96it/s]284318it [02:00, 3479.61it/s]284668it [02:00, 3373.11it/s]285025it [02:00, 3428.28it/s]285370it [02:00, 3325.36it/s]285728it [02:00, 3398.07it/s]286095it [02:01, 3326.50it/s]286468it [02:01, 3439.44it/s]286843it [02:01, 3528.01it/s]287112it [02:01, 2365.45it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-12 14:46:30 | INFO | train_inner | epoch 001:    100 / 1122 loss=nan, nll_loss=11.979, mask_ins=7.567, word_ins_ml=12.473, word_reposition=6.323, kpe=nan, ppl=nan, wps=7064.9, ups=0.34, wpb=20527, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=24.207, clip=19, loss_scale=128, train_wall=256, wall=416
2022-07-12 14:51:21 | INFO | train_inner | epoch 001:    200 / 1122 loss=23.168, nll_loss=10.924, mask_ins=4.722, word_ins_ml=11.536, word_reposition=5.517, kpe=1.393, ppl=9.42412e+06, wps=7081.6, ups=0.34, wpb=20583.2, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=18.425, clip=0, loss_scale=128, train_wall=251, wall=707
2022-07-12 14:56:10 | INFO | train_inner | epoch 001:    300 / 1122 loss=17.937, nll_loss=10.649, mask_ins=2.656, word_ins_ml=11.288, word_reposition=2.73, kpe=1.263, ppl=251000, wps=7117.5, ups=0.35, wpb=20561.3, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=6.845, clip=0, loss_scale=128, train_wall=250, wall=995
2022-07-12 15:01:00 | INFO | train_inner | epoch 001:    400 / 1122 loss=16.263, nll_loss=10.243, mask_ins=2.405, word_ins_ml=10.93, word_reposition=1.728, kpe=1.2, ppl=78667.9, wps=7099.7, ups=0.35, wpb=20576.5, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=3.498, clip=0, loss_scale=128, train_wall=251, wall=1285
2022-07-12 15:05:50 | INFO | train_inner | epoch 001:    500 / 1122 loss=15.677, nll_loss=9.932, mask_ins=2.313, word_ins_ml=10.661, word_reposition=1.549, kpe=1.154, ppl=52391.2, wps=7071.4, ups=0.34, wpb=20523.5, bsz=256, num_updates=500, lr=5.009e-05, gnorm=3.216, clip=0, loss_scale=128, train_wall=251, wall=1575
2022-07-12 15:10:52 | INFO | train_inner | epoch 001:    600 / 1122 loss=15.341, nll_loss=9.669, mask_ins=2.295, word_ins_ml=10.436, word_reposition=1.488, kpe=1.121, ppl=41511.8, wps=6784.5, ups=0.33, wpb=20491.4, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=3.009, clip=0, loss_scale=242, train_wall=263, wall=1878
2022-07-12 15:15:42 | INFO | train_inner | epoch 001:    700 / 1122 loss=15.112, nll_loss=9.45, mask_ins=2.258, word_ins_ml=10.251, word_reposition=1.506, kpe=1.098, ppl=35418.3, wps=7073.2, ups=0.34, wpb=20542.5, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=2.844, clip=0, loss_scale=256, train_wall=251, wall=2168
2022-07-12 15:20:32 | INFO | train_inner | epoch 001:    800 / 1122 loss=14.823, nll_loss=9.185, mask_ins=2.199, word_ins_ml=10.024, word_reposition=1.517, kpe=1.083, ppl=28989, wps=7094.4, ups=0.34, wpb=20579, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=2.668, clip=0, loss_scale=256, train_wall=251, wall=2458
2022-07-12 15:25:21 | INFO | train_inner | epoch 001:    900 / 1122 loss=14.58, nll_loss=8.963, mask_ins=2.168, word_ins_ml=9.834, word_reposition=1.515, kpe=1.062, ppl=24487.2, wps=7094.3, ups=0.35, wpb=20464, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=2.6, clip=0, loss_scale=256, train_wall=250, wall=2746
2022-07-12 15:30:10 | INFO | train_inner | epoch 001:   1000 / 1122 loss=14.406, nll_loss=8.798, mask_ins=2.149, word_ins_ml=9.692, word_reposition=1.511, kpe=1.054, ppl=21704.3, wps=7123.7, ups=0.35, wpb=20597.8, bsz=256, num_updates=1000, lr=0.00010008, gnorm=2.336, clip=0, loss_scale=256, train_wall=250, wall=3036
2022-07-12 15:35:00 | INFO | train_inner | epoch 001:   1100 / 1122 loss=14.25, nll_loss=8.641, mask_ins=2.136, word_ins_ml=9.557, word_reposition=1.518, kpe=1.039, ppl=19484.6, wps=7057.8, ups=0.34, wpb=20473.7, bsz=256, num_updates=1100, lr=0.000110078, gnorm=2.381, clip=0, loss_scale=453, train_wall=251, wall=3326
2022-07-12 15:36:02 | INFO | train | epoch 001 | loss nan | nll_loss 9.833 | mask_ins 2.972 | word_ins_ml 10.586 | word_reposition 2.428 | kpe nan | ppl nan | wps 7056.2 | ups 0.34 | wpb 20520.3 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 6.471 | clip 1.7 | loss_scale 220 | train_wall 2830 | wall 3388
2022-07-12 15:37:23 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 15.168 | nll_loss 9.021 | mask_ins 2.455 | word_ins_ml 9.942 | word_reposition 1.464 | kpe 1.307 | ppl 36814.1 | wps 12354.9 | wpb 2367.6 | bsz 32 | num_updates 1122
2022-07-12 15:37:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_best.pt (epoch 1 @ 1122 updates, score 15.168) (writing took 5.10138382948935 seconds)
2022-07-12 15:41:13 | INFO | train_inner | epoch 002:     78 / 1122 loss=14.108, nll_loss=8.521, mask_ins=2.131, word_ins_ml=9.454, word_reposition=1.49, kpe=1.032, ppl=17655.5, wps=5457.3, ups=0.27, wpb=20333.3, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=2.397, clip=0, loss_scale=512, train_wall=249, wall=3698
2022-07-12 15:46:02 | INFO | train_inner | epoch 002:    178 / 1122 loss=13.951, nll_loss=8.363, mask_ins=2.121, word_ins_ml=9.317, word_reposition=1.495, kpe=1.019, ppl=15841.8, wps=7104.7, ups=0.35, wpb=20587.3, bsz=256, num_updates=1300, lr=0.000130074, gnorm=2.163, clip=0, loss_scale=512, train_wall=251, wall=3988
2022-07-12 15:50:52 | INFO | train_inner | epoch 002:    278 / 1122 loss=13.78, nll_loss=8.213, mask_ins=2.097, word_ins_ml=9.185, word_reposition=1.486, kpe=1.011, ppl=14068.6, wps=7120.4, ups=0.35, wpb=20599.8, bsz=256, num_updates=1400, lr=0.000140072, gnorm=2.159, clip=0, loss_scale=512, train_wall=250, wall=4277
2022-07-12 15:55:42 | INFO | train_inner | epoch 002:    378 / 1122 loss=13.611, nll_loss=8.042, mask_ins=2.093, word_ins_ml=9.036, word_reposition=1.474, kpe=1.008, ppl=12507.7, wps=7005.6, ups=0.34, wpb=20347.3, bsz=256, num_updates=1500, lr=0.00015007, gnorm=2.182, clip=0, loss_scale=512, train_wall=251, wall=4568
2022-07-12 16:00:32 | INFO | train_inner | epoch 002:    478 / 1122 loss=13.387, nll_loss=7.825, mask_ins=2.074, word_ins_ml=8.847, word_reposition=1.465, kpe=1.003, ppl=10715.4, wps=7094.1, ups=0.34, wpb=20567.7, bsz=256, num_updates=1600, lr=0.000160068, gnorm=2.139, clip=0, loss_scale=845, train_wall=251, wall=4858
2022-07-12 16:05:22 | INFO | train_inner | epoch 002:    578 / 1122 loss=13.195, nll_loss=7.583, mask_ins=2.08, word_ins_ml=8.635, word_reposition=1.471, kpe=1.009, ppl=9377.96, wps=7093.4, ups=0.35, wpb=20536.9, bsz=256, num_updates=1700, lr=0.000170066, gnorm=2.356, clip=0, loss_scale=1024, train_wall=251, wall=5147
2022-07-12 16:10:12 | INFO | train_inner | epoch 002:    678 / 1122 loss=12.937, nll_loss=7.282, mask_ins=2.083, word_ins_ml=8.373, word_reposition=1.472, kpe=1.01, ppl=7842.06, wps=7053.4, ups=0.34, wpb=20477.4, bsz=256, num_updates=1800, lr=0.000180064, gnorm=2.472, clip=0, loss_scale=1024, train_wall=252, wall=5438
2022-07-12 16:15:25 | INFO | train_inner | epoch 002:    778 / 1122 loss=nan, nll_loss=6.954, mask_ins=2.056, word_ins_ml=8.087, word_reposition=1.445, kpe=nan, ppl=nan, wps=6565.2, ups=0.32, wpb=20576, bsz=256, num_updates=1900, lr=0.000190062, gnorm=2.591, clip=0, loss_scale=1024, train_wall=275, wall=5751
2022-07-12 16:20:16 | INFO | train_inner | epoch 002:    878 / 1122 loss=12.284, nll_loss=6.635, mask_ins=2.05, word_ins_ml=7.811, word_reposition=1.412, kpe=1.01, ppl=4986.15, wps=7030.4, ups=0.34, wpb=20447.7, bsz=256, num_updates=2000, lr=0.00020006, gnorm=2.677, clip=0, loss_scale=1024, train_wall=252, wall=6042
2022-07-12 16:25:06 | INFO | train_inner | epoch 002:    978 / 1122 loss=12.034, nll_loss=6.38, mask_ins=2.049, word_ins_ml=7.59, word_reposition=1.388, kpe=1.006, ppl=4192.51, wps=7089, ups=0.35, wpb=20513.5, bsz=256, num_updates=2100, lr=0.000210058, gnorm=2.635, clip=0, loss_scale=1567, train_wall=251, wall=6331
2022-07-12 16:29:56 | INFO | train_inner | epoch 002:   1078 / 1122 loss=nan, nll_loss=6.145, mask_ins=2.055, word_ins_ml=7.387, word_reposition=1.364, kpe=nan, ppl=nan, wps=7137.5, ups=0.34, wpb=20708.1, bsz=256, num_updates=2200, lr=0.000220056, gnorm=2.622, clip=0, loss_scale=2048, train_wall=252, wall=6621
2022-07-12 16:32:02 | INFO | train | epoch 002 | loss nan | nll_loss 7.372 | mask_ins 2.079 | word_ins_ml 8.453 | word_reposition 1.447 | kpe nan | ppl nan | wps 6853 | ups 0.33 | wpb 20521 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 2.411 | clip 0 | loss_scale 1015 | train_wall 2840 | wall 6748
2022-07-12 16:33:22 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 14.285 | nll_loss 7.997 | mask_ins 2.352 | word_ins_ml 9.139 | word_reposition 1.374 | kpe 1.42 | ppl 19957.9 | wps 12359.9 | wpb 2367.6 | bsz 32 | num_updates 2244 | best_loss 14.285
2022-07-12 16:33:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_best.pt (epoch 2 @ 2244 updates, score 14.285) (writing took 6.663931120187044 seconds)
2022-07-12 16:36:11 | INFO | train_inner | epoch 003:     56 / 1122 loss=nan, nll_loss=5.984, mask_ins=2.058, word_ins_ml=7.249, word_reposition=1.343, kpe=nan, ppl=nan, wps=5436.6, ups=0.27, wpb=20387.7, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=2.729, clip=0, loss_scale=2048, train_wall=250, wall=6996
2022-07-12 16:41:00 | INFO | train_inner | epoch 003:    156 / 1122 loss=11.388, nll_loss=5.747, mask_ins=2.039, word_ins_ml=7.043, word_reposition=1.312, kpe=0.994, ppl=2679.16, wps=7080.8, ups=0.35, wpb=20466.9, bsz=256, num_updates=2400, lr=0.000240052, gnorm=2.487, clip=0, loss_scale=2048, train_wall=250, wall=7286
2022-07-12 16:45:50 | INFO | train_inner | epoch 003:    256 / 1122 loss=11.207, nll_loss=5.597, mask_ins=2.022, word_ins_ml=6.912, word_reposition=1.277, kpe=0.996, ppl=2363.67, wps=7105.7, ups=0.35, wpb=20590.4, bsz=256, num_updates=2500, lr=0.00025005, gnorm=2.429, clip=0, loss_scale=2048, train_wall=251, wall=7575
2022-07-12 16:50:39 | INFO | train_inner | epoch 003:    356 / 1122 loss=11.031, nll_loss=5.406, mask_ins=2.035, word_ins_ml=6.746, word_reposition=1.259, kpe=0.991, ppl=2092.62, wps=7104.4, ups=0.35, wpb=20552.9, bsz=256, num_updates=2600, lr=0.000260048, gnorm=2.456, clip=0, loss_scale=2888, train_wall=250, wall=7865
2022-07-12 16:55:28 | INFO | train_inner | epoch 003:    456 / 1122 loss=10.875, nll_loss=5.264, mask_ins=2.017, word_ins_ml=6.621, word_reposition=1.245, kpe=0.992, ppl=1877.72, wps=7057.8, ups=0.35, wpb=20384, bsz=256, num_updates=2700, lr=0.000270046, gnorm=2.316, clip=0, loss_scale=4096, train_wall=250, wall=8153
2022-07-12 17:00:17 | INFO | train_inner | epoch 003:    556 / 1122 loss=10.746, nll_loss=5.141, mask_ins=2.018, word_ins_ml=6.513, word_reposition=1.224, kpe=0.991, ppl=1717.88, wps=7073.6, ups=0.35, wpb=20480.9, bsz=256, num_updates=2800, lr=0.000280044, gnorm=2.29, clip=0, loss_scale=4096, train_wall=251, wall=8443
2022-07-12 17:05:07 | INFO | train_inner | epoch 003:    656 / 1122 loss=10.64, nll_loss=5.072, mask_ins=2.009, word_ins_ml=6.453, word_reposition=1.187, kpe=0.992, ppl=1596.02, wps=7124.5, ups=0.35, wpb=20612.3, bsz=256, num_updates=2900, lr=0.000290042, gnorm=2.228, clip=0, loss_scale=4096, train_wall=251, wall=8732
2022-07-12 17:09:56 | INFO | train_inner | epoch 003:    756 / 1122 loss=10.449, nll_loss=4.889, mask_ins=1.996, word_ins_ml=6.293, word_reposition=1.168, kpe=0.992, ppl=1397.44, wps=7117.1, ups=0.35, wpb=20597.8, bsz=256, num_updates=3000, lr=0.00030004, gnorm=2.188, clip=0, loss_scale=4096, train_wall=251, wall=9022
2022-07-12 17:14:46 | INFO | train_inner | epoch 003:    856 / 1122 loss=10.069, nll_loss=4.584, mask_ins=1.915, word_ins_ml=6.023, word_reposition=1.14, kpe=0.99, ppl=1073.81, wps=7114, ups=0.35, wpb=20609.8, bsz=256, num_updates=3100, lr=0.000310038, gnorm=2.416, clip=0, loss_scale=5284, train_wall=251, wall=9311
2022-07-12 17:19:59 | INFO | train_inner | epoch 003:    956 / 1122 loss=nan, nll_loss=4.096, mask_ins=1.763, word_ins_ml=5.593, word_reposition=1.054, kpe=nan, ppl=nan, wps=6559.4, ups=0.32, wpb=20572.9, bsz=256, num_updates=3200, lr=0.000320036, gnorm=2.402, clip=0, loss_scale=8192, train_wall=275, wall=9625
2022-07-12 17:24:50 | INFO | train_inner | epoch 003:   1056 / 1122 loss=9.163, nll_loss=3.95, mask_ins=1.689, word_ins_ml=5.463, word_reposition=1.019, kpe=0.992, ppl=573.42, wps=7063.3, ups=0.34, wpb=20512.4, bsz=256, num_updates=3300, lr=0.000330034, gnorm=2.372, clip=0, loss_scale=8192, train_wall=251, wall=9915
2022-07-12 17:27:59 | INFO | train | epoch 003 | loss nan | nll_loss 4.956 | mask_ins 1.936 | word_ins_ml 6.35 | word_reposition 1.185 | kpe nan | ppl nan | wps 6859.1 | ups 0.33 | wpb 20521.3 | bsz 255.8 | num_updates 3366 | lr 0.000336633 | gnorm 2.377 | clip 0 | loss_scale 4598 | train_wall 2836 | wall 10105
2022-07-12 17:29:19 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 13.97 | nll_loss 7.497 | mask_ins 2.263 | word_ins_ml 8.732 | word_reposition 1.597 | kpe 1.377 | ppl 16046.6 | wps 12341.4 | wpb 2367.6 | bsz 32 | num_updates 3366 | best_loss 13.97
2022-07-12 17:29:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_best.pt (epoch 3 @ 3366 updates, score 13.97) (writing took 6.802363782189786 seconds)
2022-07-12 17:31:05 | INFO | train_inner | epoch 004:     34 / 1122 loss=8.954, nll_loss=3.821, mask_ins=1.608, word_ins_ml=5.348, word_reposition=1.008, kpe=0.99, ppl=496.04, wps=5430.4, ups=0.27, wpb=20354.1, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=2.402, clip=0, loss_scale=8192, train_wall=249, wall=10290
2022-07-12 17:34:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 17:35:57 | INFO | train_inner | epoch 004:    135 / 1122 loss=8.821, nll_loss=3.723, mask_ins=1.582, word_ins_ml=5.261, word_reposition=0.998, kpe=0.981, ppl=452.4, wps=7018.5, ups=0.34, wpb=20490.4, bsz=256, num_updates=3500, lr=0.00035003, gnorm=2.213, clip=0, loss_scale=6570, train_wall=253, wall=10582
2022-07-12 17:40:46 | INFO | train_inner | epoch 004:    235 / 1122 loss=8.709, nll_loss=3.66, mask_ins=1.537, word_ins_ml=5.204, word_reposition=0.983, kpe=0.986, ppl=418.54, wps=7107.1, ups=0.34, wpb=20607.3, bsz=256, num_updates=3600, lr=0.000360028, gnorm=2.189, clip=0, loss_scale=4096, train_wall=251, wall=10872
2022-07-12 17:45:35 | INFO | train_inner | epoch 004:    335 / 1122 loss=8.617, nll_loss=3.602, mask_ins=1.519, word_ins_ml=5.152, word_reposition=0.963, kpe=0.983, ppl=392.49, wps=7092.6, ups=0.35, wpb=20432.2, bsz=256, num_updates=3700, lr=0.000370026, gnorm=2.16, clip=0, loss_scale=4096, train_wall=249, wall=11160
2022-07-12 17:50:24 | INFO | train_inner | epoch 004:    435 / 1122 loss=8.483, nll_loss=3.521, mask_ins=1.486, word_ins_ml=5.079, word_reposition=0.939, kpe=0.979, ppl=357.79, wps=7133.2, ups=0.35, wpb=20656.5, bsz=256, num_updates=3800, lr=0.000380024, gnorm=2.135, clip=0, loss_scale=4096, train_wall=251, wall=11450
2022-07-12 17:55:13 | INFO | train_inner | epoch 004:    535 / 1122 loss=8.467, nll_loss=3.513, mask_ins=1.466, word_ins_ml=5.07, word_reposition=0.945, kpe=0.986, ppl=353.95, wps=7084, ups=0.35, wpb=20481.9, bsz=256, num_updates=3900, lr=0.000390022, gnorm=2.132, clip=0, loss_scale=4096, train_wall=251, wall=11739
2022-07-12 18:00:03 | INFO | train_inner | epoch 004:    635 / 1122 loss=nan, nll_loss=3.515, mask_ins=1.455, word_ins_ml=5.071, word_reposition=0.93, kpe=nan, ppl=nan, wps=7085.6, ups=0.35, wpb=20519.3, bsz=256, num_updates=4000, lr=0.00040002, gnorm=2.086, clip=0, loss_scale=5243, train_wall=251, wall=12029
2022-07-12 18:04:52 | INFO | train_inner | epoch 004:    735 / 1122 loss=8.37, nll_loss=3.475, mask_ins=1.434, word_ins_ml=5.035, word_reposition=0.917, kpe=0.984, ppl=330.95, wps=7119.9, ups=0.35, wpb=20609.6, bsz=256, num_updates=4100, lr=0.000410018, gnorm=2.071, clip=0, loss_scale=8192, train_wall=251, wall=12318
2022-07-12 18:09:42 | INFO | train_inner | epoch 004:    835 / 1122 loss=nan, nll_loss=3.43, mask_ins=1.419, word_ins_ml=4.994, word_reposition=0.916, kpe=nan, ppl=nan, wps=7061.5, ups=0.35, wpb=20446.8, bsz=256, num_updates=4200, lr=0.000420016, gnorm=1.966, clip=0, loss_scale=8192, train_wall=251, wall=12608
2022-07-12 18:14:32 | INFO | train_inner | epoch 004:    935 / 1122 loss=8.297, nll_loss=3.411, mask_ins=1.415, word_ins_ml=4.976, word_reposition=0.922, kpe=0.983, ppl=314.47, wps=7110.4, ups=0.34, wpb=20633.8, bsz=256, num_updates=4300, lr=0.000430014, gnorm=1.978, clip=0, loss_scale=8192, train_wall=251, wall=12898
2022-07-12 18:19:22 | INFO | train_inner | epoch 004:   1035 / 1122 loss=8.272, nll_loss=3.423, mask_ins=1.401, word_ins_ml=4.986, word_reposition=0.904, kpe=0.982, ppl=309.17, wps=7064.6, ups=0.35, wpb=20465.7, bsz=256, num_updates=4400, lr=0.000440012, gnorm=1.929, clip=0, loss_scale=8192, train_wall=251, wall=13188
2022-07-12 18:23:57 | INFO | train | epoch 004 | loss nan | nll_loss 3.525 | mask_ins 1.469 | word_ins_ml 5.08 | word_reposition 0.941 | kpe nan | ppl nan | wps 6850.1 | ups 0.33 | wpb 20519.3 | bsz 255.8 | num_updates 4487 | lr 0.00044871 | gnorm 2.089 | clip 0 | loss_scale 6345 | train_wall 2837 | wall 13463
2022-07-12 18:25:17 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 14.384 | nll_loss 7.485 | mask_ins 2.575 | word_ins_ml 8.733 | word_reposition 1.534 | kpe 1.542 | ppl 21381 | wps 12408.6 | wpb 2367.6 | bsz 32 | num_updates 4487 | best_loss 13.97
2022-07-12 18:25:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 4 @ 4487 updates, score 14.384) (writing took 3.5640958985313773 seconds)
2022-07-12 18:25:58 | INFO | train_inner | epoch 005:     13 / 1122 loss=8.246, nll_loss=3.381, mask_ins=1.397, word_ins_ml=4.948, word_reposition=0.901, kpe=1, ppl=303.5, wps=5143.1, ups=0.25, wpb=20374.8, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=2.079, clip=0, loss_scale=9503, train_wall=274, wall=13584
2022-07-12 18:30:48 | INFO | train_inner | epoch 005:    113 / 1122 loss=8.164, nll_loss=3.327, mask_ins=1.389, word_ins_ml=4.9, word_reposition=0.898, kpe=0.976, ppl=286.78, wps=7124.6, ups=0.35, wpb=20631.6, bsz=256, num_updates=4600, lr=0.000460008, gnorm=1.858, clip=0, loss_scale=16384, train_wall=251, wall=13873
2022-07-12 18:35:38 | INFO | train_inner | epoch 005:    213 / 1122 loss=8.109, nll_loss=3.296, mask_ins=1.364, word_ins_ml=4.871, word_reposition=0.896, kpe=0.978, ppl=276.05, wps=7073.9, ups=0.34, wpb=20520.3, bsz=256, num_updates=4700, lr=0.000470006, gnorm=1.924, clip=0, loss_scale=16384, train_wall=251, wall=14163
2022-07-12 18:35:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 18:40:29 | INFO | train_inner | epoch 005:    314 / 1122 loss=8.096, nll_loss=3.29, mask_ins=1.37, word_ins_ml=4.865, word_reposition=0.882, kpe=0.979, ppl=273.64, wps=7053.7, ups=0.34, wpb=20577.1, bsz=256, num_updates=4800, lr=0.000480004, gnorm=1.841, clip=0, loss_scale=8516, train_wall=253, wall=14455
2022-07-12 18:45:19 | INFO | train_inner | epoch 005:    414 / 1122 loss=nan, nll_loss=3.3, mask_ins=1.366, word_ins_ml=4.873, word_reposition=0.892, kpe=nan, ppl=nan, wps=7074.3, ups=0.35, wpb=20499.2, bsz=256, num_updates=4900, lr=0.000490002, gnorm=1.882, clip=0, loss_scale=8192, train_wall=251, wall=14745
2022-07-12 18:50:08 | INFO | train_inner | epoch 005:    514 / 1122 loss=8.041, nll_loss=3.278, mask_ins=1.346, word_ins_ml=4.853, word_reposition=0.87, kpe=0.972, ppl=263.31, wps=7043.1, ups=0.35, wpb=20352.1, bsz=256, num_updates=5000, lr=0.0005, gnorm=1.848, clip=0, loss_scale=8192, train_wall=250, wall=15034
2022-07-12 18:51:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 18:55:01 | INFO | train_inner | epoch 005:    615 / 1122 loss=8.053, nll_loss=3.286, mask_ins=1.342, word_ins_ml=4.859, word_reposition=0.875, kpe=0.978, ppl=265.61, wps=7004.8, ups=0.34, wpb=20513.2, bsz=256, num_updates=5100, lr=0.000495074, gnorm=1.847, clip=0, loss_scale=4867, train_wall=254, wall=15327
2022-07-12 18:58:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-12 18:59:53 | INFO | train_inner | epoch 005:    716 / 1122 loss=8.014, nll_loss=3.237, mask_ins=1.338, word_ins_ml=4.815, word_reposition=0.881, kpe=0.98, ppl=258.46, wps=7034.9, ups=0.34, wpb=20533, bsz=256, num_updates=5200, lr=0.00049029, gnorm=1.914, clip=0, loss_scale=3346, train_wall=253, wall=15619
2022-07-12 19:04:42 | INFO | train_inner | epoch 005:    816 / 1122 loss=8.024, nll_loss=3.259, mask_ins=1.337, word_ins_ml=4.833, word_reposition=0.882, kpe=0.973, ppl=260.31, wps=7136.6, ups=0.35, wpb=20655.4, bsz=256, num_updates=5300, lr=0.000485643, gnorm=1.793, clip=0, loss_scale=2048, train_wall=251, wall=15908
2022-07-12 19:09:31 | INFO | train_inner | epoch 005:    916 / 1122 loss=7.969, nll_loss=3.222, mask_ins=1.329, word_ins_ml=4.8, word_reposition=0.863, kpe=0.977, ppl=250.49, wps=7120.3, ups=0.35, wpb=20560, bsz=256, num_updates=5400, lr=0.000481125, gnorm=1.802, clip=0, loss_scale=2048, train_wall=250, wall=16197
2022-07-12 19:14:20 | INFO | train_inner | epoch 005:   1016 / 1122 loss=nan, nll_loss=3.165, mask_ins=1.305, word_ins_ml=4.749, word_reposition=0.864, kpe=nan, ppl=nan, wps=7073.8, ups=0.35, wpb=20463.8, bsz=256, num_updates=5500, lr=0.000476731, gnorm=1.699, clip=0, loss_scale=2048, train_wall=251, wall=16486
2022-07-12 19:19:09 | INFO | train_inner | epoch 005:   1116 / 1122 loss=7.893, nll_loss=3.17, mask_ins=1.312, word_ins_ml=4.752, word_reposition=0.861, kpe=0.968, ppl=237.67, wps=7118.3, ups=0.35, wpb=20571.6, bsz=256, num_updates=5600, lr=0.000472456, gnorm=1.72, clip=0, loss_scale=2048, train_wall=251, wall=16775
2022-07-12 19:19:26 | INFO | train | epoch 005 | loss nan | nll_loss 3.26 | mask_ins 1.346 | word_ins_ml 4.836 | word_reposition 0.879 | kpe nan | ppl nan | wps 6899 | ups 0.34 | wpb 20521.6 | bsz 255.8 | num_updates 5606 | lr 0.000472203 | gnorm 1.842 | clip 0 | loss_scale 6818 | train_wall 2812 | wall 16791
2022-07-12 19:20:46 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 13.267 | nll_loss 6.926 | mask_ins 2.134 | word_ins_ml 8.219 | word_reposition 1.491 | kpe 1.423 | ppl 9855.15 | wps 12393.5 | wpb 2367.6 | bsz 32 | num_updates 5606 | best_loss 13.267
2022-07-12 19:20:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_best.pt (epoch 5 @ 5606 updates, score 13.267) (writing took 6.434354117140174 seconds)
2022-07-12 19:25:24 | INFO | train_inner | epoch 006:     94 / 1122 loss=7.822, nll_loss=3.135, mask_ins=1.307, word_ins_ml=4.721, word_reposition=0.835, kpe=0.959, ppl=226.31, wps=5429.5, ups=0.27, wpb=20344.4, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=1.756, clip=0, loss_scale=2560, train_wall=250, wall=17150
2022-07-12 19:30:40 | INFO | train_inner | epoch 006:    194 / 1122 loss=7.786, nll_loss=3.108, mask_ins=1.285, word_ins_ml=4.696, word_reposition=0.843, kpe=0.961, ppl=220.73, wps=6525, ups=0.32, wpb=20599.6, bsz=256, num_updates=5800, lr=0.000464238, gnorm=1.648, clip=0, loss_scale=4096, train_wall=277, wall=17465
2022-07-12 19:35:30 | INFO | train_inner | epoch 006:    294 / 1122 loss=nan, nll_loss=3.065, mask_ins=1.295, word_ins_ml=4.657, word_reposition=0.85, kpe=nan, ppl=nan, wps=7093.8, ups=0.34, wpb=20599.2, bsz=256, num_updates=5900, lr=0.000460287, gnorm=1.654, clip=0, loss_scale=4096, train_wall=252, wall=17756
2022-07-12 19:40:20 | INFO | train_inner | epoch 006:    394 / 1122 loss=7.704, nll_loss=3.059, mask_ins=1.274, word_ins_ml=4.651, word_reposition=0.82, kpe=0.959, ppl=208.55, wps=7092.3, ups=0.34, wpb=20561, bsz=256, num_updates=6000, lr=0.000456435, gnorm=1.669, clip=0, loss_scale=4096, train_wall=251, wall=18046
2022-07-12 19:45:11 | INFO | train_inner | epoch 006:    494 / 1122 loss=7.72, nll_loss=3.066, mask_ins=1.272, word_ins_ml=4.656, word_reposition=0.835, kpe=0.956, ppl=210.77, wps=7060.5, ups=0.34, wpb=20512.8, bsz=256, num_updates=6100, lr=0.000452679, gnorm=1.62, clip=0, loss_scale=4096, train_wall=252, wall=18336
2022-07-12 19:50:01 | INFO | train_inner | epoch 006:    594 / 1122 loss=7.704, nll_loss=3.062, mask_ins=1.265, word_ins_ml=4.652, word_reposition=0.833, kpe=0.953, ppl=208.5, wps=7014.9, ups=0.34, wpb=20349.8, bsz=256, num_updates=6200, lr=0.000449013, gnorm=1.618, clip=0, loss_scale=4628, train_wall=251, wall=18626
2022-07-12 19:54:50 | INFO | train_inner | epoch 006:    694 / 1122 loss=7.694, nll_loss=3.04, mask_ins=1.273, word_ins_ml=4.632, word_reposition=0.835, kpe=0.955, ppl=207.12, wps=7145.7, ups=0.35, wpb=20646.6, bsz=256, num_updates=6300, lr=0.000445435, gnorm=1.587, clip=0, loss_scale=8192, train_wall=251, wall=18915
2022-07-12 19:59:39 | INFO | train_inner | epoch 006:    794 / 1122 loss=7.634, nll_loss=3.008, mask_ins=1.255, word_ins_ml=4.604, word_reposition=0.823, kpe=0.953, ppl=198.65, wps=7097.4, ups=0.35, wpb=20511.3, bsz=256, num_updates=6400, lr=0.000441942, gnorm=1.574, clip=0, loss_scale=8192, train_wall=251, wall=19204
2022-07-12 20:04:28 | INFO | train_inner | epoch 006:    894 / 1122 loss=nan, nll_loss=3.014, mask_ins=1.244, word_ins_ml=4.607, word_reposition=0.827, kpe=nan, ppl=nan, wps=7131.9, ups=0.35, wpb=20643.9, bsz=256, num_updates=6500, lr=0.000438529, gnorm=1.56, clip=0, loss_scale=8192, train_wall=251, wall=19494
2022-07-12 20:05:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 20:08:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-12 20:09:24 | INFO | train_inner | epoch 006:    996 / 1122 loss=7.63, nll_loss=3.023, mask_ins=1.242, word_ins_ml=4.615, word_reposition=0.812, kpe=0.96, ppl=198.08, wps=6925.9, ups=0.34, wpb=20475.1, bsz=256, num_updates=6600, lr=0.000435194, gnorm=1.72, clip=0, loss_scale=4538, train_wall=257, wall=19789
2022-07-12 20:14:14 | INFO | train_inner | epoch 006:   1096 / 1122 loss=7.619, nll_loss=2.995, mask_ins=1.245, word_ins_ml=4.591, word_reposition=0.822, kpe=0.961, ppl=196.54, wps=7077.4, ups=0.35, wpb=20507.7, bsz=256, num_updates=6700, lr=0.000431934, gnorm=1.702, clip=0, loss_scale=2048, train_wall=251, wall=20079
2022-07-12 20:15:28 | INFO | train | epoch 006 | loss nan | nll_loss 3.05 | mask_ins 1.268 | word_ins_ml 4.642 | word_reposition 0.831 | kpe nan | ppl nan | wps 6835.7 | ups 0.33 | wpb 20521 | bsz 255.8 | num_updates 6726 | lr 0.000431099 | gnorm 1.655 | clip 0 | loss_scale 4923 | train_wall 2843 | wall 20154
2022-07-12 20:16:48 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 12.844 | nll_loss 6.715 | mask_ins 2.072 | word_ins_ml 8.004 | word_reposition 1.357 | kpe 1.411 | ppl 7353.39 | wps 12355.3 | wpb 2367.6 | bsz 32 | num_updates 6726 | best_loss 12.844
2022-07-12 20:16:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_best.pt (epoch 6 @ 6726 updates, score 12.844) (writing took 6.670243154279888 seconds)
2022-07-12 20:20:30 | INFO | train_inner | epoch 007:     74 / 1122 loss=7.63, nll_loss=2.995, mask_ins=1.253, word_ins_ml=4.59, word_reposition=0.832, kpe=0.956, ppl=198.1, wps=5391.8, ups=0.27, wpb=20322.2, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=1.98, clip=0, loss_scale=2048, train_wall=252, wall=20456
2022-07-12 20:21:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-12 20:25:23 | INFO | train_inner | epoch 007:    175 / 1122 loss=7.555, nll_loss=2.967, mask_ins=1.233, word_ins_ml=4.564, word_reposition=0.809, kpe=0.949, ppl=188.1, wps=7029.2, ups=0.34, wpb=20595.2, bsz=256, num_updates=6900, lr=0.000425628, gnorm=1.764, clip=0, loss_scale=1186, train_wall=254, wall=20749
2022-07-12 20:25:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-12 20:30:16 | INFO | train_inner | epoch 007:    276 / 1122 loss=7.598, nll_loss=2.993, mask_ins=1.236, word_ins_ml=4.587, word_reposition=0.824, kpe=0.951, ppl=193.76, wps=7051.8, ups=0.34, wpb=20639.8, bsz=256, num_updates=7000, lr=0.000422577, gnorm=1.65, clip=0, loss_scale=542, train_wall=254, wall=21042
2022-07-12 20:35:30 | INFO | train_inner | epoch 007:    376 / 1122 loss=7.531, nll_loss=2.942, mask_ins=1.231, word_ins_ml=4.541, word_reposition=0.812, kpe=0.947, ppl=184.97, wps=6548, ups=0.32, wpb=20531.4, bsz=256, num_updates=7100, lr=0.000419591, gnorm=1.978, clip=0, loss_scale=512, train_wall=274, wall=21355
2022-07-12 20:40:19 | INFO | train_inner | epoch 007:    476 / 1122 loss=nan, nll_loss=2.928, mask_ins=1.225, word_ins_ml=4.529, word_reposition=0.809, kpe=nan, ppl=nan, wps=7091.1, ups=0.35, wpb=20532.2, bsz=256, num_updates=7200, lr=0.000416667, gnorm=1.527, clip=0, loss_scale=512, train_wall=251, wall=21645
2022-07-12 20:45:08 | INFO | train_inner | epoch 007:    576 / 1122 loss=7.504, nll_loss=2.937, mask_ins=1.223, word_ins_ml=4.536, word_reposition=0.802, kpe=0.944, ppl=181.58, wps=7075.1, ups=0.35, wpb=20434.1, bsz=256, num_updates=7300, lr=0.000413803, gnorm=1.514, clip=0, loss_scale=512, train_wall=250, wall=21934
2022-07-12 20:49:57 | INFO | train_inner | epoch 007:    676 / 1122 loss=7.478, nll_loss=2.897, mask_ins=1.225, word_ins_ml=4.5, word_reposition=0.811, kpe=0.943, ppl=178.25, wps=7108.3, ups=0.35, wpb=20544, bsz=256, num_updates=7400, lr=0.000410997, gnorm=1.514, clip=0, loss_scale=512, train_wall=250, wall=22223
2022-07-12 20:54:46 | INFO | train_inner | epoch 007:    776 / 1122 loss=7.45, nll_loss=2.882, mask_ins=1.222, word_ins_ml=4.487, word_reposition=0.804, kpe=0.938, ppl=174.82, wps=7119.3, ups=0.35, wpb=20543.5, bsz=256, num_updates=7500, lr=0.000408248, gnorm=1.587, clip=0, loss_scale=937, train_wall=250, wall=22511
2022-07-12 20:59:36 | INFO | train_inner | epoch 007:    876 / 1122 loss=7.459, nll_loss=2.917, mask_ins=1.216, word_ins_ml=4.516, word_reposition=0.787, kpe=0.94, ppl=176.01, wps=7087.2, ups=0.34, wpb=20546.2, bsz=256, num_updates=7600, lr=0.000405554, gnorm=1.506, clip=0, loss_scale=1024, train_wall=251, wall=22801
2022-07-12 21:04:24 | INFO | train_inner | epoch 007:    976 / 1122 loss=7.393, nll_loss=2.838, mask_ins=1.215, word_ins_ml=4.447, word_reposition=0.792, kpe=0.939, ppl=168.12, wps=7121.2, ups=0.35, wpb=20570.9, bsz=256, num_updates=7700, lr=0.000402911, gnorm=1.448, clip=0, loss_scale=1024, train_wall=250, wall=23090
2022-07-12 21:09:14 | INFO | train_inner | epoch 007:   1076 / 1122 loss=7.429, nll_loss=2.885, mask_ins=1.204, word_ins_ml=4.487, word_reposition=0.799, kpe=0.939, ppl=172.32, wps=7083.1, ups=0.35, wpb=20527.8, bsz=256, num_updates=7800, lr=0.00040032, gnorm=1.498, clip=0, loss_scale=1024, train_wall=251, wall=23380
2022-07-12 21:11:26 | INFO | train | epoch 007 | loss nan | nll_loss 2.922 | mask_ins 1.224 | word_ins_ml 4.522 | word_reposition 0.806 | kpe nan | ppl nan | wps 6843.9 | ups 0.33 | wpb 20521.1 | bsz 255.8 | num_updates 7846 | lr 0.000399145 | gnorm 1.619 | clip 0 | loss_scale 872 | train_wall 2838 | wall 23512
2022-07-12 21:12:47 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 12.878 | nll_loss 6.669 | mask_ins 2.061 | word_ins_ml 7.98 | word_reposition 1.429 | kpe 1.407 | ppl 7528.92 | wps 12317.2 | wpb 2367.6 | bsz 32 | num_updates 7846 | best_loss 12.844
2022-07-12 21:12:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 7 @ 7846 updates, score 12.878) (writing took 3.586188718676567 seconds)
2022-07-12 21:15:25 | INFO | train_inner | epoch 008:     54 / 1122 loss=7.423, nll_loss=2.884, mask_ins=1.211, word_ins_ml=4.486, word_reposition=0.793, kpe=0.933, ppl=171.65, wps=5495.5, ups=0.27, wpb=20398.9, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=1.561, clip=0, loss_scale=1024, train_wall=249, wall=23751
2022-07-12 21:20:15 | INFO | train_inner | epoch 008:    154 / 1122 loss=nan, nll_loss=2.813, mask_ins=1.197, word_ins_ml=4.424, word_reposition=0.775, kpe=nan, ppl=nan, wps=7076.9, ups=0.34, wpb=20517.5, bsz=256, num_updates=8000, lr=0.000395285, gnorm=1.454, clip=0, loss_scale=1751, train_wall=251, wall=24041
2022-07-12 21:25:04 | INFO | train_inner | epoch 008:    254 / 1122 loss=7.366, nll_loss=2.859, mask_ins=1.197, word_ins_ml=4.464, word_reposition=0.779, kpe=0.925, ppl=164.98, wps=7090.8, ups=0.35, wpb=20490.6, bsz=256, num_updates=8100, lr=0.000392837, gnorm=1.446, clip=0, loss_scale=2048, train_wall=250, wall=24330
2022-07-12 21:29:53 | INFO | train_inner | epoch 008:    354 / 1122 loss=7.33, nll_loss=2.814, mask_ins=1.186, word_ins_ml=4.424, word_reposition=0.792, kpe=0.928, ppl=160.94, wps=7122.9, ups=0.35, wpb=20563.2, bsz=256, num_updates=8200, lr=0.000390434, gnorm=1.445, clip=0, loss_scale=2048, train_wall=250, wall=24619
2022-07-12 21:34:42 | INFO | train_inner | epoch 008:    454 / 1122 loss=7.309, nll_loss=2.802, mask_ins=1.188, word_ins_ml=4.413, word_reposition=0.785, kpe=0.924, ppl=158.54, wps=7109.4, ups=0.35, wpb=20538.8, bsz=256, num_updates=8300, lr=0.000388075, gnorm=1.435, clip=0, loss_scale=2048, train_wall=250, wall=24908
2022-07-12 21:39:58 | INFO | train_inner | epoch 008:    554 / 1122 loss=7.308, nll_loss=2.79, mask_ins=1.191, word_ins_ml=4.402, word_reposition=0.787, kpe=0.928, ppl=158.45, wps=6513.2, ups=0.32, wpb=20559.9, bsz=256, num_updates=8400, lr=0.000385758, gnorm=1.479, clip=0, loss_scale=2048, train_wall=277, wall=25223
2022-07-12 21:44:48 | INFO | train_inner | epoch 008:    654 / 1122 loss=7.302, nll_loss=2.81, mask_ins=1.177, word_ins_ml=4.419, word_reposition=0.778, kpe=0.928, ppl=157.77, wps=7115.6, ups=0.34, wpb=20632.9, bsz=256, num_updates=8500, lr=0.000383482, gnorm=1.43, clip=0, loss_scale=3256, train_wall=251, wall=25513
2022-07-12 21:49:37 | INFO | train_inner | epoch 008:    754 / 1122 loss=7.275, nll_loss=2.795, mask_ins=1.175, word_ins_ml=4.405, word_reposition=0.769, kpe=0.926, ppl=154.91, wps=7087.7, ups=0.35, wpb=20485.1, bsz=256, num_updates=8600, lr=0.000381246, gnorm=1.429, clip=0, loss_scale=4096, train_wall=250, wall=25802
2022-07-12 21:54:26 | INFO | train_inner | epoch 008:    854 / 1122 loss=nan, nll_loss=2.817, mask_ins=1.182, word_ins_ml=4.424, word_reposition=0.778, kpe=nan, ppl=nan, wps=7101.6, ups=0.35, wpb=20551.8, bsz=256, num_updates=8700, lr=0.000379049, gnorm=1.473, clip=0, loss_scale=4096, train_wall=251, wall=26092
2022-07-12 21:59:15 | INFO | train_inner | epoch 008:    954 / 1122 loss=7.251, nll_loss=2.767, mask_ins=1.172, word_ins_ml=4.38, word_reposition=0.774, kpe=0.925, ppl=152.29, wps=7117.1, ups=0.35, wpb=20579.1, bsz=256, num_updates=8800, lr=0.000376889, gnorm=1.383, clip=0, loss_scale=4096, train_wall=251, wall=26381
2022-07-12 22:04:05 | INFO | train_inner | epoch 008:   1054 / 1122 loss=7.269, nll_loss=2.798, mask_ins=1.167, word_ins_ml=4.408, word_reposition=0.768, kpe=0.927, ppl=154.26, wps=7034, ups=0.35, wpb=20351.6, bsz=256, num_updates=8900, lr=0.000374766, gnorm=1.437, clip=0, loss_scale=4096, train_wall=251, wall=26670
2022-07-12 22:07:20 | INFO | train | epoch 008 | loss nan | nll_loss 2.812 | mask_ins 1.185 | word_ins_ml 4.421 | word_reposition 0.779 | kpe nan | ppl nan | wps 6865.6 | ups 0.33 | wpb 20521.1 | bsz 255.8 | num_updates 8968 | lr 0.000373342 | gnorm 1.447 | clip 0 | loss_scale 2989 | train_wall 2837 | wall 26866
2022-07-12 22:08:40 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 12.953 | nll_loss 6.725 | mask_ins 2.088 | word_ins_ml 8.029 | word_reposition 1.421 | kpe 1.416 | ppl 7930.1 | wps 12404.7 | wpb 2367.6 | bsz 32 | num_updates 8968 | best_loss 12.844
2022-07-12 22:08:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 8 @ 8968 updates, score 12.953) (writing took 3.649414268322289 seconds)
2022-07-12 22:10:15 | INFO | train_inner | epoch 009:     32 / 1122 loss=7.295, nll_loss=2.819, mask_ins=1.176, word_ins_ml=4.425, word_reposition=0.774, kpe=0.921, ppl=157.09, wps=5473.5, ups=0.27, wpb=20299.1, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=1.46, clip=0, loss_scale=6021, train_wall=249, wall=27041
2022-07-12 22:15:04 | INFO | train_inner | epoch 009:    132 / 1122 loss=7.21, nll_loss=2.751, mask_ins=1.17, word_ins_ml=4.365, word_reposition=0.764, kpe=0.91, ppl=148.01, wps=7148.1, ups=0.35, wpb=20651.8, bsz=256, num_updates=9100, lr=0.000370625, gnorm=1.394, clip=0, loss_scale=8192, train_wall=250, wall=27330
2022-07-12 22:16:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 22:19:57 | INFO | train_inner | epoch 009:    233 / 1122 loss=7.204, nll_loss=2.755, mask_ins=1.162, word_ins_ml=4.368, word_reposition=0.763, kpe=0.911, ppl=147.45, wps=7005.1, ups=0.34, wpb=20487.3, bsz=256, num_updates=9200, lr=0.000368605, gnorm=1.404, clip=0, loss_scale=5475, train_wall=254, wall=27623
2022-07-12 22:24:46 | INFO | train_inner | epoch 009:    333 / 1122 loss=7.207, nll_loss=2.734, mask_ins=1.169, word_ins_ml=4.35, word_reposition=0.774, kpe=0.914, ppl=147.75, wps=7065, ups=0.35, wpb=20444.2, bsz=256, num_updates=9300, lr=0.000366618, gnorm=1.458, clip=0, loss_scale=4096, train_wall=251, wall=27912
2022-07-12 22:29:35 | INFO | train_inner | epoch 009:    433 / 1122 loss=nan, nll_loss=2.739, mask_ins=1.148, word_ins_ml=4.354, word_reposition=0.753, kpe=nan, ppl=nan, wps=7084.6, ups=0.35, wpb=20483.7, bsz=256, num_updates=9400, lr=0.000364662, gnorm=1.414, clip=0, loss_scale=4096, train_wall=251, wall=28201
2022-07-12 22:34:24 | INFO | train_inner | epoch 009:    533 / 1122 loss=7.175, nll_loss=2.734, mask_ins=1.154, word_ins_ml=4.349, word_reposition=0.76, kpe=0.912, ppl=144.52, wps=7108.6, ups=0.35, wpb=20538, bsz=256, num_updates=9500, lr=0.000362738, gnorm=1.418, clip=0, loss_scale=4096, train_wall=251, wall=28490
2022-07-12 22:39:14 | INFO | train_inner | epoch 009:    633 / 1122 loss=7.16, nll_loss=2.73, mask_ins=1.151, word_ins_ml=4.346, word_reposition=0.756, kpe=0.907, ppl=143.03, wps=7101, ups=0.35, wpb=20555.3, bsz=256, num_updates=9600, lr=0.000360844, gnorm=1.383, clip=0, loss_scale=4096, train_wall=251, wall=28779
2022-07-12 22:43:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 22:44:30 | INFO | train_inner | epoch 009:    734 / 1122 loss=7.157, nll_loss=2.725, mask_ins=1.153, word_ins_ml=4.341, word_reposition=0.75, kpe=0.913, ppl=142.72, wps=6496.3, ups=0.32, wpb=20566.3, bsz=256, num_updates=9700, lr=0.000358979, gnorm=1.432, clip=0, loss_scale=5759, train_wall=278, wall=29096
2022-07-12 22:49:19 | INFO | train_inner | epoch 009:    834 / 1122 loss=7.21, nll_loss=2.773, mask_ins=1.152, word_ins_ml=4.383, word_reposition=0.763, kpe=0.912, ppl=148.05, wps=7092.8, ups=0.35, wpb=20505.7, bsz=256, num_updates=9800, lr=0.000357143, gnorm=1.437, clip=0, loss_scale=4096, train_wall=250, wall=29385
2022-07-12 22:54:08 | INFO | train_inner | epoch 009:    934 / 1122 loss=7.135, nll_loss=2.722, mask_ins=1.139, word_ins_ml=4.338, word_reposition=0.747, kpe=0.911, ppl=140.53, wps=7123.3, ups=0.35, wpb=20568.6, bsz=256, num_updates=9900, lr=0.000355335, gnorm=1.41, clip=0, loss_scale=4096, train_wall=250, wall=29674
2022-07-12 22:58:57 | INFO | train_inner | epoch 009:   1034 / 1122 loss=nan, nll_loss=2.738, mask_ins=1.155, word_ins_ml=4.351, word_reposition=0.768, kpe=nan, ppl=nan, wps=7101.5, ups=0.35, wpb=20519.8, bsz=256, num_updates=10000, lr=0.000353553, gnorm=1.382, clip=0, loss_scale=4096, train_wall=250, wall=29963
2022-07-12 23:03:11 | INFO | train | epoch 009 | loss nan | nll_loss 2.738 | mask_ins 1.154 | word_ins_ml 4.353 | word_reposition 0.761 | kpe nan | ppl nan | wps 6859.5 | ups 0.33 | wpb 20521.6 | bsz 255.8 | num_updates 10088 | lr 0.000352008 | gnorm 1.414 | clip 0 | loss_scale 4852 | train_wall 2835 | wall 30216
2022-07-12 23:04:30 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 12.868 | nll_loss 6.65 | mask_ins 2.066 | word_ins_ml 7.96 | word_reposition 1.435 | kpe 1.406 | ppl 7474.43 | wps 12445.2 | wpb 2367.6 | bsz 32 | num_updates 10088 | best_loss 12.844
2022-07-12 23:04:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 9 @ 10088 updates, score 12.868) (writing took 3.8952072709798813 seconds)
2022-07-12 23:05:09 | INFO | train_inner | epoch 010:     12 / 1122 loss=7.141, nll_loss=2.704, mask_ins=1.146, word_ins_ml=4.321, word_reposition=0.766, kpe=0.909, ppl=141.12, wps=5508.6, ups=0.27, wpb=20465.6, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=1.425, clip=0, loss_scale=4096, train_wall=250, wall=30334
2022-07-12 23:09:59 | INFO | train_inner | epoch 010:    112 / 1122 loss=7.089, nll_loss=2.697, mask_ins=1.13, word_ins_ml=4.315, word_reposition=0.744, kpe=0.901, ppl=136.14, wps=7096.4, ups=0.34, wpb=20574.6, bsz=256, num_updates=10200, lr=0.00035007, gnorm=1.373, clip=0, loss_scale=4219, train_wall=251, wall=30624
2022-07-12 23:13:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 23:14:50 | INFO | train_inner | epoch 010:    213 / 1122 loss=7.064, nll_loss=2.666, mask_ins=1.136, word_ins_ml=4.288, word_reposition=0.743, kpe=0.897, ppl=133.78, wps=7042.6, ups=0.34, wpb=20533.8, bsz=256, num_updates=10300, lr=0.000348367, gnorm=1.379, clip=0, loss_scale=6773, train_wall=253, wall=30916
2022-07-12 23:19:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-12 23:19:42 | INFO | train_inner | epoch 010:    314 / 1122 loss=7.097, nll_loss=2.695, mask_ins=1.134, word_ins_ml=4.313, word_reposition=0.748, kpe=0.902, ppl=136.88, wps=7057.4, ups=0.34, wpb=20602.8, bsz=256, num_updates=10400, lr=0.000346688, gnorm=1.401, clip=0, loss_scale=3812, train_wall=253, wall=31208
2022-07-12 23:24:32 | INFO | train_inner | epoch 010:    414 / 1122 loss=7.108, nll_loss=2.695, mask_ins=1.136, word_ins_ml=4.312, word_reposition=0.754, kpe=0.905, ppl=137.95, wps=7105, ups=0.35, wpb=20562.6, bsz=256, num_updates=10500, lr=0.000345033, gnorm=1.399, clip=0, loss_scale=2048, train_wall=251, wall=31497
2022-07-12 23:29:20 | INFO | train_inner | epoch 010:    514 / 1122 loss=7.073, nll_loss=2.676, mask_ins=1.132, word_ins_ml=4.295, word_reposition=0.749, kpe=0.896, ppl=134.65, wps=7103, ups=0.35, wpb=20513.3, bsz=256, num_updates=10600, lr=0.000343401, gnorm=1.389, clip=0, loss_scale=2048, train_wall=250, wall=31786
2022-07-12 23:34:10 | INFO | train_inner | epoch 010:    614 / 1122 loss=nan, nll_loss=2.719, mask_ins=1.131, word_ins_ml=4.333, word_reposition=0.761, kpe=nan, ppl=nan, wps=7148.9, ups=0.35, wpb=20699.5, bsz=256, num_updates=10700, lr=0.000341793, gnorm=1.344, clip=0, loss_scale=2048, train_wall=251, wall=32076
2022-07-12 23:39:00 | INFO | train_inner | epoch 010:    714 / 1122 loss=7.054, nll_loss=2.67, mask_ins=1.123, word_ins_ml=4.289, word_reposition=0.741, kpe=0.9, ppl=132.85, wps=7038.8, ups=0.35, wpb=20397, bsz=256, num_updates=10800, lr=0.000340207, gnorm=1.346, clip=0, loss_scale=2048, train_wall=251, wall=32365
2022-07-12 23:43:59 | INFO | train_inner | epoch 010:    814 / 1122 loss=nan, nll_loss=2.683, mask_ins=1.121, word_ins_ml=4.301, word_reposition=0.745, kpe=nan, ppl=nan, wps=6849.2, ups=0.33, wpb=20530, bsz=256, num_updates=10900, lr=0.000338643, gnorm=1.386, clip=0, loss_scale=2089, train_wall=262, wall=32665
2022-07-12 23:48:59 | INFO | train_inner | epoch 010:    914 / 1122 loss=7.028, nll_loss=2.652, mask_ins=1.116, word_ins_ml=4.273, word_reposition=0.735, kpe=0.904, ppl=130.49, wps=6864.3, ups=0.33, wpb=20540.4, bsz=256, num_updates=11000, lr=0.0003371, gnorm=1.445, clip=0, loss_scale=4096, train_wall=261, wall=32964
2022-07-12 23:53:46 | INFO | train_inner | epoch 010:   1014 / 1122 loss=7.008, nll_loss=2.647, mask_ins=1.108, word_ins_ml=4.268, word_reposition=0.73, kpe=0.902, ppl=128.74, wps=7117.8, ups=0.35, wpb=20461.9, bsz=256, num_updates=11100, lr=0.000335578, gnorm=1.34, clip=0, loss_scale=4096, train_wall=250, wall=33252
2022-07-12 23:58:33 | INFO | train_inner | epoch 010:   1114 / 1122 loss=7.013, nll_loss=2.626, mask_ins=1.113, word_ins_ml=4.25, word_reposition=0.75, kpe=0.9, ppl=129.14, wps=7147.8, ups=0.35, wpb=20472.6, bsz=256, num_updates=11200, lr=0.000334077, gnorm=1.34, clip=0, loss_scale=4096, train_wall=249, wall=33538
2022-07-12 23:58:54 | INFO | train | epoch 010 | loss nan | nll_loss 2.675 | mask_ins 1.126 | word_ins_ml 4.295 | word_reposition 0.746 | kpe nan | ppl nan | wps 6873.4 | ups 0.33 | wpb 20520.7 | bsz 255.8 | num_updates 11208 | lr 0.000333957 | gnorm 1.382 | clip 0 | loss_scale 3413 | train_wall 2831 | wall 33560
2022-07-13 00:00:13 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 12.634 | nll_loss 6.518 | mask_ins 2.049 | word_ins_ml 7.842 | word_reposition 1.365 | kpe 1.378 | ppl 6354.87 | wps 12588.4 | wpb 2367.6 | bsz 32 | num_updates 11208 | best_loss 12.634
2022-07-13 00:00:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_best.pt (epoch 10 @ 11208 updates, score 12.634) (writing took 7.104588782414794 seconds)
2022-07-13 00:04:44 | INFO | train_inner | epoch 011:     92 / 1122 loss=7.015, nll_loss=2.659, mask_ins=1.109, word_ins_ml=4.28, word_reposition=0.742, kpe=0.885, ppl=129.36, wps=5497.6, ups=0.27, wpb=20400.9, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=1.434, clip=0, loss_scale=4096, train_wall=248, wall=33909
2022-07-13 00:09:31 | INFO | train_inner | epoch 011:    192 / 1122 loss=nan, nll_loss=2.599, mask_ins=1.097, word_ins_ml=4.225, word_reposition=0.743, kpe=nan, ppl=nan, wps=7127.6, ups=0.35, wpb=20480.2, bsz=256, num_updates=11400, lr=0.000331133, gnorm=1.357, clip=0, loss_scale=4096, train_wall=250, wall=34197
2022-07-13 00:11:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-13 00:14:20 | INFO | train_inner | epoch 011:    293 / 1122 loss=nan, nll_loss=2.616, mask_ins=1.096, word_ins_ml=4.24, word_reposition=0.727, kpe=nan, ppl=nan, wps=7096, ups=0.35, wpb=20530.3, bsz=256, num_updates=11500, lr=0.00032969, gnorm=1.391, clip=0, loss_scale=5556, train_wall=251, wall=34486
2022-07-13 00:19:07 | INFO | train_inner | epoch 011:    393 / 1122 loss=6.944, nll_loss=2.594, mask_ins=1.102, word_ins_ml=4.222, word_reposition=0.734, kpe=0.886, ppl=123.1, wps=7168.9, ups=0.35, wpb=20569.7, bsz=256, num_updates=11600, lr=0.000328266, gnorm=1.372, clip=0, loss_scale=4096, train_wall=249, wall=34773
2022-07-13 00:23:54 | INFO | train_inner | epoch 011:    493 / 1122 loss=6.985, nll_loss=2.642, mask_ins=1.095, word_ins_ml=4.263, word_reposition=0.737, kpe=0.89, ppl=126.69, wps=7197.5, ups=0.35, wpb=20611.7, bsz=256, num_updates=11700, lr=0.00032686, gnorm=1.356, clip=0, loss_scale=4096, train_wall=249, wall=35059
2022-07-13 00:28:40 | INFO | train_inner | epoch 011:    593 / 1122 loss=6.961, nll_loss=2.626, mask_ins=1.093, word_ins_ml=4.249, word_reposition=0.731, kpe=0.887, ppl=124.57, wps=7168.1, ups=0.35, wpb=20563.1, bsz=256, num_updates=11800, lr=0.000325472, gnorm=1.366, clip=0, loss_scale=4096, train_wall=249, wall=35346
2022-07-13 00:33:26 | INFO | train_inner | epoch 011:    693 / 1122 loss=6.97, nll_loss=2.628, mask_ins=1.093, word_ins_ml=4.25, word_reposition=0.735, kpe=0.892, ppl=125.32, wps=7182.6, ups=0.35, wpb=20535.8, bsz=256, num_updates=11900, lr=0.000324102, gnorm=1.349, clip=0, loss_scale=4096, train_wall=248, wall=35632
2022-07-13 00:36:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-13 00:38:15 | INFO | train_inner | epoch 011:    794 / 1122 loss=6.958, nll_loss=2.609, mask_ins=1.1, word_ins_ml=4.234, word_reposition=0.731, kpe=0.893, ppl=124.3, wps=7039.2, ups=0.35, wpb=20348.6, bsz=256, num_updates=12000, lr=0.000322749, gnorm=1.337, clip=0, loss_scale=4542, train_wall=251, wall=35921
2022-07-13 00:42:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 00:43:05 | INFO | train_inner | epoch 011:    895 / 1122 loss=6.953, nll_loss=2.638, mask_ins=1.079, word_ins_ml=4.259, word_reposition=0.727, kpe=0.888, ppl=123.89, wps=7120.9, ups=0.35, wpb=20625.7, bsz=256, num_updates=12100, lr=0.000321412, gnorm=1.382, clip=0, loss_scale=3751, train_wall=251, wall=36211
2022-07-13 00:48:03 | INFO | train_inner | epoch 011:    995 / 1122 loss=6.972, nll_loss=2.624, mask_ins=1.093, word_ins_ml=4.247, word_reposition=0.74, kpe=0.892, ppl=125.54, wps=6868.3, ups=0.34, wpb=20487.8, bsz=256, num_updates=12200, lr=0.000320092, gnorm=1.327, clip=0, loss_scale=2048, train_wall=261, wall=36509
2022-07-13 00:53:14 | INFO | train_inner | epoch 011:   1095 / 1122 loss=6.943, nll_loss=2.608, mask_ins=1.088, word_ins_ml=4.232, word_reposition=0.733, kpe=0.889, ppl=123.02, wps=6617.8, ups=0.32, wpb=20565.7, bsz=256, num_updates=12300, lr=0.000318788, gnorm=1.38, clip=0, loss_scale=2048, train_wall=273, wall=36820
2022-07-13 00:54:30 | INFO | train | epoch 011 | loss nan | nll_loss 2.621 | mask_ins 1.095 | word_ins_ml 4.245 | word_reposition 0.735 | kpe nan | ppl nan | wps 6884.2 | ups 0.34 | wpb 20523.4 | bsz 255.8 | num_updates 12327 | lr 0.000318439 | gnorm 1.367 | clip 0 | loss_scale 3822 | train_wall 2828 | wall 36896
2022-07-13 00:55:49 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 12.835 | nll_loss 6.667 | mask_ins 2.044 | word_ins_ml 7.979 | word_reposition 1.42 | kpe 1.392 | ppl 7306.32 | wps 12528.3 | wpb 2367.6 | bsz 32 | num_updates 12327 | best_loss 12.634
2022-07-13 00:55:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 11 @ 12327 updates, score 12.835) (writing took 3.6840353878214955 seconds)
2022-07-13 00:59:22 | INFO | train_inner | epoch 012:     73 / 1122 loss=6.939, nll_loss=2.62, mask_ins=1.081, word_ins_ml=4.243, word_reposition=0.735, kpe=0.881, ppl=122.69, wps=5561.4, ups=0.27, wpb=20456.1, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=1.388, clip=0, loss_scale=2048, train_wall=248, wall=37188
2022-07-13 01:04:08 | INFO | train_inner | epoch 012:    173 / 1122 loss=6.854, nll_loss=2.562, mask_ins=1.07, word_ins_ml=4.191, word_reposition=0.718, kpe=0.875, ppl=115.66, wps=7157.9, ups=0.35, wpb=20488.9, bsz=256, num_updates=12500, lr=0.000316228, gnorm=1.341, clip=0, loss_scale=2048, train_wall=248, wall=37474
2022-07-13 01:08:55 | INFO | train_inner | epoch 012:    273 / 1122 loss=6.912, nll_loss=2.6, mask_ins=1.081, word_ins_ml=4.224, word_reposition=0.735, kpe=0.872, ppl=120.44, wps=7149.9, ups=0.35, wpb=20508.5, bsz=256, num_updates=12600, lr=0.00031497, gnorm=1.329, clip=0, loss_scale=2150, train_wall=249, wall=37761
2022-07-13 01:13:41 | INFO | train_inner | epoch 012:    373 / 1122 loss=6.896, nll_loss=2.585, mask_ins=1.079, word_ins_ml=4.212, word_reposition=0.727, kpe=0.877, ppl=119.11, wps=7203.3, ups=0.35, wpb=20607.2, bsz=256, num_updates=12700, lr=0.000313728, gnorm=1.389, clip=0, loss_scale=4096, train_wall=249, wall=38047
2022-07-13 01:18:27 | INFO | train_inner | epoch 012:    473 / 1122 loss=6.889, nll_loss=2.598, mask_ins=1.066, word_ins_ml=4.222, word_reposition=0.719, kpe=0.882, ppl=118.55, wps=7175.1, ups=0.35, wpb=20492.2, bsz=256, num_updates=12800, lr=0.0003125, gnorm=1.4, clip=0, loss_scale=4096, train_wall=248, wall=38333
2022-07-13 01:23:13 | INFO | train_inner | epoch 012:    573 / 1122 loss=6.872, nll_loss=2.563, mask_ins=1.073, word_ins_ml=4.192, word_reposition=0.728, kpe=0.878, ppl=117.09, wps=7200.4, ups=0.35, wpb=20611.6, bsz=256, num_updates=12900, lr=0.000311286, gnorm=1.317, clip=0, loss_scale=4096, train_wall=249, wall=38619
2022-07-13 01:27:59 | INFO | train_inner | epoch 012:    673 / 1122 loss=6.838, nll_loss=2.553, mask_ins=1.064, word_ins_ml=4.183, word_reposition=0.712, kpe=0.879, ppl=114.43, wps=7127.1, ups=0.35, wpb=20374.1, bsz=256, num_updates=13000, lr=0.000310087, gnorm=1.376, clip=0, loss_scale=4096, train_wall=248, wall=38905
2022-07-13 01:32:45 | INFO | train_inner | epoch 012:    773 / 1122 loss=6.848, nll_loss=2.548, mask_ins=1.067, word_ins_ml=4.178, word_reposition=0.723, kpe=0.879, ppl=115.21, wps=7196.8, ups=0.35, wpb=20613.2, bsz=256, num_updates=13100, lr=0.000308901, gnorm=1.364, clip=0, loss_scale=4096, train_wall=249, wall=39191
2022-07-13 01:34:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-13 01:37:35 | INFO | train_inner | epoch 012:    874 / 1122 loss=nan, nll_loss=2.571, mask_ins=1.079, word_ins_ml=4.199, word_reposition=0.719, kpe=nan, ppl=nan, wps=7096.8, ups=0.35, wpb=20557.8, bsz=256, num_updates=13200, lr=0.000307729, gnorm=1.372, clip=0, loss_scale=5353, train_wall=252, wall=39481
2022-07-13 01:42:22 | INFO | train_inner | epoch 012:    974 / 1122 loss=6.835, nll_loss=2.549, mask_ins=1.063, word_ins_ml=4.179, word_reposition=0.716, kpe=0.877, ppl=114.16, wps=7154.3, ups=0.35, wpb=20506.7, bsz=256, num_updates=13300, lr=0.00030657, gnorm=1.364, clip=0, loss_scale=4096, train_wall=249, wall=39767
2022-07-13 01:47:08 | INFO | train_inner | epoch 012:   1074 / 1122 loss=nan, nll_loss=2.591, mask_ins=1.064, word_ins_ml=4.216, word_reposition=0.719, kpe=nan, ppl=nan, wps=7163.8, ups=0.35, wpb=20498.5, bsz=256, num_updates=13400, lr=0.000305424, gnorm=1.374, clip=0, loss_scale=4096, train_wall=248, wall=40054
2022-07-13 01:49:25 | INFO | train | epoch 012 | loss nan | nll_loss 2.576 | mask_ins 1.07 | word_ins_ml 4.203 | word_reposition 0.722 | kpe nan | ppl nan | wps 6982.3 | ups 0.34 | wpb 20520.3 | bsz 255.8 | num_updates 13448 | lr 0.000304878 | gnorm 1.364 | clip 0 | loss_scale 3720 | train_wall 2790 | wall 40191
2022-07-13 01:50:43 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 12.933 | nll_loss 6.623 | mask_ins 2.082 | word_ins_ml 7.949 | word_reposition 1.462 | kpe 1.441 | ppl 7819.91 | wps 12602.9 | wpb 2367.6 | bsz 32 | num_updates 13448 | best_loss 12.634
2022-07-13 01:50:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 12 @ 13448 updates, score 12.933) (writing took 3.8002739725634456 seconds)
2022-07-13 01:53:29 | INFO | train_inner | epoch 013:     52 / 1122 loss=6.845, nll_loss=2.554, mask_ins=1.068, word_ins_ml=4.184, word_reposition=0.722, kpe=0.871, ppl=114.97, wps=5344.6, ups=0.26, wpb=20381.7, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=1.411, clip=0, loss_scale=4096, train_wall=262, wall=40435
2022-07-13 01:58:51 | INFO | train_inner | epoch 013:    152 / 1122 loss=6.808, nll_loss=2.526, mask_ins=1.059, word_ins_ml=4.159, word_reposition=0.726, kpe=0.864, ppl=112.08, wps=6393.8, ups=0.31, wpb=20591.3, bsz=256, num_updates=13600, lr=0.00030317, gnorm=1.35, clip=0, loss_scale=4096, train_wall=284, wall=40757
2022-07-13 02:03:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-13 02:03:44 | INFO | train_inner | epoch 013:    253 / 1122 loss=6.844, nll_loss=2.575, mask_ins=1.061, word_ins_ml=4.202, word_reposition=0.714, kpe=0.867, ppl=114.91, wps=7000.7, ups=0.34, wpb=20461.7, bsz=256, num_updates=13700, lr=0.000302061, gnorm=1.383, clip=0, loss_scale=5880, train_wall=253, wall=41049
2022-07-13 02:08:32 | INFO | train_inner | epoch 013:    353 / 1122 loss=nan, nll_loss=2.558, mask_ins=1.051, word_ins_ml=4.187, word_reposition=0.712, kpe=nan, ppl=nan, wps=7136.8, ups=0.35, wpb=20550.4, bsz=256, num_updates=13800, lr=0.000300965, gnorm=1.4, clip=0, loss_scale=4096, train_wall=250, wall=41337
2022-07-13 02:09:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 02:13:23 | INFO | train_inner | epoch 013:    454 / 1122 loss=6.831, nll_loss=2.564, mask_ins=1.057, word_ins_ml=4.192, word_reposition=0.714, kpe=0.868, ppl=113.85, wps=7051.9, ups=0.34, wpb=20582.5, bsz=256, num_updates=13900, lr=0.00029988, gnorm=1.377, clip=0, loss_scale=2251, train_wall=254, wall=41629
2022-07-13 02:18:09 | INFO | train_inner | epoch 013:    554 / 1122 loss=6.809, nll_loss=2.541, mask_ins=1.055, word_ins_ml=4.171, word_reposition=0.717, kpe=0.866, ppl=112.14, wps=7177.8, ups=0.35, wpb=20525.6, bsz=256, num_updates=14000, lr=0.000298807, gnorm=1.419, clip=0, loss_scale=2048, train_wall=249, wall=41915
2022-07-13 02:22:58 | INFO | train_inner | epoch 013:    654 / 1122 loss=6.798, nll_loss=2.533, mask_ins=1.05, word_ins_ml=4.164, word_reposition=0.713, kpe=0.87, ppl=111.25, wps=7093.4, ups=0.35, wpb=20467.9, bsz=256, num_updates=14100, lr=0.000297746, gnorm=1.377, clip=0, loss_scale=2048, train_wall=251, wall=42204
2022-07-13 02:27:45 | INFO | train_inner | epoch 013:    754 / 1122 loss=nan, nll_loss=2.554, mask_ins=1.045, word_ins_ml=4.182, word_reposition=0.703, kpe=nan, ppl=nan, wps=7143.3, ups=0.35, wpb=20534.2, bsz=256, num_updates=14200, lr=0.000296695, gnorm=1.344, clip=0, loss_scale=2048, train_wall=250, wall=42491
2022-07-13 02:32:32 | INFO | train_inner | epoch 013:    854 / 1122 loss=6.791, nll_loss=2.544, mask_ins=1.044, word_ins_ml=4.173, word_reposition=0.709, kpe=0.864, ppl=110.7, wps=7144.5, ups=0.35, wpb=20509.8, bsz=256, num_updates=14300, lr=0.000295656, gnorm=1.383, clip=0, loss_scale=2048, train_wall=250, wall=42778
2022-07-13 02:33:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 02:35:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-13 02:37:26 | INFO | train_inner | epoch 013:    956 / 1122 loss=6.784, nll_loss=2.53, mask_ins=1.044, word_ins_ml=4.16, word_reposition=0.71, kpe=0.871, ppl=110.22, wps=6994.8, ups=0.34, wpb=20498.2, bsz=256, num_updates=14400, lr=0.000294628, gnorm=1.358, clip=0, loss_scale=1656, train_wall=255, wall=43071
2022-07-13 02:41:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-13 02:42:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-13 02:42:20 | INFO | train_inner | epoch 013:   1058 / 1122 loss=6.783, nll_loss=2.537, mask_ins=1.047, word_ins_ml=4.167, word_reposition=0.704, kpe=0.866, ppl=110.15, wps=7010.1, ups=0.34, wpb=20623.8, bsz=256, num_updates=14500, lr=0.00029361, gnorm=1.438, clip=0, loss_scale=904, train_wall=255, wall=43365
2022-07-13 02:45:22 | INFO | train | epoch 013 | loss nan | nll_loss 2.549 | mask_ins 1.053 | word_ins_ml 4.179 | word_reposition 0.713 | kpe nan | ppl nan | wps 6821.1 | ups 0.33 | wpb 20520.8 | bsz 255.8 | num_updates 14564 | lr 0.000292964 | gnorm 1.395 | clip 0 | loss_scale 2629 | train_wall 2851 | wall 43548
2022-07-13 02:46:42 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 12.614 | nll_loss 6.458 | mask_ins 2.058 | word_ins_ml 7.783 | word_reposition 1.367 | kpe 1.405 | ppl 6266.8 | wps 12390.2 | wpb 2367.6 | bsz 32 | num_updates 14564 | best_loss 12.614
2022-07-13 02:46:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_best.pt (epoch 13 @ 14564 updates, score 12.614) (writing took 6.5813807947561145 seconds)
2022-07-13 02:48:32 | INFO | train_inner | epoch 014:     36 / 1122 loss=6.818, nll_loss=2.553, mask_ins=1.052, word_ins_ml=4.181, word_reposition=0.713, kpe=0.872, ppl=112.85, wps=5492.5, ups=0.27, wpb=20448.5, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=1.524, clip=0, loss_scale=256, train_wall=248, wall=43738
2022-07-13 02:53:21 | INFO | train_inner | epoch 014:    136 / 1122 loss=6.784, nll_loss=2.531, mask_ins=1.051, word_ins_ml=4.162, word_reposition=0.716, kpe=0.856, ppl=110.2, wps=7079.7, ups=0.35, wpb=20475.5, bsz=256, num_updates=14700, lr=0.000291606, gnorm=1.393, clip=0, loss_scale=256, train_wall=251, wall=44027
2022-07-13 02:58:34 | INFO | train_inner | epoch 014:    236 / 1122 loss=6.744, nll_loss=2.51, mask_ins=1.038, word_ins_ml=4.143, word_reposition=0.709, kpe=0.854, ppl=107.2, wps=6576.6, ups=0.32, wpb=20545.8, bsz=256, num_updates=14800, lr=0.000290619, gnorm=1.337, clip=0, loss_scale=256, train_wall=274, wall=44339
2022-07-13 03:03:34 | INFO | train_inner | epoch 014:    336 / 1122 loss=nan, nll_loss=2.511, mask_ins=1.045, word_ins_ml=4.143, word_reposition=0.703, kpe=nan, ppl=nan, wps=6847, ups=0.33, wpb=20546, bsz=256, num_updates=14900, lr=0.000289642, gnorm=1.386, clip=0, loss_scale=256, train_wall=262, wall=44639
2022-07-13 03:08:25 | INFO | train_inner | epoch 014:    436 / 1122 loss=6.767, nll_loss=2.512, mask_ins=1.049, word_ins_ml=4.145, word_reposition=0.711, kpe=0.862, ppl=108.88, wps=7035.7, ups=0.34, wpb=20503.6, bsz=256, num_updates=15000, lr=0.000288675, gnorm=1.539, clip=0, loss_scale=256, train_wall=253, wall=44931
2022-07-13 03:13:17 | INFO | train_inner | epoch 014:    536 / 1122 loss=6.687, nll_loss=2.455, mask_ins=1.037, word_ins_ml=4.094, word_reposition=0.702, kpe=0.853, ppl=103.03, wps=7027.3, ups=0.34, wpb=20496.6, bsz=256, num_updates=15100, lr=0.000287718, gnorm=1.377, clip=0, loss_scale=492, train_wall=253, wall=45223
2022-07-13 03:18:04 | INFO | train_inner | epoch 014:    636 / 1122 loss=6.715, nll_loss=2.49, mask_ins=1.035, word_ins_ml=4.125, word_reposition=0.699, kpe=0.856, ppl=105.02, wps=7145.8, ups=0.35, wpb=20538.6, bsz=256, num_updates=15200, lr=0.00028677, gnorm=1.382, clip=0, loss_scale=512, train_wall=250, wall=45510
2022-07-13 03:22:51 | INFO | train_inner | epoch 014:    736 / 1122 loss=6.716, nll_loss=2.489, mask_ins=1.033, word_ins_ml=4.124, word_reposition=0.697, kpe=0.862, ppl=105.09, wps=7196.2, ups=0.35, wpb=20617.9, bsz=256, num_updates=15300, lr=0.000285831, gnorm=1.384, clip=0, loss_scale=512, train_wall=249, wall=45797
2022-07-13 03:27:37 | INFO | train_inner | epoch 014:    836 / 1122 loss=6.785, nll_loss=2.548, mask_ins=1.046, word_ins_ml=4.176, word_reposition=0.698, kpe=0.864, ppl=110.26, wps=7121.8, ups=0.35, wpb=20408.2, bsz=256, num_updates=15400, lr=0.000284901, gnorm=1.392, clip=0, loss_scale=512, train_wall=249, wall=46083
2022-07-13 03:32:24 | INFO | train_inner | epoch 014:    936 / 1122 loss=6.752, nll_loss=2.517, mask_ins=1.035, word_ins_ml=4.148, word_reposition=0.703, kpe=0.865, ppl=107.77, wps=7159.9, ups=0.35, wpb=20511.4, bsz=256, num_updates=15500, lr=0.000283981, gnorm=1.444, clip=0, loss_scale=512, train_wall=249, wall=46370
2022-07-13 03:37:11 | INFO | train_inner | epoch 014:   1036 / 1122 loss=6.733, nll_loss=2.486, mask_ins=1.039, word_ins_ml=4.122, word_reposition=0.711, kpe=0.861, ppl=106.35, wps=7164.1, ups=0.35, wpb=20562.6, bsz=256, num_updates=15600, lr=0.000283069, gnorm=1.424, clip=0, loss_scale=922, train_wall=249, wall=46657
2022-07-13 03:41:16 | INFO | train | epoch 014 | loss nan | nll_loss 2.505 | mask_ins 1.041 | word_ins_ml 4.138 | word_reposition 0.705 | kpe nan | ppl nan | wps 6865 | ups 0.33 | wpb 20521.1 | bsz 255.8 | num_updates 15686 | lr 0.000282292 | gnorm 1.408 | clip 0 | loss_scale 486 | train_wall 2843 | wall 46902
2022-07-13 03:42:35 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 12.458 | nll_loss 6.403 | mask_ins 2.023 | word_ins_ml 7.737 | word_reposition 1.328 | kpe 1.37 | ppl 5627.03 | wps 12515.1 | wpb 2367.6 | bsz 32 | num_updates 15686 | best_loss 12.458
2022-07-13 03:42:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_best.pt (epoch 14 @ 15686 updates, score 12.458) (writing took 7.01013087388128 seconds)
2022-07-13 03:43:22 | INFO | train_inner | epoch 015:     14 / 1122 loss=nan, nll_loss=2.518, mask_ins=1.04, word_ins_ml=4.149, word_reposition=0.705, kpe=nan, ppl=nan, wps=5511.1, ups=0.27, wpb=20468.3, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=1.444, clip=0, loss_scale=1024, train_wall=248, wall=47028
2022-07-13 03:48:08 | INFO | train_inner | epoch 015:    114 / 1122 loss=6.722, nll_loss=2.492, mask_ins=1.044, word_ins_ml=4.127, word_reposition=0.705, kpe=0.847, ppl=105.56, wps=7214.3, ups=0.35, wpb=20638.2, bsz=256, num_updates=15800, lr=0.000281272, gnorm=1.388, clip=0, loss_scale=1024, train_wall=248, wall=47314
2022-07-13 03:52:55 | INFO | train_inner | epoch 015:    214 / 1122 loss=6.661, nll_loss=2.469, mask_ins=1.017, word_ins_ml=4.105, word_reposition=0.694, kpe=0.845, ppl=101.22, wps=7183.3, ups=0.35, wpb=20577.2, bsz=256, num_updates=15900, lr=0.000280386, gnorm=1.402, clip=0, loss_scale=1024, train_wall=249, wall=47601
2022-07-13 03:57:41 | INFO | train_inner | epoch 015:    314 / 1122 loss=nan, nll_loss=2.447, mask_ins=1.022, word_ins_ml=4.086, word_reposition=0.702, kpe=nan, ppl=nan, wps=7180.5, ups=0.35, wpb=20536.6, bsz=256, num_updates=16000, lr=0.000279508, gnorm=1.373, clip=0, loss_scale=1024, train_wall=248, wall=47887
2022-07-13 04:03:12 | INFO | train_inner | epoch 015:    414 / 1122 loss=6.684, nll_loss=2.471, mask_ins=1.031, word_ins_ml=4.108, word_reposition=0.697, kpe=0.849, ppl=102.82, wps=6192.8, ups=0.3, wpb=20532.1, bsz=256, num_updates=16100, lr=0.000278639, gnorm=1.402, clip=0, loss_scale=1720, train_wall=294, wall=48218
2022-07-13 04:07:59 | INFO | train_inner | epoch 015:    514 / 1122 loss=6.704, nll_loss=2.485, mask_ins=1.032, word_ins_ml=4.12, word_reposition=0.7, kpe=0.853, ppl=104.25, wps=7154, ups=0.35, wpb=20493.1, bsz=256, num_updates=16200, lr=0.000277778, gnorm=1.426, clip=0, loss_scale=2048, train_wall=249, wall=48505
2022-07-13 04:09:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-13 04:12:48 | INFO | train_inner | epoch 015:    615 / 1122 loss=nan, nll_loss=2.496, mask_ins=1.023, word_ins_ml=4.13, word_reposition=0.693, kpe=nan, ppl=nan, wps=7094.8, ups=0.35, wpb=20502.6, bsz=256, num_updates=16300, lr=0.000276924, gnorm=1.397, clip=0, loss_scale=1308, train_wall=251, wall=48794
2022-07-13 04:17:35 | INFO | train_inner | epoch 015:    715 / 1122 loss=6.694, nll_loss=2.492, mask_ins=1.021, word_ins_ml=4.125, word_reposition=0.695, kpe=0.852, ppl=103.54, wps=7145.4, ups=0.35, wpb=20502.3, bsz=256, num_updates=16400, lr=0.000276079, gnorm=1.41, clip=0, loss_scale=1024, train_wall=249, wall=49081
2022-07-13 04:22:21 | INFO | train_inner | epoch 015:    815 / 1122 loss=6.681, nll_loss=2.485, mask_ins=1.013, word_ins_ml=4.119, word_reposition=0.692, kpe=0.857, ppl=102.58, wps=7181, ups=0.35, wpb=20547.3, bsz=256, num_updates=16500, lr=0.000275241, gnorm=1.385, clip=0, loss_scale=1024, train_wall=249, wall=49367
2022-07-13 04:27:07 | INFO | train_inner | epoch 015:    915 / 1122 loss=6.699, nll_loss=2.49, mask_ins=1.028, word_ins_ml=4.124, word_reposition=0.696, kpe=0.851, ppl=103.9, wps=7154, ups=0.35, wpb=20450.5, bsz=256, num_updates=16600, lr=0.000274411, gnorm=1.448, clip=0, loss_scale=1024, train_wall=249, wall=49653
2022-07-13 04:31:54 | INFO | train_inner | epoch 015:   1015 / 1122 loss=6.686, nll_loss=2.479, mask_ins=1.023, word_ins_ml=4.114, word_reposition=0.695, kpe=0.854, ppl=102.98, wps=7146.1, ups=0.35, wpb=20537.3, bsz=256, num_updates=16700, lr=0.000273588, gnorm=1.39, clip=0, loss_scale=1024, train_wall=250, wall=49940
2022-07-13 04:36:41 | INFO | train_inner | epoch 015:   1115 / 1122 loss=6.636, nll_loss=2.44, mask_ins=1.017, word_ins_ml=4.079, word_reposition=0.69, kpe=0.85, ppl=99.44, wps=7177.1, ups=0.35, wpb=20598.7, bsz=256, num_updates=16800, lr=0.000272772, gnorm=1.389, clip=0, loss_scale=1649, train_wall=250, wall=50227
2022-07-13 04:37:00 | INFO | train | epoch 015 | loss nan | nll_loss 2.478 | mask_ins 1.025 | word_ins_ml 4.113 | word_reposition 0.696 | kpe nan | ppl nan | wps 6878.9 | ups 0.34 | wpb 20520.6 | bsz 255.8 | num_updates 16807 | lr 0.000272716 | gnorm 1.407 | clip 0 | loss_scale 1265 | train_wall 2836 | wall 50246
2022-07-13 04:38:19 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 12.634 | nll_loss 6.425 | mask_ins 2.042 | word_ins_ml 7.753 | word_reposition 1.384 | kpe 1.456 | ppl 6357.89 | wps 12503.2 | wpb 2367.6 | bsz 32 | num_updates 16807 | best_loss 12.458
2022-07-13 04:38:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 15 @ 16807 updates, score 12.634) (writing took 3.691593653522432 seconds)
2022-07-13 04:42:52 | INFO | train_inner | epoch 016:     93 / 1122 loss=nan, nll_loss=2.44, mask_ins=1.011, word_ins_ml=4.08, word_reposition=0.701, kpe=nan, ppl=nan, wps=5463.7, ups=0.27, wpb=20279.3, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=1.43, clip=0, loss_scale=2048, train_wall=251, wall=50598
2022-07-13 04:47:40 | INFO | train_inner | epoch 016:    193 / 1122 loss=6.663, nll_loss=2.483, mask_ins=1.016, word_ins_ml=4.117, word_reposition=0.695, kpe=0.834, ppl=101.32, wps=7125.8, ups=0.35, wpb=20489.7, bsz=256, num_updates=17000, lr=0.000271163, gnorm=1.383, clip=0, loss_scale=2048, train_wall=250, wall=50886
2022-07-13 04:52:28 | INFO | train_inner | epoch 016:    293 / 1122 loss=6.624, nll_loss=2.42, mask_ins=1.024, word_ins_ml=4.062, word_reposition=0.701, kpe=0.838, ppl=98.66, wps=7159.6, ups=0.35, wpb=20653.1, bsz=256, num_updates=17100, lr=0.000270369, gnorm=1.379, clip=0, loss_scale=2048, train_wall=251, wall=51174
2022-07-13 04:57:17 | INFO | train_inner | epoch 016:    393 / 1122 loss=6.632, nll_loss=2.448, mask_ins=1.017, word_ins_ml=4.086, word_reposition=0.691, kpe=0.838, ppl=99.21, wps=7092.7, ups=0.35, wpb=20446.1, bsz=256, num_updates=17200, lr=0.000269582, gnorm=1.374, clip=0, loss_scale=2048, train_wall=250, wall=51462
2022-07-13 05:02:09 | INFO | train_inner | epoch 016:    493 / 1122 loss=6.617, nll_loss=2.434, mask_ins=1.015, word_ins_ml=4.074, word_reposition=0.687, kpe=0.841, ppl=98.17, wps=7047, ups=0.34, wpb=20622.6, bsz=256, num_updates=17300, lr=0.000268802, gnorm=1.371, clip=0, loss_scale=3052, train_wall=254, wall=51755
2022-07-13 05:07:45 | INFO | train_inner | epoch 016:    593 / 1122 loss=6.627, nll_loss=2.45, mask_ins=1.007, word_ins_ml=4.087, word_reposition=0.692, kpe=0.841, ppl=98.84, wps=6125.6, ups=0.3, wpb=20549.5, bsz=256, num_updates=17400, lr=0.000268028, gnorm=1.378, clip=0, loss_scale=4096, train_wall=297, wall=52091
2022-07-13 05:12:35 | INFO | train_inner | epoch 016:    693 / 1122 loss=6.59, nll_loss=2.419, mask_ins=0.999, word_ins_ml=4.061, word_reposition=0.687, kpe=0.843, ppl=96.33, wps=7086.7, ups=0.34, wpb=20586, bsz=256, num_updates=17500, lr=0.000267261, gnorm=1.4, clip=0, loss_scale=4096, train_wall=252, wall=52381
2022-07-13 05:17:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 05:17:29 | INFO | train_inner | epoch 016:    794 / 1122 loss=6.626, nll_loss=2.456, mask_ins=1.006, word_ins_ml=4.093, word_reposition=0.684, kpe=0.844, ppl=98.79, wps=6985.1, ups=0.34, wpb=20506.2, bsz=256, num_updates=17600, lr=0.000266501, gnorm=1.419, clip=0, loss_scale=3974, train_wall=255, wall=52675
2022-07-13 05:22:18 | INFO | train_inner | epoch 016:    894 / 1122 loss=6.633, nll_loss=2.447, mask_ins=1.01, word_ins_ml=4.085, word_reposition=0.692, kpe=0.846, ppl=99.24, wps=7120.3, ups=0.35, wpb=20575.1, bsz=256, num_updates=17700, lr=0.000265747, gnorm=1.393, clip=0, loss_scale=2048, train_wall=251, wall=52964
2022-07-13 05:27:06 | INFO | train_inner | epoch 016:    994 / 1122 loss=6.643, nll_loss=2.444, mask_ins=1.013, word_ins_ml=4.082, word_reposition=0.701, kpe=0.846, ppl=99.92, wps=7120, ups=0.35, wpb=20502, bsz=256, num_updates=17800, lr=0.000264999, gnorm=1.422, clip=0, loss_scale=2048, train_wall=250, wall=53252
2022-07-13 05:31:52 | INFO | train_inner | epoch 016:   1094 / 1122 loss=nan, nll_loss=2.451, mask_ins=1.012, word_ins_ml=4.088, word_reposition=0.688, kpe=nan, ppl=nan, wps=7161.4, ups=0.35, wpb=20513.8, bsz=256, num_updates=17900, lr=0.000264258, gnorm=1.402, clip=0, loss_scale=2048, train_wall=248, wall=53538
2022-07-13 05:33:12 | INFO | train | epoch 016 | loss nan | nll_loss 2.446 | mask_ins 1.011 | word_ins_ml 4.084 | word_reposition 0.693 | kpe nan | ppl nan | wps 6823.3 | ups 0.33 | wpb 20520 | bsz 255.8 | num_updates 17928 | lr 0.000264052 | gnorm 1.394 | clip 0 | loss_scale 2676 | train_wall 2863 | wall 53617
2022-07-13 05:34:31 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 12.538 | nll_loss 6.385 | mask_ins 2.01 | word_ins_ml 7.715 | word_reposition 1.398 | kpe 1.416 | ppl 5947.01 | wps 12509.8 | wpb 2367.6 | bsz 32 | num_updates 17928 | best_loss 12.458
2022-07-13 05:34:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 16 @ 17928 updates, score 12.538) (writing took 3.6240232698619366 seconds)
2022-07-13 05:38:01 | INFO | train_inner | epoch 017:     72 / 1122 loss=6.62, nll_loss=2.449, mask_ins=1.01, word_ins_ml=4.086, word_reposition=0.692, kpe=0.832, ppl=98.35, wps=5551.5, ups=0.27, wpb=20446.3, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=1.47, clip=0, loss_scale=2048, train_wall=248, wall=53906
2022-07-13 05:42:51 | INFO | train_inner | epoch 017:    172 / 1122 loss=6.617, nll_loss=2.457, mask_ins=1.011, word_ins_ml=4.094, word_reposition=0.685, kpe=0.827, ppl=98.15, wps=7056.4, ups=0.34, wpb=20498.3, bsz=256, num_updates=18100, lr=0.000262794, gnorm=1.38, clip=0, loss_scale=2048, train_wall=252, wall=54197
2022-07-13 05:45:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 05:47:48 | INFO | train_inner | epoch 017:    273 / 1122 loss=6.59, nll_loss=2.42, mask_ins=1.011, word_ins_ml=4.061, word_reposition=0.69, kpe=0.827, ppl=96.36, wps=6899.8, ups=0.34, wpb=20481.2, bsz=256, num_updates=18200, lr=0.000262071, gnorm=1.413, clip=0, loss_scale=3204, train_wall=257, wall=54494
2022-07-13 05:52:35 | INFO | train_inner | epoch 017:    373 / 1122 loss=6.609, nll_loss=2.436, mask_ins=1.008, word_ins_ml=4.075, word_reposition=0.693, kpe=0.834, ppl=97.64, wps=7099.5, ups=0.35, wpb=20411.6, bsz=256, num_updates=18300, lr=0.000261354, gnorm=1.376, clip=0, loss_scale=2048, train_wall=250, wall=54781
2022-07-13 05:57:24 | INFO | train_inner | epoch 017:    473 / 1122 loss=6.607, nll_loss=2.444, mask_ins=1.003, word_ins_ml=4.082, word_reposition=0.692, kpe=0.83, ppl=97.47, wps=7105.7, ups=0.35, wpb=20492.9, bsz=256, num_updates=18400, lr=0.000260643, gnorm=1.402, clip=0, loss_scale=2048, train_wall=250, wall=55070
2022-07-13 06:02:11 | INFO | train_inner | epoch 017:    573 / 1122 loss=6.546, nll_loss=2.398, mask_ins=0.994, word_ins_ml=4.041, word_reposition=0.681, kpe=0.829, ppl=93.42, wps=7138.7, ups=0.35, wpb=20500.5, bsz=256, num_updates=18500, lr=0.000259938, gnorm=1.423, clip=0, loss_scale=2048, train_wall=249, wall=55357
2022-07-13 06:07:02 | INFO | train_inner | epoch 017:    673 / 1122 loss=6.555, nll_loss=2.39, mask_ins=0.997, word_ins_ml=4.034, word_reposition=0.691, kpe=0.833, ppl=94.05, wps=7092.1, ups=0.34, wpb=20622.9, bsz=256, num_updates=18600, lr=0.000259238, gnorm=1.369, clip=0, loss_scale=2048, train_wall=252, wall=55648
2022-07-13 06:12:37 | INFO | train_inner | epoch 017:    773 / 1122 loss=6.591, nll_loss=2.424, mask_ins=1, word_ins_ml=4.063, word_reposition=0.695, kpe=0.832, ppl=96.38, wps=6194.7, ups=0.3, wpb=20743.5, bsz=256, num_updates=18700, lr=0.000258544, gnorm=1.403, clip=0, loss_scale=2580, train_wall=297, wall=55982
2022-07-13 06:17:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 06:17:30 | INFO | train_inner | epoch 017:    874 / 1122 loss=6.574, nll_loss=2.414, mask_ins=1.001, word_ins_ml=4.055, word_reposition=0.684, kpe=0.833, ppl=95.27, wps=6974.8, ups=0.34, wpb=20458.2, bsz=256, num_updates=18800, lr=0.000257855, gnorm=1.392, clip=0, loss_scale=4055, train_wall=255, wall=56276
2022-07-13 06:22:18 | INFO | train_inner | epoch 017:    974 / 1122 loss=6.567, nll_loss=2.415, mask_ins=0.996, word_ins_ml=4.056, word_reposition=0.682, kpe=0.833, ppl=94.82, wps=7118.8, ups=0.35, wpb=20495.8, bsz=256, num_updates=18900, lr=0.000257172, gnorm=1.36, clip=0, loss_scale=2048, train_wall=250, wall=56564
2022-07-13 06:27:06 | INFO | train_inner | epoch 017:   1074 / 1122 loss=nan, nll_loss=2.402, mask_ins=0.992, word_ins_ml=4.044, word_reposition=0.679, kpe=nan, ppl=nan, wps=7120.6, ups=0.35, wpb=20549.3, bsz=256, num_updates=19000, lr=0.000256495, gnorm=1.448, clip=0, loss_scale=2048, train_wall=250, wall=56852
2022-07-13 06:29:23 | INFO | train | epoch 017 | loss nan | nll_loss 2.419 | mask_ins 1.002 | word_ins_ml 4.06 | word_reposition 0.687 | kpe nan | ppl nan | wps 6817.1 | ups 0.33 | wpb 20518.8 | bsz 255.8 | num_updates 19048 | lr 0.000256171 | gnorm 1.405 | clip 0 | loss_scale 2380 | train_wall 2860 | wall 56988
2022-07-13 06:30:42 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.895 | nll_loss 6.553 | mask_ins 2.068 | word_ins_ml 7.885 | word_reposition 1.453 | kpe 1.49 | ppl 7618.71 | wps 12533 | wpb 2367.6 | bsz 32 | num_updates 19048 | best_loss 12.458
2022-07-13 06:30:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 17 @ 19048 updates, score 12.895) (writing took 3.6655769739300013 seconds)
2022-07-13 06:33:16 | INFO | train_inner | epoch 018:     52 / 1122 loss=nan, nll_loss=2.413, mask_ins=0.998, word_ins_ml=4.055, word_reposition=0.683, kpe=nan, ppl=nan, wps=5499.9, ups=0.27, wpb=20306.8, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=1.513, clip=0, loss_scale=2048, train_wall=249, wall=57221
2022-07-13 06:38:03 | INFO | train_inner | epoch 018:    152 / 1122 loss=6.505, nll_loss=2.384, mask_ins=0.993, word_ins_ml=4.029, word_reposition=0.667, kpe=0.815, ppl=90.81, wps=7146.3, ups=0.35, wpb=20561.5, bsz=256, num_updates=19200, lr=0.000255155, gnorm=1.397, clip=0, loss_scale=2048, train_wall=250, wall=57509
2022-07-13 06:42:52 | INFO | train_inner | epoch 018:    252 / 1122 loss=6.541, nll_loss=2.382, mask_ins=0.996, word_ins_ml=4.027, word_reposition=0.699, kpe=0.82, ppl=93.15, wps=7121.9, ups=0.35, wpb=20583.9, bsz=256, num_updates=19300, lr=0.000254493, gnorm=1.414, clip=0, loss_scale=2048, train_wall=250, wall=57798
2022-07-13 06:47:39 | INFO | train_inner | epoch 018:    352 / 1122 loss=6.546, nll_loss=2.416, mask_ins=0.991, word_ins_ml=4.056, word_reposition=0.679, kpe=0.82, ppl=93.44, wps=7146.7, ups=0.35, wpb=20473.8, bsz=256, num_updates=19400, lr=0.000253837, gnorm=1.441, clip=0, loss_scale=3891, train_wall=249, wall=58085
2022-07-13 06:52:26 | INFO | train_inner | epoch 018:    452 / 1122 loss=6.523, nll_loss=2.388, mask_ins=0.985, word_ins_ml=4.032, word_reposition=0.686, kpe=0.821, ppl=91.97, wps=7155.5, ups=0.35, wpb=20570.9, bsz=256, num_updates=19500, lr=0.000253185, gnorm=1.428, clip=0, loss_scale=4096, train_wall=250, wall=58372
2022-07-13 06:55:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 06:57:17 | INFO | train_inner | epoch 018:    553 / 1122 loss=6.532, nll_loss=2.41, mask_ins=0.982, word_ins_ml=4.051, word_reposition=0.674, kpe=0.824, ppl=92.51, wps=7038.8, ups=0.34, wpb=20436, bsz=256, num_updates=19600, lr=0.000252538, gnorm=1.416, clip=0, loss_scale=3244, train_wall=252, wall=58663
2022-07-13 07:02:04 | INFO | train_inner | epoch 018:    653 / 1122 loss=6.552, nll_loss=2.412, mask_ins=0.991, word_ins_ml=4.053, word_reposition=0.685, kpe=0.824, ppl=93.83, wps=7116.7, ups=0.35, wpb=20449.8, bsz=256, num_updates=19700, lr=0.000251896, gnorm=1.435, clip=0, loss_scale=2048, train_wall=250, wall=58950
2022-07-13 07:06:52 | INFO | train_inner | epoch 018:    753 / 1122 loss=nan, nll_loss=2.404, mask_ins=0.997, word_ins_ml=4.046, word_reposition=0.688, kpe=nan, ppl=nan, wps=7180.7, ups=0.35, wpb=20640.4, bsz=256, num_updates=19800, lr=0.000251259, gnorm=1.422, clip=0, loss_scale=2048, train_wall=250, wall=59237
2022-07-13 07:11:39 | INFO | train_inner | epoch 018:    853 / 1122 loss=6.541, nll_loss=2.404, mask_ins=0.99, word_ins_ml=4.045, word_reposition=0.683, kpe=0.822, ppl=93.12, wps=7189.1, ups=0.35, wpb=20692.6, bsz=256, num_updates=19900, lr=0.000250627, gnorm=1.43, clip=0, loss_scale=2048, train_wall=250, wall=59525
2022-07-13 07:17:13 | INFO | train_inner | epoch 018:    953 / 1122 loss=6.533, nll_loss=2.396, mask_ins=0.994, word_ins_ml=4.039, word_reposition=0.677, kpe=0.823, ppl=92.61, wps=6128.6, ups=0.3, wpb=20469.1, bsz=256, num_updates=20000, lr=0.00025, gnorm=1.368, clip=0, loss_scale=2048, train_wall=296, wall=59859
2022-07-13 07:22:01 | INFO | train_inner | epoch 018:   1053 / 1122 loss=6.523, nll_loss=2.387, mask_ins=0.992, word_ins_ml=4.031, word_reposition=0.676, kpe=0.824, ppl=91.95, wps=7145, ups=0.35, wpb=20518.3, bsz=256, num_updates=20100, lr=0.000249377, gnorm=1.377, clip=0, loss_scale=2662, train_wall=250, wall=60146
2022-07-13 07:22:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 07:25:19 | INFO | train | epoch 018 | loss nan | nll_loss 2.401 | mask_ins 0.992 | word_ins_ml 4.043 | word_reposition 0.682 | kpe nan | ppl nan | wps 6849 | ups 0.33 | wpb 20522.2 | bsz 255.8 | num_updates 20168 | lr 0.000248957 | gnorm 1.417 | clip 0 | loss_scale 2586 | train_wall 2848 | wall 60344
2022-07-13 07:26:38 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 12.697 | nll_loss 6.513 | mask_ins 2.02 | word_ins_ml 7.843 | word_reposition 1.403 | kpe 1.431 | ppl 6641.56 | wps 12492.3 | wpb 2367.6 | bsz 32 | num_updates 20168 | best_loss 12.458
2022-07-13 07:26:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 18 @ 20168 updates, score 12.697) (writing took 3.5235178023576736 seconds)
2022-07-13 07:28:13 | INFO | train_inner | epoch 019:     32 / 1122 loss=6.54, nll_loss=2.388, mask_ins=1, word_ins_ml=4.031, word_reposition=0.692, kpe=0.816, ppl=93.06, wps=5457.1, ups=0.27, wpb=20315.7, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=1.425, clip=0, loss_scale=2372, train_wall=252, wall=60519
2022-07-13 07:33:00 | INFO | train_inner | epoch 019:    132 / 1122 loss=6.501, nll_loss=2.4, mask_ins=0.982, word_ins_ml=4.042, word_reposition=0.675, kpe=0.802, ppl=90.56, wps=7074.9, ups=0.35, wpb=20340.2, bsz=256, num_updates=20300, lr=0.000248146, gnorm=1.423, clip=0, loss_scale=2048, train_wall=250, wall=60806
2022-07-13 07:37:47 | INFO | train_inner | epoch 019:    232 / 1122 loss=nan, nll_loss=2.387, mask_ins=0.977, word_ins_ml=4.03, word_reposition=0.674, kpe=nan, ppl=nan, wps=7166.5, ups=0.35, wpb=20537.6, bsz=256, num_updates=20400, lr=0.000247537, gnorm=1.446, clip=0, loss_scale=2048, train_wall=249, wall=61093
2022-07-13 07:42:34 | INFO | train_inner | epoch 019:    332 / 1122 loss=6.527, nll_loss=2.403, mask_ins=0.99, word_ins_ml=4.044, word_reposition=0.684, kpe=0.809, ppl=92.19, wps=7142.6, ups=0.35, wpb=20525.1, bsz=256, num_updates=20500, lr=0.000246932, gnorm=1.501, clip=0, loss_scale=2048, train_wall=249, wall=61380
2022-07-13 07:47:23 | INFO | train_inner | epoch 019:    432 / 1122 loss=6.517, nll_loss=2.388, mask_ins=0.993, word_ins_ml=4.032, word_reposition=0.681, kpe=0.811, ppl=91.59, wps=7074, ups=0.35, wpb=20384.4, bsz=256, num_updates=20600, lr=0.000246332, gnorm=1.463, clip=0, loss_scale=2048, train_wall=250, wall=61668
2022-07-13 07:52:10 | INFO | train_inner | epoch 019:    532 / 1122 loss=6.513, nll_loss=2.39, mask_ins=0.986, word_ins_ml=4.033, word_reposition=0.68, kpe=0.814, ppl=91.32, wps=7191.3, ups=0.35, wpb=20677.3, bsz=256, num_updates=20700, lr=0.000245737, gnorm=1.374, clip=0, loss_scale=3543, train_wall=249, wall=61956
2022-07-13 07:55:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 07:57:02 | INFO | train_inner | epoch 019:    633 / 1122 loss=6.511, nll_loss=2.391, mask_ins=0.985, word_ins_ml=4.034, word_reposition=0.682, kpe=0.81, ppl=91.18, wps=7066.1, ups=0.34, wpb=20601.1, bsz=256, num_updates=20800, lr=0.000245145, gnorm=1.424, clip=0, loss_scale=3407, train_wall=253, wall=62247
2022-07-13 08:01:50 | INFO | train_inner | epoch 019:    733 / 1122 loss=6.477, nll_loss=2.363, mask_ins=0.976, word_ins_ml=4.009, word_reposition=0.678, kpe=0.814, ppl=89.08, wps=7185.2, ups=0.35, wpb=20711, bsz=256, num_updates=20900, lr=0.000244558, gnorm=1.435, clip=0, loss_scale=2048, train_wall=250, wall=62536
2022-07-13 08:06:37 | INFO | train_inner | epoch 019:    833 / 1122 loss=6.495, nll_loss=2.372, mask_ins=0.988, word_ins_ml=4.017, word_reposition=0.67, kpe=0.82, ppl=90.21, wps=7148.5, ups=0.35, wpb=20552, bsz=256, num_updates=21000, lr=0.000243975, gnorm=1.449, clip=0, loss_scale=2048, train_wall=249, wall=62823
2022-07-13 08:11:25 | INFO | train_inner | epoch 019:    933 / 1122 loss=nan, nll_loss=2.357, mask_ins=0.974, word_ins_ml=4.004, word_reposition=0.667, kpe=nan, ppl=nan, wps=7100.1, ups=0.35, wpb=20446.8, bsz=256, num_updates=21100, lr=0.000243396, gnorm=1.413, clip=0, loss_scale=2048, train_wall=250, wall=63111
2022-07-13 08:16:26 | INFO | train_inner | epoch 019:   1033 / 1122 loss=6.484, nll_loss=2.368, mask_ins=0.974, word_ins_ml=4.013, word_reposition=0.678, kpe=0.818, ppl=89.49, wps=6866.6, ups=0.33, wpb=20619.1, bsz=256, num_updates=21200, lr=0.000242821, gnorm=1.466, clip=0, loss_scale=2048, train_wall=262, wall=63411
2022-07-13 08:21:03 | INFO | train | epoch 019 | loss nan | nll_loss 2.38 | mask_ins 0.983 | word_ins_ml 4.024 | word_reposition 0.678 | kpe nan | ppl nan | wps 6877.5 | ups 0.34 | wpb 20518.9 | bsz 255.8 | num_updates 21289 | lr 0.000242313 | gnorm 1.448 | clip 0 | loss_scale 2324 | train_wall 2837 | wall 63689
2022-07-13 08:22:23 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 12.714 | nll_loss 6.49 | mask_ins 2.053 | word_ins_ml 7.819 | word_reposition 1.371 | kpe 1.472 | ppl 6718.86 | wps 12360.7 | wpb 2367.6 | bsz 32 | num_updates 21289 | best_loss 12.458
2022-07-13 08:22:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 19 @ 21289 updates, score 12.714) (writing took 3.842329361476004 seconds)
2022-07-13 08:22:58 | INFO | train_inner | epoch 020:     11 / 1122 loss=6.505, nll_loss=2.376, mask_ins=0.983, word_ins_ml=4.02, word_reposition=0.686, kpe=0.817, ppl=90.85, wps=5198.9, ups=0.25, wpb=20423.6, bsz=253.8, num_updates=21300, lr=0.000242251, gnorm=1.555, clip=0, loss_scale=2499, train_wall=272, wall=63804
2022-07-13 08:27:46 | INFO | train_inner | epoch 020:    111 / 1122 loss=nan, nll_loss=2.345, mask_ins=0.967, word_ins_ml=3.993, word_reposition=0.661, kpe=nan, ppl=nan, wps=7124.5, ups=0.35, wpb=20518.1, bsz=256, num_updates=21400, lr=0.000241684, gnorm=1.422, clip=0, loss_scale=4096, train_wall=250, wall=64092
2022-07-13 08:32:34 | INFO | train_inner | epoch 020:    211 / 1122 loss=6.429, nll_loss=2.352, mask_ins=0.971, word_ins_ml=3.999, word_reposition=0.661, kpe=0.798, ppl=86.19, wps=7138.3, ups=0.35, wpb=20528.1, bsz=256, num_updates=21500, lr=0.000241121, gnorm=1.428, clip=0, loss_scale=4096, train_wall=249, wall=64380
2022-07-13 08:37:23 | INFO | train_inner | epoch 020:    311 / 1122 loss=6.456, nll_loss=2.36, mask_ins=0.978, word_ins_ml=4.006, word_reposition=0.672, kpe=0.8, ppl=87.8, wps=7138.9, ups=0.35, wpb=20590.5, bsz=256, num_updates=21600, lr=0.000240563, gnorm=1.444, clip=0, loss_scale=4096, train_wall=250, wall=64668
2022-07-13 08:42:10 | INFO | train_inner | epoch 020:    411 / 1122 loss=nan, nll_loss=2.367, mask_ins=0.974, word_ins_ml=4.012, word_reposition=0.672, kpe=nan, ppl=nan, wps=7159.7, ups=0.35, wpb=20583.9, bsz=256, num_updates=21700, lr=0.000240008, gnorm=1.468, clip=0, loss_scale=4096, train_wall=249, wall=64956
2022-07-13 08:46:59 | INFO | train_inner | epoch 020:    511 / 1122 loss=6.42, nll_loss=2.33, mask_ins=0.971, word_ins_ml=3.979, word_reposition=0.667, kpe=0.803, ppl=85.64, wps=7097.4, ups=0.35, wpb=20497.8, bsz=256, num_updates=21800, lr=0.000239457, gnorm=1.462, clip=0, loss_scale=4506, train_wall=251, wall=65245
2022-07-13 08:50:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-13 08:51:49 | INFO | train_inner | epoch 020:    612 / 1122 loss=6.468, nll_loss=2.376, mask_ins=0.974, word_ins_ml=4.019, word_reposition=0.671, kpe=0.804, ppl=88.51, wps=7090.6, ups=0.34, wpb=20609.7, bsz=256, num_updates=21900, lr=0.000238909, gnorm=1.451, clip=0, loss_scale=6935, train_wall=253, wall=65535
2022-07-13 08:52:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 08:56:39 | INFO | train_inner | epoch 020:    713 / 1122 loss=6.433, nll_loss=2.353, mask_ins=0.974, word_ins_ml=3.999, word_reposition=0.653, kpe=0.807, ppl=86.43, wps=7042.3, ups=0.35, wpb=20385.6, bsz=256, num_updates=22000, lr=0.000238366, gnorm=1.45, clip=0, loss_scale=2291, train_wall=252, wall=65825
2022-07-13 09:01:27 | INFO | train_inner | epoch 020:    813 / 1122 loss=6.418, nll_loss=2.328, mask_ins=0.971, word_ins_ml=3.977, word_reposition=0.667, kpe=0.803, ppl=85.51, wps=7110.2, ups=0.35, wpb=20490.5, bsz=256, num_updates=22100, lr=0.000237826, gnorm=1.442, clip=0, loss_scale=2048, train_wall=251, wall=66113
2022-07-13 09:06:15 | INFO | train_inner | epoch 020:    913 / 1122 loss=6.459, nll_loss=2.345, mask_ins=0.983, word_ins_ml=3.993, word_reposition=0.675, kpe=0.809, ppl=87.95, wps=7164, ups=0.35, wpb=20604.9, bsz=256, num_updates=22200, lr=0.000237289, gnorm=1.463, clip=0, loss_scale=2048, train_wall=250, wall=66401
2022-07-13 09:11:03 | INFO | train_inner | epoch 020:   1013 / 1122 loss=6.439, nll_loss=2.35, mask_ins=0.97, word_ins_ml=3.997, word_reposition=0.666, kpe=0.806, ppl=86.76, wps=7141.5, ups=0.35, wpb=20556.6, bsz=256, num_updates=22300, lr=0.000236757, gnorm=1.453, clip=0, loss_scale=2048, train_wall=250, wall=66688
2022-07-13 09:11:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-13 09:15:53 | INFO | train_inner | epoch 020:   1114 / 1122 loss=6.48, nll_loss=2.364, mask_ins=0.981, word_ins_ml=4.009, word_reposition=0.68, kpe=0.81, ppl=89.26, wps=7051, ups=0.34, wpb=20497.9, bsz=256, num_updates=22400, lr=0.000236228, gnorm=1.424, clip=0, loss_scale=1136, train_wall=252, wall=66979
2022-07-13 09:16:15 | INFO | train | epoch 020 | loss nan | nll_loss 2.352 | mask_ins 0.974 | word_ins_ml 3.999 | word_reposition 0.668 | kpe nan | ppl nan | wps 6932.3 | ups 0.34 | wpb 20520.6 | bsz 255.8 | num_updates 22408 | lr 0.000236186 | gnorm 1.452 | clip 0 | loss_scale 3390 | train_wall 2804 | wall 67001
2022-07-13 09:17:34 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 12.528 | nll_loss 6.34 | mask_ins 2.002 | word_ins_ml 7.674 | word_reposition 1.393 | kpe 1.458 | ppl 5906.66 | wps 12573.5 | wpb 2367.6 | bsz 32 | num_updates 22408 | best_loss 12.458
2022-07-13 09:17:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 20 @ 22408 updates, score 12.528) (writing took 3.6264250902459025 seconds)
2022-07-13 09:22:14 | INFO | train_inner | epoch 021:     92 / 1122 loss=6.425, nll_loss=2.339, mask_ins=0.977, word_ins_ml=3.988, word_reposition=0.673, kpe=0.788, ppl=85.92, wps=5376.6, ups=0.26, wpb=20463.7, bsz=253.8, num_updates=22500, lr=0.000235702, gnorm=1.497, clip=0, loss_scale=1024, train_wall=261, wall=67360
2022-07-13 09:27:34 | INFO | train_inner | epoch 021:    192 / 1122 loss=6.435, nll_loss=2.341, mask_ins=0.979, word_ins_ml=3.989, word_reposition=0.677, kpe=0.789, ppl=86.49, wps=6382.4, ups=0.31, wpb=20455.7, bsz=256, num_updates=22600, lr=0.00023518, gnorm=1.456, clip=0, loss_scale=1024, train_wall=282, wall=67680
2022-07-13 09:32:23 | INFO | train_inner | epoch 021:    292 / 1122 loss=6.406, nll_loss=2.338, mask_ins=0.966, word_ins_ml=3.986, word_reposition=0.665, kpe=0.789, ppl=84.8, wps=7144.6, ups=0.35, wpb=20633.5, bsz=256, num_updates=22700, lr=0.000234662, gnorm=1.461, clip=0, loss_scale=1024, train_wall=251, wall=67969
2022-07-13 09:37:11 | INFO | train_inner | epoch 021:    392 / 1122 loss=6.394, nll_loss=2.326, mask_ins=0.965, word_ins_ml=3.975, word_reposition=0.665, kpe=0.789, ppl=84.07, wps=7120.9, ups=0.35, wpb=20517.7, bsz=256, num_updates=22800, lr=0.000234146, gnorm=1.46, clip=0, loss_scale=1024, train_wall=250, wall=68257
2022-07-13 09:41:59 | INFO | train_inner | epoch 021:    492 / 1122 loss=nan, nll_loss=2.324, mask_ins=0.965, word_ins_ml=3.974, word_reposition=0.662, kpe=nan, ppl=nan, wps=7172.8, ups=0.35, wpb=20638.4, bsz=256, num_updates=22900, lr=0.000233635, gnorm=1.45, clip=0, loss_scale=1823, train_wall=250, wall=68545
2022-07-13 09:46:48 | INFO | train_inner | epoch 021:    592 / 1122 loss=6.372, nll_loss=2.309, mask_ins=0.963, word_ins_ml=3.96, word_reposition=0.658, kpe=0.792, ppl=82.85, wps=7097.7, ups=0.35, wpb=20503.2, bsz=256, num_updates=23000, lr=0.000233126, gnorm=1.445, clip=0, loss_scale=2048, train_wall=251, wall=68834
2022-07-13 09:51:36 | INFO | train_inner | epoch 021:    692 / 1122 loss=6.414, nll_loss=2.358, mask_ins=0.962, word_ins_ml=4.004, word_reposition=0.651, kpe=0.797, ppl=85.27, wps=7089.8, ups=0.35, wpb=20432.8, bsz=256, num_updates=23100, lr=0.000232621, gnorm=1.476, clip=0, loss_scale=2048, train_wall=250, wall=69122
2022-07-13 09:56:24 | INFO | train_inner | epoch 021:    792 / 1122 loss=6.387, nll_loss=2.3, mask_ins=0.961, word_ins_ml=3.952, word_reposition=0.677, kpe=0.798, ppl=83.72, wps=7141, ups=0.35, wpb=20552.8, bsz=256, num_updates=23200, lr=0.000232119, gnorm=1.439, clip=0, loss_scale=2048, train_wall=250, wall=69410
2022-07-13 10:01:11 | INFO | train_inner | epoch 021:    892 / 1122 loss=6.398, nll_loss=2.333, mask_ins=0.967, word_ins_ml=3.981, word_reposition=0.651, kpe=0.799, ppl=84.35, wps=7118.6, ups=0.35, wpb=20409.5, bsz=256, num_updates=23300, lr=0.000231621, gnorm=1.492, clip=0, loss_scale=2048, train_wall=249, wall=69696
2022-07-13 10:05:58 | INFO | train_inner | epoch 021:    992 / 1122 loss=6.419, nll_loss=2.339, mask_ins=0.964, word_ins_ml=3.987, word_reposition=0.666, kpe=0.801, ppl=85.54, wps=7179.6, ups=0.35, wpb=20591, bsz=256, num_updates=23400, lr=0.000231125, gnorm=1.481, clip=0, loss_scale=3400, train_wall=249, wall=69983
2022-07-13 10:07:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 10:10:47 | INFO | train_inner | epoch 021:   1093 / 1122 loss=6.438, nll_loss=2.364, mask_ins=0.965, word_ins_ml=4.009, word_reposition=0.667, kpe=0.797, ppl=86.68, wps=7066, ups=0.35, wpb=20475.4, bsz=256, num_updates=23500, lr=0.000230633, gnorm=1.424, clip=0, loss_scale=2514, train_wall=252, wall=70273
2022-07-13 10:12:09 | INFO | train | epoch 021 | loss nan | nll_loss 2.333 | mask_ins 0.966 | word_ins_ml 3.982 | word_reposition 0.665 | kpe nan | ppl nan | wps 6858.6 | ups 0.33 | wpb 20520 | bsz 255.8 | num_updates 23529 | lr 0.000230491 | gnorm 1.459 | clip 0 | loss_scale 1833 | train_wall 2847 | wall 70355
2022-07-13 10:13:28 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 12.624 | nll_loss 6.421 | mask_ins 2.026 | word_ins_ml 7.755 | word_reposition 1.368 | kpe 1.475 | ppl 6310.35 | wps 12540.4 | wpb 2367.6 | bsz 32 | num_updates 23529 | best_loss 12.458
2022-07-13 10:13:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 21 @ 23529 updates, score 12.624) (writing took 3.990967595949769 seconds)
2022-07-13 10:16:55 | INFO | train_inner | epoch 022:     71 / 1122 loss=6.405, nll_loss=2.338, mask_ins=0.962, word_ins_ml=3.985, word_reposition=0.669, kpe=0.788, ppl=84.72, wps=5527.5, ups=0.27, wpb=20341.5, bsz=253.8, num_updates=23600, lr=0.000230144, gnorm=1.48, clip=0, loss_scale=2048, train_wall=248, wall=70641
2022-07-13 10:21:42 | INFO | train_inner | epoch 022:    171 / 1122 loss=6.335, nll_loss=2.29, mask_ins=0.957, word_ins_ml=3.943, word_reposition=0.661, kpe=0.773, ppl=80.7, wps=7172.2, ups=0.35, wpb=20554.7, bsz=256, num_updates=23700, lr=0.000229658, gnorm=1.483, clip=0, loss_scale=2048, train_wall=249, wall=70928
2022-07-13 10:26:40 | INFO | train_inner | epoch 022:    271 / 1122 loss=6.333, nll_loss=2.284, mask_ins=0.954, word_ins_ml=3.937, word_reposition=0.664, kpe=0.778, ppl=80.6, wps=6922.6, ups=0.34, wpb=20622.8, bsz=256, num_updates=23800, lr=0.000229175, gnorm=1.417, clip=0, loss_scale=2048, train_wall=260, wall=71226
2022-07-13 10:32:00 | INFO | train_inner | epoch 022:    371 / 1122 loss=6.325, nll_loss=2.299, mask_ins=0.945, word_ins_ml=3.951, word_reposition=0.654, kpe=0.775, ppl=80.2, wps=6455.3, ups=0.31, wpb=20635.9, bsz=256, num_updates=23900, lr=0.000228695, gnorm=1.461, clip=0, loss_scale=2048, train_wall=282, wall=71545
2022-07-13 10:36:46 | INFO | train_inner | epoch 022:    471 / 1122 loss=6.362, nll_loss=2.323, mask_ins=0.955, word_ins_ml=3.972, word_reposition=0.652, kpe=0.783, ppl=82.26, wps=7193.1, ups=0.35, wpb=20627.9, bsz=256, num_updates=24000, lr=0.000228218, gnorm=1.448, clip=0, loss_scale=3400, train_wall=249, wall=71832
2022-07-13 10:38:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 10:41:36 | INFO | train_inner | epoch 022:    572 / 1122 loss=6.35, nll_loss=2.307, mask_ins=0.96, word_ins_ml=3.958, word_reposition=0.649, kpe=0.784, ppl=81.57, wps=7086.3, ups=0.35, wpb=20519.2, bsz=256, num_updates=24100, lr=0.000227744, gnorm=1.45, clip=0, loss_scale=2839, train_wall=252, wall=72122
2022-07-13 10:46:22 | INFO | train_inner | epoch 022:    672 / 1122 loss=6.386, nll_loss=2.327, mask_ins=0.961, word_ins_ml=3.976, word_reposition=0.663, kpe=0.787, ppl=83.66, wps=7190.3, ups=0.35, wpb=20578, bsz=256, num_updates=24200, lr=0.000227273, gnorm=1.476, clip=0, loss_scale=2048, train_wall=249, wall=72408
2022-07-13 10:51:09 | INFO | train_inner | epoch 022:    772 / 1122 loss=6.328, nll_loss=2.308, mask_ins=0.94, word_ins_ml=3.958, word_reposition=0.644, kpe=0.786, ppl=80.33, wps=7158.8, ups=0.35, wpb=20533.8, bsz=256, num_updates=24300, lr=0.000226805, gnorm=1.479, clip=0, loss_scale=2048, train_wall=249, wall=72695
2022-07-13 10:55:56 | INFO | train_inner | epoch 022:    872 / 1122 loss=6.368, nll_loss=2.31, mask_ins=0.955, word_ins_ml=3.96, word_reposition=0.664, kpe=0.789, ppl=82.59, wps=7112.7, ups=0.35, wpb=20399, bsz=256, num_updates=24400, lr=0.000226339, gnorm=1.44, clip=0, loss_scale=2048, train_wall=249, wall=72981
2022-07-13 11:00:43 | INFO | train_inner | epoch 022:    972 / 1122 loss=nan, nll_loss=2.302, mask_ins=0.956, word_ins_ml=3.954, word_reposition=0.655, kpe=nan, ppl=nan, wps=7172.9, ups=0.35, wpb=20630, bsz=256, num_updates=24500, lr=0.000225877, gnorm=1.43, clip=0, loss_scale=2048, train_wall=250, wall=73269
2022-07-13 11:05:30 | INFO | train_inner | epoch 022:   1072 / 1122 loss=6.401, nll_loss=2.343, mask_ins=0.964, word_ins_ml=3.989, word_reposition=0.658, kpe=0.789, ppl=84.5, wps=7118.5, ups=0.35, wpb=20420.8, bsz=256, num_updates=24600, lr=0.000225417, gnorm=1.477, clip=0, loss_scale=3072, train_wall=250, wall=73556
2022-07-13 11:07:53 | INFO | train | epoch 022 | loss nan | nll_loss 2.312 | mask_ins 0.956 | word_ins_ml 3.963 | word_reposition 0.657 | kpe nan | ppl nan | wps 6880.9 | ups 0.34 | wpb 20521.9 | bsz 255.8 | num_updates 24650 | lr 0.000225189 | gnorm 1.461 | clip 0 | loss_scale 2422 | train_wall 2839 | wall 73698
2022-07-13 11:09:11 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 12.641 | nll_loss 6.44 | mask_ins 2.018 | word_ins_ml 7.771 | word_reposition 1.42 | kpe 1.432 | ppl 6386.93 | wps 12615 | wpb 2367.6 | bsz 32 | num_updates 24650 | best_loss 12.458
2022-07-13 11:09:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 22 @ 24650 updates, score 12.641) (writing took 3.6120004300028086 seconds)
2022-07-13 11:11:38 | INFO | train_inner | epoch 023:     50 / 1122 loss=6.358, nll_loss=2.322, mask_ins=0.962, word_ins_ml=3.971, word_reposition=0.646, kpe=0.779, ppl=82.04, wps=5506, ups=0.27, wpb=20241.8, bsz=253.8, num_updates=24700, lr=0.000224961, gnorm=1.5, clip=0, loss_scale=4096, train_wall=248, wall=73924
2022-07-13 11:16:25 | INFO | train_inner | epoch 023:    150 / 1122 loss=6.285, nll_loss=2.27, mask_ins=0.942, word_ins_ml=3.925, word_reposition=0.652, kpe=0.765, ppl=77.96, wps=7209.3, ups=0.35, wpb=20689.2, bsz=256, num_updates=24800, lr=0.000224507, gnorm=1.42, clip=0, loss_scale=4096, train_wall=249, wall=74211
2022-07-13 11:21:11 | INFO | train_inner | epoch 023:    250 / 1122 loss=6.326, nll_loss=2.289, mask_ins=0.954, word_ins_ml=3.942, word_reposition=0.663, kpe=0.767, ppl=80.22, wps=7159, ups=0.35, wpb=20515.8, bsz=256, num_updates=24900, lr=0.000224055, gnorm=1.467, clip=0, loss_scale=4096, train_wall=248, wall=74497
2022-07-13 11:25:58 | INFO | train_inner | epoch 023:    350 / 1122 loss=6.339, nll_loss=2.31, mask_ins=0.955, word_ins_ml=3.961, word_reposition=0.65, kpe=0.773, ppl=80.93, wps=7161.2, ups=0.35, wpb=20554.7, bsz=256, num_updates=25000, lr=0.000223607, gnorm=1.46, clip=0, loss_scale=4096, train_wall=249, wall=74784
2022-07-13 11:30:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-13 11:31:00 | INFO | train_inner | epoch 023:    451 / 1122 loss=6.338, nll_loss=2.308, mask_ins=0.95, word_ins_ml=3.959, word_reposition=0.658, kpe=0.771, ppl=80.91, wps=6807.2, ups=0.33, wpb=20496.6, bsz=256, num_updates=25100, lr=0.000223161, gnorm=1.498, clip=0, loss_scale=4907, train_wall=263, wall=75085
2022-07-13 11:36:21 | INFO | train_inner | epoch 023:    551 / 1122 loss=nan, nll_loss=2.276, mask_ins=0.95, word_ins_ml=3.93, word_reposition=0.648, kpe=nan, ppl=nan, wps=6391.9, ups=0.31, wpb=20535.5, bsz=256, num_updates=25200, lr=0.000222718, gnorm=1.441, clip=0, loss_scale=4096, train_wall=283, wall=75407
2022-07-13 11:41:07 | INFO | train_inner | epoch 023:    651 / 1122 loss=nan, nll_loss=2.313, mask_ins=0.957, word_ins_ml=3.962, word_reposition=0.665, kpe=nan, ppl=nan, wps=7177.5, ups=0.35, wpb=20566.7, bsz=256, num_updates=25300, lr=0.000222277, gnorm=1.457, clip=0, loss_scale=4096, train_wall=248, wall=75693
2022-07-13 11:45:54 | INFO | train_inner | epoch 023:    751 / 1122 loss=6.329, nll_loss=2.291, mask_ins=0.95, word_ins_ml=3.943, word_reposition=0.655, kpe=0.781, ppl=80.38, wps=7168.1, ups=0.35, wpb=20547, bsz=256, num_updates=25400, lr=0.000221839, gnorm=1.53, clip=0, loss_scale=4096, train_wall=249, wall=75980
2022-07-13 11:50:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 11:50:43 | INFO | train_inner | epoch 023:    852 / 1122 loss=6.34, nll_loss=2.307, mask_ins=0.947, word_ins_ml=3.957, word_reposition=0.657, kpe=0.778, ppl=81.01, wps=7108.7, ups=0.35, wpb=20532.6, bsz=256, num_updates=25500, lr=0.000221404, gnorm=1.5, clip=0, loss_scale=3812, train_wall=251, wall=76269
2022-07-13 11:55:30 | INFO | train_inner | epoch 023:    952 / 1122 loss=6.332, nll_loss=2.288, mask_ins=0.953, word_ins_ml=3.94, word_reposition=0.662, kpe=0.777, ppl=80.58, wps=7190.6, ups=0.35, wpb=20612.2, bsz=256, num_updates=25600, lr=0.000220971, gnorm=1.485, clip=0, loss_scale=2048, train_wall=249, wall=76555
2022-07-13 12:00:16 | INFO | train_inner | epoch 023:   1052 / 1122 loss=6.322, nll_loss=2.299, mask_ins=0.945, word_ins_ml=3.95, word_reposition=0.649, kpe=0.778, ppl=79.98, wps=7120.1, ups=0.35, wpb=20421.7, bsz=256, num_updates=25700, lr=0.000220541, gnorm=1.437, clip=0, loss_scale=2048, train_wall=249, wall=76842
2022-07-13 12:03:36 | INFO | train | epoch 023 | loss nan | nll_loss 2.296 | mask_ins 0.951 | word_ins_ml 3.948 | word_reposition 0.654 | kpe nan | ppl nan | wps 6873.8 | ups 0.33 | wpb 20521 | bsz 255.8 | num_updates 25770 | lr 0.000220241 | gnorm 1.474 | clip 0 | loss_scale 3651 | train_wall 2838 | wall 77042
2022-07-13 12:04:55 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 12.5 | nll_loss 6.357 | mask_ins 2.024 | word_ins_ml 7.697 | word_reposition 1.32 | kpe 1.458 | ppl 5790.79 | wps 12588.2 | wpb 2367.6 | bsz 32 | num_updates 25770 | best_loss 12.458
2022-07-13 12:04:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 23 @ 25770 updates, score 12.5) (writing took 3.851701314561069 seconds)
2022-07-13 12:06:25 | INFO | train_inner | epoch 024:     30 / 1122 loss=6.307, nll_loss=2.277, mask_ins=0.953, word_ins_ml=3.931, word_reposition=0.65, kpe=0.773, ppl=79.16, wps=5497.1, ups=0.27, wpb=20241.7, bsz=253.8, num_updates=25800, lr=0.000220113, gnorm=1.53, clip=0, loss_scale=2048, train_wall=249, wall=77210
2022-07-13 12:11:11 | INFO | train_inner | epoch 024:    130 / 1122 loss=nan, nll_loss=2.274, mask_ins=0.945, word_ins_ml=3.929, word_reposition=0.647, kpe=nan, ppl=nan, wps=7161.8, ups=0.35, wpb=20490.9, bsz=256, num_updates=25900, lr=0.000219687, gnorm=1.492, clip=0, loss_scale=2048, train_wall=249, wall=77496
2022-07-13 12:15:57 | INFO | train_inner | epoch 024:    230 / 1122 loss=6.304, nll_loss=2.279, mask_ins=0.947, word_ins_ml=3.933, word_reposition=0.663, kpe=0.761, ppl=79.01, wps=7153.9, ups=0.35, wpb=20502.2, bsz=256, num_updates=26000, lr=0.000219265, gnorm=1.506, clip=0, loss_scale=2089, train_wall=249, wall=77783
2022-07-13 12:20:43 | INFO | train_inner | epoch 024:    330 / 1122 loss=6.288, nll_loss=2.266, mask_ins=0.946, word_ins_ml=3.921, word_reposition=0.66, kpe=0.762, ppl=78.16, wps=7164.2, ups=0.35, wpb=20501.7, bsz=256, num_updates=26100, lr=0.000218844, gnorm=1.494, clip=0, loss_scale=4096, train_wall=249, wall=78069
2022-07-13 12:22:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 12:25:34 | INFO | train_inner | epoch 024:    431 / 1122 loss=nan, nll_loss=2.294, mask_ins=0.957, word_ins_ml=3.945, word_reposition=0.665, kpe=nan, ppl=nan, wps=7098.5, ups=0.34, wpb=20592.7, bsz=256, num_updates=26200, lr=0.000218426, gnorm=1.504, clip=0, loss_scale=2920, train_wall=252, wall=78359
2022-07-13 12:30:20 | INFO | train_inner | epoch 024:    531 / 1122 loss=6.263, nll_loss=2.263, mask_ins=0.936, word_ins_ml=3.918, word_reposition=0.647, kpe=0.762, ppl=76.8, wps=7181.3, ups=0.35, wpb=20565.9, bsz=256, num_updates=26300, lr=0.00021801, gnorm=1.487, clip=0, loss_scale=2048, train_wall=249, wall=78646
2022-07-13 12:35:20 | INFO | train_inner | epoch 024:    631 / 1122 loss=6.289, nll_loss=2.27, mask_ins=0.945, word_ins_ml=3.924, word_reposition=0.653, kpe=0.767, ppl=78.17, wps=6831.5, ups=0.33, wpb=20467.3, bsz=256, num_updates=26400, lr=0.000217597, gnorm=1.471, clip=0, loss_scale=2048, train_wall=262, wall=78945
2022-07-13 12:40:41 | INFO | train_inner | epoch 024:    731 / 1122 loss=6.297, nll_loss=2.275, mask_ins=0.943, word_ins_ml=3.928, word_reposition=0.657, kpe=0.77, ppl=78.65, wps=6405.9, ups=0.31, wpb=20589.3, bsz=256, num_updates=26500, lr=0.000217186, gnorm=1.476, clip=0, loss_scale=2048, train_wall=284, wall=79267
2022-07-13 12:45:28 | INFO | train_inner | epoch 024:    831 / 1122 loss=6.298, nll_loss=2.284, mask_ins=0.95, word_ins_ml=3.937, word_reposition=0.646, kpe=0.765, ppl=78.66, wps=7171.1, ups=0.35, wpb=20569.8, bsz=256, num_updates=26600, lr=0.000216777, gnorm=1.507, clip=0, loss_scale=2048, train_wall=249, wall=79554
2022-07-13 12:50:16 | INFO | train_inner | epoch 024:    931 / 1122 loss=6.3, nll_loss=2.283, mask_ins=0.946, word_ins_ml=3.936, word_reposition=0.647, kpe=0.771, ppl=78.79, wps=7149.4, ups=0.35, wpb=20568.2, bsz=256, num_updates=26700, lr=0.000216371, gnorm=1.512, clip=0, loss_scale=2990, train_wall=250, wall=79841
2022-07-13 12:55:03 | INFO | train_inner | epoch 024:   1031 / 1122 loss=6.331, nll_loss=2.294, mask_ins=0.953, word_ins_ml=3.945, word_reposition=0.661, kpe=0.772, ppl=80.53, wps=7132.7, ups=0.35, wpb=20517.5, bsz=256, num_updates=26800, lr=0.000215967, gnorm=1.49, clip=0, loss_scale=4096, train_wall=250, wall=80129
2022-07-13 12:59:24 | INFO | train | epoch 024 | loss nan | nll_loss 2.278 | mask_ins 0.947 | word_ins_ml 3.931 | word_reposition 0.655 | kpe nan | ppl nan | wps 6872.8 | ups 0.33 | wpb 20522.2 | bsz 255.8 | num_updates 26891 | lr 0.000215601 | gnorm 1.497 | clip 0 | loss_scale 2745 | train_wall 2843 | wall 80389
2022-07-13 13:00:43 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 12.538 | nll_loss 6.392 | mask_ins 2.01 | word_ins_ml 7.722 | word_reposition 1.309 | kpe 1.496 | ppl 5945.3 | wps 12540.7 | wpb 2367.6 | bsz 32 | num_updates 26891 | best_loss 12.458
2022-07-13 13:00:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 24 @ 26891 updates, score 12.538) (writing took 3.714163564145565 seconds)
2022-07-13 13:00:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 13:01:15 | INFO | train_inner | epoch 025:     10 / 1122 loss=6.315, nll_loss=2.298, mask_ins=0.946, word_ins_ml=3.948, word_reposition=0.651, kpe=0.77, ppl=79.63, wps=5507, ups=0.27, wpb=20463.2, bsz=253.8, num_updates=26900, lr=0.000215565, gnorm=1.548, clip=0, loss_scale=3954, train_wall=251, wall=80501
2022-07-13 13:06:02 | INFO | train_inner | epoch 025:    110 / 1122 loss=6.262, nll_loss=2.266, mask_ins=0.944, word_ins_ml=3.92, word_reposition=0.651, kpe=0.748, ppl=76.77, wps=7180.2, ups=0.35, wpb=20597.4, bsz=256, num_updates=27000, lr=0.000215166, gnorm=1.522, clip=0, loss_scale=2048, train_wall=249, wall=80787
2022-07-13 13:10:48 | INFO | train_inner | epoch 025:    210 / 1122 loss=6.25, nll_loss=2.257, mask_ins=0.945, word_ins_ml=3.913, word_reposition=0.643, kpe=0.749, ppl=76.12, wps=7172.6, ups=0.35, wpb=20535.6, bsz=256, num_updates=27100, lr=0.000214768, gnorm=1.494, clip=0, loss_scale=2048, train_wall=248, wall=81074
2022-07-13 13:15:35 | INFO | train_inner | epoch 025:    310 / 1122 loss=6.265, nll_loss=2.26, mask_ins=0.936, word_ins_ml=3.915, word_reposition=0.664, kpe=0.75, ppl=76.91, wps=7185.5, ups=0.35, wpb=20591.4, bsz=256, num_updates=27200, lr=0.000214373, gnorm=1.527, clip=0, loss_scale=2048, train_wall=249, wall=81360
2022-07-13 13:20:22 | INFO | train_inner | epoch 025:    410 / 1122 loss=6.263, nll_loss=2.26, mask_ins=0.935, word_ins_ml=3.915, word_reposition=0.657, kpe=0.755, ppl=76.77, wps=7179.6, ups=0.35, wpb=20639.8, bsz=256, num_updates=27300, lr=0.00021398, gnorm=1.538, clip=0, loss_scale=2048, train_wall=250, wall=81648
2022-07-13 13:25:09 | INFO | train_inner | epoch 025:    510 / 1122 loss=6.256, nll_loss=2.249, mask_ins=0.947, word_ins_ml=3.905, word_reposition=0.646, kpe=0.758, ppl=76.41, wps=7123.8, ups=0.35, wpb=20468, bsz=256, num_updates=27400, lr=0.000213589, gnorm=1.488, clip=0, loss_scale=2048, train_wall=250, wall=81935
2022-07-13 13:29:56 | INFO | train_inner | epoch 025:    610 / 1122 loss=nan, nll_loss=2.22, mask_ins=0.943, word_ins_ml=3.88, word_reposition=0.64, kpe=nan, ppl=nan, wps=7116.5, ups=0.35, wpb=20426.7, bsz=256, num_updates=27500, lr=0.000213201, gnorm=1.536, clip=0, loss_scale=3994, train_wall=249, wall=82222
2022-07-13 13:34:44 | INFO | train_inner | epoch 025:    710 / 1122 loss=nan, nll_loss=2.249, mask_ins=0.939, word_ins_ml=3.905, word_reposition=0.643, kpe=nan, ppl=nan, wps=7139.6, ups=0.35, wpb=20501.2, bsz=256, num_updates=27600, lr=0.000212814, gnorm=1.535, clip=0, loss_scale=4096, train_wall=249, wall=82509
2022-07-13 13:39:44 | INFO | train_inner | epoch 025:    810 / 1122 loss=6.232, nll_loss=2.238, mask_ins=0.933, word_ins_ml=3.895, word_reposition=0.646, kpe=0.757, ppl=75.18, wps=6897.6, ups=0.33, wpb=20737.8, bsz=256, num_updates=27700, lr=0.00021243, gnorm=1.512, clip=0, loss_scale=4096, train_wall=262, wall=82810
2022-07-13 13:42:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 13:45:14 | INFO | train_inner | epoch 025:    911 / 1122 loss=6.244, nll_loss=2.257, mask_ins=0.937, word_ins_ml=3.912, word_reposition=0.635, kpe=0.761, ppl=75.81, wps=6200.8, ups=0.3, wpb=20427.8, bsz=256, num_updates=27800, lr=0.000212047, gnorm=1.507, clip=0, loss_scale=2819, train_wall=290, wall=83139
2022-07-13 13:50:01 | INFO | train_inner | epoch 025:   1011 / 1122 loss=6.257, nll_loss=2.264, mask_ins=0.934, word_ins_ml=3.919, word_reposition=0.641, kpe=0.763, ppl=76.47, wps=7108.7, ups=0.35, wpb=20417.1, bsz=256, num_updates=27900, lr=0.000211667, gnorm=1.517, clip=0, loss_scale=2048, train_wall=249, wall=83427
2022-07-13 13:54:48 | INFO | train_inner | epoch 025:   1111 / 1122 loss=6.301, nll_loss=2.285, mask_ins=0.943, word_ins_ml=3.937, word_reposition=0.657, kpe=0.764, ppl=78.83, wps=7134.1, ups=0.35, wpb=20474.3, bsz=256, num_updates=28000, lr=0.000211289, gnorm=1.503, clip=0, loss_scale=2048, train_wall=249, wall=83714
2022-07-13 13:55:18 | INFO | train | epoch 025 | loss nan | nll_loss 2.256 | mask_ins 0.94 | word_ins_ml 3.912 | word_reposition 0.648 | kpe nan | ppl nan | wps 6851 | ups 0.33 | wpb 20521.4 | bsz 255.8 | num_updates 28011 | lr 0.000211247 | gnorm 1.522 | clip 0 | loss_scale 2661 | train_wall 2847 | wall 83744
2022-07-13 13:56:37 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 12.625 | nll_loss 6.422 | mask_ins 2.008 | word_ins_ml 7.761 | word_reposition 1.352 | kpe 1.504 | ppl 6317.32 | wps 12605 | wpb 2367.6 | bsz 32 | num_updates 28011 | best_loss 12.458
2022-07-13 13:56:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 25 @ 28011 updates, score 12.625) (writing took 3.61686079390347 seconds)
2022-07-13 14:01:00 | INFO | train_inner | epoch 026:     89 / 1122 loss=6.244, nll_loss=2.258, mask_ins=0.944, word_ins_ml=3.913, word_reposition=0.644, kpe=0.743, ppl=75.77, wps=5492.2, ups=0.27, wpb=20429, bsz=253.8, num_updates=28100, lr=0.000210912, gnorm=1.618, clip=0, loss_scale=2048, train_wall=251, wall=84086
2022-07-13 14:05:48 | INFO | train_inner | epoch 026:    189 / 1122 loss=6.186, nll_loss=2.218, mask_ins=0.924, word_ins_ml=3.877, word_reposition=0.644, kpe=0.741, ppl=72.79, wps=7113.6, ups=0.35, wpb=20507.2, bsz=256, num_updates=28200, lr=0.000210538, gnorm=1.531, clip=0, loss_scale=2048, train_wall=250, wall=84374
2022-07-13 14:10:37 | INFO | train_inner | epoch 026:    289 / 1122 loss=6.173, nll_loss=2.196, mask_ins=0.933, word_ins_ml=3.859, word_reposition=0.642, kpe=0.739, ppl=72.15, wps=7060.8, ups=0.35, wpb=20407.2, bsz=256, num_updates=28300, lr=0.000210166, gnorm=1.59, clip=0, loss_scale=3092, train_wall=251, wall=84663
2022-07-13 14:15:27 | INFO | train_inner | epoch 026:    389 / 1122 loss=6.241, nll_loss=2.243, mask_ins=0.94, word_ins_ml=3.899, word_reposition=0.652, kpe=0.749, ppl=75.65, wps=7104.5, ups=0.34, wpb=20618.9, bsz=256, num_updates=28400, lr=0.000209795, gnorm=1.537, clip=0, loss_scale=4096, train_wall=252, wall=84953
2022-07-13 14:20:19 | INFO | train_inner | epoch 026:    489 / 1122 loss=6.25, nll_loss=2.258, mask_ins=0.94, word_ins_ml=3.913, word_reposition=0.65, kpe=0.748, ppl=76.12, wps=7038.8, ups=0.34, wpb=20553.1, bsz=256, num_updates=28500, lr=0.000209427, gnorm=1.574, clip=0, loss_scale=4096, train_wall=253, wall=85245
2022-07-13 14:25:09 | INFO | train_inner | epoch 026:    589 / 1122 loss=6.216, nll_loss=2.243, mask_ins=0.933, word_ins_ml=3.9, word_reposition=0.636, kpe=0.746, ppl=74.31, wps=7062.9, ups=0.35, wpb=20424.2, bsz=256, num_updates=28600, lr=0.000209061, gnorm=1.56, clip=0, loss_scale=4096, train_wall=251, wall=85534
2022-07-13 14:29:55 | INFO | train_inner | epoch 026:    689 / 1122 loss=nan, nll_loss=2.229, mask_ins=0.929, word_ins_ml=3.887, word_reposition=0.633, kpe=nan, ppl=nan, wps=7171.8, ups=0.35, wpb=20563.5, bsz=256, num_updates=28700, lr=0.000208696, gnorm=1.534, clip=0, loss_scale=4096, train_wall=249, wall=85821
2022-07-13 14:32:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 14:34:45 | INFO | train_inner | epoch 026:    790 / 1122 loss=nan, nll_loss=2.207, mask_ins=0.928, word_ins_ml=3.868, word_reposition=0.652, kpe=nan, ppl=nan, wps=7100.9, ups=0.35, wpb=20577.8, bsz=256, num_updates=28800, lr=0.000208333, gnorm=1.519, clip=0, loss_scale=3102, train_wall=252, wall=86111
2022-07-13 14:39:38 | INFO | train_inner | epoch 026:    890 / 1122 loss=6.246, nll_loss=2.252, mask_ins=0.935, word_ins_ml=3.907, word_reposition=0.653, kpe=0.751, ppl=75.89, wps=7042.7, ups=0.34, wpb=20635.1, bsz=256, num_updates=28900, lr=0.000207973, gnorm=1.574, clip=0, loss_scale=2048, train_wall=254, wall=86404
2022-07-13 14:44:39 | INFO | train_inner | epoch 026:    990 / 1122 loss=6.231, nll_loss=2.248, mask_ins=0.934, word_ins_ml=3.903, word_reposition=0.637, kpe=0.756, ppl=75.09, wps=6830.2, ups=0.33, wpb=20566.7, bsz=256, num_updates=29000, lr=0.000207614, gnorm=1.545, clip=0, loss_scale=2048, train_wall=263, wall=86705
2022-07-13 14:50:02 | INFO | train_inner | epoch 026:   1090 / 1122 loss=6.223, nll_loss=2.232, mask_ins=0.928, word_ins_ml=3.889, word_reposition=0.65, kpe=0.757, ppl=74.72, wps=6368.9, ups=0.31, wpb=20534, bsz=256, num_updates=29100, lr=0.000207257, gnorm=1.526, clip=0, loss_scale=2048, train_wall=284, wall=87027
2022-07-13 14:51:33 | INFO | train | epoch 026 | loss nan | nll_loss 2.236 | mask_ins 0.933 | word_ins_ml 3.893 | word_reposition 0.645 | kpe nan | ppl nan | wps 6817 | ups 0.33 | wpb 20521.2 | bsz 255.8 | num_updates 29132 | lr 0.000207143 | gnorm 1.555 | clip 0 | loss_scale 2966 | train_wall 2861 | wall 87119
2022-07-13 14:52:53 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 12.698 | nll_loss 6.388 | mask_ins 2.009 | word_ins_ml 7.732 | word_reposition 1.43 | kpe 1.527 | ppl 6646.07 | wps 12296.8 | wpb 2367.6 | bsz 32 | num_updates 29132 | best_loss 12.458
2022-07-13 14:52:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 26 @ 29132 updates, score 12.698) (writing took 3.4199296571314335 seconds)
2022-07-13 14:56:15 | INFO | train_inner | epoch 027:     68 / 1122 loss=6.223, nll_loss=2.248, mask_ins=0.931, word_ins_ml=3.904, word_reposition=0.647, kpe=0.741, ppl=74.67, wps=5461.6, ups=0.27, wpb=20364.9, bsz=253.8, num_updates=29200, lr=0.000206901, gnorm=1.581, clip=0, loss_scale=2048, train_wall=251, wall=87400
2022-07-13 15:01:06 | INFO | train_inner | epoch 027:    168 / 1122 loss=6.19, nll_loss=2.214, mask_ins=0.935, word_ins_ml=3.874, word_reposition=0.647, kpe=0.735, ppl=73.03, wps=7081.2, ups=0.34, wpb=20666.6, bsz=256, num_updates=29300, lr=0.000206548, gnorm=1.576, clip=0, loss_scale=2806, train_wall=253, wall=87692
2022-07-13 15:05:59 | INFO | train_inner | epoch 027:    268 / 1122 loss=6.198, nll_loss=2.236, mask_ins=0.928, word_ins_ml=3.893, word_reposition=0.643, kpe=0.733, ppl=73.4, wps=7065.4, ups=0.34, wpb=20641.1, bsz=256, num_updates=29400, lr=0.000206197, gnorm=1.571, clip=0, loss_scale=4096, train_wall=253, wall=87984
2022-07-13 15:10:48 | INFO | train_inner | epoch 027:    368 / 1122 loss=nan, nll_loss=2.219, mask_ins=0.934, word_ins_ml=3.878, word_reposition=0.643, kpe=nan, ppl=nan, wps=7035.6, ups=0.35, wpb=20344.4, bsz=256, num_updates=29500, lr=0.000205847, gnorm=1.538, clip=0, loss_scale=4096, train_wall=251, wall=88273
2022-07-13 15:15:35 | INFO | train_inner | epoch 027:    468 / 1122 loss=6.214, nll_loss=2.255, mask_ins=0.931, word_ins_ml=3.91, word_reposition=0.633, kpe=0.74, ppl=74.24, wps=7120.4, ups=0.35, wpb=20437.3, bsz=256, num_updates=29600, lr=0.000205499, gnorm=1.605, clip=0, loss_scale=4096, train_wall=249, wall=88560
2022-07-13 15:20:21 | INFO | train_inner | epoch 027:    568 / 1122 loss=6.194, nll_loss=2.223, mask_ins=0.925, word_ins_ml=3.882, word_reposition=0.648, kpe=0.739, ppl=73.21, wps=7164.1, ups=0.35, wpb=20526, bsz=256, num_updates=29700, lr=0.000205152, gnorm=1.516, clip=0, loss_scale=4096, train_wall=249, wall=88847
2022-07-13 15:25:07 | INFO | train_inner | epoch 027:    668 / 1122 loss=6.24, nll_loss=2.256, mask_ins=0.936, word_ins_ml=3.91, word_reposition=0.652, kpe=0.742, ppl=75.57, wps=7190.9, ups=0.35, wpb=20575.1, bsz=256, num_updates=29800, lr=0.000204808, gnorm=1.579, clip=0, loss_scale=5120, train_wall=249, wall=89133
2022-07-13 15:29:54 | INFO | train_inner | epoch 027:    768 / 1122 loss=6.229, nll_loss=2.248, mask_ins=0.931, word_ins_ml=3.904, word_reposition=0.652, kpe=0.741, ppl=75, wps=7173.3, ups=0.35, wpb=20554.9, bsz=256, num_updates=29900, lr=0.000204465, gnorm=1.551, clip=0, loss_scale=8192, train_wall=249, wall=89420
2022-07-13 15:32:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-13 15:34:43 | INFO | train_inner | epoch 027:    869 / 1122 loss=nan, nll_loss=2.222, mask_ins=0.933, word_ins_ml=3.881, word_reposition=0.629, kpe=nan, ppl=nan, wps=7100.4, ups=0.35, wpb=20541.1, bsz=256, num_updates=30000, lr=0.000204124, gnorm=1.54, clip=0, loss_scale=6529, train_wall=252, wall=89709
2022-07-13 15:35:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 15:39:32 | INFO | train_inner | epoch 027:    970 / 1122 loss=6.196, nll_loss=2.231, mask_ins=0.92, word_ins_ml=3.889, word_reposition=0.642, kpe=0.744, ppl=73.3, wps=7125.2, ups=0.35, wpb=20576.8, bsz=256, num_updates=30100, lr=0.000203785, gnorm=1.563, clip=0, loss_scale=2433, train_wall=251, wall=89998
2022-07-13 15:44:19 | INFO | train_inner | epoch 027:   1070 / 1122 loss=6.221, nll_loss=2.249, mask_ins=0.931, word_ins_ml=3.904, word_reposition=0.638, kpe=0.748, ppl=74.6, wps=7161.2, ups=0.35, wpb=20513.8, bsz=256, num_updates=30200, lr=0.000203447, gnorm=1.564, clip=0, loss_scale=2048, train_wall=249, wall=90284
2022-07-13 15:46:58 | INFO | train | epoch 027 | loss nan | nll_loss 2.235 | mask_ins 0.93 | word_ins_ml 3.892 | word_reposition 0.643 | kpe nan | ppl nan | wps 6911.6 | ups 0.34 | wpb 20520.1 | bsz 255.8 | num_updates 30252 | lr 0.000203272 | gnorm 1.568 | clip 0 | loss_scale 4105 | train_wall 2816 | wall 90444
2022-07-13 15:48:17 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 12.42 | nll_loss 6.323 | mask_ins 2.001 | word_ins_ml 7.666 | word_reposition 1.298 | kpe 1.455 | ppl 5481.58 | wps 12587.6 | wpb 2367.6 | bsz 32 | num_updates 30252 | best_loss 12.42
2022-07-13 15:48:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_best.pt (epoch 27 @ 30252 updates, score 12.42) (writing took 7.061146657913923 seconds)
2022-07-13 15:51:03 | INFO | train_inner | epoch 028:     48 / 1122 loss=6.199, nll_loss=2.236, mask_ins=0.929, word_ins_ml=3.893, word_reposition=0.641, kpe=0.736, ppl=73.47, wps=5033.8, ups=0.25, wpb=20336.3, bsz=253.8, num_updates=30300, lr=0.000203111, gnorm=1.68, clip=0, loss_scale=2048, train_wall=280, wall=90688
2022-07-13 15:56:02 | INFO | train_inner | epoch 028:    148 / 1122 loss=6.182, nll_loss=2.226, mask_ins=0.929, word_ins_ml=3.884, word_reposition=0.643, kpe=0.726, ppl=72.61, wps=6846.2, ups=0.33, wpb=20527.9, bsz=256, num_updates=30400, lr=0.000202777, gnorm=1.57, clip=0, loss_scale=2048, train_wall=262, wall=90988
2022-07-13 16:00:49 | INFO | train_inner | epoch 028:    248 / 1122 loss=6.155, nll_loss=2.19, mask_ins=0.93, word_ins_ml=3.852, word_reposition=0.649, kpe=0.724, ppl=71.26, wps=7154.8, ups=0.35, wpb=20524.4, bsz=256, num_updates=30500, lr=0.000202444, gnorm=1.601, clip=0, loss_scale=2048, train_wall=249, wall=91275
2022-07-13 16:03:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 16:05:39 | INFO | train_inner | epoch 028:    349 / 1122 loss=6.186, nll_loss=2.232, mask_ins=0.927, word_ins_ml=3.889, word_reposition=0.64, kpe=0.73, ppl=72.81, wps=7082.2, ups=0.35, wpb=20496.5, bsz=256, num_updates=30600, lr=0.000202113, gnorm=1.583, clip=0, loss_scale=2352, train_wall=251, wall=91564
2022-07-13 16:10:25 | INFO | train_inner | epoch 028:    449 / 1122 loss=6.164, nll_loss=2.208, mask_ins=0.92, word_ins_ml=3.868, word_reposition=0.645, kpe=0.732, ppl=71.73, wps=7222.6, ups=0.35, wpb=20690.2, bsz=256, num_updates=30700, lr=0.000201784, gnorm=1.658, clip=0, loss_scale=2048, train_wall=249, wall=91851
2022-07-13 16:15:12 | INFO | train_inner | epoch 028:    549 / 1122 loss=nan, nll_loss=2.185, mask_ins=0.925, word_ins_ml=3.848, word_reposition=0.645, kpe=nan, ppl=nan, wps=7152.6, ups=0.35, wpb=20485.8, bsz=256, num_updates=30800, lr=0.000201456, gnorm=1.604, clip=0, loss_scale=2048, train_wall=249, wall=92137
2022-07-13 16:19:58 | INFO | train_inner | epoch 028:    649 / 1122 loss=nan, nll_loss=2.224, mask_ins=0.926, word_ins_ml=3.882, word_reposition=0.64, kpe=nan, ppl=nan, wps=7142.4, ups=0.35, wpb=20463, bsz=256, num_updates=30900, lr=0.000201129, gnorm=1.694, clip=0, loss_scale=2048, train_wall=249, wall=92424
2022-07-13 16:21:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-13 16:24:47 | INFO | train_inner | epoch 028:    750 / 1122 loss=6.179, nll_loss=2.22, mask_ins=0.923, word_ins_ml=3.878, word_reposition=0.64, kpe=0.737, ppl=72.45, wps=7082.9, ups=0.35, wpb=20473.7, bsz=256, num_updates=31000, lr=0.000200805, gnorm=1.634, clip=0, loss_scale=1298, train_wall=251, wall=92713
2022-07-13 16:29:33 | INFO | train_inner | epoch 028:    850 / 1122 loss=6.146, nll_loss=2.193, mask_ins=0.918, word_ins_ml=3.854, word_reposition=0.638, kpe=0.735, ppl=70.8, wps=7156.4, ups=0.35, wpb=20481.8, bsz=256, num_updates=31100, lr=0.000200482, gnorm=1.591, clip=0, loss_scale=1024, train_wall=249, wall=92999
2022-07-13 16:34:20 | INFO | train_inner | epoch 028:    950 / 1122 loss=6.182, nll_loss=2.22, mask_ins=0.918, word_ins_ml=3.879, word_reposition=0.649, kpe=0.736, ppl=72.58, wps=7151.2, ups=0.35, wpb=20498.2, bsz=256, num_updates=31200, lr=0.00020016, gnorm=1.599, clip=0, loss_scale=1024, train_wall=249, wall=93286
2022-07-13 16:39:07 | INFO | train_inner | epoch 028:   1050 / 1122 loss=6.217, nll_loss=2.259, mask_ins=0.931, word_ins_ml=3.913, word_reposition=0.632, kpe=0.741, ppl=74.39, wps=7184.7, ups=0.35, wpb=20590.3, bsz=256, num_updates=31300, lr=0.00019984, gnorm=1.647, clip=0, loss_scale=1024, train_wall=249, wall=93572
2022-07-13 16:42:32 | INFO | train | epoch 028 | loss nan | nll_loss 2.217 | mask_ins 0.925 | word_ins_ml 3.876 | word_reposition 0.642 | kpe nan | ppl nan | wps 6893.3 | ups 0.34 | wpb 20521.1 | bsz 255.8 | num_updates 31372 | lr 0.000199611 | gnorm 1.621 | clip 0 | loss_scale 1668 | train_wall 2827 | wall 93778
2022-07-13 16:43:51 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 12.329 | nll_loss 6.201 | mask_ins 1.986 | word_ins_ml 7.541 | word_reposition 1.286 | kpe 1.515 | ppl 5143.4 | wps 12541.7 | wpb 2367.6 | bsz 32 | num_updates 31372 | best_loss 12.329
2022-07-13 16:43:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_best.pt (epoch 28 @ 31372 updates, score 12.329) (writing took 6.976241455413401 seconds)
2022-07-13 16:45:19 | INFO | train_inner | epoch 029:     28 / 1122 loss=6.166, nll_loss=2.194, mask_ins=0.928, word_ins_ml=3.856, word_reposition=0.644, kpe=0.738, ppl=71.82, wps=5502.9, ups=0.27, wpb=20468.5, bsz=253.8, num_updates=31400, lr=0.000199522, gnorm=1.651, clip=0, loss_scale=1024, train_wall=249, wall=93944
2022-07-13 16:50:17 | INFO | train_inner | epoch 029:    128 / 1122 loss=6.143, nll_loss=2.2, mask_ins=0.925, word_ins_ml=3.86, word_reposition=0.634, kpe=0.724, ppl=70.68, wps=6883.5, ups=0.33, wpb=20579.7, bsz=256, num_updates=31500, lr=0.000199205, gnorm=1.737, clip=0, loss_scale=1659, train_wall=261, wall=94243
2022-07-13 16:55:27 | INFO | train_inner | epoch 029:    228 / 1122 loss=6.111, nll_loss=2.173, mask_ins=0.92, word_ins_ml=3.837, word_reposition=0.634, kpe=0.72, ppl=69.13, wps=6634.1, ups=0.32, wpb=20524.7, bsz=256, num_updates=31600, lr=0.000198889, gnorm=1.67, clip=0, loss_scale=2048, train_wall=272, wall=94553
2022-07-13 17:00:25 | INFO | train_inner | epoch 029:    328 / 1122 loss=6.161, nll_loss=2.211, mask_ins=0.927, word_ins_ml=3.87, word_reposition=0.641, kpe=0.722, ppl=71.55, wps=6878.3, ups=0.34, wpb=20527.6, bsz=256, num_updates=31700, lr=0.000198575, gnorm=1.659, clip=0, loss_scale=2048, train_wall=261, wall=94851
2022-07-13 17:05:12 | INFO | train_inner | epoch 029:    428 / 1122 loss=nan, nll_loss=2.215, mask_ins=0.922, word_ins_ml=3.875, word_reposition=0.639, kpe=nan, ppl=nan, wps=7156, ups=0.35, wpb=20502.8, bsz=256, num_updates=31800, lr=0.000198263, gnorm=1.609, clip=0, loss_scale=2048, train_wall=249, wall=95138
2022-07-13 17:09:58 | INFO | train_inner | epoch 029:    528 / 1122 loss=6.142, nll_loss=2.194, mask_ins=0.916, word_ins_ml=3.856, word_reposition=0.648, kpe=0.722, ppl=70.62, wps=7171.1, ups=0.35, wpb=20505.4, bsz=256, num_updates=31900, lr=0.000197952, gnorm=1.657, clip=0, loss_scale=2048, train_wall=249, wall=95424
2022-07-13 17:14:44 | INFO | train_inner | epoch 029:    628 / 1122 loss=nan, nll_loss=2.206, mask_ins=0.927, word_ins_ml=3.866, word_reposition=0.646, kpe=nan, ppl=nan, wps=7176.5, ups=0.35, wpb=20572.7, bsz=256, num_updates=32000, lr=0.000197642, gnorm=1.594, clip=0, loss_scale=3072, train_wall=249, wall=95710
2022-07-13 17:19:31 | INFO | train_inner | epoch 029:    728 / 1122 loss=6.138, nll_loss=2.209, mask_ins=0.915, word_ins_ml=3.868, word_reposition=0.626, kpe=0.729, ppl=70.41, wps=7157.6, ups=0.35, wpb=20525.2, bsz=256, num_updates=32100, lr=0.000197334, gnorm=1.611, clip=0, loss_scale=4096, train_wall=249, wall=95997
2022-07-13 17:24:17 | INFO | train_inner | epoch 029:    828 / 1122 loss=6.167, nll_loss=2.222, mask_ins=0.922, word_ins_ml=3.88, word_reposition=0.636, kpe=0.729, ppl=71.84, wps=7172.3, ups=0.35, wpb=20515.4, bsz=256, num_updates=32200, lr=0.000197028, gnorm=1.602, clip=0, loss_scale=4096, train_wall=248, wall=96283
2022-07-13 17:24:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 17:29:06 | INFO | train_inner | epoch 029:    929 / 1122 loss=6.134, nll_loss=2.184, mask_ins=0.919, word_ins_ml=3.846, word_reposition=0.641, kpe=0.728, ppl=70.22, wps=7138.3, ups=0.35, wpb=20628.3, bsz=256, num_updates=32300, lr=0.000196722, gnorm=1.642, clip=0, loss_scale=2312, train_wall=251, wall=96572
2022-07-13 17:33:53 | INFO | train_inner | epoch 029:   1029 / 1122 loss=6.197, nll_loss=2.248, mask_ins=0.926, word_ins_ml=3.903, word_reposition=0.639, kpe=0.73, ppl=73.37, wps=7140.9, ups=0.35, wpb=20467.3, bsz=256, num_updates=32400, lr=0.000196419, gnorm=1.568, clip=0, loss_scale=2048, train_wall=249, wall=96859
2022-07-13 17:38:18 | INFO | train | epoch 029 | loss nan | nll_loss 2.204 | mask_ins 0.921 | word_ins_ml 3.864 | word_reposition 0.638 | kpe nan | ppl nan | wps 6875.3 | ups 0.34 | wpb 20519.7 | bsz 255.8 | num_updates 32493 | lr 0.000196137 | gnorm 1.634 | clip 0 | loss_scale 2468 | train_wall 2839 | wall 97124
2022-07-13 17:39:37 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 12.735 | nll_loss 6.428 | mask_ins 2.045 | word_ins_ml 7.76 | word_reposition 1.366 | kpe 1.564 | ppl 6816.74 | wps 12609.1 | wpb 2367.6 | bsz 32 | num_updates 32493 | best_loss 12.329
2022-07-13 17:39:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 29 @ 32493 updates, score 12.735) (writing took 3.71709587238729 seconds)
2022-07-13 17:40:00 | INFO | train_inner | epoch 030:      7 / 1122 loss=6.127, nll_loss=2.193, mask_ins=0.914, word_ins_ml=3.854, word_reposition=0.634, kpe=0.724, ppl=69.88, wps=5543.5, ups=0.27, wpb=20348.7, bsz=253.8, num_updates=32500, lr=0.000196116, gnorm=1.663, clip=0, loss_scale=2048, train_wall=248, wall=97226
2022-07-13 17:44:46 | INFO | train_inner | epoch 030:    107 / 1122 loss=6.106, nll_loss=2.194, mask_ins=0.916, word_ins_ml=3.856, word_reposition=0.623, kpe=0.711, ppl=68.87, wps=7162.5, ups=0.35, wpb=20514.5, bsz=256, num_updates=32600, lr=0.000195815, gnorm=1.663, clip=0, loss_scale=2048, train_wall=249, wall=97512
2022-07-13 17:49:33 | INFO | train_inner | epoch 030:    207 / 1122 loss=6.133, nll_loss=2.216, mask_ins=0.915, word_ins_ml=3.875, word_reposition=0.63, kpe=0.712, ppl=70.16, wps=7153.6, ups=0.35, wpb=20511.3, bsz=256, num_updates=32700, lr=0.000195515, gnorm=1.62, clip=0, loss_scale=2048, train_wall=249, wall=97799
2022-07-13 17:53:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 17:54:34 | INFO | train_inner | epoch 030:    308 / 1122 loss=6.118, nll_loss=2.184, mask_ins=0.916, word_ins_ml=3.846, word_reposition=0.645, kpe=0.711, ppl=69.46, wps=6818.8, ups=0.33, wpb=20495.8, bsz=256, num_updates=32800, lr=0.000195217, gnorm=1.551, clip=0, loss_scale=3204, train_wall=263, wall=98099
2022-07-13 17:59:54 | INFO | train_inner | epoch 030:    408 / 1122 loss=6.079, nll_loss=2.161, mask_ins=0.914, word_ins_ml=3.826, word_reposition=0.63, kpe=0.71, ppl=67.62, wps=6365.2, ups=0.31, wpb=20412.4, bsz=256, num_updates=32900, lr=0.00019492, gnorm=1.58, clip=0, loss_scale=2048, train_wall=283, wall=98420
2022-07-13 18:04:42 | INFO | train_inner | epoch 030:    508 / 1122 loss=nan, nll_loss=2.178, mask_ins=0.912, word_ins_ml=3.841, word_reposition=0.639, kpe=nan, ppl=nan, wps=7194.9, ups=0.35, wpb=20695.8, bsz=256, num_updates=33000, lr=0.000194625, gnorm=1.587, clip=0, loss_scale=2048, train_wall=250, wall=98708
2022-07-13 18:09:29 | INFO | train_inner | epoch 030:    608 / 1122 loss=6.153, nll_loss=2.208, mask_ins=0.922, word_ins_ml=3.868, word_reposition=0.641, kpe=0.721, ppl=71.15, wps=7131.8, ups=0.35, wpb=20481.9, bsz=256, num_updates=33100, lr=0.000194331, gnorm=1.643, clip=0, loss_scale=2048, train_wall=249, wall=98995
2022-07-13 18:14:16 | INFO | train_inner | epoch 030:    708 / 1122 loss=6.156, nll_loss=2.234, mask_ins=0.918, word_ins_ml=3.891, word_reposition=0.63, kpe=0.717, ppl=71.3, wps=7105.2, ups=0.35, wpb=20393.5, bsz=256, num_updates=33200, lr=0.000194038, gnorm=1.689, clip=0, loss_scale=2048, train_wall=249, wall=99282
2022-07-13 18:19:03 | INFO | train_inner | epoch 030:    808 / 1122 loss=6.135, nll_loss=2.203, mask_ins=0.914, word_ins_ml=3.863, word_reposition=0.637, kpe=0.721, ppl=70.26, wps=7167.4, ups=0.35, wpb=20580.9, bsz=256, num_updates=33300, lr=0.000193746, gnorm=1.567, clip=0, loss_scale=2212, train_wall=250, wall=99569
2022-07-13 18:23:50 | INFO | train_inner | epoch 030:    908 / 1122 loss=6.121, nll_loss=2.185, mask_ins=0.919, word_ins_ml=3.847, word_reposition=0.631, kpe=0.724, ppl=69.59, wps=7190.5, ups=0.35, wpb=20586.2, bsz=256, num_updates=33400, lr=0.000193456, gnorm=1.695, clip=0, loss_scale=4096, train_wall=249, wall=99855
2022-07-13 18:28:37 | INFO | train_inner | epoch 030:   1008 / 1122 loss=6.113, nll_loss=2.193, mask_ins=0.911, word_ins_ml=3.854, word_reposition=0.629, kpe=0.719, ppl=69.2, wps=7191.3, ups=0.35, wpb=20639.5, bsz=256, num_updates=33500, lr=0.000193167, gnorm=1.595, clip=0, loss_scale=4096, train_wall=250, wall=100142
2022-07-13 18:29:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 18:33:27 | INFO | train_inner | epoch 030:   1109 / 1122 loss=nan, nll_loss=2.164, mask_ins=0.928, word_ins_ml=3.829, word_reposition=0.639, kpe=nan, ppl=nan, wps=7091.4, ups=0.35, wpb=20553.3, bsz=256, num_updates=33600, lr=0.000192879, gnorm=1.641, clip=0, loss_scale=2494, train_wall=252, wall=100432
2022-07-13 18:34:03 | INFO | train | epoch 030 | loss nan | nll_loss 2.193 | mask_ins 0.917 | word_ins_ml 3.854 | word_reposition 0.634 | kpe nan | ppl nan | wps 6870.6 | ups 0.33 | wpb 20518 | bsz 255.8 | num_updates 33613 | lr 0.000192842 | gnorm 1.626 | clip 0 | loss_scale 2572 | train_wall 2841 | wall 100468
2022-07-13 18:35:22 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 12.541 | nll_loss 6.33 | mask_ins 2.028 | word_ins_ml 7.669 | word_reposition 1.326 | kpe 1.518 | ppl 5958.86 | wps 12557.4 | wpb 2367.6 | bsz 32 | num_updates 33613 | best_loss 12.329
2022-07-13 18:35:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 30 @ 33613 updates, score 12.541) (writing took 3.5779768833890557 seconds)
2022-07-13 18:39:34 | INFO | train_inner | epoch 031:     87 / 1122 loss=nan, nll_loss=2.171, mask_ins=0.904, word_ins_ml=3.834, word_reposition=0.625, kpe=nan, ppl=nan, wps=5577.2, ups=0.27, wpb=20505, bsz=253.8, num_updates=33700, lr=0.000192593, gnorm=1.698, clip=0, loss_scale=2048, train_wall=248, wall=100800
2022-07-13 18:44:20 | INFO | train_inner | epoch 031:    187 / 1122 loss=6.09, nll_loss=2.191, mask_ins=0.915, word_ins_ml=3.853, word_reposition=0.623, kpe=0.7, ppl=68.14, wps=7207.5, ups=0.35, wpb=20621.5, bsz=256, num_updates=33800, lr=0.000192308, gnorm=1.631, clip=0, loss_scale=2048, train_wall=248, wall=101086
2022-07-13 18:49:07 | INFO | train_inner | epoch 031:    287 / 1122 loss=6.108, nll_loss=2.189, mask_ins=0.921, word_ins_ml=3.85, word_reposition=0.633, kpe=0.703, ppl=68.98, wps=7107.8, ups=0.35, wpb=20406.4, bsz=256, num_updates=33900, lr=0.000192024, gnorm=1.611, clip=0, loss_scale=2048, train_wall=250, wall=101373
2022-07-13 18:53:54 | INFO | train_inner | epoch 031:    387 / 1122 loss=6.105, nll_loss=2.193, mask_ins=0.915, word_ins_ml=3.854, word_reposition=0.631, kpe=0.705, ppl=68.84, wps=7186.5, ups=0.35, wpb=20597.6, bsz=256, num_updates=34000, lr=0.000191741, gnorm=1.658, clip=0, loss_scale=2048, train_wall=249, wall=101660
2022-07-13 18:58:53 | INFO | train_inner | epoch 031:    487 / 1122 loss=6.064, nll_loss=2.178, mask_ins=0.901, word_ins_ml=3.84, word_reposition=0.616, kpe=0.707, ppl=66.9, wps=6843.9, ups=0.33, wpb=20434.2, bsz=256, num_updates=34100, lr=0.00019146, gnorm=1.635, clip=0, loss_scale=3420, train_wall=261, wall=101958
2022-07-13 19:04:15 | INFO | train_inner | epoch 031:    587 / 1122 loss=6.058, nll_loss=2.156, mask_ins=0.904, word_ins_ml=3.821, word_reposition=0.625, kpe=0.709, ppl=66.64, wps=6406.8, ups=0.31, wpb=20634.6, bsz=256, num_updates=34200, lr=0.00019118, gnorm=1.573, clip=0, loss_scale=4096, train_wall=284, wall=102280
2022-07-13 19:09:02 | INFO | train_inner | epoch 031:    687 / 1122 loss=6.11, nll_loss=2.191, mask_ins=0.906, word_ins_ml=3.852, word_reposition=0.642, kpe=0.71, ppl=69.09, wps=7148.5, ups=0.35, wpb=20553.4, bsz=256, num_updates=34300, lr=0.000190901, gnorm=1.661, clip=0, loss_scale=4096, train_wall=250, wall=102568
2022-07-13 19:10:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 19:13:52 | INFO | train_inner | epoch 031:    788 / 1122 loss=6.085, nll_loss=2.157, mask_ins=0.913, word_ins_ml=3.822, word_reposition=0.635, kpe=0.715, ppl=67.87, wps=7092.9, ups=0.35, wpb=20553.7, bsz=256, num_updates=34400, lr=0.000190623, gnorm=1.657, clip=0, loss_scale=2535, train_wall=252, wall=102858
2022-07-13 19:18:38 | INFO | train_inner | epoch 031:    888 / 1122 loss=6.076, nll_loss=2.147, mask_ins=0.914, word_ins_ml=3.813, word_reposition=0.635, kpe=0.714, ppl=67.47, wps=7162.2, ups=0.35, wpb=20486.5, bsz=256, num_updates=34500, lr=0.000190347, gnorm=1.645, clip=0, loss_scale=2048, train_wall=249, wall=103144
2022-07-13 19:23:26 | INFO | train_inner | epoch 031:    988 / 1122 loss=6.098, nll_loss=2.175, mask_ins=0.913, word_ins_ml=3.838, word_reposition=0.634, kpe=0.713, ppl=68.5, wps=7097.7, ups=0.35, wpb=20413.2, bsz=256, num_updates=34600, lr=0.000190071, gnorm=1.695, clip=0, loss_scale=2048, train_wall=250, wall=103431
2022-07-13 19:28:13 | INFO | train_inner | epoch 031:   1088 / 1122 loss=6.066, nll_loss=2.169, mask_ins=0.905, word_ins_ml=3.832, word_reposition=0.617, kpe=0.712, ppl=66.98, wps=7156.5, ups=0.35, wpb=20541.3, bsz=256, num_updates=34700, lr=0.000189797, gnorm=1.565, clip=0, loss_scale=2048, train_wall=250, wall=103718
2022-07-13 19:29:49 | INFO | train | epoch 031 | loss nan | nll_loss 2.175 | mask_ins 0.91 | word_ins_ml 3.837 | word_reposition 0.628 | kpe nan | ppl nan | wps 6875.1 | ups 0.34 | wpb 20521.6 | bsz 255.8 | num_updates 34734 | lr 0.000189704 | gnorm 1.64 | clip 0 | loss_scale 2579 | train_wall 2844 | wall 103815
2022-07-13 19:31:08 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 12.446 | nll_loss 6.263 | mask_ins 2.001 | word_ins_ml 7.604 | word_reposition 1.308 | kpe 1.532 | ppl 5577.97 | wps 12564.2 | wpb 2367.6 | bsz 32 | num_updates 34734 | best_loss 12.329
2022-07-13 19:31:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 31 @ 34734 updates, score 12.446) (writing took 3.6861240779981017 seconds)
2022-07-13 19:34:20 | INFO | train_inner | epoch 032:     66 / 1122 loss=nan, nll_loss=2.173, mask_ins=0.912, word_ins_ml=3.835, word_reposition=0.62, kpe=nan, ppl=nan, wps=5530.6, ups=0.27, wpb=20331.6, bsz=253.8, num_updates=34800, lr=0.000189525, gnorm=1.71, clip=0, loss_scale=2048, train_wall=248, wall=104086
2022-07-13 19:39:07 | INFO | train_inner | epoch 032:    166 / 1122 loss=6.073, nll_loss=2.161, mask_ins=0.913, word_ins_ml=3.826, word_reposition=0.641, kpe=0.694, ppl=67.33, wps=7138.9, ups=0.35, wpb=20451.1, bsz=256, num_updates=34900, lr=0.000189253, gnorm=1.649, clip=0, loss_scale=3379, train_wall=249, wall=104373
2022-07-13 19:43:53 | INFO | train_inner | epoch 032:    266 / 1122 loss=6.053, nll_loss=2.156, mask_ins=0.905, word_ins_ml=3.821, word_reposition=0.633, kpe=0.694, ppl=66.39, wps=7165.6, ups=0.35, wpb=20527.4, bsz=256, num_updates=35000, lr=0.000188982, gnorm=1.634, clip=0, loss_scale=4096, train_wall=249, wall=104659
2022-07-13 19:48:40 | INFO | train_inner | epoch 032:    366 / 1122 loss=6.08, nll_loss=2.173, mask_ins=0.914, word_ins_ml=3.836, word_reposition=0.631, kpe=0.699, ppl=67.63, wps=7225, ups=0.35, wpb=20690.2, bsz=256, num_updates=35100, lr=0.000188713, gnorm=1.673, clip=0, loss_scale=4096, train_wall=249, wall=104945
2022-07-13 19:53:27 | INFO | train_inner | epoch 032:    466 / 1122 loss=6.089, nll_loss=2.177, mask_ins=0.91, word_ins_ml=3.839, word_reposition=0.643, kpe=0.697, ppl=68.07, wps=7222.8, ups=0.35, wpb=20715.2, bsz=256, num_updates=35200, lr=0.000188445, gnorm=1.68, clip=0, loss_scale=4096, train_wall=249, wall=105232
2022-07-13 19:58:14 | INFO | train_inner | epoch 032:    566 / 1122 loss=nan, nll_loss=2.147, mask_ins=0.901, word_ins_ml=3.813, word_reposition=0.624, kpe=nan, ppl=nan, wps=7161.5, ups=0.35, wpb=20554.8, bsz=256, num_updates=35300, lr=0.000188177, gnorm=1.562, clip=0, loss_scale=4096, train_wall=249, wall=105519
2022-07-13 20:02:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-13 20:03:14 | INFO | train_inner | epoch 032:    667 / 1122 loss=6.048, nll_loss=2.14, mask_ins=0.904, word_ins_ml=3.807, word_reposition=0.634, kpe=0.704, ppl=66.18, wps=6808.5, ups=0.33, wpb=20439.1, bsz=256, num_updates=35400, lr=0.000187912, gnorm=1.675, clip=0, loss_scale=5880, train_wall=262, wall=105819
2022-07-13 20:04:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 20:08:39 | INFO | train_inner | epoch 032:    768 / 1122 loss=6.068, nll_loss=2.172, mask_ins=0.907, word_ins_ml=3.835, word_reposition=0.623, kpe=0.703, ppl=67.09, wps=6253.5, ups=0.31, wpb=20364.1, bsz=256, num_updates=35500, lr=0.000187647, gnorm=1.637, clip=0, loss_scale=2352, train_wall=288, wall=106145
2022-07-13 20:13:27 | INFO | train_inner | epoch 032:    868 / 1122 loss=6.066, nll_loss=2.163, mask_ins=0.901, word_ins_ml=3.827, word_reposition=0.632, kpe=0.706, ppl=67.01, wps=7098.1, ups=0.35, wpb=20411.2, bsz=256, num_updates=35600, lr=0.000187383, gnorm=1.642, clip=0, loss_scale=2048, train_wall=250, wall=106433
2022-07-13 20:18:14 | INFO | train_inner | epoch 032:    968 / 1122 loss=6.069, nll_loss=2.174, mask_ins=0.905, word_ins_ml=3.836, word_reposition=0.624, kpe=0.704, ppl=67.14, wps=7191.9, ups=0.35, wpb=20638.5, bsz=256, num_updates=35700, lr=0.00018712, gnorm=1.629, clip=0, loss_scale=2048, train_wall=249, wall=106720
2022-07-13 20:23:01 | INFO | train_inner | epoch 032:   1068 / 1122 loss=nan, nll_loss=2.156, mask_ins=0.91, word_ins_ml=3.821, word_reposition=0.625, kpe=nan, ppl=nan, wps=7171.8, ups=0.35, wpb=20572.4, bsz=256, num_updates=35800, lr=0.000186859, gnorm=1.658, clip=0, loss_scale=2048, train_wall=249, wall=107007
2022-07-13 20:25:35 | INFO | train | epoch 032 | loss nan | nll_loss 2.164 | mask_ins 0.907 | word_ins_ml 3.828 | word_reposition 0.631 | kpe nan | ppl nan | wps 6869.5 | ups 0.33 | wpb 20520.8 | bsz 255.8 | num_updates 35854 | lr 0.000186718 | gnorm 1.652 | clip 0 | loss_scale 3269 | train_wall 2843 | wall 107160
2022-07-13 20:26:53 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 12.744 | nll_loss 6.436 | mask_ins 2.003 | word_ins_ml 7.774 | word_reposition 1.383 | kpe 1.584 | ppl 6860.83 | wps 12591.4 | wpb 2367.6 | bsz 32 | num_updates 35854 | best_loss 12.329
2022-07-13 20:26:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 32 @ 35854 updates, score 12.744) (writing took 3.490887885913253 seconds)
2022-07-13 20:29:08 | INFO | train_inner | epoch 033:     46 / 1122 loss=6.098, nll_loss=2.202, mask_ins=0.905, word_ins_ml=3.862, word_reposition=0.631, kpe=0.7, ppl=68.52, wps=5555, ups=0.27, wpb=20418.2, bsz=253.8, num_updates=35900, lr=0.000186598, gnorm=1.783, clip=0, loss_scale=2048, train_wall=248, wall=107374
2022-07-13 20:33:55 | INFO | train_inner | epoch 033:    146 / 1122 loss=6.049, nll_loss=2.154, mask_ins=0.909, word_ins_ml=3.819, word_reposition=0.636, kpe=0.686, ppl=66.23, wps=7206.3, ups=0.35, wpb=20675, bsz=256, num_updates=36000, lr=0.000186339, gnorm=1.675, clip=0, loss_scale=3564, train_wall=249, wall=107661
2022-07-13 20:38:42 | INFO | train_inner | epoch 033:    246 / 1122 loss=6.039, nll_loss=2.152, mask_ins=0.906, word_ins_ml=3.817, word_reposition=0.629, kpe=0.687, ppl=65.75, wps=7157.8, ups=0.35, wpb=20537, bsz=256, num_updates=36100, lr=0.000186081, gnorm=1.646, clip=0, loss_scale=4096, train_wall=250, wall=107948
2022-07-13 20:43:29 | INFO | train_inner | epoch 033:    346 / 1122 loss=6.011, nll_loss=2.13, mask_ins=0.898, word_ins_ml=3.797, word_reposition=0.627, kpe=0.689, ppl=64.5, wps=7179.2, ups=0.35, wpb=20584, bsz=256, num_updates=36200, lr=0.000185824, gnorm=1.67, clip=0, loss_scale=4096, train_wall=249, wall=108235
2022-07-13 20:44:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 20:48:19 | INFO | train_inner | epoch 033:    447 / 1122 loss=6.087, nll_loss=2.189, mask_ins=0.909, word_ins_ml=3.851, word_reposition=0.632, kpe=0.695, ppl=67.98, wps=7052.4, ups=0.35, wpb=20437.6, bsz=256, num_updates=36300, lr=0.000185567, gnorm=1.778, clip=0, loss_scale=2332, train_wall=252, wall=108524
2022-07-13 20:53:06 | INFO | train_inner | epoch 033:    547 / 1122 loss=6.048, nll_loss=2.15, mask_ins=0.907, word_ins_ml=3.815, word_reposition=0.637, kpe=0.689, ppl=66.18, wps=7145.4, ups=0.35, wpb=20521.4, bsz=256, num_updates=36400, lr=0.000185312, gnorm=1.651, clip=0, loss_scale=2048, train_wall=250, wall=108812
2022-07-13 20:57:52 | INFO | train_inner | epoch 033:    647 / 1122 loss=nan, nll_loss=2.148, mask_ins=0.902, word_ins_ml=3.814, word_reposition=0.627, kpe=nan, ppl=nan, wps=7149.6, ups=0.35, wpb=20485.9, bsz=256, num_updates=36500, lr=0.000185058, gnorm=1.632, clip=0, loss_scale=2048, train_wall=249, wall=109098
2022-07-13 21:02:38 | INFO | train_inner | epoch 033:    747 / 1122 loss=6.053, nll_loss=2.161, mask_ins=0.906, word_ins_ml=3.824, word_reposition=0.624, kpe=0.698, ppl=66.38, wps=7170.3, ups=0.35, wpb=20494.4, bsz=256, num_updates=36600, lr=0.000184805, gnorm=1.716, clip=0, loss_scale=2048, train_wall=248, wall=109384
2022-07-13 21:07:37 | INFO | train_inner | epoch 033:    847 / 1122 loss=6.053, nll_loss=2.17, mask_ins=0.893, word_ins_ml=3.833, word_reposition=0.629, kpe=0.698, ppl=66.38, wps=6869.5, ups=0.34, wpb=20487.5, bsz=256, num_updates=36700, lr=0.000184553, gnorm=1.683, clip=0, loss_scale=2048, train_wall=261, wall=109682
2022-07-13 21:11:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 21:13:00 | INFO | train_inner | epoch 033:    948 / 1122 loss=6.014, nll_loss=2.129, mask_ins=0.899, word_ins_ml=3.797, word_reposition=0.62, kpe=0.698, ppl=64.62, wps=6340.1, ups=0.31, wpb=20510.1, bsz=256, num_updates=36800, lr=0.000184302, gnorm=1.713, clip=0, loss_scale=3001, train_wall=285, wall=110006
2022-07-13 21:17:48 | INFO | train_inner | epoch 033:   1048 / 1122 loss=nan, nll_loss=2.166, mask_ins=0.908, word_ins_ml=3.829, word_reposition=0.632, kpe=nan, ppl=nan, wps=7171.4, ups=0.35, wpb=20620.1, bsz=256, num_updates=36900, lr=0.000184053, gnorm=1.68, clip=0, loss_scale=2048, train_wall=250, wall=110293
2022-07-13 21:21:19 | INFO | train | epoch 033 | loss nan | nll_loss 2.158 | mask_ins 0.904 | word_ins_ml 3.822 | word_reposition 0.629 | kpe nan | ppl nan | wps 6871.5 | ups 0.33 | wpb 20519.2 | bsz 255.8 | num_updates 36974 | lr 0.000183868 | gnorm 1.694 | clip 0 | loss_scale 2659 | train_wall 2842 | wall 110505
2022-07-13 21:22:38 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 12.548 | nll_loss 6.37 | mask_ins 2.002 | word_ins_ml 7.707 | word_reposition 1.327 | kpe 1.513 | ppl 5989.18 | wps 12591.4 | wpb 2367.6 | bsz 32 | num_updates 36974 | best_loss 12.329
2022-07-13 21:22:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 33 @ 36974 updates, score 12.548) (writing took 3.8279931088909507 seconds)
2022-07-13 21:23:56 | INFO | train_inner | epoch 034:     26 / 1122 loss=6.055, nll_loss=2.166, mask_ins=0.907, word_ins_ml=3.829, word_reposition=0.623, kpe=0.696, ppl=66.49, wps=5523.6, ups=0.27, wpb=20341.4, bsz=253.8, num_updates=37000, lr=0.000183804, gnorm=1.752, clip=0, loss_scale=2048, train_wall=249, wall=110662
2022-07-13 21:28:42 | INFO | train_inner | epoch 034:    126 / 1122 loss=6.006, nll_loss=2.158, mask_ins=0.892, word_ins_ml=3.822, word_reposition=0.614, kpe=0.678, ppl=64.25, wps=7145, ups=0.35, wpb=20456.1, bsz=256, num_updates=37100, lr=0.000183556, gnorm=1.724, clip=0, loss_scale=2048, train_wall=249, wall=110948
2022-07-13 21:33:29 | INFO | train_inner | epoch 034:    226 / 1122 loss=6.009, nll_loss=2.157, mask_ins=0.892, word_ins_ml=3.821, word_reposition=0.616, kpe=0.68, ppl=64.38, wps=7134.3, ups=0.35, wpb=20459.1, bsz=256, num_updates=37200, lr=0.000183309, gnorm=1.736, clip=0, loss_scale=2048, train_wall=249, wall=111235
2022-07-13 21:38:15 | INFO | train_inner | epoch 034:    326 / 1122 loss=5.976, nll_loss=2.113, mask_ins=0.894, word_ins_ml=3.783, word_reposition=0.618, kpe=0.682, ppl=62.96, wps=7165.7, ups=0.35, wpb=20500.5, bsz=256, num_updates=37300, lr=0.000183063, gnorm=1.641, clip=0, loss_scale=2396, train_wall=249, wall=111521
2022-07-13 21:43:02 | INFO | train_inner | epoch 034:    426 / 1122 loss=nan, nll_loss=2.147, mask_ins=0.899, word_ins_ml=3.812, word_reposition=0.633, kpe=nan, ppl=nan, wps=7157, ups=0.35, wpb=20517.1, bsz=256, num_updates=37400, lr=0.000182818, gnorm=1.73, clip=0, loss_scale=4096, train_wall=249, wall=111807
2022-07-13 21:47:48 | INFO | train_inner | epoch 034:    526 / 1122 loss=6.071, nll_loss=2.188, mask_ins=0.904, word_ins_ml=3.849, word_reposition=0.627, kpe=0.691, ppl=67.22, wps=7158.2, ups=0.35, wpb=20526.1, bsz=256, num_updates=37500, lr=0.000182574, gnorm=1.766, clip=0, loss_scale=4096, train_wall=249, wall=112094
2022-07-13 21:52:35 | INFO | train_inner | epoch 034:    626 / 1122 loss=6.004, nll_loss=2.131, mask_ins=0.892, word_ins_ml=3.798, word_reposition=0.628, kpe=0.686, ppl=64.18, wps=7173.4, ups=0.35, wpb=20587.2, bsz=256, num_updates=37600, lr=0.000182331, gnorm=1.643, clip=0, loss_scale=4096, train_wall=250, wall=112381
2022-07-13 21:53:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 21:57:25 | INFO | train_inner | epoch 034:    727 / 1122 loss=6.03, nll_loss=2.155, mask_ins=0.901, word_ins_ml=3.819, word_reposition=0.621, kpe=0.688, ppl=65.33, wps=7149.6, ups=0.35, wpb=20674.3, bsz=256, num_updates=37700, lr=0.000182089, gnorm=1.687, clip=0, loss_scale=2352, train_wall=251, wall=112670
2022-07-13 22:02:11 | INFO | train_inner | epoch 034:    827 / 1122 loss=5.993, nll_loss=2.119, mask_ins=0.898, word_ins_ml=3.787, word_reposition=0.619, kpe=0.69, ppl=63.71, wps=7176, ups=0.35, wpb=20579.5, bsz=256, num_updates=37800, lr=0.000181848, gnorm=1.711, clip=0, loss_scale=2048, train_wall=249, wall=112957
2022-07-13 22:06:58 | INFO | train_inner | epoch 034:    927 / 1122 loss=5.997, nll_loss=2.126, mask_ins=0.895, word_ins_ml=3.794, word_reposition=0.616, kpe=0.693, ppl=63.89, wps=7176.1, ups=0.35, wpb=20569.1, bsz=256, num_updates=37900, lr=0.000181608, gnorm=1.662, clip=0, loss_scale=2048, train_wall=249, wall=113244
2022-07-13 22:11:57 | INFO | train_inner | epoch 034:   1027 / 1122 loss=6.038, nll_loss=2.151, mask_ins=0.904, word_ins_ml=3.816, word_reposition=0.624, kpe=0.694, ppl=65.69, wps=6848.6, ups=0.33, wpb=20483.8, bsz=256, num_updates=38000, lr=0.000181369, gnorm=1.673, clip=0, loss_scale=2048, train_wall=261, wall=113543
2022-07-13 22:17:02 | INFO | train | epoch 034 | loss nan | nll_loss 2.141 | mask_ins 0.897 | word_ins_ml 3.807 | word_reposition 0.621 | kpe nan | ppl nan | wps 6882.1 | ups 0.34 | wpb 20521.6 | bsz 255.8 | num_updates 38095 | lr 0.000181143 | gnorm 1.702 | clip 0 | loss_scale 2654 | train_wall 2839 | wall 113847
2022-07-13 22:18:20 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 12.524 | nll_loss 6.29 | mask_ins 2.01 | word_ins_ml 7.626 | word_reposition 1.344 | kpe 1.543 | ppl 5888.79 | wps 12579.9 | wpb 2367.6 | bsz 32 | num_updates 38095 | best_loss 12.329
2022-07-13 22:18:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 34 @ 38095 updates, score 12.524) (writing took 3.864688042551279 seconds)
2022-07-13 22:18:38 | INFO | train_inner | epoch 035:      5 / 1122 loss=5.969, nll_loss=2.098, mask_ins=0.893, word_ins_ml=3.769, word_reposition=0.617, kpe=0.69, ppl=62.63, wps=5077.7, ups=0.25, wpb=20370.9, bsz=253.8, num_updates=38100, lr=0.000181131, gnorm=1.744, clip=0, loss_scale=2048, train_wall=281, wall=113944
2022-07-13 22:20:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 22:23:27 | INFO | train_inner | epoch 035:    106 / 1122 loss=nan, nll_loss=2.142, mask_ins=0.893, word_ins_ml=3.809, word_reposition=0.623, kpe=nan, ppl=nan, wps=7089.8, ups=0.35, wpb=20499.3, bsz=256, num_updates=38200, lr=0.000180894, gnorm=1.738, clip=0, loss_scale=2129, train_wall=251, wall=114233
2022-07-13 22:28:14 | INFO | train_inner | epoch 035:    206 / 1122 loss=6.001, nll_loss=2.144, mask_ins=0.899, word_ins_ml=3.81, word_reposition=0.622, kpe=0.67, ppl=64.03, wps=7166.2, ups=0.35, wpb=20525.2, bsz=256, num_updates=38300, lr=0.000180657, gnorm=1.72, clip=0, loss_scale=2048, train_wall=249, wall=114520
2022-07-13 22:33:01 | INFO | train_inner | epoch 035:    306 / 1122 loss=5.987, nll_loss=2.122, mask_ins=0.9, word_ins_ml=3.79, word_reposition=0.621, kpe=0.675, ppl=63.43, wps=7173.7, ups=0.35, wpb=20582.3, bsz=256, num_updates=38400, lr=0.000180422, gnorm=1.71, clip=0, loss_scale=2048, train_wall=249, wall=114807
2022-07-13 22:37:47 | INFO | train_inner | epoch 035:    406 / 1122 loss=6.012, nll_loss=2.146, mask_ins=0.901, word_ins_ml=3.811, word_reposition=0.621, kpe=0.679, ppl=64.53, wps=7171.7, ups=0.35, wpb=20554.6, bsz=256, num_updates=38500, lr=0.000180187, gnorm=1.753, clip=0, loss_scale=2048, train_wall=249, wall=115093
2022-07-13 22:42:35 | INFO | train_inner | epoch 035:    506 / 1122 loss=nan, nll_loss=2.151, mask_ins=0.896, word_ins_ml=3.816, word_reposition=0.62, kpe=nan, ppl=nan, wps=7178.1, ups=0.35, wpb=20610.7, bsz=256, num_updates=38600, lr=0.000179954, gnorm=1.812, clip=0, loss_scale=2048, train_wall=249, wall=115380
2022-07-13 22:47:21 | INFO | train_inner | epoch 035:    606 / 1122 loss=5.981, nll_loss=2.114, mask_ins=0.892, word_ins_ml=3.783, word_reposition=0.626, kpe=0.68, ppl=63.15, wps=7137.9, ups=0.35, wpb=20421.4, bsz=256, num_updates=38700, lr=0.000179721, gnorm=1.709, clip=0, loss_scale=3256, train_wall=248, wall=115666
2022-07-13 22:49:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-13 22:52:10 | INFO | train_inner | epoch 035:    707 / 1122 loss=6.012, nll_loss=2.145, mask_ins=0.898, word_ins_ml=3.81, word_reposition=0.622, kpe=0.682, ppl=64.54, wps=7123.3, ups=0.35, wpb=20604.4, bsz=256, num_updates=38800, lr=0.00017949, gnorm=1.718, clip=0, loss_scale=2737, train_wall=251, wall=115956
2022-07-13 22:56:57 | INFO | train_inner | epoch 035:    807 / 1122 loss=5.986, nll_loss=2.112, mask_ins=0.895, word_ins_ml=3.781, word_reposition=0.627, kpe=0.683, ppl=63.39, wps=7139.2, ups=0.35, wpb=20468.3, bsz=256, num_updates=38900, lr=0.000179259, gnorm=1.787, clip=0, loss_scale=2048, train_wall=249, wall=116242
2022-07-13 23:01:43 | INFO | train_inner | epoch 035:    907 / 1122 loss=5.995, nll_loss=2.132, mask_ins=0.89, word_ins_ml=3.799, word_reposition=0.623, kpe=0.684, ppl=63.79, wps=7148.7, ups=0.35, wpb=20495.2, bsz=256, num_updates=39000, lr=0.000179029, gnorm=1.705, clip=0, loss_scale=2048, train_wall=249, wall=116529
2022-07-13 23:06:30 | INFO | train_inner | epoch 035:   1007 / 1122 loss=6.051, nll_loss=2.156, mask_ins=0.908, word_ins_ml=3.82, word_reposition=0.634, kpe=0.689, ppl=66.3, wps=7156.6, ups=0.35, wpb=20541.9, bsz=256, num_updates=39100, lr=0.0001788, gnorm=1.731, clip=0, loss_scale=2048, train_wall=250, wall=116816
2022-07-13 23:11:17 | INFO | train_inner | epoch 035:   1107 / 1122 loss=5.956, nll_loss=2.113, mask_ins=0.879, word_ins_ml=3.782, word_reposition=0.611, kpe=0.684, ppl=62.06, wps=7183.3, ups=0.35, wpb=20578.9, bsz=256, num_updates=39200, lr=0.000178571, gnorm=1.668, clip=0, loss_scale=2048, train_wall=249, wall=117103
2022-07-13 23:11:59 | INFO | train | epoch 035 | loss nan | nll_loss 2.133 | mask_ins 0.895 | word_ins_ml 3.8 | word_reposition 0.622 | kpe nan | ppl nan | wps 6972 | ups 0.34 | wpb 20522.6 | bsz 255.8 | num_updates 39215 | lr 0.000178537 | gnorm 1.737 | clip 0 | loss_scale 2225 | train_wall 2793 | wall 117144
2022-07-13 23:13:17 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 12.441 | nll_loss 6.223 | mask_ins 1.982 | word_ins_ml 7.567 | word_reposition 1.345 | kpe 1.547 | ppl 5559.84 | wps 12567.8 | wpb 2367.6 | bsz 32 | num_updates 39215 | best_loss 12.329
2022-07-13 23:13:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 35 @ 39215 updates, score 12.441) (writing took 3.614936588332057 seconds)
2022-07-13 23:17:36 | INFO | train_inner | epoch 036:     85 / 1122 loss=nan, nll_loss=2.102, mask_ins=0.89, word_ins_ml=3.773, word_reposition=0.615, kpe=nan, ppl=nan, wps=5384.1, ups=0.26, wpb=20408.2, bsz=253.8, num_updates=39300, lr=0.000178344, gnorm=1.766, clip=0, loss_scale=3174, train_wall=260, wall=117482
2022-07-13 23:22:59 | INFO | train_inner | epoch 036:    185 / 1122 loss=5.969, nll_loss=2.128, mask_ins=0.893, word_ins_ml=3.795, word_reposition=0.614, kpe=0.666, ppl=62.64, wps=6372.9, ups=0.31, wpb=20569.3, bsz=256, num_updates=39400, lr=0.000178118, gnorm=1.729, clip=0, loss_scale=4096, train_wall=285, wall=117804
2022-07-13 23:27:45 | INFO | train_inner | epoch 036:    285 / 1122 loss=5.958, nll_loss=2.121, mask_ins=0.885, word_ins_ml=3.789, word_reposition=0.614, kpe=0.669, ppl=62.16, wps=7172.6, ups=0.35, wpb=20556.8, bsz=256, num_updates=39500, lr=0.000177892, gnorm=1.714, clip=0, loss_scale=4096, train_wall=249, wall=118091
2022-07-13 23:32:32 | INFO | train_inner | epoch 036:    385 / 1122 loss=5.951, nll_loss=2.109, mask_ins=0.886, word_ins_ml=3.778, word_reposition=0.621, kpe=0.666, ppl=61.87, wps=7181.7, ups=0.35, wpb=20584, bsz=256, num_updates=39600, lr=0.000177667, gnorm=1.654, clip=0, loss_scale=4096, train_wall=249, wall=118378
2022-07-13 23:37:18 | INFO | train_inner | epoch 036:    485 / 1122 loss=5.984, nll_loss=2.128, mask_ins=0.891, word_ins_ml=3.795, word_reposition=0.627, kpe=0.67, ppl=63.29, wps=7150.1, ups=0.35, wpb=20466.6, bsz=256, num_updates=39700, lr=0.000177443, gnorm=1.727, clip=0, loss_scale=4096, train_wall=248, wall=118664
2022-07-13 23:42:05 | INFO | train_inner | epoch 036:    585 / 1122 loss=5.95, nll_loss=2.105, mask_ins=0.892, word_ins_ml=3.775, word_reposition=0.613, kpe=0.67, ppl=61.81, wps=7142.5, ups=0.35, wpb=20474, bsz=256, num_updates=39800, lr=0.00017722, gnorm=1.694, clip=0, loss_scale=5857, train_wall=249, wall=118951
2022-07-13 23:44:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-13 23:46:53 | INFO | train_inner | epoch 036:    686 / 1122 loss=nan, nll_loss=2.131, mask_ins=0.901, word_ins_ml=3.798, word_reposition=0.63, kpe=nan, ppl=nan, wps=7126.6, ups=0.35, wpb=20566.1, bsz=256, num_updates=39900, lr=0.000176998, gnorm=1.697, clip=0, loss_scale=6448, train_wall=251, wall=119239
2022-07-13 23:51:40 | INFO | train_inner | epoch 036:    786 / 1122 loss=5.981, nll_loss=2.123, mask_ins=0.891, word_ins_ml=3.791, word_reposition=0.622, kpe=0.677, ppl=63.17, wps=7127.4, ups=0.35, wpb=20453.8, bsz=256, num_updates=40000, lr=0.000176777, gnorm=1.78, clip=0, loss_scale=4096, train_wall=250, wall=119526
2022-07-13 23:56:27 | INFO | train_inner | epoch 036:    886 / 1122 loss=5.981, nll_loss=2.14, mask_ins=0.886, word_ins_ml=3.805, word_reposition=0.613, kpe=0.677, ppl=63.18, wps=7143.7, ups=0.35, wpb=20461, bsz=256, num_updates=40100, lr=0.000176556, gnorm=1.747, clip=0, loss_scale=4096, train_wall=249, wall=119813
2022-07-14 00:01:14 | INFO | train_inner | epoch 036:    986 / 1122 loss=5.976, nll_loss=2.111, mask_ins=0.9, word_ins_ml=3.78, word_reposition=0.622, kpe=0.674, ppl=62.93, wps=7205.9, ups=0.35, wpb=20660.2, bsz=256, num_updates=40200, lr=0.000176336, gnorm=1.656, clip=0, loss_scale=4096, train_wall=249, wall=120099
2022-07-14 00:06:00 | INFO | train_inner | epoch 036:   1086 / 1122 loss=5.956, nll_loss=2.103, mask_ins=0.888, word_ins_ml=3.772, word_reposition=0.616, kpe=0.68, ppl=62.07, wps=7174.6, ups=0.35, wpb=20554.7, bsz=256, num_updates=40300, lr=0.000176117, gnorm=1.662, clip=0, loss_scale=4096, train_wall=249, wall=120386
2022-07-14 00:07:42 | INFO | train | epoch 036 | loss nan | nll_loss 2.12 | mask_ins 0.892 | word_ins_ml 3.788 | word_reposition 0.62 | kpe nan | ppl nan | wps 6879.6 | ups 0.34 | wpb 20520.6 | bsz 255.8 | num_updates 40336 | lr 0.000176039 | gnorm 1.71 | clip 0 | loss_scale 4410 | train_wall 2840 | wall 120488
2022-07-14 00:09:01 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 12.708 | nll_loss 6.345 | mask_ins 2.028 | word_ins_ml 7.679 | word_reposition 1.389 | kpe 1.612 | ppl 6689.83 | wps 12563.1 | wpb 2367.6 | bsz 32 | num_updates 40336 | best_loss 12.329
2022-07-14 00:09:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 36 @ 40336 updates, score 12.708) (writing took 3.7022224655374885 seconds)
2022-07-14 00:12:08 | INFO | train_inner | epoch 037:     64 / 1122 loss=5.989, nll_loss=2.13, mask_ins=0.897, word_ins_ml=3.797, word_reposition=0.628, kpe=0.666, ppl=63.5, wps=5515.5, ups=0.27, wpb=20288.7, bsz=253.8, num_updates=40400, lr=0.000175899, gnorm=1.744, clip=0, loss_scale=5366, train_wall=248, wall=120754
2022-07-14 00:12:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-14 00:16:57 | INFO | train_inner | epoch 037:    165 / 1122 loss=nan, nll_loss=2.103, mask_ins=0.89, word_ins_ml=3.773, word_reposition=0.609, kpe=nan, ppl=nan, wps=7114.3, ups=0.35, wpb=20536.7, bsz=256, num_updates=40500, lr=0.000175682, gnorm=1.72, clip=0, loss_scale=4096, train_wall=251, wall=121042
2022-07-14 00:21:55 | INFO | train_inner | epoch 037:    265 / 1122 loss=5.945, nll_loss=2.107, mask_ins=0.885, word_ins_ml=3.776, word_reposition=0.625, kpe=0.66, ppl=61.61, wps=6881, ups=0.33, wpb=20565.7, bsz=256, num_updates=40600, lr=0.000175466, gnorm=1.728, clip=0, loss_scale=4096, train_wall=261, wall=121341
2022-07-14 00:27:18 | INFO | train_inner | epoch 037:    365 / 1122 loss=5.925, nll_loss=2.092, mask_ins=0.885, word_ins_ml=3.764, word_reposition=0.614, kpe=0.663, ppl=60.77, wps=6383.6, ups=0.31, wpb=20582.8, bsz=256, num_updates=40700, lr=0.00017525, gnorm=1.741, clip=0, loss_scale=4096, train_wall=285, wall=121664
2022-07-14 00:32:05 | INFO | train_inner | epoch 037:    465 / 1122 loss=5.915, nll_loss=2.092, mask_ins=0.882, word_ins_ml=3.763, word_reposition=0.608, kpe=0.661, ppl=60.34, wps=7160.2, ups=0.35, wpb=20527, bsz=256, num_updates=40800, lr=0.000175035, gnorm=1.753, clip=0, loss_scale=4096, train_wall=249, wall=121950
2022-07-14 00:33:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-14 00:36:54 | INFO | train_inner | epoch 037:    566 / 1122 loss=5.954, nll_loss=2.113, mask_ins=0.895, word_ins_ml=3.782, word_reposition=0.61, kpe=0.667, ppl=61.99, wps=7073.5, ups=0.35, wpb=20488, bsz=256, num_updates=40900, lr=0.000174821, gnorm=1.729, clip=0, loss_scale=2656, train_wall=252, wall=122240
2022-07-14 00:41:42 | INFO | train_inner | epoch 037:    666 / 1122 loss=5.983, nll_loss=2.129, mask_ins=0.892, word_ins_ml=3.796, word_reposition=0.626, kpe=0.668, ppl=63.24, wps=7171.1, ups=0.35, wpb=20625.3, bsz=256, num_updates=41000, lr=0.000174608, gnorm=1.723, clip=0, loss_scale=2048, train_wall=250, wall=122528
2022-07-14 00:46:28 | INFO | train_inner | epoch 037:    766 / 1122 loss=5.995, nll_loss=2.145, mask_ins=0.889, word_ins_ml=3.81, word_reposition=0.625, kpe=0.671, ppl=63.77, wps=7182.4, ups=0.35, wpb=20583.2, bsz=256, num_updates=41100, lr=0.000174395, gnorm=1.767, clip=0, loss_scale=2048, train_wall=249, wall=122814
2022-07-14 00:51:15 | INFO | train_inner | epoch 037:    866 / 1122 loss=5.948, nll_loss=2.105, mask_ins=0.888, word_ins_ml=3.775, word_reposition=0.62, kpe=0.665, ppl=61.72, wps=7154.4, ups=0.35, wpb=20527.5, bsz=256, num_updates=41200, lr=0.000174183, gnorm=1.726, clip=0, loss_scale=2048, train_wall=249, wall=123101
2022-07-14 00:56:02 | INFO | train_inner | epoch 037:    966 / 1122 loss=5.946, nll_loss=2.112, mask_ins=0.882, word_ins_ml=3.781, word_reposition=0.614, kpe=0.668, ppl=61.64, wps=7172.7, ups=0.35, wpb=20548, bsz=256, num_updates=41300, lr=0.000173972, gnorm=1.714, clip=0, loss_scale=2048, train_wall=249, wall=123388
2022-07-14 01:00:48 | INFO | train_inner | epoch 037:   1066 / 1122 loss=nan, nll_loss=2.127, mask_ins=0.894, word_ins_ml=3.795, word_reposition=0.614, kpe=nan, ppl=nan, wps=7150.1, ups=0.35, wpb=20494, bsz=256, num_updates=41400, lr=0.000173762, gnorm=1.714, clip=0, loss_scale=3256, train_wall=249, wall=123674
2022-07-14 01:03:28 | INFO | train | epoch 037 | loss nan | nll_loss 2.113 | mask_ins 0.888 | word_ins_ml 3.782 | word_reposition 0.616 | kpe nan | ppl nan | wps 6869.3 | ups 0.33 | wpb 20520.3 | bsz 255.8 | num_updates 41456 | lr 0.000173645 | gnorm 1.737 | clip 0 | loss_scale 3275 | train_wall 2842 | wall 123834
2022-07-14 01:04:47 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 12.84 | nll_loss 6.469 | mask_ins 2.025 | word_ins_ml 7.807 | word_reposition 1.381 | kpe 1.628 | ppl 7333.88 | wps 12607.2 | wpb 2367.6 | bsz 32 | num_updates 41456 | best_loss 12.329
2022-07-14 01:04:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 37 @ 41456 updates, score 12.84) (writing took 4.1566150691360235 seconds)
2022-07-14 01:06:56 | INFO | train_inner | epoch 038:     44 / 1122 loss=5.921, nll_loss=2.1, mask_ins=0.877, word_ins_ml=3.77, word_reposition=0.611, kpe=0.663, ppl=60.6, wps=5524.3, ups=0.27, wpb=20322.3, bsz=253.8, num_updates=41500, lr=0.000173553, gnorm=1.808, clip=0, loss_scale=4096, train_wall=248, wall=124042
2022-07-14 01:11:43 | INFO | train_inner | epoch 038:    144 / 1122 loss=nan, nll_loss=2.113, mask_ins=0.887, word_ins_ml=3.782, word_reposition=0.621, kpe=nan, ppl=nan, wps=7202.4, ups=0.35, wpb=20614.7, bsz=256, num_updates=41600, lr=0.000173344, gnorm=1.756, clip=0, loss_scale=4096, train_wall=249, wall=124328
2022-07-14 01:16:30 | INFO | train_inner | epoch 038:    244 / 1122 loss=5.96, nll_loss=2.139, mask_ins=0.887, word_ins_ml=3.805, word_reposition=0.612, kpe=0.655, ppl=62.25, wps=7181.5, ups=0.35, wpb=20619.5, bsz=256, num_updates=41700, lr=0.000173136, gnorm=1.747, clip=0, loss_scale=4096, train_wall=249, wall=124615
2022-07-14 01:21:28 | INFO | train_inner | epoch 038:    344 / 1122 loss=5.921, nll_loss=2.091, mask_ins=0.887, word_ins_ml=3.762, word_reposition=0.619, kpe=0.654, ppl=60.61, wps=6852.7, ups=0.34, wpb=20410.5, bsz=256, num_updates=41800, lr=0.000172929, gnorm=1.725, clip=0, loss_scale=4096, train_wall=260, wall=124913
2022-07-14 01:24:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-14 01:26:16 | INFO | train_inner | epoch 038:    445 / 1122 loss=5.93, nll_loss=2.102, mask_ins=0.883, word_ins_ml=3.772, word_reposition=0.619, kpe=0.655, ppl=60.95, wps=7076.3, ups=0.35, wpb=20419.8, bsz=256, num_updates=41900, lr=0.000172722, gnorm=1.674, clip=0, loss_scale=4137, train_wall=251, wall=125202
2022-07-14 01:31:38 | INFO | train_inner | epoch 038:    545 / 1122 loss=5.906, nll_loss=2.084, mask_ins=0.881, word_ins_ml=3.756, word_reposition=0.613, kpe=0.656, ppl=59.95, wps=6359, ups=0.31, wpb=20444.1, bsz=256, num_updates=42000, lr=0.000172516, gnorm=1.742, clip=0, loss_scale=4096, train_wall=284, wall=125523
2022-07-14 01:36:24 | INFO | train_inner | epoch 038:    645 / 1122 loss=5.946, nll_loss=2.098, mask_ins=0.893, word_ins_ml=3.769, word_reposition=0.622, kpe=0.662, ppl=61.63, wps=7191.1, ups=0.35, wpb=20565.2, bsz=256, num_updates=42100, lr=0.000172311, gnorm=1.744, clip=0, loss_scale=4096, train_wall=248, wall=125809
2022-07-14 01:41:10 | INFO | train_inner | epoch 038:    745 / 1122 loss=5.929, nll_loss=2.098, mask_ins=0.886, word_ins_ml=3.768, word_reposition=0.614, kpe=0.66, ppl=60.91, wps=7176.1, ups=0.35, wpb=20586.9, bsz=256, num_updates=42200, lr=0.000172107, gnorm=1.745, clip=0, loss_scale=4096, train_wall=249, wall=126096
2022-07-14 01:45:57 | INFO | train_inner | epoch 038:    845 / 1122 loss=5.913, nll_loss=2.09, mask_ins=0.878, word_ins_ml=3.762, word_reposition=0.612, kpe=0.661, ppl=60.26, wps=7168, ups=0.35, wpb=20543.6, bsz=256, num_updates=42300, lr=0.000171904, gnorm=1.707, clip=0, loss_scale=4096, train_wall=249, wall=126383
2022-07-14 01:50:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-14 01:50:46 | INFO | train_inner | epoch 038:    946 / 1122 loss=5.964, nll_loss=2.132, mask_ins=0.883, word_ins_ml=3.799, word_reposition=0.617, kpe=0.665, ppl=62.42, wps=7130.8, ups=0.35, wpb=20611, bsz=256, num_updates=42400, lr=0.000171701, gnorm=1.804, clip=0, loss_scale=5475, train_wall=251, wall=126672
2022-07-14 01:55:33 | INFO | train_inner | epoch 038:   1046 / 1122 loss=5.903, nll_loss=2.082, mask_ins=0.878, word_ins_ml=3.754, word_reposition=0.606, kpe=0.665, ppl=59.84, wps=7122.8, ups=0.35, wpb=20456.7, bsz=256, num_updates=42500, lr=0.000171499, gnorm=1.717, clip=0, loss_scale=4096, train_wall=250, wall=126959
2022-07-14 01:57:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-14 01:59:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-14 01:59:10 | INFO | train | epoch 038 | loss nan | nll_loss 2.104 | mask_ins 0.884 | word_ins_ml 3.774 | word_reposition 0.616 | kpe nan | ppl nan | wps 6864.1 | ups 0.33 | wpb 20518.2 | bsz 255.8 | num_updates 42574 | lr 0.000171349 | gnorm 1.739 | clip 0 | loss_scale 4156 | train_wall 2838 | wall 127176
2022-07-14 02:00:29 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 12.461 | nll_loss 6.22 | mask_ins 2.023 | word_ins_ml 7.559 | word_reposition 1.292 | kpe 1.586 | ppl 5638.71 | wps 12588.2 | wpb 2367.6 | bsz 32 | num_updates 42574 | best_loss 12.329
2022-07-14 02:00:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_keywords_cased/checkpoint_last.pt (epoch 38 @ 42574 updates, score 12.461) (writing took 3.772447128780186 seconds)
2022-07-14 02:01:46 | INFO | train_inner | epoch 039:     26 / 1122 loss=5.934, nll_loss=2.105, mask_ins=0.882, word_ins_ml=3.774, word_reposition=0.619, kpe=0.659, ppl=61.14, wps=5473.6, ups=0.27, wpb=20419, bsz=253.8, num_updates=42600, lr=0.000171297, gnorm=1.792, clip=0, loss_scale=2570, train_wall=253, wall=127332
2022-07-14 02:06:33 | INFO | train_inner | epoch 039:    126 / 1122 loss=5.905, nll_loss=2.095, mask_ins=0.882, word_ins_ml=3.766, word_reposition=0.613, kpe=0.645, ppl=59.93, wps=7134.2, ups=0.35, wpb=20475.9, bsz=256, num_updates=42700, lr=0.000171096, gnorm=1.808, clip=0, loss_scale=1024, train_wall=249, wall=127619
2022-07-14 02:11:21 | INFO | train_inner | epoch 039:    226 / 1122 loss=5.902, nll_loss=2.094, mask_ins=0.874, word_ins_ml=3.764, word_reposition=0.618, kpe=0.646, ppl=59.79, wps=7130.7, ups=0.35, wpb=20496.2, bsz=256, num_updates=42800, lr=0.000170896, gnorm=1.773, clip=0, loss_scale=1024, train_wall=250, wall=127907
2022-07-14 02:16:07 | INFO | train_inner | epoch 039:    326 / 1122 loss=5.894, nll_loss=2.097, mask_ins=0.879, word_ins_ml=3.767, word_reposition=0.6, kpe=0.648, ppl=59.45, wps=7187, ups=0.35, wpb=20586, bsz=256, num_updates=42900, lr=0.000170697, gnorm=1.719, clip=0, loss_scale=1024, train_wall=249, wall=128193
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
