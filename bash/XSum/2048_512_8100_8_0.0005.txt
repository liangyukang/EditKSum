nohup: ignoring input
2022-10-13 04:00:22 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15531
2022-10-13 04:00:22 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:15531
2022-10-13 04:00:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-10-13 04:00:22 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:15531
2022-10-13 04:00:22 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15531
2022-10-13 04:00:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-10-13 04:00:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-10-13 04:00:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-10-13 04:00:22 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-10-13 04:00:22 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-10-13 04:00:22 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-10-13 04:00:22 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-10-13 04:00:22 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-10-13 04:00:22 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-10-13 04:00:22 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-10-13 04:00:22 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-10-13 04:00:28 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=8100, max_sentences=None, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=8100, max_sentences_valid=None, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:15531', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=500008, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_transformer_transformer_kpe_cased_XSum', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='6,6,6', layers_num='6,6,6', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-XSum', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=2048, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510_XSum', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=False, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='transformer', decoder='transformer', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-10-13 04:00:28 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-10-13 04:00:28 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-10-13 04:00:29 | INFO | fairseq.data.data_utils | loaded 11332 examples from: ../data-bin-bert-cased-XSum/valid.source-target.source
2022-10-13 04:00:29 | INFO | fairseq.data.data_utils | loaded 11332 examples from: ../data-bin-bert-cased-XSum/valid.source-target.target
2022-10-13 04:00:29 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-XSum valid source-target 11332 examples
2022-10-13 04:00:29 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-10-13 04:00:29 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 2048,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
2022-10-13 04:00:31 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): EditorTransformerEncoder(
    (embed_tokens): Embedding(28996, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(2049, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): EditorTransformerDecoder(
    (embed_tokens): Embedding(28996, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
    (embed_mask_ins): Embedding(256, 1536)
    (layers_reposition): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-10-13 04:00:31 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-10-13 04:00:31 | INFO | fairseq_cli.train | num. model params: 152248320 (num. trained: 152248320)
2022-10-13 04:00:31 | INFO | fairseq_cli.train | num. Encoder model params: 56926464 (Encoder num. trained: 56926464)
2022-10-13 04:00:31 | INFO | fairseq_cli.train | num. Decoder model params: 117590784 (Decoder num. trained: 117590784)
2022-10-13 04:00:34 | INFO | fairseq_cli.train | training on 4 GPUs
2022-10-13 04:00:34 | INFO | fairseq_cli.train | max tokens per GPU = 8100 and max sentences per GPU = None
2022-10-13 04:00:34 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt
2022-10-13 04:00:34 | INFO | fairseq.trainer | loading train data for epoch 1
2022-10-13 04:00:34 | INFO | fairseq.data.data_utils | loaded 204045 examples from: ../data-bin-bert-cased-XSum/train.source-target.source
2022-10-13 04:00:34 | INFO | fairseq.data.data_utils | loaded 204045 examples from: ../data-bin-bert-cased-XSum/train.source-target.target
2022-10-13 04:00:34 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-XSum train source-target 204045 examples

2022-10-13 04:00:35 | WARNING | fairseq.data.data_utils | 1291 samples have invalid sizes and will be skipped, max_positions=(2048, 512), first few sample ids=[81783, 99129, 95369, 73710, 164788, 126943, 22738, 203955, 180526, 180055]

2022-10-13 04:00:35 | WARNING | fairseq.data.data_utils | 1291 samples have invalid sizes and will be skipped, max_positions=(2048, 512), first few sample ids=[81783, 99129, 95369, 73710, 164788, 126943, 22738, 203955, 180526, 180055]

2022-10-13 04:00:35 | WARNING | fairseq.data.data_utils | 1291 samples have invalid sizes and will be skipped, max_positions=(2048, 512), first few sample ids=[81783, 99129, 95369, 73710, 164788, 126943, 22738, 203955, 180526, 180055]
2022-10-13 04:00:35 | WARNING | fairseq.data.data_utils | 1291 samples have invalid sizes and will be skipped, max_positions=(2048, 512), first few sample ids=[81783, 99129, 95369, 73710, 164788, 126943, 22738, 203955, 180526, 180055]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-10-13 04:02:37 | INFO | train_inner | epoch 001:    100 / 487 loss=23.968, nll_loss=13.913, mask_ins=7.558, word_ins_ml=14.029, word_reposition=2.382, ppl=1.64138e+07, wps=11471.7, ups=0.83, wpb=13748.2, bsz=420.1, num_updates=100, lr=1.0098e-05, gnorm=18.499, clip=0, loss_scale=128, train_wall=120, wall=123
2022-10-13 04:04:36 | INFO | train_inner | epoch 001:    200 / 487 loss=17.925, nll_loss=12.238, mask_ins=4.363, word_ins_ml=12.53, word_reposition=1.032, ppl=248827, wps=11571.6, ups=0.84, wpb=13850.7, bsz=420, num_updates=200, lr=2.0096e-05, gnorm=18.917, clip=0, loss_scale=128, train_wall=119, wall=242
2022-10-13 04:06:36 | INFO | train_inner | epoch 001:    300 / 487 loss=14.498, nll_loss=10.9, mask_ins=2.234, word_ins_ml=11.377, word_reposition=0.887, ppl=23133.6, wps=11654.9, ups=0.83, wpb=13978.8, bsz=426.3, num_updates=300, lr=3.0094e-05, gnorm=4.004, clip=0, loss_scale=128, train_wall=119, wall=362
2022-10-13 04:08:35 | INFO | train_inner | epoch 001:    400 / 487 loss=13.757, nll_loss=10.3, mask_ins=1.938, word_ins_ml=10.922, word_reposition=0.897, ppl=13848, wps=11302.7, ups=0.84, wpb=13443.3, bsz=406.2, num_updates=400, lr=4.0092e-05, gnorm=1.823, clip=0, loss_scale=128, train_wall=118, wall=481
2022-10-13 04:10:18 | INFO | train | epoch 001 | loss 16.844 | nll_loss 11.545 | mask_ins 3.642 | word_ins_ml 11.973 | word_reposition 1.23 | ppl 117669 | wps 11487.2 | ups 0.84 | wpb 13707.9 | bsz 416.2 | num_updates 487 | lr 4.87903e-05 | gnorm 9.164 | clip 0 | loss_scale 128 | train_wall 577 | wall 584
2022-10-13 04:10:18 | WARNING | fairseq.data.data_utils | 57 samples have invalid sizes and will be skipped, max_positions=(2048, 512), first few sample ids=[4035, 6763, 6786, 9418, 8487, 5896, 3904, 1150, 83, 4003]
2022-10-13 04:10:18 | WARNING | fairseq.data.data_utils | 57 samples have invalid sizes and will be skipped, max_positions=(2048, 512), first few sample ids=[4035, 6763, 6786, 9418, 8487, 5896, 3904, 1150, 83, 4003]
2022-10-13 04:10:18 | WARNING | fairseq.data.data_utils | 57 samples have invalid sizes and will be skipped, max_positions=(2048, 512), first few sample ids=[4035, 6763, 6786, 9418, 8487, 5896, 3904, 1150, 83, 4003]
2022-10-13 04:10:18 | WARNING | fairseq.data.data_utils | 57 samples have invalid sizes and will be skipped, max_positions=(2048, 512), first few sample ids=[4035, 6763, 6786, 9418, 8487, 5896, 3904, 1150, 83, 4003]
2022-10-13 04:10:30 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.338 | nll_loss 10.786 | mask_ins 2.177 | word_ins_ml 11.373 | word_reposition 0.789 | ppl 20708.6 | wps 27754.6 | wpb 1524.1 | bsz 52.2 | num_updates 487
2022-10-13 04:10:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 1 @ 487 updates, score 14.338) (writing took 2.7669838340079878 seconds)
2022-10-13 04:10:48 | INFO | train_inner | epoch 002:     13 / 487 loss=13.624, nll_loss=10.18, mask_ins=1.875, word_ins_ml=10.844, word_reposition=0.905, ppl=12623, wps=10277.5, ups=0.75, wpb=13627.1, bsz=412.4, num_updates=500, lr=5.009e-05, gnorm=1.572, clip=0, loss_scale=128, train_wall=117, wall=614
2022-10-13 04:12:46 | INFO | train_inner | epoch 002:    113 / 487 loss=13.503, nll_loss=10.072, mask_ins=1.864, word_ins_ml=10.752, word_reposition=0.887, ppl=11610.8, wps=11584.3, ups=0.84, wpb=13717.3, bsz=415.5, num_updates=600, lr=6.0088e-05, gnorm=1.589, clip=0, loss_scale=242, train_wall=117, wall=732
2022-10-13 04:14:44 | INFO | train_inner | epoch 002:    213 / 487 loss=13.346, nll_loss=9.943, mask_ins=1.842, word_ins_ml=10.639, word_reposition=0.865, ppl=10410.3, wps=11544.3, ups=0.85, wpb=13594.2, bsz=415, num_updates=700, lr=7.0086e-05, gnorm=1.597, clip=0, loss_scale=256, train_wall=117, wall=850
2022-10-13 04:16:41 | INFO | train_inner | epoch 002:    313 / 487 loss=13.267, nll_loss=9.847, mask_ins=1.853, word_ins_ml=10.557, word_reposition=0.858, ppl=9860.08, wps=11656.7, ups=0.85, wpb=13684.2, bsz=413.7, num_updates=800, lr=8.0084e-05, gnorm=1.533, clip=0, loss_scale=256, train_wall=116, wall=967
2022-10-13 04:18:40 | INFO | train_inner | epoch 002:    413 / 487 loss=13.154, nll_loss=9.744, mask_ins=1.827, word_ins_ml=10.468, word_reposition=0.859, ppl=9112.14, wps=11776.4, ups=0.84, wpb=14005.3, bsz=426.8, num_updates=900, lr=9.0082e-05, gnorm=1.487, clip=0, loss_scale=256, train_wall=118, wall=1086
2022-10-13 04:20:08 | INFO | train | epoch 002 | loss 13.286 | nll_loss 9.87 | mask_ins 1.842 | word_ins_ml 10.577 | word_reposition 0.867 | ppl 9985.68 | wps 11323.8 | ups 0.83 | wpb 13710.6 | bsz 416.3 | num_updates 974 | lr 9.74805e-05 | gnorm 1.527 | clip 0 | loss_scale 250 | train_wall 570 | wall 1174
2022-10-13 04:20:19 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 14.507 | nll_loss 11.133 | mask_ins 1.965 | word_ins_ml 11.718 | word_reposition 0.824 | ppl 23288 | wps 27641.8 | wpb 1524.1 | bsz 52.2 | num_updates 974 | best_loss 14.338
2022-10-13 04:20:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 2 @ 974 updates, score 14.507) (writing took 2.6086703120090533 seconds)
2022-10-13 04:20:52 | INFO | train_inner | epoch 003:     26 / 487 loss=13.062, nll_loss=9.642, mask_ins=1.817, word_ins_ml=10.38, word_reposition=0.864, ppl=8549.57, wps=10243.6, ups=0.76, wpb=13538.9, bsz=407.7, num_updates=1000, lr=0.00010008, gnorm=1.479, clip=0, loss_scale=256, train_wall=117, wall=1218
2022-10-13 04:22:51 | INFO | train_inner | epoch 003:    126 / 487 loss=12.969, nll_loss=9.541, mask_ins=1.819, word_ins_ml=10.294, word_reposition=0.857, ppl=8019.79, wps=11599.9, ups=0.84, wpb=13801.6, bsz=422.3, num_updates=1100, lr=0.000110078, gnorm=1.492, clip=0, loss_scale=453, train_wall=118, wall=1337
2022-10-13 04:24:51 | INFO | train_inner | epoch 003:    226 / 487 loss=12.892, nll_loss=9.465, mask_ins=1.808, word_ins_ml=10.229, word_reposition=0.855, ppl=7603.38, wps=11623, ups=0.84, wpb=13911.3, bsz=422.7, num_updates=1200, lr=0.000120076, gnorm=1.441, clip=0, loss_scale=512, train_wall=119, wall=1457
2022-10-13 04:26:51 | INFO | train_inner | epoch 003:    326 / 487 loss=12.847, nll_loss=9.395, mask_ins=1.807, word_ins_ml=10.169, word_reposition=0.871, ppl=7369.73, wps=11382.5, ups=0.84, wpb=13603.5, bsz=410.5, num_updates=1300, lr=0.000130074, gnorm=1.387, clip=0, loss_scale=512, train_wall=118, wall=1577
2022-10-13 04:28:49 | INFO | train_inner | epoch 003:    426 / 487 loss=12.777, nll_loss=9.292, mask_ins=1.809, word_ins_ml=10.08, word_reposition=0.889, ppl=7020.03, wps=11348.2, ups=0.84, wpb=13431.8, bsz=407.3, num_updates=1400, lr=0.000140072, gnorm=1.49, clip=0, loss_scale=512, train_wall=117, wall=1695
2022-10-13 04:30:01 | INFO | train | epoch 003 | loss 12.859 | nll_loss 9.404 | mask_ins 1.811 | word_ins_ml 10.176 | word_reposition 0.873 | ppl 7430.9 | wps 11240.2 | ups 0.82 | wpb 13708.1 | bsz 416.2 | num_updates 1461 | lr 0.000146171 | gnorm 1.479 | clip 0 | loss_scale 486 | train_wall 574 | wall 1767
2022-10-13 04:30:13 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 14.453 | nll_loss 11.126 | mask_ins 1.913 | word_ins_ml 11.74 | word_reposition 0.8 | ppl 22433.2 | wps 27782.4 | wpb 1524.1 | bsz 52.2 | num_updates 1461 | best_loss 14.338
2022-10-13 04:30:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 3 @ 1461 updates, score 14.453) (writing took 2.6809691689850297 seconds)
2022-10-13 04:31:02 | INFO | train_inner | epoch 004:     39 / 487 loss=12.655, nll_loss=9.133, mask_ins=1.804, word_ins_ml=9.939, word_reposition=0.912, ppl=6448.61, wps=10363.9, ups=0.75, wpb=13791.1, bsz=421.8, num_updates=1500, lr=0.00015007, gnorm=1.589, clip=0, loss_scale=512, train_wall=118, wall=1828
2022-10-13 04:33:01 | INFO | train_inner | epoch 004:    139 / 487 loss=12.509, nll_loss=8.921, mask_ins=1.817, word_ins_ml=9.752, word_reposition=0.941, ppl=5830.05, wps=11465.4, ups=0.84, wpb=13679.4, bsz=416.9, num_updates=1600, lr=0.000160068, gnorm=1.895, clip=0, loss_scale=845, train_wall=118, wall=1947
2022-10-13 04:35:00 | INFO | train_inner | epoch 004:    239 / 487 loss=12.372, nll_loss=8.774, mask_ins=1.805, word_ins_ml=9.622, word_reposition=0.945, ppl=5302.16, wps=11599.2, ups=0.84, wpb=13769.5, bsz=417.6, num_updates=1700, lr=0.000170066, gnorm=1.911, clip=0, loss_scale=1024, train_wall=118, wall=2066
2022-10-13 04:36:58 | INFO | train_inner | epoch 004:    339 / 487 loss=12.3, nll_loss=8.675, mask_ins=1.811, word_ins_ml=9.538, word_reposition=0.952, ppl=5044.31, wps=11427, ups=0.85, wpb=13504.9, bsz=406, num_updates=1800, lr=0.000180064, gnorm=2.111, clip=0, loss_scale=1024, train_wall=117, wall=2184
2022-10-13 04:38:56 | INFO | train_inner | epoch 004:    439 / 487 loss=12.202, nll_loss=8.581, mask_ins=1.79, word_ins_ml=9.457, word_reposition=0.955, ppl=4711.1, wps=11800.7, ups=0.85, wpb=13955.1, bsz=425.3, num_updates=1900, lr=0.000190062, gnorm=2.087, clip=0, loss_scale=1024, train_wall=117, wall=2302
2022-10-13 04:39:53 | INFO | train | epoch 004 | loss 12.346 | nll_loss 8.741 | mask_ins 1.803 | word_ins_ml 9.595 | word_reposition 0.947 | ppl 5204.82 | wps 11288.4 | ups 0.82 | wpb 13710.1 | bsz 416.3 | num_updates 1948 | lr 0.000194861 | gnorm 1.995 | clip 0 | loss_scale 946 | train_wall 572 | wall 2359
2022-10-13 04:40:05 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 14.359 | nll_loss 10.955 | mask_ins 1.912 | word_ins_ml 11.611 | word_reposition 0.836 | ppl 21009 | wps 27705 | wpb 1524.1 | bsz 52.2 | num_updates 1948 | best_loss 14.338
2022-10-13 04:40:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 4 @ 1948 updates, score 14.359) (writing took 2.8609800029953476 seconds)
2022-10-13 04:41:09 | INFO | train_inner | epoch 005:     52 / 487 loss=12.111, nll_loss=8.479, mask_ins=1.79, word_ins_ml=9.368, word_reposition=0.953, ppl=4423.01, wps=10481.2, ups=0.76, wpb=13872.6, bsz=419.3, num_updates=2000, lr=0.00020006, gnorm=2.228, clip=0, loss_scale=1024, train_wall=117, wall=2435
2022-10-13 04:43:07 | INFO | train_inner | epoch 005:    152 / 487 loss=12.042, nll_loss=8.382, mask_ins=1.791, word_ins_ml=9.285, word_reposition=0.966, ppl=4216.78, wps=11834.4, ups=0.85, wpb=13933.8, bsz=424.1, num_updates=2100, lr=0.000210058, gnorm=2.13, clip=0, loss_scale=1567, train_wall=117, wall=2553
2022-10-13 04:45:04 | INFO | train_inner | epoch 005:    252 / 487 loss=12.006, nll_loss=8.34, mask_ins=1.794, word_ins_ml=9.25, word_reposition=0.962, ppl=4112.06, wps=11334.7, ups=0.85, wpb=13261.7, bsz=400.6, num_updates=2200, lr=0.000220056, gnorm=2.273, clip=0, loss_scale=2048, train_wall=116, wall=2670
2022-10-13 04:47:00 | INFO | train_inner | epoch 005:    352 / 487 loss=11.953, nll_loss=8.28, mask_ins=1.787, word_ins_ml=9.199, word_reposition=0.966, ppl=3964.27, wps=11803.9, ups=0.86, wpb=13795, bsz=418.5, num_updates=2300, lr=0.000230054, gnorm=2.176, clip=0, loss_scale=2048, train_wall=116, wall=2786
2022-10-13 04:48:57 | INFO | train_inner | epoch 005:    452 / 487 loss=11.884, nll_loss=8.206, mask_ins=1.777, word_ins_ml=9.135, word_reposition=0.973, ppl=3780.04, wps=11737.3, ups=0.86, wpb=13662.2, bsz=415.9, num_updates=2400, lr=0.000240052, gnorm=2.08, clip=0, loss_scale=2048, train_wall=115, wall=2903
2022-10-13 04:49:37 | INFO | train | epoch 005 | loss 11.974 | nll_loss 8.307 | mask_ins 1.788 | word_ins_ml 9.221 | word_reposition 0.965 | ppl 4023.92 | wps 11433.8 | ups 0.83 | wpb 13711.1 | bsz 416.3 | num_updates 2435 | lr 0.000243551 | gnorm 2.161 | clip 0 | loss_scale 1840 | train_wall 565 | wall 2943
2022-10-13 04:49:48 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 14.475 | nll_loss 10.986 | mask_ins 1.966 | word_ins_ml 11.664 | word_reposition 0.845 | ppl 22772 | wps 27771.2 | wpb 1524.1 | bsz 52.2 | num_updates 2435 | best_loss 14.338
2022-10-13 04:49:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 5 @ 2435 updates, score 14.475) (writing took 2.822558638988994 seconds)
2022-10-13 04:51:08 | INFO | train_inner | epoch 006:     65 / 487 loss=11.795, nll_loss=8.112, mask_ins=1.767, word_ins_ml=9.053, word_reposition=0.976, ppl=3554.21, wps=10167.7, ups=0.76, wpb=13318, bsz=404.6, num_updates=2500, lr=0.00025005, gnorm=2.084, clip=0, loss_scale=2048, train_wall=116, wall=3034
2022-10-13 04:53:06 | INFO | train_inner | epoch 006:    165 / 487 loss=11.723, nll_loss=8.016, mask_ins=1.778, word_ins_ml=8.971, word_reposition=0.975, ppl=3380.84, wps=11587.2, ups=0.84, wpb=13715.3, bsz=418.6, num_updates=2600, lr=0.000260048, gnorm=2.201, clip=0, loss_scale=2888, train_wall=117, wall=3152
2022-10-13 04:55:04 | INFO | train_inner | epoch 006:    265 / 487 loss=11.638, nll_loss=7.932, mask_ins=1.765, word_ins_ml=8.898, word_reposition=0.975, ppl=3186.22, wps=11901.9, ups=0.85, wpb=13963.5, bsz=427.1, num_updates=2700, lr=0.000270046, gnorm=2.136, clip=0, loss_scale=4096, train_wall=116, wall=3270
2022-10-13 04:57:01 | INFO | train_inner | epoch 006:    365 / 487 loss=11.604, nll_loss=7.856, mask_ins=1.767, word_ins_ml=8.832, word_reposition=1.006, ppl=3113.45, wps=11589.8, ups=0.85, wpb=13630.9, bsz=411.3, num_updates=2800, lr=0.000280044, gnorm=1.996, clip=0, loss_scale=4096, train_wall=117, wall=3387
2022-10-13 04:58:59 | INFO | train_inner | epoch 006:    465 / 487 loss=11.506, nll_loss=7.751, mask_ins=1.762, word_ins_ml=8.741, word_reposition=1.003, ppl=2908.44, wps=11805.3, ups=0.85, wpb=13936.6, bsz=421.5, num_updates=2900, lr=0.000290042, gnorm=1.974, clip=0, loss_scale=4096, train_wall=117, wall=3505
2022-10-13 04:59:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-10-13 04:59:25 | INFO | train | epoch 006 | loss 11.632 | nll_loss 7.907 | mask_ins 1.766 | word_ins_ml 8.876 | word_reposition 0.99 | ppl 3173.3 | wps 11338.8 | ups 0.83 | wpb 13714.4 | bsz 416.4 | num_updates 2921 | lr 0.000292142 | gnorm 2.082 | clip 0 | loss_scale 3499 | train_wall 569 | wall 3531
2022-10-13 04:59:36 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 14.196 | nll_loss 10.728 | mask_ins 1.854 | word_ins_ml 11.458 | word_reposition 0.884 | ppl 18765.2 | wps 27755.5 | wpb 1524.1 | bsz 52.2 | num_updates 2921 | best_loss 14.196
2022-10-13 04:59:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 6 @ 2921 updates, score 14.196) (writing took 4.040783423988614 seconds)
2022-10-13 05:01:14 | INFO | train_inner | epoch 007:     79 / 487 loss=11.373, nll_loss=7.581, mask_ins=1.762, word_ins_ml=8.592, word_reposition=1.019, ppl=2651.5, wps=10074.9, ups=0.74, wpb=13538.7, bsz=409.4, num_updates=3000, lr=0.00030004, gnorm=2.077, clip=0, loss_scale=2129, train_wall=118, wall=3640
2022-10-13 05:03:12 | INFO | train_inner | epoch 007:    179 / 487 loss=11.248, nll_loss=7.453, mask_ins=1.746, word_ins_ml=8.481, word_reposition=1.021, ppl=2431.8, wps=12062.7, ups=0.85, wpb=14262.9, bsz=431.9, num_updates=3100, lr=0.000310038, gnorm=1.947, clip=0, loss_scale=2048, train_wall=117, wall=3758
2022-10-13 05:05:09 | INFO | train_inner | epoch 007:    279 / 487 loss=11.105, nll_loss=7.313, mask_ins=1.727, word_ins_ml=8.359, word_reposition=1.019, ppl=2202.3, wps=11617.1, ups=0.85, wpb=13611.2, bsz=415.6, num_updates=3200, lr=0.000320036, gnorm=2.066, clip=0, loss_scale=2048, train_wall=116, wall=3875
2022-10-13 05:07:07 | INFO | train_inner | epoch 007:    379 / 487 loss=11.048, nll_loss=7.217, mask_ins=1.728, word_ins_ml=8.276, word_reposition=1.045, ppl=2117.68, wps=11505, ups=0.85, wpb=13535.8, bsz=410.5, num_updates=3300, lr=0.000330034, gnorm=2, clip=0, loss_scale=2048, train_wall=117, wall=3993
2022-10-13 05:09:03 | INFO | train_inner | epoch 007:    479 / 487 loss=10.896, nll_loss=7.083, mask_ins=1.702, word_ins_ml=8.16, word_reposition=1.034, ppl=1905.22, wps=11667.3, ups=0.86, wpb=13626.3, bsz=414.7, num_updates=3400, lr=0.000340032, gnorm=1.989, clip=0, loss_scale=2048, train_wall=116, wall=4109
2022-10-13 05:09:12 | INFO | train | epoch 007 | loss 11.114 | nll_loss 7.309 | mask_ins 1.731 | word_ins_ml 8.356 | word_reposition 1.027 | ppl 2216.9 | wps 11364.3 | ups 0.83 | wpb 13710.7 | bsz 416.3 | num_updates 3408 | lr 0.000340832 | gnorm 2.024 | clip 0 | loss_scale 2048 | train_wall 567 | wall 4118
2022-10-13 05:09:24 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 13.884 | nll_loss 10.332 | mask_ins 1.809 | word_ins_ml 11.123 | word_reposition 0.951 | ppl 15113.8 | wps 27827.7 | wpb 1524.1 | bsz 52.2 | num_updates 3408 | best_loss 13.884
2022-10-13 05:09:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 7 @ 3408 updates, score 13.884) (writing took 4.069899659982184 seconds)
2022-10-13 05:11:15 | INFO | train_inner | epoch 008:     92 / 487 loss=10.692, nll_loss=6.859, mask_ins=1.688, word_ins_ml=7.963, word_reposition=1.041, ppl=1654.31, wps=10149.5, ups=0.76, wpb=13372.8, bsz=406.6, num_updates=3500, lr=0.00035003, gnorm=2.023, clip=0, loss_scale=3789, train_wall=115, wall=4241
2022-10-13 05:13:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-10-13 05:13:14 | INFO | train_inner | epoch 008:    193 / 487 loss=10.582, nll_loss=6.746, mask_ins=1.677, word_ins_ml=7.866, word_reposition=1.039, ppl=1532.91, wps=11583.9, ups=0.84, wpb=13740.1, bsz=419.5, num_updates=3600, lr=0.000360028, gnorm=2.503, clip=0, loss_scale=3853, train_wall=118, wall=4360
2022-10-13 05:15:11 | INFO | train_inner | epoch 008:    293 / 487 loss=10.491, nll_loss=6.654, mask_ins=1.664, word_ins_ml=7.787, word_reposition=1.041, ppl=1439.05, wps=11706.6, ups=0.85, wpb=13696.9, bsz=412.7, num_updates=3700, lr=0.000370026, gnorm=2.367, clip=0, loss_scale=2048, train_wall=116, wall=4477
2022-10-13 05:17:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-10-13 05:17:09 | INFO | train_inner | epoch 008:    394 / 487 loss=10.374, nll_loss=6.553, mask_ins=1.645, word_ins_ml=7.7, word_reposition=1.028, ppl=1326.63, wps=11822.1, ups=0.85, wpb=13922.4, bsz=421.9, num_updates=3800, lr=0.000380024, gnorm=3.086, clip=0, loss_scale=2028, train_wall=117, wall=4595
2022-10-13 05:18:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-10-13 05:18:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-13 05:18:56 | INFO | train | epoch 008 | loss 10.489 | nll_loss 6.659 | mask_ins 1.661 | word_ins_ml 7.791 | word_reposition 1.038 | ppl 1437.44 | wps 11321.7 | ups 0.83 | wpb 13688.7 | bsz 415.5 | num_updates 3891 | lr 0.000389122 | gnorm 2.594 | clip 0 | loss_scale 2524 | train_wall 564 | wall 4702
2022-10-13 05:19:08 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 13.718 | nll_loss 10.154 | mask_ins 1.747 | word_ins_ml 10.985 | word_reposition 0.986 | ppl 13474.6 | wps 27772 | wpb 1524.1 | bsz 52.2 | num_updates 3891 | best_loss 13.718
2022-10-13 05:19:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 8 @ 3891 updates, score 13.718) (writing took 4.068257581995567 seconds)
2022-10-13 05:19:22 | INFO | train_inner | epoch 009:      9 / 487 loss=10.294, nll_loss=6.469, mask_ins=1.626, word_ins_ml=7.627, word_reposition=1.04, ppl=1255.36, wps=10118.8, ups=0.75, wpb=13531.3, bsz=410.4, num_updates=3900, lr=0.000390022, gnorm=2.97, clip=0, loss_scale=690, train_wall=117, wall=4728
2022-10-13 05:21:19 | INFO | train_inner | epoch 009:    109 / 487 loss=10.125, nll_loss=6.303, mask_ins=1.614, word_ins_ml=7.484, word_reposition=1.026, ppl=1116.35, wps=11683.1, ups=0.86, wpb=13598.6, bsz=414.3, num_updates=4000, lr=0.00040002, gnorm=3.228, clip=0, loss_scale=256, train_wall=115, wall=4845
2022-10-13 05:23:16 | INFO | train_inner | epoch 009:    209 / 487 loss=10.11, nll_loss=6.278, mask_ins=1.613, word_ins_ml=7.465, word_reposition=1.032, ppl=1105.5, wps=11732.5, ups=0.85, wpb=13797.1, bsz=420.2, num_updates=4100, lr=0.000410018, gnorm=3.001, clip=1, loss_scale=256, train_wall=117, wall=4962
2022-10-13 05:25:14 | INFO | train_inner | epoch 009:    309 / 487 loss=10.039, nll_loss=6.226, mask_ins=1.6, word_ins_ml=7.422, word_reposition=1.017, ppl=1051.94, wps=11755.1, ups=0.85, wpb=13816.5, bsz=419.3, num_updates=4200, lr=0.000420016, gnorm=3.602, clip=0, loss_scale=256, train_wall=117, wall=5080
2022-10-13 05:27:11 | INFO | train_inner | epoch 009:    409 / 487 loss=10, nll_loss=6.186, mask_ins=1.594, word_ins_ml=7.388, word_reposition=1.017, ppl=1024.08, wps=11760.3, ups=0.85, wpb=13782.8, bsz=416.7, num_updates=4300, lr=0.000430014, gnorm=2.639, clip=0, loss_scale=256, train_wall=116, wall=5197
2022-10-13 05:28:41 | INFO | train | epoch 009 | loss 10.053 | nll_loss 6.236 | mask_ins 1.602 | word_ins_ml 7.429 | word_reposition 1.022 | ppl 1062.65 | wps 11409.7 | ups 0.83 | wpb 13709.6 | bsz 416.3 | num_updates 4378 | lr 0.000437812 | gnorm 2.969 | clip 0.2 | loss_scale 256 | train_wall 565 | wall 5287
2022-10-13 05:28:53 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 13.633 | nll_loss 10.089 | mask_ins 1.721 | word_ins_ml 10.951 | word_reposition 0.961 | ppl 12701.6 | wps 27803 | wpb 1524.1 | bsz 52.2 | num_updates 4378 | best_loss 13.633
2022-10-13 05:28:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 9 @ 4378 updates, score 13.633) (writing took 4.397813982999651 seconds)
2022-10-13 05:29:23 | INFO | train_inner | epoch 010:     22 / 487 loss=9.947, nll_loss=6.145, mask_ins=1.588, word_ins_ml=7.353, word_reposition=1.007, ppl=987.06, wps=10275.7, ups=0.76, wpb=13552.4, bsz=409.3, num_updates=4400, lr=0.000440012, gnorm=2.192, clip=0, loss_scale=284, train_wall=115, wall=5329
2022-10-13 05:31:20 | INFO | train_inner | epoch 010:    122 / 487 loss=9.805, nll_loss=6, mask_ins=1.576, word_ins_ml=7.227, word_reposition=1.002, ppl=894.74, wps=11856.9, ups=0.85, wpb=13901.5, bsz=423, num_updates=4500, lr=0.00045001, gnorm=2.16, clip=0, loss_scale=512, train_wall=116, wall=5446
2022-10-13 05:33:17 | INFO | train_inner | epoch 010:    222 / 487 loss=9.76, nll_loss=5.98, mask_ins=1.557, word_ins_ml=7.211, word_reposition=0.993, ppl=867.3, wps=11920.6, ups=0.86, wpb=13876, bsz=421.9, num_updates=4600, lr=0.000460008, gnorm=2.231, clip=0, loss_scale=512, train_wall=115, wall=5563
2022-10-13 05:35:13 | INFO | train_inner | epoch 010:    322 / 487 loss=9.684, nll_loss=5.931, mask_ins=1.539, word_ins_ml=7.17, word_reposition=0.975, ppl=822.75, wps=11794.1, ups=0.86, wpb=13756, bsz=418.9, num_updates=4700, lr=0.000470006, gnorm=2.017, clip=0, loss_scale=512, train_wall=116, wall=5679
2022-10-13 05:37:09 | INFO | train_inner | epoch 010:    422 / 487 loss=9.728, nll_loss=5.953, mask_ins=1.559, word_ins_ml=7.189, word_reposition=0.98, ppl=848.12, wps=11430.3, ups=0.86, wpb=13263, bsz=400.8, num_updates=4800, lr=0.000480004, gnorm=2.306, clip=0, loss_scale=512, train_wall=115, wall=5795
2022-10-13 05:38:25 | INFO | train | epoch 010 | loss 9.744 | nll_loss 5.967 | mask_ins 1.557 | word_ins_ml 7.2 | word_reposition 0.986 | ppl 857.35 | wps 11443.8 | ups 0.83 | wpb 13711.1 | bsz 416.3 | num_updates 4865 | lr 0.000486503 | gnorm 2.205 | clip 0 | loss_scale 506 | train_wall 563 | wall 5871
2022-10-13 05:38:36 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 13.631 | nll_loss 9.942 | mask_ins 1.692 | word_ins_ml 10.823 | word_reposition 1.116 | ppl 12688.1 | wps 27802.1 | wpb 1524.1 | bsz 52.2 | num_updates 4865 | best_loss 13.631
2022-10-13 05:38:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 10 @ 4865 updates, score 13.631) (writing took 4.433921196003212 seconds)
2022-10-13 05:39:22 | INFO | train_inner | epoch 011:     35 / 487 loss=9.67, nll_loss=5.912, mask_ins=1.544, word_ins_ml=7.154, word_reposition=0.973, ppl=814.69, wps=10358.5, ups=0.75, wpb=13720.9, bsz=418.8, num_updates=4900, lr=0.000490002, gnorm=2.518, clip=0, loss_scale=512, train_wall=116, wall=5928
2022-10-13 05:39:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-10-13 05:40:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-10-13 05:41:21 | INFO | train_inner | epoch 011:    137 / 487 loss=9.663, nll_loss=5.922, mask_ins=1.55, word_ins_ml=7.165, word_reposition=0.948, ppl=810.44, wps=11436.2, ups=0.84, wpb=13622.2, bsz=411.9, num_updates=5000, lr=0.0005, gnorm=6.667, clip=1, loss_scale=414, train_wall=118, wall=6047
2022-10-13 05:43:17 | INFO | train_inner | epoch 011:    237 / 487 loss=9.561, nll_loss=5.842, mask_ins=1.521, word_ins_ml=7.096, word_reposition=0.944, ppl=755.39, wps=11574.5, ups=0.86, wpb=13492.6, bsz=409.5, num_updates=5100, lr=0.000495074, gnorm=3.527, clip=0, loss_scale=256, train_wall=116, wall=6163
2022-10-13 05:45:13 | INFO | train_inner | epoch 011:    337 / 487 loss=9.616, nll_loss=5.893, mask_ins=1.544, word_ins_ml=7.141, word_reposition=0.93, ppl=784.66, wps=11626, ups=0.86, wpb=13484.9, bsz=409.7, num_updates=5200, lr=0.00049029, gnorm=6.253, clip=3, loss_scale=256, train_wall=115, wall=6279
2022-10-13 05:47:10 | INFO | train_inner | epoch 011:    437 / 487 loss=9.59, nll_loss=5.896, mask_ins=1.533, word_ins_ml=7.146, word_reposition=0.912, ppl=770.72, wps=12109.7, ups=0.85, wpb=14180.5, bsz=429.8, num_updates=5300, lr=0.000485643, gnorm=4.791, clip=1, loss_scale=256, train_wall=116, wall=6396
2022-10-13 05:48:08 | INFO | train | epoch 011 | loss 9.603 | nll_loss 5.88 | mask_ins 1.537 | word_ins_ml 7.129 | word_reposition 0.937 | ppl 777.71 | wps 11390.3 | ups 0.83 | wpb 13704.6 | bsz 416.1 | num_updates 5350 | lr 0.000483368 | gnorm 5.173 | clip 1.2 | loss_scale 308 | train_wall 563 | wall 6454
2022-10-13 05:48:20 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 13.255 | nll_loss 9.818 | mask_ins 1.668 | word_ins_ml 10.731 | word_reposition 0.856 | ppl 9775.38 | wps 27758.5 | wpb 1524.1 | bsz 52.2 | num_updates 5350 | best_loss 13.255
2022-10-13 05:48:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 11 @ 5350 updates, score 13.255) (writing took 4.285854611982359 seconds)
2022-10-13 05:49:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-10-13 05:49:25 | INFO | train_inner | epoch 012:     51 / 487 loss=9.567, nll_loss=5.844, mask_ins=1.533, word_ins_ml=7.099, word_reposition=0.934, ppl=758.33, wps=10286.6, ups=0.75, wpb=13795.6, bsz=418, num_updates=5400, lr=0.000481125, gnorm=9.29, clip=7, loss_scale=245, train_wall=117, wall=6531
2022-10-13 05:51:21 | INFO | train_inner | epoch 012:    151 / 487 loss=9.418, nll_loss=5.755, mask_ins=1.501, word_ins_ml=7.024, word_reposition=0.893, ppl=684.02, wps=11813.8, ups=0.86, wpb=13803.6, bsz=419.2, num_updates=5500, lr=0.000476731, gnorm=5.803, clip=3, loss_scale=128, train_wall=116, wall=6647
2022-10-13 05:53:19 | INFO | train_inner | epoch 012:    251 / 487 loss=9.512, nll_loss=5.826, mask_ins=1.516, word_ins_ml=7.087, word_reposition=0.909, ppl=730.37, wps=11556.8, ups=0.85, wpb=13564.8, bsz=411.2, num_updates=5600, lr=0.000472456, gnorm=9.248, clip=4, loss_scale=128, train_wall=116, wall=6765
2022-10-13 05:55:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-10-13 05:55:17 | INFO | train_inner | epoch 012:    352 / 487 loss=9.827, nll_loss=6.206, mask_ins=1.515, word_ins_ml=7.419, word_reposition=0.893, ppl=908.04, wps=11958.9, ups=0.84, wpb=14171.1, bsz=432.9, num_updates=5700, lr=0.000468293, gnorm=15.784, clip=16, loss_scale=121, train_wall=118, wall=6883
2022-10-13 05:57:16 | INFO | train_inner | epoch 012:    452 / 487 loss=9.591, nll_loss=5.939, mask_ins=1.507, word_ins_ml=7.186, word_reposition=0.898, ppl=770.98, wps=11463.8, ups=0.85, wpb=13561.7, bsz=411.9, num_updates=5800, lr=0.000464238, gnorm=12.849, clip=9, loss_scale=64, train_wall=117, wall=7002
2022-10-13 05:57:56 | INFO | train | epoch 012 | loss 9.591 | nll_loss 5.933 | mask_ins 1.511 | word_ins_ml 7.18 | word_reposition 0.9 | ppl 771.09 | wps 11325.7 | ups 0.83 | wpb 13714.7 | bsz 416.4 | num_updates 5835 | lr 0.000462844 | gnorm 12.004 | clip 9.7 | loss_scale 120 | train_wall 567 | wall 7042
2022-10-13 05:58:07 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 12.88 | nll_loss 9.416 | mask_ins 1.671 | word_ins_ml 10.368 | word_reposition 0.842 | ppl 7539.62 | wps 27814 | wpb 1524.1 | bsz 52.2 | num_updates 5835 | best_loss 12.88
2022-10-13 05:58:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 12 @ 5835 updates, score 12.88) (writing took 4.339057687990135 seconds)
2022-10-13 05:59:28 | INFO | train_inner | epoch 013:     65 / 487 loss=9.734, nll_loss=6.109, mask_ins=1.512, word_ins_ml=7.334, word_reposition=0.888, ppl=851.46, wps=10335.6, ups=0.76, wpb=13666.3, bsz=414.4, num_updates=5900, lr=0.000460287, gnorm=23.199, clip=30, loss_scale=64, train_wall=115, wall=7134
2022-10-13 06:01:24 | INFO | train_inner | epoch 013:    165 / 487 loss=9.599, nll_loss=5.979, mask_ins=1.502, word_ins_ml=7.222, word_reposition=0.875, ppl=775.75, wps=11522.6, ups=0.86, wpb=13397.8, bsz=407.7, num_updates=6000, lr=0.000456435, gnorm=12.175, clip=7, loss_scale=64, train_wall=115, wall=7250
2022-10-13 06:03:21 | INFO | train_inner | epoch 013:    265 / 487 loss=9.466, nll_loss=5.846, mask_ins=1.498, word_ins_ml=7.105, word_reposition=0.863, ppl=707.16, wps=11821.3, ups=0.85, wpb=13876.8, bsz=422, num_updates=6100, lr=0.000452679, gnorm=7.919, clip=6, loss_scale=64, train_wall=116, wall=7367
2022-10-13 06:03:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-10-13 06:05:19 | INFO | train_inner | epoch 013:    366 / 487 loss=9.484, nll_loss=5.855, mask_ins=1.497, word_ins_ml=7.114, word_reposition=0.874, ppl=716.1, wps=11717, ups=0.85, wpb=13791.9, bsz=418, num_updates=6200, lr=0.000449013, gnorm=9.494, clip=8, loss_scale=35, train_wall=117, wall=7485
2022-10-13 06:07:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2022-10-13 06:07:18 | INFO | train_inner | epoch 013:    467 / 487 loss=9.753, nll_loss=6.176, mask_ins=1.495, word_ins_ml=7.39, word_reposition=0.869, ppl=862.92, wps=11542.4, ups=0.84, wpb=13663.1, bsz=413.7, num_updates=6300, lr=0.000445435, gnorm=21.662, clip=20, loss_scale=30, train_wall=117, wall=7604
2022-10-13 06:07:41 | INFO | train | epoch 013 | loss 9.633 | nll_loss 6.025 | mask_ins 1.502 | word_ins_ml 7.26 | word_reposition 0.871 | ppl 794.11 | wps 11365.4 | ups 0.83 | wpb 13705.1 | bsz 416.1 | num_updates 6320 | lr 0.00044473 | gnorm 14.019 | clip 13.2 | loss_scale 49 | train_wall 564 | wall 7627
2022-10-13 06:07:52 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 12.056 | nll_loss 8.598 | mask_ins 1.633 | word_ins_ml 9.624 | word_reposition 0.798 | ppl 4257.29 | wps 27801.1 | wpb 1524.1 | bsz 52.2 | num_updates 6320 | best_loss 12.056
2022-10-13 06:07:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 13 @ 6320 updates, score 12.056) (writing took 4.186666321998928 seconds)
2022-10-13 06:09:30 | INFO | train_inner | epoch 014:     80 / 487 loss=10.249, nll_loss=6.768, mask_ins=1.505, word_ins_ml=7.898, word_reposition=0.847, ppl=1217.15, wps=10437, ups=0.76, wpb=13774.7, bsz=420.4, num_updates=6400, lr=0.000441942, gnorm=13.679, clip=11, loss_scale=16, train_wall=115, wall=7736
2022-10-13 06:11:27 | INFO | train_inner | epoch 014:    180 / 487 loss=10.326, nll_loss=6.859, mask_ins=1.508, word_ins_ml=7.974, word_reposition=0.843, ppl=1283.7, wps=11742.4, ups=0.85, wpb=13751.1, bsz=416.3, num_updates=6500, lr=0.000438529, gnorm=1.675, clip=0, loss_scale=16, train_wall=116, wall=7853
2022-10-13 06:13:23 | INFO | train_inner | epoch 014:    280 / 487 loss=10.215, nll_loss=6.754, mask_ins=1.496, word_ins_ml=7.882, word_reposition=0.837, ppl=1188.73, wps=11866.6, ups=0.86, wpb=13812.4, bsz=417.8, num_updates=6600, lr=0.000435194, gnorm=1.67, clip=0, loss_scale=16, train_wall=115, wall=7969
2022-10-13 06:15:20 | INFO | train_inner | epoch 014:    380 / 487 loss=10.158, nll_loss=6.716, mask_ins=1.488, word_ins_ml=7.847, word_reposition=0.823, ppl=1142.6, wps=11712.2, ups=0.85, wpb=13752.4, bsz=419.6, num_updates=6700, lr=0.000431934, gnorm=1.59, clip=0, loss_scale=16, train_wall=117, wall=8086
2022-10-13 06:17:17 | INFO | train_inner | epoch 014:    480 / 487 loss=10.137, nll_loss=6.681, mask_ins=1.485, word_ins_ml=7.814, word_reposition=0.838, ppl=1126.13, wps=11559.8, ups=0.86, wpb=13450.7, bsz=407.6, num_updates=6800, lr=0.000428746, gnorm=1.648, clip=0, loss_scale=16, train_wall=115, wall=8203
2022-10-13 06:17:25 | INFO | train | epoch 014 | loss 10.211 | nll_loss 6.749 | mask_ins 1.496 | word_ins_ml 7.877 | word_reposition 0.838 | ppl 1185.19 | wps 11434.9 | ups 0.83 | wpb 13709.9 | bsz 416.3 | num_updates 6807 | lr 0.000428526 | gnorm 3.828 | clip 1.8 | loss_scale 16 | train_wall 564 | wall 8210
2022-10-13 06:17:36 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.087 | nll_loss 7.544 | mask_ins 1.617 | word_ins_ml 8.67 | word_reposition 0.801 | ppl 2175.44 | wps 27782.5 | wpb 1524.1 | bsz 52.2 | num_updates 6807 | best_loss 11.087
2022-10-13 06:17:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 14 @ 6807 updates, score 11.087) (writing took 4.123190244979924 seconds)
2022-10-13 06:19:28 | INFO | train_inner | epoch 015:     93 / 487 loss=10.062, nll_loss=6.591, mask_ins=1.49, word_ins_ml=7.735, word_reposition=0.837, ppl=1069, wps=10382, ups=0.76, wpb=13664.3, bsz=415.1, num_updates=6900, lr=0.000425628, gnorm=1.543, clip=0, loss_scale=32, train_wall=115, wall=8334
2022-10-13 06:21:25 | INFO | train_inner | epoch 015:    193 / 487 loss=9.983, nll_loss=6.563, mask_ins=1.459, word_ins_ml=7.709, word_reposition=0.815, ppl=1012.29, wps=11846.9, ups=0.86, wpb=13779.8, bsz=416.4, num_updates=7000, lr=0.000422577, gnorm=1.59, clip=0, loss_scale=32, train_wall=115, wall=8451
2022-10-13 06:23:21 | INFO | train_inner | epoch 015:    293 / 487 loss=10.014, nll_loss=6.583, mask_ins=1.471, word_ins_ml=7.726, word_reposition=0.818, ppl=1034.28, wps=11545.8, ups=0.86, wpb=13434.4, bsz=407.4, num_updates=7100, lr=0.000419591, gnorm=1.602, clip=0, loss_scale=32, train_wall=115, wall=8567
2022-10-13 06:25:18 | INFO | train_inner | epoch 015:    393 / 487 loss=9.971, nll_loss=6.542, mask_ins=1.464, word_ins_ml=7.689, word_reposition=0.817, ppl=1003.59, wps=11973.4, ups=0.86, wpb=13983.8, bsz=426.1, num_updates=7200, lr=0.000416667, gnorm=1.526, clip=0, loss_scale=32, train_wall=116, wall=8684
2022-10-13 06:27:07 | INFO | train | epoch 015 | loss 9.995 | nll_loss 6.557 | mask_ins 1.471 | word_ins_ml 7.703 | word_reposition 0.82 | ppl 1020.31 | wps 11465.4 | ups 0.84 | wpb 13709.8 | bsz 416.3 | num_updates 7294 | lr 0.000413973 | gnorm 1.563 | clip 0 | loss_scale 32 | train_wall 562 | wall 8793
2022-10-13 06:27:18 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 10.989 | nll_loss 7.445 | mask_ins 1.609 | word_ins_ml 8.585 | word_reposition 0.796 | ppl 2032.97 | wps 27851.3 | wpb 1524.1 | bsz 52.2 | num_updates 7294 | best_loss 10.989
2022-10-13 06:27:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 15 @ 7294 updates, score 10.989) (writing took 4.096473207988311 seconds)
2022-10-13 06:27:29 | INFO | train_inner | epoch 016:      6 / 487 loss=9.956, nll_loss=6.516, mask_ins=1.476, word_ins_ml=7.666, word_reposition=0.815, ppl=993.24, wps=10352.7, ups=0.76, wpb=13605.9, bsz=413.8, num_updates=7300, lr=0.000413803, gnorm=1.561, clip=0, loss_scale=32, train_wall=115, wall=8815
2022-10-13 06:29:26 | INFO | train_inner | epoch 016:    106 / 487 loss=9.881, nll_loss=6.444, mask_ins=1.466, word_ins_ml=7.603, word_reposition=0.813, ppl=942.93, wps=11789.7, ups=0.86, wpb=13715.7, bsz=418.1, num_updates=7400, lr=0.000410997, gnorm=1.494, clip=0, loss_scale=60, train_wall=115, wall=8932
2022-10-13 06:31:22 | INFO | train_inner | epoch 016:    206 / 487 loss=9.869, nll_loss=6.423, mask_ins=1.468, word_ins_ml=7.583, word_reposition=0.818, ppl=935.2, wps=11802.5, ups=0.86, wpb=13774.7, bsz=418.9, num_updates=7500, lr=0.000408248, gnorm=1.515, clip=0, loss_scale=64, train_wall=116, wall=9048
2022-10-13 06:33:20 | INFO | train_inner | epoch 016:    306 / 487 loss=9.867, nll_loss=6.438, mask_ins=1.465, word_ins_ml=7.596, word_reposition=0.806, ppl=933.94, wps=11531.1, ups=0.85, wpb=13580.9, bsz=409.1, num_updates=7600, lr=0.000405554, gnorm=1.536, clip=0, loss_scale=64, train_wall=117, wall=9166
2022-10-13 06:35:18 | INFO | train_inner | epoch 016:    406 / 487 loss=9.821, nll_loss=6.398, mask_ins=1.457, word_ins_ml=7.561, word_reposition=0.803, ppl=904.46, wps=11520.5, ups=0.85, wpb=13542.3, bsz=411.8, num_updates=7700, lr=0.000402911, gnorm=1.504, clip=0, loss_scale=64, train_wall=117, wall=9284
2022-10-13 06:36:52 | INFO | train | epoch 016 | loss 9.856 | nll_loss 6.426 | mask_ins 1.463 | word_ins_ml 7.585 | word_reposition 0.808 | ppl 926.88 | wps 11408.2 | ups 0.83 | wpb 13708.3 | bsz 416.2 | num_updates 7781 | lr 0.000400809 | gnorm 1.503 | clip 0 | loss_scale 63 | train_wall 565 | wall 9378
2022-10-13 06:37:04 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.926 | nll_loss 7.401 | mask_ins 1.602 | word_ins_ml 8.569 | word_reposition 0.755 | ppl 1945.73 | wps 27924 | wpb 1524.1 | bsz 52.2 | num_updates 7781 | best_loss 10.926
2022-10-13 06:37:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 16 @ 7781 updates, score 10.926) (writing took 4.0577865640225355 seconds)
2022-10-13 06:37:30 | INFO | train_inner | epoch 017:     19 / 487 loss=9.834, nll_loss=6.421, mask_ins=1.457, word_ins_ml=7.58, word_reposition=0.797, ppl=912.97, wps=10567.9, ups=0.76, wpb=13947.6, bsz=423.9, num_updates=7800, lr=0.00040032, gnorm=1.473, clip=0, loss_scale=64, train_wall=115, wall=9416
2022-10-13 06:39:26 | INFO | train_inner | epoch 017:    119 / 487 loss=9.795, nll_loss=6.364, mask_ins=1.463, word_ins_ml=7.531, word_reposition=0.801, ppl=888.18, wps=11737.8, ups=0.86, wpb=13667.7, bsz=416.6, num_updates=7900, lr=0.000397779, gnorm=1.397, clip=0, loss_scale=112, train_wall=116, wall=9532
2022-10-13 06:41:23 | INFO | train_inner | epoch 017:    219 / 487 loss=9.802, nll_loss=6.378, mask_ins=1.455, word_ins_ml=7.543, word_reposition=0.805, ppl=892.84, wps=11617.7, ups=0.85, wpb=13612.9, bsz=415.9, num_updates=8000, lr=0.000395285, gnorm=1.425, clip=0, loss_scale=128, train_wall=116, wall=9649
2022-10-13 06:43:20 | INFO | train_inner | epoch 017:    319 / 487 loss=9.764, nll_loss=6.351, mask_ins=1.451, word_ins_ml=7.518, word_reposition=0.795, ppl=869.25, wps=11668.1, ups=0.86, wpb=13564.5, bsz=409.9, num_updates=8100, lr=0.000392837, gnorm=1.42, clip=0, loss_scale=128, train_wall=115, wall=9766
2022-10-13 06:45:16 | INFO | train_inner | epoch 017:    419 / 487 loss=9.721, nll_loss=6.332, mask_ins=1.44, word_ins_ml=7.501, word_reposition=0.779, ppl=843.69, wps=12028.7, ups=0.86, wpb=13989.8, bsz=423.5, num_updates=8200, lr=0.000390434, gnorm=1.446, clip=0, loss_scale=128, train_wall=115, wall=9882
2022-10-13 06:46:35 | INFO | train | epoch 017 | loss 9.772 | nll_loss 6.36 | mask_ins 1.452 | word_ins_ml 7.526 | word_reposition 0.794 | ppl 874.33 | wps 11462.8 | ups 0.84 | wpb 13710.5 | bsz 416.3 | num_updates 8268 | lr 0.000388826 | gnorm 1.423 | clip 0 | loss_scale 122 | train_wall 563 | wall 9961
2022-10-13 06:46:46 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.901 | nll_loss 7.365 | mask_ins 1.58 | word_ins_ml 8.551 | word_reposition 0.771 | ppl 1912.52 | wps 27822.6 | wpb 1524.1 | bsz 52.2 | num_updates 8268 | best_loss 10.901
2022-10-13 06:46:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_best.pt (epoch 17 @ 8268 updates, score 10.901) (writing took 4.089070430985885 seconds)
2022-10-13 06:47:28 | INFO | train_inner | epoch 018:     32 / 487 loss=9.723, nll_loss=6.321, mask_ins=1.441, word_ins_ml=7.492, word_reposition=0.79, ppl=845.2, wps=10553.9, ups=0.76, wpb=13898.4, bsz=422.4, num_updates=8300, lr=0.000388075, gnorm=1.402, clip=0, loss_scale=128, train_wall=115, wall=10014
2022-10-13 06:49:25 | INFO | train_inner | epoch 018:    132 / 487 loss=9.682, nll_loss=6.267, mask_ins=1.444, word_ins_ml=7.445, word_reposition=0.793, ppl=821.64, wps=11797.9, ups=0.85, wpb=13802.3, bsz=417.1, num_updates=8400, lr=0.000385758, gnorm=1.399, clip=0, loss_scale=209, train_wall=116, wall=10131
2022-10-13 06:51:22 | INFO | train_inner | epoch 018:    232 / 487 loss=9.65, nll_loss=6.247, mask_ins=1.437, word_ins_ml=7.427, word_reposition=0.786, ppl=803.47, wps=11794.9, ups=0.85, wpb=13835.5, bsz=420.2, num_updates=8500, lr=0.000383482, gnorm=1.482, clip=0, loss_scale=256, train_wall=116, wall=10248
2022-10-13 06:53:18 | INFO | train_inner | epoch 018:    332 / 487 loss=9.68, nll_loss=6.274, mask_ins=1.443, word_ins_ml=7.45, word_reposition=0.787, ppl=820.27, wps=11731.2, ups=0.86, wpb=13674.4, bsz=417, num_updates=8600, lr=0.000381246, gnorm=1.384, clip=0, loss_scale=256, train_wall=116, wall=10364
2022-10-13 06:55:14 | INFO | train_inner | epoch 018:    432 / 487 loss=9.67, nll_loss=6.274, mask_ins=1.436, word_ins_ml=7.45, word_reposition=0.784, ppl=814.85, wps=11653.2, ups=0.86, wpb=13501.2, bsz=408.5, num_updates=8700, lr=0.000379049, gnorm=1.405, clip=0, loss_scale=256, train_wall=115, wall=10480
2022-10-13 06:56:18 | INFO | train | epoch 018 | loss 9.67 | nll_loss 6.266 | mask_ins 1.439 | word_ins_ml 7.443 | word_reposition 0.788 | ppl 814.62 | wps 11438.5 | ups 0.83 | wpb 13709 | bsz 416.2 | num_updates 8755 | lr 0.000377857 | gnorm 1.42 | clip 0 | loss_scale 238 | train_wall 564 | wall 10544
2022-10-13 06:56:30 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.941 | nll_loss 7.406 | mask_ins 1.562 | word_ins_ml 8.612 | word_reposition 0.767 | ppl 1965.9 | wps 27758.1 | wpb 1524.1 | bsz 52.2 | num_updates 8755 | best_loss 10.901
2022-10-13 06:56:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 18 @ 8755 updates, score 10.941) (writing took 2.7593706169864163 seconds)
2022-10-13 06:57:25 | INFO | train_inner | epoch 019:     45 / 487 loss=9.649, nll_loss=6.243, mask_ins=1.435, word_ins_ml=7.422, word_reposition=0.792, ppl=802.66, wps=10279.2, ups=0.77, wpb=13422, bsz=405.5, num_updates=8800, lr=0.000376889, gnorm=1.391, clip=0, loss_scale=256, train_wall=115, wall=10611
2022-10-13 06:59:21 | INFO | train_inner | epoch 019:    145 / 487 loss=9.574, nll_loss=6.199, mask_ins=1.417, word_ins_ml=7.384, word_reposition=0.774, ppl=762.37, wps=12062.8, ups=0.86, wpb=13982.8, bsz=424.2, num_updates=8900, lr=0.000374766, gnorm=1.364, clip=0, loss_scale=387, train_wall=115, wall=10727
2022-10-13 07:01:17 | INFO | train_inner | epoch 019:    245 / 487 loss=9.599, nll_loss=6.196, mask_ins=1.437, word_ins_ml=7.381, word_reposition=0.781, ppl=775.71, wps=11760.7, ups=0.86, wpb=13719.5, bsz=418, num_updates=9000, lr=0.000372678, gnorm=1.391, clip=0, loss_scale=512, train_wall=116, wall=10843
2022-10-13 07:03:14 | INFO | train_inner | epoch 019:    345 / 487 loss=9.58, nll_loss=6.202, mask_ins=1.425, word_ins_ml=7.386, word_reposition=0.77, ppl=765.47, wps=11693.1, ups=0.86, wpb=13611.9, bsz=415, num_updates=9100, lr=0.000370625, gnorm=1.394, clip=0, loss_scale=512, train_wall=116, wall=10960
2022-10-13 07:05:09 | INFO | train_inner | epoch 019:    445 / 487 loss=9.621, nll_loss=6.218, mask_ins=1.435, word_ins_ml=7.4, word_reposition=0.786, ppl=787.38, wps=11740.7, ups=0.87, wpb=13554.7, bsz=409.1, num_updates=9200, lr=0.000368605, gnorm=1.34, clip=0, loss_scale=512, train_wall=115, wall=11075
2022-10-13 07:05:58 | INFO | train | epoch 019 | loss 9.593 | nll_loss 6.201 | mask_ins 1.429 | word_ins_ml 7.386 | word_reposition 0.778 | ppl 772.15 | wps 11522.4 | ups 0.84 | wpb 13710.8 | bsz 416.3 | num_updates 9242 | lr 0.000367766 | gnorm 1.369 | clip 0 | loss_scale 463 | train_wall 561 | wall 11124
2022-10-13 07:06:09 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.018 | nll_loss 7.474 | mask_ins 1.575 | word_ins_ml 8.689 | word_reposition 0.753 | ppl 2074 | wps 27840.2 | wpb 1524.1 | bsz 52.2 | num_updates 9242 | best_loss 10.901
2022-10-13 07:06:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 19 @ 9242 updates, score 11.018) (writing took 2.692387145012617 seconds)
2022-10-13 07:07:19 | INFO | train_inner | epoch 020:     58 / 487 loss=9.527, nll_loss=6.145, mask_ins=1.422, word_ins_ml=7.337, word_reposition=0.768, ppl=737.99, wps=10575, ups=0.77, wpb=13716.3, bsz=418.6, num_updates=9300, lr=0.000366618, gnorm=1.363, clip=0, loss_scale=512, train_wall=115, wall=11205
2022-10-13 07:09:16 | INFO | train_inner | epoch 020:    158 / 487 loss=9.518, nll_loss=6.125, mask_ins=1.426, word_ins_ml=7.318, word_reposition=0.774, ppl=733.07, wps=11888.1, ups=0.86, wpb=13861.4, bsz=423.9, num_updates=9400, lr=0.000364662, gnorm=1.477, clip=0, loss_scale=712, train_wall=116, wall=11322
2022-10-13 07:11:12 | INFO | train_inner | epoch 020:    258 / 487 loss=9.527, nll_loss=6.14, mask_ins=1.421, word_ins_ml=7.332, word_reposition=0.774, ppl=737.7, wps=11684.3, ups=0.86, wpb=13550.4, bsz=408.5, num_updates=9500, lr=0.000362738, gnorm=1.368, clip=0, loss_scale=1024, train_wall=115, wall=11438
2022-10-13 07:13:08 | INFO | train_inner | epoch 020:    358 / 487 loss=9.513, nll_loss=6.131, mask_ins=1.417, word_ins_ml=7.324, word_reposition=0.772, ppl=730.43, wps=12071.2, ups=0.86, wpb=14078.4, bsz=429.8, num_updates=9600, lr=0.000360844, gnorm=1.344, clip=0, loss_scale=1024, train_wall=116, wall=11554
2022-10-13 07:15:05 | INFO | train_inner | epoch 020:    458 / 487 loss=9.558, nll_loss=6.147, mask_ins=1.438, word_ins_ml=7.338, word_reposition=0.782, ppl=753.71, wps=11407.4, ups=0.85, wpb=13359.3, bsz=402.6, num_updates=9700, lr=0.000358979, gnorm=1.396, clip=0, loss_scale=1024, train_wall=116, wall=11671
2022-10-13 07:15:39 | INFO | train | epoch 020 | loss 9.528 | nll_loss 6.138 | mask_ins 1.424 | word_ins_ml 7.33 | word_reposition 0.774 | ppl 738.12 | wps 11485.2 | ups 0.84 | wpb 13710.9 | bsz 416.3 | num_updates 9729 | lr 0.000358444 | gnorm 1.387 | clip 0 | loss_scale 899 | train_wall 563 | wall 11705
2022-10-13 07:15:51 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.073 | nll_loss 7.555 | mask_ins 1.576 | word_ins_ml 8.771 | word_reposition 0.726 | ppl 2154.99 | wps 27817.8 | wpb 1524.1 | bsz 52.2 | num_updates 9729 | best_loss 10.901
2022-10-13 07:15:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 20 @ 9729 updates, score 11.073) (writing took 2.516455557983136 seconds)
2022-10-13 07:17:16 | INFO | train_inner | epoch 021:     71 / 487 loss=9.509, nll_loss=6.125, mask_ins=1.415, word_ins_ml=7.318, word_reposition=0.775, ppl=728.41, wps=10608.7, ups=0.76, wpb=13884.5, bsz=421.9, num_updates=9800, lr=0.000357143, gnorm=1.379, clip=0, loss_scale=1024, train_wall=116, wall=11802
2022-10-13 07:19:12 | INFO | train_inner | epoch 021:    171 / 487 loss=9.485, nll_loss=6.101, mask_ins=1.417, word_ins_ml=7.298, word_reposition=0.771, ppl=716.64, wps=11800.9, ups=0.86, wpb=13716.4, bsz=416.9, num_updates=9900, lr=0.000355335, gnorm=1.366, clip=0, loss_scale=1300, train_wall=115, wall=11918
2022-10-13 07:21:10 | INFO | train_inner | epoch 021:    271 / 487 loss=9.504, nll_loss=6.121, mask_ins=1.428, word_ins_ml=7.315, word_reposition=0.761, ppl=726.02, wps=11746.1, ups=0.85, wpb=13784, bsz=416.9, num_updates=10000, lr=0.000353553, gnorm=1.372, clip=0, loss_scale=2048, train_wall=116, wall=12036
2022-10-13 07:23:07 | INFO | train_inner | epoch 021:    371 / 487 loss=9.454, nll_loss=6.081, mask_ins=1.413, word_ins_ml=7.279, word_reposition=0.762, ppl=701.39, wps=11483.7, ups=0.85, wpb=13482.9, bsz=407.8, num_updates=10100, lr=0.000351799, gnorm=1.343, clip=0, loss_scale=2048, train_wall=117, wall=12153
2022-10-13 07:25:05 | INFO | train_inner | epoch 021:    471 / 487 loss=9.498, nll_loss=6.122, mask_ins=1.413, word_ins_ml=7.314, word_reposition=0.771, ppl=723.13, wps=11753.1, ups=0.85, wpb=13842.5, bsz=422.2, num_updates=10200, lr=0.00035007, gnorm=1.378, clip=0, loss_scale=2048, train_wall=117, wall=12271
2022-10-13 07:25:23 | INFO | train | epoch 021 | loss 9.485 | nll_loss 6.104 | mask_ins 1.417 | word_ins_ml 7.3 | word_reposition 0.768 | ppl 716.77 | wps 11431.3 | ups 0.83 | wpb 13710.3 | bsz 416.3 | num_updates 10216 | lr 0.000349796 | gnorm 1.369 | clip 0 | loss_scale 1745 | train_wall 566 | wall 12289
2022-10-13 07:25:35 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.201 | nll_loss 7.67 | mask_ins 1.59 | word_ins_ml 8.878 | word_reposition 0.733 | ppl 2353.98 | wps 27894 | wpb 1524.1 | bsz 52.2 | num_updates 10216 | best_loss 10.901
2022-10-13 07:25:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 21 @ 10216 updates, score 11.201) (writing took 2.36969817901263 seconds)
2022-10-13 07:27:16 | INFO | train_inner | epoch 022:     84 / 487 loss=9.443, nll_loss=6.077, mask_ins=1.407, word_ins_ml=7.276, word_reposition=0.76, ppl=696.19, wps=10668.9, ups=0.76, wpb=13981.7, bsz=425.4, num_updates=10300, lr=0.000348367, gnorm=1.353, clip=0, loss_scale=2048, train_wall=116, wall=12402
2022-10-13 07:29:13 | INFO | train_inner | epoch 022:    184 / 487 loss=9.404, nll_loss=6.025, mask_ins=1.419, word_ins_ml=7.231, word_reposition=0.754, ppl=677.58, wps=11756.1, ups=0.85, wpb=13751.7, bsz=419.6, num_updates=10400, lr=0.000346688, gnorm=1.35, clip=0, loss_scale=2355, train_wall=116, wall=12519
2022-10-13 07:31:09 | INFO | train_inner | epoch 022:    284 / 487 loss=9.447, nll_loss=6.065, mask_ins=1.412, word_ins_ml=7.265, word_reposition=0.77, ppl=698.1, wps=11725.9, ups=0.86, wpb=13654.7, bsz=413.6, num_updates=10500, lr=0.000345033, gnorm=1.39, clip=0, loss_scale=4096, train_wall=116, wall=12635
2022-10-13 07:33:06 | INFO | train_inner | epoch 022:    384 / 487 loss=9.404, nll_loss=6.035, mask_ins=1.409, word_ins_ml=7.239, word_reposition=0.757, ppl=677.66, wps=11731.6, ups=0.86, wpb=13635.3, bsz=411, num_updates=10600, lr=0.000343401, gnorm=1.286, clip=0, loss_scale=4096, train_wall=115, wall=12752
2022-10-13 07:35:02 | INFO | train_inner | epoch 022:    484 / 487 loss=9.41, nll_loss=6.051, mask_ins=1.397, word_ins_ml=7.252, word_reposition=0.761, ppl=680.38, wps=11614.1, ups=0.86, wpb=13508.8, bsz=410.8, num_updates=10700, lr=0.000341793, gnorm=1.365, clip=0, loss_scale=4096, train_wall=115, wall=12868
2022-10-13 07:35:05 | INFO | train | epoch 022 | loss 9.42 | nll_loss 6.049 | mask_ins 1.408 | word_ins_ml 7.251 | word_reposition 0.76 | ppl 684.89 | wps 11478 | ups 0.84 | wpb 13710.6 | bsz 416.3 | num_updates 10703 | lr 0.000341745 | gnorm 1.35 | clip 0 | loss_scale 3385 | train_wall 563 | wall 12871
2022-10-13 07:35:16 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 11.163 | nll_loss 7.642 | mask_ins 1.571 | word_ins_ml 8.863 | word_reposition 0.73 | ppl 2292.72 | wps 27818.9 | wpb 1524.1 | bsz 52.2 | num_updates 10703 | best_loss 10.901
2022-10-13 07:35:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 22 @ 10703 updates, score 11.163) (writing took 2.6960923979931977 seconds)
2022-10-13 07:37:12 | INFO | train_inner | epoch 023:     97 / 487 loss=9.405, nll_loss=6.052, mask_ins=1.405, word_ins_ml=7.253, word_reposition=0.747, ppl=677.91, wps=10672.4, ups=0.77, wpb=13860.1, bsz=419.6, num_updates=10800, lr=0.000340207, gnorm=1.312, clip=0, loss_scale=4096, train_wall=115, wall=12998
2022-10-13 07:39:08 | INFO | train_inner | epoch 023:    197 / 487 loss=9.363, nll_loss=6.007, mask_ins=1.393, word_ins_ml=7.214, word_reposition=0.757, ppl=658.55, wps=11649.5, ups=0.86, wpb=13567, bsz=412.4, num_updates=10900, lr=0.000338643, gnorm=1.367, clip=0, loss_scale=4219, train_wall=116, wall=13114
2022-10-13 07:41:04 | INFO | train_inner | epoch 023:    297 / 487 loss=9.379, nll_loss=6.025, mask_ins=1.404, word_ins_ml=7.23, word_reposition=0.744, ppl=665.65, wps=11845.8, ups=0.86, wpb=13733.6, bsz=416.9, num_updates=11000, lr=0.0003371, gnorm=1.323, clip=0, loss_scale=8192, train_wall=115, wall=13230
2022-10-13 07:43:01 | INFO | train_inner | epoch 023:    397 / 487 loss=9.394, nll_loss=6.022, mask_ins=1.408, word_ins_ml=7.226, word_reposition=0.76, ppl=672.75, wps=11783.6, ups=0.86, wpb=13749.6, bsz=416.4, num_updates=11100, lr=0.000335578, gnorm=1.342, clip=0, loss_scale=8192, train_wall=116, wall=13347
2022-10-13 07:43:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-10-13 07:44:45 | INFO | train | epoch 023 | loss 9.377 | nll_loss 6.02 | mask_ins 1.4 | word_ins_ml 7.225 | word_reposition 0.752 | ppl 664.87 | wps 11488 | ups 0.84 | wpb 13712.4 | bsz 416.3 | num_updates 11189 | lr 0.000334241 | gnorm 1.335 | clip 0 | loss_scale 6199 | train_wall 562 | wall 13451
2022-10-13 07:44:57 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 11.285 | nll_loss 7.777 | mask_ins 1.582 | word_ins_ml 8.96 | word_reposition 0.743 | ppl 2495.66 | wps 27874.3 | wpb 1524.1 | bsz 52.2 | num_updates 11189 | best_loss 10.901
2022-10-13 07:44:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 23 @ 11189 updates, score 11.285) (writing took 2.6918857370037585 seconds)
2022-10-13 07:45:12 | INFO | train_inner | epoch 024:     11 / 487 loss=9.332, nll_loss=5.984, mask_ins=1.388, word_ins_ml=7.194, word_reposition=0.75, ppl=644.45, wps=10364.1, ups=0.76, wpb=13586.1, bsz=415.9, num_updates=11200, lr=0.000334077, gnorm=1.349, clip=0, loss_scale=6002, train_wall=116, wall=13478
2022-10-13 07:47:09 | INFO | train_inner | epoch 024:    111 / 487 loss=9.31, nll_loss=5.957, mask_ins=1.395, word_ins_ml=7.17, word_reposition=0.744, ppl=634.53, wps=12037.1, ups=0.86, wpb=14048.5, bsz=425.7, num_updates=11300, lr=0.000332595, gnorm=1.311, clip=0, loss_scale=4096, train_wall=116, wall=13595
2022-10-13 07:49:05 | INFO | train_inner | epoch 024:    211 / 487 loss=9.288, nll_loss=5.949, mask_ins=1.385, word_ins_ml=7.163, word_reposition=0.74, ppl=625.11, wps=11736.1, ups=0.86, wpb=13670.1, bsz=415, num_updates=11400, lr=0.000331133, gnorm=1.34, clip=0, loss_scale=4096, train_wall=116, wall=13711
2022-10-13 07:51:01 | INFO | train_inner | epoch 024:    311 / 487 loss=9.317, nll_loss=5.975, mask_ins=1.388, word_ins_ml=7.186, word_reposition=0.743, ppl=637.61, wps=11723.4, ups=0.86, wpb=13613.6, bsz=415.5, num_updates=11500, lr=0.00032969, gnorm=1.342, clip=0, loss_scale=4096, train_wall=115, wall=13827
2022-10-13 07:52:58 | INFO | train_inner | epoch 024:    411 / 487 loss=9.362, nll_loss=6.002, mask_ins=1.402, word_ins_ml=7.209, word_reposition=0.751, ppl=657.82, wps=11742.3, ups=0.85, wpb=13744.2, bsz=416.1, num_updates=11600, lr=0.000328266, gnorm=1.342, clip=0, loss_scale=4096, train_wall=116, wall=13944
2022-10-13 07:54:27 | INFO | train | epoch 024 | loss 9.321 | nll_loss 5.974 | mask_ins 1.391 | word_ins_ml 7.185 | word_reposition 0.745 | ppl 639.4 | wps 11479.5 | ups 0.84 | wpb 13710.7 | bsz 416.3 | num_updates 11676 | lr 0.000327196 | gnorm 1.335 | clip 0 | loss_scale 4247 | train_wall 563 | wall 14033
2022-10-13 07:54:38 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 11.303 | nll_loss 7.768 | mask_ins 1.618 | word_ins_ml 8.956 | word_reposition 0.73 | ppl 2526.98 | wps 27949.1 | wpb 1524.1 | bsz 52.2 | num_updates 11676 | best_loss 10.901
2022-10-13 07:54:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 24 @ 11676 updates, score 11.303) (writing took 2.717355614004191 seconds)
2022-10-13 07:55:09 | INFO | train_inner | epoch 025:     24 / 487 loss=9.318, nll_loss=5.977, mask_ins=1.384, word_ins_ml=7.187, word_reposition=0.747, ppl=638.36, wps=10343, ups=0.77, wpb=13487.2, bsz=408.1, num_updates=11700, lr=0.00032686, gnorm=1.336, clip=0, loss_scale=5816, train_wall=115, wall=14075
2022-10-13 07:56:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-10-13 07:57:06 | INFO | train_inner | epoch 025:    125 / 487 loss=9.281, nll_loss=5.929, mask_ins=1.384, word_ins_ml=7.146, word_reposition=0.751, ppl=621.96, wps=11662.4, ups=0.85, wpb=13653, bsz=415.4, num_updates=11800, lr=0.000325472, gnorm=1.35, clip=0, loss_scale=6489, train_wall=116, wall=14192
2022-10-13 07:59:03 | INFO | train_inner | epoch 025:    225 / 487 loss=9.274, nll_loss=5.918, mask_ins=1.387, word_ins_ml=7.136, word_reposition=0.752, ppl=619.1, wps=11862.5, ups=0.86, wpb=13865.1, bsz=420.7, num_updates=11900, lr=0.000324102, gnorm=1.318, clip=0, loss_scale=4096, train_wall=116, wall=14309
2022-10-13 08:00:59 | INFO | train_inner | epoch 025:    325 / 487 loss=9.294, nll_loss=5.944, mask_ins=1.387, word_ins_ml=7.158, word_reposition=0.749, ppl=627.86, wps=11782, ups=0.86, wpb=13736, bsz=418.1, num_updates=12000, lr=0.000322749, gnorm=1.307, clip=0, loss_scale=4096, train_wall=116, wall=14425
2022-10-13 08:02:55 | INFO | train_inner | epoch 025:    425 / 487 loss=9.299, nll_loss=5.959, mask_ins=1.381, word_ins_ml=7.172, word_reposition=0.746, ppl=629.91, wps=11628.7, ups=0.87, wpb=13436.9, bsz=407.3, num_updates=12100, lr=0.000321412, gnorm=1.304, clip=0, loss_scale=4096, train_wall=115, wall=14541
2022-10-13 08:04:07 | INFO | train | epoch 025 | loss 9.284 | nll_loss 5.934 | mask_ins 1.384 | word_ins_ml 7.149 | word_reposition 0.75 | ppl 623.23 | wps 11482.4 | ups 0.84 | wpb 13713.5 | bsz 416.4 | num_updates 12162 | lr 0.000320592 | gnorm 1.32 | clip 0 | loss_scale 4794 | train_wall 562 | wall 14613
2022-10-13 08:04:19 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 11.499 | nll_loss 7.979 | mask_ins 1.608 | word_ins_ml 9.126 | word_reposition 0.765 | ppl 2895.13 | wps 27792.5 | wpb 1524.1 | bsz 52.2 | num_updates 12162 | best_loss 10.901
2022-10-13 08:04:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 25 @ 12162 updates, score 11.499) (writing took 2.714577256992925 seconds)
2022-10-13 08:05:06 | INFO | train_inner | epoch 026:     38 / 487 loss=9.259, nll_loss=5.906, mask_ins=1.387, word_ins_ml=7.125, word_reposition=0.748, ppl=612.83, wps=10649.7, ups=0.76, wpb=13993.2, bsz=422.8, num_updates=12200, lr=0.000320092, gnorm=1.31, clip=0, loss_scale=4096, train_wall=116, wall=14672
2022-10-13 08:07:05 | INFO | train_inner | epoch 026:    138 / 487 loss=9.249, nll_loss=5.908, mask_ins=1.383, word_ins_ml=7.127, word_reposition=0.738, ppl=608.45, wps=11541.9, ups=0.84, wpb=13693.6, bsz=416, num_updates=12300, lr=0.000318788, gnorm=1.283, clip=0, loss_scale=5325, train_wall=118, wall=14791
2022-10-13 08:09:02 | INFO | train_inner | epoch 026:    238 / 487 loss=9.211, nll_loss=5.88, mask_ins=1.37, word_ins_ml=7.102, word_reposition=0.738, ppl=592.54, wps=11826.5, ups=0.85, wpb=13842.9, bsz=423.4, num_updates=12400, lr=0.0003175, gnorm=1.298, clip=0, loss_scale=8192, train_wall=116, wall=14908
2022-10-13 08:09:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-10-13 08:11:00 | INFO | train_inner | epoch 026:    339 / 487 loss=9.214, nll_loss=5.885, mask_ins=1.368, word_ins_ml=7.106, word_reposition=0.74, ppl=593.77, wps=11611.7, ups=0.85, wpb=13701.6, bsz=416.9, num_updates=12500, lr=0.000316228, gnorm=1.296, clip=0, loss_scale=5272, train_wall=117, wall=15026
2022-10-13 08:12:56 | INFO | train_inner | epoch 026:    439 / 487 loss=9.259, nll_loss=5.913, mask_ins=1.379, word_ins_ml=7.13, word_reposition=0.75, ppl=612.67, wps=11475.7, ups=0.86, wpb=13301, bsz=400.9, num_updates=12600, lr=0.00031497, gnorm=1.31, clip=0, loss_scale=4096, train_wall=115, wall=15142
2022-10-13 08:13:51 | INFO | train | epoch 026 | loss 9.236 | nll_loss 5.899 | mask_ins 1.377 | word_ins_ml 7.118 | word_reposition 0.741 | ppl 602.9 | wps 11404 | ups 0.83 | wpb 13711.6 | bsz 416.3 | num_updates 12648 | lr 0.000314372 | gnorm 1.305 | clip 0 | loss_scale 5433 | train_wall 566 | wall 15197
2022-10-13 08:14:03 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 11.479 | nll_loss 7.997 | mask_ins 1.599 | word_ins_ml 9.137 | word_reposition 0.743 | ppl 2854.01 | wps 27855 | wpb 1524.1 | bsz 52.2 | num_updates 12648 | best_loss 10.901
2022-10-13 08:14:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 26 @ 12648 updates, score 11.479) (writing took 2.5531314400141127 seconds)
2022-10-13 08:15:06 | INFO | train_inner | epoch 027:     52 / 487 loss=9.234, nll_loss=5.902, mask_ins=1.383, word_ins_ml=7.121, word_reposition=0.73, ppl=602.01, wps=10565, ups=0.77, wpb=13785.7, bsz=417.4, num_updates=12700, lr=0.000313728, gnorm=1.369, clip=0, loss_scale=4096, train_wall=115, wall=15272
2022-10-13 08:17:03 | INFO | train_inner | epoch 027:    152 / 487 loss=9.187, nll_loss=5.852, mask_ins=1.371, word_ins_ml=7.078, word_reposition=0.739, ppl=582.96, wps=12055.2, ups=0.86, wpb=14083.7, bsz=429.1, num_updates=12800, lr=0.0003125, gnorm=1.352, clip=0, loss_scale=4096, train_wall=116, wall=15389
2022-10-13 08:19:01 | INFO | train_inner | epoch 027:    252 / 487 loss=9.183, nll_loss=5.857, mask_ins=1.375, word_ins_ml=7.082, word_reposition=0.726, ppl=581.32, wps=11615, ups=0.85, wpb=13689.1, bsz=416.5, num_updates=12900, lr=0.000311286, gnorm=1.328, clip=0, loss_scale=4096, train_wall=117, wall=15507
2022-10-13 08:20:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-10-13 08:21:00 | INFO | train_inner | epoch 027:    353 / 487 loss=9.194, nll_loss=5.873, mask_ins=1.369, word_ins_ml=7.096, word_reposition=0.729, ppl=585.53, wps=11366.7, ups=0.84, wpb=13485, bsz=409.5, num_updates=13000, lr=0.000310087, gnorm=1.3, clip=0, loss_scale=4745, train_wall=118, wall=15626
2022-10-13 08:22:57 | INFO | train_inner | epoch 027:    453 / 487 loss=9.189, nll_loss=5.871, mask_ins=1.37, word_ins_ml=7.093, word_reposition=0.725, ppl=583.76, wps=11576.3, ups=0.85, wpb=13635.1, bsz=412.4, num_updates=13100, lr=0.000308901, gnorm=1.316, clip=0, loss_scale=4096, train_wall=117, wall=15743
2022-10-13 08:23:37 | INFO | train | epoch 027 | loss 9.194 | nll_loss 5.869 | mask_ins 1.373 | word_ins_ml 7.092 | word_reposition 0.729 | ppl 585.75 | wps 11385.6 | ups 0.83 | wpb 13713.3 | bsz 416.4 | num_updates 13134 | lr 0.000308501 | gnorm 1.328 | clip 0 | loss_scale 4231 | train_wall 567 | wall 15783
2022-10-13 08:23:48 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 11.641 | nll_loss 8.162 | mask_ins 1.594 | word_ins_ml 9.272 | word_reposition 0.776 | ppl 3194.46 | wps 27902.3 | wpb 1524.1 | bsz 52.2 | num_updates 13134 | best_loss 10.901
2022-10-13 08:23:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 27 @ 13134 updates, score 11.641) (writing took 2.6086032339953817 seconds)
2022-10-13 08:25:08 | INFO | train_inner | epoch 028:     66 / 487 loss=9.204, nll_loss=5.878, mask_ins=1.376, word_ins_ml=7.099, word_reposition=0.728, ppl=589.72, wps=10768.7, ups=0.77, wpb=14027.2, bsz=426.8, num_updates=13200, lr=0.000307729, gnorm=1.338, clip=0, loss_scale=4096, train_wall=115, wall=15874
2022-10-13 08:25:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-10-13 08:27:07 | INFO | train_inner | epoch 028:    167 / 487 loss=9.175, nll_loss=5.834, mask_ins=1.379, word_ins_ml=7.061, word_reposition=0.735, ppl=578.02, wps=11468.4, ups=0.84, wpb=13638, bsz=414.8, num_updates=13300, lr=0.00030657, gnorm=1.318, clip=0, loss_scale=2372, train_wall=118, wall=15993
2022-10-13 08:29:04 | INFO | train_inner | epoch 028:    267 / 487 loss=9.165, nll_loss=5.84, mask_ins=1.368, word_ins_ml=7.066, word_reposition=0.731, ppl=574.11, wps=11408.4, ups=0.85, wpb=13407.3, bsz=404.1, num_updates=13400, lr=0.000305424, gnorm=1.301, clip=0, loss_scale=2048, train_wall=116, wall=16110
2022-10-13 08:31:01 | INFO | train_inner | epoch 028:    367 / 487 loss=9.186, nll_loss=5.856, mask_ins=1.38, word_ins_ml=7.081, word_reposition=0.726, ppl=582.34, wps=11951.6, ups=0.86, wpb=13924.6, bsz=426.5, num_updates=13500, lr=0.00030429, gnorm=1.368, clip=0, loss_scale=2048, train_wall=116, wall=16227
2022-10-13 08:32:57 | INFO | train_inner | epoch 028:    467 / 487 loss=9.211, nll_loss=5.892, mask_ins=1.371, word_ins_ml=7.112, word_reposition=0.729, ppl=592.62, wps=11513.9, ups=0.86, wpb=13447.3, bsz=407, num_updates=13600, lr=0.00030317, gnorm=1.282, clip=0, loss_scale=2048, train_wall=116, wall=16343
2022-10-13 08:33:21 | INFO | train | epoch 028 | loss 9.185 | nll_loss 5.858 | mask_ins 1.374 | word_ins_ml 7.082 | word_reposition 0.73 | ppl 582.25 | wps 11412.1 | ups 0.83 | wpb 13708.4 | bsz 416.2 | num_updates 13620 | lr 0.000302947 | gnorm 1.321 | clip 0 | loss_scale 2393 | train_wall 565 | wall 16367
2022-10-13 08:33:32 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 11.724 | nll_loss 8.22 | mask_ins 1.621 | word_ins_ml 9.278 | word_reposition 0.825 | ppl 3382.21 | wps 27823.7 | wpb 1524.1 | bsz 52.2 | num_updates 13620 | best_loss 10.901
2022-10-13 08:33:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 28 @ 13620 updates, score 11.724) (writing took 2.638734228996327 seconds)
2022-10-13 08:35:08 | INFO | train_inner | epoch 029:     80 / 487 loss=9.141, nll_loss=5.822, mask_ins=1.363, word_ins_ml=7.051, word_reposition=0.727, ppl=564.52, wps=10712.1, ups=0.76, wpb=14027.6, bsz=428.7, num_updates=13700, lr=0.000302061, gnorm=1.315, clip=0, loss_scale=2048, train_wall=116, wall=16474
2022-10-13 08:37:05 | INFO | train_inner | epoch 029:    180 / 487 loss=9.128, nll_loss=5.805, mask_ins=1.369, word_ins_ml=7.036, word_reposition=0.723, ppl=559.53, wps=11493.1, ups=0.86, wpb=13374.8, bsz=403.8, num_updates=13800, lr=0.000300965, gnorm=1.317, clip=0, loss_scale=3543, train_wall=115, wall=16591
2022-10-13 08:39:02 | INFO | train_inner | epoch 029:    280 / 487 loss=9.142, nll_loss=5.82, mask_ins=1.364, word_ins_ml=7.05, word_reposition=0.729, ppl=565.1, wps=11760.8, ups=0.85, wpb=13762.3, bsz=418.1, num_updates=13900, lr=0.00029988, gnorm=1.296, clip=0, loss_scale=4096, train_wall=116, wall=16708
2022-10-13 08:40:59 | INFO | train_inner | epoch 029:    380 / 487 loss=9.155, nll_loss=5.839, mask_ins=1.361, word_ins_ml=7.066, word_reposition=0.729, ppl=570.26, wps=11705.7, ups=0.86, wpb=13678.2, bsz=412.3, num_updates=14000, lr=0.000298807, gnorm=1.328, clip=0, loss_scale=4096, train_wall=116, wall=16825
2022-10-13 08:42:57 | INFO | train_inner | epoch 029:    480 / 487 loss=9.174, nll_loss=5.839, mask_ins=1.379, word_ins_ml=7.065, word_reposition=0.73, ppl=577.78, wps=11766, ups=0.85, wpb=13868.1, bsz=422.3, num_updates=14100, lr=0.000297746, gnorm=1.319, clip=0, loss_scale=4096, train_wall=117, wall=16943
2022-10-13 08:43:04 | INFO | train | epoch 029 | loss 9.148 | nll_loss 5.823 | mask_ins 1.368 | word_ins_ml 7.052 | word_reposition 0.728 | ppl 567.3 | wps 11438.1 | ups 0.83 | wpb 13711 | bsz 416.3 | num_updates 14107 | lr 0.000297672 | gnorm 1.315 | clip 0 | loss_scale 3646 | train_wall 565 | wall 16950
2022-10-13 08:43:16 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 12.069 | nll_loss 8.565 | mask_ins 1.67 | word_ins_ml 9.546 | word_reposition 0.853 | ppl 4297.25 | wps 27818 | wpb 1524.1 | bsz 52.2 | num_updates 14107 | best_loss 10.901
2022-10-13 08:43:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 29 @ 14107 updates, score 12.069) (writing took 2.6145002389966976 seconds)
2022-10-13 08:45:08 | INFO | train_inner | epoch 030:     93 / 487 loss=9.112, nll_loss=5.789, mask_ins=1.359, word_ins_ml=7.023, word_reposition=0.73, ppl=553.17, wps=10514.7, ups=0.76, wpb=13873.3, bsz=420.5, num_updates=14200, lr=0.000296695, gnorm=1.32, clip=0, loss_scale=4096, train_wall=117, wall=17074
2022-10-13 08:47:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-10-13 08:47:07 | INFO | train_inner | epoch 030:    194 / 487 loss=9.134, nll_loss=5.825, mask_ins=1.356, word_ins_ml=7.053, word_reposition=0.725, ppl=561.84, wps=11637.9, ups=0.85, wpb=13761.8, bsz=413.8, num_updates=14300, lr=0.000295656, gnorm=1.325, clip=0, loss_scale=6448, train_wall=117, wall=17193
2022-10-13 08:49:07 | INFO | train_inner | epoch 030:    294 / 487 loss=9.094, nll_loss=5.783, mask_ins=1.356, word_ins_ml=7.017, word_reposition=0.721, ppl=546.64, wps=11507, ups=0.83, wpb=13789.9, bsz=420.8, num_updates=14400, lr=0.000294628, gnorm=1.295, clip=0, loss_scale=4096, train_wall=119, wall=17313
2022-10-13 08:51:05 | INFO | train_inner | epoch 030:    394 / 487 loss=9.098, nll_loss=5.794, mask_ins=1.354, word_ins_ml=7.026, word_reposition=0.717, ppl=547.96, wps=11502.6, ups=0.85, wpb=13599.5, bsz=414.7, num_updates=14500, lr=0.00029361, gnorm=1.31, clip=0, loss_scale=4096, train_wall=117, wall=17431
2022-10-13 08:52:53 | INFO | train | epoch 030 | loss 9.114 | nll_loss 5.801 | mask_ins 1.358 | word_ins_ml 7.032 | word_reposition 0.724 | ppl 554.03 | wps 11314.1 | ups 0.83 | wpb 13709.5 | bsz 416.3 | num_updates 14593 | lr 0.000292673 | gnorm 1.317 | clip 0 | loss_scale 4584 | train_wall 570 | wall 17539
2022-10-13 08:53:05 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 12.046 | nll_loss 8.56 | mask_ins 1.67 | word_ins_ml 9.527 | word_reposition 0.849 | ppl 4228.4 | wps 27734 | wpb 1524.1 | bsz 52.2 | num_updates 14593 | best_loss 10.901
2022-10-13 08:53:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 30 @ 14593 updates, score 12.046) (writing took 2.5711469940142706 seconds)
2022-10-13 08:53:16 | INFO | train_inner | epoch 031:      7 / 487 loss=9.138, nll_loss=5.816, mask_ins=1.365, word_ins_ml=7.045, word_reposition=0.728, ppl=563.49, wps=10269.4, ups=0.77, wpb=13423, bsz=407.8, num_updates=14600, lr=0.000292603, gnorm=1.333, clip=0, loss_scale=4096, train_wall=116, wall=17561
2022-10-13 08:55:14 | INFO | train_inner | epoch 031:    107 / 487 loss=9.107, nll_loss=5.777, mask_ins=1.368, word_ins_ml=7.011, word_reposition=0.729, ppl=551.55, wps=11353.4, ups=0.85, wpb=13427.7, bsz=404.8, num_updates=14700, lr=0.000291606, gnorm=1.347, clip=0, loss_scale=4096, train_wall=117, wall=17680
2022-10-13 08:57:12 | INFO | train_inner | epoch 031:    207 / 487 loss=9.103, nll_loss=5.782, mask_ins=1.368, word_ins_ml=7.015, word_reposition=0.72, ppl=549.85, wps=11607.8, ups=0.85, wpb=13689.1, bsz=415.7, num_updates=14800, lr=0.000290619, gnorm=1.324, clip=0, loss_scale=4096, train_wall=117, wall=17798
2022-10-13 08:57:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-10-13 08:59:11 | INFO | train_inner | epoch 031:    308 / 487 loss=9.05, nll_loss=5.753, mask_ins=1.342, word_ins_ml=6.99, word_reposition=0.718, ppl=530.09, wps=11592.3, ups=0.84, wpb=13772.1, bsz=418.3, num_updates=14900, lr=0.000289642, gnorm=1.282, clip=0, loss_scale=5272, train_wall=118, wall=17917
2022-10-13 09:01:07 | INFO | train_inner | epoch 031:    408 / 487 loss=9.078, nll_loss=5.767, mask_ins=1.351, word_ins_ml=7.002, word_reposition=0.725, ppl=540.48, wps=11779.8, ups=0.86, wpb=13774.9, bsz=418.9, num_updates=15000, lr=0.000288675, gnorm=1.273, clip=0, loss_scale=4096, train_wall=116, wall=18033
2022-10-13 09:02:39 | INFO | train | epoch 031 | loss 9.085 | nll_loss 5.772 | mask_ins 1.355 | word_ins_ml 7.007 | word_reposition 0.723 | ppl 543.08 | wps 11360 | ups 0.83 | wpb 13702.3 | bsz 416 | num_updates 15079 | lr 0.000287918 | gnorm 1.303 | clip 0 | loss_scale 4340 | train_wall 568 | wall 18125
2022-10-13 09:02:51 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 12.044 | nll_loss 8.552 | mask_ins 1.66 | word_ins_ml 9.512 | word_reposition 0.872 | ppl 4222.84 | wps 27863 | wpb 1524.1 | bsz 52.2 | num_updates 15079 | best_loss 10.901
2022-10-13 09:02:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 31 @ 15079 updates, score 12.044) (writing took 2.8900161939964164 seconds)
2022-10-13 09:03:18 | INFO | train_inner | epoch 032:     21 / 487 loss=9.081, nll_loss=5.777, mask_ins=1.348, word_ins_ml=7.011, word_reposition=0.722, ppl=541.67, wps=10601.9, ups=0.76, wpb=13866.1, bsz=421.9, num_updates=15100, lr=0.000287718, gnorm=1.31, clip=0, loss_scale=4096, train_wall=115, wall=18164
2022-10-13 09:05:14 | INFO | train_inner | epoch 032:    121 / 487 loss=9.058, nll_loss=5.726, mask_ins=1.363, word_ins_ml=6.967, word_reposition=0.728, ppl=532.84, wps=11881.6, ups=0.86, wpb=13795.6, bsz=420.7, num_updates=15200, lr=0.00028677, gnorm=1.327, clip=0, loss_scale=4096, train_wall=115, wall=18280
2022-10-13 09:07:12 | INFO | train_inner | epoch 032:    221 / 487 loss=9.035, nll_loss=5.742, mask_ins=1.343, word_ins_ml=6.981, word_reposition=0.711, ppl=524.65, wps=11854.9, ups=0.85, wpb=13906.2, bsz=422.8, num_updates=15300, lr=0.000285831, gnorm=1.33, clip=0, loss_scale=4096, train_wall=116, wall=18398
2022-10-13 09:08:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-10-13 09:09:09 | INFO | train_inner | epoch 032:    322 / 487 loss=9.051, nll_loss=5.756, mask_ins=1.351, word_ins_ml=6.992, word_reposition=0.708, ppl=530.42, wps=11664, ups=0.85, wpb=13744.4, bsz=417.4, num_updates=15400, lr=0.000284901, gnorm=1.333, clip=0, loss_scale=4339, train_wall=117, wall=18515
2022-10-13 09:11:05 | INFO | train_inner | epoch 032:    422 / 487 loss=9.092, nll_loss=5.762, mask_ins=1.368, word_ins_ml=6.998, word_reposition=0.725, ppl=545.57, wps=11728.1, ups=0.86, wpb=13598.8, bsz=411.6, num_updates=15500, lr=0.000283981, gnorm=1.311, clip=0, loss_scale=4096, train_wall=115, wall=18631
2022-10-13 09:12:21 | INFO | train | epoch 032 | loss 9.062 | nll_loss 5.75 | mask_ins 1.355 | word_ins_ml 6.988 | word_reposition 0.719 | ppl 534.32 | wps 11465.5 | ups 0.84 | wpb 13711.4 | bsz 416.3 | num_updates 15565 | lr 0.000283387 | gnorm 1.323 | clip 0 | loss_scale 4146 | train_wall 562 | wall 18707
2022-10-13 09:12:32 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 12.232 | nll_loss 8.675 | mask_ins 1.73 | word_ins_ml 9.598 | word_reposition 0.903 | ppl 4809.62 | wps 27890.5 | wpb 1524.1 | bsz 52.2 | num_updates 15565 | best_loss 10.901
2022-10-13 09:12:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_kpe_cased_XSum/checkpoint_last.pt (epoch 32 @ 15565 updates, score 12.232) (writing took 2.8829478150000796 seconds)
2022-10-13 09:13:15 | INFO | train_inner | epoch 033:     35 / 487 loss=9.079, nll_loss=5.763, mask_ins=1.354, word_ins_ml=6.999, word_reposition=0.726, ppl=540.64, wps=10431.4, ups=0.77, wpb=13519.7, bsz=410.8, num_updates=15600, lr=0.000283069, gnorm=1.311, clip=0, loss_scale=4096, train_wall=114, wall=18761
2022-10-13 09:15:12 | INFO | train_inner | epoch 033:    135 / 487 loss=9.007, nll_loss=5.692, mask_ins=1.35, word_ins_ml=6.937, word_reposition=0.72, ppl=514.47, wps=11675.6, ups=0.86, wpb=13607.9, bsz=411.1, num_updates=15700, lr=0.000282166, gnorm=1.294, clip=0, loss_scale=4096, train_wall=116, wall=18878
2022-10-13 09:16:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-10-13 09:17:09 | INFO | train_inner | epoch 033:    236 / 487 loss=9.055, nll_loss=5.753, mask_ins=1.352, word_ins_ml=6.99, word_reposition=0.714, ppl=531.88, wps=11876.1, ups=0.85, wpb=13928.1, bsz=424.4, num_updates=15800, lr=0.000281272, gnorm=1.286, clip=0, loss_scale=3305, train_wall=116, wall=18995
2022-10-13 09:19:05 | INFO | train_inner | epoch 033:    336 / 487 loss=9.021, nll_loss=5.712, mask_ins=1.351, word_ins_ml=6.954, word_reposition=0.716, ppl=519.37, wps=11869, ups=0.86, wpb=13761.4, bsz=418.9, num_updates=15900, lr=0.000280386, gnorm=1.302, clip=0, loss_scale=2048, train_wall=115, wall=19111
2022-10-13 09:21:01 | INFO | train_inner | epoch 033:    436 / 487 loss=9.046, nll_loss=5.739, mask_ins=1.349, word_ins_ml=6.977, word_reposition=0.72, ppl=528.68, wps=11777.8, ups=0.86, wpb=13640.4, bsz=413.2, num_updates=16000, lr=0.000279508, gnorm=1.305, clip=0, loss_scale=2048, train_wall=115, wall=19227
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
train.sh: line 39: constraint: command not found
