nohup: ignoring input
2022-08-14 16:16:09 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:18510
2022-08-14 16:16:09 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:18510
2022-08-14 16:16:09 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:18510
2022-08-14 16:16:09 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:18510
2022-08-14 16:16:09 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-08-14 16:16:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-08-14 16:16:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-08-14 16:16:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-08-14 16:16:10 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-14 16:16:10 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-08-14 16:16:10 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-14 16:16:10 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-08-14 16:16:10 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-14 16:16:10 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-08-14 16:16:10 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-08-14 16:16:10 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-08-14 16:16:14 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=4, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=4, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:18510', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=False, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-08-14 16:16:14 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-08-14 16:16:14 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-08-14 16:16:14 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.source
2022-08-14 16:16:14 | INFO | fairseq.data.data_utils | loaded 13368 examples from: ../data-bin-bert-cased-510/valid.source-target.target
2022-08-14 16:16:14 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 valid source-target 13368 examples
2022-08-14 16:16:14 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-14 16:16:14 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-14 16:16:14 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-08-14 16:16:18 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight']
2022-08-14 16:16:18 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-08-14 16:16:18 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-08-14 16:16:18 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-08-14 16:16:18 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
2022-08-14 16:16:20 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-08-14 16:16:20 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 662
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 398
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']
2022-08-14 16:16:20 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-08-14 16:16:20 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-08-14 16:16:20 | INFO | fairseq_cli.train | num. model params: 380360448 (num. trained: 380360448)
2022-08-14 16:16:20 | INFO | fairseq_cli.train | num. Encoder model params: 146077440 (Encoder num. trained: 146077440)
2022-08-14 16:16:20 | INFO | fairseq_cli.train | num. Decoder model params: 234283008 (Decoder num. trained: 234283008)
Trained parameters: len 662
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 398
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']Trained parameters: len 662
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 398
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']2022-08-14 16:16:20 | INFO | fairseq_cli.train | training on 4 GPUs
2022-08-14 16:16:20 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 4
2022-08-14 16:16:20 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_bert12_cased/checkpoint_last.pt
2022-08-14 16:16:20 | INFO | fairseq.trainer | loading train data for epoch 1
2022-08-14 16:16:21 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.source
2022-08-14 16:16:21 | INFO | fairseq.data.data_utils | loaded 287112 examples from: ../data-bin-bert-cased-510/train.source-target.target
2022-08-14 16:16:21 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-510 train source-target 287112 examples
Trained parameters: len 662
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 398
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.6.attention.self.query.weight', 'decoder.layers.6.attention.self.query.bias', 'decoder.layers.6.attention.self.key.weight', 'decoder.layers.6.attention.self.key.bias', 'decoder.layers.6.attention.self.value.weight', 'decoder.layers.6.attention.self.value.bias', 'decoder.layers.6.attention.output.dense.weight', 'decoder.layers.6.attention.output.dense.bias', 'decoder.layers.6.attention.output.LayerNorm.weight', 'decoder.layers.6.attention.output.LayerNorm.bias', 'decoder.layers.6.intermediate.dense.weight', 'decoder.layers.6.intermediate.dense.bias', 'decoder.layers.6.output.dense.weight', 'decoder.layers.6.output.dense.bias', 'decoder.layers.6.output.LayerNorm.weight', 'decoder.layers.6.output.LayerNorm.bias', 'decoder.layers.7.attention.self.query.weight', 'decoder.layers.7.attention.self.query.bias', 'decoder.layers.7.attention.self.key.weight', 'decoder.layers.7.attention.self.key.bias', 'decoder.layers.7.attention.self.value.weight', 'decoder.layers.7.attention.self.value.bias', 'decoder.layers.7.attention.output.dense.weight', 'decoder.layers.7.attention.output.dense.bias', 'decoder.layers.7.attention.output.LayerNorm.weight', 'decoder.layers.7.attention.output.LayerNorm.bias', 'decoder.layers.7.intermediate.dense.weight', 'decoder.layers.7.intermediate.dense.bias', 'decoder.layers.7.output.dense.weight', 'decoder.layers.7.output.dense.bias', 'decoder.layers.7.output.LayerNorm.weight', 'decoder.layers.7.output.LayerNorm.bias', 'decoder.layers.8.attention.self.query.weight', 'decoder.layers.8.attention.self.query.bias', 'decoder.layers.8.attention.self.key.weight', 'decoder.layers.8.attention.self.key.bias', 'decoder.layers.8.attention.self.value.weight', 'decoder.layers.8.attention.self.value.bias', 'decoder.layers.8.attention.output.dense.weight', 'decoder.layers.8.attention.output.dense.bias', 'decoder.layers.8.attention.output.LayerNorm.weight', 'decoder.layers.8.attention.output.LayerNorm.bias', 'decoder.layers.8.intermediate.dense.weight', 'decoder.layers.8.intermediate.dense.bias', 'decoder.layers.8.output.dense.weight', 'decoder.layers.8.output.dense.bias', 'decoder.layers.8.output.LayerNorm.weight', 'decoder.layers.8.output.LayerNorm.bias', 'decoder.layers.9.attention.self.query.weight', 'decoder.layers.9.attention.self.query.bias', 'decoder.layers.9.attention.self.key.weight', 'decoder.layers.9.attention.self.key.bias', 'decoder.layers.9.attention.self.value.weight', 'decoder.layers.9.attention.self.value.bias', 'decoder.layers.9.attention.output.dense.weight', 'decoder.layers.9.attention.output.dense.bias', 'decoder.layers.9.attention.output.LayerNorm.weight', 'decoder.layers.9.attention.output.LayerNorm.bias', 'decoder.layers.9.intermediate.dense.weight', 'decoder.layers.9.intermediate.dense.bias', 'decoder.layers.9.output.dense.weight', 'decoder.layers.9.output.dense.bias', 'decoder.layers.9.output.LayerNorm.weight', 'decoder.layers.9.output.LayerNorm.bias', 'decoder.layers.10.attention.self.query.weight', 'decoder.layers.10.attention.self.query.bias', 'decoder.layers.10.attention.self.key.weight', 'decoder.layers.10.attention.self.key.bias', 'decoder.layers.10.attention.self.value.weight', 'decoder.layers.10.attention.self.value.bias', 'decoder.layers.10.attention.output.dense.weight', 'decoder.layers.10.attention.output.dense.bias', 'decoder.layers.10.attention.output.LayerNorm.weight', 'decoder.layers.10.attention.output.LayerNorm.bias', 'decoder.layers.10.intermediate.dense.weight', 'decoder.layers.10.intermediate.dense.bias', 'decoder.layers.10.output.dense.weight', 'decoder.layers.10.output.dense.bias', 'decoder.layers.10.output.LayerNorm.weight', 'decoder.layers.10.output.LayerNorm.bias', 'decoder.layers.11.attention.self.query.weight', 'decoder.layers.11.attention.self.query.bias', 'decoder.layers.11.attention.self.key.weight', 'decoder.layers.11.attention.self.key.bias', 'decoder.layers.11.attention.self.value.weight', 'decoder.layers.11.attention.self.value.bias', 'decoder.layers.11.attention.output.dense.weight', 'decoder.layers.11.attention.output.dense.bias', 'decoder.layers.11.attention.output.LayerNorm.weight', 'decoder.layers.11.attention.output.LayerNorm.bias', 'decoder.layers.11.intermediate.dense.weight', 'decoder.layers.11.intermediate.dense.bias', 'decoder.layers.11.output.dense.weight', 'decoder.layers.11.output.dense.bias', 'decoder.layers.11.output.LayerNorm.weight', 'decoder.layers.11.output.LayerNorm.bias', 'decoder.output_projection.weight']2022-08-14 16:16:23 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-08-14 16:21:14 | INFO | train_inner | epoch 001:    100 / 2244 loss=25.715, nll_loss=12.02, mask_ins=7.582, word_ins_ml=12.519, word_reposition=5.615, ppl=5.50828e+07, wps=3569.9, ups=0.35, wpb=10229.3, bsz=128, num_updates=100, lr=1.0098e-05, gnorm=24.176, clip=16, train_wall=291, wall=294
2022-08-14 16:26:02 | INFO | train_inner | epoch 001:    200 / 2244 loss=20.484, nll_loss=11.21, mask_ins=4.603, word_ins_ml=11.779, word_reposition=4.102, ppl=1.46696e+06, wps=3553.8, ups=0.35, wpb=10227.9, bsz=128, num_updates=200, lr=2.0096e-05, gnorm=19.031, clip=0, train_wall=287, wall=582
2022-08-14 16:30:50 | INFO | train_inner | epoch 001:    300 / 2244 loss=15.898, nll_loss=11.095, mask_ins=2.33, word_ins_ml=11.654, word_reposition=1.914, ppl=61048.3, wps=3565.4, ups=0.35, wpb=10267.4, bsz=128, num_updates=300, lr=3.0094e-05, gnorm=6.548, clip=0, train_wall=287, wall=870
2022-08-14 16:35:38 | INFO | train_inner | epoch 001:    400 / 2244 loss=14.771, nll_loss=10.771, mask_ins=1.979, word_ins_ml=11.369, word_reposition=1.422, ppl=27952.8, wps=3569.5, ups=0.35, wpb=10272.4, bsz=128, num_updates=400, lr=4.0092e-05, gnorm=3.724, clip=0, train_wall=287, wall=1157
2022-08-14 16:40:27 | INFO | train_inner | epoch 001:    500 / 2244 loss=14.488, nll_loss=10.608, mask_ins=1.921, word_ins_ml=11.234, word_reposition=1.334, ppl=22979.9, wps=3541.5, ups=0.35, wpb=10226.4, bsz=128, num_updates=500, lr=5.009e-05, gnorm=3.32, clip=0, train_wall=288, wall=1446
2022-08-14 16:45:15 | INFO | train_inner | epoch 001:    600 / 2244 loss=14.231, nll_loss=10.231, mask_ins=1.918, word_ins_ml=10.915, word_reposition=1.398, ppl=19230.6, wps=3552.8, ups=0.35, wpb=10251.2, bsz=128, num_updates=600, lr=6.0088e-05, gnorm=3.342, clip=0, train_wall=288, wall=1735
2022-08-14 16:50:03 | INFO | train_inner | epoch 001:    700 / 2244 loss=13.882, nll_loss=9.807, mask_ins=1.9, word_ins_ml=10.556, word_reposition=1.426, ppl=15097.3, wps=3567.5, ups=0.35, wpb=10267.7, bsz=128, num_updates=700, lr=7.0086e-05, gnorm=3.142, clip=0, train_wall=287, wall=2023
2022-08-14 16:54:50 | INFO | train_inner | epoch 001:    800 / 2244 loss=13.555, nll_loss=9.456, mask_ins=1.858, word_ins_ml=10.255, word_reposition=1.443, ppl=12037.1, wps=3573.2, ups=0.35, wpb=10250.6, bsz=128, num_updates=800, lr=8.0084e-05, gnorm=3.163, clip=0, train_wall=286, wall=2309
2022-08-14 16:59:37 | INFO | train_inner | epoch 001:    900 / 2244 loss=13.278, nll_loss=9.213, mask_ins=1.796, word_ins_ml=10.046, word_reposition=1.436, ppl=9930.73, wps=3587, ups=0.35, wpb=10292.4, bsz=128, num_updates=900, lr=9.0082e-05, gnorm=2.95, clip=0, train_wall=286, wall=2596
2022-08-14 17:04:24 | INFO | train_inner | epoch 001:   1000 / 2244 loss=13.079, nll_loss=9.011, mask_ins=1.766, word_ins_ml=9.873, word_reposition=1.44, ppl=8653.24, wps=3573.2, ups=0.35, wpb=10263.9, bsz=128, num_updates=1000, lr=0.00010008, gnorm=2.723, clip=0, train_wall=287, wall=2884
2022-08-14 17:09:11 | INFO | train_inner | epoch 001:   1100 / 2244 loss=12.948, nll_loss=8.858, mask_ins=1.768, word_ins_ml=9.739, word_reposition=1.441, ppl=7901.75, wps=3594.8, ups=0.35, wpb=10321.8, bsz=128, num_updates=1100, lr=0.000110078, gnorm=2.572, clip=0, train_wall=287, wall=3171
2022-08-14 17:13:58 | INFO | train_inner | epoch 001:   1200 / 2244 loss=12.728, nll_loss=8.697, mask_ins=1.731, word_ins_ml=9.598, word_reposition=1.399, ppl=6783.73, wps=3570.4, ups=0.35, wpb=10242.6, bsz=128, num_updates=1200, lr=0.000120076, gnorm=2.698, clip=0, train_wall=286, wall=3458
2022-08-14 17:18:44 | INFO | train_inner | epoch 001:   1300 / 2244 loss=12.603, nll_loss=8.566, mask_ins=1.723, word_ins_ml=9.483, word_reposition=1.397, ppl=6222.88, wps=3604.4, ups=0.35, wpb=10298.7, bsz=128, num_updates=1300, lr=0.000130074, gnorm=2.662, clip=0, train_wall=285, wall=3743
2022-08-14 17:23:29 | INFO | train_inner | epoch 001:   1400 / 2244 loss=12.47, nll_loss=8.434, mask_ins=1.715, word_ins_ml=9.368, word_reposition=1.387, ppl=5675.05, wps=3619.3, ups=0.35, wpb=10323.2, bsz=128, num_updates=1400, lr=0.000140072, gnorm=2.687, clip=0, train_wall=285, wall=4029
2022-08-14 17:28:14 | INFO | train_inner | epoch 001:   1500 / 2244 loss=12.386, nll_loss=8.356, mask_ins=1.717, word_ins_ml=9.299, word_reposition=1.37, ppl=5354.21, wps=3588.5, ups=0.35, wpb=10236.6, bsz=128, num_updates=1500, lr=0.00015007, gnorm=2.721, clip=0, train_wall=285, wall=4314
2022-08-14 17:33:00 | INFO | train_inner | epoch 001:   1600 / 2244 loss=12.243, nll_loss=8.222, mask_ins=1.698, word_ins_ml=9.181, word_reposition=1.364, ppl=4847.5, wps=3625.2, ups=0.35, wpb=10352.8, bsz=128, num_updates=1600, lr=0.000160068, gnorm=2.55, clip=0, train_wall=285, wall=4599
2022-08-14 17:37:45 | INFO | train_inner | epoch 001:   1700 / 2244 loss=12.159, nll_loss=8.115, mask_ins=1.709, word_ins_ml=9.087, word_reposition=1.364, ppl=4574.46, wps=3605.2, ups=0.35, wpb=10294.4, bsz=128, num_updates=1700, lr=0.000170066, gnorm=2.659, clip=0, train_wall=285, wall=4885
2022-08-14 17:42:31 | INFO | train_inner | epoch 001:   1800 / 2244 loss=12.063, nll_loss=8.024, mask_ins=1.695, word_ins_ml=9.006, word_reposition=1.362, ppl=4279.54, wps=3580.4, ups=0.35, wpb=10220.4, bsz=128, num_updates=1800, lr=0.000180064, gnorm=2.724, clip=0, train_wall=285, wall=5170
2022-08-14 17:47:16 | INFO | train_inner | epoch 001:   1900 / 2244 loss=11.911, nll_loss=7.912, mask_ins=1.669, word_ins_ml=8.908, word_reposition=1.335, ppl=3852.16, wps=3600.4, ups=0.35, wpb=10268.9, bsz=128, num_updates=1900, lr=0.000190062, gnorm=2.64, clip=0, train_wall=285, wall=5456
2022-08-14 17:52:02 | INFO | train_inner | epoch 001:   2000 / 2244 loss=11.837, nll_loss=7.845, mask_ins=1.66, word_ins_ml=8.847, word_reposition=1.329, ppl=3658.06, wps=3603.4, ups=0.35, wpb=10297.8, bsz=128, num_updates=2000, lr=0.00020006, gnorm=2.822, clip=0, train_wall=285, wall=5741
2022-08-14 17:56:46 | INFO | train_inner | epoch 001:   2100 / 2244 loss=11.722, nll_loss=7.722, mask_ins=1.665, word_ins_ml=8.739, word_reposition=1.317, ppl=3378.82, wps=3606.2, ups=0.35, wpb=10265.8, bsz=128, num_updates=2100, lr=0.000210058, gnorm=2.47, clip=0, train_wall=284, wall=6026
2022-08-14 18:01:32 | INFO | train_inner | epoch 001:   2200 / 2244 loss=11.657, nll_loss=7.631, mask_ins=1.673, word_ins_ml=8.659, word_reposition=1.325, ppl=3228.73, wps=3589.8, ups=0.35, wpb=10243.3, bsz=128, num_updates=2200, lr=0.000220056, gnorm=2.655, clip=0, train_wall=285, wall=6311
2022-08-14 18:03:35 | INFO | train | epoch 001 | loss 13.868 | nll_loss 9.141 | mask_ins 2.175 | word_ins_ml 9.978 | word_reposition 1.715 | ppl 14948.9 | wps 3583.5 | ups 0.35 | wpb 10263.9 | bsz 127.9 | num_updates 2244 | lr 0.000224455 | gnorm 4.684 | clip 0.7 | train_wall 6418 | wall 6435
2022-08-14 18:04:55 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.769 | nll_loss 7.692 | mask_ins 1.723 | word_ins_ml 8.771 | word_reposition 1.275 | ppl 3490.54 | wps 12351.1 | wpb 1183.8 | bsz 16 | num_updates 2244
2022-08-14 18:05:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 1 @ 2244 updates, score 11.769) (writing took 7.954236578196287 seconds)
2022-08-14 18:07:43 | INFO | train_inner | epoch 002:     56 / 2244 loss=11.479, nll_loss=7.51, mask_ins=1.621, word_ins_ml=8.552, word_reposition=1.306, ppl=2854.65, wps=2735.7, ups=0.27, wpb=10152.6, bsz=126.8, num_updates=2300, lr=0.000230054, gnorm=2.714, clip=0, train_wall=282, wall=6683
2022-08-14 18:12:28 | INFO | train_inner | epoch 002:    156 / 2244 loss=11.433, nll_loss=7.439, mask_ins=1.653, word_ins_ml=8.49, word_reposition=1.29, ppl=2764.39, wps=3606.8, ups=0.35, wpb=10276.8, bsz=128, num_updates=2400, lr=0.000240052, gnorm=2.402, clip=0, train_wall=284, wall=6967
2022-08-14 18:17:13 | INFO | train_inner | epoch 002:    256 / 2244 loss=11.398, nll_loss=7.415, mask_ins=1.636, word_ins_ml=8.469, word_reposition=1.293, ppl=2698.29, wps=3607, ups=0.35, wpb=10289.7, bsz=128, num_updates=2500, lr=0.00025005, gnorm=3.201, clip=0, train_wall=285, wall=7253
2022-08-14 18:22:00 | INFO | train_inner | epoch 002:    356 / 2244 loss=11.36, nll_loss=7.378, mask_ins=1.639, word_ins_ml=8.435, word_reposition=1.286, ppl=2627.91, wps=3609, ups=0.35, wpb=10346.9, bsz=128, num_updates=2600, lr=0.000260048, gnorm=4.875, clip=2, train_wall=286, wall=7539
2022-08-14 18:26:45 | INFO | train_inner | epoch 002:    456 / 2244 loss=11.261, nll_loss=7.296, mask_ins=1.622, word_ins_ml=8.362, word_reposition=1.277, ppl=2454.42, wps=3592.1, ups=0.35, wpb=10235.3, bsz=128, num_updates=2700, lr=0.000270046, gnorm=3.667, clip=0, train_wall=284, wall=7824
2022-08-14 18:31:29 | INFO | train_inner | epoch 002:    556 / 2244 loss=11.193, nll_loss=7.205, mask_ins=1.618, word_ins_ml=8.282, word_reposition=1.292, ppl=2341.42, wps=3622.2, ups=0.35, wpb=10284.9, bsz=128, num_updates=2800, lr=0.000280044, gnorm=2.834, clip=0, train_wall=283, wall=8108
2022-08-14 18:36:13 | INFO | train_inner | epoch 002:    656 / 2244 loss=11.174, nll_loss=7.202, mask_ins=1.619, word_ins_ml=8.28, word_reposition=1.275, ppl=2309.96, wps=3603, ups=0.35, wpb=10249.7, bsz=128, num_updates=2900, lr=0.000290042, gnorm=3.678, clip=2, train_wall=284, wall=8393
2022-08-14 18:40:58 | INFO | train_inner | epoch 002:    756 / 2244 loss=11.167, nll_loss=7.187, mask_ins=1.615, word_ins_ml=8.266, word_reposition=1.286, ppl=2298.98, wps=3615.7, ups=0.35, wpb=10292, bsz=128, num_updates=3000, lr=0.00030004, gnorm=5.564, clip=2, train_wall=284, wall=8677
2022-08-14 18:45:42 | INFO | train_inner | epoch 002:    856 / 2244 loss=11.117, nll_loss=7.134, mask_ins=1.624, word_ins_ml=8.219, word_reposition=1.274, ppl=2221.31, wps=3601.9, ups=0.35, wpb=10245, bsz=128, num_updates=3100, lr=0.000310038, gnorm=3.327, clip=0, train_wall=284, wall=8962
2022-08-14 18:50:27 | INFO | train_inner | epoch 002:    956 / 2244 loss=11.071, nll_loss=7.095, mask_ins=1.612, word_ins_ml=8.185, word_reposition=1.274, ppl=2151.4, wps=3608.7, ups=0.35, wpb=10267, bsz=128, num_updates=3200, lr=0.000320036, gnorm=5.265, clip=3, train_wall=284, wall=9246
2022-08-14 18:55:13 | INFO | train_inner | epoch 002:   1056 / 2244 loss=10.988, nll_loss=7.032, mask_ins=1.595, word_ins_ml=8.13, word_reposition=1.263, ppl=2030.8, wps=3605.6, ups=0.35, wpb=10303.5, bsz=128, num_updates=3300, lr=0.000330034, gnorm=4.714, clip=1, train_wall=285, wall=9532
2022-08-14 18:59:59 | INFO | train_inner | epoch 002:   1156 / 2244 loss=11.114, nll_loss=7.124, mask_ins=1.627, word_ins_ml=8.208, word_reposition=1.279, ppl=2217, wps=3573.1, ups=0.35, wpb=10238.1, bsz=128, num_updates=3400, lr=0.000340032, gnorm=4.977, clip=2, train_wall=286, wall=9819
2022-08-14 19:04:45 | INFO | train_inner | epoch 002:   1256 / 2244 loss=11.48, nll_loss=7.607, mask_ins=1.604, word_ins_ml=8.618, word_reposition=1.258, ppl=2856.46, wps=3600.3, ups=0.35, wpb=10288.3, bsz=128, num_updates=3500, lr=0.00035003, gnorm=2.328, clip=0, train_wall=285, wall=10104
2022-08-14 19:09:29 | INFO | train_inner | epoch 002:   1356 / 2244 loss=11.444, nll_loss=7.574, mask_ins=1.599, word_ins_ml=8.589, word_reposition=1.257, ppl=2786.79, wps=3613.1, ups=0.35, wpb=10275.4, bsz=128, num_updates=3600, lr=0.000360028, gnorm=2.095, clip=0, train_wall=284, wall=10389
2022-08-14 19:14:14 | INFO | train_inner | epoch 002:   1456 / 2244 loss=11.56, nll_loss=7.664, mask_ins=1.627, word_ins_ml=8.667, word_reposition=1.266, ppl=3019.75, wps=3595.1, ups=0.35, wpb=10242.3, bsz=128, num_updates=3700, lr=0.000370026, gnorm=5.202, clip=2, train_wall=284, wall=10674
2022-08-14 19:19:00 | INFO | train_inner | epoch 002:   1556 / 2244 loss=11.442, nll_loss=7.565, mask_ins=1.611, word_ins_ml=8.58, word_reposition=1.251, ppl=2782.44, wps=3591.1, ups=0.35, wpb=10251.6, bsz=128, num_updates=3800, lr=0.000380024, gnorm=2.15, clip=0, train_wall=285, wall=10959
2022-08-14 19:23:45 | INFO | train_inner | epoch 002:   1656 / 2244 loss=11.445, nll_loss=7.549, mask_ins=1.622, word_ins_ml=8.565, word_reposition=1.259, ppl=2788.78, wps=3590.5, ups=0.35, wpb=10263.5, bsz=128, num_updates=3900, lr=0.000390022, gnorm=2.1, clip=0, train_wall=285, wall=11245
2022-08-14 19:28:32 | INFO | train_inner | epoch 002:   1756 / 2244 loss=11.468, nll_loss=7.585, mask_ins=1.622, word_ins_ml=8.595, word_reposition=1.25, ppl=2831.8, wps=3554.9, ups=0.35, wpb=10189.2, bsz=128, num_updates=4000, lr=0.00040002, gnorm=2.07, clip=0, train_wall=286, wall=11532
2022-08-14 19:33:19 | INFO | train_inner | epoch 002:   1856 / 2244 loss=11.445, nll_loss=7.584, mask_ins=1.614, word_ins_ml=8.593, word_reposition=1.238, ppl=2787.84, wps=3577.9, ups=0.35, wpb=10271.6, bsz=128, num_updates=4100, lr=0.000410018, gnorm=2.071, clip=0, train_wall=286, wall=11819
2022-08-14 19:38:07 | INFO | train_inner | epoch 002:   1956 / 2244 loss=11.379, nll_loss=7.521, mask_ins=1.596, word_ins_ml=8.538, word_reposition=1.245, ppl=2664.01, wps=3573.9, ups=0.35, wpb=10269, bsz=128, num_updates=4200, lr=0.000420016, gnorm=2.228, clip=0, train_wall=287, wall=12106
2022-08-14 19:42:54 | INFO | train_inner | epoch 002:   2056 / 2244 loss=11.42, nll_loss=7.57, mask_ins=1.608, word_ins_ml=8.58, word_reposition=1.232, ppl=2740.37, wps=3576.4, ups=0.35, wpb=10265, bsz=128, num_updates=4300, lr=0.000430014, gnorm=2.119, clip=0, train_wall=286, wall=12393
2022-08-14 19:47:40 | INFO | train_inner | epoch 002:   2156 / 2244 loss=11.377, nll_loss=7.551, mask_ins=1.579, word_ins_ml=8.563, word_reposition=1.235, ppl=2660.38, wps=3587.4, ups=0.35, wpb=10287.8, bsz=128, num_updates=4400, lr=0.000440012, gnorm=2.034, clip=0, train_wall=286, wall=12680
2022-08-14 19:51:50 | INFO | train | epoch 002 | loss 11.326 | nll_loss 7.402 | mask_ins 1.615 | word_ins_ml 8.445 | word_reposition 1.265 | ppl 2566.97 | wps 3546.3 | ups 0.35 | wpb 10263.9 | bsz 127.9 | num_updates 4488 | lr 0.00044881 | gnorm 3.225 | clip 0.6 | train_wall 6392 | wall 12929
2022-08-14 19:53:09 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.997 | nll_loss 7.865 | mask_ins 1.919 | word_ins_ml 8.861 | word_reposition 1.218 | ppl 4088.83 | wps 12510.8 | wpb 1183.8 | bsz 16 | num_updates 4488 | best_loss 11.769
2022-08-14 19:53:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 2 @ 4488 updates, score 11.997) (writing took 36.048039861023426 seconds)
2022-08-14 19:54:19 | INFO | train_inner | epoch 003:     12 / 2244 loss=11.344, nll_loss=7.51, mask_ins=1.59, word_ins_ml=8.526, word_reposition=1.228, ppl=2600.2, wps=2555.9, ups=0.25, wpb=10184, bsz=126.8, num_updates=4500, lr=0.00045001, gnorm=2.211, clip=0, train_wall=283, wall=13078
2022-08-14 19:59:05 | INFO | train_inner | epoch 003:    112 / 2244 loss=11.343, nll_loss=7.493, mask_ins=1.585, word_ins_ml=8.512, word_reposition=1.246, ppl=2597.67, wps=3596.8, ups=0.35, wpb=10302.6, bsz=128, num_updates=4600, lr=0.000460008, gnorm=3.856, clip=2, train_wall=286, wall=13365
2022-08-14 20:03:51 | INFO | train_inner | epoch 003:    212 / 2244 loss=11.366, nll_loss=7.523, mask_ins=1.582, word_ins_ml=8.538, word_reposition=1.246, ppl=2639.73, wps=3576.6, ups=0.35, wpb=10223.4, bsz=128, num_updates=4700, lr=0.000470006, gnorm=2.156, clip=0, train_wall=285, wall=13651
2022-08-14 20:08:36 | INFO | train_inner | epoch 003:    312 / 2244 loss=11.395, nll_loss=7.522, mask_ins=1.613, word_ins_ml=8.537, word_reposition=1.246, ppl=2692.98, wps=3604.7, ups=0.35, wpb=10271.7, bsz=128, num_updates=4800, lr=0.000480004, gnorm=3.01, clip=1, train_wall=284, wall=13936
2022-08-14 20:13:21 | INFO | train_inner | epoch 003:    412 / 2244 loss=11.337, nll_loss=7.506, mask_ins=1.584, word_ins_ml=8.522, word_reposition=1.231, ppl=2586.67, wps=3584.4, ups=0.35, wpb=10232.5, bsz=128, num_updates=4900, lr=0.000490002, gnorm=2.141, clip=0, train_wall=285, wall=14221
2022-08-14 20:18:07 | INFO | train_inner | epoch 003:    512 / 2244 loss=11.349, nll_loss=7.518, mask_ins=1.585, word_ins_ml=8.532, word_reposition=1.232, ppl=2608.73, wps=3609.1, ups=0.35, wpb=10291.5, bsz=128, num_updates=5000, lr=0.0005, gnorm=2.096, clip=0, train_wall=285, wall=14506
2022-08-14 20:22:52 | INFO | train_inner | epoch 003:    612 / 2244 loss=11.38, nll_loss=7.561, mask_ins=1.583, word_ins_ml=8.569, word_reposition=1.228, ppl=2664.74, wps=3578.6, ups=0.35, wpb=10222.9, bsz=128, num_updates=5100, lr=0.000495074, gnorm=2.072, clip=0, train_wall=285, wall=14792
2022-08-14 20:27:38 | INFO | train_inner | epoch 003:    712 / 2244 loss=11.356, nll_loss=7.514, mask_ins=1.587, word_ins_ml=8.529, word_reposition=1.24, ppl=2621.25, wps=3600.1, ups=0.35, wpb=10273.8, bsz=128, num_updates=5200, lr=0.00049029, gnorm=2.108, clip=0, train_wall=285, wall=15077
2022-08-14 20:32:23 | INFO | train_inner | epoch 003:    812 / 2244 loss=11.372, nll_loss=7.546, mask_ins=1.582, word_ins_ml=8.554, word_reposition=1.235, ppl=2649.91, wps=3605.1, ups=0.35, wpb=10288.4, bsz=128, num_updates=5300, lr=0.000485643, gnorm=2.036, clip=0, train_wall=285, wall=15363
2022-08-14 20:37:09 | INFO | train_inner | epoch 003:    912 / 2244 loss=11.391, nll_loss=7.532, mask_ins=1.606, word_ins_ml=8.543, word_reposition=1.242, ppl=2686.28, wps=3597.7, ups=0.35, wpb=10276.4, bsz=128, num_updates=5400, lr=0.000481125, gnorm=2.085, clip=0, train_wall=285, wall=15648
2022-08-14 20:41:53 | INFO | train_inner | epoch 003:   1012 / 2244 loss=11.379, nll_loss=7.544, mask_ins=1.602, word_ins_ml=8.553, word_reposition=1.224, ppl=2663.59, wps=3584.8, ups=0.35, wpb=10206.5, bsz=128, num_updates=5500, lr=0.000476731, gnorm=6.052, clip=1, train_wall=284, wall=15933
2022-08-14 20:46:39 | INFO | train_inner | epoch 003:   1112 / 2244 loss=11.36, nll_loss=7.521, mask_ins=1.594, word_ins_ml=8.533, word_reposition=1.233, ppl=2628.41, wps=3602, ups=0.35, wpb=10283.2, bsz=128, num_updates=5600, lr=0.000472456, gnorm=2.54, clip=1, train_wall=285, wall=16219
2022-08-14 20:51:23 | INFO | train_inner | epoch 003:   1212 / 2244 loss=11.336, nll_loss=7.503, mask_ins=1.6, word_ins_ml=8.517, word_reposition=1.219, ppl=2585.95, wps=3610.1, ups=0.35, wpb=10255.7, bsz=128, num_updates=5700, lr=0.000468293, gnorm=1.991, clip=0, train_wall=283, wall=16503
2022-08-14 20:56:07 | INFO | train_inner | epoch 003:   1312 / 2244 loss=11.351, nll_loss=7.524, mask_ins=1.603, word_ins_ml=8.534, word_reposition=1.214, ppl=2611.57, wps=3599.5, ups=0.35, wpb=10225.5, bsz=128, num_updates=5800, lr=0.000464238, gnorm=2.286, clip=0, train_wall=283, wall=16787
2022-08-14 21:00:53 | INFO | train_inner | epoch 003:   1412 / 2244 loss=11.227, nll_loss=7.429, mask_ins=1.563, word_ins_ml=8.451, word_reposition=1.214, ppl=2397.75, wps=3596.9, ups=0.35, wpb=10269.2, bsz=128, num_updates=5900, lr=0.000460287, gnorm=2.469, clip=0, train_wall=285, wall=17072
2022-08-14 21:05:38 | INFO | train_inner | epoch 003:   1512 / 2244 loss=11.288, nll_loss=7.484, mask_ins=1.577, word_ins_ml=8.5, word_reposition=1.211, ppl=2501.01, wps=3622.5, ups=0.35, wpb=10337.8, bsz=128, num_updates=6000, lr=0.000456435, gnorm=1.998, clip=0, train_wall=285, wall=17358
2022-08-14 21:10:23 | INFO | train_inner | epoch 003:   1612 / 2244 loss=11.237, nll_loss=7.424, mask_ins=1.584, word_ins_ml=8.446, word_reposition=1.207, ppl=2414.29, wps=3596.2, ups=0.35, wpb=10236, bsz=128, num_updates=6100, lr=0.000452679, gnorm=1.942, clip=0, train_wall=284, wall=17642
2022-08-14 21:15:08 | INFO | train_inner | epoch 003:   1712 / 2244 loss=11.261, nll_loss=7.435, mask_ins=1.574, word_ins_ml=8.456, word_reposition=1.232, ppl=2454.78, wps=3610.4, ups=0.35, wpb=10309.1, bsz=128, num_updates=6200, lr=0.000449013, gnorm=3.263, clip=1, train_wall=285, wall=17928
2022-08-14 21:19:54 | INFO | train_inner | epoch 003:   1812 / 2244 loss=11.268, nll_loss=7.455, mask_ins=1.581, word_ins_ml=8.473, word_reposition=1.214, ppl=2465.38, wps=3606.9, ups=0.35, wpb=10302.6, bsz=128, num_updates=6300, lr=0.000445435, gnorm=2.627, clip=1, train_wall=285, wall=18213
2022-08-14 21:24:39 | INFO | train_inner | epoch 003:   1912 / 2244 loss=11.216, nll_loss=7.398, mask_ins=1.57, word_ins_ml=8.423, word_reposition=1.224, ppl=2379.46, wps=3601.4, ups=0.35, wpb=10273.9, bsz=128, num_updates=6400, lr=0.000441942, gnorm=2.093, clip=0, train_wall=285, wall=18499
2022-08-14 21:29:25 | INFO | train_inner | epoch 003:   2012 / 2244 loss=11.259, nll_loss=7.444, mask_ins=1.59, word_ins_ml=8.463, word_reposition=1.207, ppl=2450.99, wps=3591.4, ups=0.35, wpb=10275, bsz=128, num_updates=6500, lr=0.000438529, gnorm=23.479, clip=1, train_wall=285, wall=18785
2022-08-14 21:34:11 | INFO | train_inner | epoch 003:   2112 / 2244 loss=11.202, nll_loss=7.401, mask_ins=1.576, word_ins_ml=8.425, word_reposition=1.201, ppl=2356.06, wps=3603.3, ups=0.35, wpb=10313.6, bsz=128, num_updates=6600, lr=0.000435194, gnorm=1.943, clip=0, train_wall=286, wall=19071
2022-08-14 21:38:57 | INFO | train_inner | epoch 003:   2212 / 2244 loss=11.183, nll_loss=7.41, mask_ins=1.555, word_ins_ml=8.433, word_reposition=1.195, ppl=2324.98, wps=3577.6, ups=0.35, wpb=10215.6, bsz=128, num_updates=6700, lr=0.000431934, gnorm=2.189, clip=0, train_wall=285, wall=19357
2022-08-14 21:40:26 | INFO | train | epoch 003 | loss 11.311 | nll_loss 7.486 | mask_ins 1.585 | word_ins_ml 8.502 | word_reposition 1.224 | ppl 2540.75 | wps 3534.3 | ups 0.34 | wpb 10263.9 | bsz 127.9 | num_updates 6732 | lr 0.000430906 | gnorm 3.452 | clip 0.4 | train_wall 6387 | wall 19446
2022-08-14 21:41:45 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.507 | nll_loss 7.609 | mask_ins 1.677 | word_ins_ml 8.642 | word_reposition 1.187 | ppl 2909.84 | wps 12561.3 | wpb 1183.8 | bsz 16 | num_updates 6732 | best_loss 11.507
2022-08-14 21:42:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 3 @ 6732 updates, score 11.507) (writing took 41.71146576292813 seconds)
2022-08-14 21:45:40 | INFO | train_inner | epoch 004:     68 / 2244 loss=11.249, nll_loss=7.445, mask_ins=1.568, word_ins_ml=8.463, word_reposition=1.219, ppl=2434.63, wps=2512, ups=0.25, wpb=10132.2, bsz=126.8, num_updates=6800, lr=0.000428746, gnorm=2.008, clip=0, train_wall=282, wall=19760
2022-08-14 21:50:26 | INFO | train_inner | epoch 004:    168 / 2244 loss=11.086, nll_loss=7.325, mask_ins=1.542, word_ins_ml=8.359, word_reposition=1.185, ppl=2174.08, wps=3581.2, ups=0.35, wpb=10237.8, bsz=128, num_updates=6900, lr=0.000425628, gnorm=2.003, clip=0, train_wall=285, wall=20046
2022-08-14 21:55:13 | INFO | train_inner | epoch 004:    268 / 2244 loss=11.214, nll_loss=7.423, mask_ins=1.568, word_ins_ml=8.444, word_reposition=1.202, ppl=2375.92, wps=3579.6, ups=0.35, wpb=10258.8, bsz=128, num_updates=7000, lr=0.000422577, gnorm=2.118, clip=0, train_wall=286, wall=20332
2022-08-14 21:59:58 | INFO | train_inner | epoch 004:    368 / 2244 loss=11.103, nll_loss=7.334, mask_ins=1.548, word_ins_ml=8.366, word_reposition=1.189, ppl=2199.6, wps=3599, ups=0.35, wpb=10261.6, bsz=128, num_updates=7100, lr=0.000419591, gnorm=2.03, clip=0, train_wall=284, wall=20617
2022-08-14 22:04:43 | INFO | train_inner | epoch 004:    468 / 2244 loss=11.065, nll_loss=7.295, mask_ins=1.541, word_ins_ml=8.332, word_reposition=1.192, ppl=2142.89, wps=3602.5, ups=0.35, wpb=10267.3, bsz=128, num_updates=7200, lr=0.000416667, gnorm=1.891, clip=0, train_wall=284, wall=20902
2022-08-14 22:09:29 | INFO | train_inner | epoch 004:    568 / 2244 loss=11.116, nll_loss=7.323, mask_ins=1.559, word_ins_ml=8.356, word_reposition=1.201, ppl=2219.15, wps=3596.8, ups=0.35, wpb=10289.4, bsz=128, num_updates=7300, lr=0.000413803, gnorm=1.907, clip=0, train_wall=285, wall=21189
2022-08-14 22:14:14 | INFO | train_inner | epoch 004:    668 / 2244 loss=11.09, nll_loss=7.301, mask_ins=1.551, word_ins_ml=8.337, word_reposition=1.202, ppl=2179.5, wps=3618.1, ups=0.35, wpb=10315.6, bsz=128, num_updates=7400, lr=0.000410997, gnorm=1.883, clip=0, train_wall=284, wall=21474
2022-08-14 22:18:58 | INFO | train_inner | epoch 004:    768 / 2244 loss=11.16, nll_loss=7.351, mask_ins=1.572, word_ins_ml=8.38, word_reposition=1.209, ppl=2288.44, wps=3618.4, ups=0.35, wpb=10281.1, bsz=128, num_updates=7500, lr=0.000408248, gnorm=1.917, clip=0, train_wall=283, wall=21758
2022-08-14 22:23:42 | INFO | train_inner | epoch 004:    868 / 2244 loss=11.096, nll_loss=7.318, mask_ins=1.555, word_ins_ml=8.352, word_reposition=1.189, ppl=2188.39, wps=3612.3, ups=0.35, wpb=10265.2, bsz=128, num_updates=7600, lr=0.000405554, gnorm=1.898, clip=0, train_wall=284, wall=22042
2022-08-14 22:28:27 | INFO | train_inner | epoch 004:    968 / 2244 loss=11.116, nll_loss=7.34, mask_ins=1.56, word_ins_ml=8.371, word_reposition=1.186, ppl=2219.28, wps=3580.4, ups=0.35, wpb=10203.5, bsz=128, num_updates=7700, lr=0.000402911, gnorm=1.914, clip=0, train_wall=284, wall=22327
2022-08-14 22:33:11 | INFO | train_inner | epoch 004:   1068 / 2244 loss=11.057, nll_loss=7.289, mask_ins=1.543, word_ins_ml=8.326, word_reposition=1.188, ppl=2130.22, wps=3628.1, ups=0.35, wpb=10284.5, bsz=128, num_updates=7800, lr=0.00040032, gnorm=1.86, clip=0, train_wall=283, wall=22610
2022-08-14 22:37:55 | INFO | train_inner | epoch 004:   1168 / 2244 loss=11.103, nll_loss=7.323, mask_ins=1.551, word_ins_ml=8.355, word_reposition=1.196, ppl=2199.77, wps=3610.8, ups=0.35, wpb=10268.7, bsz=128, num_updates=7900, lr=0.000397779, gnorm=1.898, clip=0, train_wall=284, wall=22895
2022-08-14 22:42:40 | INFO | train_inner | epoch 004:   1268 / 2244 loss=11.129, nll_loss=7.328, mask_ins=1.571, word_ins_ml=8.359, word_reposition=1.199, ppl=2240.05, wps=3606.9, ups=0.35, wpb=10272, bsz=128, num_updates=8000, lr=0.000395285, gnorm=1.884, clip=0, train_wall=284, wall=23180
2022-08-14 22:47:25 | INFO | train_inner | epoch 004:   1368 / 2244 loss=11.066, nll_loss=7.289, mask_ins=1.542, word_ins_ml=8.325, word_reposition=1.2, ppl=2144.56, wps=3610.2, ups=0.35, wpb=10280.9, bsz=128, num_updates=8100, lr=0.000392837, gnorm=1.894, clip=0, train_wall=284, wall=23464
2022-08-14 22:52:10 | INFO | train_inner | epoch 004:   1468 / 2244 loss=11.086, nll_loss=7.321, mask_ins=1.555, word_ins_ml=8.353, word_reposition=1.178, ppl=2174.1, wps=3598.9, ups=0.35, wpb=10260.5, bsz=128, num_updates=8200, lr=0.000390434, gnorm=1.857, clip=0, train_wall=284, wall=23749
2022-08-14 22:56:54 | INFO | train_inner | epoch 004:   1568 / 2244 loss=11.042, nll_loss=7.304, mask_ins=1.535, word_ins_ml=8.338, word_reposition=1.169, ppl=2108.87, wps=3618.6, ups=0.35, wpb=10267, bsz=128, num_updates=8300, lr=0.000388075, gnorm=1.915, clip=0, train_wall=283, wall=24033
2022-08-14 23:01:38 | INFO | train_inner | epoch 004:   1668 / 2244 loss=11.062, nll_loss=7.279, mask_ins=1.56, word_ins_ml=8.316, word_reposition=1.186, ppl=2138.08, wps=3605.9, ups=0.35, wpb=10258.6, bsz=128, num_updates=8400, lr=0.000385758, gnorm=1.822, clip=0, train_wall=284, wall=24318
2022-08-14 23:06:22 | INFO | train_inner | epoch 004:   1768 / 2244 loss=11.055, nll_loss=7.296, mask_ins=1.544, word_ins_ml=8.331, word_reposition=1.179, ppl=2127.25, wps=3593.1, ups=0.35, wpb=10219.2, bsz=128, num_updates=8500, lr=0.000383482, gnorm=1.898, clip=0, train_wall=284, wall=24602
2022-08-14 23:11:05 | INFO | train_inner | epoch 004:   1868 / 2244 loss=11.115, nll_loss=7.344, mask_ins=1.561, word_ins_ml=8.373, word_reposition=1.181, ppl=2217.51, wps=3623.2, ups=0.35, wpb=10249.9, bsz=128, num_updates=8600, lr=0.000381246, gnorm=1.87, clip=0, train_wall=282, wall=24885
2022-08-14 23:15:49 | INFO | train_inner | epoch 004:   1968 / 2244 loss=11.05, nll_loss=7.283, mask_ins=1.544, word_ins_ml=8.319, word_reposition=1.187, ppl=2120.76, wps=3614.8, ups=0.35, wpb=10239.7, bsz=128, num_updates=8700, lr=0.000379049, gnorm=1.86, clip=0, train_wall=283, wall=25168
2022-08-14 23:20:32 | INFO | train_inner | epoch 004:   2068 / 2244 loss=11.049, nll_loss=7.277, mask_ins=1.555, word_ins_ml=8.314, word_reposition=1.18, ppl=2118.3, wps=3637.5, ups=0.35, wpb=10294.4, bsz=128, num_updates=8800, lr=0.000376889, gnorm=1.847, clip=0, train_wall=282, wall=25451
2022-08-14 23:25:17 | INFO | train_inner | epoch 004:   2168 / 2244 loss=11.058, nll_loss=7.288, mask_ins=1.553, word_ins_ml=8.324, word_reposition=1.181, ppl=2131.73, wps=3629.4, ups=0.35, wpb=10340.3, bsz=128, num_updates=8900, lr=0.000374766, gnorm=1.843, clip=0, train_wall=284, wall=25736
2022-08-14 23:28:51 | INFO | train | epoch 004 | loss 11.093 | nll_loss 7.316 | mask_ins 1.554 | word_ins_ml 8.349 | word_reposition 1.19 | ppl 2184.72 | wps 3541.2 | ups 0.35 | wpb 10263.9 | bsz 127.9 | num_updates 8976 | lr 0.000373176 | gnorm 1.907 | clip 0 | train_wall 6369 | wall 25950
2022-08-14 23:30:09 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.321 | nll_loss 7.477 | mask_ins 1.653 | word_ins_ml 8.52 | word_reposition 1.148 | ppl 2559.18 | wps 12612.3 | wpb 1183.8 | bsz 16 | num_updates 8976 | best_loss 11.321
2022-08-14 23:30:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 4 @ 8976 updates, score 11.321) (writing took 33.78317650407553 seconds)
2022-08-14 23:31:50 | INFO | train_inner | epoch 005:     24 / 2244 loss=11.032, nll_loss=7.247, mask_ins=1.558, word_ins_ml=8.288, word_reposition=1.187, ppl=2094.32, wps=2597.1, ups=0.25, wpb=10223.7, bsz=126.8, num_updates=9000, lr=0.000372678, gnorm=1.94, clip=0, train_wall=281, wall=26130
2022-08-14 23:36:33 | INFO | train_inner | epoch 005:    124 / 2244 loss=10.937, nll_loss=7.186, mask_ins=1.526, word_ins_ml=8.234, word_reposition=1.177, ppl=1961.08, wps=3635.5, ups=0.35, wpb=10285.9, bsz=128, num_updates=9100, lr=0.000370625, gnorm=1.851, clip=0, train_wall=282, wall=26413
2022-08-14 23:41:16 | INFO | train_inner | epoch 005:    224 / 2244 loss=10.989, nll_loss=7.205, mask_ins=1.558, word_ins_ml=8.252, word_reposition=1.179, ppl=2032.19, wps=3622.9, ups=0.35, wpb=10266.1, bsz=128, num_updates=9200, lr=0.000368605, gnorm=1.91, clip=0, train_wall=283, wall=26696
2022-08-14 23:46:00 | INFO | train_inner | epoch 005:    324 / 2244 loss=10.997, nll_loss=7.233, mask_ins=1.545, word_ins_ml=8.276, word_reposition=1.176, ppl=2044.13, wps=3610.2, ups=0.35, wpb=10242.4, bsz=128, num_updates=9300, lr=0.000366618, gnorm=1.862, clip=0, train_wall=283, wall=26980
2022-08-14 23:50:43 | INFO | train_inner | epoch 005:    424 / 2244 loss=10.942, nll_loss=7.19, mask_ins=1.53, word_ins_ml=8.237, word_reposition=1.175, ppl=1967.71, wps=3608.5, ups=0.35, wpb=10205.5, bsz=128, num_updates=9400, lr=0.000364662, gnorm=1.86, clip=0, train_wall=282, wall=27263
2022-08-14 23:55:27 | INFO | train_inner | epoch 005:    524 / 2244 loss=10.972, nll_loss=7.199, mask_ins=1.542, word_ins_ml=8.246, word_reposition=1.184, ppl=2008.43, wps=3595.7, ups=0.35, wpb=10227.4, bsz=128, num_updates=9500, lr=0.000362738, gnorm=1.868, clip=0, train_wall=284, wall=27547
2022-08-15 00:00:12 | INFO | train_inner | epoch 005:    624 / 2244 loss=10.96, nll_loss=7.212, mask_ins=1.535, word_ins_ml=8.256, word_reposition=1.169, ppl=1991.99, wps=3586, ups=0.35, wpb=10212.4, bsz=128, num_updates=9600, lr=0.000360844, gnorm=1.812, clip=0, train_wall=284, wall=27832
2022-08-15 00:04:56 | INFO | train_inner | epoch 005:    724 / 2244 loss=10.929, nll_loss=7.163, mask_ins=1.543, word_ins_ml=8.215, word_reposition=1.171, ppl=1949.85, wps=3624.7, ups=0.35, wpb=10272.4, bsz=128, num_updates=9700, lr=0.000358979, gnorm=1.838, clip=0, train_wall=283, wall=28115
2022-08-15 00:09:39 | INFO | train_inner | epoch 005:    824 / 2244 loss=10.957, nll_loss=7.195, mask_ins=1.549, word_ins_ml=8.242, word_reposition=1.167, ppl=1988.37, wps=3651.8, ups=0.35, wpb=10331.2, bsz=128, num_updates=9800, lr=0.000357143, gnorm=1.851, clip=0, train_wall=282, wall=28398
2022-08-15 00:14:23 | INFO | train_inner | epoch 005:    924 / 2244 loss=10.938, nll_loss=7.18, mask_ins=1.545, word_ins_ml=8.228, word_reposition=1.165, ppl=1962.44, wps=3637.2, ups=0.35, wpb=10331.9, bsz=128, num_updates=9900, lr=0.000355335, gnorm=1.853, clip=0, train_wall=283, wall=28682
2022-08-15 00:19:08 | INFO | train_inner | epoch 005:   1024 / 2244 loss=10.948, nll_loss=7.165, mask_ins=1.547, word_ins_ml=8.216, word_reposition=1.186, ppl=1975.95, wps=3616.4, ups=0.35, wpb=10308.7, bsz=128, num_updates=10000, lr=0.000353553, gnorm=1.821, clip=0, train_wall=284, wall=28967
2022-08-15 00:23:51 | INFO | train_inner | epoch 005:   1124 / 2244 loss=10.957, nll_loss=7.192, mask_ins=1.544, word_ins_ml=8.239, word_reposition=1.174, ppl=1987.87, wps=3614.2, ups=0.35, wpb=10253.7, bsz=128, num_updates=10100, lr=0.000351799, gnorm=1.856, clip=0, train_wall=283, wall=29251
2022-08-15 00:28:35 | INFO | train_inner | epoch 005:   1224 / 2244 loss=10.944, nll_loss=7.212, mask_ins=1.525, word_ins_ml=8.256, word_reposition=1.162, ppl=1969.89, wps=3627.4, ups=0.35, wpb=10285.2, bsz=128, num_updates=10200, lr=0.00035007, gnorm=1.904, clip=0, train_wall=283, wall=29535
2022-08-15 00:33:19 | INFO | train_inner | epoch 005:   1324 / 2244 loss=10.968, nll_loss=7.219, mask_ins=1.547, word_ins_ml=8.262, word_reposition=1.159, ppl=2003.16, wps=3614.8, ups=0.35, wpb=10257.7, bsz=128, num_updates=10300, lr=0.000348367, gnorm=1.807, clip=0, train_wall=283, wall=29818
2022-08-15 00:38:03 | INFO | train_inner | epoch 005:   1424 / 2244 loss=10.944, nll_loss=7.188, mask_ins=1.537, word_ins_ml=8.236, word_reposition=1.171, ppl=1970.21, wps=3621.3, ups=0.35, wpb=10298.7, bsz=128, num_updates=10400, lr=0.000346688, gnorm=1.75, clip=0, train_wall=284, wall=30103
2022-08-15 00:42:47 | INFO | train_inner | epoch 005:   1524 / 2244 loss=10.97, nll_loss=7.231, mask_ins=1.538, word_ins_ml=8.272, word_reposition=1.16, ppl=2006.15, wps=3624.8, ups=0.35, wpb=10290.7, bsz=128, num_updates=10500, lr=0.000345033, gnorm=1.788, clip=0, train_wall=283, wall=30387
2022-08-15 00:47:29 | INFO | train_inner | epoch 005:   1624 / 2244 loss=10.883, nll_loss=7.156, mask_ins=1.524, word_ins_ml=8.207, word_reposition=1.152, ppl=1888.15, wps=3617, ups=0.35, wpb=10218.1, bsz=128, num_updates=10600, lr=0.000343401, gnorm=1.8, clip=0, train_wall=282, wall=30669
2022-08-15 00:52:13 | INFO | train_inner | epoch 005:   1724 / 2244 loss=10.943, nll_loss=7.164, mask_ins=1.548, word_ins_ml=8.214, word_reposition=1.181, ppl=1968.61, wps=3636.3, ups=0.35, wpb=10307.9, bsz=128, num_updates=10700, lr=0.000341793, gnorm=2.515, clip=1, train_wall=283, wall=30953
2022-08-15 00:56:56 | INFO | train_inner | epoch 005:   1824 / 2244 loss=10.997, nll_loss=7.225, mask_ins=1.55, word_ins_ml=8.268, word_reposition=1.18, ppl=2044.25, wps=3617.6, ups=0.35, wpb=10238, bsz=128, num_updates=10800, lr=0.000340207, gnorm=2.17, clip=1, train_wall=282, wall=31236
2022-08-15 01:01:40 | INFO | train_inner | epoch 005:   1924 / 2244 loss=10.873, nll_loss=7.137, mask_ins=1.528, word_ins_ml=8.19, word_reposition=1.155, ppl=1875.83, wps=3608.5, ups=0.35, wpb=10246.2, bsz=128, num_updates=10900, lr=0.000338643, gnorm=1.809, clip=0, train_wall=283, wall=31520
2022-08-15 01:06:24 | INFO | train_inner | epoch 005:   2024 / 2244 loss=10.891, nll_loss=7.16, mask_ins=1.522, word_ins_ml=8.21, word_reposition=1.158, ppl=1898.37, wps=3635.9, ups=0.35, wpb=10338.6, bsz=128, num_updates=11000, lr=0.0003371, gnorm=1.764, clip=0, train_wall=284, wall=31804
2022-08-15 01:11:07 | INFO | train_inner | epoch 005:   2124 / 2244 loss=10.922, nll_loss=7.172, mask_ins=1.546, word_ins_ml=8.22, word_reposition=1.156, ppl=1939.75, wps=3599.5, ups=0.35, wpb=10193.7, bsz=128, num_updates=11100, lr=0.000335578, gnorm=1.777, clip=0, train_wall=283, wall=32087
2022-08-15 01:15:52 | INFO | train_inner | epoch 005:   2224 / 2244 loss=10.901, nll_loss=7.151, mask_ins=1.538, word_ins_ml=8.202, word_reposition=1.161, ppl=1912.01, wps=3621.6, ups=0.35, wpb=10288.3, bsz=128, num_updates=11200, lr=0.000334077, gnorm=1.778, clip=0, train_wall=283, wall=32371
2022-08-15 01:16:46 | INFO | train | epoch 005 | loss 10.945 | nll_loss 7.189 | mask_ins 1.54 | word_ins_ml 8.236 | word_reposition 1.169 | ppl 1971.44 | wps 3556.5 | ups 0.35 | wpb 10263.9 | bsz 127.9 | num_updates 11220 | lr 0.000333779 | gnorm 1.876 | clip 0.1 | train_wall 6350 | wall 32426
2022-08-15 01:18:05 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.209 | nll_loss 7.347 | mask_ins 1.635 | word_ins_ml 8.409 | word_reposition 1.164 | ppl 2366.66 | wps 12634.5 | wpb 1183.8 | bsz 16 | num_updates 11220 | best_loss 11.209
2022-08-15 01:18:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 5 @ 11220 updates, score 11.209) (writing took 30.533264681696892 seconds)
2022-08-15 01:22:21 | INFO | train_inner | epoch 006:     80 / 2244 loss=10.869, nll_loss=7.157, mask_ins=1.502, word_ins_ml=8.208, word_reposition=1.159, ppl=1869.82, wps=2617.8, ups=0.26, wpb=10191.4, bsz=126.8, num_updates=11300, lr=0.000332595, gnorm=1.838, clip=0, train_wall=280, wall=32760
2022-08-15 01:27:03 | INFO | train_inner | epoch 006:    180 / 2244 loss=10.83, nll_loss=7.092, mask_ins=1.522, word_ins_ml=8.151, word_reposition=1.157, ppl=1820.26, wps=3631.2, ups=0.35, wpb=10249.6, bsz=128, num_updates=11400, lr=0.000331133, gnorm=1.786, clip=0, train_wall=282, wall=33043
2022-08-15 01:31:47 | INFO | train_inner | epoch 006:    280 / 2244 loss=10.819, nll_loss=7.096, mask_ins=1.519, word_ins_ml=8.154, word_reposition=1.146, ppl=1806.65, wps=3610.4, ups=0.35, wpb=10255.2, bsz=128, num_updates=11500, lr=0.00032969, gnorm=1.786, clip=0, train_wall=283, wall=33327
2022-08-15 01:36:30 | INFO | train_inner | epoch 006:    380 / 2244 loss=10.83, nll_loss=7.104, mask_ins=1.522, word_ins_ml=8.161, word_reposition=1.147, ppl=1820.74, wps=3617.6, ups=0.35, wpb=10239.5, bsz=128, num_updates=11600, lr=0.000328266, gnorm=1.774, clip=0, train_wall=282, wall=33610
2022-08-15 01:41:14 | INFO | train_inner | epoch 006:    480 / 2244 loss=10.874, nll_loss=7.136, mask_ins=1.539, word_ins_ml=8.189, word_reposition=1.146, ppl=1876.34, wps=3610.2, ups=0.35, wpb=10231.3, bsz=128, num_updates=11700, lr=0.00032686, gnorm=1.742, clip=0, train_wall=283, wall=33893
2022-08-15 01:45:57 | INFO | train_inner | epoch 006:    580 / 2244 loss=10.81, nll_loss=7.093, mask_ins=1.512, word_ins_ml=8.152, word_reposition=1.147, ppl=1795.87, wps=3622.8, ups=0.35, wpb=10273.5, bsz=128, num_updates=11800, lr=0.000325472, gnorm=1.788, clip=0, train_wall=283, wall=34177
2022-08-15 01:50:40 | INFO | train_inner | epoch 006:    680 / 2244 loss=10.761, nll_loss=7.046, mask_ins=1.504, word_ins_ml=8.11, word_reposition=1.147, ppl=1735.63, wps=3628.7, ups=0.35, wpb=10262.9, bsz=128, num_updates=11900, lr=0.000324102, gnorm=1.768, clip=0, train_wall=282, wall=34460
2022-08-15 01:55:24 | INFO | train_inner | epoch 006:    780 / 2244 loss=10.828, nll_loss=7.083, mask_ins=1.523, word_ins_ml=8.143, word_reposition=1.163, ppl=1818.2, wps=3618.7, ups=0.35, wpb=10269.1, bsz=128, num_updates=12000, lr=0.000322749, gnorm=1.754, clip=0, train_wall=283, wall=34743
2022-08-15 02:00:07 | INFO | train_inner | epoch 006:    880 / 2244 loss=10.819, nll_loss=7.115, mask_ins=1.504, word_ins_ml=8.171, word_reposition=1.144, ppl=1806.07, wps=3627, ups=0.35, wpb=10262, bsz=128, num_updates=12100, lr=0.000321412, gnorm=1.766, clip=0, train_wall=282, wall=35026
2022-08-15 02:04:51 | INFO | train_inner | epoch 006:    980 / 2244 loss=10.815, nll_loss=7.089, mask_ins=1.515, word_ins_ml=8.148, word_reposition=1.152, ppl=1801.36, wps=3640.9, ups=0.35, wpb=10332.6, bsz=128, num_updates=12200, lr=0.000320092, gnorm=1.757, clip=0, train_wall=283, wall=35310
2022-08-15 02:09:35 | INFO | train_inner | epoch 006:   1080 / 2244 loss=10.819, nll_loss=7.102, mask_ins=1.526, word_ins_ml=8.16, word_reposition=1.133, ppl=1806.59, wps=3591.4, ups=0.35, wpb=10205.9, bsz=128, num_updates=12300, lr=0.000318788, gnorm=1.744, clip=0, train_wall=284, wall=35594
2022-08-15 02:14:19 | INFO | train_inner | epoch 006:   1180 / 2244 loss=10.847, nll_loss=7.088, mask_ins=1.531, word_ins_ml=8.147, word_reposition=1.17, ppl=1841.51, wps=3632.9, ups=0.35, wpb=10331.5, bsz=128, num_updates=12400, lr=0.0003175, gnorm=2.061, clip=1, train_wall=284, wall=35879
2022-08-15 02:19:04 | INFO | train_inner | epoch 006:   1280 / 2244 loss=10.751, nll_loss=7.056, mask_ins=1.492, word_ins_ml=8.119, word_reposition=1.141, ppl=1723.88, wps=3596.8, ups=0.35, wpb=10238.8, bsz=128, num_updates=12500, lr=0.000316228, gnorm=1.765, clip=0, train_wall=284, wall=36163
2022-08-15 02:23:48 | INFO | train_inner | epoch 006:   1380 / 2244 loss=10.767, nll_loss=7.053, mask_ins=1.506, word_ins_ml=8.116, word_reposition=1.145, ppl=1742.74, wps=3612.3, ups=0.35, wpb=10254.4, bsz=128, num_updates=12600, lr=0.00031497, gnorm=1.73, clip=0, train_wall=283, wall=36447
2022-08-15 02:28:33 | INFO | train_inner | epoch 006:   1480 / 2244 loss=10.797, nll_loss=7.079, mask_ins=1.511, word_ins_ml=8.138, word_reposition=1.147, ppl=1779.02, wps=3642.3, ups=0.35, wpb=10378.6, bsz=128, num_updates=12700, lr=0.000313728, gnorm=1.783, clip=0, train_wall=284, wall=36732
2022-08-15 02:33:17 | INFO | train_inner | epoch 006:   1580 / 2244 loss=10.771, nll_loss=7.062, mask_ins=1.501, word_ins_ml=8.124, word_reposition=1.147, ppl=1747.97, wps=3603, ups=0.35, wpb=10234.5, bsz=128, num_updates=12800, lr=0.0003125, gnorm=1.761, clip=0, train_wall=283, wall=37016
2022-08-15 02:38:03 | INFO | train_inner | epoch 006:   1680 / 2244 loss=10.788, nll_loss=7.066, mask_ins=1.525, word_ins_ml=8.128, word_reposition=1.135, ppl=1767.68, wps=3581.4, ups=0.35, wpb=10240, bsz=128, num_updates=12900, lr=0.000311286, gnorm=1.78, clip=0, train_wall=285, wall=37302
2022-08-15 02:42:49 | INFO | train_inner | epoch 006:   1780 / 2244 loss=10.796, nll_loss=7.101, mask_ins=1.51, word_ins_ml=8.158, word_reposition=1.128, ppl=1777.69, wps=3565.6, ups=0.35, wpb=10208.8, bsz=128, num_updates=13000, lr=0.000310087, gnorm=1.775, clip=0, train_wall=286, wall=37588
2022-08-15 02:47:35 | INFO | train_inner | epoch 006:   1880 / 2244 loss=10.728, nll_loss=7.043, mask_ins=1.487, word_ins_ml=8.107, word_reposition=1.135, ppl=1696.42, wps=3602.2, ups=0.35, wpb=10318.5, bsz=128, num_updates=13100, lr=0.000308901, gnorm=1.802, clip=0, train_wall=286, wall=37875
2022-08-15 02:52:20 | INFO | train_inner | epoch 006:   1980 / 2244 loss=10.799, nll_loss=7.074, mask_ins=1.521, word_ins_ml=8.134, word_reposition=1.144, ppl=1781.11, wps=3608.5, ups=0.35, wpb=10288.5, bsz=128, num_updates=13200, lr=0.000307729, gnorm=1.731, clip=0, train_wall=284, wall=38160
2022-08-15 02:57:06 | INFO | train_inner | epoch 006:   2080 / 2244 loss=10.753, nll_loss=7.036, mask_ins=1.511, word_ins_ml=8.101, word_reposition=1.141, ppl=1726.07, wps=3594.4, ups=0.35, wpb=10260.3, bsz=128, num_updates=13300, lr=0.00030657, gnorm=1.717, clip=0, train_wall=285, wall=38446
2022-08-15 03:01:52 | INFO | train_inner | epoch 006:   2180 / 2244 loss=10.792, nll_loss=7.07, mask_ins=1.519, word_ins_ml=8.13, word_reposition=1.144, ppl=1773.36, wps=3592.1, ups=0.35, wpb=10263.5, bsz=128, num_updates=13400, lr=0.000305424, gnorm=1.736, clip=0, train_wall=285, wall=38731
2022-08-15 03:04:55 | INFO | train | epoch 006 | loss 10.801 | nll_loss 7.082 | mask_ins 1.513 | word_ins_ml 8.142 | word_reposition 1.145 | ppl 1783.92 | wps 3549.5 | ups 0.35 | wpb 10263.9 | bsz 127.9 | num_updates 13464 | lr 0.000304697 | gnorm 1.779 | clip 0 | train_wall 6366 | wall 38915
2022-08-15 03:06:15 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.08 | nll_loss 7.296 | mask_ins 1.63 | word_ins_ml 8.356 | word_reposition 1.094 | ppl 2164.97 | wps 12438.2 | wpb 1183.8 | bsz 16 | num_updates 13464 | best_loss 11.08
2022-08-15 03:07:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 6 @ 13464 updates, score 11.08) (writing took 79.90586203150451 seconds)
2022-08-15 03:09:18 | INFO | train_inner | epoch 007:     36 / 2244 loss=10.805, nll_loss=7.069, mask_ins=1.527, word_ins_ml=8.129, word_reposition=1.149, ppl=1789.13, wps=2286, ups=0.22, wpb=10194.5, bsz=126.8, num_updates=13500, lr=0.00030429, gnorm=1.822, clip=0, train_wall=286, wall=39177
2022-08-15 03:14:04 | INFO | train_inner | epoch 007:    136 / 2244 loss=10.841, nll_loss=7.095, mask_ins=1.544, word_ins_ml=8.153, word_reposition=1.144, ppl=1834.58, wps=3565.3, ups=0.35, wpb=10204.6, bsz=128, num_updates=13600, lr=0.00030317, gnorm=1.732, clip=0, train_wall=286, wall=39463
2022-08-15 03:18:50 | INFO | train_inner | epoch 007:    236 / 2244 loss=10.769, nll_loss=7.048, mask_ins=1.513, word_ins_ml=8.112, word_reposition=1.144, ppl=1744.88, wps=3584.9, ups=0.35, wpb=10277.9, bsz=128, num_updates=13700, lr=0.000302061, gnorm=1.735, clip=0, train_wall=286, wall=39750
2022-08-15 03:23:36 | INFO | train_inner | epoch 007:    336 / 2244 loss=10.692, nll_loss=6.994, mask_ins=1.496, word_ins_ml=8.064, word_reposition=1.133, ppl=1654.34, wps=3596.2, ups=0.35, wpb=10283.2, bsz=128, num_updates=13800, lr=0.000300965, gnorm=1.746, clip=0, train_wall=285, wall=40036
2022-08-15 03:28:24 | INFO | train_inner | epoch 007:    436 / 2244 loss=10.702, nll_loss=6.979, mask_ins=1.507, word_ins_ml=8.05, word_reposition=1.145, ppl=1666.19, wps=3585.5, ups=0.35, wpb=10317, bsz=128, num_updates=13900, lr=0.00029988, gnorm=1.755, clip=0, train_wall=287, wall=40324
2022-08-15 03:33:12 | INFO | train_inner | epoch 007:    536 / 2244 loss=10.726, nll_loss=7.025, mask_ins=1.502, word_ins_ml=8.091, word_reposition=1.132, ppl=1693.63, wps=3548, ups=0.35, wpb=10217.2, bsz=128, num_updates=14000, lr=0.000298807, gnorm=1.72, clip=0, train_wall=287, wall=40612
2022-08-15 03:37:59 | INFO | train_inner | epoch 007:    636 / 2244 loss=10.713, nll_loss=7.015, mask_ins=1.504, word_ins_ml=8.082, word_reposition=1.127, ppl=1678.16, wps=3595.3, ups=0.35, wpb=10308.7, bsz=128, num_updates=14100, lr=0.000297746, gnorm=1.734, clip=0, train_wall=286, wall=40898
2022-08-15 03:42:48 | INFO | train_inner | epoch 007:    736 / 2244 loss=10.755, nll_loss=7.055, mask_ins=1.503, word_ins_ml=8.118, word_reposition=1.134, ppl=1728.01, wps=3558.2, ups=0.35, wpb=10270.7, bsz=128, num_updates=14200, lr=0.000296695, gnorm=1.728, clip=0, train_wall=288, wall=41187
2022-08-15 03:47:34 | INFO | train_inner | epoch 007:    836 / 2244 loss=10.702, nll_loss=7.004, mask_ins=1.501, word_ins_ml=8.073, word_reposition=1.128, ppl=1665.6, wps=3590.6, ups=0.35, wpb=10291.4, bsz=128, num_updates=14300, lr=0.000295656, gnorm=1.762, clip=0, train_wall=286, wall=41474
2022-08-15 03:52:20 | INFO | train_inner | epoch 007:    936 / 2244 loss=10.682, nll_loss=6.999, mask_ins=1.488, word_ins_ml=8.068, word_reposition=1.125, ppl=1642.42, wps=3602.4, ups=0.35, wpb=10285.5, bsz=128, num_updates=14400, lr=0.000294628, gnorm=1.777, clip=0, train_wall=285, wall=41759
2022-08-15 03:57:06 | INFO | train_inner | epoch 007:   1036 / 2244 loss=10.766, nll_loss=7.061, mask_ins=1.515, word_ins_ml=8.122, word_reposition=1.129, ppl=1741.75, wps=3586.1, ups=0.35, wpb=10261, bsz=128, num_updates=14500, lr=0.00029361, gnorm=1.752, clip=0, train_wall=286, wall=42045
2022-08-15 04:01:53 | INFO | train_inner | epoch 007:   1136 / 2244 loss=10.685, nll_loss=6.996, mask_ins=1.491, word_ins_ml=8.065, word_reposition=1.129, ppl=1646.76, wps=3570.9, ups=0.35, wpb=10242.3, bsz=128, num_updates=14600, lr=0.000292603, gnorm=1.7, clip=0, train_wall=286, wall=42332
2022-08-15 04:06:38 | INFO | train_inner | epoch 007:   1236 / 2244 loss=10.7, nll_loss=6.993, mask_ins=1.511, word_ins_ml=8.064, word_reposition=1.125, ppl=1663.19, wps=3612.2, ups=0.35, wpb=10321.5, bsz=128, num_updates=14700, lr=0.000291606, gnorm=1.724, clip=0, train_wall=285, wall=42618
2022-08-15 04:11:24 | INFO | train_inner | epoch 007:   1336 / 2244 loss=10.697, nll_loss=6.98, mask_ins=1.503, word_ins_ml=8.051, word_reposition=1.143, ppl=1660.36, wps=3567.2, ups=0.35, wpb=10206.2, bsz=128, num_updates=14800, lr=0.000290619, gnorm=1.732, clip=0, train_wall=285, wall=42904
2022-08-15 04:16:12 | INFO | train_inner | epoch 007:   1436 / 2244 loss=10.728, nll_loss=7.004, mask_ins=1.519, word_ins_ml=8.072, word_reposition=1.137, ppl=1696.24, wps=3583.2, ups=0.35, wpb=10302.8, bsz=128, num_updates=14900, lr=0.000289642, gnorm=1.739, clip=0, train_wall=287, wall=43192
2022-08-15 04:21:00 | INFO | train_inner | epoch 007:   1536 / 2244 loss=10.693, nll_loss=6.997, mask_ins=1.489, word_ins_ml=8.066, word_reposition=1.139, ppl=1655.69, wps=3569.4, ups=0.35, wpb=10279.3, bsz=128, num_updates=15000, lr=0.000288675, gnorm=1.69, clip=0, train_wall=287, wall=43480
2022-08-15 04:25:47 | INFO | train_inner | epoch 007:   1636 / 2244 loss=10.674, nll_loss=6.991, mask_ins=1.498, word_ins_ml=8.061, word_reposition=1.115, ppl=1634.07, wps=3568.1, ups=0.35, wpb=10250, bsz=128, num_updates=15100, lr=0.000287718, gnorm=1.719, clip=0, train_wall=287, wall=43767
2022-08-15 04:30:36 | INFO | train_inner | epoch 007:   1736 / 2244 loss=10.684, nll_loss=6.991, mask_ins=1.499, word_ins_ml=8.061, word_reposition=1.124, ppl=1644.67, wps=3540.5, ups=0.35, wpb=10224.6, bsz=128, num_updates=15200, lr=0.00028677, gnorm=1.751, clip=0, train_wall=288, wall=44056
2022-08-15 04:35:24 | INFO | train_inner | epoch 007:   1836 / 2244 loss=10.657, nll_loss=6.958, mask_ins=1.488, word_ins_ml=8.033, word_reposition=1.136, ppl=1615.18, wps=3552.2, ups=0.35, wpb=10232.9, bsz=128, num_updates=15300, lr=0.000285831, gnorm=1.75, clip=0, train_wall=287, wall=44344
2022-08-15 04:40:12 | INFO | train_inner | epoch 007:   1936 / 2244 loss=10.694, nll_loss=6.984, mask_ins=1.512, word_ins_ml=8.055, word_reposition=1.128, ppl=1656.29, wps=3563.9, ups=0.35, wpb=10263.5, bsz=128, num_updates=15400, lr=0.000284901, gnorm=1.715, clip=0, train_wall=287, wall=44632
2022-08-15 04:45:01 | INFO | train_inner | epoch 007:   2036 / 2244 loss=10.676, nll_loss=6.973, mask_ins=1.491, word_ins_ml=8.045, word_reposition=1.139, ppl=1636.16, wps=3552, ups=0.35, wpb=10273.1, bsz=128, num_updates=15500, lr=0.000283981, gnorm=1.717, clip=0, train_wall=289, wall=44921
2022-08-15 04:49:50 | INFO | train_inner | epoch 007:   2136 / 2244 loss=10.687, nll_loss=6.998, mask_ins=1.484, word_ins_ml=8.067, word_reposition=1.136, ppl=1648.86, wps=3581.7, ups=0.35, wpb=10343.7, bsz=128, num_updates=15600, lr=0.000283069, gnorm=1.76, clip=0, train_wall=288, wall=45210
2022-08-15 04:54:38 | INFO | train_inner | epoch 007:   2236 / 2244 loss=10.616, nll_loss=6.955, mask_ins=1.466, word_ins_ml=8.029, word_reposition=1.122, ppl=1569.7, wps=3559.7, ups=0.35, wpb=10244.9, bsz=128, num_updates=15700, lr=0.000282166, gnorm=1.729, clip=0, train_wall=287, wall=45498
2022-08-15 04:54:59 | INFO | train | epoch 007 | loss 10.709 | nll_loss 7.005 | mask_ins 1.502 | word_ins_ml 8.074 | word_reposition 1.133 | ppl 1674.04 | wps 3487.6 | ups 0.34 | wpb 10263.9 | bsz 127.9 | num_updates 15708 | lr 0.000282094 | gnorm 1.738 | clip 0 | train_wall 6430 | wall 45519
2022-08-15 04:56:20 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.995 | nll_loss 7.227 | mask_ins 1.613 | word_ins_ml 8.296 | word_reposition 1.085 | ppl 2040.24 | wps 12260 | wpb 1183.8 | bsz 16 | num_updates 15708 | best_loss 10.995
2022-08-15 04:56:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 7 @ 15708 updates, score 10.995) (writing took 28.868274316191673 seconds)
2022-08-15 05:01:14 | INFO | train_inner | epoch 008:     92 / 2244 loss=10.677, nll_loss=6.959, mask_ins=1.512, word_ins_ml=8.033, word_reposition=1.132, ppl=1637.31, wps=2568.5, ups=0.25, wpb=10161.2, bsz=126.8, num_updates=15800, lr=0.000281272, gnorm=1.776, clip=0, train_wall=285, wall=45893
2022-08-15 05:06:03 | INFO | train_inner | epoch 008:    192 / 2244 loss=10.622, nll_loss=6.932, mask_ins=1.487, word_ins_ml=8.01, word_reposition=1.126, ppl=1576.43, wps=3546.3, ups=0.35, wpb=10258.4, bsz=128, num_updates=15900, lr=0.000280386, gnorm=1.74, clip=0, train_wall=289, wall=46182
2022-08-15 05:10:52 | INFO | train_inner | epoch 008:    292 / 2244 loss=10.655, nll_loss=6.945, mask_ins=1.503, word_ins_ml=8.02, word_reposition=1.132, ppl=1612.03, wps=3577.5, ups=0.35, wpb=10354.9, bsz=128, num_updates=16000, lr=0.000279508, gnorm=1.712, clip=0, train_wall=289, wall=46472
2022-08-15 05:15:42 | INFO | train_inner | epoch 008:    392 / 2244 loss=10.642, nll_loss=6.943, mask_ins=1.494, word_ins_ml=8.019, word_reposition=1.129, ppl=1598.28, wps=3544.5, ups=0.34, wpb=10278.2, bsz=128, num_updates=16100, lr=0.000278639, gnorm=1.745, clip=0, train_wall=289, wall=46762
2022-08-15 05:20:32 | INFO | train_inner | epoch 008:    492 / 2244 loss=10.651, nll_loss=6.944, mask_ins=1.493, word_ins_ml=8.02, word_reposition=1.138, ppl=1607.68, wps=3560.3, ups=0.35, wpb=10307.9, bsz=128, num_updates=16200, lr=0.000277778, gnorm=1.707, clip=0, train_wall=289, wall=47051
2022-08-15 05:25:20 | INFO | train_inner | epoch 008:    592 / 2244 loss=10.667, nll_loss=6.987, mask_ins=1.496, word_ins_ml=8.057, word_reposition=1.114, ppl=1625.81, wps=3562.1, ups=0.35, wpb=10280.2, bsz=128, num_updates=16300, lr=0.000276924, gnorm=1.735, clip=0, train_wall=288, wall=47340
2022-08-15 05:30:10 | INFO | train_inner | epoch 008:    692 / 2244 loss=10.586, nll_loss=6.913, mask_ins=1.474, word_ins_ml=7.992, word_reposition=1.119, ppl=1536.62, wps=3548.7, ups=0.35, wpb=10262.1, bsz=128, num_updates=16400, lr=0.000276079, gnorm=1.77, clip=0, train_wall=289, wall=47629
2022-08-15 05:35:00 | INFO | train_inner | epoch 008:    792 / 2244 loss=10.651, nll_loss=6.97, mask_ins=1.488, word_ins_ml=8.042, word_reposition=1.121, ppl=1607.86, wps=3539.4, ups=0.34, wpb=10276.5, bsz=128, num_updates=16500, lr=0.000275241, gnorm=1.776, clip=0, train_wall=290, wall=47920
2022-08-15 05:39:49 | INFO | train_inner | epoch 008:    892 / 2244 loss=10.612, nll_loss=6.915, mask_ins=1.492, word_ins_ml=7.994, word_reposition=1.125, ppl=1564.67, wps=3552, ups=0.35, wpb=10259.1, bsz=128, num_updates=16600, lr=0.000274411, gnorm=1.724, clip=0, train_wall=288, wall=48208
2022-08-15 05:44:38 | INFO | train_inner | epoch 008:    992 / 2244 loss=10.685, nll_loss=6.974, mask_ins=1.508, word_ins_ml=8.045, word_reposition=1.133, ppl=1646.74, wps=3555.3, ups=0.35, wpb=10278.4, bsz=128, num_updates=16700, lr=0.000273588, gnorm=1.718, clip=0, train_wall=288, wall=48497
2022-08-15 05:49:27 | INFO | train_inner | epoch 008:   1092 / 2244 loss=10.631, nll_loss=6.922, mask_ins=1.502, word_ins_ml=8.001, word_reposition=1.129, ppl=1585.76, wps=3552.5, ups=0.35, wpb=10269.8, bsz=128, num_updates=16800, lr=0.000272772, gnorm=1.729, clip=0, train_wall=288, wall=48787
2022-08-15 05:54:16 | INFO | train_inner | epoch 008:   1192 / 2244 loss=10.626, nll_loss=6.936, mask_ins=1.489, word_ins_ml=8.012, word_reposition=1.125, ppl=1580.09, wps=3547.1, ups=0.35, wpb=10256.5, bsz=128, num_updates=16900, lr=0.000271964, gnorm=1.701, clip=0, train_wall=289, wall=49076
2022-08-15 05:59:06 | INFO | train_inner | epoch 008:   1292 / 2244 loss=10.642, nll_loss=6.974, mask_ins=1.482, word_ins_ml=8.045, word_reposition=1.115, ppl=1598.2, wps=3557.1, ups=0.34, wpb=10315.5, bsz=128, num_updates=17000, lr=0.000271163, gnorm=1.707, clip=0, train_wall=289, wall=49366
2022-08-15 06:03:57 | INFO | train_inner | epoch 008:   1392 / 2244 loss=10.623, nll_loss=6.92, mask_ins=1.491, word_ins_ml=7.998, word_reposition=1.134, ppl=1576.71, wps=3524.2, ups=0.34, wpb=10239.5, bsz=128, num_updates=17100, lr=0.000270369, gnorm=1.743, clip=0, train_wall=290, wall=49656
2022-08-15 06:08:45 | INFO | train_inner | epoch 008:   1492 / 2244 loss=10.693, nll_loss=6.991, mask_ins=1.507, word_ins_ml=8.06, word_reposition=1.126, ppl=1655.17, wps=3554.8, ups=0.35, wpb=10268.1, bsz=128, num_updates=17200, lr=0.000269582, gnorm=1.876, clip=0, train_wall=288, wall=49945
2022-08-15 06:13:34 | INFO | train_inner | epoch 008:   1592 / 2244 loss=10.675, nll_loss=6.983, mask_ins=1.491, word_ins_ml=8.054, word_reposition=1.131, ppl=1635.33, wps=3554.9, ups=0.35, wpb=10272.8, bsz=128, num_updates=17300, lr=0.000268802, gnorm=1.775, clip=0, train_wall=288, wall=50234
2022-08-15 06:18:22 | INFO | train_inner | epoch 008:   1692 / 2244 loss=10.65, nll_loss=6.937, mask_ins=1.504, word_ins_ml=8.014, word_reposition=1.133, ppl=1606.88, wps=3557.5, ups=0.35, wpb=10243.9, bsz=128, num_updates=17400, lr=0.000268028, gnorm=1.705, clip=0, train_wall=287, wall=50522
2022-08-15 06:23:12 | INFO | train_inner | epoch 008:   1792 / 2244 loss=10.538, nll_loss=6.858, mask_ins=1.477, word_ins_ml=7.944, word_reposition=1.117, ppl=1486.61, wps=3542.3, ups=0.35, wpb=10260.6, bsz=128, num_updates=17500, lr=0.000267261, gnorm=1.71, clip=0, train_wall=289, wall=50812
2022-08-15 06:28:02 | INFO | train_inner | epoch 008:   1892 / 2244 loss=10.598, nll_loss=6.925, mask_ins=1.476, word_ins_ml=8.002, word_reposition=1.12, ppl=1550.02, wps=3545.8, ups=0.34, wpb=10288.2, bsz=128, num_updates=17600, lr=0.000266501, gnorm=12.48, clip=1, train_wall=290, wall=51102
2022-08-15 06:32:52 | INFO | train_inner | epoch 008:   1992 / 2244 loss=10.604, nll_loss=6.907, mask_ins=1.498, word_ins_ml=7.987, word_reposition=1.119, ppl=1556.7, wps=3531.1, ups=0.35, wpb=10218, bsz=128, num_updates=17700, lr=0.000265747, gnorm=1.77, clip=0, train_wall=289, wall=51391
2022-08-15 06:37:40 | INFO | train_inner | epoch 008:   2092 / 2244 loss=10.571, nll_loss=6.911, mask_ins=1.471, word_ins_ml=7.99, word_reposition=1.11, ppl=1521.21, wps=3551.9, ups=0.35, wpb=10227.9, bsz=128, num_updates=17800, lr=0.000264999, gnorm=2.551, clip=1, train_wall=287, wall=51679
2022-08-15 06:42:30 | INFO | train_inner | epoch 008:   2192 / 2244 loss=10.679, nll_loss=6.977, mask_ins=1.504, word_ins_ml=8.047, word_reposition=1.128, ppl=1639.72, wps=3514.2, ups=0.34, wpb=10207.8, bsz=128, num_updates=17900, lr=0.000264258, gnorm=1.741, clip=0, train_wall=290, wall=51970
2022-08-15 06:44:59 | INFO | train | epoch 008 | loss 10.635 | nll_loss 6.942 | mask_ins 1.493 | word_ins_ml 8.018 | word_reposition 1.125 | ppl 1590.73 | wps 3490 | ups 0.34 | wpb 10263.9 | bsz 127.9 | num_updates 17952 | lr 0.000263875 | gnorm 2.261 | clip 0.1 | train_wall 6476 | wall 52118
2022-08-15 06:46:21 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 11.465 | nll_loss 7.575 | mask_ins 1.627 | word_ins_ml 8.594 | word_reposition 1.244 | ppl 2827.22 | wps 12095 | wpb 1183.8 | bsz 16 | num_updates 17952 | best_loss 10.995
2022-08-15 06:46:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 8 @ 17952 updates, score 11.465) (writing took 32.03821978345513 seconds)
2022-08-15 06:49:09 | INFO | train_inner | epoch 009:     48 / 2244 loss=10.604, nll_loss=6.911, mask_ins=1.482, word_ins_ml=7.99, word_reposition=1.132, ppl=1555.97, wps=2555.5, ups=0.25, wpb=10189.8, bsz=126.8, num_updates=18000, lr=0.000263523, gnorm=1.85, clip=0, train_wall=284, wall=52368
2022-08-15 06:53:55 | INFO | train_inner | epoch 009:    148 / 2244 loss=10.608, nll_loss=6.931, mask_ins=1.49, word_ins_ml=8.008, word_reposition=1.111, ppl=1560.43, wps=3591.5, ups=0.35, wpb=10285, bsz=128, num_updates=18100, lr=0.000262794, gnorm=1.703, clip=0, train_wall=286, wall=52655
2022-08-15 06:58:44 | INFO | train_inner | epoch 009:    248 / 2244 loss=10.533, nll_loss=6.85, mask_ins=1.478, word_ins_ml=7.937, word_reposition=1.117, ppl=1481.18, wps=3561.9, ups=0.35, wpb=10275.9, bsz=128, num_updates=18200, lr=0.000262071, gnorm=1.736, clip=0, train_wall=288, wall=52943
2022-08-15 07:03:32 | INFO | train_inner | epoch 009:    348 / 2244 loss=10.603, nll_loss=6.923, mask_ins=1.488, word_ins_ml=8.001, word_reposition=1.114, ppl=1555.14, wps=3566, ups=0.35, wpb=10289.8, bsz=128, num_updates=18300, lr=0.000261354, gnorm=1.697, clip=0, train_wall=288, wall=53232
2022-08-15 07:08:22 | INFO | train_inner | epoch 009:    448 / 2244 loss=10.577, nll_loss=6.889, mask_ins=1.485, word_ins_ml=7.971, word_reposition=1.122, ppl=1528.02, wps=3552.1, ups=0.35, wpb=10285.1, bsz=128, num_updates=18400, lr=0.000260643, gnorm=2.963, clip=1, train_wall=289, wall=53521
2022-08-15 07:13:11 | INFO | train_inner | epoch 009:    548 / 2244 loss=10.565, nll_loss=6.884, mask_ins=1.474, word_ins_ml=7.967, word_reposition=1.124, ppl=1515.23, wps=3543.8, ups=0.35, wpb=10249.4, bsz=128, num_updates=18500, lr=0.000259938, gnorm=2.094, clip=1, train_wall=289, wall=53811
2022-08-15 07:18:00 | INFO | train_inner | epoch 009:    648 / 2244 loss=10.531, nll_loss=6.868, mask_ins=1.478, word_ins_ml=7.953, word_reposition=1.1, ppl=1479.82, wps=3561.1, ups=0.35, wpb=10292.3, bsz=128, num_updates=18600, lr=0.000259238, gnorm=1.692, clip=0, train_wall=288, wall=54100
2022-08-15 07:22:48 | INFO | train_inner | epoch 009:    748 / 2244 loss=10.553, nll_loss=6.893, mask_ins=1.473, word_ins_ml=7.975, word_reposition=1.105, ppl=1502.77, wps=3579.3, ups=0.35, wpb=10292.7, bsz=128, num_updates=18700, lr=0.000258544, gnorm=1.709, clip=0, train_wall=287, wall=54387
2022-08-15 07:27:35 | INFO | train_inner | epoch 009:    848 / 2244 loss=10.578, nll_loss=6.9, mask_ins=1.47, word_ins_ml=7.98, word_reposition=1.128, ppl=1528.23, wps=3565.8, ups=0.35, wpb=10261, bsz=128, num_updates=18800, lr=0.000257855, gnorm=1.727, clip=0, train_wall=287, wall=54675
2022-08-15 07:32:24 | INFO | train_inner | epoch 009:    948 / 2244 loss=10.604, nll_loss=6.932, mask_ins=1.483, word_ins_ml=8.009, word_reposition=1.113, ppl=1556.85, wps=3544.5, ups=0.35, wpb=10234.5, bsz=128, num_updates=18900, lr=0.000257172, gnorm=1.699, clip=0, train_wall=288, wall=54964
2022-08-15 07:37:13 | INFO | train_inner | epoch 009:   1048 / 2244 loss=10.596, nll_loss=6.914, mask_ins=1.486, word_ins_ml=7.993, word_reposition=1.117, ppl=1548.27, wps=3559.6, ups=0.35, wpb=10269.2, bsz=128, num_updates=19000, lr=0.000256495, gnorm=1.725, clip=0, train_wall=288, wall=55252
2022-08-15 07:42:02 | INFO | train_inner | epoch 009:   1148 / 2244 loss=10.592, nll_loss=6.927, mask_ins=1.48, word_ins_ml=8.004, word_reposition=1.108, ppl=1543.09, wps=3544.4, ups=0.35, wpb=10254.5, bsz=128, num_updates=19100, lr=0.000255822, gnorm=1.704, clip=0, train_wall=289, wall=55541
2022-08-15 07:46:51 | INFO | train_inner | epoch 009:   1248 / 2244 loss=10.56, nll_loss=6.879, mask_ins=1.479, word_ins_ml=7.963, word_reposition=1.119, ppl=1510.15, wps=3575.9, ups=0.35, wpb=10336.7, bsz=128, num_updates=19200, lr=0.000255155, gnorm=1.714, clip=0, train_wall=288, wall=55831
2022-08-15 07:51:40 | INFO | train_inner | epoch 009:   1348 / 2244 loss=10.539, nll_loss=6.863, mask_ins=1.48, word_ins_ml=7.948, word_reposition=1.111, ppl=1488.19, wps=3540.2, ups=0.35, wpb=10248, bsz=128, num_updates=19300, lr=0.000254493, gnorm=1.692, clip=0, train_wall=289, wall=56120
2022-08-15 07:56:28 | INFO | train_inner | epoch 009:   1448 / 2244 loss=10.559, nll_loss=6.895, mask_ins=1.475, word_ins_ml=7.976, word_reposition=1.108, ppl=1508.82, wps=3576.4, ups=0.35, wpb=10288.2, bsz=128, num_updates=19400, lr=0.000253837, gnorm=1.7, clip=0, train_wall=287, wall=56408
2022-08-15 08:01:17 | INFO | train_inner | epoch 009:   1548 / 2244 loss=10.589, nll_loss=6.911, mask_ins=1.484, word_ins_ml=7.989, word_reposition=1.116, ppl=1540.75, wps=3550.2, ups=0.35, wpb=10265.4, bsz=128, num_updates=19500, lr=0.000253185, gnorm=1.716, clip=0, train_wall=289, wall=56697
2022-08-15 08:06:07 | INFO | train_inner | epoch 009:   1648 / 2244 loss=10.512, nll_loss=6.875, mask_ins=1.455, word_ins_ml=7.958, word_reposition=1.098, ppl=1460.2, wps=3536.8, ups=0.35, wpb=10238.5, bsz=128, num_updates=19600, lr=0.000252538, gnorm=1.694, clip=0, train_wall=289, wall=56986
2022-08-15 08:10:57 | INFO | train_inner | epoch 009:   1748 / 2244 loss=10.527, nll_loss=6.867, mask_ins=1.456, word_ins_ml=7.951, word_reposition=1.119, ppl=1475.31, wps=3518.7, ups=0.34, wpb=10202, bsz=128, num_updates=19700, lr=0.000251896, gnorm=1.705, clip=0, train_wall=289, wall=57276
2022-08-15 08:15:46 | INFO | train_inner | epoch 009:   1848 / 2244 loss=10.597, nll_loss=6.916, mask_ins=1.491, word_ins_ml=7.994, word_reposition=1.112, ppl=1548.51, wps=3540.6, ups=0.35, wpb=10232, bsz=128, num_updates=19800, lr=0.000251259, gnorm=1.716, clip=0, train_wall=288, wall=57565
2022-08-15 08:20:36 | INFO | train_inner | epoch 009:   1948 / 2244 loss=10.574, nll_loss=6.894, mask_ins=1.488, word_ins_ml=7.975, word_reposition=1.111, ppl=1524, wps=3547.2, ups=0.34, wpb=10299.2, bsz=128, num_updates=19900, lr=0.000250627, gnorm=1.701, clip=0, train_wall=290, wall=57856
2022-08-15 08:25:25 | INFO | train_inner | epoch 009:   2048 / 2244 loss=10.518, nll_loss=6.872, mask_ins=1.456, word_ins_ml=7.956, word_reposition=1.106, ppl=1466.7, wps=3544.1, ups=0.35, wpb=10242.4, bsz=128, num_updates=20000, lr=0.00025, gnorm=1.7, clip=0, train_wall=288, wall=58145
2022-08-15 08:30:14 | INFO | train_inner | epoch 009:   2148 / 2244 loss=10.557, nll_loss=6.889, mask_ins=1.474, word_ins_ml=7.97, word_reposition=1.113, ppl=1506.8, wps=3559, ups=0.35, wpb=10271, bsz=128, num_updates=20100, lr=0.000249377, gnorm=1.687, clip=0, train_wall=288, wall=58433
2022-08-15 08:34:49 | INFO | train | epoch 009 | loss 10.565 | nll_loss 6.893 | mask_ins 1.478 | word_ins_ml 7.974 | word_reposition 1.113 | ppl 1515.03 | wps 3494.9 | ups 0.34 | wpb 10263.9 | bsz 127.9 | num_updates 20196 | lr 0.000248784 | gnorm 1.781 | clip 0.1 | train_wall 6462 | wall 58709
2022-08-15 08:36:11 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.894 | nll_loss 7.13 | mask_ins 1.579 | word_ins_ml 8.216 | word_reposition 1.098 | ppl 1902.28 | wps 12084.2 | wpb 1183.8 | bsz 16 | num_updates 20196 | best_loss 10.894
2022-08-15 08:36:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 9 @ 20196 updates, score 10.894) (writing took 39.34447233937681 seconds)
2022-08-15 08:37:02 | INFO | train_inner | epoch 010:      4 / 2244 loss=10.574, nll_loss=6.888, mask_ins=1.492, word_ins_ml=7.97, word_reposition=1.112, ppl=1524.77, wps=2496.7, ups=0.25, wpb=10185.6, bsz=126.8, num_updates=20200, lr=0.000248759, gnorm=1.722, clip=0, train_wall=286, wall=58841
2022-08-15 08:41:49 | INFO | train_inner | epoch 010:    104 / 2244 loss=10.541, nll_loss=6.861, mask_ins=1.482, word_ins_ml=7.946, word_reposition=1.112, ppl=1490.26, wps=3558.3, ups=0.35, wpb=10224.9, bsz=128, num_updates=20300, lr=0.000248146, gnorm=1.712, clip=0, train_wall=287, wall=59129
2022-08-15 08:46:38 | INFO | train_inner | epoch 010:    204 / 2244 loss=10.517, nll_loss=6.833, mask_ins=1.47, word_ins_ml=7.921, word_reposition=1.125, ppl=1465.01, wps=3545.1, ups=0.35, wpb=10262.4, bsz=128, num_updates=20400, lr=0.000247537, gnorm=1.689, clip=0, train_wall=289, wall=59418
2022-08-15 08:51:28 | INFO | train_inner | epoch 010:    304 / 2244 loss=10.531, nll_loss=6.858, mask_ins=1.473, word_ins_ml=7.943, word_reposition=1.115, ppl=1479.56, wps=3546.8, ups=0.35, wpb=10257.6, bsz=128, num_updates=20500, lr=0.000246932, gnorm=1.699, clip=0, train_wall=289, wall=59707
2022-08-15 08:56:17 | INFO | train_inner | epoch 010:    404 / 2244 loss=10.51, nll_loss=6.819, mask_ins=1.476, word_ins_ml=7.91, word_reposition=1.125, ppl=1458.55, wps=3557.8, ups=0.35, wpb=10298.8, bsz=128, num_updates=20600, lr=0.000246332, gnorm=1.705, clip=0, train_wall=289, wall=59997
2022-08-15 09:01:06 | INFO | train_inner | epoch 010:    504 / 2244 loss=10.495, nll_loss=6.846, mask_ins=1.47, word_ins_ml=7.933, word_reposition=1.092, ppl=1442.85, wps=3548.9, ups=0.35, wpb=10261.1, bsz=128, num_updates=20700, lr=0.000245737, gnorm=1.745, clip=0, train_wall=289, wall=60286
2022-08-15 09:05:56 | INFO | train_inner | epoch 010:    604 / 2244 loss=10.51, nll_loss=6.841, mask_ins=1.474, word_ins_ml=7.928, word_reposition=1.108, ppl=1458.23, wps=3559.4, ups=0.34, wpb=10321.8, bsz=128, num_updates=20800, lr=0.000245145, gnorm=1.694, clip=0, train_wall=289, wall=60576
2022-08-15 09:10:46 | INFO | train_inner | epoch 010:    704 / 2244 loss=10.583, nll_loss=6.901, mask_ins=1.492, word_ins_ml=7.981, word_reposition=1.11, ppl=1533.99, wps=3547.5, ups=0.34, wpb=10288.8, bsz=128, num_updates=20900, lr=0.000244558, gnorm=1.682, clip=0, train_wall=289, wall=60866
2022-08-15 09:15:37 | INFO | train_inner | epoch 010:    804 / 2244 loss=10.494, nll_loss=6.823, mask_ins=1.467, word_ins_ml=7.914, word_reposition=1.114, ppl=1442.57, wps=3522, ups=0.34, wpb=10230.5, bsz=128, num_updates=21000, lr=0.000243975, gnorm=1.681, clip=0, train_wall=290, wall=61156
2022-08-15 09:20:25 | INFO | train_inner | epoch 010:    904 / 2244 loss=10.458, nll_loss=6.805, mask_ins=1.464, word_ins_ml=7.897, word_reposition=1.096, ppl=1406.28, wps=3542.3, ups=0.35, wpb=10227.6, bsz=128, num_updates=21100, lr=0.000243396, gnorm=1.739, clip=0, train_wall=288, wall=61445
2022-08-15 09:25:16 | INFO | train_inner | epoch 010:   1004 / 2244 loss=10.43, nll_loss=6.774, mask_ins=1.452, word_ins_ml=7.87, word_reposition=1.108, ppl=1379.44, wps=3526.2, ups=0.34, wpb=10235.7, bsz=128, num_updates=21200, lr=0.000242821, gnorm=1.711, clip=0, train_wall=290, wall=61735
2022-08-15 09:30:06 | INFO | train_inner | epoch 010:   1104 / 2244 loss=10.556, nll_loss=6.887, mask_ins=1.468, word_ins_ml=7.969, word_reposition=1.119, ppl=1505.6, wps=3511.5, ups=0.34, wpb=10203.7, bsz=128, num_updates=21300, lr=0.000242251, gnorm=1.698, clip=0, train_wall=290, wall=62026
2022-08-15 09:34:56 | INFO | train_inner | epoch 010:   1204 / 2244 loss=10.502, nll_loss=6.854, mask_ins=1.454, word_ins_ml=7.939, word_reposition=1.109, ppl=1449.94, wps=3553.1, ups=0.35, wpb=10296.8, bsz=128, num_updates=21400, lr=0.000241684, gnorm=1.709, clip=0, train_wall=289, wall=62316
2022-08-15 09:39:45 | INFO | train_inner | epoch 010:   1304 / 2244 loss=10.521, nll_loss=6.861, mask_ins=1.467, word_ins_ml=7.946, word_reposition=1.108, ppl=1469.75, wps=3538.1, ups=0.35, wpb=10213.5, bsz=128, num_updates=21500, lr=0.000241121, gnorm=1.7, clip=0, train_wall=288, wall=62604
2022-08-15 09:44:34 | INFO | train_inner | epoch 010:   1404 / 2244 loss=10.535, nll_loss=6.875, mask_ins=1.476, word_ins_ml=7.958, word_reposition=1.1, ppl=1483.38, wps=3583.3, ups=0.35, wpb=10382.3, bsz=128, num_updates=21600, lr=0.000240563, gnorm=1.693, clip=0, train_wall=289, wall=62894
2022-08-15 09:49:24 | INFO | train_inner | epoch 010:   1504 / 2244 loss=10.509, nll_loss=6.832, mask_ins=1.461, word_ins_ml=7.92, word_reposition=1.129, ppl=1457.63, wps=3557.1, ups=0.35, wpb=10285.9, bsz=128, num_updates=21700, lr=0.000240008, gnorm=1.687, clip=0, train_wall=289, wall=63183
2022-08-15 09:54:13 | INFO | train_inner | epoch 010:   1604 / 2244 loss=10.457, nll_loss=6.81, mask_ins=1.447, word_ins_ml=7.901, word_reposition=1.108, ppl=1405.26, wps=3560.4, ups=0.35, wpb=10289.6, bsz=128, num_updates=21800, lr=0.000239457, gnorm=1.711, clip=0, train_wall=288, wall=63472
2022-08-15 09:59:03 | INFO | train_inner | epoch 010:   1704 / 2244 loss=10.5, nll_loss=6.841, mask_ins=1.47, word_ins_ml=7.928, word_reposition=1.102, ppl=1448.4, wps=3561.4, ups=0.34, wpb=10340.1, bsz=128, num_updates=21900, lr=0.000238909, gnorm=1.74, clip=0, train_wall=290, wall=63763
2022-08-15 10:03:52 | INFO | train_inner | epoch 010:   1804 / 2244 loss=10.531, nll_loss=6.854, mask_ins=1.488, word_ins_ml=7.939, word_reposition=1.103, ppl=1479.65, wps=3558.6, ups=0.35, wpb=10281.3, bsz=128, num_updates=22000, lr=0.000238366, gnorm=1.72, clip=0, train_wall=288, wall=64052
2022-08-15 10:08:41 | INFO | train_inner | epoch 010:   1904 / 2244 loss=10.514, nll_loss=6.83, mask_ins=1.488, word_ins_ml=7.919, word_reposition=1.107, ppl=1462.03, wps=3541.7, ups=0.35, wpb=10256.6, bsz=128, num_updates=22100, lr=0.000237826, gnorm=1.695, clip=0, train_wall=289, wall=64341
2022-08-15 10:13:30 | INFO | train_inner | epoch 010:   2004 / 2244 loss=10.495, nll_loss=6.8, mask_ins=1.483, word_ins_ml=7.893, word_reposition=1.119, ppl=1443.19, wps=3547.2, ups=0.35, wpb=10245.4, bsz=128, num_updates=22200, lr=0.000237289, gnorm=1.709, clip=0, train_wall=288, wall=64630
2022-08-15 10:18:20 | INFO | train_inner | epoch 010:   2104 / 2244 loss=10.495, nll_loss=6.833, mask_ins=1.477, word_ins_ml=7.921, word_reposition=1.098, ppl=1443.37, wps=3535.9, ups=0.35, wpb=10236.8, bsz=128, num_updates=22300, lr=0.000236757, gnorm=1.744, clip=0, train_wall=289, wall=64919
2022-08-15 10:23:09 | INFO | train_inner | epoch 010:   2204 / 2244 loss=10.532, nll_loss=6.854, mask_ins=1.481, word_ins_ml=7.939, word_reposition=1.112, ppl=1480.43, wps=3531.9, ups=0.35, wpb=10226.5, bsz=128, num_updates=22400, lr=0.000236228, gnorm=1.68, clip=0, train_wall=289, wall=65209
2022-08-15 10:25:04 | INFO | train | epoch 010 | loss 10.511 | nll_loss 6.841 | mask_ins 1.472 | word_ins_ml 7.928 | word_reposition 1.11 | ppl 1458.77 | wps 3482 | ups 0.34 | wpb 10263.9 | bsz 127.9 | num_updates 22440 | lr 0.000236017 | gnorm 1.708 | clip 0 | train_wall 6479 | wall 65323
2022-08-15 10:26:26 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.854 | nll_loss 7.12 | mask_ins 1.584 | word_ins_ml 8.213 | word_reposition 1.057 | ppl 1850.54 | wps 12081.1 | wpb 1183.8 | bsz 16 | num_updates 22440 | best_loss 10.854
2022-08-15 10:27:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_best.pt (epoch 10 @ 22440 updates, score 10.854) (writing took 45.555639216676354 seconds)
2022-08-15 10:30:03 | INFO | train_inner | epoch 011:     60 / 2244 loss=10.472, nll_loss=6.812, mask_ins=1.472, word_ins_ml=7.903, word_reposition=1.098, ppl=1420.26, wps=2465.5, ups=0.24, wpb=10197.9, bsz=126.8, num_updates=22500, lr=0.000235702, gnorm=1.762, clip=0, train_wall=286, wall=65623
2022-08-15 10:34:53 | INFO | train_inner | epoch 011:    160 / 2244 loss=10.526, nll_loss=6.833, mask_ins=1.486, word_ins_ml=7.921, word_reposition=1.118, ppl=1474, wps=3561.5, ups=0.35, wpb=10311.5, bsz=128, num_updates=22600, lr=0.00023518, gnorm=1.713, clip=0, train_wall=289, wall=65912
2022-08-15 10:39:41 | INFO | train_inner | epoch 011:    260 / 2244 loss=10.454, nll_loss=6.796, mask_ins=1.469, word_ins_ml=7.889, word_reposition=1.096, ppl=1402.74, wps=3541.7, ups=0.35, wpb=10224.9, bsz=128, num_updates=22700, lr=0.000234662, gnorm=1.702, clip=0, train_wall=288, wall=66201
2022-08-15 10:44:29 | INFO | train_inner | epoch 011:    360 / 2244 loss=10.47, nll_loss=6.789, mask_ins=1.488, word_ins_ml=7.883, word_reposition=1.099, ppl=1418.45, wps=3568.2, ups=0.35, wpb=10259.5, bsz=128, num_updates=22800, lr=0.000234146, gnorm=1.742, clip=0, train_wall=287, wall=66488
2022-08-15 10:49:17 | INFO | train_inner | epoch 011:    460 / 2244 loss=10.509, nll_loss=6.836, mask_ins=1.48, word_ins_ml=7.924, word_reposition=1.106, ppl=1457.55, wps=3575, ups=0.35, wpb=10289.8, bsz=128, num_updates=22900, lr=0.000233635, gnorm=1.721, clip=0, train_wall=287, wall=66776
2022-08-15 10:54:05 | INFO | train_inner | epoch 011:    560 / 2244 loss=10.463, nll_loss=6.801, mask_ins=1.465, word_ins_ml=7.893, word_reposition=1.104, ppl=1411.78, wps=3555.5, ups=0.35, wpb=10248.7, bsz=128, num_updates=23000, lr=0.000233126, gnorm=1.689, clip=0, train_wall=288, wall=67064
2022-08-15 10:58:52 | INFO | train_inner | epoch 011:    660 / 2244 loss=10.535, nll_loss=6.863, mask_ins=1.483, word_ins_ml=7.947, word_reposition=1.105, ppl=1483.66, wps=3558.9, ups=0.35, wpb=10223.1, bsz=128, num_updates=23100, lr=0.000232621, gnorm=1.696, clip=0, train_wall=287, wall=67352
2022-08-15 11:03:40 | INFO | train_inner | epoch 011:    760 / 2244 loss=10.427, nll_loss=6.792, mask_ins=1.447, word_ins_ml=7.886, word_reposition=1.094, ppl=1376.37, wps=3572.6, ups=0.35, wpb=10286.5, bsz=128, num_updates=23200, lr=0.000232119, gnorm=1.819, clip=0, train_wall=287, wall=67640
2022-08-15 11:08:29 | INFO | train_inner | epoch 011:    860 / 2244 loss=10.485, nll_loss=6.81, mask_ins=1.479, word_ins_ml=7.901, word_reposition=1.105, ppl=1433.67, wps=3545.6, ups=0.35, wpb=10250.4, bsz=128, num_updates=23300, lr=0.000231621, gnorm=1.702, clip=0, train_wall=288, wall=67929
2022-08-15 11:13:16 | INFO | train_inner | epoch 011:    960 / 2244 loss=10.499, nll_loss=6.84, mask_ins=1.469, word_ins_ml=7.927, word_reposition=1.104, ppl=1447.47, wps=3567.9, ups=0.35, wpb=10250.7, bsz=128, num_updates=23400, lr=0.000231125, gnorm=1.692, clip=0, train_wall=287, wall=68216
2022-08-15 11:18:03 | INFO | train_inner | epoch 011:   1060 / 2244 loss=10.363, nll_loss=6.73, mask_ins=1.442, word_ins_ml=7.831, word_reposition=1.09, ppl=1316.54, wps=3582.6, ups=0.35, wpb=10263, bsz=128, num_updates=23500, lr=0.000230633, gnorm=1.708, clip=0, train_wall=286, wall=68503
2022-08-15 11:22:50 | INFO | train_inner | epoch 011:   1160 / 2244 loss=10.354, nll_loss=6.741, mask_ins=1.42, word_ins_ml=7.84, word_reposition=1.094, ppl=1308.98, wps=3568, ups=0.35, wpb=10256.8, bsz=128, num_updates=23600, lr=0.000230144, gnorm=1.726, clip=0, train_wall=287, wall=68790
2022-08-15 11:27:38 | INFO | train_inner | epoch 011:   1260 / 2244 loss=10.51, nll_loss=6.832, mask_ins=1.486, word_ins_ml=7.92, word_reposition=1.104, ppl=1457.95, wps=3570.3, ups=0.35, wpb=10285.4, bsz=128, num_updates=23700, lr=0.000229658, gnorm=1.691, clip=0, train_wall=287, wall=69078
2022-08-15 11:32:25 | INFO | train_inner | epoch 011:   1360 / 2244 loss=10.421, nll_loss=6.773, mask_ins=1.458, word_ins_ml=7.869, word_reposition=1.094, ppl=1371.11, wps=3576.4, ups=0.35, wpb=10250.6, bsz=128, num_updates=23800, lr=0.000229175, gnorm=1.693, clip=0, train_wall=286, wall=69365
2022-08-15 11:37:12 | INFO | train_inner | epoch 011:   1460 / 2244 loss=10.467, nll_loss=6.825, mask_ins=1.455, word_ins_ml=7.913, word_reposition=1.099, ppl=1415.18, wps=3585.6, ups=0.35, wpb=10287, bsz=128, num_updates=23900, lr=0.000228695, gnorm=1.683, clip=0, train_wall=286, wall=69652
2022-08-15 11:42:00 | INFO | train_inner | epoch 011:   1560 / 2244 loss=10.474, nll_loss=6.813, mask_ins=1.469, word_ins_ml=7.903, word_reposition=1.101, ppl=1422.27, wps=3574.7, ups=0.35, wpb=10279.7, bsz=128, num_updates=24000, lr=0.000228218, gnorm=1.688, clip=0, train_wall=287, wall=69939
2022-08-15 11:46:47 | INFO | train_inner | epoch 011:   1660 / 2244 loss=10.499, nll_loss=6.837, mask_ins=1.47, word_ins_ml=7.924, word_reposition=1.104, ppl=1446.93, wps=3562.1, ups=0.35, wpb=10228.1, bsz=128, num_updates=24100, lr=0.000227744, gnorm=1.691, clip=0, train_wall=287, wall=70226
2022-08-15 11:51:35 | INFO | train_inner | epoch 011:   1760 / 2244 loss=10.442, nll_loss=6.764, mask_ins=1.483, word_ins_ml=7.861, word_reposition=1.098, ppl=1391.01, wps=3573.8, ups=0.35, wpb=10301.9, bsz=128, num_updates=24200, lr=0.000227273, gnorm=1.698, clip=0, train_wall=288, wall=70515
2022-08-15 11:56:24 | INFO | train_inner | epoch 011:   1860 / 2244 loss=10.419, nll_loss=6.789, mask_ins=1.442, word_ins_ml=7.882, word_reposition=1.095, ppl=1369.29, wps=3560.1, ups=0.35, wpb=10281.8, bsz=128, num_updates=24300, lr=0.000226805, gnorm=1.694, clip=0, train_wall=288, wall=70803
2022-08-15 12:01:12 | INFO | train_inner | epoch 011:   1960 / 2244 loss=10.403, nll_loss=6.76, mask_ins=1.442, word_ins_ml=7.857, word_reposition=1.103, ppl=1353.59, wps=3571, ups=0.35, wpb=10278.3, bsz=128, num_updates=24400, lr=0.000226339, gnorm=1.707, clip=0, train_wall=287, wall=71091
2022-08-15 12:06:02 | INFO | train_inner | epoch 011:   2060 / 2244 loss=10.409, nll_loss=6.744, mask_ins=1.459, word_ins_ml=7.843, word_reposition=1.106, ppl=1359.35, wps=3556.1, ups=0.34, wpb=10332.7, bsz=128, num_updates=24500, lr=0.000225877, gnorm=1.676, clip=0, train_wall=290, wall=71382
2022-08-15 12:10:51 | INFO | train_inner | epoch 011:   2160 / 2244 loss=10.432, nll_loss=6.796, mask_ins=1.449, word_ins_ml=7.888, word_reposition=1.095, ppl=1381.7, wps=3568, ups=0.35, wpb=10306, bsz=128, num_updates=24600, lr=0.000225417, gnorm=1.672, clip=0, train_wall=288, wall=71671
2022-08-15 12:14:51 | INFO | train | epoch 011 | loss 10.455 | nll_loss 6.799 | mask_ins 1.464 | word_ins_ml 7.891 | word_reposition 1.1 | ppl 1403.31 | wps 3496.6 | ups 0.34 | wpb 10263.9 | bsz 127.9 | num_updates 24684 | lr 0.000225034 | gnorm 1.707 | clip 0 | train_wall 6445 | wall 71910
2022-08-15 12:16:12 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.877 | nll_loss 7.132 | mask_ins 1.561 | word_ins_ml 8.247 | word_reposition 1.069 | ppl 1880.7 | wps 12177.3 | wpb 1183.8 | bsz 16 | num_updates 24684 | best_loss 10.854
2022-08-15 12:16:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_cased/checkpoint_last.pt (epoch 11 @ 24684 updates, score 10.877) (writing took 31.041362950578332 seconds)
2022-08-15 12:17:28 | INFO | train_inner | epoch 012:     16 / 2244 loss=10.462, nll_loss=6.833, mask_ins=1.461, word_ins_ml=7.921, word_reposition=1.08, ppl=1410.71, wps=2545.7, ups=0.25, wpb=10115.8, bsz=126.8, num_updates=24700, lr=0.000224961, gnorm=1.731, clip=0, train_wall=284, wall=72068
2022-08-15 12:22:21 | INFO | train_inner | epoch 012:    116 / 2244 loss=10.374, nll_loss=6.713, mask_ins=1.455, word_ins_ml=7.816, word_reposition=1.103, ppl=1327.09, wps=3510.6, ups=0.34, wpb=10279.3, bsz=128, num_updates=24800, lr=0.000224507, gnorm=1.705, clip=0, train_wall=292, wall=72361
2022-08-15 12:27:11 | INFO | train_inner | epoch 012:    216 / 2244 loss=10.406, nll_loss=6.738, mask_ins=1.478, word_ins_ml=7.838, word_reposition=1.089, ppl=1356.49, wps=3533.7, ups=0.34, wpb=10245, bsz=128, num_updates=24900, lr=0.000224055, gnorm=1.688, clip=0, train_wall=289, wall=72651
2022-08-15 12:32:00 | INFO | train_inner | epoch 012:    316 / 2244 loss=10.386, nll_loss=6.743, mask_ins=1.456, word_ins_ml=7.843, word_reposition=1.087, ppl=1338.04, wps=3544.8, ups=0.35, wpb=10232.6, bsz=128, num_updates=25000, lr=0.000223607, gnorm=1.694, clip=0, train_wall=288, wall=72939
2022-08-15 12:36:50 | INFO | train_inner | epoch 012:    416 / 2244 loss=10.371, nll_loss=6.744, mask_ins=1.439, word_ins_ml=7.843, word_reposition=1.089, ppl=1324.17, wps=3548.1, ups=0.34, wpb=10284.7, bsz=128, num_updates=25100, lr=0.000223161, gnorm=1.709, clip=0, train_wall=289, wall=73229
2022-08-15 12:41:39 | INFO | train_inner | epoch 012:    516 / 2244 loss=10.426, nll_loss=6.77, mask_ins=1.46, word_ins_ml=7.866, word_reposition=1.1, ppl=1375.67, wps=3559.6, ups=0.35, wpb=10288.1, bsz=128, num_updates=25200, lr=0.000222718, gnorm=1.707, clip=0, train_wall=288, wall=73518
2022-08-15 12:46:30 | INFO | train_inner | epoch 012:    616 / 2244 loss=10.401, nll_loss=6.755, mask_ins=1.439, word_ins_ml=7.853, word_reposition=1.11, ppl=1352.43, wps=3511.7, ups=0.34, wpb=10226.1, bsz=128, num_updates=25300, lr=0.000222277, gnorm=1.718, clip=0, train_wall=291, wall=73809
2022-08-15 12:51:20 | INFO | train_inner | epoch 012:    716 / 2244 loss=10.407, nll_loss=6.755, mask_ins=1.455, word_ins_ml=7.852, word_reposition=1.1, ppl=1358.17, wps=3550.5, ups=0.34, wpb=10301.7, bsz=128, num_updates=25400, lr=0.000221839, gnorm=1.703, clip=0, train_wall=290, wall=74100
2022-08-15 12:56:09 | INFO | train_inner | epoch 012:    816 / 2244 loss=10.436, nll_loss=6.781, mask_ins=1.46, word_ins_ml=7.875, word_reposition=1.101, ppl=1384.88, wps=3561.6, ups=0.35, wpb=10310.4, bsz=128, num_updates=25500, lr=0.000221404, gnorm=1.905, clip=0, train_wall=289, wall=74389
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
