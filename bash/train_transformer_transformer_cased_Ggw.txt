nohup: ignoring input
2022-07-31 14:54:58 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:11225
2022-07-31 14:54:58 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:11225
2022-07-31 14:54:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-31 14:54:58 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:11225
2022-07-31 14:54:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-31 14:54:58 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:11225
2022-07-31 14:54:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-31 14:54:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-31 14:54:58 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-31 14:54:58 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-31 14:54:58 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-31 14:54:58 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-31 14:54:58 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-31 14:54:58 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-31 14:54:58 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-31 14:54:58 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-31 14:55:02 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=32, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=32, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:11225', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_transformer_transformer_cased_Ggw', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='6,6,6', layers_num='6,6,6', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, constraint=False, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='../data-bin-bert-cased-Ggw', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='../cached_examples_bert_cased_510_Ggw', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=False, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='transformer', decoder='transformer', keywords_gran='token', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-31 14:55:02 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-31 14:55:02 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-07-31 14:55:02 | INFO | fairseq.data.data_utils | loaded 189612 examples from: ../data-bin-bert-cased-Ggw/valid.source-target.source
2022-07-31 14:55:02 | INFO | fairseq.data.data_utils | loaded 189612 examples from: ../data-bin-bert-cased-Ggw/valid.source-target.target
2022-07-31 14:55:02 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-Ggw valid source-target 189612 examples
2022-07-31 14:55:02 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-31 14:55:02 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
2022-07-31 14:55:04 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): EditorTransformerEncoder(
    (embed_tokens): Embedding(28996, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): EditorTransformerDecoder(
    (embed_tokens): Embedding(28996, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
    (embed_mask_ins): Embedding(256, 1536)
    (layers_reposition): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-07-31 14:55:04 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-31 14:55:04 | INFO | fairseq_cli.train | num. model params: 151068672 (num. trained: 151068672)
2022-07-31 14:55:04 | INFO | fairseq_cli.train | num. Encoder model params: 55746816 (Encoder num. trained: 55746816)
2022-07-31 14:55:04 | INFO | fairseq_cli.train | num. Decoder model params: 117590784 (Decoder num. trained: 117590784)
Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']2022-07-31 14:55:05 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-31 14:55:05 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 32
2022-07-31 14:55:05 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_last.pt
2022-07-31 14:55:05 | INFO | fairseq.trainer | loading train data for epoch 1
Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']2022-07-31 14:55:05 | INFO | fairseq.data.data_utils | loaded 3803212 examples from: ../data-bin-bert-cased-Ggw/train.source-target.source
2022-07-31 14:55:05 | INFO | fairseq.data.data_utils | loaded 3803212 examples from: ../data-bin-bert-cased-Ggw/train.source-target.target
2022-07-31 14:55:05 | INFO | fairseq.tasks.translation | ../data-bin-bert-cased-Ggw train source-target 3803212 examples
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-31 14:56:59 | INFO | train_inner | epoch 001:    100 / 3715 loss=23.622, nll_loss=14.306, mask_ins=7.54, word_ins_ml=14.383, word_reposition=1.7, ppl=1.29098e+07, wps=17424.6, ups=1.19, wpb=14605, bsz=1024, num_updates=100, lr=1.0098e-05, gnorm=17.368, clip=0, loss_scale=128, train_wall=85, wall=115
2022-07-31 14:58:22 | INFO | train_inner | epoch 001:    200 / 3715 loss=17.997, nll_loss=12.724, mask_ins=4.306, word_ins_ml=12.969, word_reposition=0.723, ppl=261661, wps=17695.7, ups=1.21, wpb=14603.1, bsz=1024, num_updates=200, lr=2.0096e-05, gnorm=17.841, clip=0, loss_scale=128, train_wall=81, wall=197
2022-07-31 14:59:44 | INFO | train_inner | epoch 001:    300 / 3715 loss=14.616, nll_loss=11.322, mask_ins=2.276, word_ins_ml=11.766, word_reposition=0.574, ppl=25114.3, wps=17883.7, ups=1.21, wpb=14772, bsz=1024, num_updates=300, lr=3.0094e-05, gnorm=3.78, clip=0, loss_scale=128, train_wall=81, wall=280
2022-07-31 15:01:08 | INFO | train_inner | epoch 001:    400 / 3715 loss=13.788, nll_loss=10.658, mask_ins=1.976, word_ins_ml=11.253, word_reposition=0.558, ppl=14142.1, wps=17658.4, ups=1.2, wpb=14728.8, bsz=1024, num_updates=400, lr=4.0092e-05, gnorm=1.648, clip=0, loss_scale=128, train_wall=82, wall=363
2022-07-31 15:02:29 | INFO | train_inner | epoch 001:    500 / 3715 loss=13.494, nll_loss=10.356, mask_ins=1.901, word_ins_ml=11.005, word_reposition=0.588, ppl=11538.6, wps=17964.5, ups=1.23, wpb=14607.3, bsz=1024, num_updates=500, lr=5.009e-05, gnorm=1.606, clip=0, loss_scale=128, train_wall=80, wall=444
2022-07-31 15:03:49 | INFO | train_inner | epoch 001:    600 / 3715 loss=13.228, nll_loss=10.041, mask_ins=1.883, word_ins_ml=10.732, word_reposition=0.613, ppl=9591.88, wps=18148.3, ups=1.24, wpb=14581.3, bsz=1024, num_updates=600, lr=6.0088e-05, gnorm=1.476, clip=0, loss_scale=242, train_wall=79, wall=525
2022-07-31 15:05:12 | INFO | train_inner | epoch 001:    700 / 3715 loss=12.989, nll_loss=9.741, mask_ins=1.884, word_ins_ml=10.469, word_reposition=0.636, ppl=8130.19, wps=17899.6, ups=1.21, wpb=14738.3, bsz=1024, num_updates=700, lr=7.0086e-05, gnorm=1.554, clip=0, loss_scale=256, train_wall=81, wall=607
2022-07-31 15:06:33 | INFO | train_inner | epoch 001:    800 / 3715 loss=12.751, nll_loss=9.488, mask_ins=1.874, word_ins_ml=10.245, word_reposition=0.631, ppl=6891.38, wps=17926.2, ups=1.22, wpb=14637, bsz=1024, num_updates=800, lr=8.0084e-05, gnorm=1.521, clip=0, loss_scale=256, train_wall=80, wall=689
2022-07-31 15:07:58 | INFO | train_inner | epoch 001:    900 / 3715 loss=12.55, nll_loss=9.275, mask_ins=1.862, word_ins_ml=10.057, word_reposition=0.631, ppl=5994.83, wps=17048.2, ups=1.17, wpb=14512.1, bsz=1024, num_updates=900, lr=9.0082e-05, gnorm=1.566, clip=0, loss_scale=256, train_wall=83, wall=774
2022-07-31 15:09:39 | INFO | train_inner | epoch 001:   1000 / 3715 loss=12.403, nll_loss=9.089, mask_ins=1.877, word_ins_ml=9.893, word_reposition=0.633, ppl=5415.77, wps=14438.3, ups=0.99, wpb=14576.3, bsz=1024, num_updates=1000, lr=0.00010008, gnorm=1.532, clip=0, loss_scale=256, train_wall=99, wall=875
2022-07-31 15:11:21 | INFO | train_inner | epoch 001:   1100 / 3715 loss=12.241, nll_loss=8.906, mask_ins=1.869, word_ins_ml=9.734, word_reposition=0.637, ppl=4839.04, wps=14384.2, ups=0.98, wpb=14664.5, bsz=1024, num_updates=1100, lr=0.000110078, gnorm=1.347, clip=0, loss_scale=453, train_wall=100, wall=977
2022-07-31 15:13:03 | INFO | train_inner | epoch 001:   1200 / 3715 loss=12.118, nll_loss=8.766, mask_ins=1.875, word_ins_ml=9.611, word_reposition=0.632, ppl=4444.11, wps=14478.3, ups=0.99, wpb=14698.2, bsz=1024, num_updates=1200, lr=0.000120076, gnorm=1.483, clip=0, loss_scale=512, train_wall=100, wall=1078
2022-07-31 15:14:43 | INFO | train_inner | epoch 001:   1300 / 3715 loss=11.97, nll_loss=8.616, mask_ins=1.857, word_ins_ml=9.481, word_reposition=0.633, ppl=4010.96, wps=14611, ups=1, wpb=14674.2, bsz=1024, num_updates=1300, lr=0.000130074, gnorm=1.421, clip=0, loss_scale=512, train_wall=99, wall=1179
2022-07-31 15:16:23 | INFO | train_inner | epoch 001:   1400 / 3715 loss=11.886, nll_loss=8.51, mask_ins=1.863, word_ins_ml=9.387, word_reposition=0.636, ppl=3785.21, wps=14736.5, ups=1, wpb=14762.6, bsz=1024, num_updates=1400, lr=0.000140072, gnorm=1.595, clip=0, loss_scale=512, train_wall=98, wall=1279
2022-07-31 15:18:05 | INFO | train_inner | epoch 001:   1500 / 3715 loss=11.711, nll_loss=8.291, mask_ins=1.851, word_ins_ml=9.194, word_reposition=0.666, ppl=3351.94, wps=14459.3, ups=0.99, wpb=14653.9, bsz=1024, num_updates=1500, lr=0.00015007, gnorm=1.52, clip=0, loss_scale=512, train_wall=100, wall=1380
2022-07-31 15:19:31 | INFO | train_inner | epoch 001:   1600 / 3715 loss=11.555, nll_loss=8.119, mask_ins=1.838, word_ins_ml=9.043, word_reposition=0.674, ppl=3009.83, wps=16987.1, ups=1.15, wpb=14712.5, bsz=1024, num_updates=1600, lr=0.000160068, gnorm=1.692, clip=0, loss_scale=845, train_wall=85, wall=1467
2022-07-31 15:20:53 | INFO | train_inner | epoch 001:   1700 / 3715 loss=11.362, nll_loss=7.899, mask_ins=1.822, word_ins_ml=8.85, word_reposition=0.69, ppl=2632.79, wps=18056.2, ups=1.23, wpb=14722.9, bsz=1023.8, num_updates=1700, lr=0.000170066, gnorm=1.708, clip=0, loss_scale=1024, train_wall=80, wall=1548
2022-07-31 15:22:16 | INFO | train_inner | epoch 001:   1800 / 3715 loss=11.196, nll_loss=7.716, mask_ins=1.809, word_ins_ml=8.69, word_reposition=0.697, ppl=2345.87, wps=17634, ups=1.21, wpb=14623.7, bsz=1024, num_updates=1800, lr=0.000180064, gnorm=1.976, clip=0, loss_scale=1024, train_wall=81, wall=1631
2022-07-31 15:23:37 | INFO | train_inner | epoch 001:   1900 / 3715 loss=11.026, nll_loss=7.555, mask_ins=1.776, word_ins_ml=8.549, word_reposition=0.701, ppl=2084.69, wps=18023.3, ups=1.23, wpb=14622.6, bsz=1024, num_updates=1900, lr=0.000190062, gnorm=2.138, clip=0, loss_scale=1024, train_wall=79, wall=1712
2022-07-31 15:25:06 | INFO | train_inner | epoch 001:   2000 / 3715 loss=10.866, nll_loss=7.366, mask_ins=1.769, word_ins_ml=8.384, word_reposition=0.713, ppl=1866.68, wps=16625.6, ups=1.13, wpb=14718.6, bsz=1024, num_updates=2000, lr=0.00020006, gnorm=2.1, clip=0, loss_scale=1024, train_wall=87, wall=1801
2022-07-31 15:26:52 | INFO | train_inner | epoch 001:   2100 / 3715 loss=10.71, nll_loss=7.223, mask_ins=1.746, word_ins_ml=8.259, word_reposition=0.705, ppl=1674.62, wps=13768.9, ups=0.94, wpb=14676.5, bsz=1024, num_updates=2100, lr=0.000210058, gnorm=2.246, clip=0, loss_scale=1567, train_wall=105, wall=1908
2022-07-31 15:28:43 | INFO | train_inner | epoch 001:   2200 / 3715 loss=10.57, nll_loss=7.063, mask_ins=1.734, word_ins_ml=8.121, word_reposition=0.716, ppl=1520.43, wps=13281.4, ups=0.9, wpb=14703.8, bsz=1024, num_updates=2200, lr=0.000220056, gnorm=2.085, clip=0, loss_scale=2048, train_wall=109, wall=2018
2022-07-31 15:30:30 | INFO | train_inner | epoch 001:   2300 / 3715 loss=10.405, nll_loss=6.894, mask_ins=1.716, word_ins_ml=7.973, word_reposition=0.716, ppl=1356.29, wps=13905.1, ups=0.94, wpb=14839.5, bsz=1024, num_updates=2300, lr=0.000230054, gnorm=2.042, clip=0, loss_scale=2048, train_wall=105, wall=2125
2022-07-31 15:32:16 | INFO | train_inner | epoch 001:   2400 / 3715 loss=10.252, nll_loss=6.766, mask_ins=1.7, word_ins_ml=7.861, word_reposition=0.691, ppl=1219.83, wps=13808.8, ups=0.94, wpb=14630.8, bsz=1024, num_updates=2400, lr=0.000240052, gnorm=2.143, clip=0, loss_scale=2048, train_wall=104, wall=2231
2022-07-31 15:34:02 | INFO | train_inner | epoch 001:   2500 / 3715 loss=10.134, nll_loss=6.636, mask_ins=1.683, word_ins_ml=7.749, word_reposition=0.702, ppl=1123.63, wps=13836.4, ups=0.94, wpb=14677.7, bsz=1024, num_updates=2500, lr=0.00025005, gnorm=2.114, clip=0, loss_scale=2048, train_wall=104, wall=2337
2022-07-31 15:35:47 | INFO | train_inner | epoch 001:   2600 / 3715 loss=9.969, nll_loss=6.485, mask_ins=1.66, word_ins_ml=7.618, word_reposition=0.691, ppl=1002.05, wps=14029.5, ups=0.95, wpb=14766, bsz=1024, num_updates=2600, lr=0.000260048, gnorm=2.082, clip=0, loss_scale=2888, train_wall=103, wall=2442
2022-07-31 15:37:34 | INFO | train_inner | epoch 001:   2700 / 3715 loss=9.805, nll_loss=6.325, mask_ins=1.649, word_ins_ml=7.478, word_reposition=0.678, ppl=894.76, wps=13821.9, ups=0.94, wpb=14768.3, bsz=1024, num_updates=2700, lr=0.000270046, gnorm=1.927, clip=0, loss_scale=4096, train_wall=105, wall=2549
2022-07-31 15:39:20 | INFO | train_inner | epoch 001:   2800 / 3715 loss=9.665, nll_loss=6.201, mask_ins=1.62, word_ins_ml=7.371, word_reposition=0.674, ppl=811.65, wps=13864.1, ups=0.94, wpb=14687.4, bsz=1024, num_updates=2800, lr=0.000280044, gnorm=1.817, clip=0, loss_scale=4096, train_wall=104, wall=2655
2022-07-31 15:40:54 | INFO | train_inner | epoch 001:   2900 / 3715 loss=9.55, nll_loss=6.087, mask_ins=1.608, word_ins_ml=7.272, word_reposition=0.67, ppl=749.58, wps=15499.4, ups=1.06, wpb=14564.6, bsz=1024, num_updates=2900, lr=0.000290042, gnorm=1.99, clip=0, loss_scale=4096, train_wall=92, wall=2749
2022-07-31 15:42:39 | INFO | train_inner | epoch 001:   3000 / 3715 loss=9.453, nll_loss=5.989, mask_ins=1.597, word_ins_ml=7.188, word_reposition=0.667, ppl=700.65, wps=13911.3, ups=0.95, wpb=14712.7, bsz=1024, num_updates=3000, lr=0.00030004, gnorm=1.995, clip=0, loss_scale=4096, train_wall=104, wall=2855
2022-07-31 15:44:25 | INFO | train_inner | epoch 001:   3100 / 3715 loss=9.297, nll_loss=5.863, mask_ins=1.572, word_ins_ml=7.077, word_reposition=0.648, ppl=629.07, wps=13842.2, ups=0.94, wpb=14666.7, bsz=1024, num_updates=3100, lr=0.000310038, gnorm=1.807, clip=0, loss_scale=5284, train_wall=104, wall=2961
2022-07-31 15:46:12 | INFO | train_inner | epoch 001:   3200 / 3715 loss=9.201, nll_loss=5.774, mask_ins=1.558, word_ins_ml=7.001, word_reposition=0.641, ppl=588.51, wps=13594.3, ups=0.93, wpb=14545.5, bsz=1024, num_updates=3200, lr=0.000320036, gnorm=1.754, clip=0, loss_scale=8192, train_wall=105, wall=3068
2022-07-31 15:48:01 | INFO | train_inner | epoch 001:   3300 / 3715 loss=9.092, nll_loss=5.671, mask_ins=1.546, word_ins_ml=6.912, word_reposition=0.634, ppl=545.76, wps=13545.4, ups=0.92, wpb=14682.3, bsz=1024, num_updates=3300, lr=0.000330034, gnorm=1.813, clip=0, loss_scale=8192, train_wall=107, wall=3176
2022-07-31 15:49:50 | INFO | train_inner | epoch 001:   3400 / 3715 loss=8.989, nll_loss=5.588, mask_ins=1.535, word_ins_ml=6.84, word_reposition=0.613, ppl=508.18, wps=13324.2, ups=0.92, wpb=14554.9, bsz=1024, num_updates=3400, lr=0.000340032, gnorm=1.826, clip=0, loss_scale=8192, train_wall=107, wall=3285
2022-07-31 15:51:36 | INFO | train_inner | epoch 001:   3500 / 3715 loss=8.914, nll_loss=5.532, mask_ins=1.519, word_ins_ml=6.792, word_reposition=0.604, ppl=482.44, wps=13867.2, ups=0.94, wpb=14699.8, bsz=1024, num_updates=3500, lr=0.00035003, gnorm=1.733, clip=0, loss_scale=8192, train_wall=104, wall=3391
2022-07-31 15:53:23 | INFO | train_inner | epoch 001:   3600 / 3715 loss=8.858, nll_loss=5.479, mask_ins=1.508, word_ins_ml=6.746, word_reposition=0.605, ppl=464.01, wps=13634, ups=0.93, wpb=14646.9, bsz=1024, num_updates=3600, lr=0.000360028, gnorm=1.62, clip=0, loss_scale=9585, train_wall=106, wall=3499
2022-07-31 15:55:09 | INFO | train_inner | epoch 001:   3700 / 3715 loss=8.773, nll_loss=5.4, mask_ins=1.503, word_ins_ml=6.677, word_reposition=0.592, ppl=437.37, wps=13770.4, ups=0.94, wpb=14580.1, bsz=1024, num_updates=3700, lr=0.000370026, gnorm=1.692, clip=0, loss_scale=16384, train_wall=104, wall=3605
2022-07-31 15:55:25 | INFO | train | epoch 001 | loss 11.53 | nll_loss 7.929 | mask_ins 1.975 | word_ins_ml 8.874 | word_reposition 0.681 | ppl 2957.6 | wps 15174.2 | ups 1.03 | wpb 14662 | bsz 1023.7 | num_updates 3715 | lr 0.000371526 | gnorm 2.688 | clip 0 | loss_scale 2823 | train_wall 3526 | wall 3620
2022-07-31 15:56:47 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.639 | nll_loss 5.133 | mask_ins 1.478 | word_ins_ml 6.564 | word_reposition 0.598 | ppl 398.76 | wps 33894.5 | wpb 1849.4 | bsz 127.9 | num_updates 3715
2022-07-31 15:56:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_best.pt (epoch 1 @ 3715 updates, score 8.639) (writing took 2.870105341076851 seconds)
2022-07-31 15:56:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-31 15:58:23 | INFO | train_inner | epoch 002:     86 / 3715 loss=8.723, nll_loss=5.362, mask_ins=1.496, word_ins_ml=6.644, word_reposition=0.582, ppl=422.67, wps=7482.2, ups=0.52, wpb=14456.6, bsz=1014.7, num_updates=3800, lr=0.000380024, gnorm=1.762, clip=0, loss_scale=9814, train_wall=106, wall=3798
2022-07-31 16:00:10 | INFO | train_inner | epoch 002:    186 / 3715 loss=8.647, nll_loss=5.3, mask_ins=1.481, word_ins_ml=6.591, word_reposition=0.575, ppl=400.8, wps=13684.1, ups=0.93, wpb=14695.8, bsz=1024, num_updates=3900, lr=0.000390022, gnorm=1.65, clip=0, loss_scale=8192, train_wall=106, wall=3905
2022-07-31 16:01:57 | INFO | train_inner | epoch 002:    286 / 3715 loss=8.625, nll_loss=5.267, mask_ins=1.484, word_ins_ml=6.563, word_reposition=0.578, ppl=394.83, wps=13646.6, ups=0.93, wpb=14649.1, bsz=1024, num_updates=4000, lr=0.00040002, gnorm=1.652, clip=0, loss_scale=8192, train_wall=106, wall=4013
2022-07-31 16:03:44 | INFO | train_inner | epoch 002:    386 / 3715 loss=8.569, nll_loss=5.235, mask_ins=1.472, word_ins_ml=6.536, word_reposition=0.562, ppl=379.82, wps=13685.2, ups=0.93, wpb=14640.1, bsz=1024, num_updates=4100, lr=0.000410018, gnorm=1.636, clip=0, loss_scale=8192, train_wall=105, wall=4120
2022-07-31 16:05:32 | INFO | train_inner | epoch 002:    486 / 3715 loss=8.505, nll_loss=5.174, mask_ins=1.465, word_ins_ml=6.482, word_reposition=0.558, ppl=363.24, wps=13595.1, ups=0.93, wpb=14637.1, bsz=1024, num_updates=4200, lr=0.000420016, gnorm=1.61, clip=0, loss_scale=8192, train_wall=106, wall=4227
2022-07-31 16:06:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-31 16:07:21 | INFO | train_inner | epoch 002:    587 / 3715 loss=8.493, nll_loss=5.162, mask_ins=1.46, word_ins_ml=6.473, word_reposition=0.561, ppl=360.38, wps=13505.2, ups=0.91, wpb=14774.8, bsz=1024, num_updates=4300, lr=0.000430014, gnorm=1.604, clip=0, loss_scale=10301, train_wall=108, wall=4337
2022-07-31 16:09:10 | INFO | train_inner | epoch 002:    687 / 3715 loss=8.419, nll_loss=5.108, mask_ins=1.45, word_ins_ml=6.425, word_reposition=0.543, ppl=342.16, wps=13497.2, ups=0.92, wpb=14628.4, bsz=1024, num_updates=4400, lr=0.000440012, gnorm=1.596, clip=0, loss_scale=8192, train_wall=107, wall=4445
2022-07-31 16:10:58 | INFO | train_inner | epoch 002:    787 / 3715 loss=8.392, nll_loss=5.083, mask_ins=1.441, word_ins_ml=6.404, word_reposition=0.548, ppl=335.95, wps=13504.1, ups=0.92, wpb=14684.7, bsz=1024, num_updates=4500, lr=0.00045001, gnorm=1.59, clip=0, loss_scale=8192, train_wall=107, wall=4554
2022-07-31 16:12:46 | INFO | train_inner | epoch 002:    887 / 3715 loss=8.367, nll_loss=5.066, mask_ins=1.434, word_ins_ml=6.389, word_reposition=0.544, ppl=330.24, wps=13628.9, ups=0.93, wpb=14613, bsz=1023.8, num_updates=4600, lr=0.000460008, gnorm=1.692, clip=0, loss_scale=8192, train_wall=105, wall=4661
2022-07-31 16:14:33 | INFO | train_inner | epoch 002:    987 / 3715 loss=8.326, nll_loss=5.031, mask_ins=1.431, word_ins_ml=6.358, word_reposition=0.537, ppl=320.82, wps=13682.5, ups=0.93, wpb=14663.2, bsz=1024, num_updates=4700, lr=0.000470006, gnorm=1.468, clip=0, loss_scale=8192, train_wall=105, wall=4768
2022-07-31 16:16:07 | INFO | train_inner | epoch 002:   1087 / 3715 loss=8.278, nll_loss=4.979, mask_ins=1.423, word_ins_ml=6.313, word_reposition=0.542, ppl=310.36, wps=15613.6, ups=1.06, wpb=14665.1, bsz=1024, num_updates=4800, lr=0.000480004, gnorm=1.595, clip=0, loss_scale=10813, train_wall=92, wall=4862
2022-07-31 16:17:54 | INFO | train_inner | epoch 002:   1187 / 3715 loss=8.263, nll_loss=4.981, mask_ins=1.421, word_ins_ml=6.315, word_reposition=0.527, ppl=307.13, wps=13643.7, ups=0.93, wpb=14607.2, bsz=1024, num_updates=4900, lr=0.000490002, gnorm=1.58, clip=0, loss_scale=16384, train_wall=105, wall=4969
2022-07-31 16:19:40 | INFO | train_inner | epoch 002:   1287 / 3715 loss=8.238, nll_loss=4.956, mask_ins=1.414, word_ins_ml=6.293, word_reposition=0.531, ppl=301.85, wps=13874, ups=0.94, wpb=14725.2, bsz=1024, num_updates=5000, lr=0.0005, gnorm=1.606, clip=0, loss_scale=16384, train_wall=104, wall=5075
2022-07-31 16:21:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-31 16:21:28 | INFO | train_inner | epoch 002:   1388 / 3715 loss=8.2, nll_loss=4.92, mask_ins=1.41, word_ins_ml=6.261, word_reposition=0.53, ppl=294.16, wps=13687.2, ups=0.93, wpb=14730.8, bsz=1024, num_updates=5100, lr=0.000495074, gnorm=1.504, clip=0, loss_scale=15816, train_wall=106, wall=5183
2022-07-31 16:21:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-31 16:23:17 | INFO | train_inner | epoch 002:   1489 / 3715 loss=8.185, nll_loss=4.918, mask_ins=1.408, word_ins_ml=6.259, word_reposition=0.517, ppl=290.98, wps=13441, ups=0.92, wpb=14653.2, bsz=1024, num_updates=5200, lr=0.00049029, gnorm=1.517, clip=0, loss_scale=5069, train_wall=107, wall=5292
2022-07-31 16:25:04 | INFO | train_inner | epoch 002:   1589 / 3715 loss=8.147, nll_loss=4.882, mask_ins=1.402, word_ins_ml=6.228, word_reposition=0.517, ppl=283.52, wps=13577.2, ups=0.93, wpb=14545.7, bsz=1024, num_updates=5300, lr=0.000485643, gnorm=1.527, clip=0, loss_scale=4096, train_wall=105, wall=5399
2022-07-31 16:26:50 | INFO | train_inner | epoch 002:   1689 / 3715 loss=8.096, nll_loss=4.84, mask_ins=1.394, word_ins_ml=6.19, word_reposition=0.512, ppl=273.68, wps=13875, ups=0.94, wpb=14693.1, bsz=1024, num_updates=5400, lr=0.000481125, gnorm=1.527, clip=0, loss_scale=4096, train_wall=104, wall=5505
2022-07-31 16:28:36 | INFO | train_inner | epoch 002:   1789 / 3715 loss=8.071, nll_loss=4.823, mask_ins=1.377, word_ins_ml=6.176, word_reposition=0.518, ppl=268.9, wps=13822.3, ups=0.94, wpb=14713.1, bsz=1024, num_updates=5500, lr=0.000476731, gnorm=1.443, clip=0, loss_scale=4096, train_wall=105, wall=5612
2022-07-31 16:30:23 | INFO | train_inner | epoch 002:   1889 / 3715 loss=8.019, nll_loss=4.775, mask_ins=1.378, word_ins_ml=6.133, word_reposition=0.509, ppl=259.48, wps=13729, ups=0.93, wpb=14705, bsz=1024, num_updates=5600, lr=0.000472456, gnorm=1.46, clip=0, loss_scale=4096, train_wall=105, wall=5719
2022-07-31 16:32:10 | INFO | train_inner | epoch 002:   1989 / 3715 loss=8.011, nll_loss=4.777, mask_ins=1.374, word_ins_ml=6.134, word_reposition=0.503, ppl=258.02, wps=13697.9, ups=0.93, wpb=14663.6, bsz=1024, num_updates=5700, lr=0.000468293, gnorm=1.528, clip=0, loss_scale=6758, train_wall=105, wall=5826
2022-07-31 16:33:57 | INFO | train_inner | epoch 002:   2089 / 3715 loss=7.98, nll_loss=4.742, mask_ins=1.375, word_ins_ml=6.104, word_reposition=0.501, ppl=252.47, wps=13765.3, ups=0.94, wpb=14658.4, bsz=1024, num_updates=5800, lr=0.000464238, gnorm=1.444, clip=0, loss_scale=8192, train_wall=105, wall=5932
2022-07-31 16:35:44 | INFO | train_inner | epoch 002:   2189 / 3715 loss=7.971, nll_loss=4.732, mask_ins=1.369, word_ins_ml=6.094, word_reposition=0.507, ppl=250.89, wps=13778.8, ups=0.94, wpb=14726.9, bsz=1024, num_updates=5900, lr=0.000460287, gnorm=1.523, clip=0, loss_scale=8192, train_wall=105, wall=6039
2022-07-31 16:37:31 | INFO | train_inner | epoch 002:   2289 / 3715 loss=7.911, nll_loss=4.686, mask_ins=1.358, word_ins_ml=6.054, word_reposition=0.499, ppl=240.66, wps=13643.9, ups=0.93, wpb=14669.2, bsz=1024, num_updates=6000, lr=0.000456435, gnorm=1.343, clip=0, loss_scale=8192, train_wall=106, wall=6147
2022-07-31 16:39:18 | INFO | train_inner | epoch 002:   2389 / 3715 loss=7.883, nll_loss=4.666, mask_ins=1.357, word_ins_ml=6.036, word_reposition=0.49, ppl=236, wps=13715.8, ups=0.94, wpb=14669.3, bsz=1024, num_updates=6100, lr=0.000452679, gnorm=1.37, clip=0, loss_scale=8192, train_wall=105, wall=6254
2022-07-31 16:40:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-31 16:41:06 | INFO | train_inner | epoch 002:   2490 / 3715 loss=7.873, nll_loss=4.658, mask_ins=1.356, word_ins_ml=6.028, word_reposition=0.489, ppl=234.39, wps=13534.3, ups=0.93, wpb=14591.3, bsz=1024, num_updates=6200, lr=0.000449013, gnorm=1.396, clip=0, loss_scale=8598, train_wall=106, wall=6361
2022-07-31 16:42:52 | INFO | train_inner | epoch 002:   2590 / 3715 loss=7.825, nll_loss=4.618, mask_ins=1.341, word_ins_ml=5.992, word_reposition=0.492, ppl=226.81, wps=13766.6, ups=0.94, wpb=14600.4, bsz=1024, num_updates=6300, lr=0.000445435, gnorm=1.421, clip=0, loss_scale=8192, train_wall=104, wall=6467
2022-07-31 16:44:39 | INFO | train_inner | epoch 002:   2690 / 3715 loss=7.829, nll_loss=4.628, mask_ins=1.346, word_ins_ml=6.002, word_reposition=0.482, ppl=227.44, wps=13625.8, ups=0.93, wpb=14589.5, bsz=1024, num_updates=6400, lr=0.000441942, gnorm=1.345, clip=0, loss_scale=8192, train_wall=105, wall=6574
2022-07-31 16:46:25 | INFO | train_inner | epoch 002:   2790 / 3715 loss=7.793, nll_loss=4.591, mask_ins=1.339, word_ins_ml=5.968, word_reposition=0.485, ppl=221.73, wps=13821.9, ups=0.94, wpb=14703.9, bsz=1024, num_updates=6500, lr=0.000438529, gnorm=1.298, clip=0, loss_scale=8192, train_wall=105, wall=6681
2022-07-31 16:48:12 | INFO | train_inner | epoch 002:   2890 / 3715 loss=7.763, nll_loss=4.567, mask_ins=1.331, word_ins_ml=5.947, word_reposition=0.485, ppl=217.15, wps=13808.1, ups=0.94, wpb=14677.6, bsz=1024, num_updates=6600, lr=0.000435194, gnorm=1.346, clip=0, loss_scale=8192, train_wall=105, wall=6787
2022-07-31 16:49:58 | INFO | train_inner | epoch 002:   2990 / 3715 loss=7.729, nll_loss=4.554, mask_ins=1.328, word_ins_ml=5.935, word_reposition=0.466, ppl=212.1, wps=13685.2, ups=0.94, wpb=14564.3, bsz=1024, num_updates=6700, lr=0.000431934, gnorm=1.398, clip=0, loss_scale=11223, train_wall=105, wall=6894
2022-07-31 16:51:45 | INFO | train_inner | epoch 002:   3090 / 3715 loss=7.738, nll_loss=4.554, mask_ins=1.327, word_ins_ml=5.935, word_reposition=0.476, ppl=213.49, wps=13639.9, ups=0.93, wpb=14627.9, bsz=1024, num_updates=6800, lr=0.000428746, gnorm=1.361, clip=0, loss_scale=16384, train_wall=106, wall=7001
2022-07-31 16:53:17 | INFO | train_inner | epoch 002:   3190 / 3715 loss=7.701, nll_loss=4.518, mask_ins=1.322, word_ins_ml=5.903, word_reposition=0.477, ppl=208.07, wps=16158.6, ups=1.09, wpb=14829.8, bsz=1024, num_updates=6900, lr=0.000425628, gnorm=1.28, clip=0, loss_scale=16384, train_wall=90, wall=7093
2022-07-31 16:53:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-31 16:55:05 | INFO | train_inner | epoch 002:   3291 / 3715 loss=7.681, nll_loss=4.508, mask_ins=1.316, word_ins_ml=5.893, word_reposition=0.472, ppl=205.18, wps=13537.5, ups=0.93, wpb=14626.9, bsz=1024, num_updates=7000, lr=0.000422577, gnorm=1.281, clip=0, loss_scale=10301, train_wall=106, wall=7201
2022-07-31 16:56:52 | INFO | train_inner | epoch 002:   3391 / 3715 loss=7.649, nll_loss=4.472, mask_ins=1.311, word_ins_ml=5.862, word_reposition=0.476, ppl=200.74, wps=13734.1, ups=0.93, wpb=14695.9, bsz=1024, num_updates=7100, lr=0.000419591, gnorm=1.282, clip=0, loss_scale=8192, train_wall=105, wall=7308
2022-07-31 16:58:40 | INFO | train_inner | epoch 002:   3491 / 3715 loss=7.64, nll_loss=4.472, mask_ins=1.314, word_ins_ml=5.861, word_reposition=0.465, ppl=199.46, wps=13745.2, ups=0.93, wpb=14751.9, bsz=1024, num_updates=7200, lr=0.000416667, gnorm=1.264, clip=0, loss_scale=8192, train_wall=106, wall=7415
2022-07-31 17:00:27 | INFO | train_inner | epoch 002:   3591 / 3715 loss=7.637, nll_loss=4.464, mask_ins=1.311, word_ins_ml=5.854, word_reposition=0.472, ppl=199.1, wps=13554.4, ups=0.93, wpb=14588.2, bsz=1024, num_updates=7300, lr=0.000413803, gnorm=1.252, clip=0, loss_scale=8192, train_wall=106, wall=7523
2022-07-31 17:01:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-31 17:02:14 | INFO | train_inner | epoch 002:   3692 / 3715 loss=7.611, nll_loss=4.441, mask_ins=1.311, word_ins_ml=5.834, word_reposition=0.466, ppl=195.5, wps=13790.6, ups=0.93, wpb=14787.3, bsz=1024, num_updates=7400, lr=0.000410997, gnorm=1.326, clip=0, loss_scale=6813, train_wall=105, wall=7630
2022-07-31 17:02:38 | INFO | train | epoch 002 | loss 8.07 | nll_loss 4.82 | mask_ins 1.386 | word_ins_ml 6.171 | word_reposition 0.513 | ppl 268.71 | wps 13480 | ups 0.92 | wpb 14661.6 | bsz 1023.7 | num_updates 7423 | lr 0.00041036 | gnorm 1.469 | clip 0 | loss_scale 8943 | train_wall 3883 | wall 7653
2022-07-31 17:03:59 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.453 | nll_loss 4.167 | mask_ins 1.275 | word_ins_ml 5.702 | word_reposition 0.477 | ppl 175.21 | wps 33649.4 | wpb 1849.4 | bsz 127.9 | num_updates 7423 | best_loss 7.453
2022-07-31 17:04:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_best.pt (epoch 2 @ 7423 updates, score 7.453) (writing took 4.480516599491239 seconds)
2022-07-31 17:05:26 | INFO | train_inner | epoch 003:     77 / 3715 loss=7.575, nll_loss=4.405, mask_ins=1.308, word_ins_ml=5.802, word_reposition=0.465, ppl=190.63, wps=7652.4, ups=0.52, wpb=14647.4, bsz=1014.7, num_updates=7500, lr=0.000408248, gnorm=1.358, clip=0, loss_scale=4096, train_wall=104, wall=7821
2022-07-31 17:07:12 | INFO | train_inner | epoch 003:    177 / 3715 loss=7.549, nll_loss=4.393, mask_ins=1.3, word_ins_ml=5.791, word_reposition=0.458, ppl=187.32, wps=13699.7, ups=0.94, wpb=14569.5, bsz=1024, num_updates=7600, lr=0.000405554, gnorm=1.325, clip=0, loss_scale=4096, train_wall=105, wall=7928
2022-07-31 17:08:58 | INFO | train_inner | epoch 003:    277 / 3715 loss=7.522, nll_loss=4.371, mask_ins=1.295, word_ins_ml=5.772, word_reposition=0.455, ppl=183.77, wps=13892.2, ups=0.94, wpb=14733.8, bsz=1024, num_updates=7700, lr=0.000402911, gnorm=1.256, clip=0, loss_scale=4096, train_wall=104, wall=8034
2022-07-31 17:10:45 | INFO | train_inner | epoch 003:    377 / 3715 loss=7.512, nll_loss=4.352, mask_ins=1.296, word_ins_ml=5.755, word_reposition=0.461, ppl=182.58, wps=13809.4, ups=0.94, wpb=14724.5, bsz=1024, num_updates=7800, lr=0.00040032, gnorm=1.298, clip=0, loss_scale=4096, train_wall=105, wall=8140
2022-07-31 17:12:32 | INFO | train_inner | epoch 003:    477 / 3715 loss=7.498, nll_loss=4.348, mask_ins=1.294, word_ins_ml=5.751, word_reposition=0.453, ppl=180.79, wps=13734.3, ups=0.93, wpb=14708.9, bsz=1024, num_updates=7900, lr=0.000397779, gnorm=1.377, clip=0, loss_scale=4997, train_wall=105, wall=8247
2022-07-31 17:13:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-31 17:14:19 | INFO | train_inner | epoch 003:    578 / 3715 loss=7.507, nll_loss=4.36, mask_ins=1.294, word_ins_ml=5.761, word_reposition=0.451, ppl=181.86, wps=13703.9, ups=0.93, wpb=14706.1, bsz=1024, num_updates=8000, lr=0.000395285, gnorm=1.305, clip=0, loss_scale=5637, train_wall=106, wall=8355
2022-07-31 17:16:06 | INFO | train_inner | epoch 003:    678 / 3715 loss=7.473, nll_loss=4.327, mask_ins=1.283, word_ins_ml=5.733, word_reposition=0.457, ppl=177.61, wps=13795.2, ups=0.94, wpb=14727.6, bsz=1024, num_updates=8100, lr=0.000392837, gnorm=1.23, clip=0, loss_scale=4096, train_wall=105, wall=8461
2022-07-31 17:17:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-31 17:17:55 | INFO | train_inner | epoch 003:    779 / 3715 loss=7.462, nll_loss=4.319, mask_ins=1.284, word_ins_ml=5.725, word_reposition=0.452, ppl=176.27, wps=13471.3, ups=0.92, wpb=14630.2, bsz=1024, num_updates=8200, lr=0.000390434, gnorm=1.44, clip=0, loss_scale=3265, train_wall=107, wall=8570
2022-07-31 17:19:42 | INFO | train_inner | epoch 003:    879 / 3715 loss=7.459, nll_loss=4.321, mask_ins=1.284, word_ins_ml=5.726, word_reposition=0.449, ppl=175.93, wps=13744.9, ups=0.94, wpb=14696.2, bsz=1024, num_updates=8300, lr=0.000388075, gnorm=1.275, clip=0, loss_scale=2048, train_wall=105, wall=8677
2022-07-31 17:21:29 | INFO | train_inner | epoch 003:    979 / 3715 loss=7.459, nll_loss=4.324, mask_ins=1.278, word_ins_ml=5.729, word_reposition=0.452, ppl=175.94, wps=13582, ups=0.93, wpb=14607.1, bsz=1024, num_updates=8400, lr=0.000385758, gnorm=1.277, clip=0, loss_scale=2048, train_wall=106, wall=8784
2022-07-31 17:23:15 | INFO | train_inner | epoch 003:   1079 / 3715 loss=7.43, nll_loss=4.291, mask_ins=1.275, word_ins_ml=5.7, word_reposition=0.456, ppl=172.5, wps=13811.5, ups=0.94, wpb=14671.2, bsz=1024, num_updates=8500, lr=0.000383482, gnorm=1.272, clip=0, loss_scale=2048, train_wall=105, wall=8891
2022-07-31 17:25:03 | INFO | train_inner | epoch 003:   1179 / 3715 loss=7.437, nll_loss=4.296, mask_ins=1.284, word_ins_ml=5.703, word_reposition=0.449, ppl=173.27, wps=13540.1, ups=0.93, wpb=14585.3, bsz=1024, num_updates=8600, lr=0.000381246, gnorm=1.303, clip=0, loss_scale=2048, train_wall=106, wall=8998
2022-07-31 17:26:50 | INFO | train_inner | epoch 003:   1279 / 3715 loss=7.429, nll_loss=4.294, mask_ins=1.279, word_ins_ml=5.702, word_reposition=0.449, ppl=172.33, wps=13600.4, ups=0.93, wpb=14558.7, bsz=1024, num_updates=8700, lr=0.000379049, gnorm=1.221, clip=0, loss_scale=2642, train_wall=105, wall=9105
2022-07-31 17:28:37 | INFO | train_inner | epoch 003:   1379 / 3715 loss=7.392, nll_loss=4.262, mask_ins=1.277, word_ins_ml=5.673, word_reposition=0.442, ppl=168.01, wps=13762, ups=0.94, wpb=14671.5, bsz=1024, num_updates=8800, lr=0.000376889, gnorm=1.272, clip=0, loss_scale=4096, train_wall=105, wall=9212
2022-07-31 17:30:12 | INFO | train_inner | epoch 003:   1479 / 3715 loss=7.395, nll_loss=4.269, mask_ins=1.269, word_ins_ml=5.679, word_reposition=0.446, ppl=168.32, wps=15397.8, ups=1.05, wpb=14634.9, bsz=1024, num_updates=8900, lr=0.000374766, gnorm=1.261, clip=0, loss_scale=4096, train_wall=93, wall=9307
2022-07-31 17:31:58 | INFO | train_inner | epoch 003:   1579 / 3715 loss=7.376, nll_loss=4.254, mask_ins=1.266, word_ins_ml=5.666, word_reposition=0.444, ppl=166.16, wps=13705.9, ups=0.94, wpb=14596.2, bsz=1024, num_updates=9000, lr=0.000372678, gnorm=1.178, clip=0, loss_scale=4096, train_wall=105, wall=9414
2022-07-31 17:33:45 | INFO | train_inner | epoch 003:   1679 / 3715 loss=7.362, nll_loss=4.253, mask_ins=1.262, word_ins_ml=5.665, word_reposition=0.435, ppl=164.48, wps=13699.6, ups=0.94, wpb=14616.6, bsz=1024, num_updates=9100, lr=0.000370625, gnorm=1.194, clip=0, loss_scale=4096, train_wall=105, wall=9520
2022-07-31 17:35:32 | INFO | train_inner | epoch 003:   1779 / 3715 loss=7.39, nll_loss=4.262, mask_ins=1.271, word_ins_ml=5.672, word_reposition=0.447, ppl=167.68, wps=13796.2, ups=0.94, wpb=14742.9, bsz=1024, num_updates=9200, lr=0.000368605, gnorm=1.24, clip=0, loss_scale=4792, train_wall=105, wall=9627
2022-07-31 17:37:19 | INFO | train_inner | epoch 003:   1879 / 3715 loss=7.363, nll_loss=4.241, mask_ins=1.269, word_ins_ml=5.653, word_reposition=0.441, ppl=164.65, wps=13584.6, ups=0.93, wpb=14615.4, bsz=1024, num_updates=9300, lr=0.000366618, gnorm=1.202, clip=0, loss_scale=8192, train_wall=106, wall=9735
2022-07-31 17:39:06 | INFO | train_inner | epoch 003:   1979 / 3715 loss=7.337, nll_loss=4.23, mask_ins=1.256, word_ins_ml=5.644, word_reposition=0.438, ppl=161.7, wps=13650.9, ups=0.94, wpb=14578.3, bsz=1023.8, num_updates=9400, lr=0.000364662, gnorm=1.19, clip=0, loss_scale=8192, train_wall=105, wall=9842
2022-07-31 17:40:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-31 17:40:55 | INFO | train_inner | epoch 003:   2080 / 3715 loss=7.307, nll_loss=4.196, mask_ins=1.254, word_ins_ml=5.614, word_reposition=0.439, ppl=158.36, wps=13440.3, ups=0.92, wpb=14615.2, bsz=1024, num_updates=9500, lr=0.000362738, gnorm=1.233, clip=0, loss_scale=7665, train_wall=107, wall=9950
2022-07-31 17:42:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-31 17:42:44 | INFO | train_inner | epoch 003:   2181 / 3715 loss=7.318, nll_loss=4.201, mask_ins=1.26, word_ins_ml=5.618, word_reposition=0.441, ppl=159.59, wps=13500.8, ups=0.92, wpb=14736.1, bsz=1024, num_updates=9600, lr=0.000360844, gnorm=1.205, clip=0, loss_scale=3630, train_wall=107, wall=10059
2022-07-31 17:44:30 | INFO | train_inner | epoch 003:   2281 / 3715 loss=7.282, nll_loss=4.181, mask_ins=1.249, word_ins_ml=5.6, word_reposition=0.433, ppl=155.59, wps=13861.4, ups=0.94, wpb=14692.3, bsz=1024, num_updates=9700, lr=0.000358979, gnorm=1.2, clip=0, loss_scale=2048, train_wall=104, wall=10165
2022-07-31 17:46:18 | INFO | train_inner | epoch 003:   2381 / 3715 loss=7.26, nll_loss=4.16, mask_ins=1.248, word_ins_ml=5.581, word_reposition=0.431, ppl=153.26, wps=13635.7, ups=0.93, wpb=14668.5, bsz=1024, num_updates=9800, lr=0.000357143, gnorm=1.222, clip=0, loss_scale=2048, train_wall=106, wall=10273
2022-07-31 17:48:05 | INFO | train_inner | epoch 003:   2481 / 3715 loss=7.303, nll_loss=4.187, mask_ins=1.256, word_ins_ml=5.605, word_reposition=0.442, ppl=157.94, wps=13716.8, ups=0.93, wpb=14770.3, bsz=1024, num_updates=9900, lr=0.000355335, gnorm=1.135, clip=0, loss_scale=2048, train_wall=106, wall=10381
2022-07-31 17:49:54 | INFO | train_inner | epoch 003:   2581 / 3715 loss=7.282, nll_loss=4.19, mask_ins=1.25, word_ins_ml=5.607, word_reposition=0.425, ppl=155.68, wps=13432.8, ups=0.92, wpb=14554.8, bsz=1024, num_updates=10000, lr=0.000353553, gnorm=1.163, clip=0, loss_scale=2048, train_wall=107, wall=10489
2022-07-31 17:51:42 | INFO | train_inner | epoch 003:   2681 / 3715 loss=7.28, nll_loss=4.166, mask_ins=1.256, word_ins_ml=5.586, word_reposition=0.439, ppl=155.44, wps=13612.1, ups=0.93, wpb=14713.1, bsz=1024, num_updates=10100, lr=0.000351799, gnorm=1.231, clip=0, loss_scale=2273, train_wall=106, wall=10597
2022-07-31 17:53:29 | INFO | train_inner | epoch 003:   2781 / 3715 loss=7.277, nll_loss=4.182, mask_ins=1.246, word_ins_ml=5.6, word_reposition=0.432, ppl=155.12, wps=13660.7, ups=0.94, wpb=14609.8, bsz=1024, num_updates=10200, lr=0.00035007, gnorm=1.185, clip=0, loss_scale=4096, train_wall=105, wall=10704
2022-07-31 17:55:16 | INFO | train_inner | epoch 003:   2881 / 3715 loss=7.254, nll_loss=4.156, mask_ins=1.245, word_ins_ml=5.577, word_reposition=0.432, ppl=152.68, wps=13690.4, ups=0.93, wpb=14665.8, bsz=1024, num_updates=10300, lr=0.000348367, gnorm=1.209, clip=0, loss_scale=4096, train_wall=105, wall=10811
2022-07-31 17:57:03 | INFO | train_inner | epoch 003:   2981 / 3715 loss=7.241, nll_loss=4.148, mask_ins=1.244, word_ins_ml=5.569, word_reposition=0.428, ppl=151.28, wps=13778.8, ups=0.94, wpb=14732.7, bsz=1024, num_updates=10400, lr=0.000346688, gnorm=1.19, clip=0, loss_scale=4096, train_wall=105, wall=10918
2022-07-31 17:58:49 | INFO | train_inner | epoch 003:   3081 / 3715 loss=7.228, nll_loss=4.131, mask_ins=1.242, word_ins_ml=5.554, word_reposition=0.432, ppl=149.92, wps=13879, ups=0.94, wpb=14775.5, bsz=1024, num_updates=10500, lr=0.000345033, gnorm=1.188, clip=0, loss_scale=4096, train_wall=105, wall=11025
2022-07-31 18:00:36 | INFO | train_inner | epoch 003:   3181 / 3715 loss=7.249, nll_loss=4.158, mask_ins=1.242, word_ins_ml=5.578, word_reposition=0.429, ppl=152.07, wps=13673.6, ups=0.93, wpb=14632, bsz=1024, num_updates=10600, lr=0.000343401, gnorm=1.152, clip=0, loss_scale=4096, train_wall=105, wall=11132
2022-07-31 18:02:23 | INFO | train_inner | epoch 003:   3281 / 3715 loss=7.233, nll_loss=4.134, mask_ins=1.242, word_ins_ml=5.557, word_reposition=0.434, ppl=150.42, wps=13660.2, ups=0.93, wpb=14651.3, bsz=1024, num_updates=10700, lr=0.000341793, gnorm=1.162, clip=0, loss_scale=8151, train_wall=105, wall=11239
2022-07-31 18:04:11 | INFO | train_inner | epoch 003:   3381 / 3715 loss=7.212, nll_loss=4.123, mask_ins=1.235, word_ins_ml=5.547, word_reposition=0.431, ppl=148.26, wps=13611.6, ups=0.93, wpb=14570.1, bsz=1024, num_updates=10800, lr=0.000340207, gnorm=1.198, clip=0, loss_scale=8192, train_wall=105, wall=11346
2022-07-31 18:05:57 | INFO | train_inner | epoch 003:   3481 / 3715 loss=7.201, nll_loss=4.112, mask_ins=1.231, word_ins_ml=5.537, word_reposition=0.433, ppl=147.15, wps=13855, ups=0.94, wpb=14734.9, bsz=1024, num_updates=10900, lr=0.000338643, gnorm=1.177, clip=0, loss_scale=8192, train_wall=105, wall=11452
2022-07-31 18:07:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-31 18:07:46 | INFO | train_inner | epoch 003:   3582 / 3715 loss=7.189, nll_loss=4.107, mask_ins=1.233, word_ins_ml=5.532, word_reposition=0.424, ppl=145.93, wps=13532.2, ups=0.92, wpb=14720.8, bsz=1024, num_updates=11000, lr=0.0003371, gnorm=1.13, clip=0, loss_scale=6610, train_wall=107, wall=11561
2022-07-31 18:09:26 | INFO | train_inner | epoch 003:   3682 / 3715 loss=7.212, nll_loss=4.112, mask_ins=1.238, word_ins_ml=5.537, word_reposition=0.438, ppl=148.3, wps=14516, ups=1, wpb=14587, bsz=1024, num_updates=11100, lr=0.000335578, gnorm=1.188, clip=0, loss_scale=4096, train_wall=99, wall=11662
2022-07-31 18:09:54 | INFO | train | epoch 003 | loss 7.355 | nll_loss 4.236 | mask_ins 1.264 | word_ins_ml 5.649 | word_reposition 0.442 | ppl 163.72 | wps 13476.7 | ups 0.92 | wpb 14661.9 | bsz 1023.7 | num_updates 11133 | lr 0.00033508 | gnorm 1.233 | clip 0 | loss_scale 4332 | train_wall 3886 | wall 11690
2022-07-31 18:11:15 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.092 | nll_loss 3.877 | mask_ins 1.214 | word_ins_ml 5.444 | word_reposition 0.433 | ppl 136.39 | wps 33693.4 | wpb 1849.4 | bsz 127.9 | num_updates 11133 | best_loss 7.092
2022-07-31 18:11:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_best.pt (epoch 3 @ 11133 updates, score 7.092) (writing took 4.245523499324918 seconds)
2022-07-31 18:12:32 | INFO | train_inner | epoch 004:     67 / 3715 loss=7.147, nll_loss=4.07, mask_ins=1.225, word_ins_ml=5.5, word_reposition=0.422, ppl=141.69, wps=7827.5, ups=0.54, wpb=14514.7, bsz=1014.7, num_updates=11200, lr=0.000334077, gnorm=1.174, clip=0, loss_scale=4096, train_wall=98, wall=11847
2022-07-31 18:14:19 | INFO | train_inner | epoch 004:    167 / 3715 loss=7.126, nll_loss=4.044, mask_ins=1.224, word_ins_ml=5.477, word_reposition=0.425, ppl=139.66, wps=13787.9, ups=0.93, wpb=14777.2, bsz=1024, num_updates=11300, lr=0.000332595, gnorm=1.125, clip=0, loss_scale=4096, train_wall=105, wall=11954
2022-07-31 18:16:06 | INFO | train_inner | epoch 004:    267 / 3715 loss=7.15, nll_loss=4.065, mask_ins=1.228, word_ins_ml=5.495, word_reposition=0.427, ppl=142.06, wps=13770.7, ups=0.93, wpb=14753.4, bsz=1024, num_updates=11400, lr=0.000331133, gnorm=1.12, clip=0, loss_scale=4096, train_wall=105, wall=12061
2022-07-31 18:17:54 | INFO | train_inner | epoch 004:    367 / 3715 loss=7.126, nll_loss=4.041, mask_ins=1.228, word_ins_ml=5.474, word_reposition=0.424, ppl=139.69, wps=13587.4, ups=0.93, wpb=14645, bsz=1024, num_updates=11500, lr=0.00032969, gnorm=1.137, clip=0, loss_scale=5202, train_wall=106, wall=12169
2022-07-31 18:19:42 | INFO | train_inner | epoch 004:    467 / 3715 loss=7.12, nll_loss=4.036, mask_ins=1.223, word_ins_ml=5.469, word_reposition=0.428, ppl=139.09, wps=13509, ups=0.92, wpb=14630.5, bsz=1024, num_updates=11600, lr=0.000328266, gnorm=1.153, clip=0, loss_scale=8192, train_wall=107, wall=12277
2022-07-31 18:21:29 | INFO | train_inner | epoch 004:    567 / 3715 loss=7.136, nll_loss=4.045, mask_ins=1.23, word_ins_ml=5.477, word_reposition=0.429, ppl=140.65, wps=13575.1, ups=0.93, wpb=14573.7, bsz=1024, num_updates=11700, lr=0.00032686, gnorm=1.164, clip=0, loss_scale=8192, train_wall=106, wall=12385
2022-07-31 18:23:17 | INFO | train_inner | epoch 004:    667 / 3715 loss=7.102, nll_loss=4.026, mask_ins=1.22, word_ins_ml=5.46, word_reposition=0.422, ppl=137.34, wps=13683.3, ups=0.93, wpb=14708, bsz=1024, num_updates=11800, lr=0.000325472, gnorm=1.158, clip=0, loss_scale=8192, train_wall=106, wall=12492
2022-07-31 18:25:02 | INFO | train_inner | epoch 004:    767 / 3715 loss=7.104, nll_loss=4.037, mask_ins=1.218, word_ins_ml=5.47, word_reposition=0.416, ppl=137.57, wps=13879.2, ups=0.95, wpb=14656, bsz=1024, num_updates=11900, lr=0.000324102, gnorm=1.111, clip=0, loss_scale=8192, train_wall=104, wall=12598
2022-07-31 18:26:49 | INFO | train_inner | epoch 004:    867 / 3715 loss=7.097, nll_loss=4.019, mask_ins=1.216, word_ins_ml=5.454, word_reposition=0.426, ppl=136.91, wps=13761.9, ups=0.93, wpb=14734.9, bsz=1024, num_updates=12000, lr=0.000322749, gnorm=1.117, clip=0, loss_scale=9421, train_wall=105, wall=12705
2022-07-31 18:27:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-31 18:27:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-31 18:28:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-31 18:28:40 | INFO | train_inner | epoch 004:    970 / 3715 loss=7.123, nll_loss=4.048, mask_ins=1.218, word_ins_ml=5.479, word_reposition=0.425, ppl=139.4, wps=13153, ups=0.9, wpb=14600.7, bsz=1024, num_updates=12100, lr=0.000321412, gnorm=1.164, clip=0, loss_scale=7615, train_wall=109, wall=12816
2022-07-31 18:30:27 | INFO | train_inner | epoch 004:   1070 / 3715 loss=7.096, nll_loss=4.02, mask_ins=1.217, word_ins_ml=5.455, word_reposition=0.424, ppl=136.82, wps=13676.6, ups=0.94, wpb=14621.1, bsz=1023.8, num_updates=12200, lr=0.000320092, gnorm=1.123, clip=0, loss_scale=2048, train_wall=105, wall=12923
2022-07-31 18:32:15 | INFO | train_inner | epoch 004:   1170 / 3715 loss=7.119, nll_loss=4.037, mask_ins=1.227, word_ins_ml=5.469, word_reposition=0.423, ppl=139.05, wps=13686, ups=0.93, wpb=14688.7, bsz=1024, num_updates=12300, lr=0.000318788, gnorm=1.177, clip=0, loss_scale=2048, train_wall=106, wall=13030
2022-07-31 18:34:01 | INFO | train_inner | epoch 004:   1270 / 3715 loss=7.06, nll_loss=3.986, mask_ins=1.217, word_ins_ml=5.424, word_reposition=0.42, ppl=133.48, wps=13824.3, ups=0.94, wpb=14688, bsz=1024, num_updates=12400, lr=0.0003175, gnorm=1.147, clip=0, loss_scale=2048, train_wall=105, wall=13136
2022-07-31 18:35:48 | INFO | train_inner | epoch 004:   1370 / 3715 loss=7.07, nll_loss=4.006, mask_ins=1.206, word_ins_ml=5.442, word_reposition=0.422, ppl=134.35, wps=13784.4, ups=0.94, wpb=14724.3, bsz=1024, num_updates=12500, lr=0.000316228, gnorm=1.106, clip=0, loss_scale=2048, train_wall=105, wall=13243
2022-07-31 18:37:35 | INFO | train_inner | epoch 004:   1470 / 3715 loss=7.062, nll_loss=3.991, mask_ins=1.213, word_ins_ml=5.428, word_reposition=0.422, ppl=133.66, wps=13709.4, ups=0.94, wpb=14659.7, bsz=1024, num_updates=12600, lr=0.00031497, gnorm=1.178, clip=0, loss_scale=2560, train_wall=105, wall=13350
2022-07-31 18:39:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-31 18:39:24 | INFO | train_inner | epoch 004:   1571 / 3715 loss=7.063, nll_loss=3.998, mask_ins=1.205, word_ins_ml=5.434, word_reposition=0.423, ppl=133.75, wps=13562.4, ups=0.92, wpb=14767.4, bsz=1024, num_updates=12700, lr=0.000313728, gnorm=1.113, clip=0, loss_scale=3812, train_wall=107, wall=13459
2022-07-31 18:39:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-31 18:41:11 | INFO | train_inner | epoch 004:   1672 / 3715 loss=7.056, nll_loss=3.995, mask_ins=1.221, word_ins_ml=5.432, word_reposition=0.404, ppl=133.08, wps=13614.3, ups=0.93, wpb=14623.7, bsz=1024, num_updates=12800, lr=0.0003125, gnorm=1.166, clip=0, loss_scale=1156, train_wall=106, wall=13566
2022-07-31 18:42:59 | INFO | train_inner | epoch 004:   1772 / 3715 loss=7.045, nll_loss=3.98, mask_ins=1.211, word_ins_ml=5.418, word_reposition=0.416, ppl=132.05, wps=13663.7, ups=0.93, wpb=14757.7, bsz=1024, num_updates=12900, lr=0.000311286, gnorm=1.139, clip=0, loss_scale=1024, train_wall=106, wall=13674
2022-07-31 18:44:30 | INFO | train_inner | epoch 004:   1872 / 3715 loss=7.062, nll_loss=3.99, mask_ins=1.213, word_ins_ml=5.426, word_reposition=0.422, ppl=133.62, wps=16108.7, ups=1.1, wpb=14683, bsz=1024, num_updates=13000, lr=0.000310087, gnorm=1.149, clip=0, loss_scale=1024, train_wall=89, wall=13766
2022-07-31 18:46:17 | INFO | train_inner | epoch 004:   1972 / 3715 loss=7.037, nll_loss=3.967, mask_ins=1.206, word_ins_ml=5.407, word_reposition=0.424, ppl=131.31, wps=13616.9, ups=0.93, wpb=14594.5, bsz=1024, num_updates=13100, lr=0.000308901, gnorm=1.123, clip=0, loss_scale=1024, train_wall=105, wall=13873
2022-07-31 18:48:04 | INFO | train_inner | epoch 004:   2072 / 3715 loss=7.034, nll_loss=3.977, mask_ins=1.207, word_ins_ml=5.415, word_reposition=0.412, ppl=131.02, wps=13738.9, ups=0.94, wpb=14600.8, bsz=1024, num_updates=13200, lr=0.000307729, gnorm=1.1, clip=0, loss_scale=1024, train_wall=105, wall=13979
2022-07-31 18:49:51 | INFO | train_inner | epoch 004:   2172 / 3715 loss=7.045, nll_loss=3.988, mask_ins=1.205, word_ins_ml=5.424, word_reposition=0.416, ppl=132.08, wps=13658.6, ups=0.93, wpb=14632.6, bsz=1024, num_updates=13300, lr=0.00030657, gnorm=1.115, clip=0, loss_scale=1802, train_wall=105, wall=14086
2022-07-31 18:51:38 | INFO | train_inner | epoch 004:   2272 / 3715 loss=7.012, nll_loss=3.948, mask_ins=1.203, word_ins_ml=5.389, word_reposition=0.42, ppl=129.07, wps=13824.9, ups=0.93, wpb=14827.5, bsz=1024, num_updates=13400, lr=0.000305424, gnorm=1.115, clip=0, loss_scale=2048, train_wall=105, wall=14193
2022-07-31 18:52:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-31 18:53:26 | INFO | train_inner | epoch 004:   2373 / 3715 loss=7.01, nll_loss=3.953, mask_ins=1.203, word_ins_ml=5.393, word_reposition=0.413, ppl=128.89, wps=13676, ups=0.93, wpb=14739.3, bsz=1024, num_updates=13500, lr=0.00030429, gnorm=1.233, clip=0, loss_scale=1673, train_wall=106, wall=14301
2022-07-31 18:55:12 | INFO | train_inner | epoch 004:   2473 / 3715 loss=7.021, nll_loss=3.963, mask_ins=1.2, word_ins_ml=5.402, word_reposition=0.418, ppl=129.87, wps=13831, ups=0.94, wpb=14742.2, bsz=1024, num_updates=13600, lr=0.00030317, gnorm=1.182, clip=0, loss_scale=1024, train_wall=105, wall=14408
2022-07-31 18:57:00 | INFO | train_inner | epoch 004:   2573 / 3715 loss=7.005, nll_loss=3.954, mask_ins=1.202, word_ins_ml=5.395, word_reposition=0.408, ppl=128.42, wps=13501.7, ups=0.93, wpb=14573.5, bsz=1024, num_updates=13700, lr=0.000302061, gnorm=1.435, clip=0, loss_scale=1024, train_wall=106, wall=14516
2022-07-31 18:58:46 | INFO | train_inner | epoch 004:   2673 / 3715 loss=7.005, nll_loss=3.955, mask_ins=1.195, word_ins_ml=5.395, word_reposition=0.416, ppl=128.48, wps=13825.5, ups=0.94, wpb=14666.3, bsz=1024, num_updates=13800, lr=0.000300965, gnorm=1.119, clip=0, loss_scale=1024, train_wall=104, wall=14622
2022-07-31 19:00:33 | INFO | train_inner | epoch 004:   2773 / 3715 loss=6.998, nll_loss=3.953, mask_ins=1.192, word_ins_ml=5.393, word_reposition=0.413, ppl=127.79, wps=13623.7, ups=0.93, wpb=14573.7, bsz=1024, num_updates=13900, lr=0.00029988, gnorm=1.119, clip=0, loss_scale=1024, train_wall=105, wall=14729
2022-07-31 19:02:20 | INFO | train_inner | epoch 004:   2873 / 3715 loss=7.022, nll_loss=3.969, mask_ins=1.203, word_ins_ml=5.408, word_reposition=0.411, ppl=129.95, wps=13652.2, ups=0.94, wpb=14552.2, bsz=1024, num_updates=14000, lr=0.000298807, gnorm=1.218, clip=0, loss_scale=1280, train_wall=105, wall=14835
2022-07-31 19:04:07 | INFO | train_inner | epoch 004:   2973 / 3715 loss=7.005, nll_loss=3.966, mask_ins=1.204, word_ins_ml=5.404, word_reposition=0.397, ppl=128.45, wps=13703.7, ups=0.94, wpb=14611.2, bsz=1024, num_updates=14100, lr=0.000297746, gnorm=1.234, clip=0, loss_scale=2048, train_wall=105, wall=14942
2022-07-31 19:05:54 | INFO | train_inner | epoch 004:   3073 / 3715 loss=7.005, nll_loss=3.958, mask_ins=1.197, word_ins_ml=5.398, word_reposition=0.41, ppl=128.42, wps=13705.8, ups=0.94, wpb=14653.1, bsz=1024, num_updates=14200, lr=0.000296695, gnorm=1.21, clip=0, loss_scale=2048, train_wall=105, wall=15049
2022-07-31 19:07:41 | INFO | train_inner | epoch 004:   3173 / 3715 loss=7.001, nll_loss=3.957, mask_ins=1.19, word_ins_ml=5.397, word_reposition=0.415, ppl=128.09, wps=13658.5, ups=0.93, wpb=14620, bsz=1024, num_updates=14300, lr=0.000295656, gnorm=1.123, clip=0, loss_scale=2048, train_wall=105, wall=15156
2022-07-31 19:09:27 | INFO | train_inner | epoch 004:   3273 / 3715 loss=6.988, nll_loss=3.943, mask_ins=1.194, word_ins_ml=5.384, word_reposition=0.409, ppl=126.92, wps=13624.4, ups=0.94, wpb=14509.5, bsz=1024, num_updates=14400, lr=0.000294628, gnorm=1.096, clip=0, loss_scale=2048, train_wall=105, wall=15262
2022-07-31 19:11:14 | INFO | train_inner | epoch 004:   3373 / 3715 loss=6.985, nll_loss=3.944, mask_ins=1.198, word_ins_ml=5.385, word_reposition=0.402, ppl=126.7, wps=13635.5, ups=0.93, wpb=14594.9, bsz=1024, num_updates=14500, lr=0.00029361, gnorm=1.08, clip=0, loss_scale=2314, train_wall=105, wall=15370
2022-07-31 19:13:01 | INFO | train_inner | epoch 004:   3473 / 3715 loss=6.978, nll_loss=3.928, mask_ins=1.19, word_ins_ml=5.371, word_reposition=0.417, ppl=126.09, wps=13747.9, ups=0.93, wpb=14741.6, bsz=1024, num_updates=14600, lr=0.000292603, gnorm=1.101, clip=0, loss_scale=4096, train_wall=105, wall=15477
2022-07-31 19:14:49 | INFO | train_inner | epoch 004:   3573 / 3715 loss=7.003, nll_loss=3.951, mask_ins=1.197, word_ins_ml=5.391, word_reposition=0.415, ppl=128.23, wps=13662.7, ups=0.93, wpb=14733.6, bsz=1024, num_updates=14700, lr=0.000291606, gnorm=1.088, clip=0, loss_scale=4096, train_wall=106, wall=15585
2022-07-31 19:16:36 | INFO | train_inner | epoch 004:   3673 / 3715 loss=6.992, nll_loss=3.945, mask_ins=1.198, word_ins_ml=5.385, word_reposition=0.409, ppl=127.29, wps=13740.1, ups=0.94, wpb=14657.7, bsz=1024, num_updates=14800, lr=0.000290619, gnorm=1.157, clip=0, loss_scale=4096, train_wall=105, wall=15691
2022-07-31 19:17:20 | INFO | train | epoch 004 | loss 7.052 | nll_loss 3.989 | mask_ins 1.209 | word_ins_ml 5.426 | word_reposition 0.417 | ppl 132.71 | wps 13440 | ups 0.92 | wpb 14661.1 | bsz 1023.7 | num_updates 14842 | lr 0.000290208 | gnorm 1.15 | clip 0 | loss_scale 3270 | train_wall 3895 | wall 15735
2022-07-31 19:18:27 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.89 | nll_loss 3.733 | mask_ins 1.169 | word_ins_ml 5.308 | word_reposition 0.413 | ppl 118.58 | wps 41005.2 | wpb 1849.4 | bsz 127.9 | num_updates 14842 | best_loss 6.89
2022-07-31 19:18:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_best.pt (epoch 4 @ 14842 updates, score 6.89) (writing took 3.9275635927915573 seconds)
2022-07-31 19:19:33 | INFO | train_inner | epoch 005:     58 / 3715 loss=6.944, nll_loss=3.903, mask_ins=1.185, word_ins_ml=5.348, word_reposition=0.411, ppl=123.16, wps=8215.5, ups=0.56, wpb=14551.8, bsz=1014.7, num_updates=14900, lr=0.000289642, gnorm=1.111, clip=0, loss_scale=4096, train_wall=105, wall=15868
2022-07-31 19:21:20 | INFO | train_inner | epoch 005:    158 / 3715 loss=6.929, nll_loss=3.88, mask_ins=1.191, word_ins_ml=5.328, word_reposition=0.41, ppl=121.82, wps=13638, ups=0.93, wpb=14604.2, bsz=1024, num_updates=15000, lr=0.000288675, gnorm=1.117, clip=0, loss_scale=4137, train_wall=105, wall=15975
2022-07-31 19:21:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-31 19:23:09 | INFO | train_inner | epoch 005:    259 / 3715 loss=6.903, nll_loss=3.86, mask_ins=1.186, word_ins_ml=5.31, word_reposition=0.407, ppl=119.67, wps=13507.8, ups=0.92, wpb=14658.8, bsz=1024, num_updates=15100, lr=0.000287718, gnorm=1.12, clip=0, loss_scale=4623, train_wall=107, wall=16084
2022-07-31 19:24:56 | INFO | train_inner | epoch 005:    359 / 3715 loss=6.909, nll_loss=3.88, mask_ins=1.181, word_ins_ml=5.328, word_reposition=0.4, ppl=120.2, wps=13693.5, ups=0.93, wpb=14658.4, bsz=1024, num_updates=15200, lr=0.00028677, gnorm=1.109, clip=0, loss_scale=4096, train_wall=105, wall=16191
2022-07-31 19:25:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-31 19:26:43 | INFO | train_inner | epoch 005:    460 / 3715 loss=6.94, nll_loss=3.892, mask_ins=1.19, word_ins_ml=5.339, word_reposition=0.412, ppl=122.81, wps=13657.8, ups=0.93, wpb=14705, bsz=1024, num_updates=15300, lr=0.000285831, gnorm=1.269, clip=0, loss_scale=2555, train_wall=106, wall=16299
2022-07-31 19:28:31 | INFO | train_inner | epoch 005:    560 / 3715 loss=6.901, nll_loss=3.871, mask_ins=1.184, word_ins_ml=5.32, word_reposition=0.396, ppl=119.47, wps=13595.4, ups=0.93, wpb=14641.9, bsz=1023.8, num_updates=15400, lr=0.000284901, gnorm=1.195, clip=0, loss_scale=2048, train_wall=106, wall=16406
2022-07-31 19:30:17 | INFO | train_inner | epoch 005:    660 / 3715 loss=6.904, nll_loss=3.862, mask_ins=1.182, word_ins_ml=5.312, word_reposition=0.41, ppl=119.74, wps=13888.3, ups=0.94, wpb=14760.9, bsz=1024, num_updates=15500, lr=0.000283981, gnorm=1.223, clip=0, loss_scale=2048, train_wall=105, wall=16513
2022-07-31 19:32:04 | INFO | train_inner | epoch 005:    760 / 3715 loss=6.922, nll_loss=3.874, mask_ins=1.188, word_ins_ml=5.322, word_reposition=0.412, ppl=121.25, wps=13710, ups=0.93, wpb=14671.4, bsz=1024, num_updates=15600, lr=0.000283069, gnorm=1.118, clip=0, loss_scale=2048, train_wall=105, wall=16620
2022-07-31 19:33:51 | INFO | train_inner | epoch 005:    860 / 3715 loss=6.89, nll_loss=3.845, mask_ins=1.182, word_ins_ml=5.297, word_reposition=0.412, ppl=118.64, wps=13876.8, ups=0.93, wpb=14879.1, bsz=1024, num_updates=15700, lr=0.000282166, gnorm=1.102, clip=0, loss_scale=2048, train_wall=105, wall=16727
2022-07-31 19:35:39 | INFO | train_inner | epoch 005:    960 / 3715 loss=6.907, nll_loss=3.866, mask_ins=1.187, word_ins_ml=5.315, word_reposition=0.405, ppl=120.05, wps=13781.7, ups=0.93, wpb=14770.2, bsz=1024, num_updates=15800, lr=0.000281272, gnorm=1.114, clip=0, loss_scale=3359, train_wall=105, wall=16834
2022-07-31 19:37:26 | INFO | train_inner | epoch 005:   1060 / 3715 loss=6.907, nll_loss=3.878, mask_ins=1.179, word_ins_ml=5.326, word_reposition=0.402, ppl=119.98, wps=13595.2, ups=0.93, wpb=14565.5, bsz=1024, num_updates=15900, lr=0.000280386, gnorm=1.206, clip=0, loss_scale=4096, train_wall=105, wall=16941
2022-07-31 19:39:13 | INFO | train_inner | epoch 005:   1160 / 3715 loss=6.893, nll_loss=3.857, mask_ins=1.183, word_ins_ml=5.307, word_reposition=0.403, ppl=118.82, wps=13680.3, ups=0.93, wpb=14645.8, bsz=1024, num_updates=16000, lr=0.000279508, gnorm=1.148, clip=0, loss_scale=4096, train_wall=105, wall=17048
2022-07-31 19:39:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-31 19:41:01 | INFO | train_inner | epoch 005:   1261 / 3715 loss=6.916, nll_loss=3.874, mask_ins=1.185, word_ins_ml=5.323, word_reposition=0.408, ppl=120.73, wps=13623.2, ups=0.93, wpb=14691.1, bsz=1024, num_updates=16100, lr=0.000278639, gnorm=1.149, clip=0, loss_scale=2251, train_wall=106, wall=17156
2022-07-31 19:42:48 | INFO | train_inner | epoch 005:   1361 / 3715 loss=6.891, nll_loss=3.856, mask_ins=1.181, word_ins_ml=5.306, word_reposition=0.403, ppl=118.65, wps=13653.4, ups=0.93, wpb=14635.9, bsz=1024, num_updates=16200, lr=0.000277778, gnorm=1.162, clip=0, loss_scale=2048, train_wall=105, wall=17263
2022-07-31 19:44:36 | INFO | train_inner | epoch 005:   1461 / 3715 loss=6.89, nll_loss=3.857, mask_ins=1.178, word_ins_ml=5.307, word_reposition=0.405, ppl=118.56, wps=13497.1, ups=0.92, wpb=14649.9, bsz=1024, num_updates=16300, lr=0.000276924, gnorm=1.084, clip=0, loss_scale=2048, train_wall=107, wall=17372
2022-07-31 19:46:24 | INFO | train_inner | epoch 005:   1561 / 3715 loss=6.869, nll_loss=3.841, mask_ins=1.18, word_ins_ml=5.292, word_reposition=0.397, ppl=116.87, wps=13594.8, ups=0.93, wpb=14657.3, bsz=1024, num_updates=16400, lr=0.000276079, gnorm=1.101, clip=0, loss_scale=2048, train_wall=106, wall=17480
2022-07-31 19:48:11 | INFO | train_inner | epoch 005:   1661 / 3715 loss=6.852, nll_loss=3.834, mask_ins=1.167, word_ins_ml=5.286, word_reposition=0.399, ppl=115.53, wps=13789.5, ups=0.94, wpb=14692.9, bsz=1024, num_updates=16500, lr=0.000275241, gnorm=1.058, clip=0, loss_scale=2048, train_wall=105, wall=17586
2022-07-31 19:49:57 | INFO | train_inner | epoch 005:   1761 / 3715 loss=6.893, nll_loss=3.862, mask_ins=1.175, word_ins_ml=5.311, word_reposition=0.407, ppl=118.87, wps=13758.8, ups=0.94, wpb=14588.9, bsz=1024, num_updates=16600, lr=0.000274411, gnorm=1.072, clip=0, loss_scale=3666, train_wall=104, wall=17692
2022-07-31 19:51:44 | INFO | train_inner | epoch 005:   1861 / 3715 loss=6.869, nll_loss=3.849, mask_ins=1.173, word_ins_ml=5.299, word_reposition=0.396, ppl=116.86, wps=13661.6, ups=0.93, wpb=14619.3, bsz=1024, num_updates=16700, lr=0.000273588, gnorm=1.044, clip=0, loss_scale=4096, train_wall=105, wall=17799
2022-07-31 19:53:17 | INFO | train_inner | epoch 005:   1961 / 3715 loss=6.86, nll_loss=3.829, mask_ins=1.176, word_ins_ml=5.282, word_reposition=0.402, ppl=116.15, wps=15764.1, ups=1.07, wpb=14697.3, bsz=1024, num_updates=16800, lr=0.000272772, gnorm=1.088, clip=0, loss_scale=4096, train_wall=92, wall=17893
2022-07-31 19:54:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-31 19:55:04 | INFO | train_inner | epoch 005:   2062 / 3715 loss=6.872, nll_loss=3.85, mask_ins=1.176, word_ins_ml=5.3, word_reposition=0.395, ppl=117.09, wps=13702.7, ups=0.93, wpb=14692.7, bsz=1024, num_updates=16900, lr=0.000271964, gnorm=1.064, clip=0, loss_scale=3447, train_wall=105, wall=18000
2022-07-31 19:56:51 | INFO | train_inner | epoch 005:   2162 / 3715 loss=6.86, nll_loss=3.831, mask_ins=1.178, word_ins_ml=5.283, word_reposition=0.399, ppl=116.19, wps=13807, ups=0.94, wpb=14671.9, bsz=1024, num_updates=17000, lr=0.000271163, gnorm=1.067, clip=0, loss_scale=2048, train_wall=105, wall=18106
2022-07-31 19:57:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-31 19:58:38 | INFO | train_inner | epoch 005:   2263 / 3715 loss=6.865, nll_loss=3.837, mask_ins=1.179, word_ins_ml=5.289, word_reposition=0.398, ppl=116.57, wps=13531.4, ups=0.93, wpb=14552.3, bsz=1024, num_updates=17100, lr=0.000270369, gnorm=1.578, clip=0, loss_scale=1582, train_wall=106, wall=18214
2022-07-31 20:00:24 | INFO | train_inner | epoch 005:   2363 / 3715 loss=6.852, nll_loss=3.824, mask_ins=1.167, word_ins_ml=5.276, word_reposition=0.409, ppl=115.52, wps=13944.2, ups=0.94, wpb=14827, bsz=1024, num_updates=17200, lr=0.000269582, gnorm=1.162, clip=0, loss_scale=1024, train_wall=105, wall=18320
2022-07-31 20:02:12 | INFO | train_inner | epoch 005:   2463 / 3715 loss=6.843, nll_loss=3.824, mask_ins=1.172, word_ins_ml=5.276, word_reposition=0.395, ppl=114.82, wps=13687.2, ups=0.93, wpb=14655, bsz=1024, num_updates=17300, lr=0.000268802, gnorm=1.089, clip=0, loss_scale=1024, train_wall=105, wall=18427
2022-07-31 20:03:58 | INFO | train_inner | epoch 005:   2563 / 3715 loss=6.854, nll_loss=3.829, mask_ins=1.172, word_ins_ml=5.281, word_reposition=0.402, ppl=115.68, wps=13762.8, ups=0.94, wpb=14711.6, bsz=1024, num_updates=17400, lr=0.000268028, gnorm=1.07, clip=0, loss_scale=1024, train_wall=105, wall=18534
2022-07-31 20:05:45 | INFO | train_inner | epoch 005:   2663 / 3715 loss=6.845, nll_loss=3.835, mask_ins=1.166, word_ins_ml=5.287, word_reposition=0.392, ppl=114.95, wps=13675, ups=0.93, wpb=14625.9, bsz=1024, num_updates=17500, lr=0.000267261, gnorm=1.056, clip=0, loss_scale=1024, train_wall=105, wall=18641
2022-07-31 20:07:33 | INFO | train_inner | epoch 005:   2763 / 3715 loss=6.852, nll_loss=3.834, mask_ins=1.17, word_ins_ml=5.285, word_reposition=0.397, ppl=115.5, wps=13693.1, ups=0.93, wpb=14673.1, bsz=1024, num_updates=17600, lr=0.000266501, gnorm=1.069, clip=0, loss_scale=1372, train_wall=105, wall=18748
2022-07-31 20:09:20 | INFO | train_inner | epoch 005:   2863 / 3715 loss=6.845, nll_loss=3.827, mask_ins=1.17, word_ins_ml=5.279, word_reposition=0.396, ppl=114.98, wps=13702.2, ups=0.93, wpb=14687.3, bsz=1024, num_updates=17700, lr=0.000265747, gnorm=1.033, clip=0, loss_scale=2048, train_wall=105, wall=18855
2022-07-31 20:11:07 | INFO | train_inner | epoch 005:   2963 / 3715 loss=6.82, nll_loss=3.805, mask_ins=1.166, word_ins_ml=5.26, word_reposition=0.395, ppl=112.99, wps=13708.4, ups=0.93, wpb=14662.9, bsz=1024, num_updates=17800, lr=0.000264999, gnorm=1.045, clip=0, loss_scale=2048, train_wall=105, wall=18962
2022-07-31 20:12:55 | INFO | train_inner | epoch 005:   3063 / 3715 loss=6.846, nll_loss=3.823, mask_ins=1.171, word_ins_ml=5.275, word_reposition=0.4, ppl=115.04, wps=13627.5, ups=0.93, wpb=14693.1, bsz=1024, num_updates=17900, lr=0.000264258, gnorm=1.056, clip=0, loss_scale=2048, train_wall=106, wall=19070
2022-07-31 20:14:41 | INFO | train_inner | epoch 005:   3163 / 3715 loss=6.816, nll_loss=3.799, mask_ins=1.16, word_ins_ml=5.254, word_reposition=0.402, ppl=112.69, wps=13681, ups=0.94, wpb=14613.4, bsz=1024, num_updates=18000, lr=0.000263523, gnorm=1.067, clip=0, loss_scale=2048, train_wall=105, wall=19177
2022-07-31 20:16:29 | INFO | train_inner | epoch 005:   3263 / 3715 loss=6.808, nll_loss=3.789, mask_ins=1.165, word_ins_ml=5.245, word_reposition=0.398, ppl=112.07, wps=13582.5, ups=0.93, wpb=14598.1, bsz=1024, num_updates=18100, lr=0.000262794, gnorm=1.055, clip=0, loss_scale=2499, train_wall=106, wall=19284
2022-07-31 20:18:17 | INFO | train_inner | epoch 005:   3363 / 3715 loss=6.841, nll_loss=3.825, mask_ins=1.164, word_ins_ml=5.277, word_reposition=0.4, ppl=114.62, wps=13559.5, ups=0.93, wpb=14651.8, bsz=1024, num_updates=18200, lr=0.000262071, gnorm=1.063, clip=0, loss_scale=4096, train_wall=106, wall=19392
2022-07-31 20:20:04 | INFO | train_inner | epoch 005:   3463 / 3715 loss=6.832, nll_loss=3.815, mask_ins=1.167, word_ins_ml=5.268, word_reposition=0.397, ppl=113.91, wps=13732.6, ups=0.93, wpb=14696.7, bsz=1024, num_updates=18300, lr=0.000261354, gnorm=1.061, clip=0, loss_scale=4096, train_wall=105, wall=19499
2022-07-31 20:21:51 | INFO | train_inner | epoch 005:   3563 / 3715 loss=6.822, nll_loss=3.804, mask_ins=1.165, word_ins_ml=5.258, word_reposition=0.399, ppl=113.14, wps=13652.8, ups=0.93, wpb=14611.9, bsz=1024, num_updates=18400, lr=0.000260643, gnorm=1.038, clip=0, loss_scale=4096, train_wall=105, wall=19606
2022-07-31 20:22:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-31 20:23:39 | INFO | train_inner | epoch 005:   3664 / 3715 loss=6.808, nll_loss=3.787, mask_ins=1.166, word_ins_ml=5.243, word_reposition=0.399, ppl=112.02, wps=13488.2, ups=0.92, wpb=14617.5, bsz=1024, num_updates=18500, lr=0.000259938, gnorm=1.062, clip=0, loss_scale=2778, train_wall=107, wall=19715
2022-07-31 20:24:33 | INFO | train | epoch 005 | loss 6.87 | nll_loss 3.842 | mask_ins 1.175 | word_ins_ml 5.293 | word_reposition 0.402 | ppl 116.99 | wps 13484 | ups 0.92 | wpb 14661.4 | bsz 1023.7 | num_updates 18551 | lr 0.00025958 | gnorm 1.114 | clip 0 | loss_scale 2674 | train_wall 3897 | wall 19768
2022-07-31 20:25:54 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.792 | nll_loss 3.637 | mask_ins 1.164 | word_ins_ml 5.215 | word_reposition 0.413 | ppl 110.82 | wps 33820.9 | wpb 1849.4 | bsz 127.9 | num_updates 18551 | best_loss 6.792
2022-07-31 20:25:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_best.pt (epoch 5 @ 18551 updates, score 6.792) (writing took 4.428579673171043 seconds)
2022-07-31 20:26:51 | INFO | train_inner | epoch 006:     49 / 3715 loss=6.784, nll_loss=3.78, mask_ins=1.155, word_ins_ml=5.237, word_reposition=0.392, ppl=110.23, wps=7509.5, ups=0.52, wpb=14409.4, bsz=1014.7, num_updates=18600, lr=0.000259238, gnorm=1.074, clip=0, loss_scale=2048, train_wall=105, wall=19907
2022-07-31 20:28:24 | INFO | train_inner | epoch 006:    149 / 3715 loss=6.759, nll_loss=3.747, mask_ins=1.159, word_ins_ml=5.208, word_reposition=0.392, ppl=108.33, wps=15865.7, ups=1.08, wpb=14743, bsz=1024, num_updates=18700, lr=0.000258544, gnorm=1.072, clip=0, loss_scale=2048, train_wall=91, wall=20000
2022-07-31 20:30:10 | INFO | train_inner | epoch 006:    249 / 3715 loss=6.755, nll_loss=3.746, mask_ins=1.159, word_ins_ml=5.207, word_reposition=0.389, ppl=108.02, wps=13748.8, ups=0.94, wpb=14588.1, bsz=1023.8, num_updates=18800, lr=0.000257855, gnorm=1.035, clip=0, loss_scale=2048, train_wall=104, wall=20106
2022-07-31 20:31:57 | INFO | train_inner | epoch 006:    349 / 3715 loss=6.76, nll_loss=3.747, mask_ins=1.161, word_ins_ml=5.208, word_reposition=0.391, ppl=108.36, wps=13713.1, ups=0.93, wpb=14686.5, bsz=1024, num_updates=18900, lr=0.000257172, gnorm=1.033, clip=0, loss_scale=2048, train_wall=105, wall=20213
2022-07-31 20:33:44 | INFO | train_inner | epoch 006:    449 / 3715 loss=6.781, nll_loss=3.765, mask_ins=1.16, word_ins_ml=5.224, word_reposition=0.397, ppl=109.98, wps=13589.6, ups=0.93, wpb=14566.2, bsz=1024, num_updates=19000, lr=0.000256495, gnorm=1.065, clip=0, loss_scale=3133, train_wall=105, wall=20320
2022-07-31 20:35:31 | INFO | train_inner | epoch 006:    549 / 3715 loss=6.79, nll_loss=3.779, mask_ins=1.164, word_ins_ml=5.236, word_reposition=0.389, ppl=110.62, wps=13629.9, ups=0.93, wpb=14578.3, bsz=1024, num_updates=19100, lr=0.000255822, gnorm=1.075, clip=0, loss_scale=4096, train_wall=105, wall=20427
2022-07-31 20:37:18 | INFO | train_inner | epoch 006:    649 / 3715 loss=6.765, nll_loss=3.751, mask_ins=1.161, word_ins_ml=5.211, word_reposition=0.393, ppl=108.76, wps=13873.8, ups=0.94, wpb=14744.3, bsz=1024, num_updates=19200, lr=0.000255155, gnorm=1.043, clip=0, loss_scale=4096, train_wall=105, wall=20533
2022-07-31 20:37:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-31 20:39:06 | INFO | train_inner | epoch 006:    750 / 3715 loss=6.729, nll_loss=3.717, mask_ins=1.151, word_ins_ml=5.181, word_reposition=0.396, ppl=106.05, wps=13636.2, ups=0.92, wpb=14816.6, bsz=1024, num_updates=19300, lr=0.000254493, gnorm=1.081, clip=0, loss_scale=2595, train_wall=107, wall=20642
2022-07-31 20:40:54 | INFO | train_inner | epoch 006:    850 / 3715 loss=6.76, nll_loss=3.753, mask_ins=1.159, word_ins_ml=5.213, word_reposition=0.388, ppl=108.39, wps=13679.9, ups=0.93, wpb=14710.6, bsz=1024, num_updates=19400, lr=0.000253837, gnorm=1.123, clip=0, loss_scale=2048, train_wall=106, wall=20749
2022-07-31 20:42:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-31 20:42:42 | INFO | train_inner | epoch 006:    951 / 3715 loss=6.766, nll_loss=3.756, mask_ins=1.157, word_ins_ml=5.215, word_reposition=0.394, ppl=108.86, wps=13517.8, ups=0.92, wpb=14660.9, bsz=1024, num_updates=19500, lr=0.000253185, gnorm=1.049, clip=0, loss_scale=1805, train_wall=107, wall=20858
2022-07-31 20:44:29 | INFO | train_inner | epoch 006:   1051 / 3715 loss=6.783, nll_loss=3.77, mask_ins=1.159, word_ins_ml=5.228, word_reposition=0.396, ppl=110.1, wps=13789.4, ups=0.93, wpb=14753.1, bsz=1024, num_updates=19600, lr=0.000252538, gnorm=1.051, clip=0, loss_scale=1024, train_wall=105, wall=20965
2022-07-31 20:46:18 | INFO | train_inner | epoch 006:   1151 / 3715 loss=6.757, nll_loss=3.751, mask_ins=1.156, word_ins_ml=5.21, word_reposition=0.39, ppl=108.12, wps=13651.3, ups=0.92, wpb=14786.7, bsz=1024, num_updates=19700, lr=0.000251896, gnorm=1.151, clip=0, loss_scale=1024, train_wall=107, wall=21073
2022-07-31 20:48:05 | INFO | train_inner | epoch 006:   1251 / 3715 loss=6.745, nll_loss=3.739, mask_ins=1.153, word_ins_ml=5.2, word_reposition=0.393, ppl=107.3, wps=13705.6, ups=0.93, wpb=14671.2, bsz=1024, num_updates=19800, lr=0.000251259, gnorm=1.067, clip=0, loss_scale=1024, train_wall=105, wall=21180
2022-07-31 20:49:51 | INFO | train_inner | epoch 006:   1351 / 3715 loss=6.748, nll_loss=3.741, mask_ins=1.154, word_ins_ml=5.202, word_reposition=0.392, ppl=107.48, wps=13720.5, ups=0.94, wpb=14620.6, bsz=1024, num_updates=19900, lr=0.000250627, gnorm=1.037, clip=0, loss_scale=1024, train_wall=105, wall=21287
2022-07-31 20:51:38 | INFO | train_inner | epoch 006:   1451 / 3715 loss=6.769, nll_loss=3.763, mask_ins=1.158, word_ins_ml=5.221, word_reposition=0.39, ppl=109.06, wps=13674.7, ups=0.94, wpb=14584.6, bsz=1024, num_updates=20000, lr=0.00025, gnorm=1.039, clip=0, loss_scale=1147, train_wall=105, wall=21393
2022-07-31 20:53:25 | INFO | train_inner | epoch 006:   1551 / 3715 loss=6.752, nll_loss=3.745, mask_ins=1.161, word_ins_ml=5.205, word_reposition=0.386, ppl=107.8, wps=13837.7, ups=0.94, wpb=14780.3, bsz=1024, num_updates=20100, lr=0.000249377, gnorm=1.051, clip=0, loss_scale=2048, train_wall=105, wall=21500
2022-07-31 20:55:12 | INFO | train_inner | epoch 006:   1651 / 3715 loss=6.757, nll_loss=3.744, mask_ins=1.157, word_ins_ml=5.204, word_reposition=0.396, ppl=108.19, wps=13631.5, ups=0.93, wpb=14665.2, bsz=1024, num_updates=20200, lr=0.000248759, gnorm=1.059, clip=0, loss_scale=2048, train_wall=106, wall=21608
2022-07-31 20:56:59 | INFO | train_inner | epoch 006:   1751 / 3715 loss=6.752, nll_loss=3.744, mask_ins=1.158, word_ins_ml=5.204, word_reposition=0.391, ppl=107.78, wps=13796, ups=0.94, wpb=14709.9, bsz=1024, num_updates=20300, lr=0.000248146, gnorm=1.046, clip=0, loss_scale=2048, train_wall=105, wall=21714
2022-07-31 20:58:47 | INFO | train_inner | epoch 006:   1851 / 3715 loss=6.733, nll_loss=3.739, mask_ins=1.15, word_ins_ml=5.2, word_reposition=0.383, ppl=106.39, wps=13524.6, ups=0.93, wpb=14560.8, bsz=1024, num_updates=20400, lr=0.000247537, gnorm=1.026, clip=0, loss_scale=2048, train_wall=106, wall=21822
2022-07-31 21:00:33 | INFO | train_inner | epoch 006:   1951 / 3715 loss=6.769, nll_loss=3.756, mask_ins=1.162, word_ins_ml=5.215, word_reposition=0.392, ppl=109.09, wps=13728.4, ups=0.94, wpb=14668.9, bsz=1024, num_updates=20500, lr=0.000246932, gnorm=1.057, clip=0, loss_scale=2048, train_wall=105, wall=21929
2022-07-31 21:02:21 | INFO | train_inner | epoch 006:   2051 / 3715 loss=6.766, nll_loss=3.756, mask_ins=1.164, word_ins_ml=5.215, word_reposition=0.388, ppl=108.83, wps=13662.9, ups=0.93, wpb=14688.3, bsz=1024, num_updates=20600, lr=0.000246332, gnorm=1.068, clip=0, loss_scale=4096, train_wall=106, wall=22036
2022-07-31 21:04:08 | INFO | train_inner | epoch 006:   2151 / 3715 loss=6.727, nll_loss=3.725, mask_ins=1.152, word_ins_ml=5.187, word_reposition=0.388, ppl=105.95, wps=13608.6, ups=0.93, wpb=14586.9, bsz=1024, num_updates=20700, lr=0.000245737, gnorm=1.033, clip=0, loss_scale=4096, train_wall=105, wall=22144
2022-07-31 21:05:39 | INFO | train_inner | epoch 006:   2251 / 3715 loss=6.741, nll_loss=3.74, mask_ins=1.147, word_ins_ml=5.2, word_reposition=0.394, ppl=106.96, wps=16142.4, ups=1.09, wpb=14742.1, bsz=1024, num_updates=20800, lr=0.000245145, gnorm=1.035, clip=0, loss_scale=4096, train_wall=90, wall=22235
2022-07-31 21:07:26 | INFO | train_inner | epoch 006:   2351 / 3715 loss=6.724, nll_loss=3.725, mask_ins=1.149, word_ins_ml=5.187, word_reposition=0.388, ppl=105.71, wps=13716.9, ups=0.94, wpb=14606.1, bsz=1024, num_updates=20900, lr=0.000244558, gnorm=1.049, clip=0, loss_scale=4096, train_wall=105, wall=22341
2022-07-31 21:09:13 | INFO | train_inner | epoch 006:   2451 / 3715 loss=6.726, nll_loss=3.714, mask_ins=1.15, word_ins_ml=5.178, word_reposition=0.399, ppl=105.88, wps=13770.3, ups=0.94, wpb=14727.2, bsz=1024, num_updates=21000, lr=0.000243975, gnorm=1.042, clip=0, loss_scale=4096, train_wall=105, wall=22448
2022-07-31 21:11:00 | INFO | train_inner | epoch 006:   2551 / 3715 loss=6.723, nll_loss=3.725, mask_ins=1.151, word_ins_ml=5.187, word_reposition=0.385, ppl=105.65, wps=13745.3, ups=0.94, wpb=14656.1, bsz=1024, num_updates=21100, lr=0.000243396, gnorm=1.052, clip=0, loss_scale=7700, train_wall=105, wall=22555
2022-07-31 21:12:47 | INFO | train_inner | epoch 006:   2651 / 3715 loss=6.716, nll_loss=3.716, mask_ins=1.145, word_ins_ml=5.179, word_reposition=0.392, ppl=105.11, wps=13674.6, ups=0.93, wpb=14647.2, bsz=1024, num_updates=21200, lr=0.000242821, gnorm=1.018, clip=0, loss_scale=8192, train_wall=105, wall=22662
2022-07-31 21:14:34 | INFO | train_inner | epoch 006:   2751 / 3715 loss=6.708, nll_loss=3.706, mask_ins=1.152, word_ins_ml=5.17, word_reposition=0.385, ppl=104.54, wps=13520.2, ups=0.93, wpb=14568.3, bsz=1024, num_updates=21300, lr=0.000242251, gnorm=1.028, clip=0, loss_scale=8192, train_wall=106, wall=22770
2022-07-31 21:16:22 | INFO | train_inner | epoch 006:   2851 / 3715 loss=6.731, nll_loss=3.74, mask_ins=1.148, word_ins_ml=5.2, word_reposition=0.382, ppl=106.2, wps=13672, ups=0.93, wpb=14673.7, bsz=1024, num_updates=21400, lr=0.000241684, gnorm=1.046, clip=0, loss_scale=8192, train_wall=106, wall=22877
2022-07-31 21:17:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-31 21:18:10 | INFO | train_inner | epoch 006:   2952 / 3715 loss=6.711, nll_loss=3.713, mask_ins=1.146, word_ins_ml=5.176, word_reposition=0.389, ppl=104.79, wps=13513.4, ups=0.92, wpb=14612.6, bsz=1024, num_updates=21500, lr=0.000241121, gnorm=1.062, clip=0, loss_scale=6326, train_wall=106, wall=22985
2022-07-31 21:19:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-31 21:19:58 | INFO | train_inner | epoch 006:   3053 / 3715 loss=6.726, nll_loss=3.73, mask_ins=1.149, word_ins_ml=5.191, word_reposition=0.386, ppl=105.85, wps=13647.4, ups=0.93, wpb=14700, bsz=1024, num_updates=21600, lr=0.000240563, gnorm=1.065, clip=0, loss_scale=3042, train_wall=106, wall=23093
2022-07-31 21:21:44 | INFO | train_inner | epoch 006:   3153 / 3715 loss=6.717, nll_loss=3.714, mask_ins=1.153, word_ins_ml=5.177, word_reposition=0.386, ppl=105.17, wps=13767, ups=0.94, wpb=14626.6, bsz=1024, num_updates=21700, lr=0.000240008, gnorm=1.081, clip=0, loss_scale=2048, train_wall=105, wall=23199
2022-07-31 21:23:31 | INFO | train_inner | epoch 006:   3253 / 3715 loss=6.73, nll_loss=3.738, mask_ins=1.146, word_ins_ml=5.198, word_reposition=0.386, ppl=106.13, wps=13602.8, ups=0.93, wpb=14610.2, bsz=1024, num_updates=21800, lr=0.000239457, gnorm=1.01, clip=0, loss_scale=2048, train_wall=106, wall=23307
2022-07-31 21:25:18 | INFO | train_inner | epoch 006:   3353 / 3715 loss=6.717, nll_loss=3.727, mask_ins=1.147, word_ins_ml=5.188, word_reposition=0.383, ppl=105.23, wps=13726.4, ups=0.94, wpb=14617.2, bsz=1024, num_updates=21900, lr=0.000238909, gnorm=1.053, clip=0, loss_scale=2048, train_wall=105, wall=23413
2022-07-31 21:27:05 | INFO | train_inner | epoch 006:   3453 / 3715 loss=6.726, nll_loss=3.73, mask_ins=1.151, word_ins_ml=5.191, word_reposition=0.385, ppl=105.89, wps=13711.6, ups=0.93, wpb=14675, bsz=1024, num_updates=22000, lr=0.000238366, gnorm=1.029, clip=0, loss_scale=2048, train_wall=105, wall=23520
2022-07-31 21:28:51 | INFO | train_inner | epoch 006:   3553 / 3715 loss=6.696, nll_loss=3.697, mask_ins=1.144, word_ins_ml=5.162, word_reposition=0.39, ppl=103.65, wps=13798.1, ups=0.94, wpb=14724.7, bsz=1024, num_updates=22100, lr=0.000237826, gnorm=1.034, clip=0, loss_scale=2867, train_wall=105, wall=23627
2022-07-31 21:30:39 | INFO | train_inner | epoch 006:   3653 / 3715 loss=6.697, nll_loss=3.705, mask_ins=1.147, word_ins_ml=5.168, word_reposition=0.381, ppl=103.73, wps=13693.4, ups=0.93, wpb=14656.3, bsz=1024, num_updates=22200, lr=0.000237289, gnorm=1.031, clip=0, loss_scale=4096, train_wall=105, wall=23734
2022-07-31 21:31:45 | INFO | train | epoch 006 | loss 6.742 | nll_loss 3.738 | mask_ins 1.154 | word_ins_ml 5.199 | word_reposition 0.389 | ppl 107.02 | wps 13494.1 | ups 0.92 | wpb 14661.8 | bsz 1023.7 | num_updates 22262 | lr 0.000236959 | gnorm 1.053 | clip 0 | loss_scale 3240 | train_wall 3882 | wall 23800
2022-07-31 21:33:07 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.712 | nll_loss 3.576 | mask_ins 1.142 | word_ins_ml 5.151 | word_reposition 0.419 | ppl 104.83 | wps 33610.9 | wpb 1849.4 | bsz 127.9 | num_updates 22262 | best_loss 6.712
2022-07-31 21:33:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_best.pt (epoch 6 @ 22262 updates, score 6.712) (writing took 4.26386365108192 seconds)
2022-07-31 21:33:52 | INFO | train_inner | epoch 007:     38 / 3715 loss=6.695, nll_loss=3.703, mask_ins=1.147, word_ins_ml=5.167, word_reposition=0.382, ppl=103.61, wps=7488.2, ups=0.52, wpb=14489.3, bsz=1014.7, num_updates=22300, lr=0.000236757, gnorm=1.037, clip=0, loss_scale=4096, train_wall=106, wall=23927
2022-07-31 21:35:40 | INFO | train_inner | epoch 007:    138 / 3715 loss=6.652, nll_loss=3.66, mask_ins=1.138, word_ins_ml=5.13, word_reposition=0.385, ppl=100.58, wps=13543.8, ups=0.93, wpb=14617.9, bsz=1024, num_updates=22400, lr=0.000236228, gnorm=1.028, clip=0, loss_scale=4096, train_wall=106, wall=24035
2022-07-31 21:37:26 | INFO | train_inner | epoch 007:    238 / 3715 loss=6.682, nll_loss=3.689, mask_ins=1.143, word_ins_ml=5.155, word_reposition=0.383, ppl=102.64, wps=13798.3, ups=0.95, wpb=14584, bsz=1024, num_updates=22500, lr=0.000235702, gnorm=1.013, clip=0, loss_scale=4096, train_wall=104, wall=24141
2022-07-31 21:39:11 | INFO | train_inner | epoch 007:    338 / 3715 loss=6.663, nll_loss=3.659, mask_ins=1.142, word_ins_ml=5.129, word_reposition=0.392, ppl=101.31, wps=13945, ups=0.94, wpb=14757.7, bsz=1024, num_updates=22600, lr=0.00023518, gnorm=1.027, clip=0, loss_scale=5243, train_wall=104, wall=24247
2022-07-31 21:40:59 | INFO | train_inner | epoch 007:    438 / 3715 loss=6.675, nll_loss=3.687, mask_ins=1.142, word_ins_ml=5.153, word_reposition=0.38, ppl=102.18, wps=13547.6, ups=0.93, wpb=14622.6, bsz=1024, num_updates=22700, lr=0.000234662, gnorm=1.029, clip=0, loss_scale=8192, train_wall=106, wall=24355
2022-07-31 21:42:43 | INFO | train_inner | epoch 007:    538 / 3715 loss=6.664, nll_loss=3.668, mask_ins=1.139, word_ins_ml=5.137, word_reposition=0.388, ppl=101.4, wps=14169.1, ups=0.97, wpb=14646.9, bsz=1024, num_updates=22800, lr=0.000234146, gnorm=1.034, clip=0, loss_scale=8192, train_wall=102, wall=24458
2022-07-31 21:44:19 | INFO | train_inner | epoch 007:    638 / 3715 loss=6.67, nll_loss=3.678, mask_ins=1.138, word_ins_ml=5.145, word_reposition=0.386, ppl=101.81, wps=15227, ups=1.04, wpb=14693.7, bsz=1024, num_updates=22900, lr=0.000233635, gnorm=1.032, clip=0, loss_scale=8192, train_wall=95, wall=24555
2022-07-31 21:46:06 | INFO | train_inner | epoch 007:    738 / 3715 loss=6.654, nll_loss=3.671, mask_ins=1.135, word_ins_ml=5.139, word_reposition=0.38, ppl=100.67, wps=13681.3, ups=0.93, wpb=14648.8, bsz=1024, num_updates=23000, lr=0.000233126, gnorm=1.003, clip=0, loss_scale=8192, train_wall=105, wall=24662
2022-07-31 21:47:54 | INFO | train_inner | epoch 007:    838 / 3715 loss=6.683, nll_loss=3.684, mask_ins=1.148, word_ins_ml=5.15, word_reposition=0.385, ppl=102.76, wps=13686.3, ups=0.93, wpb=14671.3, bsz=1024, num_updates=23100, lr=0.000232621, gnorm=1.068, clip=0, loss_scale=9503, train_wall=105, wall=24769
2022-07-31 21:49:43 | INFO | train_inner | epoch 007:    938 / 3715 loss=6.68, nll_loss=3.678, mask_ins=1.142, word_ins_ml=5.145, word_reposition=0.393, ppl=102.52, wps=13421.1, ups=0.91, wpb=14677.6, bsz=1024, num_updates=23200, lr=0.000232119, gnorm=1.041, clip=0, loss_scale=16384, train_wall=108, wall=24878
2022-07-31 21:50:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-31 21:51:33 | INFO | train_inner | epoch 007:   1039 / 3715 loss=6.666, nll_loss=3.674, mask_ins=1.139, word_ins_ml=5.141, word_reposition=0.386, ppl=101.55, wps=13243.6, ups=0.9, wpb=14641.6, bsz=1024, num_updates=23300, lr=0.000231621, gnorm=1.019, clip=0, loss_scale=11031, train_wall=109, wall=24989
2022-07-31 21:53:23 | INFO | train_inner | epoch 007:   1139 / 3715 loss=6.669, nll_loss=3.666, mask_ins=1.141, word_ins_ml=5.134, word_reposition=0.393, ppl=101.73, wps=13487.8, ups=0.91, wpb=14792.3, bsz=1024, num_updates=23400, lr=0.000231125, gnorm=0.996, clip=0, loss_scale=8192, train_wall=108, wall=25099
2022-07-31 21:55:13 | INFO | train_inner | epoch 007:   1239 / 3715 loss=6.659, nll_loss=3.667, mask_ins=1.14, word_ins_ml=5.135, word_reposition=0.384, ppl=101.06, wps=13391.2, ups=0.91, wpb=14708.7, bsz=1024, num_updates=23500, lr=0.000230633, gnorm=1.023, clip=0, loss_scale=8192, train_wall=108, wall=25208
2022-07-31 21:57:03 | INFO | train_inner | epoch 007:   1339 / 3715 loss=6.647, nll_loss=3.662, mask_ins=1.135, word_ins_ml=5.13, word_reposition=0.382, ppl=100.19, wps=13404, ups=0.91, wpb=14691.4, bsz=1024, num_updates=23600, lr=0.000230144, gnorm=1.057, clip=0, loss_scale=8192, train_wall=108, wall=25318
2022-07-31 21:58:50 | INFO | train_inner | epoch 007:   1439 / 3715 loss=6.652, nll_loss=3.666, mask_ins=1.137, word_ins_ml=5.134, word_reposition=0.381, ppl=100.54, wps=13724.2, ups=0.93, wpb=14756.9, bsz=1024, num_updates=23700, lr=0.000229658, gnorm=1.014, clip=0, loss_scale=8192, train_wall=106, wall=25426
2022-07-31 22:00:39 | INFO | train_inner | epoch 007:   1539 / 3715 loss=6.64, nll_loss=3.653, mask_ins=1.137, word_ins_ml=5.122, word_reposition=0.381, ppl=99.71, wps=13436.5, ups=0.92, wpb=14620.9, bsz=1024, num_updates=23800, lr=0.000229175, gnorm=1.007, clip=0, loss_scale=12616, train_wall=107, wall=25534
2022-07-31 22:00:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-31 22:02:28 | INFO | train_inner | epoch 007:   1640 / 3715 loss=6.655, nll_loss=3.667, mask_ins=1.141, word_ins_ml=5.135, word_reposition=0.38, ppl=100.81, wps=13446.2, ups=0.92, wpb=14673.3, bsz=1024, num_updates=23900, lr=0.000228695, gnorm=1.016, clip=0, loss_scale=9409, train_wall=107, wall=25643
2022-07-31 22:04:18 | INFO | train_inner | epoch 007:   1740 / 3715 loss=6.636, nll_loss=3.655, mask_ins=1.128, word_ins_ml=5.124, word_reposition=0.384, ppl=99.43, wps=13272.9, ups=0.91, wpb=14630.7, bsz=1024, num_updates=24000, lr=0.000228218, gnorm=1.006, clip=0, loss_scale=8192, train_wall=108, wall=25754
2022-07-31 22:05:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-31 22:06:09 | INFO | train_inner | epoch 007:   1841 / 3715 loss=6.659, nll_loss=3.669, mask_ins=1.144, word_ins_ml=5.136, word_reposition=0.379, ppl=101.04, wps=13128.7, ups=0.9, wpb=14595.1, bsz=1024, num_updates=24100, lr=0.000227744, gnorm=1.025, clip=0, loss_scale=7340, train_wall=109, wall=25865
2022-07-31 22:08:00 | INFO | train_inner | epoch 007:   1941 / 3715 loss=6.657, nll_loss=3.676, mask_ins=1.133, word_ins_ml=5.142, word_reposition=0.381, ppl=100.88, wps=13223.3, ups=0.91, wpb=14591.1, bsz=1024, num_updates=24200, lr=0.000227273, gnorm=1.026, clip=0, loss_scale=4096, train_wall=108, wall=25975
2022-07-31 22:09:51 | INFO | train_inner | epoch 007:   2041 / 3715 loss=6.632, nll_loss=3.646, mask_ins=1.136, word_ins_ml=5.116, word_reposition=0.38, ppl=99.17, wps=13261.2, ups=0.9, wpb=14717.9, bsz=1024, num_updates=24300, lr=0.000226805, gnorm=1.018, clip=0, loss_scale=4096, train_wall=109, wall=26086
2022-07-31 22:11:42 | INFO | train_inner | epoch 007:   2141 / 3715 loss=6.639, nll_loss=3.655, mask_ins=1.133, word_ins_ml=5.124, word_reposition=0.381, ppl=99.64, wps=13012.4, ups=0.9, wpb=14536.1, bsz=1023.8, num_updates=24400, lr=0.000226339, gnorm=1.021, clip=0, loss_scale=4096, train_wall=110, wall=26198
2022-07-31 22:13:33 | INFO | train_inner | epoch 007:   2241 / 3715 loss=6.638, nll_loss=3.651, mask_ins=1.135, word_ins_ml=5.12, word_reposition=0.382, ppl=99.58, wps=13262.5, ups=0.9, wpb=14705.9, bsz=1024, num_updates=24500, lr=0.000225877, gnorm=1.035, clip=0, loss_scale=4096, train_wall=109, wall=26309
2022-07-31 22:15:24 | INFO | train_inner | epoch 007:   2341 / 3715 loss=6.65, nll_loss=3.665, mask_ins=1.135, word_ins_ml=5.132, word_reposition=0.383, ppl=100.42, wps=13303.9, ups=0.9, wpb=14720.7, bsz=1024, num_updates=24600, lr=0.000225417, gnorm=0.999, clip=0, loss_scale=4465, train_wall=109, wall=26419
2022-07-31 22:17:04 | INFO | train_inner | epoch 007:   2441 / 3715 loss=6.653, nll_loss=3.671, mask_ins=1.133, word_ins_ml=5.138, word_reposition=0.382, ppl=100.63, wps=14702.7, ups=1, wpb=14698.8, bsz=1024, num_updates=24700, lr=0.000224961, gnorm=1.005, clip=0, loss_scale=8192, train_wall=98, wall=26519
2022-07-31 22:18:54 | INFO | train_inner | epoch 007:   2541 / 3715 loss=6.608, nll_loss=3.631, mask_ins=1.129, word_ins_ml=5.102, word_reposition=0.377, ppl=97.54, wps=13420.8, ups=0.91, wpb=14700, bsz=1024, num_updates=24800, lr=0.000224507, gnorm=1.005, clip=0, loss_scale=8192, train_wall=108, wall=26629
2022-07-31 22:20:41 | INFO | train_inner | epoch 007:   2641 / 3715 loss=6.631, nll_loss=3.651, mask_ins=1.128, word_ins_ml=5.12, word_reposition=0.383, ppl=99.14, wps=13506.8, ups=0.93, wpb=14551.6, bsz=1024, num_updates=24900, lr=0.000224055, gnorm=1.02, clip=0, loss_scale=8192, train_wall=106, wall=26737
2022-07-31 22:22:30 | INFO | train_inner | epoch 007:   2741 / 3715 loss=6.628, nll_loss=3.638, mask_ins=1.129, word_ins_ml=5.109, word_reposition=0.39, ppl=98.89, wps=13564.4, ups=0.92, wpb=14707.9, bsz=1024, num_updates=25000, lr=0.000223607, gnorm=1.023, clip=0, loss_scale=8192, train_wall=107, wall=26845
2022-07-31 22:23:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-31 22:24:19 | INFO | train_inner | epoch 007:   2842 / 3715 loss=6.62, nll_loss=3.635, mask_ins=1.128, word_ins_ml=5.106, word_reposition=0.386, ppl=98.38, wps=13504.7, ups=0.91, wpb=14776.8, bsz=1024, num_updates=25100, lr=0.000223161, gnorm=1.017, clip=0, loss_scale=7300, train_wall=108, wall=26955
2022-07-31 22:26:08 | INFO | train_inner | epoch 007:   2942 / 3715 loss=6.667, nll_loss=3.674, mask_ins=1.143, word_ins_ml=5.14, word_reposition=0.383, ppl=101.6, wps=13471, ups=0.92, wpb=14634.1, bsz=1024, num_updates=25200, lr=0.000222718, gnorm=1.012, clip=0, loss_scale=4096, train_wall=107, wall=27063
2022-07-31 22:27:56 | INFO | train_inner | epoch 007:   3042 / 3715 loss=6.607, nll_loss=3.622, mask_ins=1.133, word_ins_ml=5.094, word_reposition=0.38, ppl=97.5, wps=13589.6, ups=0.93, wpb=14665.2, bsz=1024, num_updates=25300, lr=0.000222277, gnorm=1.017, clip=0, loss_scale=4096, train_wall=106, wall=27171
2022-07-31 22:29:43 | INFO | train_inner | epoch 007:   3142 / 3715 loss=6.616, nll_loss=3.64, mask_ins=1.133, word_ins_ml=5.11, word_reposition=0.372, ppl=98.06, wps=13641.9, ups=0.94, wpb=14584.8, bsz=1024, num_updates=25400, lr=0.000221839, gnorm=1.028, clip=0, loss_scale=4096, train_wall=105, wall=27278
2022-07-31 22:31:31 | INFO | train_inner | epoch 007:   3242 / 3715 loss=6.603, nll_loss=3.625, mask_ins=1.125, word_ins_ml=5.096, word_reposition=0.382, ppl=97.21, wps=13598.8, ups=0.92, wpb=14713, bsz=1024, num_updates=25500, lr=0.000221404, gnorm=1.027, clip=0, loss_scale=4096, train_wall=106, wall=27386
2022-07-31 22:33:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-31 22:33:21 | INFO | train_inner | epoch 007:   3343 / 3715 loss=6.594, nll_loss=3.623, mask_ins=1.127, word_ins_ml=5.095, word_reposition=0.372, ppl=96.58, wps=13236.5, ups=0.91, wpb=14625.2, bsz=1024, num_updates=25600, lr=0.000220971, gnorm=0.994, clip=0, loss_scale=4461, train_wall=109, wall=27497
2022-07-31 22:33:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-31 22:35:12 | INFO | train_inner | epoch 007:   3444 / 3715 loss=6.633, nll_loss=3.654, mask_ins=1.131, word_ins_ml=5.122, word_reposition=0.379, ppl=99.26, wps=13370.3, ups=0.91, wpb=14745.6, bsz=1024, num_updates=25700, lr=0.000220541, gnorm=1.298, clip=0, loss_scale=2251, train_wall=108, wall=27607
2022-07-31 22:37:01 | INFO | train_inner | epoch 007:   3544 / 3715 loss=6.624, nll_loss=3.647, mask_ins=1.134, word_ins_ml=5.116, word_reposition=0.375, ppl=98.66, wps=13333.7, ups=0.91, wpb=14646.3, bsz=1024, num_updates=25800, lr=0.000220113, gnorm=1.063, clip=0, loss_scale=2048, train_wall=108, wall=27717
2022-07-31 22:38:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-31 22:38:49 | INFO | train_inner | epoch 007:   3645 / 3715 loss=6.657, nll_loss=3.671, mask_ins=1.135, word_ins_ml=5.138, word_reposition=0.385, ppl=100.91, wps=13625.6, ups=0.93, wpb=14696.8, bsz=1024, num_updates=25900, lr=0.000219687, gnorm=1.136, clip=0, loss_scale=1947, train_wall=106, wall=27825
2022-07-31 22:40:03 | INFO | train | epoch 007 | loss 6.646 | nll_loss 3.659 | mask_ins 1.136 | word_ins_ml 5.128 | word_reposition 0.383 | ppl 100.16 | wps 13266.3 | ups 0.9 | wpb 14662.7 | bsz 1023.7 | num_updates 25970 | lr 0.000219391 | gnorm 1.034 | clip 0 | loss_scale 6572 | train_wall 3945 | wall 27899
2022-07-31 22:41:25 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.605 | nll_loss 3.513 | mask_ins 1.118 | word_ins_ml 5.093 | word_reposition 0.393 | ppl 97.33 | wps 33509.6 | wpb 1849.4 | bsz 127.9 | num_updates 25970 | best_loss 6.605
2022-07-31 22:41:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_best.pt (epoch 7 @ 25970 updates, score 6.605) (writing took 4.308657523244619 seconds)
2022-07-31 22:42:02 | INFO | train_inner | epoch 008:     30 / 3715 loss=6.624, nll_loss=3.646, mask_ins=1.132, word_ins_ml=5.115, word_reposition=0.376, ppl=98.64, wps=7506.4, ups=0.52, wpb=14460, bsz=1014.7, num_updates=26000, lr=0.000219265, gnorm=1.308, clip=0, loss_scale=1024, train_wall=105, wall=28017
2022-07-31 22:43:50 | INFO | train_inner | epoch 008:    130 / 3715 loss=6.586, nll_loss=3.607, mask_ins=1.126, word_ins_ml=5.081, word_reposition=0.379, ppl=96.07, wps=13706.1, ups=0.93, wpb=14757.8, bsz=1024, num_updates=26100, lr=0.000218844, gnorm=1.274, clip=0, loss_scale=1024, train_wall=106, wall=28125
2022-07-31 22:45:35 | INFO | train_inner | epoch 008:    230 / 3715 loss=6.572, nll_loss=3.589, mask_ins=1.124, word_ins_ml=5.065, word_reposition=0.383, ppl=95.14, wps=13928.3, ups=0.95, wpb=14732, bsz=1024, num_updates=26200, lr=0.000218426, gnorm=1.018, clip=0, loss_scale=1024, train_wall=104, wall=28231
2022-07-31 22:47:22 | INFO | train_inner | epoch 008:    330 / 3715 loss=6.57, nll_loss=3.593, mask_ins=1.121, word_ins_ml=5.068, word_reposition=0.381, ppl=94.99, wps=13712, ups=0.93, wpb=14667.3, bsz=1024, num_updates=26300, lr=0.00021801, gnorm=1.038, clip=0, loss_scale=1024, train_wall=105, wall=28338
2022-07-31 22:49:11 | INFO | train_inner | epoch 008:    430 / 3715 loss=6.595, nll_loss=3.612, mask_ins=1.126, word_ins_ml=5.086, word_reposition=0.383, ppl=96.68, wps=13512, ups=0.92, wpb=14649.5, bsz=1024, num_updates=26400, lr=0.000217597, gnorm=1.411, clip=0, loss_scale=1024, train_wall=107, wall=28446
2022-07-31 22:50:59 | INFO | train_inner | epoch 008:    530 / 3715 loss=6.585, nll_loss=3.602, mask_ins=1.128, word_ins_ml=5.076, word_reposition=0.381, ppl=95.99, wps=13463.3, ups=0.92, wpb=14638.1, bsz=1024, num_updates=26500, lr=0.000217186, gnorm=1.189, clip=0, loss_scale=2028, train_wall=107, wall=28555
2022-07-31 22:52:35 | INFO | train_inner | epoch 008:    630 / 3715 loss=6.584, nll_loss=3.605, mask_ins=1.125, word_ins_ml=5.079, word_reposition=0.379, ppl=95.96, wps=15331.1, ups=1.05, wpb=14571.9, bsz=1024, num_updates=26600, lr=0.000216777, gnorm=1.151, clip=0, loss_scale=2048, train_wall=93, wall=28650
2022-07-31 22:53:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-31 22:53:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-31 22:54:26 | INFO | train_inner | epoch 008:    732 / 3715 loss=6.602, nll_loss=3.615, mask_ins=1.129, word_ins_ml=5.088, word_reposition=0.385, ppl=97.17, wps=13105.6, ups=0.9, wpb=14621.2, bsz=1024, num_updates=26700, lr=0.000216371, gnorm=1.259, clip=0, loss_scale=964, train_wall=110, wall=28761
2022-07-31 22:56:15 | INFO | train_inner | epoch 008:    832 / 3715 loss=6.566, nll_loss=3.596, mask_ins=1.124, word_ins_ml=5.07, word_reposition=0.372, ppl=94.73, wps=13475.1, ups=0.92, wpb=14652.4, bsz=1024, num_updates=26800, lr=0.000215967, gnorm=1.105, clip=0, loss_scale=512, train_wall=107, wall=28870
2022-07-31 22:58:04 | INFO | train_inner | epoch 008:    932 / 3715 loss=6.581, nll_loss=3.611, mask_ins=1.123, word_ins_ml=5.084, word_reposition=0.374, ppl=95.73, wps=13404.4, ups=0.92, wpb=14648.8, bsz=1024, num_updates=26900, lr=0.000215565, gnorm=1.478, clip=1, loss_scale=512, train_wall=108, wall=28980
2022-07-31 22:59:53 | INFO | train_inner | epoch 008:   1032 / 3715 loss=6.554, nll_loss=3.584, mask_ins=1.124, word_ins_ml=5.06, word_reposition=0.369, ppl=93.97, wps=13394.4, ups=0.92, wpb=14638.1, bsz=1024, num_updates=27000, lr=0.000215166, gnorm=1.251, clip=0, loss_scale=512, train_wall=108, wall=29089
2022-07-31 23:01:42 | INFO | train_inner | epoch 008:   1132 / 3715 loss=6.583, nll_loss=3.613, mask_ins=1.123, word_ins_ml=5.086, word_reposition=0.374, ppl=95.9, wps=13462.7, ups=0.92, wpb=14577.2, bsz=1024, num_updates=27100, lr=0.000214768, gnorm=1.383, clip=0, loss_scale=512, train_wall=107, wall=29197
2022-07-31 23:03:30 | INFO | train_inner | epoch 008:   1232 / 3715 loss=6.573, nll_loss=3.605, mask_ins=1.122, word_ins_ml=5.079, word_reposition=0.373, ppl=95.23, wps=13601.8, ups=0.93, wpb=14697.5, bsz=1024, num_updates=27200, lr=0.000214373, gnorm=1.193, clip=0, loss_scale=758, train_wall=106, wall=29305
2022-07-31 23:05:18 | INFO | train_inner | epoch 008:   1332 / 3715 loss=6.598, nll_loss=3.618, mask_ins=1.128, word_ins_ml=5.091, word_reposition=0.38, ppl=96.88, wps=13462.9, ups=0.92, wpb=14616.9, bsz=1024, num_updates=27300, lr=0.00021398, gnorm=1.251, clip=0, loss_scale=1024, train_wall=107, wall=29414
2022-07-31 23:05:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-31 23:07:08 | INFO | train_inner | epoch 008:   1433 / 3715 loss=6.583, nll_loss=3.6, mask_ins=1.122, word_ins_ml=5.075, word_reposition=0.385, ppl=95.86, wps=13444.1, ups=0.91, wpb=14704, bsz=1024, num_updates=27400, lr=0.000213589, gnorm=1.365, clip=0, loss_scale=674, train_wall=108, wall=29523
2022-07-31 23:08:57 | INFO | train_inner | epoch 008:   1533 / 3715 loss=6.562, nll_loss=3.587, mask_ins=1.123, word_ins_ml=5.063, word_reposition=0.376, ppl=94.5, wps=13459.4, ups=0.92, wpb=14690.9, bsz=1024, num_updates=27500, lr=0.000213201, gnorm=1.1, clip=0, loss_scale=512, train_wall=107, wall=29632
2022-07-31 23:10:45 | INFO | train_inner | epoch 008:   1633 / 3715 loss=6.586, nll_loss=3.609, mask_ins=1.124, word_ins_ml=5.082, word_reposition=0.379, ppl=96.05, wps=13675.1, ups=0.93, wpb=14741.7, bsz=1024, num_updates=27600, lr=0.000212814, gnorm=1.112, clip=0, loss_scale=512, train_wall=106, wall=29740
2022-07-31 23:12:33 | INFO | train_inner | epoch 008:   1733 / 3715 loss=6.554, nll_loss=3.588, mask_ins=1.117, word_ins_ml=5.064, word_reposition=0.373, ppl=93.94, wps=13581.3, ups=0.92, wpb=14746.2, bsz=1024, num_updates=27700, lr=0.00021243, gnorm=1.054, clip=0, loss_scale=512, train_wall=107, wall=29849
2022-07-31 23:14:22 | INFO | train_inner | epoch 008:   1833 / 3715 loss=6.61, nll_loss=3.631, mask_ins=1.126, word_ins_ml=5.101, word_reposition=0.382, ppl=97.66, wps=13435.3, ups=0.92, wpb=14579.5, bsz=1024, num_updates=27800, lr=0.000212047, gnorm=1.067, clip=0, loss_scale=512, train_wall=107, wall=29957
2022-07-31 23:16:10 | INFO | train_inner | epoch 008:   1933 / 3715 loss=6.566, nll_loss=3.605, mask_ins=1.116, word_ins_ml=5.078, word_reposition=0.371, ppl=94.75, wps=13582, ups=0.93, wpb=14656.5, bsz=1024, num_updates=27900, lr=0.000211667, gnorm=1.153, clip=0, loss_scale=804, train_wall=106, wall=30065
2022-07-31 23:17:58 | INFO | train_inner | epoch 008:   2033 / 3715 loss=6.599, nll_loss=3.629, mask_ins=1.124, word_ins_ml=5.1, word_reposition=0.375, ppl=96.93, wps=13490.6, ups=0.92, wpb=14626, bsz=1024, num_updates=28000, lr=0.000211289, gnorm=1.058, clip=0, loss_scale=1024, train_wall=107, wall=30173
2022-07-31 23:19:46 | INFO | train_inner | epoch 008:   2133 / 3715 loss=6.571, nll_loss=3.594, mask_ins=1.121, word_ins_ml=5.069, word_reposition=0.38, ppl=95.06, wps=13509, ups=0.92, wpb=14638.8, bsz=1024, num_updates=28100, lr=0.000210912, gnorm=1.099, clip=0, loss_scale=1024, train_wall=107, wall=30282
2022-07-31 23:21:34 | INFO | train_inner | epoch 008:   2233 / 3715 loss=6.536, nll_loss=3.57, mask_ins=1.114, word_ins_ml=5.047, word_reposition=0.375, ppl=92.79, wps=13536.2, ups=0.93, wpb=14610, bsz=1024, num_updates=28200, lr=0.000210538, gnorm=1.01, clip=0, loss_scale=1024, train_wall=106, wall=30390
2022-07-31 23:23:23 | INFO | train_inner | epoch 008:   2333 / 3715 loss=6.571, nll_loss=3.6, mask_ins=1.121, word_ins_ml=5.074, word_reposition=0.376, ppl=95.06, wps=13560.9, ups=0.92, wpb=14697.6, bsz=1024, num_updates=28300, lr=0.000210166, gnorm=1.059, clip=0, loss_scale=1024, train_wall=107, wall=30498
2022-07-31 23:25:10 | INFO | train_inner | epoch 008:   2433 / 3715 loss=6.556, nll_loss=3.587, mask_ins=1.118, word_ins_ml=5.063, word_reposition=0.375, ppl=94.09, wps=13650.7, ups=0.93, wpb=14635.1, bsz=1024, num_updates=28400, lr=0.000209795, gnorm=1.022, clip=0, loss_scale=1485, train_wall=105, wall=30605
2022-07-31 23:26:57 | INFO | train_inner | epoch 008:   2533 / 3715 loss=6.531, nll_loss=3.568, mask_ins=1.114, word_ins_ml=5.045, word_reposition=0.371, ppl=92.46, wps=13652.1, ups=0.93, wpb=14681.6, bsz=1024, num_updates=28500, lr=0.000209427, gnorm=1.022, clip=0, loss_scale=2048, train_wall=106, wall=30713
2022-07-31 23:28:38 | INFO | train_inner | epoch 008:   2633 / 3715 loss=6.554, nll_loss=3.584, mask_ins=1.119, word_ins_ml=5.059, word_reposition=0.376, ppl=93.98, wps=14560.5, ups=0.99, wpb=14705, bsz=1024, num_updates=28600, lr=0.000209061, gnorm=0.99, clip=0, loss_scale=2048, train_wall=99, wall=30814
2022-07-31 23:30:22 | INFO | train_inner | epoch 008:   2733 / 3715 loss=6.56, nll_loss=3.585, mask_ins=1.124, word_ins_ml=5.061, word_reposition=0.375, ppl=94.34, wps=14198.4, ups=0.97, wpb=14704.1, bsz=1024, num_updates=28700, lr=0.000208696, gnorm=1, clip=0, loss_scale=2048, train_wall=102, wall=30917
2022-07-31 23:32:10 | INFO | train_inner | epoch 008:   2833 / 3715 loss=6.551, nll_loss=3.578, mask_ins=1.115, word_ins_ml=5.054, word_reposition=0.383, ppl=93.76, wps=13739, ups=0.93, wpb=14801.8, bsz=1024, num_updates=28800, lr=0.000208333, gnorm=0.993, clip=0, loss_scale=2048, train_wall=106, wall=31025
2022-07-31 23:33:56 | INFO | train_inner | epoch 008:   2933 / 3715 loss=6.577, nll_loss=3.604, mask_ins=1.123, word_ins_ml=5.077, word_reposition=0.377, ppl=95.46, wps=13717.2, ups=0.94, wpb=14620.7, bsz=1024, num_updates=28900, lr=0.000207973, gnorm=1.019, clip=0, loss_scale=2724, train_wall=105, wall=31132
2022-07-31 23:35:43 | INFO | train_inner | epoch 008:   3033 / 3715 loss=6.581, nll_loss=3.607, mask_ins=1.126, word_ins_ml=5.079, word_reposition=0.375, ppl=95.73, wps=13825.2, ups=0.93, wpb=14795.6, bsz=1024, num_updates=29000, lr=0.000207614, gnorm=0.98, clip=0, loss_scale=4096, train_wall=105, wall=31239
2022-07-31 23:37:31 | INFO | train_inner | epoch 008:   3133 / 3715 loss=6.575, nll_loss=3.607, mask_ins=1.122, word_ins_ml=5.08, word_reposition=0.374, ppl=95.37, wps=13635.1, ups=0.93, wpb=14635.4, bsz=1024, num_updates=29100, lr=0.000207257, gnorm=1.049, clip=0, loss_scale=4096, train_wall=106, wall=31346
2022-07-31 23:39:18 | INFO | train_inner | epoch 008:   3233 / 3715 loss=6.545, nll_loss=3.577, mask_ins=1.12, word_ins_ml=5.054, word_reposition=0.371, ppl=93.37, wps=13629.5, ups=0.93, wpb=14666.9, bsz=1024, num_updates=29200, lr=0.000206901, gnorm=1.03, clip=0, loss_scale=4096, train_wall=106, wall=31454
2022-07-31 23:41:05 | INFO | train_inner | epoch 008:   3333 / 3715 loss=6.573, nll_loss=3.607, mask_ins=1.118, word_ins_ml=5.08, word_reposition=0.375, ppl=95.21, wps=13624.1, ups=0.93, wpb=14582.7, bsz=1024, num_updates=29300, lr=0.000206548, gnorm=0.976, clip=0, loss_scale=4096, train_wall=105, wall=31561
2022-07-31 23:42:53 | INFO | train_inner | epoch 008:   3433 / 3715 loss=6.574, nll_loss=3.602, mask_ins=1.12, word_ins_ml=5.075, word_reposition=0.38, ppl=95.3, wps=13656.9, ups=0.93, wpb=14639.5, bsz=1024, num_updates=29400, lr=0.000206197, gnorm=1.023, clip=0, loss_scale=4956, train_wall=105, wall=31668
2022-07-31 23:44:39 | INFO | train_inner | epoch 008:   3533 / 3715 loss=6.555, nll_loss=3.591, mask_ins=1.115, word_ins_ml=5.065, word_reposition=0.375, ppl=94.03, wps=13905.3, ups=0.94, wpb=14736, bsz=1024, num_updates=29500, lr=0.000205847, gnorm=0.983, clip=0, loss_scale=8192, train_wall=104, wall=31774
2022-07-31 23:46:25 | INFO | train_inner | epoch 008:   3633 / 3715 loss=6.544, nll_loss=3.577, mask_ins=1.119, word_ins_ml=5.053, word_reposition=0.371, ppl=93.3, wps=13664.7, ups=0.94, wpb=14579.6, bsz=1023.8, num_updates=29600, lr=0.000205499, gnorm=0.989, clip=0, loss_scale=8192, train_wall=105, wall=31881
2022-07-31 23:47:52 | INFO | train | epoch 008 | loss 6.571 | nll_loss 3.599 | mask_ins 1.122 | word_ins_ml 5.073 | word_reposition 0.377 | ppl 95.1 | wps 13376.7 | ups 0.91 | wpb 14662.1 | bsz 1023.7 | num_updates 29682 | lr 0.000205215 | gnorm 1.118 | clip 0 | loss_scale 2093 | train_wall 3917 | wall 31967
2022-07-31 23:49:13 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.559 | nll_loss 3.464 | mask_ins 1.119 | word_ins_ml 5.05 | word_reposition 0.39 | ppl 94.32 | wps 33842 | wpb 1849.4 | bsz 127.9 | num_updates 29682 | best_loss 6.559
2022-07-31 23:49:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_best.pt (epoch 8 @ 29682 updates, score 6.559) (writing took 4.079793453216553 seconds)
2022-07-31 23:49:36 | INFO | train_inner | epoch 009:     18 / 3715 loss=6.565, nll_loss=3.59, mask_ins=1.122, word_ins_ml=5.065, word_reposition=0.377, ppl=94.68, wps=7676.3, ups=0.52, wpb=14662.1, bsz=1014.7, num_updates=29700, lr=0.000205152, gnorm=1.001, clip=0, loss_scale=8192, train_wall=104, wall=32072
2022-07-31 23:51:24 | INFO | train_inner | epoch 009:    118 / 3715 loss=6.535, nll_loss=3.572, mask_ins=1.12, word_ins_ml=5.049, word_reposition=0.366, ppl=92.74, wps=13562.4, ups=0.93, wpb=14552.3, bsz=1024, num_updates=29800, lr=0.000204808, gnorm=0.989, clip=0, loss_scale=8192, train_wall=106, wall=32179
2022-07-31 23:53:10 | INFO | train_inner | epoch 009:    218 / 3715 loss=6.495, nll_loss=3.532, mask_ins=1.11, word_ins_ml=5.014, word_reposition=0.372, ppl=90.22, wps=13666.9, ups=0.94, wpb=14540.7, bsz=1024, num_updates=29900, lr=0.000204465, gnorm=1.005, clip=0, loss_scale=8929, train_wall=105, wall=32285
2022-07-31 23:53:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-31 23:54:57 | INFO | train_inner | epoch 009:    319 / 3715 loss=6.502, nll_loss=3.544, mask_ins=1.109, word_ins_ml=5.024, word_reposition=0.368, ppl=90.64, wps=13683.5, ups=0.93, wpb=14637, bsz=1024, num_updates=30000, lr=0.000204124, gnorm=0.988, clip=0, loss_scale=8354, train_wall=105, wall=32392
2022-07-31 23:55:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-31 23:56:45 | INFO | train_inner | epoch 009:    420 / 3715 loss=6.513, nll_loss=3.553, mask_ins=1.112, word_ins_ml=5.032, word_reposition=0.368, ppl=91.3, wps=13669.5, ups=0.93, wpb=14720, bsz=1024, num_updates=30100, lr=0.000203785, gnorm=1.014, clip=0, loss_scale=5556, train_wall=106, wall=32500
2022-07-31 23:58:32 | INFO | train_inner | epoch 009:    520 / 3715 loss=6.506, nll_loss=3.545, mask_ins=1.108, word_ins_ml=5.026, word_reposition=0.373, ppl=90.89, wps=13730.7, ups=0.93, wpb=14801.6, bsz=1024, num_updates=30200, lr=0.000203447, gnorm=0.992, clip=0, loss_scale=4096, train_wall=106, wall=32608
2022-07-31 23:58:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-01 00:00:21 | INFO | train_inner | epoch 009:    621 / 3715 loss=6.517, nll_loss=3.553, mask_ins=1.113, word_ins_ml=5.032, word_reposition=0.372, ppl=91.56, wps=13528, ups=0.92, wpb=14752.3, bsz=1024, num_updates=30300, lr=0.000203111, gnorm=1.017, clip=0, loss_scale=2129, train_wall=107, wall=32717
2022-08-01 00:02:03 | INFO | train_inner | epoch 009:    721 / 3715 loss=6.496, nll_loss=3.542, mask_ins=1.109, word_ins_ml=5.022, word_reposition=0.365, ppl=90.24, wps=14338.2, ups=0.99, wpb=14494.1, bsz=1024, num_updates=30400, lr=0.000202777, gnorm=0.978, clip=0, loss_scale=2048, train_wall=99, wall=32818
2022-08-01 00:03:44 | INFO | train_inner | epoch 009:    821 / 3715 loss=6.52, nll_loss=3.554, mask_ins=1.111, word_ins_ml=5.033, word_reposition=0.376, ppl=91.78, wps=14397.7, ups=0.98, wpb=14620.2, bsz=1024, num_updates=30500, lr=0.000202444, gnorm=1.025, clip=0, loss_scale=2048, train_wall=100, wall=32919
2022-08-01 00:05:30 | INFO | train_inner | epoch 009:    921 / 3715 loss=6.51, nll_loss=3.546, mask_ins=1.113, word_ins_ml=5.025, word_reposition=0.372, ppl=91.14, wps=13790.1, ups=0.94, wpb=14651.3, bsz=1024, num_updates=30600, lr=0.000202113, gnorm=0.985, clip=0, loss_scale=2048, train_wall=104, wall=33026
2022-08-01 00:07:17 | INFO | train_inner | epoch 009:   1021 / 3715 loss=6.515, nll_loss=3.544, mask_ins=1.117, word_ins_ml=5.024, word_reposition=0.374, ppl=91.45, wps=13821.2, ups=0.94, wpb=14765.6, bsz=1024, num_updates=30700, lr=0.000201784, gnorm=1.024, clip=0, loss_scale=2048, train_wall=105, wall=33133
2022-08-01 00:09:03 | INFO | train_inner | epoch 009:   1121 / 3715 loss=6.507, nll_loss=3.543, mask_ins=1.11, word_ins_ml=5.023, word_reposition=0.374, ppl=90.92, wps=13924.6, ups=0.94, wpb=14738.7, bsz=1024, num_updates=30800, lr=0.000201456, gnorm=0.989, clip=0, loss_scale=3789, train_wall=104, wall=33238
2022-08-01 00:10:51 | INFO | train_inner | epoch 009:   1221 / 3715 loss=6.504, nll_loss=3.545, mask_ins=1.105, word_ins_ml=5.024, word_reposition=0.375, ppl=90.74, wps=13566.5, ups=0.93, wpb=14610.2, bsz=1024, num_updates=30900, lr=0.000201129, gnorm=0.997, clip=0, loss_scale=4096, train_wall=106, wall=33346
2022-08-01 00:12:38 | INFO | train_inner | epoch 009:   1321 / 3715 loss=6.501, nll_loss=3.542, mask_ins=1.105, word_ins_ml=5.022, word_reposition=0.374, ppl=90.6, wps=13773.8, ups=0.94, wpb=14714.2, bsz=1024, num_updates=31000, lr=0.000200805, gnorm=0.984, clip=0, loss_scale=4096, train_wall=105, wall=33453
2022-08-01 00:14:24 | INFO | train_inner | epoch 009:   1421 / 3715 loss=6.519, nll_loss=3.554, mask_ins=1.116, word_ins_ml=5.032, word_reposition=0.37, ppl=91.68, wps=13640.1, ups=0.94, wpb=14577.5, bsz=1024, num_updates=31100, lr=0.000200482, gnorm=0.992, clip=0, loss_scale=4096, train_wall=105, wall=33560
2022-08-01 00:16:11 | INFO | train_inner | epoch 009:   1521 / 3715 loss=6.522, nll_loss=3.557, mask_ins=1.116, word_ins_ml=5.036, word_reposition=0.371, ppl=91.91, wps=13718.2, ups=0.94, wpb=14658.8, bsz=1024, num_updates=31200, lr=0.00020016, gnorm=0.992, clip=0, loss_scale=4096, train_wall=105, wall=33667
2022-08-01 00:17:58 | INFO | train_inner | epoch 009:   1621 / 3715 loss=6.502, nll_loss=3.544, mask_ins=1.112, word_ins_ml=5.024, word_reposition=0.366, ppl=90.62, wps=13642.2, ups=0.94, wpb=14587.8, bsz=1024, num_updates=31300, lr=0.00019984, gnorm=0.976, clip=0, loss_scale=7086, train_wall=105, wall=33774
2022-08-01 00:19:45 | INFO | train_inner | epoch 009:   1721 / 3715 loss=6.531, nll_loss=3.562, mask_ins=1.111, word_ins_ml=5.039, word_reposition=0.38, ppl=92.48, wps=13751.5, ups=0.93, wpb=14752.1, bsz=1024, num_updates=31400, lr=0.000199522, gnorm=0.993, clip=0, loss_scale=8192, train_wall=106, wall=33881
2022-08-01 00:21:33 | INFO | train_inner | epoch 009:   1821 / 3715 loss=6.487, nll_loss=3.53, mask_ins=1.106, word_ins_ml=5.011, word_reposition=0.369, ppl=89.7, wps=13724.2, ups=0.93, wpb=14719.5, bsz=1024, num_updates=31500, lr=0.000199205, gnorm=0.982, clip=0, loss_scale=8192, train_wall=106, wall=33988
2022-08-01 00:23:19 | INFO | train_inner | epoch 009:   1921 / 3715 loss=6.489, nll_loss=3.531, mask_ins=1.106, word_ins_ml=5.011, word_reposition=0.371, ppl=89.82, wps=13769.9, ups=0.94, wpb=14645.2, bsz=1024, num_updates=31600, lr=0.000198889, gnorm=0.98, clip=0, loss_scale=8192, train_wall=105, wall=34094
2022-08-01 00:25:05 | INFO | train_inner | epoch 009:   2021 / 3715 loss=6.502, nll_loss=3.546, mask_ins=1.109, word_ins_ml=5.025, word_reposition=0.367, ppl=90.62, wps=13808.2, ups=0.94, wpb=14620.5, bsz=1024, num_updates=31700, lr=0.000198575, gnorm=0.983, clip=0, loss_scale=8192, train_wall=104, wall=34200
2022-08-01 00:26:54 | INFO | train_inner | epoch 009:   2121 / 3715 loss=6.502, nll_loss=3.535, mask_ins=1.109, word_ins_ml=5.016, word_reposition=0.377, ppl=90.62, wps=13574, ups=0.92, wpb=14751.8, bsz=1024, num_updates=31800, lr=0.000198263, gnorm=0.97, clip=0, loss_scale=13189, train_wall=107, wall=34309
2022-08-01 00:28:42 | INFO | train_inner | epoch 009:   2221 / 3715 loss=6.518, nll_loss=3.556, mask_ins=1.108, word_ins_ml=5.035, word_reposition=0.375, ppl=91.63, wps=13533.4, ups=0.92, wpb=14718, bsz=1024, num_updates=31900, lr=0.000197952, gnorm=0.973, clip=0, loss_scale=16384, train_wall=107, wall=34418
2022-08-01 00:30:29 | INFO | train_inner | epoch 009:   2321 / 3715 loss=6.503, nll_loss=3.543, mask_ins=1.107, word_ins_ml=5.022, word_reposition=0.373, ppl=90.71, wps=13740.9, ups=0.94, wpb=14647, bsz=1024, num_updates=32000, lr=0.000197642, gnorm=0.975, clip=0, loss_scale=16384, train_wall=105, wall=34524
2022-08-01 00:32:16 | INFO | train_inner | epoch 009:   2421 / 3715 loss=6.501, nll_loss=3.537, mask_ins=1.112, word_ins_ml=5.018, word_reposition=0.371, ppl=90.56, wps=13685, ups=0.93, wpb=14637.6, bsz=1024, num_updates=32100, lr=0.000197334, gnorm=0.984, clip=0, loss_scale=16384, train_wall=105, wall=34631
2022-08-01 00:32:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-01 00:34:04 | INFO | train_inner | epoch 009:   2522 / 3715 loss=6.507, nll_loss=3.545, mask_ins=1.102, word_ins_ml=5.024, word_reposition=0.38, ppl=90.95, wps=13621.4, ups=0.93, wpb=14686.7, bsz=1024, num_updates=32200, lr=0.000197028, gnorm=0.983, clip=0, loss_scale=10544, train_wall=106, wall=34739
2022-08-01 00:35:51 | INFO | train_inner | epoch 009:   2622 / 3715 loss=6.497, nll_loss=3.529, mask_ins=1.11, word_ins_ml=5.01, word_reposition=0.377, ppl=90.33, wps=13619.7, ups=0.93, wpb=14649.3, bsz=1024, num_updates=32300, lr=0.000196722, gnorm=0.991, clip=0, loss_scale=8192, train_wall=106, wall=34847
2022-08-01 00:37:31 | INFO | train_inner | epoch 009:   2722 / 3715 loss=6.492, nll_loss=3.541, mask_ins=1.101, word_ins_ml=5.02, word_reposition=0.371, ppl=90.01, wps=14836.4, ups=1.01, wpb=14716.3, bsz=1024, num_updates=32400, lr=0.000196419, gnorm=1, clip=0, loss_scale=8192, train_wall=97, wall=34946
2022-08-01 00:39:11 | INFO | train_inner | epoch 009:   2822 / 3715 loss=6.481, nll_loss=3.525, mask_ins=1.106, word_ins_ml=5.006, word_reposition=0.369, ppl=89.32, wps=14551.9, ups=0.99, wpb=14690.2, bsz=1024, num_updates=32500, lr=0.000196116, gnorm=1.015, clip=0, loss_scale=8192, train_wall=99, wall=35047
2022-08-01 00:40:58 | INFO | train_inner | epoch 009:   2922 / 3715 loss=6.496, nll_loss=3.536, mask_ins=1.112, word_ins_ml=5.016, word_reposition=0.368, ppl=90.28, wps=13860.2, ups=0.94, wpb=14714.7, bsz=1024, num_updates=32600, lr=0.000195815, gnorm=0.981, clip=0, loss_scale=8192, train_wall=104, wall=35153
2022-08-01 00:42:44 | INFO | train_inner | epoch 009:   3022 / 3715 loss=6.489, nll_loss=3.543, mask_ins=1.103, word_ins_ml=5.022, word_reposition=0.363, ppl=89.8, wps=13761.5, ups=0.94, wpb=14628.9, bsz=1024, num_updates=32700, lr=0.000195515, gnorm=0.976, clip=0, loss_scale=13107, train_wall=105, wall=35259
2022-08-01 00:44:32 | INFO | train_inner | epoch 009:   3122 / 3715 loss=6.502, nll_loss=3.541, mask_ins=1.109, word_ins_ml=5.021, word_reposition=0.372, ppl=90.61, wps=13599.5, ups=0.93, wpb=14683, bsz=1024, num_updates=32800, lr=0.000195217, gnorm=1.013, clip=0, loss_scale=16384, train_wall=106, wall=35367
2022-08-01 00:46:20 | INFO | train_inner | epoch 009:   3222 / 3715 loss=6.495, nll_loss=3.537, mask_ins=1.105, word_ins_ml=5.017, word_reposition=0.373, ppl=90.2, wps=13514.1, ups=0.92, wpb=14623.7, bsz=1024, num_updates=32900, lr=0.00019492, gnorm=1.008, clip=0, loss_scale=16384, train_wall=106, wall=35476
2022-08-01 00:48:07 | INFO | train_inner | epoch 009:   3322 / 3715 loss=6.501, nll_loss=3.542, mask_ins=1.107, word_ins_ml=5.021, word_reposition=0.374, ppl=90.57, wps=13791.6, ups=0.94, wpb=14737.8, bsz=1024, num_updates=33000, lr=0.000194625, gnorm=0.984, clip=0, loss_scale=16384, train_wall=105, wall=35582
2022-08-01 00:49:54 | INFO | train_inner | epoch 009:   3422 / 3715 loss=6.494, nll_loss=3.533, mask_ins=1.107, word_ins_ml=5.014, word_reposition=0.374, ppl=90.15, wps=13661.3, ups=0.94, wpb=14579.4, bsz=1024, num_updates=33100, lr=0.000194331, gnorm=0.985, clip=0, loss_scale=16384, train_wall=105, wall=35689
2022-08-01 00:50:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-01 00:51:41 | INFO | train_inner | epoch 009:   3523 / 3715 loss=6.484, nll_loss=3.534, mask_ins=1.102, word_ins_ml=5.014, word_reposition=0.368, ppl=89.52, wps=13665.2, ups=0.93, wpb=14708.8, bsz=1024, num_updates=33200, lr=0.000194038, gnorm=0.977, clip=0, loss_scale=10950, train_wall=106, wall=35797
2022-08-01 00:52:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-01 00:53:30 | INFO | train_inner | epoch 009:   3624 / 3715 loss=6.479, nll_loss=3.526, mask_ins=1.102, word_ins_ml=5.007, word_reposition=0.371, ppl=89.22, wps=13440.8, ups=0.92, wpb=14618.3, bsz=1024, num_updates=33300, lr=0.000193746, gnorm=0.989, clip=0, loss_scale=6205, train_wall=107, wall=35906
2022-08-01 00:55:07 | INFO | train | epoch 009 | loss 6.503 | nll_loss 3.542 | mask_ins 1.109 | word_ins_ml 5.022 | word_reposition 0.372 | ppl 90.68 | wps 13476 | ups 0.92 | wpb 14661.1 | bsz 1023.7 | num_updates 33391 | lr 0.000193482 | gnorm 0.991 | clip 0 | loss_scale 8414 | train_wall 3885 | wall 36003
2022-08-01 00:56:29 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.514 | nll_loss 3.419 | mask_ins 1.108 | word_ins_ml 5.008 | word_reposition 0.399 | ppl 91.39 | wps 33707 | wpb 1849.4 | bsz 127.9 | num_updates 33391 | best_loss 6.514
2022-08-01 00:56:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_best.pt (epoch 9 @ 33391 updates, score 6.514) (writing took 4.033805586397648 seconds)
2022-08-01 00:56:42 | INFO | train_inner | epoch 010:      9 / 3715 loss=6.48, nll_loss=3.525, mask_ins=1.104, word_ins_ml=5.006, word_reposition=0.37, ppl=89.26, wps=7544.1, ups=0.52, wpb=14468.1, bsz=1014.7, num_updates=33400, lr=0.000193456, gnorm=0.989, clip=0, loss_scale=4096, train_wall=105, wall=36097
2022-08-01 00:56:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-01 00:58:31 | INFO | train_inner | epoch 010:    110 / 3715 loss=6.457, nll_loss=3.498, mask_ins=1.103, word_ins_ml=4.982, word_reposition=0.372, ppl=87.86, wps=13444, ups=0.92, wpb=14651.8, bsz=1024, num_updates=33500, lr=0.000193167, gnorm=0.987, clip=0, loss_scale=2312, train_wall=107, wall=36206
2022-08-01 01:00:18 | INFO | train_inner | epoch 010:    210 / 3715 loss=6.458, nll_loss=3.503, mask_ins=1.103, word_ins_ml=4.987, word_reposition=0.367, ppl=87.89, wps=13583.4, ups=0.93, wpb=14602, bsz=1024, num_updates=33600, lr=0.000192879, gnorm=0.991, clip=0, loss_scale=2048, train_wall=106, wall=36314
2022-08-01 01:02:05 | INFO | train_inner | epoch 010:    310 / 3715 loss=6.445, nll_loss=3.499, mask_ins=1.095, word_ins_ml=4.984, word_reposition=0.366, ppl=87.12, wps=13850.4, ups=0.94, wpb=14738.5, bsz=1023.8, num_updates=33700, lr=0.000192593, gnorm=0.965, clip=0, loss_scale=2048, train_wall=105, wall=36420
2022-08-01 01:03:51 | INFO | train_inner | epoch 010:    410 / 3715 loss=6.467, nll_loss=3.506, mask_ins=1.106, word_ins_ml=4.99, word_reposition=0.371, ppl=88.45, wps=13620.5, ups=0.94, wpb=14527.3, bsz=1024, num_updates=33800, lr=0.000192308, gnorm=0.991, clip=0, loss_scale=2048, train_wall=105, wall=36527
2022-08-01 01:05:38 | INFO | train_inner | epoch 010:    510 / 3715 loss=6.45, nll_loss=3.506, mask_ins=1.1, word_ins_ml=4.99, word_reposition=0.36, ppl=87.41, wps=13692.3, ups=0.94, wpb=14571.2, bsz=1024, num_updates=33900, lr=0.000192024, gnorm=0.977, clip=0, loss_scale=2048, train_wall=105, wall=36633
2022-08-01 01:07:24 | INFO | train_inner | epoch 010:    610 / 3715 loss=6.441, nll_loss=3.487, mask_ins=1.098, word_ins_ml=4.973, word_reposition=0.37, ppl=86.89, wps=13860.6, ups=0.94, wpb=14777.7, bsz=1024, num_updates=34000, lr=0.000191741, gnorm=0.992, clip=0, loss_scale=3604, train_wall=105, wall=36740
2022-08-01 01:09:11 | INFO | train_inner | epoch 010:    710 / 3715 loss=6.479, nll_loss=3.523, mask_ins=1.104, word_ins_ml=5.004, word_reposition=0.371, ppl=89.21, wps=13868.3, ups=0.94, wpb=14788.8, bsz=1024, num_updates=34100, lr=0.00019146, gnorm=0.975, clip=0, loss_scale=4096, train_wall=105, wall=36847
2022-08-01 01:10:57 | INFO | train_inner | epoch 010:    810 / 3715 loss=6.456, nll_loss=3.505, mask_ins=1.101, word_ins_ml=4.989, word_reposition=0.366, ppl=87.78, wps=13926.9, ups=0.95, wpb=14727.4, bsz=1024, num_updates=34200, lr=0.00019118, gnorm=0.973, clip=0, loss_scale=4096, train_wall=104, wall=36952
2022-08-01 01:12:24 | INFO | train_inner | epoch 010:    910 / 3715 loss=6.466, nll_loss=3.507, mask_ins=1.103, word_ins_ml=4.99, word_reposition=0.373, ppl=88.4, wps=16923.9, ups=1.15, wpb=14731.1, bsz=1024, num_updates=34300, lr=0.000190901, gnorm=0.988, clip=0, loss_scale=4096, train_wall=85, wall=37039
2022-08-01 01:12:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-01 01:14:02 | INFO | train_inner | epoch 010:   1011 / 3715 loss=6.464, nll_loss=3.505, mask_ins=1.105, word_ins_ml=4.989, word_reposition=0.369, ppl=88.27, wps=14930.1, ups=1.02, wpb=14644.7, bsz=1024, num_updates=34400, lr=0.000190623, gnorm=1.164, clip=0, loss_scale=2433, train_wall=96, wall=37137
2022-08-01 01:15:39 | INFO | train_inner | epoch 010:   1111 / 3715 loss=6.449, nll_loss=3.498, mask_ins=1.097, word_ins_ml=4.982, word_reposition=0.37, ppl=87.34, wps=15180.5, ups=1.03, wpb=14762.9, bsz=1024, num_updates=34500, lr=0.000190347, gnorm=0.968, clip=0, loss_scale=2048, train_wall=96, wall=37235
2022-08-01 01:17:17 | INFO | train_inner | epoch 010:   1211 / 3715 loss=6.435, nll_loss=3.484, mask_ins=1.093, word_ins_ml=4.97, word_reposition=0.372, ppl=86.54, wps=14920.3, ups=1.02, wpb=14593.6, bsz=1024, num_updates=34600, lr=0.000190071, gnorm=0.986, clip=0, loss_scale=2048, train_wall=96, wall=37332
2022-08-01 01:18:55 | INFO | train_inner | epoch 010:   1311 / 3715 loss=6.449, nll_loss=3.501, mask_ins=1.091, word_ins_ml=4.984, word_reposition=0.373, ppl=87.36, wps=15011.6, ups=1.03, wpb=14639.9, bsz=1024, num_updates=34700, lr=0.000189797, gnorm=0.964, clip=0, loss_scale=2048, train_wall=96, wall=37430
2022-08-01 01:20:33 | INFO | train_inner | epoch 010:   1411 / 3715 loss=6.442, nll_loss=3.496, mask_ins=1.097, word_ins_ml=4.98, word_reposition=0.364, ppl=86.92, wps=14857.8, ups=1.02, wpb=14596.3, bsz=1024, num_updates=34800, lr=0.000189525, gnorm=0.965, clip=0, loss_scale=2048, train_wall=96, wall=37528
2022-08-01 01:22:04 | INFO | train_inner | epoch 010:   1511 / 3715 loss=6.467, nll_loss=3.519, mask_ins=1.097, word_ins_ml=5.001, word_reposition=0.369, ppl=88.47, wps=15978.9, ups=1.1, wpb=14579.4, bsz=1024, num_updates=34900, lr=0.000189253, gnorm=0.974, clip=0, loss_scale=3482, train_wall=90, wall=37619
2022-08-01 01:23:28 | INFO | train_inner | epoch 010:   1611 / 3715 loss=6.437, nll_loss=3.491, mask_ins=1.098, word_ins_ml=4.976, word_reposition=0.364, ppl=86.67, wps=17572.3, ups=1.2, wpb=14693.4, bsz=1024, num_updates=35000, lr=0.000188982, gnorm=0.966, clip=0, loss_scale=4096, train_wall=82, wall=37703
2022-08-01 01:25:01 | INFO | train_inner | epoch 010:   1711 / 3715 loss=6.441, nll_loss=3.493, mask_ins=1.096, word_ins_ml=4.978, word_reposition=0.368, ppl=86.91, wps=15677.2, ups=1.07, wpb=14676.8, bsz=1024, num_updates=35100, lr=0.000188713, gnorm=0.965, clip=0, loss_scale=4096, train_wall=92, wall=37797
2022-08-01 01:26:48 | INFO | train_inner | epoch 010:   1811 / 3715 loss=6.447, nll_loss=3.496, mask_ins=1.1, word_ins_ml=4.98, word_reposition=0.368, ppl=87.26, wps=13712.6, ups=0.94, wpb=14653.3, bsz=1024, num_updates=35200, lr=0.000188445, gnorm=0.996, clip=0, loss_scale=4096, train_wall=105, wall=37904
2022-08-01 01:26:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-01 01:27:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-01 01:28:37 | INFO | train_inner | epoch 010:   1913 / 3715 loss=6.474, nll_loss=3.518, mask_ins=1.109, word_ins_ml=4.999, word_reposition=0.366, ppl=88.91, wps=13507.2, ups=0.92, wpb=14672.9, bsz=1024, num_updates=35300, lr=0.000188177, gnorm=1.049, clip=0, loss_scale=1275, train_wall=107, wall=38012
2022-08-01 01:30:24 | INFO | train_inner | epoch 010:   2013 / 3715 loss=6.439, nll_loss=3.487, mask_ins=1.1, word_ins_ml=4.972, word_reposition=0.367, ppl=86.76, wps=13774.4, ups=0.93, wpb=14735.3, bsz=1024, num_updates=35400, lr=0.000187912, gnorm=0.98, clip=0, loss_scale=1024, train_wall=105, wall=38119
2022-08-01 01:32:11 | INFO | train_inner | epoch 010:   2113 / 3715 loss=6.461, nll_loss=3.502, mask_ins=1.098, word_ins_ml=4.986, word_reposition=0.377, ppl=88.1, wps=13791, ups=0.93, wpb=14798.2, bsz=1024, num_updates=35500, lr=0.000187647, gnorm=0.973, clip=0, loss_scale=1024, train_wall=106, wall=38226
2022-08-01 01:33:59 | INFO | train_inner | epoch 010:   2213 / 3715 loss=6.431, nll_loss=3.484, mask_ins=1.098, word_ins_ml=4.97, word_reposition=0.364, ppl=86.31, wps=13513.7, ups=0.93, wpb=14540.6, bsz=1024, num_updates=35600, lr=0.000187383, gnorm=1.019, clip=0, loss_scale=1024, train_wall=106, wall=38334
2022-08-01 01:35:45 | INFO | train_inner | epoch 010:   2313 / 3715 loss=6.468, nll_loss=3.509, mask_ins=1.104, word_ins_ml=4.991, word_reposition=0.373, ppl=88.54, wps=13725.5, ups=0.94, wpb=14588.1, bsz=1024, num_updates=35700, lr=0.00018712, gnorm=1.134, clip=0, loss_scale=1024, train_wall=105, wall=38440
2022-08-01 01:36:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-01 01:37:33 | INFO | train_inner | epoch 010:   2414 / 3715 loss=6.444, nll_loss=3.492, mask_ins=1.101, word_ins_ml=4.977, word_reposition=0.366, ppl=87.05, wps=13379.2, ups=0.92, wpb=14485.6, bsz=1024, num_updates=35800, lr=0.000186859, gnorm=1.124, clip=0, loss_scale=1318, train_wall=107, wall=38549
2022-08-01 01:39:21 | INFO | train_inner | epoch 010:   2514 / 3715 loss=6.469, nll_loss=3.514, mask_ins=1.101, word_ins_ml=4.995, word_reposition=0.373, ppl=88.59, wps=13681, ups=0.93, wpb=14683.4, bsz=1024, num_updates=35900, lr=0.000186598, gnorm=1.023, clip=0, loss_scale=1024, train_wall=106, wall=38656
2022-08-01 01:41:08 | INFO | train_inner | epoch 010:   2614 / 3715 loss=6.452, nll_loss=3.507, mask_ins=1.099, word_ins_ml=4.99, word_reposition=0.364, ppl=87.54, wps=13621.8, ups=0.93, wpb=14617.1, bsz=1024, num_updates=36000, lr=0.000186339, gnorm=0.969, clip=0, loss_scale=1024, train_wall=106, wall=38763
2022-08-01 01:42:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-01 01:42:56 | INFO | train_inner | epoch 010:   2715 / 3715 loss=6.443, nll_loss=3.5, mask_ins=1.095, word_ins_ml=4.984, word_reposition=0.363, ppl=87.01, wps=13667.8, ups=0.92, wpb=14806.1, bsz=1024, num_updates=36100, lr=0.000186081, gnorm=1.001, clip=0, loss_scale=912, train_wall=107, wall=38872
2022-08-01 01:44:43 | INFO | train_inner | epoch 010:   2815 / 3715 loss=6.448, nll_loss=3.503, mask_ins=1.094, word_ins_ml=4.986, word_reposition=0.368, ppl=87.33, wps=13786.3, ups=0.93, wpb=14776.3, bsz=1024, num_updates=36200, lr=0.000185824, gnorm=0.999, clip=0, loss_scale=512, train_wall=105, wall=38979
2022-08-01 01:46:30 | INFO | train_inner | epoch 010:   2915 / 3715 loss=6.466, nll_loss=3.514, mask_ins=1.095, word_ins_ml=4.996, word_reposition=0.375, ppl=88.41, wps=13849.9, ups=0.94, wpb=14770.2, bsz=1024, num_updates=36300, lr=0.000185567, gnorm=1.037, clip=0, loss_scale=512, train_wall=105, wall=39085
2022-08-01 01:48:17 | INFO | train_inner | epoch 010:   3015 / 3715 loss=6.445, nll_loss=3.5, mask_ins=1.098, word_ins_ml=4.984, word_reposition=0.363, ppl=87.12, wps=13813.8, ups=0.94, wpb=14730.8, bsz=1024, num_updates=36400, lr=0.000185312, gnorm=0.992, clip=0, loss_scale=512, train_wall=105, wall=39192
2022-08-01 01:50:04 | INFO | train_inner | epoch 010:   3115 / 3715 loss=6.478, nll_loss=3.516, mask_ins=1.102, word_ins_ml=4.998, word_reposition=0.378, ppl=89.12, wps=13760.2, ups=0.93, wpb=14770.4, bsz=1024, num_updates=36500, lr=0.000185058, gnorm=1.093, clip=0, loss_scale=512, train_wall=106, wall=39299
2022-08-01 01:51:52 | INFO | train_inner | epoch 010:   3215 / 3715 loss=6.461, nll_loss=3.508, mask_ins=1.097, word_ins_ml=4.991, word_reposition=0.373, ppl=88.09, wps=13540.7, ups=0.93, wpb=14625.2, bsz=1024, num_updates=36600, lr=0.000184805, gnorm=0.963, clip=0, loss_scale=563, train_wall=106, wall=39407
2022-08-01 01:53:38 | INFO | train_inner | epoch 010:   3315 / 3715 loss=6.447, nll_loss=3.503, mask_ins=1.101, word_ins_ml=4.986, word_reposition=0.36, ppl=87.23, wps=13897.5, ups=0.94, wpb=14713.3, bsz=1024, num_updates=36700, lr=0.000184553, gnorm=1.023, clip=0, loss_scale=1024, train_wall=104, wall=39513
2022-08-01 01:55:25 | INFO | train_inner | epoch 010:   3415 / 3715 loss=6.427, nll_loss=3.482, mask_ins=1.094, word_ins_ml=4.967, word_reposition=0.366, ppl=86.04, wps=13718.6, ups=0.93, wpb=14744, bsz=1024, num_updates=36800, lr=0.000184302, gnorm=0.979, clip=0, loss_scale=1024, train_wall=106, wall=39621
2022-08-01 01:57:13 | INFO | train_inner | epoch 010:   3515 / 3715 loss=6.438, nll_loss=3.495, mask_ins=1.094, word_ins_ml=4.979, word_reposition=0.365, ppl=86.7, wps=13557.1, ups=0.93, wpb=14579.5, bsz=1024, num_updates=36900, lr=0.000184053, gnorm=0.98, clip=0, loss_scale=1024, train_wall=106, wall=39728
2022-08-01 01:59:01 | INFO | train_inner | epoch 010:   3615 / 3715 loss=6.441, nll_loss=3.495, mask_ins=1.098, word_ins_ml=4.979, word_reposition=0.364, ppl=86.87, wps=13458.7, ups=0.93, wpb=14497.3, bsz=1024, num_updates=37000, lr=0.000183804, gnorm=0.974, clip=0, loss_scale=1024, train_wall=106, wall=39836
2022-08-01 02:00:46 | INFO | train_inner | epoch 010:   3715 / 3715 loss=6.437, nll_loss=3.493, mask_ins=1.095, word_ins_ml=4.977, word_reposition=0.365, ppl=86.65, wps=13684.6, ups=0.95, wpb=14423.6, bsz=1014.7, num_updates=37100, lr=0.000183556, gnorm=0.975, clip=0, loss_scale=1024, train_wall=104, wall=39941
2022-08-01 02:00:46 | INFO | train | epoch 010 | loss 6.452 | nll_loss 3.501 | mask_ins 1.099 | word_ins_ml 4.985 | word_reposition 0.368 | ppl 87.54 | wps 13807.7 | ups 0.94 | wpb 14663.1 | bsz 1023.7 | num_updates 37100 | lr 0.000183556 | gnorm 1.002 | clip 0 | loss_scale 1901 | train_wall 3789 | wall 39941
2022-08-01 02:02:07 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.468 | nll_loss 3.398 | mask_ins 1.092 | word_ins_ml 4.984 | word_reposition 0.393 | ppl 88.55 | wps 33948.8 | wpb 1849.4 | bsz 127.9 | num_updates 37100 | best_loss 6.468
2022-08-01 02:02:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_best.pt (epoch 10 @ 37100 updates, score 6.468) (writing took 4.253407288342714 seconds)
2022-08-01 02:03:41 | INFO | train_inner | epoch 011:    100 / 3715 loss=6.414, nll_loss=3.465, mask_ins=1.099, word_ins_ml=4.953, word_reposition=0.362, ppl=85.27, wps=8368.9, ups=0.57, wpb=14669.8, bsz=1024, num_updates=37200, lr=0.000183309, gnorm=0.965, clip=0, loss_scale=2028, train_wall=89, wall=40117
2022-08-01 02:05:29 | INFO | train_inner | epoch 011:    200 / 3715 loss=6.397, nll_loss=3.445, mask_ins=1.096, word_ins_ml=4.935, word_reposition=0.367, ppl=84.29, wps=13674.8, ups=0.93, wpb=14694.8, bsz=1024, num_updates=37300, lr=0.000183063, gnorm=0.966, clip=0, loss_scale=2048, train_wall=106, wall=40224
2022-08-01 02:07:15 | INFO | train_inner | epoch 011:    300 / 3715 loss=6.409, nll_loss=3.46, mask_ins=1.095, word_ins_ml=4.948, word_reposition=0.366, ppl=84.96, wps=13787.4, ups=0.94, wpb=14695.7, bsz=1024, num_updates=37400, lr=0.000182818, gnorm=0.981, clip=0, loss_scale=2048, train_wall=105, wall=40331
2022-08-01 02:08:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-01 02:09:02 | INFO | train_inner | epoch 011:    401 / 3715 loss=6.407, nll_loss=3.458, mask_ins=1.093, word_ins_ml=4.947, word_reposition=0.367, ppl=84.86, wps=13683.1, ups=0.94, wpb=14634.3, bsz=1024, num_updates=37500, lr=0.000182574, gnorm=0.979, clip=0, loss_scale=1511, train_wall=105, wall=40438
2022-08-01 02:10:49 | INFO | train_inner | epoch 011:    501 / 3715 loss=6.419, nll_loss=3.471, mask_ins=1.096, word_ins_ml=4.958, word_reposition=0.365, ppl=85.59, wps=13640.1, ups=0.94, wpb=14511.2, bsz=1024, num_updates=37600, lr=0.000182331, gnorm=0.964, clip=0, loss_scale=1024, train_wall=105, wall=40544
2022-08-01 02:12:36 | INFO | train_inner | epoch 011:    601 / 3715 loss=6.417, nll_loss=3.471, mask_ins=1.095, word_ins_ml=4.958, word_reposition=0.363, ppl=85.45, wps=13728.1, ups=0.94, wpb=14674.8, bsz=1024, num_updates=37700, lr=0.000182089, gnorm=0.958, clip=0, loss_scale=1024, train_wall=105, wall=40651
2022-08-01 02:14:22 | INFO | train_inner | epoch 011:    701 / 3715 loss=6.411, nll_loss=3.461, mask_ins=1.097, word_ins_ml=4.95, word_reposition=0.365, ppl=85.08, wps=13879, ups=0.94, wpb=14725.9, bsz=1024, num_updates=37800, lr=0.000181848, gnorm=0.982, clip=0, loss_scale=1024, train_wall=104, wall=40757
2022-08-01 02:16:08 | INFO | train_inner | epoch 011:    801 / 3715 loss=6.4, nll_loss=3.456, mask_ins=1.091, word_ins_ml=4.945, word_reposition=0.364, ppl=84.43, wps=13868.1, ups=0.94, wpb=14739.9, bsz=1024, num_updates=37900, lr=0.000181608, gnorm=0.986, clip=0, loss_scale=1024, train_wall=105, wall=40863
2022-08-01 02:17:55 | INFO | train_inner | epoch 011:    901 / 3715 loss=6.4, nll_loss=3.451, mask_ins=1.09, word_ins_ml=4.94, word_reposition=0.371, ppl=84.46, wps=13734.7, ups=0.93, wpb=14711.8, bsz=1024, num_updates=38000, lr=0.000181369, gnorm=1.082, clip=0, loss_scale=1444, train_wall=105, wall=40971
2022-08-01 02:19:43 | INFO | train_inner | epoch 011:   1001 / 3715 loss=6.423, nll_loss=3.472, mask_ins=1.098, word_ins_ml=4.959, word_reposition=0.367, ppl=85.82, wps=13663.2, ups=0.93, wpb=14686.7, bsz=1024, num_updates=38100, lr=0.000181131, gnorm=0.969, clip=0, loss_scale=2048, train_wall=106, wall=41078
2022-08-01 02:21:29 | INFO | train_inner | epoch 011:   1101 / 3715 loss=6.436, nll_loss=3.484, mask_ins=1.094, word_ins_ml=4.969, word_reposition=0.373, ppl=86.58, wps=13679.3, ups=0.94, wpb=14625.4, bsz=1024, num_updates=38200, lr=0.000180894, gnorm=0.963, clip=0, loss_scale=2048, train_wall=105, wall=41185
2022-08-01 02:23:17 | INFO | train_inner | epoch 011:   1201 / 3715 loss=6.403, nll_loss=3.458, mask_ins=1.089, word_ins_ml=4.946, word_reposition=0.369, ppl=84.61, wps=13736.1, ups=0.93, wpb=14722.7, bsz=1024, num_updates=38300, lr=0.000180657, gnorm=0.971, clip=0, loss_scale=2048, train_wall=105, wall=41292
2022-08-01 02:25:04 | INFO | train_inner | epoch 011:   1301 / 3715 loss=6.428, nll_loss=3.486, mask_ins=1.098, word_ins_ml=4.971, word_reposition=0.359, ppl=86.13, wps=13508.5, ups=0.93, wpb=14491.9, bsz=1024, num_updates=38400, lr=0.000180422, gnorm=0.964, clip=0, loss_scale=2048, train_wall=106, wall=41399
2022-08-01 02:26:52 | INFO | train_inner | epoch 011:   1401 / 3715 loss=6.421, nll_loss=3.474, mask_ins=1.09, word_ins_ml=4.96, word_reposition=0.372, ppl=85.71, wps=13749.1, ups=0.93, wpb=14791.2, bsz=1024, num_updates=38500, lr=0.000180187, gnorm=0.976, clip=0, loss_scale=2642, train_wall=106, wall=41507
2022-08-01 02:28:38 | INFO | train_inner | epoch 011:   1501 / 3715 loss=6.405, nll_loss=3.458, mask_ins=1.093, word_ins_ml=4.946, word_reposition=0.367, ppl=84.76, wps=13802.4, ups=0.94, wpb=14724.2, bsz=1024, num_updates=38600, lr=0.000179954, gnorm=0.993, clip=0, loss_scale=4096, train_wall=105, wall=41614
2022-08-01 02:29:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-01 02:29:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-01 02:30:27 | INFO | train_inner | epoch 011:   1603 / 3715 loss=6.412, nll_loss=3.474, mask_ins=1.087, word_ins_ml=4.96, word_reposition=0.365, ppl=85.17, wps=13542.7, ups=0.92, wpb=14751.9, bsz=1024, num_updates=38700, lr=0.000179721, gnorm=1.105, clip=0, loss_scale=2078, train_wall=107, wall=41723
2022-08-01 02:32:16 | INFO | train_inner | epoch 011:   1703 / 3715 loss=6.413, nll_loss=3.476, mask_ins=1.09, word_ins_ml=4.962, word_reposition=0.361, ppl=85.22, wps=13454.1, ups=0.92, wpb=14672.2, bsz=1024, num_updates=38800, lr=0.00017949, gnorm=0.958, clip=0, loss_scale=1024, train_wall=107, wall=41832
2022-08-01 02:34:05 | INFO | train_inner | epoch 011:   1803 / 3715 loss=6.375, nll_loss=3.435, mask_ins=1.086, word_ins_ml=4.926, word_reposition=0.362, ppl=82.97, wps=13527.8, ups=0.92, wpb=14654.4, bsz=1024, num_updates=38900, lr=0.000179259, gnorm=0.966, clip=0, loss_scale=1024, train_wall=107, wall=41940
2022-08-01 02:35:53 | INFO | train_inner | epoch 011:   1903 / 3715 loss=6.396, nll_loss=3.459, mask_ins=1.087, word_ins_ml=4.947, word_reposition=0.362, ppl=84.23, wps=13454.5, ups=0.92, wpb=14611.9, bsz=1024, num_updates=39000, lr=0.000179029, gnorm=0.962, clip=0, loss_scale=1024, train_wall=107, wall=42049
2022-08-01 02:37:40 | INFO | train_inner | epoch 011:   2003 / 3715 loss=6.379, nll_loss=3.442, mask_ins=1.088, word_ins_ml=4.932, word_reposition=0.359, ppl=83.23, wps=13662.8, ups=0.93, wpb=14625.1, bsz=1023.8, num_updates=39100, lr=0.0001788, gnorm=0.964, clip=0, loss_scale=1024, train_wall=105, wall=42156
2022-08-01 02:39:27 | INFO | train_inner | epoch 011:   2103 / 3715 loss=6.391, nll_loss=3.451, mask_ins=1.083, word_ins_ml=4.94, word_reposition=0.367, ppl=83.9, wps=13693.6, ups=0.94, wpb=14564.7, bsz=1024, num_updates=39200, lr=0.000178571, gnorm=0.941, clip=0, loss_scale=1423, train_wall=105, wall=42262
2022-08-01 02:41:15 | INFO | train_inner | epoch 011:   2203 / 3715 loss=6.404, nll_loss=3.462, mask_ins=1.093, word_ins_ml=4.949, word_reposition=0.361, ppl=84.67, wps=13499.7, ups=0.92, wpb=14677.9, bsz=1024, num_updates=39300, lr=0.000178344, gnorm=0.964, clip=0, loss_scale=2048, train_wall=107, wall=42371
2022-08-01 02:43:03 | INFO | train_inner | epoch 011:   2303 / 3715 loss=6.391, nll_loss=3.452, mask_ins=1.089, word_ins_ml=4.94, word_reposition=0.362, ppl=83.9, wps=13603.8, ups=0.93, wpb=14636.6, bsz=1024, num_updates=39400, lr=0.000178118, gnorm=0.961, clip=0, loss_scale=2048, train_wall=106, wall=42478
2022-08-01 02:44:51 | INFO | train_inner | epoch 011:   2403 / 3715 loss=6.399, nll_loss=3.467, mask_ins=1.088, word_ins_ml=4.954, word_reposition=0.357, ppl=84.39, wps=13516.2, ups=0.93, wpb=14603.2, bsz=1024, num_updates=39500, lr=0.000177892, gnorm=0.976, clip=0, loss_scale=2048, train_wall=106, wall=42586
2022-08-01 02:46:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-01 02:46:40 | INFO | train_inner | epoch 011:   2504 / 3715 loss=6.405, nll_loss=3.462, mask_ins=1.09, word_ins_ml=4.949, word_reposition=0.365, ppl=84.73, wps=13584.2, ups=0.92, wpb=14779.7, bsz=1024, num_updates=39600, lr=0.000177667, gnorm=1.001, clip=0, loss_scale=1886, train_wall=107, wall=42695
2022-08-01 02:48:12 | INFO | train_inner | epoch 011:   2604 / 3715 loss=6.41, nll_loss=3.458, mask_ins=1.094, word_ins_ml=4.946, word_reposition=0.37, ppl=85.05, wps=15849.9, ups=1.08, wpb=14670.6, bsz=1024, num_updates=39700, lr=0.000177443, gnorm=0.98, clip=0, loss_scale=1024, train_wall=91, wall=42788
2022-08-01 02:50:01 | INFO | train_inner | epoch 011:   2704 / 3715 loss=6.397, nll_loss=3.462, mask_ins=1.088, word_ins_ml=4.949, word_reposition=0.359, ppl=84.26, wps=13489.5, ups=0.92, wpb=14684.5, bsz=1024, num_updates=39800, lr=0.00017722, gnorm=0.958, clip=0, loss_scale=1024, train_wall=107, wall=42897
2022-08-01 02:51:48 | INFO | train_inner | epoch 011:   2804 / 3715 loss=6.416, nll_loss=3.475, mask_ins=1.094, word_ins_ml=4.961, word_reposition=0.361, ppl=85.42, wps=13738.1, ups=0.93, wpb=14703.6, bsz=1024, num_updates=39900, lr=0.000176998, gnorm=0.961, clip=0, loss_scale=1024, train_wall=105, wall=43004
2022-08-01 02:53:37 | INFO | train_inner | epoch 011:   2904 / 3715 loss=6.388, nll_loss=3.446, mask_ins=1.087, word_ins_ml=4.935, word_reposition=0.367, ppl=83.78, wps=13492.4, ups=0.92, wpb=14628.8, bsz=1024, num_updates=40000, lr=0.000176777, gnorm=0.953, clip=0, loss_scale=1024, train_wall=107, wall=43112
2022-08-01 02:55:24 | INFO | train_inner | epoch 011:   3004 / 3715 loss=6.396, nll_loss=3.464, mask_ins=1.084, word_ins_ml=4.95, word_reposition=0.362, ppl=84.23, wps=13643.4, ups=0.93, wpb=14635.3, bsz=1024, num_updates=40100, lr=0.000176556, gnorm=0.967, clip=0, loss_scale=1065, train_wall=105, wall=43219
2022-08-01 02:57:13 | INFO | train_inner | epoch 011:   3104 / 3715 loss=6.397, nll_loss=3.466, mask_ins=1.088, word_ins_ml=4.953, word_reposition=0.356, ppl=84.27, wps=13471.8, ups=0.92, wpb=14669.5, bsz=1024, num_updates=40200, lr=0.000176336, gnorm=0.965, clip=0, loss_scale=2048, train_wall=107, wall=43328
2022-08-01 02:59:01 | INFO | train_inner | epoch 011:   3204 / 3715 loss=6.411, nll_loss=3.475, mask_ins=1.089, word_ins_ml=4.961, word_reposition=0.362, ppl=85.1, wps=13548, ups=0.93, wpb=14643.2, bsz=1024, num_updates=40300, lr=0.000176117, gnorm=0.951, clip=0, loss_scale=2048, train_wall=106, wall=43436
2022-08-01 03:00:50 | INFO | train_inner | epoch 011:   3304 / 3715 loss=6.389, nll_loss=3.456, mask_ins=1.085, word_ins_ml=4.943, word_reposition=0.361, ppl=83.81, wps=13507.3, ups=0.92, wpb=14694.9, bsz=1024, num_updates=40400, lr=0.000175899, gnorm=0.952, clip=0, loss_scale=2048, train_wall=107, wall=43545
2022-08-01 03:02:38 | INFO | train_inner | epoch 011:   3404 / 3715 loss=6.401, nll_loss=3.461, mask_ins=1.087, word_ins_ml=4.948, word_reposition=0.365, ppl=84.51, wps=13473.1, ups=0.92, wpb=14585.5, bsz=1024, num_updates=40500, lr=0.000175682, gnorm=0.968, clip=0, loss_scale=2048, train_wall=106, wall=43653
2022-08-01 03:04:26 | INFO | train_inner | epoch 011:   3504 / 3715 loss=6.395, nll_loss=3.456, mask_ins=1.093, word_ins_ml=4.944, word_reposition=0.358, ppl=84.14, wps=13520.8, ups=0.92, wpb=14642.5, bsz=1024, num_updates=40600, lr=0.000175466, gnorm=1.009, clip=0, loss_scale=2048, train_wall=107, wall=43762
2022-08-01 03:06:15 | INFO | train_inner | epoch 011:   3604 / 3715 loss=6.414, nll_loss=3.477, mask_ins=1.091, word_ins_ml=4.963, word_reposition=0.36, ppl=85.24, wps=13523.9, ups=0.92, wpb=14677.8, bsz=1024, num_updates=40700, lr=0.00017525, gnorm=0.957, clip=0, loss_scale=3932, train_wall=107, wall=43870
2022-08-01 03:06:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-01 03:08:04 | INFO | train_inner | epoch 011:   3705 / 3715 loss=6.421, nll_loss=3.475, mask_ins=1.097, word_ins_ml=4.961, word_reposition=0.363, ppl=85.68, wps=13502.2, ups=0.92, wpb=14694.4, bsz=1024, num_updates=40800, lr=0.000175035, gnorm=1.038, clip=0, loss_scale=2474, train_wall=107, wall=43979
2022-08-01 03:08:14 | INFO | train | epoch 011 | loss 6.405 | nll_loss 3.463 | mask_ins 1.091 | word_ins_ml 4.95 | word_reposition 0.364 | ppl 84.75 | wps 13438.9 | ups 0.92 | wpb 14661.9 | bsz 1023.7 | num_updates 40810 | lr 0.000175014 | gnorm 0.978 | clip 0 | loss_scale 1772 | train_wall 3897 | wall 43989
2022-08-01 03:09:35 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.45 | nll_loss 3.372 | mask_ins 1.089 | word_ins_ml 4.961 | word_reposition 0.4 | ppl 87.41 | wps 33758.2 | wpb 1849.4 | bsz 127.9 | num_updates 40810 | best_loss 6.45
2022-08-01 03:09:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_best.pt (epoch 11 @ 40810 updates, score 6.45) (writing took 4.386559549719095 seconds)
2022-08-01 03:11:17 | INFO | train_inner | epoch 012:     90 / 3715 loss=6.359, nll_loss=3.426, mask_ins=1.088, word_ins_ml=4.917, word_reposition=0.354, ppl=82.1, wps=7504.8, ups=0.52, wpb=14492.5, bsz=1014.7, num_updates=40900, lr=0.000174821, gnorm=0.984, clip=0, loss_scale=2048, train_wall=106, wall=44172
2022-08-01 03:13:04 | INFO | train_inner | epoch 012:    190 / 3715 loss=6.336, nll_loss=3.408, mask_ins=1.079, word_ins_ml=4.902, word_reposition=0.355, ppl=80.8, wps=13703.5, ups=0.93, wpb=14726.4, bsz=1024, num_updates=41000, lr=0.000174608, gnorm=0.951, clip=0, loss_scale=2048, train_wall=106, wall=44280
2022-08-01 03:14:55 | INFO | train_inner | epoch 012:    290 / 3715 loss=6.374, nll_loss=3.433, mask_ins=1.089, word_ins_ml=4.924, word_reposition=0.362, ppl=82.95, wps=13199.7, ups=0.9, wpb=14669.4, bsz=1024, num_updates=41100, lr=0.000174395, gnorm=0.962, clip=0, loss_scale=2048, train_wall=109, wall=44391
2022-08-01 03:16:47 | INFO | train_inner | epoch 012:    390 / 3715 loss=6.356, nll_loss=3.422, mask_ins=1.082, word_ins_ml=4.915, word_reposition=0.36, ppl=81.93, wps=13174.6, ups=0.9, wpb=14698.2, bsz=1024, num_updates=41200, lr=0.000174183, gnorm=0.997, clip=0, loss_scale=2048, train_wall=110, wall=44502
2022-08-01 03:18:35 | INFO | train_inner | epoch 012:    490 / 3715 loss=6.379, nll_loss=3.439, mask_ins=1.086, word_ins_ml=4.928, word_reposition=0.364, ppl=83.21, wps=13634.6, ups=0.92, wpb=14796.2, bsz=1024, num_updates=41300, lr=0.000173972, gnorm=0.958, clip=0, loss_scale=3441, train_wall=107, wall=44611
2022-08-01 03:20:24 | INFO | train_inner | epoch 012:    590 / 3715 loss=6.362, nll_loss=3.435, mask_ins=1.079, word_ins_ml=4.926, word_reposition=0.357, ppl=82.23, wps=13473.2, ups=0.92, wpb=14622.2, bsz=1024, num_updates=41400, lr=0.000173762, gnorm=0.987, clip=0, loss_scale=4096, train_wall=107, wall=44719
2022-08-01 03:22:13 | INFO | train_inner | epoch 012:    690 / 3715 loss=6.353, nll_loss=3.415, mask_ins=1.079, word_ins_ml=4.908, word_reposition=0.365, ppl=81.72, wps=13523.9, ups=0.92, wpb=14731.4, bsz=1024, num_updates=41500, lr=0.000173553, gnorm=0.965, clip=0, loss_scale=4096, train_wall=107, wall=44828
2022-08-01 03:24:01 | INFO | train_inner | epoch 012:    790 / 3715 loss=6.374, nll_loss=3.43, mask_ins=1.087, word_ins_ml=4.921, word_reposition=0.366, ppl=82.96, wps=13684.6, ups=0.92, wpb=14844.3, bsz=1024, num_updates=41600, lr=0.000173344, gnorm=0.958, clip=0, loss_scale=4096, train_wall=107, wall=44937
2022-08-01 03:25:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-01 03:25:50 | INFO | train_inner | epoch 012:    891 / 3715 loss=6.367, nll_loss=3.43, mask_ins=1.085, word_ins_ml=4.921, word_reposition=0.361, ppl=82.52, wps=13406.1, ups=0.92, wpb=14638.8, bsz=1024, num_updates=41700, lr=0.000173136, gnorm=0.989, clip=0, loss_scale=3751, train_wall=107, wall=45046
2022-08-01 03:27:38 | INFO | train_inner | epoch 012:    991 / 3715 loss=6.361, nll_loss=3.425, mask_ins=1.085, word_ins_ml=4.916, word_reposition=0.361, ppl=82.22, wps=13623, ups=0.93, wpb=14690.7, bsz=1024, num_updates=41800, lr=0.000172929, gnorm=0.957, clip=0, loss_scale=2048, train_wall=106, wall=45154
2022-08-01 03:29:27 | INFO | train_inner | epoch 012:   1091 / 3715 loss=6.358, nll_loss=3.425, mask_ins=1.079, word_ins_ml=4.916, word_reposition=0.363, ppl=82.03, wps=13513.7, ups=0.92, wpb=14708.6, bsz=1024, num_updates=41900, lr=0.000172722, gnorm=0.989, clip=0, loss_scale=2048, train_wall=107, wall=45263
2022-08-01 03:31:17 | INFO | train_inner | epoch 012:   1191 / 3715 loss=6.355, nll_loss=3.42, mask_ins=1.078, word_ins_ml=4.912, word_reposition=0.365, ppl=81.83, wps=13310, ups=0.91, wpb=14652.2, bsz=1024, num_updates=42000, lr=0.000172516, gnorm=0.979, clip=0, loss_scale=2048, train_wall=108, wall=45373
2022-08-01 03:32:57 | INFO | train_inner | epoch 012:   1291 / 3715 loss=6.386, nll_loss=3.447, mask_ins=1.095, word_ins_ml=4.936, word_reposition=0.356, ppl=83.65, wps=14646.9, ups=1, wpb=14657.1, bsz=1024, num_updates=42100, lr=0.000172311, gnorm=0.952, clip=0, loss_scale=2048, train_wall=98, wall=45473
2022-08-01 03:34:41 | INFO | train_inner | epoch 012:   1391 / 3715 loss=6.368, nll_loss=3.432, mask_ins=1.083, word_ins_ml=4.922, word_reposition=0.363, ppl=82.59, wps=14149, ups=0.96, wpb=14683.5, bsz=1024, num_updates=42200, lr=0.000172107, gnorm=0.954, clip=0, loss_scale=2150, train_wall=102, wall=45577
2022-08-01 03:36:28 | INFO | train_inner | epoch 012:   1491 / 3715 loss=6.365, nll_loss=3.428, mask_ins=1.086, word_ins_ml=4.919, word_reposition=0.36, ppl=82.41, wps=13713, ups=0.93, wpb=14667.3, bsz=1024, num_updates=42300, lr=0.000171904, gnorm=0.949, clip=0, loss_scale=4096, train_wall=105, wall=45683
2022-08-01 03:38:16 | INFO | train_inner | epoch 012:   1591 / 3715 loss=6.37, nll_loss=3.432, mask_ins=1.085, word_ins_ml=4.923, word_reposition=0.362, ppl=82.71, wps=13595.4, ups=0.93, wpb=14647, bsz=1024, num_updates=42400, lr=0.000171701, gnorm=0.964, clip=0, loss_scale=4096, train_wall=106, wall=45791
2022-08-01 03:40:04 | INFO | train_inner | epoch 012:   1691 / 3715 loss=6.362, nll_loss=3.424, mask_ins=1.082, word_ins_ml=4.915, word_reposition=0.364, ppl=82.23, wps=13618.5, ups=0.92, wpb=14742, bsz=1024, num_updates=42500, lr=0.000171499, gnorm=0.957, clip=0, loss_scale=4096, train_wall=107, wall=45899
2022-08-01 03:41:52 | INFO | train_inner | epoch 012:   1791 / 3715 loss=6.346, nll_loss=3.422, mask_ins=1.076, word_ins_ml=4.913, word_reposition=0.357, ppl=81.35, wps=13465.1, ups=0.93, wpb=14545.9, bsz=1024, num_updates=42600, lr=0.000171297, gnorm=0.952, clip=0, loss_scale=4096, train_wall=106, wall=46007
2022-08-01 03:43:39 | INFO | train_inner | epoch 012:   1891 / 3715 loss=6.38, nll_loss=3.431, mask_ins=1.093, word_ins_ml=4.922, word_reposition=0.365, ppl=83.29, wps=13909.5, ups=0.94, wpb=14864.6, bsz=1024, num_updates=42700, lr=0.000171096, gnorm=0.999, clip=0, loss_scale=4096, train_wall=105, wall=46114
2022-08-01 03:45:26 | INFO | train_inner | epoch 012:   1991 / 3715 loss=6.361, nll_loss=3.421, mask_ins=1.084, word_ins_ml=4.913, word_reposition=0.364, ppl=82.17, wps=13798.6, ups=0.94, wpb=14745, bsz=1024, num_updates=42800, lr=0.000170896, gnorm=0.96, clip=0, loss_scale=7905, train_wall=105, wall=46221
2022-08-01 03:47:13 | INFO | train_inner | epoch 012:   2091 / 3715 loss=6.359, nll_loss=3.421, mask_ins=1.085, word_ins_ml=4.913, word_reposition=0.361, ppl=82.07, wps=13685.8, ups=0.93, wpb=14687.4, bsz=1024, num_updates=42900, lr=0.000170697, gnorm=0.957, clip=0, loss_scale=8192, train_wall=106, wall=46329
2022-08-01 03:49:00 | INFO | train_inner | epoch 012:   2191 / 3715 loss=6.372, nll_loss=3.429, mask_ins=1.085, word_ins_ml=4.92, word_reposition=0.368, ppl=82.85, wps=13786.6, ups=0.94, wpb=14681.3, bsz=1024, num_updates=43000, lr=0.000170499, gnorm=0.954, clip=0, loss_scale=8192, train_wall=105, wall=46435
2022-08-01 03:50:47 | INFO | train_inner | epoch 012:   2291 / 3715 loss=6.363, nll_loss=3.43, mask_ins=1.083, word_ins_ml=4.921, word_reposition=0.359, ppl=82.33, wps=13623.6, ups=0.93, wpb=14573.8, bsz=1024, num_updates=43100, lr=0.000170301, gnorm=0.964, clip=0, loss_scale=8192, train_wall=105, wall=46542
2022-08-01 03:52:34 | INFO | train_inner | epoch 012:   2391 / 3715 loss=6.356, nll_loss=3.436, mask_ins=1.079, word_ins_ml=4.926, word_reposition=0.351, ppl=81.89, wps=13610, ups=0.93, wpb=14576.2, bsz=1024, num_updates=43200, lr=0.000170103, gnorm=0.959, clip=0, loss_scale=8192, train_wall=105, wall=46649
2022-08-01 03:54:21 | INFO | train_inner | epoch 012:   2491 / 3715 loss=6.39, nll_loss=3.455, mask_ins=1.083, word_ins_ml=4.942, word_reposition=0.365, ppl=83.89, wps=13625.3, ups=0.93, wpb=14621.9, bsz=1024, num_updates=43300, lr=0.000169907, gnorm=0.955, clip=0, loss_scale=14828, train_wall=106, wall=46756
2022-08-01 03:55:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-01 03:56:09 | INFO | train_inner | epoch 012:   2592 / 3715 loss=6.376, nll_loss=3.438, mask_ins=1.085, word_ins_ml=4.927, word_reposition=0.364, ppl=83.05, wps=13575.6, ups=0.93, wpb=14667.7, bsz=1024, num_updates=43400, lr=0.000169711, gnorm=0.959, clip=0, loss_scale=13626, train_wall=106, wall=46864
2022-08-01 03:57:56 | INFO | train_inner | epoch 012:   2692 / 3715 loss=6.355, nll_loss=3.425, mask_ins=1.084, word_ins_ml=4.916, word_reposition=0.356, ppl=81.88, wps=13641.8, ups=0.93, wpb=14640.6, bsz=1024, num_updates=43500, lr=0.000169516, gnorm=0.972, clip=0, loss_scale=8192, train_wall=106, wall=46972
2022-08-01 03:59:43 | INFO | train_inner | epoch 012:   2792 / 3715 loss=6.372, nll_loss=3.44, mask_ins=1.081, word_ins_ml=4.929, word_reposition=0.361, ppl=82.82, wps=13704.7, ups=0.94, wpb=14570.3, bsz=1024, num_updates=43600, lr=0.000169321, gnorm=0.967, clip=0, loss_scale=8192, train_wall=105, wall=47078
2022-08-01 04:01:30 | INFO | train_inner | epoch 012:   2892 / 3715 loss=6.354, nll_loss=3.42, mask_ins=1.084, word_ins_ml=4.911, word_reposition=0.359, ppl=81.78, wps=13553, ups=0.93, wpb=14563.9, bsz=1024, num_updates=43700, lr=0.000169128, gnorm=0.959, clip=0, loss_scale=8192, train_wall=106, wall=47186
2022-08-01 04:03:17 | INFO | train_inner | epoch 012:   2992 / 3715 loss=6.357, nll_loss=3.431, mask_ins=1.081, word_ins_ml=4.921, word_reposition=0.355, ppl=81.96, wps=13644.8, ups=0.93, wpb=14614.9, bsz=1024, num_updates=43800, lr=0.000168934, gnorm=0.948, clip=0, loss_scale=8192, train_wall=105, wall=47293
2022-08-01 04:05:04 | INFO | train_inner | epoch 012:   3092 / 3715 loss=6.354, nll_loss=3.425, mask_ins=1.079, word_ins_ml=4.916, word_reposition=0.359, ppl=81.8, wps=13735.6, ups=0.94, wpb=14616.4, bsz=1024, num_updates=43900, lr=0.000168742, gnorm=0.976, clip=0, loss_scale=9994, train_wall=105, wall=47399
2022-08-01 04:06:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-01 04:06:51 | INFO | train_inner | epoch 012:   3193 / 3715 loss=6.376, nll_loss=3.443, mask_ins=1.084, word_ins_ml=4.931, word_reposition=0.36, ppl=83.04, wps=13621.3, ups=0.93, wpb=14591.4, bsz=1024, num_updates=44000, lr=0.00016855, gnorm=0.983, clip=0, loss_scale=12815, train_wall=105, wall=47506
2022-08-01 04:08:38 | INFO | train_inner | epoch 012:   3293 / 3715 loss=6.348, nll_loss=3.42, mask_ins=1.076, word_ins_ml=4.911, word_reposition=0.36, ppl=81.45, wps=13733.6, ups=0.93, wpb=14721.6, bsz=1024, num_updates=44100, lr=0.000168359, gnorm=0.942, clip=0, loss_scale=8192, train_wall=105, wall=47613
2022-08-01 04:10:24 | INFO | train_inner | epoch 012:   3393 / 3715 loss=6.361, nll_loss=3.43, mask_ins=1.078, word_ins_ml=4.921, word_reposition=0.363, ppl=82.22, wps=13824.3, ups=0.94, wpb=14684.5, bsz=1024, num_updates=44200, lr=0.000168168, gnorm=0.945, clip=0, loss_scale=8192, train_wall=104, wall=47720
2022-08-01 04:12:10 | INFO | train_inner | epoch 012:   3493 / 3715 loss=6.351, nll_loss=3.424, mask_ins=1.079, word_ins_ml=4.914, word_reposition=0.358, ppl=81.65, wps=13730.3, ups=0.94, wpb=14594.8, bsz=1024, num_updates=44300, lr=0.000167978, gnorm=0.954, clip=0, loss_scale=8192, train_wall=105, wall=47826
2022-08-01 04:13:58 | INFO | train_inner | epoch 012:   3593 / 3715 loss=6.379, nll_loss=3.442, mask_ins=1.088, word_ins_ml=4.93, word_reposition=0.36, ppl=83.22, wps=13630.2, ups=0.93, wpb=14661.6, bsz=1024, num_updates=44400, lr=0.000167789, gnorm=0.956, clip=0, loss_scale=8192, train_wall=106, wall=47933
2022-08-01 04:15:45 | INFO | train_inner | epoch 012:   3693 / 3715 loss=6.337, nll_loss=3.411, mask_ins=1.077, word_ins_ml=4.904, word_reposition=0.357, ppl=80.87, wps=13663.3, ups=0.93, wpb=14634.8, bsz=1023.8, num_updates=44500, lr=0.0001676, gnorm=0.946, clip=0, loss_scale=10813, train_wall=105, wall=48041
2022-08-01 04:15:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-01 04:16:08 | INFO | train | epoch 012 | loss 6.363 | nll_loss 3.428 | mask_ins 1.083 | word_ins_ml 4.919 | word_reposition 0.361 | ppl 82.3 | wps 13354.2 | ups 0.91 | wpb 14661.9 | bsz 1023.7 | num_updates 44521 | lr 0.000167561 | gnorm 0.964 | clip 0 | loss_scale 6182 | train_wall 3923 | wall 48063
2022-08-01 04:17:13 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.401 | nll_loss 3.344 | mask_ins 1.082 | word_ins_ml 4.936 | word_reposition 0.383 | ppl 84.49 | wps 42096.4 | wpb 1849.4 | bsz 127.9 | num_updates 44521 | best_loss 6.401
2022-08-01 04:17:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_best.pt (epoch 12 @ 44521 updates, score 6.401) (writing took 4.366862406954169 seconds)
2022-08-01 04:18:41 | INFO | train_inner | epoch 013:     79 / 3715 loss=6.317, nll_loss=3.382, mask_ins=1.075, word_ins_ml=4.878, word_reposition=0.363, ppl=79.7, wps=8276.7, ups=0.57, wpb=14572.5, bsz=1014.7, num_updates=44600, lr=0.000167412, gnorm=0.985, clip=0, loss_scale=9165, train_wall=105, wall=48217
2022-08-01 04:20:28 | INFO | train_inner | epoch 013:    179 / 3715 loss=6.335, nll_loss=3.402, mask_ins=1.077, word_ins_ml=4.896, word_reposition=0.362, ppl=80.7, wps=13720, ups=0.94, wpb=14585.4, bsz=1024, num_updates=44700, lr=0.000167225, gnorm=0.96, clip=0, loss_scale=8192, train_wall=105, wall=48323
2022-08-01 04:22:15 | INFO | train_inner | epoch 013:    279 / 3715 loss=6.317, nll_loss=3.38, mask_ins=1.079, word_ins_ml=4.876, word_reposition=0.363, ppl=79.75, wps=13725.3, ups=0.93, wpb=14722.8, bsz=1024, num_updates=44800, lr=0.000167038, gnorm=0.968, clip=0, loss_scale=8192, train_wall=106, wall=48430
2022-08-01 04:24:02 | INFO | train_inner | epoch 013:    379 / 3715 loss=6.325, nll_loss=3.393, mask_ins=1.08, word_ins_ml=4.888, word_reposition=0.358, ppl=80.2, wps=13601.1, ups=0.93, wpb=14645.1, bsz=1024, num_updates=44900, lr=0.000166852, gnorm=0.958, clip=0, loss_scale=8192, train_wall=106, wall=48538
2022-08-01 04:25:50 | INFO | train_inner | epoch 013:    479 / 3715 loss=6.337, nll_loss=3.402, mask_ins=1.082, word_ins_ml=4.896, word_reposition=0.358, ppl=80.84, wps=13576.3, ups=0.93, wpb=14570.8, bsz=1024, num_updates=45000, lr=0.000166667, gnorm=0.967, clip=0, loss_scale=8192, train_wall=106, wall=48645
2022-08-01 04:26:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-08-01 04:27:37 | INFO | train_inner | epoch 013:    580 / 3715 loss=6.338, nll_loss=3.405, mask_ins=1.079, word_ins_ml=4.899, word_reposition=0.36, ppl=80.9, wps=13658.3, ups=0.93, wpb=14638.4, bsz=1024, num_updates=45100, lr=0.000166482, gnorm=0.96, clip=0, loss_scale=8841, train_wall=105, wall=48752
2022-08-01 04:29:23 | INFO | train_inner | epoch 013:    680 / 3715 loss=6.311, nll_loss=3.384, mask_ins=1.074, word_ins_ml=4.88, word_reposition=0.356, ppl=79.38, wps=13837.9, ups=0.94, wpb=14731.8, bsz=1024, num_updates=45200, lr=0.000166298, gnorm=0.954, clip=0, loss_scale=8192, train_wall=105, wall=48859
2022-08-01 04:30:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-08-01 04:31:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-08-01 04:31:12 | INFO | train_inner | epoch 013:    782 / 3715 loss=6.333, nll_loss=3.403, mask_ins=1.076, word_ins_ml=4.896, word_reposition=0.36, ppl=80.59, wps=13499.7, ups=0.92, wpb=14634.1, bsz=1024, num_updates=45300, lr=0.000166114, gnorm=0.976, clip=0, loss_scale=6746, train_wall=107, wall=48967
2022-08-01 04:31:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-08-01 04:33:01 | INFO | train_inner | epoch 013:    883 / 3715 loss=6.301, nll_loss=3.372, mask_ins=1.072, word_ins_ml=4.869, word_reposition=0.361, ppl=78.87, wps=13481.9, ups=0.92, wpb=14680.3, bsz=1024, num_updates=45400, lr=0.000165931, gnorm=1.148, clip=0, loss_scale=1369, train_wall=107, wall=49076
2022-08-01 04:34:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-01 04:34:50 | INFO | train_inner | epoch 013:    984 / 3715 loss=6.333, nll_loss=3.407, mask_ins=1.079, word_ins_ml=4.9, word_reposition=0.353, ppl=80.61, wps=13533.6, ups=0.92, wpb=14733.1, bsz=1024, num_updates=45500, lr=0.000165748, gnorm=1.075, clip=0, loss_scale=978, train_wall=107, wall=49185
2022-08-01 04:36:37 | INFO | train_inner | epoch 013:   1084 / 3715 loss=6.326, nll_loss=3.399, mask_ins=1.076, word_ins_ml=4.893, word_reposition=0.357, ppl=80.2, wps=13679.8, ups=0.93, wpb=14721.5, bsz=1024, num_updates=45600, lr=0.000165567, gnorm=1.092, clip=0, loss_scale=512, train_wall=106, wall=49293
2022-08-01 04:38:25 | INFO | train_inner | epoch 013:   1184 / 3715 loss=6.332, nll_loss=3.401, mask_ins=1.079, word_ins_ml=4.894, word_reposition=0.359, ppl=80.57, wps=13653.3, ups=0.93, wpb=14691.2, bsz=1024, num_updates=45700, lr=0.000165385, gnorm=1.258, clip=0, loss_scale=512, train_wall=106, wall=49400
2022-08-01 04:39:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-08-01 04:40:12 | INFO | train_inner | epoch 013:   1285 / 3715 loss=6.34, nll_loss=3.407, mask_ins=1.078, word_ins_ml=4.9, word_reposition=0.362, ppl=81.01, wps=13727.2, ups=0.93, wpb=14703.1, bsz=1024, num_updates=45800, lr=0.000165205, gnorm=1.038, clip=0, loss_scale=360, train_wall=105, wall=49507
2022-08-01 04:41:58 | INFO | train_inner | epoch 013:   1385 / 3715 loss=6.312, nll_loss=3.382, mask_ins=1.075, word_ins_ml=4.878, word_reposition=0.359, ppl=79.44, wps=14058.3, ups=0.94, wpb=14921.1, bsz=1024, num_updates=45900, lr=0.000165025, gnorm=0.985, clip=0, loss_scale=256, train_wall=104, wall=49613
2022-08-01 04:42:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-01 04:43:46 | INFO | train_inner | epoch 013:   1486 / 3715 loss=6.326, nll_loss=3.393, mask_ins=1.078, word_ins_ml=4.888, word_reposition=0.36, ppl=80.2, wps=13645.1, ups=0.93, wpb=14668.6, bsz=1024, num_updates=46000, lr=0.000164845, gnorm=1.098, clip=0, loss_scale=169, train_wall=106, wall=49721
2022-08-01 04:45:31 | INFO | train_inner | epoch 013:   1586 / 3715 loss=6.299, nll_loss=3.373, mask_ins=1.075, word_ins_ml=4.87, word_reposition=0.354, ppl=78.75, wps=13885.8, ups=0.95, wpb=14658.4, bsz=1024, num_updates=46100, lr=0.000164666, gnorm=1.473, clip=1, loss_scale=128, train_wall=104, wall=49827
2022-08-01 04:47:18 | INFO | train_inner | epoch 013:   1686 / 3715 loss=6.346, nll_loss=3.421, mask_ins=1.082, word_ins_ml=4.913, word_reposition=0.351, ppl=81.34, wps=13679.4, ups=0.94, wpb=14581.6, bsz=1024, num_updates=46200, lr=0.000164488, gnorm=0.993, clip=0, loss_scale=128, train_wall=105, wall=49933
2022-08-01 04:49:05 | INFO | train_inner | epoch 013:   1786 / 3715 loss=6.333, nll_loss=3.402, mask_ins=1.079, word_ins_ml=4.895, word_reposition=0.359, ppl=80.63, wps=13674, ups=0.93, wpb=14668.9, bsz=1024, num_updates=46300, lr=0.00016431, gnorm=1.009, clip=0, loss_scale=128, train_wall=106, wall=50040
2022-08-01 04:50:52 | INFO | train_inner | epoch 013:   1886 / 3715 loss=6.34, nll_loss=3.408, mask_ins=1.074, word_ins_ml=4.9, word_reposition=0.366, ppl=81.01, wps=13739.8, ups=0.93, wpb=14702.2, bsz=1024, num_updates=46400, lr=0.000164133, gnorm=1.491, clip=0, loss_scale=128, train_wall=105, wall=50147
2022-08-01 04:52:38 | INFO | train_inner | epoch 013:   1986 / 3715 loss=6.35, nll_loss=3.421, mask_ins=1.077, word_ins_ml=4.912, word_reposition=0.361, ppl=81.56, wps=13847.7, ups=0.94, wpb=14678.3, bsz=1024, num_updates=46500, lr=0.000163956, gnorm=1.787, clip=0, loss_scale=201, train_wall=104, wall=50253
2022-08-01 04:54:26 | INFO | train_inner | epoch 013:   2086 / 3715 loss=6.368, nll_loss=3.434, mask_ins=1.086, word_ins_ml=4.923, word_reposition=0.36, ppl=82.61, wps=13547.5, ups=0.93, wpb=14597.5, bsz=1024, num_updates=46600, lr=0.00016378, gnorm=2.055, clip=1, loss_scale=256, train_wall=106, wall=50361
2022-08-01 04:56:13 | INFO | train_inner | epoch 013:   2186 / 3715 loss=6.321, nll_loss=3.391, mask_ins=1.072, word_ins_ml=4.886, word_reposition=0.362, ppl=79.93, wps=13742, ups=0.94, wpb=14694.6, bsz=1024, num_updates=46700, lr=0.000163605, gnorm=1.775, clip=0, loss_scale=256, train_wall=105, wall=50468
2022-08-01 04:58:00 | INFO | train_inner | epoch 013:   2286 / 3715 loss=6.342, nll_loss=3.408, mask_ins=1.083, word_ins_ml=4.901, word_reposition=0.358, ppl=81.1, wps=13650.1, ups=0.93, wpb=14678.9, bsz=1024, num_updates=46800, lr=0.00016343, gnorm=2.221, clip=0, loss_scale=256, train_wall=106, wall=50576
2022-08-01 04:58:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-01 04:59:50 | INFO | train_inner | epoch 013:   2387 / 3715 loss=6.331, nll_loss=3.407, mask_ins=1.076, word_ins_ml=4.9, word_reposition=0.355, ppl=80.48, wps=13353.1, ups=0.91, wpb=14618, bsz=1024, num_updates=46900, lr=0.000163256, gnorm=1.28, clip=0, loss_scale=166, train_wall=108, wall=50685
2022-08-01 05:01:25 | INFO | train_inner | epoch 013:   2487 / 3715 loss=6.333, nll_loss=3.407, mask_ins=1.078, word_ins_ml=4.9, word_reposition=0.355, ppl=80.62, wps=15233.8, ups=1.05, wpb=14574.6, bsz=1024, num_updates=47000, lr=0.000163082, gnorm=1.548, clip=1, loss_scale=128, train_wall=94, wall=50781
2022-08-01 05:03:13 | INFO | train_inner | epoch 013:   2587 / 3715 loss=6.332, nll_loss=3.405, mask_ins=1.079, word_ins_ml=4.898, word_reposition=0.355, ppl=80.56, wps=13731.4, ups=0.93, wpb=14746.3, bsz=1024, num_updates=47100, lr=0.000162909, gnorm=1.949, clip=1, loss_scale=128, train_wall=106, wall=50888
2022-08-01 05:05:01 | INFO | train_inner | epoch 013:   2687 / 3715 loss=6.328, nll_loss=3.395, mask_ins=1.078, word_ins_ml=4.889, word_reposition=0.36, ppl=80.36, wps=13564.3, ups=0.92, wpb=14713.2, bsz=1024, num_updates=47200, lr=0.000162736, gnorm=1.368, clip=0, loss_scale=128, train_wall=107, wall=50997
2022-08-01 05:06:50 | INFO | train_inner | epoch 013:   2787 / 3715 loss=6.346, nll_loss=3.413, mask_ins=1.081, word_ins_ml=4.905, word_reposition=0.36, ppl=81.36, wps=13468.1, ups=0.92, wpb=14595.2, bsz=1024, num_updates=47300, lr=0.000162564, gnorm=1.295, clip=0, loss_scale=128, train_wall=107, wall=51105
2022-08-01 05:08:37 | INFO | train_inner | epoch 013:   2887 / 3715 loss=6.322, nll_loss=3.392, mask_ins=1.076, word_ins_ml=4.886, word_reposition=0.359, ppl=79.98, wps=13644.2, ups=0.93, wpb=14597.1, bsz=1024, num_updates=47400, lr=0.000162392, gnorm=1.396, clip=1, loss_scale=204, train_wall=105, wall=51212
2022-08-01 05:09:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-08-01 05:10:24 | INFO | train_inner | epoch 013:   2988 / 3715 loss=6.339, nll_loss=3.409, mask_ins=1.084, word_ins_ml=4.901, word_reposition=0.354, ppl=80.94, wps=13556.9, ups=0.93, wpb=14583.9, bsz=1024, num_updates=47500, lr=0.000162221, gnorm=1.146, clip=0, loss_scale=171, train_wall=106, wall=51320
2022-08-01 05:12:11 | INFO | train_inner | epoch 013:   3088 / 3715 loss=6.336, nll_loss=3.405, mask_ins=1.077, word_ins_ml=4.898, word_reposition=0.36, ppl=80.78, wps=13687.6, ups=0.94, wpb=14631, bsz=1024, num_updates=47600, lr=0.000162051, gnorm=1.103, clip=0, loss_scale=128, train_wall=105, wall=51426
2022-08-01 05:13:58 | INFO | train_inner | epoch 013:   3188 / 3715 loss=6.316, nll_loss=3.387, mask_ins=1.074, word_ins_ml=4.882, word_reposition=0.359, ppl=79.64, wps=13746.1, ups=0.94, wpb=14685.6, bsz=1024, num_updates=47700, lr=0.000161881, gnorm=1.593, clip=1, loss_scale=128, train_wall=105, wall=51533
2022-08-01 05:15:45 | INFO | train_inner | epoch 013:   3288 / 3715 loss=6.323, nll_loss=3.398, mask_ins=1.081, word_ins_ml=4.892, word_reposition=0.351, ppl=80.07, wps=13749.4, ups=0.94, wpb=14665.7, bsz=1024, num_updates=47800, lr=0.000161712, gnorm=1.094, clip=0, loss_scale=128, train_wall=105, wall=51640
2022-08-01 05:16:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-08-01 05:17:32 | INFO | train_inner | epoch 013:   3389 / 3715 loss=6.316, nll_loss=3.39, mask_ins=1.072, word_ins_ml=4.885, word_reposition=0.359, ppl=79.69, wps=13492.7, ups=0.93, wpb=14549.8, bsz=1023.8, num_updates=47900, lr=0.000161543, gnorm=1.11, clip=0, loss_scale=106, train_wall=106, wall=51748
2022-08-01 05:19:18 | INFO | train_inner | epoch 013:   3489 / 3715 loss=6.341, nll_loss=3.411, mask_ins=1.08, word_ins_ml=4.903, word_reposition=0.358, ppl=81.07, wps=13780.4, ups=0.94, wpb=14615.3, bsz=1024, num_updates=48000, lr=0.000161374, gnorm=1.09, clip=0, loss_scale=64, train_wall=104, wall=51854
2022-08-01 05:21:05 | INFO | train_inner | epoch 013:   3589 / 3715 loss=6.341, nll_loss=3.409, mask_ins=1.074, word_ins_ml=4.902, word_reposition=0.365, ppl=81.05, wps=13800.7, ups=0.94, wpb=14720.4, bsz=1024, num_updates=48100, lr=0.000161206, gnorm=1.006, clip=0, loss_scale=64, train_wall=105, wall=51961
2022-08-01 05:22:53 | INFO | train_inner | epoch 013:   3689 / 3715 loss=6.318, nll_loss=3.401, mask_ins=1.075, word_ins_ml=4.894, word_reposition=0.349, ppl=79.76, wps=13598.9, ups=0.93, wpb=14685.6, bsz=1024, num_updates=48200, lr=0.000161039, gnorm=1.013, clip=0, loss_scale=64, train_wall=106, wall=52069
2022-08-01 05:23:20 | INFO | train | epoch 013 | loss 6.33 | nll_loss 3.4 | mask_ins 1.078 | word_ins_ml 4.894 | word_reposition 0.358 | ppl 80.44 | wps 13473.5 | ups 0.92 | wpb 14661.9 | bsz 1023.7 | num_updates 48226 | lr 0.000160996 | gnorm 1.249 | clip 0.2 | loss_scale 1902 | train_wall 3897 | wall 52095
2022-08-01 05:24:41 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.406 | nll_loss 3.345 | mask_ins 1.073 | word_ins_ml 4.938 | word_reposition 0.394 | ppl 84.79 | wps 33737 | wpb 1849.4 | bsz 127.9 | num_updates 48226 | best_loss 6.401
2022-08-01 05:24:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased_Ggw/checkpoint_last.pt (epoch 13 @ 48226 updates, score 6.406) (writing took 2.867445558309555 seconds)
2022-08-01 05:26:03 | INFO | train_inner | epoch 014:     74 / 3715 loss=6.301, nll_loss=3.374, mask_ins=1.071, word_ins_ml=4.871, word_reposition=0.359, ppl=78.83, wps=7670.7, ups=0.53, wpb=14549, bsz=1014.7, num_updates=48300, lr=0.000160872, gnorm=1.281, clip=1, loss_scale=64, train_wall=104, wall=52258
2022-08-01 05:27:51 | INFO | train_inner | epoch 014:    174 / 3715 loss=6.303, nll_loss=3.37, mask_ins=1.075, word_ins_ml=4.868, word_reposition=0.36, ppl=78.94, wps=13594.6, ups=0.92, wpb=14710.2, bsz=1024, num_updates=48400, lr=0.000160706, gnorm=1.626, clip=1, loss_scale=78, train_wall=106, wall=52366
2022-08-01 05:29:38 | INFO | train_inner | epoch 014:    274 / 3715 loss=6.291, nll_loss=3.365, mask_ins=1.071, word_ins_ml=4.863, word_reposition=0.358, ppl=78.3, wps=13629.9, ups=0.93, wpb=14638.1, bsz=1024, num_updates=48500, lr=0.00016054, gnorm=1.343, clip=1, loss_scale=128, train_wall=106, wall=52474
2022-08-01 05:31:28 | INFO | train_inner | epoch 014:    374 / 3715 loss=6.295, nll_loss=3.376, mask_ins=1.072, word_ins_ml=4.873, word_reposition=0.35, ppl=78.5, wps=13403.4, ups=0.92, wpb=14637.9, bsz=1024, num_updates=48600, lr=0.000160375, gnorm=0.989, clip=0, loss_scale=128, train_wall=107, wall=52583
2022-08-01 05:33:20 | INFO | train_inner | epoch 014:    474 / 3715 loss=6.276, nll_loss=3.349, mask_ins=1.07, word_ins_ml=4.849, word_reposition=0.356, ppl=77.49, wps=13037.4, ups=0.89, wpb=14610, bsz=1024, num_updates=48700, lr=0.00016021, gnorm=1.067, clip=0, loss_scale=128, train_wall=110, wall=52695
2022-08-01 05:35:12 | INFO | train_inner | epoch 014:    574 / 3715 loss=6.284, nll_loss=3.363, mask_ins=1.071, word_ins_ml=4.861, word_reposition=0.353, ppl=77.93, wps=13037, ups=0.89, wpb=14621.9, bsz=1024, num_updates=48800, lr=0.000160046, gnorm=1.119, clip=0, loss_scale=128, train_wall=110, wall=52807
2022-08-01 05:37:02 | INFO | train_inner | epoch 014:    674 / 3715 loss=6.295, nll_loss=3.376, mask_ins=1.073, word_ins_ml=4.872, word_reposition=0.349, ppl=78.51, wps=13209.1, ups=0.91, wpb=14589.4, bsz=1024, num_updates=48900, lr=0.000159882, gnorm=1.073, clip=0, loss_scale=141, train_wall=109, wall=52918
2022-08-01 05:38:52 | INFO | train_inner | epoch 014:    774 / 3715 loss=6.278, nll_loss=3.364, mask_ins=1.064, word_ins_ml=4.862, word_reposition=0.352, ppl=77.58, wps=13280.4, ups=0.91, wpb=14605.5, bsz=1024, num_updates=49000, lr=0.000159719, gnorm=1.032, clip=0, loss_scale=256, train_wall=108, wall=53028
2022-08-01 05:40:42 | INFO | train_inner | epoch 014:    874 / 3715 loss=6.304, nll_loss=3.379, mask_ins=1.074, word_ins_ml=4.875, word_reposition=0.354, ppl=78.99, wps=13424.1, ups=0.91, wpb=14676.2, bsz=1024, num_updates=49100, lr=0.000159556, gnorm=1.097, clip=0, loss_scale=256, train_wall=108, wall=53137
2022-08-01 05:42:29 | INFO | train_inner | epoch 014:    974 / 3715 loss=6.303, nll_loss=3.376, mask_ins=1.072, word_ins_ml=4.872, word_reposition=0.359, ppl=78.96, wps=13738.4, ups=0.93, wpb=14730.1, bsz=1024, num_updates=49200, lr=0.000159394, gnorm=1.005, clip=0, loss_scale=256, train_wall=105, wall=53244
2022-08-01 05:44:16 | INFO | train_inner | epoch 014:   1074 / 3715 loss=6.29, nll_loss=3.365, mask_ins=1.068, word_ins_ml=4.863, word_reposition=0.36, ppl=78.26, wps=13715.7, ups=0.94, wpb=14656.6, bsz=1024, num_updates=49300, lr=0.000159232, gnorm=1.079, clip=0, loss_scale=256, train_wall=105, wall=53351
2022-08-01 05:45:53 | INFO | train_inner | epoch 014:   1174 / 3715 loss=6.299, nll_loss=3.372, mask_ins=1.078, word_ins_ml=4.869, word_reposition=0.352, ppl=78.72, wps=15074.5, ups=1.02, wpb=14724, bsz=1024, num_updates=49400, lr=0.000159071, gnorm=1.691, clip=1, loss_scale=256, train_wall=96, wall=53449
2022-08-01 05:47:39 | INFO | train_inner | epoch 014:   1274 / 3715 loss=6.282, nll_loss=3.361, mask_ins=1.075, word_ins_ml=4.859, word_reposition=0.347, ppl=77.8, wps=13877.8, ups=0.94, wpb=14721.9, bsz=1024, num_updates=49500, lr=0.00015891, gnorm=1.048, clip=0, loss_scale=507, train_wall=104, wall=53555
2022-08-01 05:49:27 | INFO | train_inner | epoch 014:   1374 / 3715 loss=6.303, nll_loss=3.376, mask_ins=1.077, word_ins_ml=4.872, word_reposition=0.355, ppl=78.97, wps=13590.9, ups=0.93, wpb=14575.2, bsz=1024, num_updates=49600, lr=0.00015875, gnorm=1.101, clip=0, loss_scale=512, train_wall=105, wall=53662
2022-08-01 05:51:14 | INFO | train_inner | epoch 014:   1474 / 3715 loss=6.321, nll_loss=3.39, mask_ins=1.073, word_ins_ml=4.884, word_reposition=0.364, ppl=79.95, wps=13808.8, ups=0.94, wpb=14758.8, bsz=1024, num_updates=49700, lr=0.00015859, gnorm=1.203, clip=0, loss_scale=512, train_wall=105, wall=53769
2022-08-01 05:53:01 | INFO | train_inner | epoch 014:   1574 / 3715 loss=6.301, nll_loss=3.377, mask_ins=1.076, word_ins_ml=4.873, word_reposition=0.352, ppl=78.86, wps=13578.1, ups=0.93, wpb=14633.8, bsz=1024, num_updates=49800, lr=0.000158431, gnorm=1.018, clip=0, loss_scale=512, train_wall=106, wall=53877
2022-08-01 05:54:49 | INFO | train_inner | epoch 014:   1674 / 3715 loss=6.306, nll_loss=3.382, mask_ins=1.069, word_ins_ml=4.878, word_reposition=0.359, ppl=79.11, wps=13552.1, ups=0.93, wpb=14615, bsz=1024, num_updates=49900, lr=0.000158272, gnorm=1.075, clip=0, loss_scale=512, train_wall=106, wall=53985
2022-08-01 05:56:37 | INFO | train_inner | epoch 014:   1774 / 3715 loss=6.297, nll_loss=3.37, mask_ins=1.072, word_ins_ml=4.866, word_reposition=0.359, ppl=78.62, wps=13730, ups=0.93, wpb=14765.9, bsz=1024, num_updates=50000, lr=0.000158114, gnorm=1.016, clip=0, loss_scale=952, train_wall=106, wall=54092
2022-08-01 05:56:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-08-01 05:58:26 | INFO | train_inner | epoch 014:   1875 / 3715 loss=6.279, nll_loss=3.369, mask_ins=1.063, word_ins_ml=4.866, word_reposition=0.35, ppl=77.63, wps=13475.4, ups=0.91, wpb=14744.3, bsz=1024, num_updates=50100, lr=0.000157956, gnorm=1.036, clip=0, loss_scale=563, train_wall=108, wall=54202
2022-08-01 06:00:16 | INFO | train_inner | epoch 014:   1975 / 3715 loss=6.314, nll_loss=3.389, mask_ins=1.073, word_ins_ml=4.884, word_reposition=0.358, ppl=79.56, wps=13321.3, ups=0.91, wpb=14643, bsz=1024, num_updates=50200, lr=0.000157799, gnorm=1.027, clip=0, loss_scale=512, train_wall=108, wall=54311
2022-08-01 06:02:04 | INFO | train_inner | epoch 014:   2075 / 3715 loss=6.304, nll_loss=3.386, mask_ins=1.069, word_ins_ml=4.881, word_reposition=0.354, ppl=79.02, wps=13548.3, ups=0.93, wpb=14611.7, bsz=1024, num_updates=50300, lr=0.000157642, gnorm=1.049, clip=0, loss_scale=512, train_wall=106, wall=54419
2022-08-01 06:03:51 | INFO | train_inner | epoch 014:   2175 / 3715 loss=6.315, nll_loss=3.386, mask_ins=1.077, word_ins_ml=4.881, word_reposition=0.357, ppl=79.61, wps=13813.2, ups=0.93, wpb=14785.9, bsz=1024, num_updates=50400, lr=0.000157485, gnorm=1.27, clip=0, loss_scale=512, train_wall=105, wall=54526
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
