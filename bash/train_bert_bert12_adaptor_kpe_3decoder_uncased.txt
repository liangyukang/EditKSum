nohup: ignoring input
2022-07-06 14:53:45 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:12205
2022-07-06 14:53:45 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:12205
2022-07-06 14:53:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-06 14:53:45 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:12205
2022-07-06 14:53:45 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:12205
2022-07-06 14:53:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-06 14:53:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-06 14:53:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-06 14:53:45 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-06 14:53:45 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-06 14:53:45 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-06 14:53:45 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-06 14:53:45 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-06 14:53:45 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-06 14:53:45 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-06 14:53:45 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-06 14:53:49 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:12205', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='6,6,6', layers_num='6,6,6', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='/data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_uncased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-uncased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, share_all_embeddings=True, no_share_discriminator=True, no_share_maskpredictor=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-uncased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-uncased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-uncased')
2022-07-06 14:53:49 | INFO | fairseq.tasks.translation | [source] dictionary: 30522 types
2022-07-06 14:53:49 | INFO | fairseq.tasks.translation | [target] dictionary: 30522 types
2022-07-06 14:53:49 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510/valid.source-target.source
start load cached examples valid ...
0it [00:00, ?it/s]2022-07-06 14:53:49 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510/valid.source-target.target
2022-07-06 14:53:49 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]378it [00:00, 3779.02it/s]397it [00:00, 3961.07it/s]401it [00:00, 4003.56it/s]398it [00:00, 3976.69it/s]756it [00:00, 3504.68it/s]802it [00:00, 3568.41it/s]796it [00:00, 3548.63it/s]794it [00:00, 3423.25it/s]1126it [00:00, 3586.55it/s]1174it [00:00, 3630.09it/s]1156it [00:00, 3567.90it/s]1166it [00:00, 3544.22it/s]1486it [00:00, 3393.78it/s]1540it [00:00, 3506.34it/s]1515it [00:00, 3455.31it/s]1524it [00:00, 3417.68it/s]1883it [00:00, 3590.57it/s]1940it [00:00, 3673.13it/s]1915it [00:00, 3639.69it/s]1917it [00:00, 3590.51it/s]2245it [00:00, 3507.31it/s]2310it [00:00, 3588.04it/s]2281it [00:00, 3531.53it/s]2279it [00:00, 3474.19it/s]2650it [00:00, 3675.22it/s]2716it [00:00, 3733.64it/s]2688it [00:00, 3698.96it/s]2681it [00:00, 3641.43it/s]3030it [00:00, 3713.66it/s]3091it [00:00, 3660.47it/s]3071it [00:00, 3719.74it/s]3074it [00:00, 3601.56it/s]3403it [00:00, 3599.82it/s]3489it [00:00, 3756.41it/s]3475it [00:00, 3720.73it/s]3445it [00:00, 3592.04it/s]3789it [00:01, 3675.86it/s]3889it [00:01, 3827.32it/s]3873it [00:01, 3795.20it/s]3828it [00:01, 3661.14it/s]4158it [00:01, 3577.69it/s]4273it [00:01, 3698.78it/s]4254it [00:01, 3650.42it/s]4196it [00:01, 3556.43it/s]4524it [00:01, 3600.94it/s]4645it [00:01, 3704.07it/s]4624it [00:01, 3663.69it/s]4566it [00:01, 3596.74it/s]4886it [00:01, 3508.07it/s]5017it [00:01, 3592.61it/s]4992it [00:01, 3531.30it/s]4927it [00:01, 3459.16it/s]5266it [00:01, 3592.68it/s]5402it [00:01, 3665.18it/s]5380it [00:01, 3630.97it/s]5310it [00:01, 3563.98it/s]5627it [00:01, 3425.31it/s]5770it [00:01, 3513.01it/s]5745it [00:01, 3450.19it/s]5669it [00:01, 3402.52it/s]5986it [00:01, 3470.17it/s]6132it [00:01, 3540.79it/s]6107it [00:01, 3497.80it/s]6026it [00:01, 3449.30it/s]6343it [00:01, 3497.71it/s]6373it [00:01, 3433.92it/s]6459it [00:01, 3364.24it/s]6488it [00:02, 3372.43it/s]6828it [00:02, 2175.00it/s]6695it [00:02, 2053.04it/s]6798it [00:02, 1967.95it/s]6718it [00:02, 1885.38it/s]7185it [00:02, 2461.20it/s]7049it [00:02, 2345.17it/s]7139it [00:02, 2243.15it/s]7060it [00:02, 2168.86it/s]7483it [00:02, 2487.81it/s]7349it [00:02, 2402.08it/s]7437it [00:02, 2400.46it/s]7351it [00:02, 2306.70it/s]7840it [00:02, 2744.80it/s]7704it [00:02, 2667.91it/s]7784it [00:02, 2650.23it/s]7709it [00:02, 2596.99it/s]8168it [00:02, 2816.68it/s]8058it [00:02, 2883.90it/s]8141it [00:02, 2880.74it/s]8054it [00:02, 2806.23it/s]8518it [00:02, 2986.02it/s]8380it [00:02, 2911.41it/s]8464it [00:02, 2916.05it/s]8374it [00:02, 2855.44it/s]8875it [00:02, 3143.86it/s]8719it [00:02, 3039.62it/s]8814it [00:02, 3068.44it/s]8728it [00:02, 3036.43it/s]9204it [00:02, 3111.97it/s]9041it [00:02, 3023.03it/s]9140it [00:02, 3062.62it/s]9054it [00:02, 3022.86it/s]9550it [00:02, 3207.41it/s]9395it [00:02, 3165.37it/s]9494it [00:02, 3184.55it/s]9411it [00:03, 3172.86it/s]9879it [00:03, 3151.80it/s]9735it [00:03, 3230.92it/s]9854it [00:03, 3301.77it/s]9760it [00:03, 3262.88it/s]10225it [00:03, 3237.60it/s]10066it [00:03, 3160.32it/s]10192it [00:03, 3221.62it/s]10095it [00:03, 3174.74it/s]10575it [00:03, 3312.81it/s]10420it [00:03, 3267.10it/s]10520it [00:03, 3235.31it/s]10437it [00:03, 3242.68it/s]10910it [00:03, 3151.82it/s]10751it [00:03, 3173.88it/s]10848it [00:03, 3171.31it/s]10767it [00:03, 3145.20it/s]11251it [00:03, 3224.48it/s]11088it [00:03, 3227.67it/s]11206it [00:03, 3287.22it/s]11121it [00:03, 3256.90it/s]11577it [00:03, 3143.41it/s]11441it [00:03, 3314.61it/s]11544it [00:03, 3190.12it/s]11475it [00:03, 3338.45it/s]11921it [00:03, 3226.66it/s]11775it [00:03, 3175.73it/s]11904it [00:03, 3304.89it/s]11812it [00:03, 3212.02it/s]12279it [00:03, 3328.31it/s]12129it [00:03, 3279.41it/s]12266it [00:03, 3393.79it/s]12169it [00:03, 3313.74it/s]12614it [00:03, 3202.17it/s]12460it [00:03, 3185.48it/s]12608it [00:03, 3267.59it/s]12503it [00:03, 3209.23it/s]12967it [00:03, 3295.93it/s]12813it [00:04, 3283.96it/s]12965it [00:04, 3352.74it/s]12850it [00:04, 3282.56it/s]13299it [00:04, 3206.95it/s]13164it [00:04, 3348.13it/s]13368it [00:04, 3245.11it/s]
2022-07-06 14:53:53 | INFO | root | success load 13368 data
2022-07-06 14:53:53 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-uncased' is a path or url to a directory containing tokenizer files.
2022-07-06 14:53:53 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-uncased/added_tokens.json. We won't load it.
2022-07-06 14:53:53 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-uncased/special_tokens_map.json. We won't load it.
2022-07-06 14:53:53 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-uncased/vocab.txt
2022-07-06 14:53:53 | INFO | transformer.tokenization_utils | loading file None
2022-07-06 14:53:53 | INFO | transformer.tokenization_utils | loading file None
2022-07-06 14:53:53 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-uncased/tokenizer_config.json
13303it [00:04, 3254.21it/s]13204it [00:04, 3356.77it/s]13368it [00:04, 3213.72it/s]
13368it [00:04, 3193.10it/s]
13368it [00:04, 3162.61it/s]
2022-07-06 14:53:54 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-uncased/config.json
2022-07-06 14:53:54 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-07-06 14:53:54 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-uncased/pytorch_model.bin
2022-07-06 14:53:58 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-06 14:53:58 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-06 14:53:58 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-uncased/config.json
2022-07-06 14:53:58 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-07-06 14:53:58 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-uncased-decoder/pytorch_model.bin
2022-07-06 14:53:59 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_msk.encoder_attn_fc1.weight', 'layers.0.adapter_msk.encoder_attn_fc2.weight', 'layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_msk.encoder_attn_fc1.weight', 'layers.1.adapter_msk.encoder_attn_fc2.weight', 'layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_msk.encoder_attn_fc1.weight', 'layers.2.adapter_msk.encoder_attn_fc2.weight', 'layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_msk.encoder_attn_fc1.weight', 'layers.3.adapter_msk.encoder_attn_fc2.weight', 'layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_msk.encoder_attn_fc1.weight', 'layers.4.adapter_msk.encoder_attn_fc2.weight', 'layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_msk.encoder_attn_fc1.weight', 'layers.5.adapter_msk.encoder_attn_fc2.weight', 'layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-06 14:53:59 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'layers.6.attention.self.query.weight', 'layers.6.attention.self.query.bias', 'layers.6.attention.self.key.weight', 'layers.6.attention.self.key.bias', 'layers.6.attention.self.value.weight', 'layers.6.attention.self.value.bias', 'layers.6.attention.output.dense.weight', 'layers.6.attention.output.dense.bias', 'layers.6.intermediate.dense.weight', 'layers.6.intermediate.dense.bias', 'layers.6.output.dense.weight', 'layers.6.output.dense.bias', 'layers.7.attention.self.query.weight', 'layers.7.attention.self.query.bias', 'layers.7.attention.self.key.weight', 'layers.7.attention.self.key.bias', 'layers.7.attention.self.value.weight', 'layers.7.attention.self.value.bias', 'layers.7.attention.output.dense.weight', 'layers.7.attention.output.dense.bias', 'layers.7.intermediate.dense.weight', 'layers.7.intermediate.dense.bias', 'layers.7.output.dense.weight', 'layers.7.output.dense.bias', 'layers.8.attention.self.query.weight', 'layers.8.attention.self.query.bias', 'layers.8.attention.self.key.weight', 'layers.8.attention.self.key.bias', 'layers.8.attention.self.value.weight', 'layers.8.attention.self.value.bias', 'layers.8.attention.output.dense.weight', 'layers.8.attention.output.dense.bias', 'layers.8.intermediate.dense.weight', 'layers.8.intermediate.dense.bias', 'layers.8.output.dense.weight', 'layers.8.output.dense.bias', 'layers.9.attention.self.query.weight', 'layers.9.attention.self.query.bias', 'layers.9.attention.self.key.weight', 'layers.9.attention.self.key.bias', 'layers.9.attention.self.value.weight', 'layers.9.attention.self.value.bias', 'layers.9.attention.output.dense.weight', 'layers.9.attention.output.dense.bias', 'layers.9.intermediate.dense.weight', 'layers.9.intermediate.dense.bias', 'layers.9.output.dense.weight', 'layers.9.output.dense.bias', 'layers.10.attention.self.query.weight', 'layers.10.attention.self.query.bias', 'layers.10.attention.self.key.weight', 'layers.10.attention.self.key.bias', 'layers.10.attention.self.value.weight', 'layers.10.attention.self.value.bias', 'layers.10.attention.output.dense.weight', 'layers.10.attention.output.dense.bias', 'layers.10.intermediate.dense.weight', 'layers.10.intermediate.dense.bias', 'layers.10.output.dense.weight', 'layers.10.output.dense.bias', 'layers.11.attention.self.query.weight', 'layers.11.attention.self.query.bias', 'layers.11.attention.self.key.weight', 'layers.11.attention.self.key.bias', 'layers.11.attention.self.value.weight', 'layers.11.attention.self.value.bias', 'layers.11.attention.output.dense.weight', 'layers.11.attention.output.dense.bias', 'layers.11.intermediate.dense.weight', 'layers.11.intermediate.dense.bias', 'layers.11.output.dense.weight', 'layers.11.output.dense.bias', 'layers.6.attention.output.LayerNorm.weight', 'layers.6.attention.output.LayerNorm.bias', 'layers.6.output.LayerNorm.weight', 'layers.6.output.LayerNorm.bias', 'layers.7.attention.output.LayerNorm.weight', 'layers.7.attention.output.LayerNorm.bias', 'layers.7.output.LayerNorm.weight', 'layers.7.output.LayerNorm.bias', 'layers.8.attention.output.LayerNorm.weight', 'layers.8.attention.output.LayerNorm.bias', 'layers.8.output.LayerNorm.weight', 'layers.8.output.LayerNorm.bias', 'layers.9.attention.output.LayerNorm.weight', 'layers.9.attention.output.LayerNorm.bias', 'layers.9.output.LayerNorm.weight', 'layers.9.output.LayerNorm.bias', 'layers.10.attention.output.LayerNorm.weight', 'layers.10.attention.output.LayerNorm.bias', 'layers.10.output.LayerNorm.weight', 'layers.10.output.LayerNorm.bias', 'layers.11.attention.output.LayerNorm.weight', 'layers.11.attention.output.LayerNorm.bias', 'layers.11.output.LayerNorm.weight', 'layers.11.output.LayerNorm.bias']
Trained parameters: len 488
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 308
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.output_projection.weight']Trained parameters: len 488
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 308
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.output_projection.weight']
2022-07-06 14:53:59 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=30522, bias=False)
  )
)
2022-07-06 14:53:59 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-06 14:53:59 | INFO | fairseq_cli.train | num. model params: 308677379 (num. trained: 308677379)
2022-07-06 14:53:59 | INFO | fairseq_cli.train | num. Encoder model params: 147644675 (Encoder num. trained: 147644675)
2022-07-06 14:53:59 | INFO | fairseq_cli.train | num. Decoder model params: 161032704 (Decoder num. trained: 161032704)
Trained parameters: len 488
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 308
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.output_projection.weight']2022-07-06 14:53:59 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-06 14:53:59 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-06 14:53:59 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt
2022-07-06 14:53:59 | INFO | fairseq.trainer | loading train data for epoch 1
2022-07-06 14:53:59 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510/train.source-target.source
2022-07-06 14:53:59 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510/train.source-target.target
2022-07-06 14:53:59 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]361it [00:00, 3605.34it/s]
start load cached examples train ...
0it [00:00, ?it/s]722it [00:00, 3385.84it/s]Trained parameters: len 488
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.output_projection.weight']
Trained parameters not adapter: len 308
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.embeddings.word_embeddings.weight', 'decoder.embeddings.position_embeddings.weight', 'decoder.embeddings.token_type_embeddings.weight', 'decoder.embeddings.LayerNorm.weight', 'decoder.embeddings.LayerNorm.bias', 'decoder.layers.0.attention.self.query.weight', 'decoder.layers.0.attention.self.query.bias', 'decoder.layers.0.attention.self.key.weight', 'decoder.layers.0.attention.self.key.bias', 'decoder.layers.0.attention.self.value.weight', 'decoder.layers.0.attention.self.value.bias', 'decoder.layers.0.attention.output.dense.weight', 'decoder.layers.0.attention.output.dense.bias', 'decoder.layers.0.attention.output.LayerNorm.weight', 'decoder.layers.0.attention.output.LayerNorm.bias', 'decoder.layers.0.intermediate.dense.weight', 'decoder.layers.0.intermediate.dense.bias', 'decoder.layers.0.output.dense.weight', 'decoder.layers.0.output.dense.bias', 'decoder.layers.0.output.LayerNorm.weight', 'decoder.layers.0.output.LayerNorm.bias', 'decoder.layers.1.attention.self.query.weight', 'decoder.layers.1.attention.self.query.bias', 'decoder.layers.1.attention.self.key.weight', 'decoder.layers.1.attention.self.key.bias', 'decoder.layers.1.attention.self.value.weight', 'decoder.layers.1.attention.self.value.bias', 'decoder.layers.1.attention.output.dense.weight', 'decoder.layers.1.attention.output.dense.bias', 'decoder.layers.1.attention.output.LayerNorm.weight', 'decoder.layers.1.attention.output.LayerNorm.bias', 'decoder.layers.1.intermediate.dense.weight', 'decoder.layers.1.intermediate.dense.bias', 'decoder.layers.1.output.dense.weight', 'decoder.layers.1.output.dense.bias', 'decoder.layers.1.output.LayerNorm.weight', 'decoder.layers.1.output.LayerNorm.bias', 'decoder.layers.2.attention.self.query.weight', 'decoder.layers.2.attention.self.query.bias', 'decoder.layers.2.attention.self.key.weight', 'decoder.layers.2.attention.self.key.bias', 'decoder.layers.2.attention.self.value.weight', 'decoder.layers.2.attention.self.value.bias', 'decoder.layers.2.attention.output.dense.weight', 'decoder.layers.2.attention.output.dense.bias', 'decoder.layers.2.attention.output.LayerNorm.weight', 'decoder.layers.2.attention.output.LayerNorm.bias', 'decoder.layers.2.intermediate.dense.weight', 'decoder.layers.2.intermediate.dense.bias', 'decoder.layers.2.output.dense.weight', 'decoder.layers.2.output.dense.bias', 'decoder.layers.2.output.LayerNorm.weight', 'decoder.layers.2.output.LayerNorm.bias', 'decoder.layers.3.attention.self.query.weight', 'decoder.layers.3.attention.self.query.bias', 'decoder.layers.3.attention.self.key.weight', 'decoder.layers.3.attention.self.key.bias', 'decoder.layers.3.attention.self.value.weight', 'decoder.layers.3.attention.self.value.bias', 'decoder.layers.3.attention.output.dense.weight', 'decoder.layers.3.attention.output.dense.bias', 'decoder.layers.3.attention.output.LayerNorm.weight', 'decoder.layers.3.attention.output.LayerNorm.bias', 'decoder.layers.3.intermediate.dense.weight', 'decoder.layers.3.intermediate.dense.bias', 'decoder.layers.3.output.dense.weight', 'decoder.layers.3.output.dense.bias', 'decoder.layers.3.output.LayerNorm.weight', 'decoder.layers.3.output.LayerNorm.bias', 'decoder.layers.4.attention.self.query.weight', 'decoder.layers.4.attention.self.query.bias', 'decoder.layers.4.attention.self.key.weight', 'decoder.layers.4.attention.self.key.bias', 'decoder.layers.4.attention.self.value.weight', 'decoder.layers.4.attention.self.value.bias', 'decoder.layers.4.attention.output.dense.weight', 'decoder.layers.4.attention.output.dense.bias', 'decoder.layers.4.attention.output.LayerNorm.weight', 'decoder.layers.4.attention.output.LayerNorm.bias', 'decoder.layers.4.intermediate.dense.weight', 'decoder.layers.4.intermediate.dense.bias', 'decoder.layers.4.output.dense.weight', 'decoder.layers.4.output.dense.bias', 'decoder.layers.4.output.LayerNorm.weight', 'decoder.layers.4.output.LayerNorm.bias', 'decoder.layers.5.attention.self.query.weight', 'decoder.layers.5.attention.self.query.bias', 'decoder.layers.5.attention.self.key.weight', 'decoder.layers.5.attention.self.key.bias', 'decoder.layers.5.attention.self.value.weight', 'decoder.layers.5.attention.self.value.bias', 'decoder.layers.5.attention.output.dense.weight', 'decoder.layers.5.attention.output.dense.bias', 'decoder.layers.5.attention.output.LayerNorm.weight', 'decoder.layers.5.attention.output.LayerNorm.bias', 'decoder.layers.5.intermediate.dense.weight', 'decoder.layers.5.intermediate.dense.bias', 'decoder.layers.5.output.dense.weight', 'decoder.layers.5.output.dense.bias', 'decoder.layers.5.output.LayerNorm.weight', 'decoder.layers.5.output.LayerNorm.bias', 'decoder.output_projection.weight']
start load cached examples train ...
0it [00:00, ?it/s]362it [00:00, 3616.58it/s]1062it [00:00, 3356.20it/s]325it [00:00, 3249.10it/s]724it [00:00, 3124.61it/s]1399it [00:00, 3269.34it/s]651it [00:00, 3253.19it/s]1071it [00:00, 3266.38it/s]1727it [00:00, 2882.16it/s]977it [00:00, 2892.38it/s]1436it [00:00, 3409.04it/s]2079it [00:00, 3075.86it/s]1326it [00:00, 3111.49it/s]1780it [00:00, 3316.57it/s]1643it [00:00, 3130.20it/s]2393it [00:00, 3050.90it/s]2159it [00:00, 3468.96it/s]
start load cached examples train ...
0it [00:00, ?it/s]2023it [00:00, 3349.06it/s]2767it [00:00, 3255.80it/s]2508it [00:00, 3388.47it/s]382it [00:00, 3813.32it/s]3148it [00:00, 3421.55it/s]2364it [00:00, 3316.13it/s]2888it [00:00, 3513.25it/s]764it [00:00, 3521.56it/s]2745it [00:00, 3468.73it/s]3494it [00:01, 3324.68it/s]3241it [00:00, 3420.49it/s]1148it [00:00, 3658.58it/s]3129it [00:00, 3580.43it/s]3867it [00:01, 3443.08it/s]3597it [00:01, 3461.15it/s]1524it [00:00, 3484.67it/s]3489it [00:01, 3465.46it/s]4214it [00:01, 3344.04it/s]3968it [00:01, 3534.55it/s]1898it [00:00, 3571.04it/s]3863it [00:01, 3546.00it/s]4591it [00:01, 3467.14it/s]4323it [00:01, 3426.44it/s]2287it [00:00, 3674.82it/s]4219it [00:01, 3450.54it/s]4940it [00:01, 3381.59it/s]4697it [00:01, 3516.79it/s]2657it [00:00, 3500.51it/s]4594it [00:01, 3535.47it/s]5293it [00:01, 3422.50it/s]5050it [00:01, 3394.94it/s]3032it [00:00, 3574.53it/s]5658it [00:01, 3487.03it/s]4949it [00:01, 3406.53it/s]5425it [00:01, 3497.34it/s]3392it [00:00, 3432.23it/s]5315it [00:01, 3477.91it/s]6008it [00:01, 3400.18it/s]5777it [00:01, 3354.30it/s]3769it [00:01, 3530.21it/s]5690it [00:01, 3556.31it/s]6377it [00:01, 3482.92it/s]6155it [00:01, 3473.54it/s]4125it [00:01, 3424.58it/s]6047it [00:01, 3421.19it/s]6727it [00:02, 3388.15it/s]6533it [00:01, 3562.10it/s]4507it [00:01, 3538.64it/s]6414it [00:01, 3491.19it/s]7103it [00:02, 3493.79it/s]6891it [00:02, 3441.68it/s]4884it [00:01, 3431.62it/s]7454it [00:02, 3377.14it/s]6765it [00:01, 3371.36it/s]7270it [00:02, 3540.72it/s]5262it [00:01, 3530.21it/s]7127it [00:02, 3439.77it/s]5631it [00:01, 3573.78it/s]5990it [00:01, 3455.54it/s]6367it [00:01, 3544.63it/s]6724it [00:01, 3265.76it/s]7107it [00:02, 3420.70it/s]7626it [00:02, 1263.97it/s]7454it [00:02, 3354.89it/s]7794it [00:02, 1179.56it/s]7985it [00:02, 1563.86it/s]8171it [00:03, 1502.12it/s]8313it [00:03, 1822.44it/s]8488it [00:03, 1754.30it/s]7473it [00:02, 3330.67it/s]8693it [00:03, 2180.68it/s]8861it [00:03, 2105.29it/s]7808it [00:03, 1033.82it/s]9074it [00:03, 2514.35it/s]9181it [00:03, 2312.20it/s]8179it [00:03, 1334.34it/s]9415it [00:03, 2681.69it/s]9561it [00:03, 2643.09it/s]8485it [00:03, 1572.74it/s]9797it [00:03, 2957.35it/s]9938it [00:03, 2914.88it/s]8850it [00:03, 1915.21it/s]10147it [00:03, 2990.45it/s]10286it [00:03, 2969.60it/s]9163it [00:03, 2085.41it/s]10534it [00:03, 3219.19it/s]10652it [00:03, 3150.08it/s]9539it [00:03, 2434.80it/s]7793it [00:03, 1044.06it/s]10886it [00:03, 3220.12it/s]10998it [00:03, 3168.59it/s]9923it [00:03, 2752.95it/s]8167it [00:03, 1344.54it/s]11275it [00:03, 3402.78it/s]11367it [00:04, 3312.37it/s]10264it [00:03, 2850.43it/s]8483it [00:03, 1593.68it/s]11657it [00:03, 3520.14it/s]11715it [00:04, 3271.55it/s]10630it [00:03, 3057.14it/s]8864it [00:03, 1956.64it/s]12022it [00:04, 3451.27it/s]12096it [00:04, 3421.82it/s]10972it [00:04, 3097.73it/s]9185it [00:03, 2176.11it/s]12393it [00:04, 3524.61it/s]12471it [00:04, 3515.41it/s]11353it [00:04, 3291.60it/s]9570it [00:03, 2530.72it/s]12752it [00:04, 3448.37it/s]12830it [00:04, 3449.14it/s]9952it [00:03, 2828.57it/s]11702it [00:04, 3243.51it/s]13151it [00:04, 3601.18it/s]13221it [00:04, 3581.10it/s]12074it [00:04, 3375.25it/s]10303it [00:03, 2932.35it/s]13516it [00:04, 3514.30it/s]13583it [00:04, 3462.66it/s]12460it [00:04, 3511.77it/s]10678it [00:03, 3141.43it/s]13914it [00:04, 3640.19it/s]13964it [00:04, 3561.71it/s]12820it [00:04, 3371.28it/s]11030it [00:03, 3046.72it/s]14281it [00:04, 3511.19it/s]14323it [00:04, 3440.79it/s]13211it [00:04, 3522.32it/s]11419it [00:04, 3270.40it/s]14675it [00:04, 3631.87it/s]14710it [00:04, 3560.67it/s]13569it [00:04, 3440.80it/s]11768it [00:04, 3217.34it/s]15041it [00:04, 3539.66it/s]15069it [00:05, 3504.87it/s]13966it [00:04, 3589.97it/s]12157it [00:04, 3400.40it/s]15424it [00:05, 3622.37it/s]15447it [00:05, 3583.80it/s]14329it [00:04, 3504.89it/s]12513it [00:04, 3380.77it/s]15804it [00:05, 3671.74it/s]15825it [00:05, 3640.76it/s]14707it [00:05, 3581.21it/s]12904it [00:04, 3528.74it/s]16173it [00:05, 3514.92it/s]16191it [00:05, 3496.86it/s]15068it [00:05, 3526.86it/s]13303it [00:04, 3660.19it/s]16548it [00:05, 3580.20it/s]16584it [00:05, 3621.04it/s]15443it [00:05, 3591.38it/s]13675it [00:04, 3546.52it/s]15818it [00:05, 3636.97it/s]14074it [00:04, 3672.01it/s]16183it [00:05, 3503.95it/s]14445it [00:04, 3579.23it/s]16558it [00:05, 3574.13it/s]14808it [00:05, 3592.07it/s]15170it [00:05, 3519.93it/s]15556it [00:05, 3617.95it/s]15920it [00:05, 3485.87it/s]16300it [00:05, 3575.59it/s]16704it [00:05, 3710.35it/s]16908it [00:06, 1037.10it/s]16948it [00:06, 969.55it/s] 17297it [00:06, 1342.21it/s]17324it [00:06, 1248.45it/s]17623it [00:06, 1589.42it/s]17646it [00:06, 1494.84it/s]18012it [00:06, 1953.92it/s]18037it [00:06, 1860.25it/s]18403it [00:06, 2314.93it/s]18391it [00:06, 2157.68it/s]18752it [00:06, 2522.13it/s]16917it [00:06, 892.81it/s] 18729it [00:07, 2389.47it/s]19138it [00:06, 2825.22it/s]17293it [00:06, 1162.82it/s]19110it [00:07, 2705.44it/s]19493it [00:06, 2927.64it/s]17623it [00:06, 1411.49it/s]19459it [00:07, 2845.51it/s]19875it [00:07, 3152.41it/s]18013it [00:07, 1770.67it/s]19826it [00:07, 3053.11it/s]20231it [00:07, 3187.02it/s]18393it [00:07, 2117.64it/s]17077it [00:06, 1017.92it/s]20175it [00:07, 3113.10it/s]20613it [00:07, 3356.51it/s]18738it [00:07, 2342.48it/s]17471it [00:06, 1318.43it/s]20555it [00:07, 3297.52it/s]20981it [00:07, 3445.27it/s]19121it [00:07, 2663.66it/s]17796it [00:06, 1565.85it/s]20925it [00:07, 3407.64it/s]21342it [00:07, 3381.81it/s]18188it [00:06, 1930.09it/s]19472it [00:07, 2806.76it/s]21284it [00:07, 3362.98it/s]21729it [00:07, 3518.17it/s]18525it [00:06, 2171.26it/s]19843it [00:07, 3031.06it/s]21672it [00:07, 3509.33it/s]22090it [00:07, 3433.12it/s]18910it [00:07, 2513.42it/s]20194it [00:07, 3080.42it/s]22033it [00:07, 3414.32it/s]22473it [00:07, 3544.15it/s]19288it [00:07, 2797.49it/s]20562it [00:07, 3238.20it/s]22399it [00:08, 3482.38it/s]22833it [00:07, 3473.44it/s]20927it [00:07, 3350.04it/s]19645it [00:07, 2835.69it/s]22753it [00:08, 3437.43it/s]23218it [00:08, 3580.45it/s]21281it [00:07, 3297.96it/s]20028it [00:07, 3080.97it/s]23134it [00:08, 3544.09it/s]23580it [00:08, 3461.02it/s]21669it [00:08, 3459.45it/s]20379it [00:07, 3120.63it/s]23496it [00:08, 3565.71it/s]23960it [00:08, 3556.19it/s]20765it [00:07, 3316.35it/s]22026it [00:08, 3374.93it/s]23855it [00:08, 3460.26it/s]22398it [00:08, 3470.84it/s]24344it [00:08, 3451.99it/s]21121it [00:07, 3249.59it/s]24238it [00:08, 3564.61it/s]24723it [00:08, 3544.83it/s]22751it [00:08, 3432.78it/s]21504it [00:07, 3408.37it/s]24597it [00:08, 3439.49it/s]25103it [00:08, 3617.29it/s]23118it [00:08, 3500.04it/s]21858it [00:07, 3365.88it/s]24969it [00:08, 3519.01it/s]23493it [00:08, 3571.54it/s]25467it [00:08, 3482.86it/s]22227it [00:07, 3455.29it/s]25323it [00:08, 3404.32it/s]23853it [00:08, 3464.11it/s]25852it [00:08, 3586.60it/s]22613it [00:08, 3569.11it/s]25683it [00:08, 3460.03it/s]24215it [00:08, 3507.95it/s]26213it [00:08, 3416.15it/s]22976it [00:08, 3332.06it/s]26031it [00:09, 3333.54it/s]24568it [00:08, 3407.40it/s]26595it [00:09, 3529.79it/s]23341it [00:08, 3418.75it/s]26408it [00:09, 3456.87it/s]24946it [00:09, 3512.34it/s]26951it [00:09, 3437.54it/s]23689it [00:08, 3350.13it/s]26779it [00:09, 3528.87it/s]25299it [00:09, 3402.03it/s]27350it [00:09, 3594.18it/s]24070it [00:08, 3478.81it/s]27134it [00:09, 3423.48it/s]25679it [00:09, 3514.33it/s]27712it [00:09, 3491.45it/s]24422it [00:08, 3402.95it/s]27513it [00:09, 3527.82it/s]26033it [00:09, 3396.25it/s]28102it [00:09, 3605.74it/s]24787it [00:08, 3472.64it/s]27868it [00:09, 3444.71it/s]26375it [00:09, 3352.83it/s]28466it [00:09, 3613.30it/s]25161it [00:08, 3550.13it/s]28238it [00:09, 3505.90it/s]26749it [00:09, 3462.73it/s]25518it [00:08, 3425.68it/s]28590it [00:09, 3397.58it/s]27097it [00:09, 3404.25it/s]25883it [00:09, 3488.50it/s]27481it [00:09, 3528.05it/s]26234it [00:09, 3370.09it/s]27835it [00:09, 3322.30it/s]26592it [00:09, 3429.54it/s]28210it [00:09, 3442.03it/s]26937it [00:09, 3295.30it/s]28557it [00:10, 3306.66it/s]27325it [00:09, 3460.50it/s]27702it [00:09, 3548.70it/s]28059it [00:09, 3430.23it/s]28416it [00:09, 3469.74it/s]28829it [00:10, 775.45it/s] 28932it [00:11, 807.50it/s] 29209it [00:10, 1024.15it/s]29288it [00:11, 1051.41it/s]29521it [00:11, 1244.28it/s]29594it [00:11, 1277.18it/s]29903it [00:11, 1580.08it/s]29946it [00:11, 1587.03it/s]30285it [00:11, 1931.95it/s]30291it [00:11, 1866.01it/s]30628it [00:11, 2156.36it/s]30664it [00:11, 2217.18it/s]31002it [00:11, 2478.64it/s]31035it [00:11, 2533.60it/s]31346it [00:11, 2648.01it/s]31377it [00:11, 2669.10it/s]31731it [00:11, 2936.28it/s]28891it [00:11, 653.59it/s] 31753it [00:11, 2933.60it/s]32082it [00:11, 3022.52it/s]29263it [00:11, 879.20it/s]32098it [00:11, 2997.56it/s]32463it [00:11, 3226.36it/s]29561it [00:11, 1079.02it/s]32469it [00:12, 3185.44it/s]28765it [00:11, 692.71it/s] 29932it [00:11, 1393.74it/s]32818it [00:12, 3188.83it/s]29120it [00:11, 911.08it/s]32816it [00:12, 3055.56it/s]30277it [00:12, 1693.40it/s]33193it [00:12, 3340.92it/s]29453it [00:11, 1136.70it/s]33187it [00:12, 3230.13it/s]30598it [00:12, 1948.66it/s]33576it [00:12, 3476.24it/s]29827it [00:11, 1454.56it/s]33561it [00:12, 3371.35it/s]30973it [00:12, 2303.03it/s]33937it [00:12, 3416.87it/s]30202it [00:11, 1794.93it/s]33911it [00:12, 3311.95it/s]31309it [00:12, 2491.13it/s]34325it [00:12, 3547.77it/s]30534it [00:11, 2038.19it/s]34291it [00:12, 3447.33it/s]31677it [00:12, 2770.72it/s]34687it [00:12, 3454.20it/s]30894it [00:11, 2346.89it/s]34643it [00:12, 3370.37it/s]32018it [00:12, 2888.77it/s]35061it [00:12, 3535.19it/s]31230it [00:12, 2527.91it/s]35002it [00:12, 3432.65it/s]32390it [00:12, 3105.75it/s]35419it [00:12, 3438.38it/s]31607it [00:12, 2821.66it/s]35350it [00:12, 3349.46it/s]32756it [00:12, 3254.59it/s]35798it [00:12, 3537.82it/s]31971it [00:12, 3027.71it/s]35727it [00:13, 3469.78it/s]33108it [00:12, 3214.02it/s]36165it [00:12, 3574.17it/s]32321it [00:12, 3075.62it/s]36101it [00:13, 3538.90it/s]33481it [00:12, 3356.35it/s]36525it [00:13, 3434.69it/s]32673it [00:12, 3193.56it/s]36457it [00:13, 3270.85it/s]33831it [00:13, 3189.08it/s]36902it [00:13, 3529.30it/s]33017it [00:12, 3161.08it/s]36831it [00:13, 3400.35it/s]34216it [00:13, 3369.29it/s]37258it [00:13, 3420.45it/s]33367it [00:12, 3254.97it/s]37176it [00:13, 3318.94it/s]34563it [00:13, 3283.86it/s]37629it [00:13, 3502.67it/s]33706it [00:12, 3221.00it/s]37531it [00:13, 3383.65it/s]34932it [00:13, 3397.09it/s]37982it [00:13, 3417.29it/s]34079it [00:12, 3365.45it/s]37873it [00:13, 3302.64it/s]35294it [00:13, 3458.94it/s]38359it [00:13, 3517.12it/s]34455it [00:12, 3477.81it/s]38251it [00:13, 3438.55it/s]35645it [00:13, 3376.78it/s]38713it [00:13, 3388.87it/s]34809it [00:13, 3331.10it/s]38597it [00:13, 3399.04it/s]36011it [00:13, 3457.31it/s]39091it [00:13, 3500.10it/s]35183it [00:13, 3447.16it/s]38939it [00:14, 3342.27it/s]36360it [00:13, 3370.91it/s]39464it [00:13, 3566.11it/s]35532it [00:13, 3347.59it/s]39320it [00:14, 3475.65it/s]36712it [00:13, 3413.21it/s]39823it [00:14, 3457.97it/s]35903it [00:13, 3448.62it/s]39669it [00:14, 3369.96it/s]37055it [00:14, 3331.74it/s]40207it [00:14, 3566.21it/s]36251it [00:13, 3321.61it/s]40022it [00:14, 3405.93it/s]37415it [00:14, 3407.70it/s]40566it [00:14, 3367.63it/s]36601it [00:13, 3369.70it/s]40372it [00:14, 3309.63it/s]37760it [00:14, 3417.37it/s]40945it [00:14, 3486.47it/s]36974it [00:13, 3472.69it/s]40742it [00:14, 3418.84it/s]38103it [00:14, 3356.79it/s]41297it [00:14, 3409.45it/s]37324it [00:13, 3326.14it/s]41106it [00:14, 3482.26it/s]38469it [00:14, 3444.78it/s]41679it [00:14, 3524.26it/s]37674it [00:13, 3373.35it/s]41456it [00:14, 3406.36it/s]38815it [00:14, 3354.35it/s]42053it [00:14, 3430.30it/s]38014it [00:13, 3280.99it/s]41831it [00:14, 3504.93it/s]39184it [00:14, 3449.19it/s]42437it [00:14, 3544.54it/s]38384it [00:14, 3399.49it/s]42183it [00:14, 3388.80it/s]39534it [00:14, 3354.57it/s]42808it [00:14, 3591.54it/s]38726it [00:14, 3271.57it/s]42558it [00:15, 3491.13it/s]39908it [00:14, 3465.05it/s]43169it [00:14, 3460.55it/s]39099it [00:14, 3401.71it/s]42909it [00:15, 3390.19it/s]40274it [00:14, 3519.62it/s]43538it [00:15, 3524.84it/s]39445it [00:14, 3417.87it/s]43279it [00:15, 3478.71it/s]40627it [00:15, 3408.42it/s]39789it [00:14, 3311.27it/s]43632it [00:15, 3492.76it/s]40988it [00:15, 3464.21it/s]40156it [00:14, 3413.53it/s]41336it [00:15, 3376.76it/s]40499it [00:14, 3316.19it/s]41675it [00:15, 3327.69it/s]40866it [00:14, 3417.00it/s]42054it [00:15, 3302.66it/s]41214it [00:14, 3285.18it/s]42437it [00:15, 3450.93it/s]41587it [00:15, 3411.18it/s]42818it [00:15, 3552.00it/s]41965it [00:15, 3515.81it/s]43175it [00:15, 3426.25it/s]42319it [00:15, 3393.11it/s]43544it [00:15, 3500.38it/s]42661it [00:15, 3371.17it/s]43000it [00:15, 3266.41it/s]43367it [00:15, 3380.98it/s]43731it [00:15, 3453.33it/s]43893it [00:16, 695.17it/s] 44278it [00:16, 932.92it/s]44642it [00:16, 1179.95it/s]45027it [00:16, 1501.48it/s]45409it [00:16, 1841.30it/s]45752it [00:17, 2090.13it/s]43983it [00:17, 558.55it/s] 46123it [00:17, 2408.01it/s]44361it [00:17, 760.75it/s]46470it [00:17, 2589.56it/s]44676it [00:17, 957.37it/s]46842it [00:17, 2855.06it/s]45040it [00:17, 1239.44it/s]45398it [00:17, 1545.14it/s]47191it [00:17, 2957.89it/s]47576it [00:17, 3188.28it/s]45728it [00:17, 1807.99it/s]47950it [00:17, 3337.19it/s]46100it [00:17, 2155.80it/s]46439it [00:17, 2369.98it/s]48310it [00:17, 3278.78it/s]46804it [00:18, 2657.30it/s]48688it [00:17, 3417.27it/s]47161it [00:18, 2799.62it/s]49044it [00:18, 3340.15it/s]47539it [00:18, 3045.03it/s]49405it [00:18, 3414.36it/s]43896it [00:18, 499.09it/s] 47917it [00:18, 3239.05it/s]49754it [00:18, 3341.14it/s]44278it [00:18, 683.93it/s]50130it [00:18, 3457.78it/s]48274it [00:18, 3145.29it/s]44643it [00:18, 892.06it/s]44078it [00:17, 522.19it/s] 50504it [00:18, 3537.24it/s]48650it [00:18, 3309.71it/s]45021it [00:18, 1163.97it/s]44443it [00:17, 707.73it/s]50862it [00:18, 3421.67it/s]45363it [00:18, 1431.16it/s]49000it [00:18, 3170.46it/s]44728it [00:17, 875.03it/s]51245it [00:18, 3536.11it/s]49376it [00:18, 3330.29it/s]45691it [00:18, 1689.42it/s]45108it [00:18, 1168.53it/s]51602it [00:18, 3410.69it/s]46060it [00:18, 2030.74it/s]49721it [00:18, 3294.07it/s]45462it [00:18, 1465.91it/s]51985it [00:18, 3528.29it/s]50096it [00:19, 3421.26it/s]46398it [00:18, 2246.37it/s]45787it [00:18, 1719.99it/s]50451it [00:19, 3451.12it/s]52341it [00:18, 3435.17it/s]46775it [00:18, 2573.90it/s]46157it [00:18, 2070.03it/s]52714it [00:19, 3519.30it/s]47154it [00:19, 2839.02it/s]50801it [00:19, 3359.95it/s]46492it [00:18, 2262.97it/s]53068it [00:19, 3427.97it/s]51180it [00:19, 3481.83it/s]47504it [00:19, 2932.24it/s]46863it [00:18, 2578.06it/s]53448it [00:19, 3530.50it/s]47874it [00:19, 3129.36it/s]51532it [00:19, 3376.71it/s]47200it [00:18, 2643.07it/s]53823it [00:19, 3593.43it/s]51908it [00:19, 3484.55it/s]48224it [00:19, 3126.51it/s]47575it [00:18, 2913.55it/s]54184it [00:19, 3489.36it/s]48602it [00:19, 3302.45it/s]52259it [00:19, 3347.24it/s]47933it [00:18, 3083.49it/s]54564it [00:19, 3578.08it/s]52621it [00:19, 3413.41it/s]48952it [00:19, 3137.63it/s]48276it [00:18, 3050.88it/s]54924it [00:19, 3439.87it/s]52992it [00:19, 3496.74it/s]49328it [00:19, 3305.49it/s]48649it [00:19, 3233.53it/s]55293it [00:19, 3510.54it/s]53344it [00:19, 3396.44it/s]48992it [00:19, 3211.47it/s]49684it [00:19, 3245.65it/s]55646it [00:19, 3403.19it/s]53715it [00:20, 3486.35it/s]49357it [00:19, 3333.72it/s]50063it [00:19, 3396.00it/s]56028it [00:20, 3521.09it/s]54066it [00:20, 3407.21it/s]50426it [00:19, 3461.46it/s]49701it [00:19, 3274.19it/s]54431it [00:20, 3475.83it/s]56403it [00:20, 3429.04it/s]50067it [00:19, 3383.68it/s]50778it [00:20, 3375.10it/s]56784it [00:20, 3535.51it/s]54780it [00:20, 3382.81it/s]50422it [00:19, 3431.39it/s]51151it [00:20, 3476.24it/s]57169it [00:20, 3625.91it/s]55150it [00:20, 3472.12it/s]50770it [00:19, 3352.50it/s]51503it [00:20, 3383.79it/s]55517it [00:20, 3529.12it/s]57534it [00:20, 3501.89it/s]51153it [00:19, 3488.54it/s]51867it [00:20, 3456.40it/s]57915it [00:20, 3589.59it/s]55871it [00:20, 3391.49it/s]51505it [00:19, 3354.70it/s]52215it [00:20, 3369.85it/s]56243it [00:20, 3485.46it/s]58276it [00:20, 3434.40it/s]51871it [00:20, 3439.86it/s]52567it [00:20, 3412.09it/s]58649it [00:20, 3516.96it/s]56594it [00:20, 3354.55it/s]52937it [00:20, 3495.84it/s]52218it [00:20, 3348.63it/s]56967it [00:21, 3461.10it/s]59003it [00:20, 3402.04it/s]52583it [00:20, 3434.29it/s]53288it [00:20, 3362.72it/s]59378it [00:21, 3499.00it/s]57315it [00:21, 3361.04it/s]52958it [00:20, 3525.11it/s]53661it [00:20, 3467.72it/s]59749it [00:21, 3557.80it/s]57689it [00:21, 3467.76it/s]54010it [00:21, 3360.29it/s]53313it [00:20, 3376.29it/s]58063it [00:21, 3546.55it/s]60107it [00:21, 3434.69it/s]54393it [00:21, 3493.09it/s]53680it [00:20, 3459.19it/s]60477it [00:21, 3510.71it/s]58420it [00:21, 3419.87it/s]54028it [00:20, 3383.37it/s]54745it [00:21, 3337.67it/s]58781it [00:21, 3466.77it/s]60830it [00:21, 3404.26it/s]54399it [00:20, 3476.64it/s]55120it [00:21, 3453.93it/s]61209it [00:21, 3513.97it/s]59130it [00:21, 3369.12it/s]55487it [00:21, 3516.08it/s]54749it [00:20, 3374.36it/s]59496it [00:21, 3451.69it/s]61562it [00:21, 3375.56it/s]55095it [00:20, 3398.60it/s]55841it [00:21, 3372.25it/s]61941it [00:21, 3493.23it/s]55453it [00:21, 3451.21it/s]56207it [00:21, 3452.02it/s]59843it [00:21, 3204.47it/s]62293it [00:21, 3395.68it/s]55800it [00:21, 3316.36it/s]60209it [00:21, 3330.11it/s]56555it [00:21, 3340.62it/s]60581it [00:22, 3440.72it/s]56137it [00:21, 3329.34it/s]56935it [00:21, 3470.34it/s]56472it [00:21, 3282.14it/s]60929it [00:22, 3357.43it/s]57285it [00:21, 3397.72it/s]56847it [00:21, 3416.28it/s]61307it [00:22, 3476.64it/s]57653it [00:22, 3478.01it/s]57224it [00:21, 3519.39it/s]58027it [00:22, 3551.50it/s]61657it [00:22, 3368.81it/s]57577it [00:21, 3418.02it/s]62022it [00:22, 3447.16it/s]58384it [00:22, 3383.85it/s]57931it [00:21, 3453.04it/s]58758it [00:22, 3483.87it/s]58278it [00:21, 3346.14it/s]59109it [00:22, 3357.55it/s]58638it [00:21, 3417.97it/s]59467it [00:22, 3418.70it/s]58981it [00:22, 3238.54it/s]59811it [00:22, 3187.67it/s]59318it [00:22, 3269.10it/s]60134it [00:22, 3154.90it/s]59653it [00:22, 3291.96it/s]60507it [00:22, 3315.10it/s]59984it [00:22, 3201.14it/s]60842it [00:23, 3260.46it/s]60344it [00:22, 3313.97it/s]61222it [00:23, 3413.08it/s]60677it [00:22, 3241.17it/s]61566it [00:23, 3321.17it/s]61042it [00:22, 3357.43it/s]61943it [00:23, 3449.47it/s]61407it [00:22, 3441.87it/s]62290it [00:23, 3354.30it/s]61753it [00:22, 3306.04it/s]62635it [00:23, 557.09it/s] 62124it [00:23, 3421.07it/s]63009it [00:23, 756.61it/s]63328it [00:23, 956.62it/s]63698it [00:24, 1244.89it/s]64032it [00:24, 1508.68it/s]64405it [00:24, 1855.88it/s]64783it [00:24, 2206.90it/s]65131it [00:24, 2420.38it/s]65502it [00:24, 2708.11it/s]62369it [00:24, 489.10it/s] 65850it [00:24, 2814.36it/s]62742it [00:24, 668.65it/s]66220it [00:24, 3036.22it/s]63115it [00:24, 893.12it/s]63427it [00:25, 1099.58it/s]66567it [00:24, 3076.14it/s]63779it [00:25, 1387.22it/s]66945it [00:24, 3263.36it/s]67321it [00:25, 3399.61it/s]64100it [00:25, 1645.93it/s]64464it [00:25, 1987.19it/s]67679it [00:25, 3278.12it/s]64841it [00:25, 2323.72it/s]68041it [00:25, 3373.06it/s]65184it [00:25, 2508.04it/s]68389it [00:25, 3272.40it/s]65554it [00:25, 2786.35it/s]68759it [00:25, 3391.30it/s]65898it [00:25, 2883.23it/s]69104it [00:25, 3311.10it/s]66269it [00:25, 3096.25it/s]69459it [00:25, 3376.83it/s]66615it [00:25, 3086.40it/s]69834it [00:25, 3482.06it/s]66983it [00:26, 3245.88it/s]70185it [00:25, 3389.48it/s]67339it [00:26, 3331.29it/s]70555it [00:26, 3478.03it/s]67687it [00:26, 3279.74it/s]70905it [00:26, 3370.34it/s]68049it [00:26, 3376.08it/s]71275it [00:26, 3462.64it/s]62628it [00:26, 391.91it/s] 68395it [00:26, 3298.34it/s]71623it [00:26, 3356.57it/s]62987it [00:26, 537.93it/s]68767it [00:26, 3417.93it/s]71999it [00:26, 3469.71it/s]63299it [00:26, 696.03it/s]69114it [00:26, 3319.21it/s]72365it [00:26, 3522.97it/s]63665it [00:26, 934.29it/s]69471it [00:26, 3388.54it/s]72719it [00:26, 3409.72it/s]64027it [00:26, 1211.01it/s]69841it [00:26, 3468.76it/s]73087it [00:26, 3486.02it/s]64353it [00:26, 1460.59it/s]70191it [00:26, 3372.00it/s]73437it [00:26, 3381.40it/s]64712it [00:26, 1787.83it/s]70559it [00:27, 3460.00it/s]73822it [00:26, 3514.02it/s]65044it [00:26, 2035.05it/s]70907it [00:27, 3218.52it/s]74175it [00:27, 3423.63it/s]62468it [00:26, 327.50it/s] 65397it [00:27, 2337.08it/s]71274it [00:27, 3343.19it/s]74548it [00:27, 3509.21it/s]62833it [00:26, 454.40it/s]65730it [00:27, 2499.07it/s]74920it [00:27, 3569.09it/s]71613it [00:27, 3276.84it/s]63193it [00:26, 613.52it/s]66107it [00:27, 2801.54it/s]71987it [00:27, 3407.97it/s]75278it [00:27, 3440.06it/s]63540it [00:26, 809.03it/s]66466it [00:27, 3000.25it/s]72343it [00:27, 3450.79it/s]75653it [00:27, 3520.46it/s]63913it [00:26, 1069.07it/s]66811it [00:27, 3047.56it/s]72691it [00:27, 3365.62it/s]64241it [00:26, 1311.82it/s]76007it [00:27, 3400.87it/s]67184it [00:27, 3231.32it/s]73072it [00:27, 3492.40it/s]64572it [00:27, 1588.05it/s]76376it [00:27, 3481.07it/s]67532it [00:27, 3183.66it/s]64898it [00:27, 1851.53it/s]73423it [00:27, 3387.29it/s]76726it [00:27, 3336.63it/s]67908it [00:27, 3342.92it/s]65264it [00:27, 2193.65it/s]73790it [00:28, 3465.73it/s]77095it [00:27, 3434.47it/s]68256it [00:27, 3245.51it/s]65632it [00:27, 2509.23it/s]74139it [00:28, 3384.53it/s]77473it [00:28, 3363.13it/s]68628it [00:27, 3377.67it/s]65977it [00:27, 2671.38it/s]74506it [00:28, 3466.06it/s]77844it [00:28, 3460.68it/s]68995it [00:28, 3458.98it/s]66314it [00:27, 2842.34it/s]74860it [00:28, 3487.08it/s]78216it [00:28, 3532.47it/s]69347it [00:28, 3196.74it/s]66651it [00:27, 2924.78it/s]75210it [00:28, 3341.69it/s]78571it [00:28, 3420.52it/s]69715it [00:28, 3328.73it/s]67017it [00:27, 3119.17it/s]75580it [00:28, 3442.16it/s]78943it [00:28, 3497.50it/s]70055it [00:28, 3282.32it/s]67391it [00:27, 3288.94it/s]75926it [00:28, 3353.21it/s]79295it [00:28, 3385.46it/s]70418it [00:28, 3380.69it/s]67742it [00:27, 3232.04it/s]76285it [00:28, 3421.27it/s]79666it [00:28, 3476.91it/s]70760it [00:28, 3304.98it/s]68108it [00:28, 3349.32it/s]76631it [00:28, 3298.72it/s]80016it [00:28, 3377.94it/s]71126it [00:28, 3404.86it/s]68455it [00:28, 3268.61it/s]76996it [00:29, 3398.29it/s]80389it [00:28, 3476.74it/s]71487it [00:28, 3463.57it/s]68826it [00:28, 3392.47it/s]77367it [00:29, 3487.79it/s]80746it [00:28, 3503.41it/s]71836it [00:28, 3361.62it/s]69172it [00:28, 3291.72it/s]77718it [00:29, 3380.24it/s]81098it [00:29, 3381.38it/s]72196it [00:29, 3428.58it/s]69541it [00:28, 3403.89it/s]78076it [00:29, 3436.40it/s]81463it [00:29, 3449.65it/s]72541it [00:29, 3181.54it/s]69889it [00:28, 3425.33it/s]78421it [00:29, 3213.59it/s]81810it [00:29, 3358.65it/s]72923it [00:29, 3357.91it/s]70235it [00:28, 3325.21it/s]78789it [00:29, 3343.30it/s]82183it [00:29, 3462.64it/s]73274it [00:29, 3258.01it/s]70599it [00:28, 3414.77it/s]79151it [00:29, 3273.61it/s]82531it [00:29, 3356.61it/s]73657it [00:29, 3416.41it/s]70943it [00:28, 3322.53it/s]79506it [00:29, 3350.87it/s]82905it [00:29, 3465.94it/s]74027it [00:29, 3495.30it/s]71301it [00:28, 3395.09it/s]79877it [00:29, 3452.92it/s]83277it [00:29, 3537.08it/s]74380it [00:29, 3388.69it/s]71643it [00:29, 3315.48it/s]80225it [00:29, 3343.71it/s]83632it [00:29, 3413.92it/s]74749it [00:29, 3472.31it/s]72006it [00:29, 3406.13it/s]80598it [00:30, 3453.09it/s]84001it [00:29, 3492.67it/s]75099it [00:29, 3319.66it/s]72369it [00:29, 3471.38it/s]80946it [00:30, 3315.26it/s]84352it [00:30, 3330.55it/s]75470it [00:30, 3429.68it/s]72718it [00:29, 3351.11it/s]81310it [00:30, 3406.55it/s]84719it [00:30, 3426.52it/s]73100it [00:29, 3484.69it/s]75816it [00:30, 3299.89it/s]85064it [00:30, 3328.14it/s]81670it [00:30, 3298.31it/s]76176it [00:30, 3383.17it/s]73451it [00:29, 3348.37it/s]85432it [00:30, 3426.58it/s]82025it [00:30, 3367.44it/s]76529it [00:30, 3423.79it/s]73821it [00:29, 3447.66it/s]82372it [00:30, 3395.90it/s]85803it [00:30, 3502.76it/s]76874it [00:30, 3294.48it/s]74168it [00:29, 3356.49it/s]82714it [00:30, 3318.01it/s]77247it [00:30, 3418.04it/s]74541it [00:29, 3463.43it/s]83080it [00:30, 3416.42it/s]74900it [00:30, 3499.99it/s]77591it [00:30, 3304.45it/s]83423it [00:30, 3318.93it/s]77958it [00:30, 3407.59it/s]75252it [00:30, 3385.63it/s]83788it [00:31, 3412.99it/s]75610it [00:30, 3439.41it/s]78313it [00:30, 3285.48it/s]84141it [00:31, 3446.81it/s]78684it [00:30, 3403.38it/s]75956it [00:30, 3338.37it/s]84487it [00:31, 3344.38it/s]79052it [00:31, 3481.80it/s]76309it [00:30, 3391.61it/s]84852it [00:31, 3432.22it/s]76650it [00:30, 3303.52it/s]79403it [00:31, 3340.13it/s]85197it [00:31, 3328.46it/s]76996it [00:30, 3347.31it/s]79769it [00:31, 3430.46it/s]85545it [00:31, 3371.27it/s]77360it [00:30, 3432.31it/s]80115it [00:31, 3255.68it/s]85884it [00:31, 3271.27it/s]77705it [00:30, 3301.14it/s]80488it [00:31, 3388.89it/s]78061it [00:30, 3374.87it/s]80833it [00:31, 3310.42it/s]78400it [00:31, 3272.23it/s]81198it [00:31, 3405.64it/s]78758it [00:31, 3359.12it/s]81559it [00:31, 3462.91it/s]79110it [00:31, 3405.53it/s]81907it [00:31, 3349.10it/s]79452it [00:31, 3285.42it/s]82276it [00:32, 3445.53it/s]79813it [00:31, 3378.36it/s]82623it [00:32, 3309.37it/s]80153it [00:31, 3264.69it/s]82995it [00:32, 3424.99it/s]80513it [00:31, 3360.48it/s]83353it [00:32, 3198.97it/s]80851it [00:31, 3217.45it/s]83703it [00:32, 3277.18it/s]81216it [00:31, 3338.86it/s]84072it [00:32, 3392.26it/s]81569it [00:32, 3393.58it/s]84415it [00:32, 3298.47it/s]81911it [00:32, 3278.50it/s]84769it [00:32, 3366.59it/s]82263it [00:32, 3346.42it/s]85108it [00:32, 3268.07it/s]82600it [00:32, 3223.76it/s]85474it [00:32, 3379.48it/s]86155it [00:33, 408.39it/s] 82970it [00:32, 3357.46it/s]85814it [00:33, 3299.42it/s]86518it [00:33, 557.67it/s]83326it [00:32, 3413.94it/s]86824it [00:33, 713.05it/s]83669it [00:32, 3311.00it/s]87195it [00:33, 957.13it/s]84039it [00:32, 3421.40it/s]87567it [00:33, 1244.99it/s]84383it [00:32, 3136.50it/s]87899it [00:33, 1504.45it/s]84733it [00:33, 3236.66it/s]88267it [00:33, 1842.48it/s]85062it [00:33, 3115.60it/s]88607it [00:33, 2100.54it/s]85425it [00:33, 3259.04it/s]88979it [00:33, 2430.78it/s]85793it [00:33, 3378.64it/s]89324it [00:34, 2596.77it/s]89686it [00:34, 2840.84it/s]90055it [00:34, 3056.73it/s]90406it [00:34, 3065.06it/s]86213it [00:34, 355.59it/s] 90762it [00:34, 3195.86it/s]86578it [00:34, 496.06it/s]91106it [00:34, 3137.52it/s]86863it [00:34, 631.15it/s]91462it [00:34, 3252.67it/s]87234it [00:34, 864.51it/s]91822it [00:34, 3183.39it/s]87607it [00:35, 1144.44it/s]92179it [00:34, 3288.42it/s]87932it [00:35, 1394.73it/s]92536it [00:35, 3365.74it/s]88301it [00:35, 1734.38it/s]92879it [00:35, 3231.35it/s]88636it [00:35, 1979.57it/s]93239it [00:35, 3334.76it/s]89006it [00:35, 2317.90it/s]93577it [00:35, 3239.84it/s]89345it [00:35, 2501.29it/s]93937it [00:35, 3340.24it/s]89715it [00:35, 2782.65it/s]94297it [00:35, 3412.58it/s]90077it [00:35, 2992.17it/s]94641it [00:35, 3290.84it/s]90425it [00:35, 2914.21it/s]94999it [00:35, 3373.51it/s]90767it [00:35, 3044.28it/s]95339it [00:35, 3268.99it/s]91098it [00:36, 3027.86it/s]95699it [00:35, 3363.64it/s]91455it [00:36, 3174.30it/s]96038it [00:36, 3220.46it/s]91813it [00:36, 3286.11it/s]96398it [00:36, 3327.62it/s]92153it [00:36, 3199.66it/s]96758it [00:36, 3403.77it/s]92511it [00:36, 3306.15it/s]97101it [00:36, 3282.69it/s]92848it [00:36, 3185.54it/s]97460it [00:36, 3370.09it/s]93207it [00:36, 3298.29it/s]97799it [00:36, 3263.93it/s]93541it [00:36, 3210.38it/s]98159it [00:36, 3358.15it/s]86146it [00:36, 303.56it/s] 93886it [00:36, 3276.42it/s]98518it [00:36, 3423.67it/s]86503it [00:36, 422.15it/s]94240it [00:37, 3352.55it/s]86804it [00:36, 549.81it/s]98862it [00:36, 3305.55it/s]94578it [00:37, 3219.24it/s]87158it [00:36, 746.63it/s]99225it [00:37, 3397.76it/s]94936it [00:37, 3321.33it/s]87534it [00:37, 1005.57it/s]99567it [00:37, 3251.78it/s]95271it [00:37, 3219.79it/s]87858it [00:37, 1240.04it/s]99929it [00:37, 3355.43it/s]95628it [00:37, 3318.02it/s]88218it [00:37, 1556.29it/s]100267it [00:37, 3246.97it/s]95986it [00:37, 3392.85it/s]88548it [00:37, 1819.29it/s]100629it [00:37, 3351.13it/s]96327it [00:37, 3252.31it/s]88919it [00:37, 2169.30it/s]100987it [00:37, 3417.25it/s]96686it [00:37, 3347.44it/s]89278it [00:37, 2466.33it/s]101331it [00:37, 3296.99it/s]97023it [00:37, 3243.21it/s]89623it [00:37, 2620.80it/s]101687it [00:37, 3372.11it/s]97383it [00:37, 3343.44it/s]89993it [00:37, 2882.02it/s]102026it [00:37, 3264.93it/s]97720it [00:38, 3103.73it/s]102385it [00:37, 3357.49it/s]90338it [00:37, 2795.22it/s]98080it [00:38, 3239.13it/s]102734it [00:38, 3394.49it/s]90695it [00:37, 2990.51it/s]98438it [00:38, 3333.95it/s]91026it [00:38, 2989.19it/s]103075it [00:38, 3280.81it/s]86135it [00:37, 264.80it/s] 98775it [00:38, 3239.92it/s]91385it [00:38, 3148.94it/s]103434it [00:38, 3368.38it/s]86495it [00:37, 369.07it/s]99134it [00:38, 3337.40it/s]91733it [00:38, 3240.34it/s]86789it [00:37, 478.94it/s]103773it [00:38, 3261.63it/s]99471it [00:38, 3241.86it/s]87144it [00:37, 655.67it/s]104134it [00:38, 3361.18it/s]92070it [00:38, 3162.95it/s]99819it [00:38, 3309.62it/s]87509it [00:37, 883.15it/s]92428it [00:38, 3278.64it/s]104472it [00:38, 3246.85it/s]100174it [00:38, 3378.94it/s]87831it [00:38, 1106.17it/s]104832it [00:38, 3345.78it/s]92764it [00:38, 3169.07it/s]100514it [00:38, 3264.86it/s]88185it [00:38, 1403.42it/s]105187it [00:38, 3403.44it/s]93124it [00:38, 3290.05it/s]100871it [00:39, 3352.19it/s]88514it [00:38, 1661.42it/s]93483it [00:38, 3375.59it/s]105529it [00:38, 3273.63it/s]101208it [00:39, 3225.54it/s]88876it [00:38, 1999.94it/s]105874it [00:39, 3324.12it/s]93825it [00:38, 3180.79it/s]101551it [00:39, 3276.79it/s]89237it [00:38, 2318.08it/s]94184it [00:39, 3293.10it/s]106208it [00:39, 3222.61it/s]101901it [00:39, 3199.05it/s]89578it [00:38, 2512.11it/s]106565it [00:39, 3320.25it/s]94518it [00:39, 3202.20it/s]102259it [00:39, 3306.59it/s]89936it [00:38, 2763.12it/s]106925it [00:39, 3398.95it/s]94877it [00:39, 3310.41it/s]102617it [00:39, 3383.28it/s]90277it [00:38, 2859.40it/s]107267it [00:39, 3281.43it/s]95211it [00:39, 3183.86it/s]102957it [00:39, 3268.44it/s]90609it [00:38, 2972.84it/s]107627it [00:39, 3371.21it/s]95571it [00:39, 3299.26it/s]103305it [00:39, 3326.57it/s]90965it [00:38, 3130.22it/s]107966it [00:39, 3264.82it/s]95928it [00:39, 3376.84it/s]103640it [00:39, 3232.76it/s]91304it [00:39, 3100.29it/s]108329it [00:39, 3367.88it/s]96268it [00:39, 3235.12it/s]104000it [00:39, 3332.85it/s]91660it [00:39, 3228.01it/s]108668it [00:39, 3268.27it/s]96628it [00:39, 3337.83it/s]104358it [00:40, 3403.56it/s]91997it [00:39, 3104.34it/s]109018it [00:39, 3333.94it/s]96965it [00:39, 3227.78it/s]104700it [00:40, 3264.96it/s]92346it [00:39, 3210.40it/s]109377it [00:40, 3408.29it/s]97292it [00:40, 3238.22it/s]105030it [00:40, 3272.45it/s]92675it [00:39, 3126.15it/s]109720it [00:40, 3284.01it/s]97650it [00:40, 3335.91it/s]105359it [00:40, 3179.05it/s]92997it [00:39, 3151.40it/s]110082it [00:40, 3380.02it/s]97986it [00:40, 3228.33it/s]105716it [00:40, 3289.79it/s]93356it [00:39, 3275.01it/s]110422it [00:40, 3278.86it/s]98345it [00:40, 3332.15it/s]106071it [00:40, 3363.79it/s]93687it [00:39, 3193.19it/s]110785it [00:40, 3378.33it/s]98680it [00:40, 3203.17it/s]106409it [00:40, 3246.67it/s]94040it [00:39, 3289.78it/s]111142it [00:40, 3277.45it/s]99034it [00:40, 3298.60it/s]106766it [00:40, 3337.59it/s]94372it [00:40, 3164.31it/s]111505it [00:40, 3376.07it/s]99383it [00:40, 3205.14it/s]107102it [00:40, 3200.79it/s]94723it [00:40, 3261.29it/s]111866it [00:40, 3441.45it/s]99731it [00:40, 3281.67it/s]107458it [00:41, 3301.69it/s]95052it [00:40, 3246.75it/s]112212it [00:40, 3288.20it/s]100085it [00:40, 3354.73it/s]107791it [00:41, 3210.48it/s]95379it [00:40, 3128.78it/s]112575it [00:41, 3383.49it/s]100422it [00:40, 3233.75it/s]108146it [00:41, 3306.00it/s]95721it [00:40, 3210.73it/s]112916it [00:41, 3268.49it/s]100752it [00:41, 3250.85it/s]108495it [00:41, 3358.13it/s]96044it [00:40, 3097.92it/s]113277it [00:41, 3365.27it/s]101079it [00:41, 3162.70it/s]108833it [00:41, 3226.90it/s]96395it [00:40, 3213.65it/s]113638it [00:41, 3435.60it/s]101439it [00:41, 3285.72it/s]109191it [00:41, 3326.76it/s]96743it [00:40, 3289.11it/s]113984it [00:41, 3317.69it/s]101797it [00:41, 3370.13it/s]109526it [00:41, 3235.83it/s]97074it [00:40, 3173.19it/s]114346it [00:41, 3401.85it/s]102136it [00:41, 3226.98it/s]109880it [00:41, 3321.86it/s]97423it [00:40, 3262.25it/s]114688it [00:41, 3286.79it/s]102495it [00:41, 3329.80it/s]110230it [00:41, 3361.57it/s]97751it [00:41, 3121.05it/s]115050it [00:41, 3380.28it/s]102830it [00:41, 3221.82it/s]110568it [00:41, 3217.35it/s]98100it [00:41, 3223.84it/s]115390it [00:41, 3235.04it/s]103177it [00:41, 3291.53it/s]110926it [00:42, 3318.57it/s]98445it [00:41, 3288.83it/s]103534it [00:41, 3369.63it/s]111260it [00:42, 3221.68it/s]98776it [00:41, 3177.59it/s]103873it [00:42, 3245.88it/s]111611it [00:42, 3303.20it/s]99109it [00:41, 3219.36it/s]104203it [00:42, 3261.30it/s]111948it [00:42, 3322.20it/s]99433it [00:41, 3149.52it/s]104531it [00:42, 3155.66it/s]112282it [00:42, 3224.27it/s]99793it [00:41, 3278.75it/s]104890it [00:42, 3278.48it/s]112641it [00:42, 3328.66it/s]100140it [00:41, 3333.61it/s]105245it [00:42, 3355.16it/s]112976it [00:42, 3231.50it/s]100475it [00:41, 3212.94it/s]105582it [00:42, 3208.58it/s]113335it [00:42, 3333.02it/s]100824it [00:42, 3291.88it/s]105941it [00:42, 3314.94it/s]113670it [00:42, 3210.05it/s]101155it [00:42, 3182.64it/s]106275it [00:42, 3215.25it/s]114031it [00:43, 3321.73it/s]101505it [00:42, 3272.82it/s]106621it [00:42, 3284.31it/s]114389it [00:43, 3394.17it/s]101852it [00:42, 3330.02it/s]106952it [00:42, 3188.76it/s]114730it [00:43, 3276.50it/s]102187it [00:42, 3216.25it/s]107307it [00:43, 3289.83it/s]115076it [00:43, 3327.66it/s]102518it [00:42, 3242.35it/s]107643it [00:43, 3308.48it/s]102844it [00:42, 3152.09it/s]107975it [00:43, 3218.61it/s]103193it [00:42, 3249.01it/s]108338it [00:43, 3336.21it/s]103543it [00:42, 3319.61it/s]108673it [00:43, 3209.78it/s]103877it [00:42, 3199.52it/s]109037it [00:43, 3330.93it/s]104227it [00:43, 3283.46it/s]109396it [00:43, 3404.69it/s]104557it [00:43, 3174.05it/s]109738it [00:43, 3247.92it/s]104903it [00:43, 3254.92it/s]110095it [00:43, 3337.73it/s]105248it [00:43, 3309.70it/s]110431it [00:44, 3226.63it/s]105581it [00:43, 3168.82it/s]110774it [00:44, 3283.17it/s]105907it [00:43, 3194.39it/s]111119it [00:44, 3330.08it/s]106228it [00:43, 3102.55it/s]111454it [00:44, 3218.04it/s]106558it [00:43, 3158.84it/s]111801it [00:44, 3288.19it/s]106916it [00:43, 3280.18it/s]112132it [00:44, 3195.43it/s]107246it [00:44, 3192.66it/s]112493it [00:44, 3313.58it/s]107569it [00:44, 3201.96it/s]112826it [00:44, 3180.72it/s]107891it [00:44, 3139.93it/s]113187it [00:44, 3300.94it/s]108251it [00:44, 3271.60it/s]113543it [00:44, 3375.69it/s]108615it [00:44, 3377.07it/s]113883it [00:45, 3226.47it/s]108954it [00:44, 3243.54it/s]114246it [00:45, 3339.03it/s]109310it [00:44, 3333.28it/s]115716it [00:45, 304.86it/s] 114583it [00:45, 3101.24it/s]109645it [00:44, 3134.36it/s]116080it [00:45, 427.18it/s]114932it [00:45, 3207.41it/s]110006it [00:44, 3265.98it/s]116384it [00:45, 557.25it/s]115292it [00:45, 3316.24it/s]110336it [00:44, 3146.90it/s]116746it [00:45, 760.57it/s]110698it [00:45, 3278.19it/s]117090it [00:45, 980.96it/s]111060it [00:45, 3376.18it/s]117451it [00:45, 1267.15it/s]111400it [00:45, 3242.76it/s]117815it [00:46, 1586.24it/s]111759it [00:45, 3340.97it/s]118149it [00:46, 1837.29it/s]112096it [00:45, 3181.12it/s]118514it [00:46, 2170.97it/s]112453it [00:45, 3290.28it/s]118850it [00:46, 2364.93it/s]112809it [00:45, 3366.16it/s]119210it [00:46, 2642.49it/s]113148it [00:45, 3038.25it/s]119568it [00:46, 2870.90it/s]113507it [00:45, 3186.70it/s]119911it [00:46, 2918.50it/s]113833it [00:46, 3137.54it/s]120258it [00:46, 3063.41it/s]114195it [00:46, 3271.50it/s]120594it [00:46, 3058.38it/s]115411it [00:47, 290.09it/s] 114527it [00:46, 3177.87it/s]120955it [00:46, 3208.49it/s]115769it [00:47, 404.86it/s]114884it [00:46, 3288.63it/s]121292it [00:47, 3158.11it/s]116120it [00:47, 552.25it/s]115243it [00:46, 3373.90it/s]121654it [00:47, 3285.87it/s]116423it [00:47, 709.59it/s]122017it [00:47, 3381.91it/s]116787it [00:47, 953.30it/s]122362it [00:47, 3286.57it/s]117105it [00:47, 1183.50it/s]122728it [00:47, 3392.68it/s]117450it [00:47, 1480.20it/s]123072it [00:47, 3284.66it/s]117804it [00:47, 1804.49it/s]123436it [00:47, 3385.60it/s]118136it [00:47, 2049.83it/s]123787it [00:47, 3421.09it/s]118496it [00:47, 2367.34it/s]124132it [00:47, 3301.71it/s]118831it [00:48, 2530.95it/s]124496it [00:48, 3396.39it/s]119190it [00:48, 2781.82it/s]124838it [00:48, 3283.77it/s]119540it [00:48, 2960.53it/s]125195it [00:48, 3364.43it/s]119879it [00:48, 2976.75it/s]125534it [00:48, 3252.94it/s]120238it [00:48, 3140.69it/s]125897it [00:48, 3360.39it/s]120575it [00:48, 3107.08it/s]126257it [00:48, 3428.27it/s]120927it [00:48, 3220.47it/s]126602it [00:48, 3297.22it/s]121262it [00:48, 3248.60it/s]126952it [00:48, 3354.14it/s]121596it [00:48, 3174.60it/s]127289it [00:48, 3258.51it/s]121958it [00:49, 3299.81it/s]127651it [00:48, 3361.57it/s]122293it [00:49, 3213.09it/s]128010it [00:49, 3258.95it/s]122658it [00:49, 3336.17it/s]128366it [00:49, 3343.05it/s]122995it [00:49, 3232.80it/s]128728it [00:49, 3422.50it/s]123346it [00:49, 3311.91it/s]129072it [00:49, 3303.09it/s]123706it [00:49, 3394.04it/s]129434it [00:49, 3392.49it/s]124048it [00:49, 3275.40it/s]129775it [00:49, 3275.35it/s]124406it [00:49, 3361.84it/s]130121it [00:49, 3326.47it/s]124744it [00:49, 3161.37it/s]130483it [00:49, 3410.67it/s]125103it [00:49, 3281.32it/s]130826it [00:49, 3296.82it/s]125459it [00:50, 3360.89it/s]115628it [00:49, 248.31it/s] 131187it [00:50, 3384.31it/s]125798it [00:50, 3253.78it/s]115991it [00:50, 348.95it/s]131527it [00:50, 3267.29it/s]126158it [00:50, 3352.06it/s]116274it [00:50, 450.11it/s]131887it [00:50, 3360.74it/s]126496it [00:50, 3215.64it/s]116638it [00:50, 625.66it/s]132225it [00:50, 3256.72it/s]126854it [00:50, 3316.42it/s]117002it [00:50, 845.34it/s]132585it [00:50, 3352.74it/s]127188it [00:50, 3219.78it/s]117324it [00:50, 1060.81it/s]132944it [00:50, 3420.66it/s]127548it [00:50, 3325.69it/s]117673it [00:50, 1346.97it/s]133288it [00:50, 3263.68it/s]127904it [00:50, 3390.79it/s]117998it [00:50, 1598.34it/s]133648it [00:50, 3356.97it/s]128245it [00:50, 3099.99it/s]118343it [00:50, 1910.29it/s]133986it [00:50, 3254.96it/s]128603it [00:51, 3230.51it/s]118691it [00:50, 2214.95it/s]134347it [00:50, 3355.65it/s]128932it [00:51, 3162.23it/s]119023it [00:50, 2395.28it/s]134710it [00:51, 3432.48it/s]129291it [00:51, 3282.00it/s]119382it [00:51, 2672.51it/s]135055it [00:51, 3304.74it/s]129648it [00:51, 3363.71it/s]119715it [00:51, 2755.85it/s]135414it [00:51, 3384.83it/s]129987it [00:51, 3236.94it/s]120064it [00:51, 2943.45it/s]135755it [00:51, 3270.30it/s]130346it [00:51, 3336.36it/s]120426it [00:51, 3122.84it/s]136114it [00:51, 3360.17it/s]130682it [00:51, 3224.43it/s]120766it [00:51, 3083.42it/s]136452it [00:51, 3215.70it/s]131040it [00:51, 3324.05it/s]121125it [00:51, 3223.01it/s]136810it [00:51, 3317.72it/s]131375it [00:51, 3223.20it/s]121462it [00:51, 3126.07it/s]137168it [00:51, 3392.73it/s]131715it [00:52, 3272.95it/s]121809it [00:51, 3220.64it/s]137510it [00:51, 3278.49it/s]115583it [00:51, 229.56it/s] 132060it [00:52, 3322.03it/s]122140it [00:51, 3134.13it/s]137872it [00:52, 3374.50it/s]115945it [00:51, 322.80it/s]132394it [00:52, 3229.64it/s]122490it [00:52, 3235.58it/s]138212it [00:52, 3270.10it/s]116252it [00:51, 426.26it/s]132751it [00:52, 3325.91it/s]122852it [00:52, 3344.46it/s]138572it [00:52, 3362.35it/s]116605it [00:51, 585.07it/s]133085it [00:52, 3203.62it/s]123191it [00:52, 3245.01it/s]138930it [00:52, 3257.75it/s]116967it [00:51, 791.86it/s]133443it [00:52, 3310.17it/s]123550it [00:52, 3341.94it/s]139291it [00:52, 3355.85it/s]117289it [00:51, 999.54it/s]133801it [00:52, 3387.23it/s]123887it [00:52, 3204.45it/s]139635it [00:52, 3378.49it/s]117647it [00:51, 1287.46it/s]134142it [00:52, 3279.00it/s]124248it [00:52, 3317.53it/s]139975it [00:52, 3285.48it/s]117974it [00:51, 1539.03it/s]134501it [00:52, 3367.80it/s]124609it [00:52, 3401.43it/s]140334it [00:52, 3371.38it/s]118334it [00:52, 1873.65it/s]134840it [00:52, 3238.97it/s]124952it [00:52, 3252.04it/s]118684it [00:52, 2179.28it/s]140673it [00:52, 3258.74it/s]135193it [00:53, 3320.74it/s]125294it [00:52, 3299.53it/s]141030it [00:52, 3347.40it/s]119021it [00:52, 2310.30it/s]135548it [00:53, 3386.83it/s]125627it [00:52, 3204.05it/s]141391it [00:53, 3422.06it/s]119372it [00:52, 2577.69it/s]135889it [00:53, 3275.52it/s]125985it [00:53, 3311.23it/s]141735it [00:53, 3289.55it/s]119699it [00:52, 2689.27it/s]136246it [00:53, 3357.60it/s]126332it [00:53, 3182.81it/s]142092it [00:53, 3369.63it/s]120050it [00:52, 2896.47it/s]136584it [00:53, 3228.50it/s]126691it [00:53, 3295.44it/s]142431it [00:53, 3255.04it/s]120413it [00:52, 3089.41it/s]136943it [00:53, 3330.23it/s]127050it [00:53, 3379.52it/s]142790it [00:53, 3348.23it/s]120752it [00:52, 3052.79it/s]137278it [00:53, 3234.68it/s]127390it [00:53, 3242.31it/s]143130it [00:53, 3249.07it/s]121112it [00:52, 3201.43it/s]137638it [00:53, 3337.06it/s]127748it [00:53, 3338.18it/s]143474it [00:53, 3301.16it/s]121449it [00:53, 3132.01it/s]137996it [00:53, 3406.35it/s]128084it [00:53, 3222.91it/s]143832it [00:53, 3379.66it/s]121811it [00:53, 3266.67it/s]138338it [00:54, 3257.88it/s]128441it [00:53, 3321.58it/s]144172it [00:53, 3269.01it/s]122147it [00:53, 3120.10it/s]138688it [00:54, 3326.95it/s]128778it [00:53, 3334.78it/s]144531it [00:54, 3360.16it/s]122500it [00:53, 3232.28it/s]139023it [00:54, 3206.14it/s]129113it [00:54, 3222.04it/s]144869it [00:54, 3268.79it/s]122852it [00:53, 3311.88it/s]139385it [00:54, 3322.47it/s]129474it [00:54, 3332.26it/s]145230it [00:54, 3359.70it/s]123188it [00:53, 3225.73it/s]139744it [00:54, 3399.16it/s]129809it [00:54, 3205.97it/s]145592it [00:54, 3433.11it/s]123536it [00:53, 3297.97it/s]140086it [00:54, 3259.14it/s]130169it [00:54, 3316.19it/s]145937it [00:54, 3308.06it/s]123869it [00:53, 3213.98it/s]140444it [00:54, 3348.57it/s]130527it [00:54, 3391.38it/s]146293it [00:54, 3378.09it/s]124216it [00:53, 3286.33it/s]140781it [00:54, 3246.75it/s]130868it [00:54, 3265.86it/s]146633it [00:54, 3262.15it/s]124575it [00:53, 3374.35it/s]141137it [00:54, 3335.83it/s]131214it [00:54, 3319.70it/s]146978it [00:54, 3314.45it/s]124915it [00:54, 3234.17it/s]141473it [00:54, 3202.43it/s]131548it [00:54, 3212.20it/s]147331it [00:54, 3223.73it/s]125270it [00:54, 3324.41it/s]141826it [00:55, 3294.56it/s]131904it [00:54, 3309.53it/s]147692it [00:54, 3332.60it/s]125605it [00:54, 3193.02it/s]142176it [00:55, 3353.18it/s]148051it [00:55, 3404.22it/s]132237it [00:54, 3035.79it/s]125951it [00:54, 3268.43it/s]142513it [00:55, 3146.51it/s]132596it [00:55, 3187.87it/s]148393it [00:55, 3285.27it/s]126296it [00:54, 3318.85it/s]142863it [00:55, 3243.99it/s]132949it [00:55, 3281.93it/s]148753it [00:55, 3373.61it/s]126630it [00:54, 3214.49it/s]143191it [00:55, 3136.10it/s]133282it [00:55, 3165.59it/s]149092it [00:55, 3264.77it/s]126967it [00:54, 3257.25it/s]143537it [00:55, 3225.90it/s]133631it [00:55, 3254.89it/s]149447it [00:55, 3346.26it/s]127295it [00:54, 3133.36it/s]143881it [00:55, 3284.84it/s]149802it [00:55, 3404.25it/s]133960it [00:55, 3075.75it/s]127630it [00:54, 3193.26it/s]144212it [00:55, 3093.11it/s]134308it [00:55, 3187.35it/s]150144it [00:55, 3262.28it/s]127975it [00:55, 3265.93it/s]144559it [00:55, 3196.85it/s]134655it [00:55, 3266.45it/s]150498it [00:55, 3339.87it/s]128303it [00:55, 3047.36it/s]144882it [00:56, 3008.08it/s]150834it [00:55, 3180.35it/s]134985it [00:55, 3060.16it/s]128648it [00:55, 3158.03it/s]145229it [00:56, 3135.53it/s]151188it [00:56, 3281.67it/s]135332it [00:55, 3173.79it/s]128968it [00:55, 2993.96it/s]145572it [00:56, 3216.58it/s]151531it [00:56, 3180.76it/s]135654it [00:56, 3012.41it/s]129309it [00:55, 3107.58it/s]145897it [00:56, 3028.12it/s]151886it [00:56, 3284.44it/s]135999it [00:56, 3132.22it/s]129645it [00:55, 3178.69it/s]146237it [00:56, 3130.63it/s]152240it [00:56, 3356.19it/s]136336it [00:56, 3197.40it/s]129966it [00:55, 3033.40it/s]146554it [00:56, 2947.89it/s]136659it [00:56, 3029.05it/s]130298it [00:55, 3112.94it/s]146901it [00:56, 3089.81it/s]137005it [00:56, 3148.01it/s]130612it [00:55, 2946.43it/s]147244it [00:56, 3184.29it/s]137324it [00:56, 2976.40it/s]130960it [00:56, 3093.00it/s]147566it [00:56, 3013.98it/s]137673it [00:56, 3114.21it/s]131303it [00:56, 3188.22it/s]147892it [00:57, 3082.29it/s]138019it [00:56, 3211.23it/s]131625it [00:56, 3007.87it/s]148204it [00:57, 2935.90it/s]138344it [00:56, 3035.58it/s]131968it [00:56, 3124.29it/s]148547it [00:57, 3071.64it/s]138679it [00:57, 3122.48it/s]132284it [00:56, 2975.11it/s]148887it [00:57, 3164.96it/s]138995it [00:57, 2957.27it/s]132618it [00:56, 3075.81it/s]149207it [00:57, 3003.18it/s]139346it [00:57, 3110.72it/s]132962it [00:56, 3179.27it/s]149541it [00:57, 3095.67it/s]139685it [00:57, 3187.57it/s]133283it [00:56, 3017.90it/s]149854it [00:57, 2953.41it/s]140007it [00:57, 3024.43it/s]133625it [00:56, 3128.61it/s]150193it [00:57, 3074.93it/s]140357it [00:57, 3157.05it/s]133941it [00:57, 2955.27it/s]150529it [00:57, 3153.94it/s]140677it [00:57, 2958.69it/s]134275it [00:57, 3062.13it/s]150847it [00:57, 2974.73it/s]141021it [00:57, 3089.37it/s]134619it [00:57, 3167.66it/s]151179it [00:58, 3071.16it/s]141367it [00:57, 3191.62it/s]134939it [00:57, 3014.33it/s]151519it [00:58, 3162.67it/s]141690it [00:58, 3020.49it/s]135283it [00:57, 3133.41it/s]151838it [00:58, 2986.77it/s]142013it [00:58, 3070.61it/s]135600it [00:57, 2940.04it/s]152173it [00:58, 3087.31it/s]142324it [00:58, 2942.97it/s]135945it [00:57, 3078.87it/s]142667it [00:58, 3078.48it/s]136287it [00:57, 3174.63it/s]143013it [00:58, 3184.91it/s]136608it [00:57, 3019.09it/s]143335it [00:58, 3023.54it/s]136929it [00:57, 3071.88it/s]143673it [00:58, 3122.45it/s]137252it [00:58, 2944.86it/s]143989it [00:58, 2982.49it/s]137601it [00:58, 3095.45it/s]144333it [00:58, 3109.85it/s]137941it [00:58, 3181.19it/s]144675it [00:58, 3197.76it/s]138262it [00:58, 3010.76it/s]144998it [00:59, 3037.50it/s]138567it [00:58, 3005.15it/s]145340it [00:59, 3144.53it/s]138908it [00:58, 3119.59it/s]145658it [00:59, 3006.19it/s]139223it [00:58, 2978.20it/s]145997it [00:59, 3111.72it/s]139568it [00:58, 3109.15it/s]146341it [00:59, 3203.23it/s]139882it [00:58, 2980.07it/s]146664it [00:59, 3030.41it/s]140230it [00:59, 3119.78it/s]146999it [00:59, 3119.81it/s]140561it [00:59, 3172.83it/s]147333it [00:59, 2982.66it/s]140881it [00:59, 3013.86it/s]147683it [00:59, 3116.88it/s]141225it [00:59, 3132.70it/s]148032it [01:00, 3220.71it/s]141541it [00:59, 2969.72it/s]141861it [00:59, 3033.63it/s]148357it [01:00, 3003.41it/s]142204it [00:59, 3144.84it/s]148699it [01:00, 3118.25it/s]149015it [01:00, 2991.45it/s]142521it [00:59, 2988.32it/s]149354it [01:00, 3100.30it/s]142862it [00:59, 3104.75it/s]149703it [01:00, 3209.07it/s]143176it [01:00, 2960.53it/s]143508it [01:00, 3060.80it/s]150027it [01:00, 3046.89it/s]152578it [01:00, 243.32it/s] 143851it [01:00, 3165.88it/s]150362it [01:00, 3130.33it/s]152931it [01:00, 339.21it/s]144171it [01:00, 3011.51it/s]150693it [01:00, 2996.60it/s]153280it [01:01, 462.45it/s]144516it [01:00, 3133.00it/s]151039it [01:01, 3124.15it/s]153639it [01:01, 630.52it/s]151379it [01:01, 3201.83it/s]144833it [01:00, 2958.41it/s]154002it [01:01, 844.30it/s]145175it [01:00, 3084.91it/s]151702it [01:01, 2977.17it/s]154324it [01:01, 1057.05it/s]145523it [01:00, 3195.92it/s]152050it [01:01, 3116.27it/s]154658it [01:01, 1321.52it/s]145846it [01:00, 3043.76it/s]152373it [01:01, 2978.94it/s]154979it [01:01, 1573.43it/s]146179it [01:00, 3122.27it/s]155342it [01:01, 1916.85it/s]155702it [01:01, 2240.34it/s]146495it [01:01, 2963.24it/s]146843it [01:01, 3102.85it/s]156040it [01:01, 2420.85it/s]147189it [01:01, 3202.45it/s]156402it [01:02, 2696.13it/s]147513it [01:01, 3016.85it/s]156739it [01:02, 2786.58it/s]147855it [01:01, 3128.74it/s]157098it [01:02, 2992.40it/s]157462it [01:02, 3164.47it/s]148173it [01:01, 2973.97it/s]148516it [01:01, 3100.62it/s]157807it [01:02, 2987.91it/s]148844it [01:01, 3150.84it/s]158168it [01:02, 3153.77it/s]149162it [01:01, 2991.69it/s]158501it [01:02, 3110.04it/s]149506it [01:02, 3115.37it/s]158865it [01:02, 3255.82it/s]149847it [01:02, 3197.87it/s]159201it [01:02, 3202.28it/s]159566it [01:02, 3327.50it/s]150170it [01:02, 3025.40it/s]159928it [01:03, 3411.04it/s]150511it [01:02, 3132.82it/s]152485it [01:03, 213.44it/s] 160274it [01:03, 3285.06it/s]152830it [01:03, 302.09it/s]150828it [01:02, 2993.46it/s]160632it [01:03, 3366.77it/s]153175it [01:03, 420.62it/s]151159it [01:02, 3081.12it/s]160972it [01:03, 3262.83it/s]151501it [01:02, 3176.94it/s]153459it [01:03, 541.73it/s]161333it [01:03, 3361.70it/s]153770it [01:03, 715.41it/s]151822it [01:02, 2990.57it/s]161680it [01:03, 3219.76it/s]152169it [01:02, 3124.00it/s]154120it [01:03, 938.09it/s]162045it [01:03, 3341.00it/s]154468it [01:03, 1216.29it/s]162408it [01:03, 3423.45it/s]154810it [01:03, 1514.31it/s]162753it [01:03, 3314.48it/s]155125it [01:04, 1729.13it/s]163118it [01:04, 3409.06it/s]155468it [01:04, 2041.11it/s]163461it [01:04, 3301.25it/s]155800it [01:04, 2185.50it/s]163828it [01:04, 3405.65it/s]156146it [01:04, 2464.77it/s]164195it [01:04, 3482.10it/s]156489it [01:04, 2695.00it/s]164545it [01:04, 3350.65it/s]156811it [01:04, 2673.79it/s]164897it [01:04, 3398.39it/s]157153it [01:04, 2864.31it/s]165239it [01:04, 3297.57it/s]157480it [01:04, 2817.11it/s]165603it [01:04, 3394.39it/s]157819it [01:04, 2967.54it/s]165944it [01:04, 3310.82it/s]158166it [01:05, 3104.12it/s]166312it [01:04, 3415.28it/s]158489it [01:05, 2973.61it/s]166684it [01:05, 3502.99it/s]158835it [01:05, 3107.69it/s]167036it [01:05, 3381.66it/s]159160it [01:05, 2977.23it/s]167404it [01:05, 3465.50it/s]159511it [01:05, 3123.93it/s]167752it [01:05, 3306.49it/s]159846it [01:05, 3186.28it/s]168122it [01:05, 3417.83it/s]160169it [01:05, 2986.45it/s]168466it [01:05, 3335.87it/s]160511it [01:05, 3104.40it/s]168834it [01:05, 3432.18it/s]160840it [01:05, 2964.42it/s]169200it [01:05, 3497.22it/s]161190it [01:06, 3106.99it/s]169552it [01:05, 3359.32it/s]161525it [01:06, 3173.89it/s]169916it [01:06, 3439.06it/s]161846it [01:06, 3032.30it/s]170262it [01:06, 3333.83it/s]162193it [01:06, 3154.59it/s]170627it [01:06, 3421.88it/s]162520it [01:06, 3007.59it/s]170971it [01:06, 3271.11it/s]162870it [01:06, 3142.34it/s]171337it [01:06, 3380.10it/s]163221it [01:06, 3245.99it/s]171695it [01:06, 3435.94it/s]172041it [01:06, 3319.06it/s]163549it [01:06, 3015.25it/s]172409it [01:06, 3419.86it/s]163901it [01:06, 3152.54it/s]172753it [01:06, 3303.65it/s]164221it [01:07, 3003.81it/s]173130it [01:06, 3435.36it/s]164570it [01:07, 3137.60it/s]164929it [01:07, 3263.86it/s]173476it [01:07, 3315.51it/s]173827it [01:07, 3369.66it/s]165259it [01:07, 3070.33it/s]174200it [01:07, 3471.23it/s]165609it [01:07, 3189.48it/s]174549it [01:07, 3384.18it/s]165932it [01:07, 3057.75it/s]152675it [01:07, 176.13it/s] 174910it [01:07, 3439.19it/s]166288it [01:07, 3196.31it/s]153006it [01:07, 247.47it/s]175256it [01:07, 3349.19it/s]166643it [01:07, 3295.40it/s]153282it [01:07, 326.01it/s]175618it [01:07, 3425.70it/s]166976it [01:07, 3125.51it/s]153632it [01:07, 462.34it/s]175962it [01:07, 3306.58it/s]167315it [01:08, 3198.45it/s]153981it [01:07, 637.41it/s]176324it [01:07, 3395.46it/s]167638it [01:08, 3055.98it/s]154283it [01:07, 808.02it/s]176693it [01:08, 3479.72it/s]167994it [01:08, 3196.29it/s]154633it [01:07, 1069.11it/s]177043it [01:08, 3292.58it/s]168349it [01:08, 3296.94it/s]154963it [01:08, 1305.55it/s]177410it [01:08, 3399.31it/s]168682it [01:08, 3123.59it/s]155314it [01:08, 1624.44it/s]177753it [01:08, 3284.93it/s]169030it [01:08, 3216.60it/s]155663it [01:08, 1936.87it/s]178125it [01:08, 3408.20it/s]169355it [01:08, 3028.87it/s]155984it [01:08, 2105.97it/s]178480it [01:08, 3296.43it/s]169705it [01:08, 3158.73it/s]156336it [01:08, 2405.26it/s]178854it [01:08, 3420.08it/s]170054it [01:08, 3251.88it/s]156655it [01:08, 2479.14it/s]179217it [01:08, 3478.66it/s]170383it [01:08, 3085.75it/s]157007it [01:08, 2729.59it/s]179567it [01:08, 3357.23it/s]170732it [01:09, 3197.21it/s]157347it [01:08, 2901.33it/s]179911it [01:09, 3378.95it/s]171055it [01:09, 3012.15it/s]157672it [01:08, 2846.72it/s]180251it [01:09, 3304.51it/s]171404it [01:09, 3143.95it/s]158016it [01:09, 3004.99it/s]180613it [01:09, 3394.71it/s]171749it [01:09, 3229.18it/s]158336it [01:09, 2907.08it/s]180987it [01:09, 3493.69it/s]152485it [01:08, 183.27it/s] 172076it [01:09, 3070.86it/s]158676it [01:09, 3039.09it/s]181338it [01:09, 3353.64it/s]152831it [01:08, 259.63it/s]172431it [01:09, 3203.58it/s]159027it [01:09, 3169.75it/s]181700it [01:09, 3426.25it/s]153176it [01:08, 362.40it/s]172755it [01:09, 3044.89it/s]159353it [01:09, 3034.60it/s]182045it [01:09, 3330.17it/s]153462it [01:08, 470.56it/s]173104it [01:09, 3168.12it/s]159705it [01:09, 3168.89it/s]182408it [01:09, 3415.90it/s]153811it [01:09, 648.43it/s]173440it [01:09, 3013.35it/s]160028it [01:09, 2990.76it/s]182751it [01:09, 3320.95it/s]154122it [01:09, 825.96it/s]173790it [01:10, 3144.22it/s]160376it [01:09, 3125.68it/s]183100it [01:09, 3363.53it/s]154470it [01:09, 1086.81it/s]174140it [01:10, 3244.33it/s]160718it [01:09, 3207.49it/s]183472it [01:10, 3464.50it/s]154807it [01:09, 1368.10it/s]174468it [01:10, 3082.42it/s]161043it [01:10, 3038.92it/s]183820it [01:10, 3348.91it/s]155122it [01:09, 1595.95it/s]174810it [01:10, 3156.11it/s]161352it [01:10, 3012.05it/s]184192it [01:10, 3453.66it/s]155468it [01:09, 1917.69it/s]175129it [01:10, 3008.71it/s]184539it [01:10, 3356.23it/s]161682it [01:10, 2914.31it/s]155802it [01:09, 2094.73it/s]175475it [01:10, 3132.07it/s]184914it [01:10, 3468.30it/s]162033it [01:10, 3077.36it/s]156151it [01:09, 2390.44it/s]175824it [01:10, 3233.61it/s]162383it [01:10, 3195.68it/s]185263it [01:10, 3352.16it/s]156500it [01:09, 2645.04it/s]185621it [01:10, 3417.05it/s]176150it [01:10, 3055.10it/s]162706it [01:10, 3055.89it/s]156825it [01:10, 2658.93it/s]185976it [01:10, 3454.04it/s]176492it [01:10, 3156.13it/s]163053it [01:10, 3170.90it/s]157163it [01:10, 2840.58it/s]186323it [01:10, 3349.73it/s]176811it [01:11, 2991.31it/s]163373it [01:10, 3027.24it/s]157482it [01:10, 2807.86it/s]186706it [01:10, 3488.25it/s]177151it [01:11, 3104.77it/s]163725it [01:10, 3164.96it/s]157821it [01:10, 2960.83it/s]187057it [01:11, 3388.24it/s]177500it [01:11, 3206.85it/s]164050it [01:11, 3187.84it/s]158167it [01:10, 3097.62it/s]187431it [01:11, 3489.44it/s]177824it [01:11, 3057.02it/s]164372it [01:11, 3044.62it/s]158491it [01:10, 2955.84it/s]187782it [01:11, 3374.64it/s]178184it [01:11, 3207.92it/s]164727it [01:11, 3185.99it/s]158838it [01:10, 3095.94it/s]188155it [01:11, 3474.71it/s]165049it [01:11, 3045.96it/s]178508it [01:11, 3035.39it/s]159162it [01:10, 2970.44it/s]188525it [01:11, 3537.63it/s]165402it [01:11, 3180.16it/s]178854it [01:11, 3153.37it/s]159512it [01:10, 3108.83it/s]188881it [01:11, 3396.57it/s]165742it [01:11, 3235.90it/s]179198it [01:11, 3232.39it/s]159849it [01:11, 3181.77it/s]189227it [01:11, 3413.73it/s]166068it [01:11, 3093.33it/s]179525it [01:11, 3052.24it/s]160172it [01:11, 3022.93it/s]189570it [01:11, 3297.92it/s]166424it [01:11, 3223.79it/s]179878it [01:12, 3184.52it/s]160515it [01:11, 3135.67it/s]189943it [01:11, 3420.26it/s]166749it [01:11, 3088.47it/s]180200it [01:12, 3045.39it/s]190287it [01:12, 3341.03it/s]160841it [01:11, 2944.71it/s]167080it [01:11, 3150.06it/s]180521it [01:12, 3089.75it/s]190655it [01:12, 3438.06it/s]161190it [01:11, 3093.77it/s]167434it [01:12, 3260.01it/s]180878it [01:12, 3224.77it/s]191021it [01:12, 3500.92it/s]161535it [01:11, 3192.17it/s]167763it [01:12, 3114.61it/s]181203it [01:12, 3068.90it/s]191373it [01:12, 3378.90it/s]161859it [01:11, 3044.19it/s]168121it [01:12, 3246.51it/s]181547it [01:12, 3173.28it/s]191744it [01:12, 3472.80it/s]162208it [01:11, 3167.58it/s]168449it [01:12, 3099.92it/s]181868it [01:12, 3030.91it/s]192093it [01:12, 3338.51it/s]162529it [01:11, 3008.34it/s]168806it [01:12, 3230.90it/s]182228it [01:12, 3189.53it/s]192466it [01:12, 3448.37it/s]162878it [01:12, 3140.94it/s]169148it [01:12, 3284.68it/s]182582it [01:12, 3288.41it/s]192813it [01:12, 3350.53it/s]163227it [01:12, 3239.51it/s]169479it [01:12, 3109.03it/s]182914it [01:12, 3058.25it/s]193184it [01:12, 3451.83it/s]163554it [01:12, 3085.10it/s]169833it [01:12, 3230.26it/s]183271it [01:13, 3198.05it/s]193544it [01:12, 3493.74it/s]163893it [01:12, 3170.49it/s]170160it [01:12, 3082.01it/s]183596it [01:13, 3048.45it/s]193895it [01:13, 3383.80it/s]164213it [01:12, 3036.69it/s]170504it [01:13, 3180.19it/s]183949it [01:13, 3180.70it/s]194271it [01:13, 3491.08it/s]164564it [01:12, 3169.61it/s]170858it [01:13, 3280.66it/s]184304it [01:13, 3284.58it/s]194622it [01:13, 3387.28it/s]164921it [01:12, 3282.28it/s]171189it [01:13, 3104.68it/s]184636it [01:13, 3126.51it/s]194991it [01:13, 3474.14it/s]165252it [01:12, 3088.02it/s]171542it [01:13, 3222.93it/s]184996it [01:13, 3258.74it/s]195340it [01:13, 3361.21it/s]165605it [01:12, 3211.80it/s]171868it [01:13, 3033.79it/s]185326it [01:13, 3037.20it/s]195709it [01:13, 3454.87it/s]165930it [01:12, 3082.07it/s]172223it [01:13, 3174.52it/s]185675it [01:13, 3161.51it/s]196070it [01:13, 3498.02it/s]166287it [01:13, 3218.05it/s]172581it [01:13, 3288.69it/s]186034it [01:13, 3281.83it/s]196421it [01:13, 3380.26it/s]166633it [01:13, 3285.61it/s]172914it [01:13, 3127.08it/s]186367it [01:14, 3123.26it/s]196801it [01:13, 3500.85it/s]166965it [01:13, 3127.06it/s]173259it [01:13, 3216.29it/s]186740it [01:14, 3290.19it/s]197153it [01:14, 3381.04it/s]167320it [01:13, 3245.49it/s]173584it [01:14, 3062.04it/s]197529it [01:14, 3487.53it/s]187073it [01:14, 3130.69it/s]167648it [01:13, 3095.30it/s]173935it [01:14, 3186.37it/s]187428it [01:14, 3247.03it/s]197880it [01:14, 3389.54it/s]167983it [01:13, 3164.91it/s]198238it [01:14, 3442.03it/s]174283it [01:14, 3052.27it/s]187757it [01:14, 3026.25it/s]168343it [01:13, 3287.54it/s]198611it [01:14, 3523.22it/s]174638it [01:14, 3188.75it/s]188112it [01:14, 3169.56it/s]168675it [01:13, 3105.99it/s]174981it [01:14, 3255.15it/s]188456it [01:14, 3243.82it/s]169030it [01:13, 3230.24it/s]175310it [01:14, 3096.82it/s]188785it [01:14, 3082.73it/s]169357it [01:14, 3077.99it/s]175660it [01:14, 3208.22it/s]189130it [01:14, 3184.19it/s]169698it [01:14, 3170.25it/s]175984it [01:14, 3014.26it/s]189452it [01:15, 3044.78it/s]170048it [01:14, 3262.61it/s]176331it [01:14, 3137.96it/s]189784it [01:15, 3119.96it/s]170377it [01:14, 3098.67it/s]176685it [01:15, 3249.93it/s]190140it [01:15, 3243.10it/s]170723it [01:14, 3199.78it/s]177014it [01:15, 3059.99it/s]190467it [01:15, 3070.29it/s]171046it [01:14, 3016.73it/s]177353it [01:15, 3150.95it/s]190813it [01:15, 3177.80it/s]171398it [01:14, 3155.93it/s]177672it [01:15, 2987.84it/s]191134it [01:15, 3028.03it/s]171743it [01:14, 3238.68it/s]178031it [01:15, 3153.61it/s]191488it [01:15, 3168.88it/s]172070it [01:14, 3051.97it/s]178385it [01:15, 3260.64it/s]191843it [01:15, 3276.09it/s]172425it [01:15, 3190.89it/s]178715it [01:15, 3101.03it/s]192174it [01:15, 3068.64it/s]172748it [01:15, 3044.06it/s]179055it [01:15, 3183.86it/s]192536it [01:16, 3220.37it/s]173108it [01:15, 3198.23it/s]179377it [01:15, 3030.59it/s]192863it [01:16, 3068.62it/s]173442it [01:15, 3032.45it/s]179739it [01:15, 3193.61it/s]193219it [01:16, 3204.36it/s]173794it [01:15, 3166.58it/s]180090it [01:16, 3281.33it/s]193564it [01:16, 3272.41it/s]174150it [01:15, 3276.75it/s]180421it [01:16, 3114.91it/s]193895it [01:16, 3113.51it/s]174482it [01:15, 3117.46it/s]180768it [01:16, 3214.09it/s]194256it [01:16, 3253.10it/s]174824it [01:15, 3200.08it/s]181093it [01:16, 3066.28it/s]194585it [01:16, 3045.91it/s]175148it [01:15, 3051.42it/s]181438it [01:16, 3172.41it/s]194941it [01:16, 3186.63it/s]175496it [01:16, 3169.83it/s]181778it [01:16, 3235.33it/s]195280it [01:16, 3072.72it/s]175847it [01:16, 3265.79it/s]182104it [01:16, 3080.50it/s]195634it [01:17, 3201.01it/s]176177it [01:16, 3062.04it/s]182459it [01:16, 3212.39it/s]195981it [01:17, 3275.58it/s]176522it [01:16, 3169.33it/s]182784it [01:16, 3075.10it/s]196312it [01:17, 3099.28it/s]176843it [01:16, 3036.93it/s]183121it [01:17, 3157.07it/s]196680it [01:17, 3252.57it/s]177172it [01:16, 3105.48it/s]183471it [01:17, 3255.28it/s]197009it [01:17, 3043.28it/s]177523it [01:16, 3218.89it/s]183799it [01:17, 3095.18it/s]197372it [01:17, 3202.79it/s]177848it [01:16, 3053.58it/s]184152it [01:17, 3216.56it/s]197719it [01:17, 3276.47it/s]178203it [01:16, 3192.26it/s]184477it [01:17, 3003.32it/s]198051it [01:17, 3113.90it/s]178526it [01:16, 3009.57it/s]184838it [01:17, 3171.05it/s]198405it [01:17, 3231.57it/s]178880it [01:17, 3156.20it/s]185194it [01:17, 3279.17it/s]179230it [01:17, 3253.28it/s]185526it [01:17, 3094.65it/s]179559it [01:17, 3100.07it/s]185893it [01:17, 3253.02it/s]179901it [01:17, 3188.10it/s]186223it [01:18, 3056.45it/s]180223it [01:17, 3070.80it/s]186590it [01:18, 3224.08it/s]180576it [01:17, 3199.91it/s]186918it [01:18, 3120.71it/s]180935it [01:17, 3310.26it/s]187278it [01:18, 3254.46it/s]181269it [01:17, 3107.35it/s]187638it [01:18, 3350.36it/s]181614it [01:17, 3202.99it/s]187976it [01:18, 3174.59it/s]181938it [01:18, 3049.05it/s]188310it [01:18, 3220.71it/s]182301it [01:18, 3209.78it/s]188635it [01:18, 3089.55it/s]182651it [01:18, 3291.76it/s]188992it [01:18, 3222.20it/s]182984it [01:18, 3111.70it/s]189349it [01:19, 3319.21it/s]183343it [01:18, 3244.63it/s]189684it [01:19, 3131.61it/s]183671it [01:18, 3067.90it/s]190028it [01:19, 3216.07it/s]184027it [01:18, 3202.54it/s]190353it [01:19, 3092.15it/s]184362it [01:18, 3084.94it/s]190711it [01:19, 3229.23it/s]184720it [01:18, 3222.27it/s]191063it [01:19, 3311.53it/s]185072it [01:19, 3305.45it/s]191397it [01:19, 3147.80it/s]185406it [01:19, 3130.14it/s]191758it [01:19, 3276.96it/s]185764it [01:19, 3254.93it/s]192089it [01:19, 3085.80it/s]186093it [01:19, 3132.46it/s]192452it [01:19, 3236.99it/s]186451it [01:19, 3257.11it/s]192780it [01:20, 3124.53it/s]186831it [01:19, 3410.13it/s]193142it [01:20, 3263.51it/s]198965it [01:20, 196.89it/s] 187175it [01:19, 3215.30it/s]193502it [01:20, 3357.93it/s]199340it [01:20, 278.03it/s]187534it [01:19, 3318.37it/s]193841it [01:20, 3104.43it/s]199645it [01:20, 365.61it/s]187870it [01:19, 3142.21it/s]194201it [01:20, 3239.82it/s]200006it [01:20, 505.69it/s]188228it [01:20, 3263.72it/s]200374it [01:20, 690.13it/s]194530it [01:20, 3114.74it/s]188562it [01:20, 3137.75it/s]200704it [01:20, 885.31it/s]194892it [01:20, 3253.74it/s]188912it [01:20, 3230.51it/s]201076it [01:20, 1163.40it/s]195272it [01:20, 3406.84it/s]189269it [01:20, 3326.59it/s]201414it [01:21, 1418.94it/s]195617it [01:20, 3187.66it/s]189605it [01:20, 3155.43it/s]201788it [01:21, 1761.79it/s]195974it [01:21, 3291.72it/s]189965it [01:20, 3277.91it/s]202130it [01:21, 2010.58it/s]196308it [01:21, 3132.66it/s]190296it [01:20, 3106.18it/s]202512it [01:21, 2365.24it/s]196679it [01:21, 3291.36it/s]190630it [01:20, 3170.51it/s]202898it [01:21, 2692.10it/s]197013it [01:21, 3141.58it/s]190989it [01:20, 3289.76it/s]203257it [01:21, 2819.43it/s]197364it [01:21, 3242.63it/s]191321it [01:20, 3127.62it/s]203624it [01:21, 3029.01it/s]197715it [01:21, 3315.98it/s]191671it [01:21, 3230.40it/s]203977it [01:21, 3055.78it/s]198050it [01:21, 3190.17it/s]191997it [01:21, 3117.71it/s]204350it [01:21, 3234.71it/s]198414it [01:21, 3315.57it/s]192353it [01:21, 3240.52it/s]204701it [01:22, 3206.91it/s]192723it [01:21, 3372.38it/s]205062it [01:22, 3316.20it/s]193063it [01:21, 3174.73it/s]205429it [01:22, 3269.35it/s]193422it [01:21, 3289.89it/s]205805it [01:22, 3404.81it/s]206188it [01:22, 3525.48it/s]193755it [01:21, 3147.84it/s]194117it [01:21, 3278.07it/s]206547it [01:22, 3428.77it/s]206921it [01:22, 3515.06it/s]194448it [01:21, 3095.68it/s]207277it [01:22, 3425.72it/s]194799it [01:22, 3209.57it/s]207640it [01:22, 3483.26it/s]195180it [01:22, 3379.45it/s]207991it [01:22, 3396.27it/s]195522it [01:22, 3168.70it/s]208378it [01:23, 3531.99it/s]195883it [01:22, 3289.77it/s]208761it [01:23, 3617.19it/s]196217it [01:22, 3126.10it/s]209125it [01:23, 3499.22it/s]196585it [01:22, 3279.27it/s]209492it [01:23, 3547.75it/s]196944it [01:22, 3366.62it/s]209849it [01:23, 3403.61it/s]197285it [01:22, 3201.95it/s]210205it [01:23, 3446.26it/s]197646it [01:22, 3315.14it/s]210552it [01:23, 3345.10it/s]197981it [01:23, 3189.39it/s]210926it [01:23, 3456.50it/s]198326it [01:23, 3260.76it/s]211298it [01:23, 3531.31it/s]198655it [01:23, 3125.21it/s]211653it [01:24, 3411.14it/s]212031it [01:24, 3516.78it/s]212385it [01:24, 3405.17it/s]212763it [01:24, 3511.87it/s]213116it [01:24, 3368.27it/s]198732it [01:24, 162.91it/s] 213485it [01:24, 3458.39it/s]199099it [01:24, 233.30it/s]213833it [01:24, 3358.56it/s]199446it [01:24, 323.51it/s]214206it [01:24, 3462.59it/s]199740it [01:24, 422.69it/s]214575it [01:24, 3527.59it/s]200084it [01:25, 578.80it/s]214930it [01:24, 3390.37it/s]200390it [01:25, 742.84it/s]215305it [01:25, 3492.73it/s]200748it [01:25, 994.64it/s]215657it [01:25, 3339.74it/s]201105it [01:25, 1284.34it/s]216031it [01:25, 3451.18it/s]201432it [01:25, 1528.07it/s]216379it [01:25, 3382.62it/s]201783it [01:25, 1849.11it/s]216759it [01:25, 3500.60it/s]202108it [01:25, 2090.76it/s]217146it [01:25, 3607.62it/s]202485it [01:25, 2442.39it/s]217509it [01:25, 3484.89it/s]202858it [01:25, 2739.09it/s]217906it [01:25, 3621.68it/s]203206it [01:25, 2836.70it/s]218270it [01:25, 3469.83it/s]203558it [01:26, 3009.83it/s]218658it [01:26, 3583.92it/s]203899it [01:26, 2954.73it/s]219019it [01:26, 3467.03it/s]204260it [01:26, 3127.73it/s]219409it [01:26, 3589.26it/s]204595it [01:26, 3017.22it/s]219770it [01:26, 3495.78it/s]204956it [01:26, 3175.92it/s]220155it [01:26, 3596.49it/s]205315it [01:26, 3290.43it/s]220549it [01:26, 3526.23it/s]205654it [01:26, 3111.60it/s]220920it [01:26, 3577.35it/s]206013it [01:26, 3243.52it/s]221301it [01:26, 3643.66it/s]206345it [01:26, 3108.43it/s]221667it [01:26, 3560.97it/s]206704it [01:27, 3241.91it/s]222042it [01:26, 3614.16it/s]207067it [01:27, 3350.16it/s]222405it [01:27, 3452.12it/s]207407it [01:27, 3171.26it/s]222783it [01:27, 3543.57it/s]207757it [01:27, 3262.45it/s]223140it [01:27, 3415.36it/s]208088it [01:27, 3138.51it/s]223510it [01:27, 3495.45it/s]208455it [01:27, 3286.60it/s]223875it [01:27, 3539.12it/s]208789it [01:27, 3144.87it/s]224231it [01:27, 3414.71it/s]209159it [01:27, 3299.49it/s]224598it [01:27, 3486.10it/s]209511it [01:27, 3361.68it/s]224949it [01:27, 3361.08it/s]209850it [01:28, 3137.86it/s]225316it [01:27, 3448.85it/s]210199it [01:28, 3234.39it/s]225663it [01:28, 3345.58it/s]210527it [01:28, 3097.76it/s]226029it [01:28, 3433.52it/s]210885it [01:28, 3231.52it/s]226382it [01:28, 3460.98it/s]211243it [01:28, 3328.39it/s]226730it [01:28, 3339.95it/s]211579it [01:28, 3137.91it/s]227093it [01:28, 3423.18it/s]211939it [01:28, 3265.67it/s]227437it [01:28, 3304.66it/s]212270it [01:28, 3112.48it/s]227805it [01:28, 3411.61it/s]212627it [01:28, 3239.53it/s]228148it [01:28, 3304.56it/s]212982it [01:28, 3326.53it/s]228519it [01:28, 3419.97it/s]213318it [01:29, 3154.66it/s]228875it [01:28, 3460.16it/s]213644it [01:29, 3183.51it/s]229223it [01:29, 3333.77it/s]213965it [01:29, 3062.14it/s]229603it [01:29, 3465.26it/s]214319it [01:29, 3186.84it/s]229952it [01:29, 3353.64it/s]230320it [01:29, 3444.49it/s]214669it [01:29, 3042.22it/s]198749it [01:29, 148.85it/s] 215022it [01:29, 3174.56it/s]230667it [01:29, 3324.80it/s]199118it [01:29, 212.90it/s]215382it [01:29, 3294.24it/s]231045it [01:29, 3454.35it/s]199484it [01:29, 299.73it/s]231402it [01:29, 3486.57it/s]215715it [01:29, 3102.70it/s]199787it [01:29, 392.59it/s]231753it [01:29, 3366.09it/s]216071it [01:29, 3227.94it/s]200140it [01:29, 540.17it/s]232120it [01:29, 3451.39it/s]216398it [01:30, 3094.10it/s]200450it [01:29, 695.00it/s]232467it [01:30, 3338.00it/s]216756it [01:30, 3228.68it/s]200805it [01:29, 928.51it/s]232835it [01:30, 3434.26it/s]217123it [01:30, 3354.05it/s]201162it [01:30, 1203.69it/s]233181it [01:30, 3338.96it/s]217462it [01:30, 3156.77it/s]201489it [01:30, 1434.64it/s]233551it [01:30, 3440.44it/s]217827it [01:30, 3293.26it/s]201857it [01:30, 1778.05it/s]233921it [01:30, 3515.34it/s]218161it [01:30, 3179.45it/s]202185it [01:30, 1993.46it/s]234274it [01:30, 3360.59it/s]218522it [01:30, 3298.25it/s]202561it [01:30, 2345.84it/s]234636it [01:30, 3433.92it/s]218869it [01:30, 3176.54it/s]202913it [01:30, 2484.08it/s]234982it [01:30, 3326.85it/s]219240it [01:30, 3325.31it/s]203277it [01:30, 2750.94it/s]235350it [01:30, 3425.17it/s]219609it [01:31, 3429.00it/s]203635it [01:30, 2956.79it/s]235695it [01:30, 3299.01it/s]198971it [01:30, 152.56it/s] 219955it [01:31, 3249.30it/s]203976it [01:30, 2902.16it/s]236058it [01:31, 3391.62it/s]199326it [01:30, 217.75it/s]220330it [01:31, 3388.62it/s]204331it [01:31, 3065.44it/s]236419it [01:31, 3454.18it/s]199593it [01:30, 283.19it/s]220673it [01:31, 3232.71it/s]204662it [01:31, 2989.32it/s]199925it [01:30, 394.69it/s]236766it [01:31, 3302.69it/s]221038it [01:31, 3347.07it/s]205023it [01:31, 3156.91it/s]200265it [01:30, 544.41it/s]237132it [01:31, 3402.67it/s]221389it [01:31, 3197.50it/s]205386it [01:31, 3287.46it/s]200566it [01:30, 701.46it/s]237475it [01:31, 3295.80it/s]221767it [01:31, 3357.49it/s]205726it [01:31, 3138.17it/s]200904it [01:30, 931.64it/s]237837it [01:31, 3387.87it/s]222127it [01:31, 3425.69it/s]206078it [01:31, 3243.29it/s]238189it [01:31, 3275.90it/s]201231it [01:31, 1155.89it/s]222473it [01:31, 3226.65it/s]206410it [01:31, 3128.25it/s]238553it [01:31, 3376.94it/s]201592it [01:31, 1479.71it/s]222835it [01:32, 3335.84it/s]206765it [01:31, 3244.71it/s]238914it [01:31, 3441.43it/s]201953it [01:31, 1816.98it/s]223173it [01:32, 3161.28it/s]207113it [01:31, 3129.49it/s]239260it [01:32, 3277.73it/s]202280it [01:31, 2011.83it/s]223529it [01:32, 3271.43it/s]207463it [01:32, 3224.36it/s]239624it [01:32, 3379.68it/s]202645it [01:31, 2343.65it/s]223883it [01:32, 3345.10it/s]207826it [01:32, 3338.18it/s]239965it [01:32, 3276.72it/s]202972it [01:31, 2451.80it/s]224221it [01:32, 3149.74it/s]208163it [01:32, 3181.91it/s]240329it [01:32, 3379.05it/s]203329it [01:31, 2713.87it/s]224575it [01:32, 3254.96it/s]208530it [01:32, 3319.10it/s]240692it [01:32, 3450.07it/s]203662it [01:31, 2867.17it/s]224905it [01:32, 3105.64it/s]208866it [01:32, 3154.65it/s]241039it [01:32, 3320.40it/s]203990it [01:31, 2827.33it/s]225256it [01:32, 3216.70it/s]209231it [01:32, 3292.47it/s]241408it [01:32, 3425.17it/s]204325it [01:31, 2964.44it/s]209582it [01:32, 3353.78it/s]225589it [01:32, 3050.89it/s]241753it [01:32, 3288.68it/s]204644it [01:32, 2919.03it/s]225941it [01:33, 3178.93it/s]209921it [01:32, 3172.06it/s]242117it [01:32, 3387.10it/s]204984it [01:32, 3049.47it/s]226291it [01:33, 3269.02it/s]210263it [01:32, 3236.29it/s]242458it [01:32, 3297.33it/s]205338it [01:32, 3186.98it/s]226621it [01:33, 3109.83it/s]210590it [01:32, 3096.42it/s]242819it [01:33, 3385.56it/s]205666it [01:32, 3055.63it/s]226971it [01:33, 3218.20it/s]210950it [01:33, 3235.32it/s]243189it [01:33, 3474.50it/s]206008it [01:32, 3155.81it/s]211310it [01:33, 3336.89it/s]227296it [01:33, 3043.63it/s]243538it [01:33, 3353.51it/s]206330it [01:32, 3025.10it/s]227636it [01:33, 3142.65it/s]243908it [01:33, 3451.86it/s]211647it [01:33, 3124.05it/s]206674it [01:32, 3139.86it/s]227986it [01:33, 3242.20it/s]212010it [01:33, 3263.48it/s]244255it [01:33, 3328.28it/s]207035it [01:32, 3272.05it/s]228314it [01:33, 3087.39it/s]244604it [01:33, 3373.65it/s]212341it [01:33, 3118.42it/s]207366it [01:32, 3085.93it/s]228666it [01:33, 3208.77it/s]244943it [01:33, 3269.86it/s]212700it [01:33, 3248.20it/s]207723it [01:33, 3220.12it/s]245309it [01:33, 3378.83it/s]228990it [01:33, 3039.37it/s]213029it [01:33, 3074.25it/s]208049it [01:33, 3067.88it/s]245676it [01:33, 3462.45it/s]229349it [01:34, 3191.48it/s]213389it [01:33, 3217.85it/s]208407it [01:33, 3210.52it/s]229704it [01:34, 3293.22it/s]246024it [01:34, 3340.08it/s]213744it [01:33, 3311.14it/s]208771it [01:33, 3331.36it/s]246386it [01:34, 3419.36it/s]230037it [01:34, 3121.52it/s]214079it [01:34, 3153.36it/s]209108it [01:33, 3143.87it/s]246730it [01:34, 3313.74it/s]230387it [01:34, 3227.57it/s]214429it [01:34, 3239.36it/s]209469it [01:33, 3271.90it/s]247086it [01:34, 3382.69it/s]230714it [01:34, 3051.28it/s]214756it [01:34, 3075.28it/s]209800it [01:33, 3042.54it/s]247429it [01:34, 3286.73it/s]231073it [01:34, 3198.77it/s]215111it [01:34, 3206.88it/s]210151it [01:33, 3170.20it/s]247794it [01:34, 3390.56it/s]231418it [01:34, 3269.40it/s]215472it [01:34, 3319.32it/s]210473it [01:33, 3051.16it/s]248160it [01:34, 3467.25it/s]231748it [01:34, 3103.12it/s]215807it [01:34, 3146.50it/s]210818it [01:34, 3161.39it/s]248508it [01:34, 3345.16it/s]232099it [01:34, 3210.98it/s]216159it [01:34, 3250.67it/s]211155it [01:34, 3220.29it/s]248877it [01:34, 3443.77it/s]232424it [01:35, 3071.57it/s]216488it [01:34, 3117.45it/s]211480it [01:34, 3088.11it/s]249223it [01:34, 3340.33it/s]232778it [01:35, 3200.66it/s]216854it [01:34, 3269.59it/s]211828it [01:34, 3196.84it/s]249578it [01:35, 3398.54it/s]233138it [01:35, 3313.69it/s]217193it [01:35, 3139.75it/s]212152it [01:34, 3071.84it/s]249949it [01:35, 3311.54it/s]233473it [01:35, 3117.60it/s]217547it [01:35, 3251.72it/s]212495it [01:34, 3171.63it/s]250320it [01:35, 3424.09it/s]233830it [01:35, 3242.96it/s]217927it [01:35, 3407.65it/s]212852it [01:34, 3280.72it/s]250698it [01:35, 3524.55it/s]234158it [01:35, 3072.85it/s]218271it [01:35, 3232.01it/s]213183it [01:34, 3081.27it/s]251053it [01:35, 3397.47it/s]234512it [01:35, 3201.10it/s]218643it [01:35, 3367.76it/s]213537it [01:34, 3208.77it/s]251419it [01:35, 3462.27it/s]234836it [01:35, 3054.51it/s]218984it [01:35, 3197.63it/s]213862it [01:35, 3044.28it/s]251767it [01:35, 3343.51it/s]235191it [01:35, 3191.80it/s]219366it [01:35, 3370.31it/s]214220it [01:35, 3192.32it/s]252128it [01:35, 3411.93it/s]235527it [01:36, 3238.86it/s]219713it [01:35, 3214.38it/s]214568it [01:35, 3272.33it/s]252471it [01:35, 3285.53it/s]235854it [01:36, 3073.33it/s]220090it [01:35, 3368.61it/s]214899it [01:35, 3073.45it/s]252836it [01:36, 3388.40it/s]236209it [01:36, 3206.48it/s]220466it [01:36, 3479.37it/s]215256it [01:35, 3209.52it/s]253202it [01:36, 3465.06it/s]236533it [01:36, 3045.89it/s]220818it [01:36, 3310.10it/s]215581it [01:35, 3037.48it/s]253551it [01:36, 3341.80it/s]236885it [01:36, 3176.01it/s]221185it [01:36, 3410.62it/s]215932it [01:35, 3166.03it/s]253917it [01:36, 3431.26it/s]237231it [01:36, 3255.08it/s]221530it [01:36, 3279.23it/s]216277it [01:35, 3244.76it/s]254262it [01:36, 3311.09it/s]221885it [01:36, 3355.18it/s]237560it [01:36, 3060.27it/s]254629it [01:36, 3412.15it/s]216605it [01:35, 3092.16it/s]237911it [01:36, 3183.99it/s]222233it [01:36, 3177.06it/s]254984it [01:36, 3448.59it/s]216954it [01:35, 3203.49it/s]238234it [01:36, 3019.82it/s]222587it [01:36, 3277.15it/s]255331it [01:36, 3308.21it/s]217278it [01:36, 3068.70it/s]238583it [01:37, 3149.69it/s]222947it [01:36, 3367.36it/s]255698it [01:36, 3409.47it/s]217630it [01:36, 3194.68it/s]238933it [01:37, 3247.14it/s]223287it [01:36, 3196.96it/s]218002it [01:36, 3345.07it/s]223642it [01:36, 3294.54it/s]239261it [01:37, 3042.52it/s]218340it [01:36, 3141.34it/s]239614it [01:37, 3177.27it/s]223975it [01:37, 3142.96it/s]218719it [01:36, 3321.81it/s]224340it [01:37, 3284.15it/s]239936it [01:37, 3031.40it/s]219056it [01:36, 3177.14it/s]224696it [01:37, 3362.05it/s]240286it [01:37, 3159.89it/s]219415it [01:36, 3292.39it/s]240636it [01:37, 3255.81it/s]225035it [01:37, 3179.39it/s]219748it [01:36, 3153.11it/s]225375it [01:37, 3240.48it/s]240965it [01:37, 3038.71it/s]220104it [01:36, 3264.87it/s]241325it [01:37, 3192.11it/s]225702it [01:37, 3095.06it/s]220484it [01:37, 3416.69it/s]226055it [01:37, 3214.58it/s]241649it [01:37, 3042.18it/s]220829it [01:37, 3215.63it/s]226411it [01:37, 3311.80it/s]242006it [01:38, 3187.25it/s]221193it [01:37, 3334.05it/s]242363it [01:38, 3294.15it/s]226745it [01:37, 3125.73it/s]221531it [01:37, 3196.02it/s]227088it [01:38, 3210.40it/s]242696it [01:38, 3112.18it/s]221894it [01:37, 3316.22it/s]243037it [01:38, 3194.11it/s]227413it [01:38, 3050.56it/s]222232it [01:37, 3130.89it/s]227766it [01:38, 3183.86it/s]243360it [01:38, 3047.03it/s]222583it [01:37, 3234.03it/s]243717it [01:38, 3192.28it/s]228113it [01:38, 3048.53it/s]222933it [01:37, 3308.74it/s]228454it [01:38, 3146.61it/s]244069it [01:38, 3039.17it/s]223267it [01:37, 3104.88it/s]228807it [01:38, 3253.16it/s]244420it [01:38, 3166.81it/s]223626it [01:38, 3237.56it/s]244774it [01:38, 3270.69it/s]229136it [01:38, 3079.65it/s]223954it [01:38, 3082.69it/s]229503it [01:38, 3242.25it/s]245105it [01:39, 3088.54it/s]224311it [01:38, 3217.35it/s]245456it [01:39, 3204.54it/s]229831it [01:38, 3100.21it/s]224660it [01:38, 3294.49it/s]230177it [01:39, 3198.61it/s]245781it [01:39, 3052.33it/s]224993it [01:38, 3113.89it/s]230528it [01:39, 3286.34it/s]246130it [01:39, 3173.63it/s]225334it [01:38, 3195.79it/s]246479it [01:39, 3263.50it/s]230860it [01:39, 3125.22it/s]225657it [01:38, 3046.50it/s]231227it [01:39, 3278.58it/s]246809it [01:39, 3070.38it/s]225998it [01:38, 3147.28it/s]247162it [01:39, 3197.78it/s]231559it [01:39, 3116.09it/s]226334it [01:38, 3206.84it/s]231897it [01:39, 3189.24it/s]247486it [01:39, 3048.79it/s]226658it [01:39, 3048.23it/s]232250it [01:39, 3283.51it/s]247839it [01:39, 3182.64it/s]227008it [01:39, 3175.04it/s]248190it [01:40, 3274.12it/s]232581it [01:39, 3112.36it/s]227329it [01:39, 3026.84it/s]232937it [01:39, 3237.34it/s]248521it [01:40, 3098.48it/s]227677it [01:39, 3151.78it/s]233264it [01:40, 3100.91it/s]248863it [01:40, 3187.68it/s]228013it [01:39, 3209.47it/s]233605it [01:40, 3186.99it/s]249185it [01:40, 3025.63it/s]228337it [01:39, 3030.05it/s]233963it [01:40, 3297.70it/s]249536it [01:40, 3159.34it/s]228683it [01:39, 3148.20it/s]234296it [01:40, 3134.60it/s]249885it [01:40, 3253.28it/s]229001it [01:39, 2972.54it/s]234648it [01:40, 3242.57it/s]250214it [01:40, 3096.15it/s]229353it [01:39, 3124.42it/s]250559it [01:40, 3194.44it/s]234976it [01:40, 3053.39it/s]229695it [01:39, 3208.19it/s]235328it [01:40, 3182.67it/s]250882it [01:40, 3047.37it/s]230019it [01:40, 3058.69it/s]251238it [01:41, 3191.02it/s]235673it [01:40, 3046.15it/s]230362it [01:40, 3162.04it/s]251588it [01:41, 3276.92it/s]236029it [01:40, 3186.02it/s]230682it [01:40, 2998.76it/s]236384it [01:41, 3287.17it/s]251919it [01:41, 3091.69it/s]231038it [01:40, 3153.12it/s]252275it [01:41, 3221.75it/s]236717it [01:41, 3090.18it/s]231383it [01:40, 3235.27it/s]237073it [01:41, 3215.18it/s]252601it [01:41, 3029.56it/s]231710it [01:40, 3044.70it/s]252956it [01:41, 3173.52it/s]237399it [01:41, 3066.79it/s]232053it [01:40, 3151.34it/s]253307it [01:41, 3268.04it/s]237751it [01:41, 3191.02it/s]232372it [01:40, 2999.42it/s]238102it [01:41, 3280.98it/s]253638it [01:41, 3076.90it/s]232709it [01:40, 3100.86it/s]253983it [01:41, 3179.02it/s]238434it [01:41, 3053.56it/s]233063it [01:41, 3224.44it/s]238783it [01:41, 3173.33it/s]254305it [01:42, 2985.00it/s]233389it [01:41, 3067.60it/s]254658it [01:42, 3134.34it/s]239105it [01:41, 3022.18it/s]233734it [01:41, 3173.30it/s]239460it [01:41, 3167.38it/s]254989it [01:42, 3004.58it/s]234055it [01:41, 3029.52it/s]239818it [01:42, 3283.19it/s]255340it [01:42, 3142.47it/s]234400it [01:41, 3146.72it/s]255697it [01:42, 3261.50it/s]240150it [01:42, 3093.25it/s]234752it [01:41, 3252.85it/s]240503it [01:42, 3208.95it/s]235080it [01:41, 3068.44it/s]240828it [01:42, 3074.01it/s]235425it [01:41, 3175.33it/s]241188it [01:42, 3218.73it/s]235746it [01:41, 3010.94it/s]241533it [01:42, 3282.58it/s]236085it [01:42, 3114.51it/s]241864it [01:42, 3123.05it/s]236435it [01:42, 3222.17it/s]242218it [01:42, 3240.24it/s]236761it [01:42, 3053.18it/s]242545it [01:42, 3081.62it/s]237113it [01:42, 3183.58it/s]242885it [01:43, 3169.10it/s]237435it [01:42, 3008.72it/s]243233it [01:43, 3045.26it/s]237785it [01:42, 3143.57it/s]243592it [01:43, 3195.15it/s]238124it [01:42, 3212.08it/s]243953it [01:43, 3310.44it/s]238449it [01:42, 3061.92it/s]238802it [01:42, 3192.26it/s]244288it [01:43, 3109.87it/s]244645it [01:43, 3237.63it/s]239125it [01:43, 3027.03it/s]244973it [01:43, 3083.81it/s]239473it [01:43, 3151.21it/s]245327it [01:43, 3210.62it/s]239809it [01:43, 3208.36it/s]245673it [01:43, 3279.67it/s]240133it [01:43, 3069.95it/s]240487it [01:43, 3201.70it/s]246004it [01:44, 3115.93it/s]246357it [01:44, 3232.17it/s]240810it [01:43, 2971.87it/s]246684it [01:44, 3084.65it/s]241169it [01:43, 3140.63it/s]256041it [01:44, 152.25it/s] 247028it [01:44, 3182.55it/s]241525it [01:43, 3257.80it/s]256415it [01:44, 217.23it/s]247387it [01:44, 3297.34it/s]241855it [01:43, 3120.74it/s]256737it [01:44, 292.58it/s]247720it [01:44, 3123.60it/s]242212it [01:44, 3246.09it/s]257109it [01:44, 411.63it/s]248076it [01:44, 3244.21it/s]257474it [01:44, 564.70it/s]242540it [01:44, 3080.40it/s]248404it [01:44, 3059.49it/s]257803it [01:44, 733.62it/s]242889it [01:44, 3193.14it/s]248757it [01:44, 3187.78it/s]258177it [01:45, 981.58it/s]243232it [01:44, 3043.56it/s]258515it [01:45, 1218.56it/s]249113it [01:45, 3069.43it/s]243582it [01:44, 3167.55it/s]258891it [01:45, 1547.29it/s]249473it [01:45, 3213.31it/s]243942it [01:44, 3289.47it/s]249829it [01:45, 3308.76it/s]259257it [01:45, 1828.76it/s]244275it [01:44, 3110.82it/s]259628it [01:45, 2163.77it/s]250164it [01:45, 3122.90it/s]244628it [01:44, 3226.95it/s]260002it [01:45, 2483.74it/s]250530it [01:45, 3271.94it/s]244955it [01:44, 3058.00it/s]260355it [01:45, 2637.94it/s]250862it [01:45, 3131.91it/s]245309it [01:44, 3189.90it/s]260727it [01:45, 2890.54it/s]251219it [01:45, 3253.81it/s]245659it [01:45, 3277.24it/s]261077it [01:45, 2928.39it/s]251563it [01:45, 3305.58it/s]245990it [01:45, 3095.85it/s]261451it [01:45, 3137.15it/s]251897it [01:45, 3116.47it/s]246330it [01:45, 3179.49it/s]261798it [01:46, 3131.62it/s]252260it [01:46, 3259.56it/s]246652it [01:45, 3048.05it/s]262169it [01:46, 3288.47it/s]252590it [01:46, 3090.75it/s]247000it [01:45, 3168.51it/s]262538it [01:46, 3399.47it/s]252937it [01:46, 3195.35it/s]247356it [01:45, 3278.52it/s]262892it [01:46, 3308.12it/s]253295it [01:46, 3302.50it/s]247687it [01:45, 3091.62it/s]263256it [01:46, 3401.10it/s]253629it [01:46, 3133.63it/s]248043it [01:45, 3220.88it/s]263604it [01:46, 3255.86it/s]253986it [01:46, 3255.11it/s]248369it [01:45, 3053.79it/s]263975it [01:46, 3381.58it/s]254315it [01:46, 3071.37it/s]248725it [01:46, 3194.48it/s]264319it [01:46, 3303.37it/s]254674it [01:46, 3213.01it/s]249077it [01:46, 3285.00it/s]264687it [01:46, 3408.13it/s]255000it [01:46, 3077.92it/s]249409it [01:46, 3124.07it/s]265057it [01:47, 3490.54it/s]255346it [01:46, 3183.84it/s]249742it [01:46, 3176.79it/s]265409it [01:47, 3382.98it/s]255697it [01:47, 3274.54it/s]250063it [01:46, 3067.39it/s]265781it [01:47, 3477.66it/s]250428it [01:46, 3231.84it/s]266131it [01:47, 3369.64it/s]250781it [01:46, 3316.66it/s]266505it [01:47, 3473.23it/s]251115it [01:46, 3149.37it/s]266855it [01:47, 3347.31it/s]251456it [01:46, 3222.10it/s]267229it [01:47, 3457.11it/s]251781it [01:47, 3076.96it/s]267599it [01:47, 3527.15it/s]252145it [01:47, 3234.14it/s]267954it [01:47, 3405.54it/s]252472it [01:47, 3049.44it/s]268331it [01:47, 3507.62it/s]252827it [01:47, 3186.60it/s]268684it [01:48, 3395.30it/s]253181it [01:47, 3285.86it/s]269055it [01:48, 3483.44it/s]253513it [01:47, 3095.50it/s]269405it [01:48, 3340.14it/s]253868it [01:47, 3220.05it/s]269777it [01:48, 3446.52it/s]254194it [01:47, 3031.16it/s]270159it [01:48, 3552.14it/s]254552it [01:47, 3180.69it/s]270517it [01:48, 3423.15it/s]254888it [01:48, 3230.51it/s]270884it [01:48, 3492.26it/s]255215it [01:48, 3077.63it/s]271235it [01:48, 3366.66it/s]255560it [01:48, 3180.79it/s]271604it [01:48, 3456.24it/s]255882it [01:48, 3054.24it/s]271952it [01:49, 3316.63it/s]272323it [01:49, 3427.10it/s]272695it [01:49, 3509.24it/s]273048it [01:49, 3384.00it/s]273420it [01:49, 3477.88it/s]273770it [01:49, 3376.70it/s]274147it [01:49, 3487.74it/s]274498it [01:49, 3340.05it/s]274868it [01:49, 3439.61it/s]275218it [01:50, 3346.09it/s]275595it [01:50, 3466.58it/s]275969it [01:50, 3543.20it/s]276325it [01:50, 3414.65it/s]276694it [01:50, 3492.92it/s]277045it [01:50, 3387.84it/s]277400it [01:50, 3433.87it/s]277745it [01:50, 3340.25it/s]278120it [01:50, 3457.66it/s]278495it [01:50, 3541.58it/s]278851it [01:51, 3405.99it/s]279220it [01:51, 3487.26it/s]256027it [01:51, 123.86it/s] 279571it [01:51, 3368.78it/s]256386it [01:51, 176.89it/s]279936it [01:51, 3448.41it/s]256738it [01:51, 246.94it/s]280283it [01:51, 3287.86it/s]257097it [01:51, 345.50it/s]280650it [01:51, 3395.19it/s]257449it [01:51, 473.78it/s]281020it [01:51, 3482.22it/s]257763it [01:51, 614.29it/s]281371it [01:51, 3351.55it/s]258125it [01:52, 829.91it/s]281731it [01:51, 3422.22it/s]282076it [01:52, 3321.23it/s]258445it [01:52, 1034.74it/s]282439it [01:52, 3408.63it/s]258807it [01:52, 1333.86it/s]259170it [01:52, 1659.02it/s]282782it [01:52, 3269.86it/s]283149it [01:52, 3382.37it/s]259505it [01:52, 1872.86it/s]283513it [01:52, 3455.99it/s]259857it [01:52, 2182.19it/s]283861it [01:52, 3340.39it/s]260184it [01:52, 2332.22it/s]284229it [01:52, 3437.11it/s]260541it [01:52, 2611.10it/s]284575it [01:52, 3324.17it/s]260904it [01:52, 2859.82it/s]284944it [01:52, 3427.79it/s]261242it [01:53, 2837.76it/s]285298it [01:52, 3312.39it/s]261606it [01:53, 3043.65it/s]285654it [01:53, 3380.59it/s]261939it [01:53, 2975.90it/s]286023it [01:53, 3468.97it/s]262299it [01:53, 3142.78it/s]286372it [01:53, 3356.10it/s]262630it [01:53, 3011.23it/s]286744it [01:53, 3458.45it/s]262994it [01:53, 3181.50it/s]287092it [01:53, 3361.45it/s]287112it [01:53, 2529.81it/s]
263347it [01:53, 3277.71it/s]263683it [01:53, 3122.47it/s]264046it [01:53, 3263.14it/s]264378it [01:53, 3050.08it/s]264744it [01:54, 3215.20it/s]265109it [01:54, 3336.26it/s]265448it [01:54, 3166.15it/s]265817it [01:54, 3310.23it/s]266153it [01:54, 3149.85it/s]266516it [01:54, 3281.74it/s]266849it [01:54, 3080.42it/s]267213it [01:54, 3233.86it/s]267580it [01:54, 3355.22it/s]267920it [01:55, 3177.66it/s]268288it [01:55, 3307.29it/s]268623it [01:55, 3155.85it/s]268964it [01:55, 3225.70it/s]269326it [01:55, 3337.26it/s]269663it [01:55, 3164.56it/s]270026it [01:55, 3294.53it/s]270359it [01:55, 3129.64it/s]270721it [01:55, 3265.66it/s]271052it [01:56, 3089.49it/s]271415it [01:56, 3239.07it/s]271777it [01:56, 3346.73it/s]272116it [01:56, 3178.02it/s]272477it [01:56, 3297.08it/s]272811it [01:56, 3115.22it/s]273171it [01:56, 3249.33it/s]273528it [01:56, 3338.60it/s]256028it [01:56, 116.24it/s] 273866it [01:56, 3161.18it/s]256375it [01:56, 164.55it/s]274223it [01:57, 3274.91it/s]256736it [01:56, 234.02it/s]274554it [01:57, 3092.29it/s]257031it [01:56, 309.37it/s]274904it [01:57, 3203.11it/s]257379it [01:57, 431.03it/s]275228it [01:57, 3059.12it/s]257686it [01:57, 564.58it/s]275586it [01:57, 3201.96it/s]258033it [01:57, 764.17it/s]275939it [01:57, 3294.09it/s]258388it [01:57, 1013.12it/s]258713it [01:57, 1244.49it/s]276272it [01:57, 3103.35it/s]259081it [01:57, 1578.11it/s]276627it [01:57, 3227.39it/s]259410it [01:57, 1803.99it/s]276954it [01:57, 3071.30it/s]259761it [01:57, 2119.53it/s]277313it [01:58, 3215.74it/s]277668it [01:58, 3306.34it/s]260101it [01:57, 2292.89it/s]260461it [01:57, 2584.25it/s]278002it [01:58, 3117.80it/s]260825it [01:58, 2837.91it/s]278355it [01:58, 3232.74it/s]261163it [01:58, 2844.94it/s]278682it [01:58, 3086.33it/s]261515it [01:58, 3019.42it/s]279042it [01:58, 3228.35it/s]279387it [01:58, 3289.26it/s]261847it [01:58, 2969.81it/s]262205it [01:58, 3133.99it/s]279719it [01:58, 3117.08it/s]262566it [01:58, 3266.17it/s]280068it [01:58, 3220.52it/s]262905it [01:58, 3129.80it/s]280394it [01:58, 3072.33it/s]263255it [01:58, 3231.79it/s]280751it [01:59, 3210.65it/s]263586it [01:58, 3093.13it/s]281098it [01:59, 3051.44it/s]263951it [01:59, 3241.25it/s]281454it [01:59, 3191.04it/s]281808it [01:59, 3289.12it/s]264301it [01:59, 3124.56it/s]264642it [01:59, 3203.16it/s]282141it [01:59, 3121.00it/s]265001it [01:59, 3306.15it/s]282495it [01:59, 3237.62it/s]265335it [01:59, 3164.07it/s]282823it [01:59, 3043.10it/s]265701it [01:59, 3295.85it/s]283176it [01:59, 3175.83it/s]266034it [01:59, 3135.48it/s]283525it [01:59, 3264.31it/s]266401it [01:59, 3276.77it/s]283855it [02:00, 3089.53it/s]266766it [01:59, 3380.70it/s]284212it [02:00, 3223.04it/s]267107it [02:00, 3152.17it/s]284538it [02:00, 3052.89it/s]267473it [02:00, 3291.93it/s]284895it [02:00, 3196.50it/s]267807it [02:00, 3150.38it/s]285251it [02:00, 3297.47it/s]268177it [02:00, 3301.76it/s]285585it [02:00, 3129.64it/s]268511it [02:00, 3159.74it/s]285931it [02:00, 3219.67it/s]268860it [02:00, 3223.02it/s]286257it [02:00, 3077.97it/s]269219it [02:00, 3326.86it/s]286612it [02:00, 3208.00it/s]255882it [02:00, 3054.24it/s]255902it [02:00, 66.14it/s]  286972it [02:01, 3319.08it/s]269555it [02:00, 3163.23it/s]287112it [02:01, 2370.86it/s]
2022-07-06 14:56:00 | INFO | root | success load 287112 data
2022-07-06 14:56:00 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-uncased' is a path or url to a directory containing tokenizer files.
2022-07-06 14:56:00 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-uncased/added_tokens.json. We won't load it.
2022-07-06 14:56:00 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-uncased/special_tokens_map.json. We won't load it.
2022-07-06 14:56:00 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-uncased/vocab.txt
2022-07-06 14:56:00 | INFO | transformer.tokenization_utils | loading file None
2022-07-06 14:56:00 | INFO | transformer.tokenization_utils | loading file None
2022-07-06 14:56:00 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-uncased/tokenizer_config.json
256260it [02:00, 107.06it/s]269921it [02:00, 3301.15it/s]256626it [02:00, 165.24it/s]270255it [02:01, 3155.35it/s]256959it [02:00, 236.61it/s]270615it [02:01, 3279.55it/s]257312it [02:00, 340.34it/s]270973it [02:01, 3364.59it/s]257640it [02:00, 462.30it/s]271312it [02:01, 3125.32it/s]257998it [02:00, 641.27it/s]271670it [02:01, 3249.12it/s]258353it [02:00, 861.68it/s]272000it [02:01, 3112.37it/s]258687it [02:01, 1082.60it/s]272352it [02:01, 3223.50it/s]259044it [02:01, 1381.91it/s]272702it [02:01, 3076.70it/s]259373it [02:01, 1614.66it/s]273037it [02:01, 3148.86it/s]259733it [02:01, 1950.39it/s]273393it [02:01, 3264.01it/s]260079it [02:01, 2243.96it/s]273723it [02:02, 3112.08it/s]260412it [02:01, 2369.75it/s]274083it [02:02, 3246.63it/s]260768it [02:01, 2642.06it/s]274411it [02:02, 3093.21it/s]261096it [02:01, 2672.06it/s]274770it [02:02, 3230.68it/s]261456it [02:01, 2904.60it/s]275128it [02:02, 3328.31it/s]261782it [02:02, 2865.94it/s]275464it [02:02, 3113.50it/s]262138it [02:02, 3048.67it/s]275819it [02:02, 3232.64it/s]262481it [02:02, 3152.95it/s]276147it [02:02, 3085.29it/s]262812it [02:02, 3049.63it/s]276505it [02:02, 3220.69it/s]263139it [02:02, 3110.26it/s]276833it [02:03, 3236.94it/s]263461it [02:02, 2946.69it/s]277160it [02:03, 2973.22it/s]263813it [02:02, 3103.07it/s]277512it [02:03, 3121.36it/s]264172it [02:02, 3238.33it/s]277830it [02:03, 3011.48it/s]264502it [02:02, 3087.63it/s]278188it [02:03, 3167.52it/s]264861it [02:03, 3226.86it/s]278551it [02:03, 3297.56it/s]265189it [02:03, 3098.82it/s]278885it [02:03, 3139.10it/s]265555it [02:03, 3254.75it/s]279243it [02:03, 3261.54it/s]265908it [02:03, 3332.95it/s]279573it [02:03, 3060.64it/s]266245it [02:03, 3178.01it/s]279931it [02:04, 3201.92it/s]266601it [02:03, 3284.56it/s]280262it [02:04, 3051.55it/s]266933it [02:03, 3148.45it/s]280619it [02:04, 3193.68it/s]267290it [02:03, 3266.84it/s]280977it [02:04, 3302.10it/s]267656it [02:03, 3379.56it/s]281311it [02:04, 3077.00it/s]267997it [02:03, 3199.57it/s]281662it [02:04, 3191.07it/s]268358it [02:04, 3312.80it/s]281986it [02:04, 3056.49it/s]268693it [02:04, 3148.72it/s]282345it [02:04, 3204.74it/s]269049it [02:04, 3261.70it/s]282694it [02:04, 3283.59it/s]269379it [02:04, 3099.00it/s]283026it [02:05, 3103.60it/s]269742it [02:04, 3246.02it/s]283378it [02:05, 3219.86it/s]270100it [02:04, 3340.93it/s]283704it [02:05, 3017.21it/s]270438it [02:04, 3158.05it/s]284061it [02:05, 3168.63it/s]270800it [02:04, 3286.06it/s]284419it [02:05, 3284.75it/s]271133it [02:04, 3099.85it/s]284752it [02:05, 3115.87it/s]271494it [02:05, 3240.15it/s]285110it [02:05, 3245.25it/s]271846it [02:05, 3317.46it/s]285439it [02:05, 3052.03it/s]272181it [02:05, 3164.43it/s]285789it [02:05, 3174.79it/s]272522it [02:05, 3232.80it/s]286142it [02:06, 3024.63it/s]272849it [02:05, 3091.06it/s]286497it [02:06, 3167.05it/s]273197it [02:05, 3198.80it/s]286854it [02:06, 3279.24it/s]273541it [02:05, 3076.43it/s]287112it [02:06, 2272.41it/s]
273892it [02:05, 3195.84it/s]274258it [02:05, 3325.87it/s]274594it [02:06, 3151.37it/s]274953it [02:06, 3274.00it/s]275284it [02:06, 3128.33it/s]275645it [02:06, 3261.91it/s]275991it [02:06, 3310.05it/s]276325it [02:06, 3165.68it/s]276683it [02:06, 3282.40it/s]277014it [02:06, 3113.24it/s]277380it [02:06, 3264.42it/s]277733it [02:06, 3337.98it/s]278070it [02:07, 3185.76it/s]278418it [02:07, 3266.77it/s]278748it [02:07, 3132.10it/s]279099it [02:07, 3236.87it/s]279426it [02:07, 3093.10it/s]279777it [02:07, 3207.92it/s]280135it [02:07, 3311.99it/s]280469it [02:07, 3112.94it/s]280801it [02:07, 3168.91it/s]281121it [02:08, 2983.41it/s]281471it [02:08, 3125.23it/s]281830it [02:08, 3255.38it/s]282159it [02:08, 3057.28it/s]282510it [02:08, 3182.98it/s]282833it [02:08, 3021.33it/s]283191it [02:08, 3175.92it/s]283549it [02:08, 3289.00it/s]283882it [02:08, 3096.38it/s]284245it [02:09, 3244.01it/s]284574it [02:09, 3103.27it/s]284937it [02:09, 3249.41it/s]285285it [02:09, 3314.06it/s]285620it [02:09, 3155.47it/s]285979it [02:09, 3277.40it/s]286310it [02:09, 3090.51it/s]286660it [02:09, 3203.12it/s]286984it [02:09, 3086.56it/s]287112it [02:09, 2209.21it/s]
2022-07-06 14:56:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-06 15:01:10 | INFO | train_inner | epoch 001:    101 / 1122 loss=nan, nll_loss=11.706, mask_ins=7.449, word_ins_ml=12.222, word_reposition=3.804, kpe=nan, ppl=nan, wps=6878, ups=0.34, wpb=19977.7, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=28.384, clip=22, loss_scale=64, train_wall=271, wall=431
2022-07-06 15:06:03 | INFO | train_inner | epoch 001:    201 / 1122 loss=18.687, nll_loss=10.835, mask_ins=4.11, word_ins_ml=11.449, word_reposition=1.523, kpe=1.604, ppl=421914, wps=6818.9, ups=0.34, wpb=19958.5, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=18.788, clip=0, loss_scale=64, train_wall=255, wall=724
2022-07-06 15:10:55 | INFO | train_inner | epoch 001:    301 / 1122 loss=nan, nll_loss=10.492, mask_ins=2.209, word_ins_ml=11.134, word_reposition=1.451, kpe=nan, ppl=nan, wps=6824.7, ups=0.34, wpb=19927.8, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=4.881, clip=0, loss_scale=64, train_wall=255, wall=1016
2022-07-06 15:15:43 | INFO | train_inner | epoch 001:    401 / 1122 loss=15.162, nll_loss=9.793, mask_ins=1.979, word_ins_ml=10.525, word_reposition=1.524, kpe=1.133, ppl=36671.1, wps=6909.8, ups=0.35, wpb=19932.9, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=3.491, clip=0, loss_scale=64, train_wall=252, wall=1304
2022-07-06 15:20:28 | INFO | train_inner | epoch 001:    501 / 1122 loss=14.63, nll_loss=9.368, mask_ins=1.865, word_ins_ml=10.158, word_reposition=1.524, kpe=1.083, ppl=25363, wps=7028.2, ups=0.35, wpb=19990.7, bsz=256, num_updates=500, lr=5.009e-05, gnorm=3.268, clip=0, loss_scale=64, train_wall=248, wall=1589
2022-07-06 15:25:45 | INFO | train_inner | epoch 001:    601 / 1122 loss=14.308, nll_loss=9.084, mask_ins=1.851, word_ins_ml=9.914, word_reposition=1.489, kpe=1.054, ppl=20282.9, wps=6275.8, ups=0.32, wpb=19889.1, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=3.189, clip=0, loss_scale=121, train_wall=280, wall=1905
2022-07-06 15:30:36 | INFO | train_inner | epoch 001:    701 / 1122 loss=14.067, nll_loss=8.859, mask_ins=1.837, word_ins_ml=9.72, word_reposition=1.471, kpe=1.038, ppl=17162.1, wps=6834.2, ups=0.34, wpb=19945.9, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=3.051, clip=0, loss_scale=128, train_wall=253, wall=2197
2022-07-06 15:35:26 | INFO | train_inner | epoch 001:    801 / 1122 loss=13.828, nll_loss=8.678, mask_ins=1.781, word_ins_ml=9.564, word_reposition=1.462, kpe=1.022, ppl=14543.6, wps=6883.1, ups=0.35, wpb=19906.5, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=2.917, clip=0, loss_scale=128, train_wall=251, wall=2486
2022-07-06 15:40:11 | INFO | train_inner | epoch 001:    901 / 1122 loss=13.571, nll_loss=8.497, mask_ins=1.713, word_ins_ml=9.407, word_reposition=1.44, kpe=1.011, ppl=12171.7, wps=6954.8, ups=0.35, wpb=19834.4, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=2.646, clip=0, loss_scale=128, train_wall=247, wall=2772
2022-07-06 15:44:55 | INFO | train_inner | epoch 001:   1001 / 1122 loss=13.386, nll_loss=8.366, mask_ins=1.68, word_ins_ml=9.294, word_reposition=1.414, kpe=0.998, ppl=10701.4, wps=7010.2, ups=0.35, wpb=19947.6, bsz=256, num_updates=1000, lr=0.00010008, gnorm=2.492, clip=0, loss_scale=128, train_wall=246, wall=3056
2022-07-06 15:49:41 | INFO | train_inner | epoch 001:   1101 / 1122 loss=13.272, nll_loss=8.261, mask_ins=1.665, word_ins_ml=9.202, word_reposition=1.419, kpe=0.986, ppl=9889.45, wps=6956.9, ups=0.35, wpb=19888.8, bsz=256, num_updates=1100, lr=0.000110078, gnorm=2.437, clip=0, loss_scale=227, train_wall=247, wall=3342
2022-07-06 15:50:40 | INFO | train | epoch 001 | loss nan | nll_loss 9.426 | mask_ins 2.542 | word_ins_ml 10.216 | word_reposition 1.679 | kpe nan | ppl nan | wps 6846.1 | ups 0.34 | wpb 19912.7 | bsz 255.8 | num_updates 1121 | lr 0.000112178 | gnorm 6.792 | clip 2 | loss_scale 110 | train_wall 2856 | wall 3401
2022-07-06 15:51:50 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.304 | nll_loss 8.32 | mask_ins 1.922 | word_ins_ml 9.306 | word_reposition 1.655 | kpe 1.42 | ppl 20229.1 | wps 13729.5 | wpb 2279.4 | bsz 32 | num_updates 1121
2022-07-06 15:51:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_best.pt (epoch 1 @ 1121 updates, score 14.304) (writing took 7.8026152877137065 seconds)
2022-07-06 15:55:42 | INFO | train_inner | epoch 002:     79 / 1122 loss=13.125, nll_loss=8.141, mask_ins=1.651, word_ins_ml=9.099, word_reposition=1.407, kpe=0.969, ppl=8936.02, wps=5471, ups=0.28, wpb=19753.5, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=2.41, clip=0, loss_scale=256, train_wall=246, wall=3703
2022-07-06 16:00:28 | INFO | train_inner | epoch 002:    179 / 1122 loss=12.993, nll_loss=8.024, mask_ins=1.637, word_ins_ml=8.996, word_reposition=1.4, kpe=0.96, ppl=8151.14, wps=6986.9, ups=0.35, wpb=19967.6, bsz=256, num_updates=1300, lr=0.000130074, gnorm=2.232, clip=0, loss_scale=256, train_wall=248, wall=3989
2022-07-06 16:05:14 | INFO | train_inner | epoch 002:    279 / 1122 loss=12.862, nll_loss=7.935, mask_ins=1.621, word_ins_ml=8.918, word_reposition=1.368, kpe=0.955, ppl=7445.95, wps=6993.4, ups=0.35, wpb=19987.9, bsz=256, num_updates=1400, lr=0.000140072, gnorm=2.194, clip=0, loss_scale=256, train_wall=247, wall=4275
2022-07-06 16:10:00 | INFO | train_inner | epoch 002:    379 / 1122 loss=12.765, nll_loss=7.848, mask_ins=1.607, word_ins_ml=8.843, word_reposition=1.355, kpe=0.959, ppl=6960.88, wps=6909, ups=0.35, wpb=19739.3, bsz=256, num_updates=1500, lr=0.00015007, gnorm=2.115, clip=0, loss_scale=256, train_wall=247, wall=4561
2022-07-06 16:14:45 | INFO | train_inner | epoch 002:    479 / 1122 loss=12.676, nll_loss=7.76, mask_ins=1.6, word_ins_ml=8.767, word_reposition=1.359, kpe=0.951, ppl=6544.54, wps=6981.8, ups=0.35, wpb=19938.2, bsz=256, num_updates=1600, lr=0.000160068, gnorm=2.111, clip=0, loss_scale=422, train_wall=247, wall=4846
2022-07-06 16:19:31 | INFO | train_inner | epoch 002:    579 / 1122 loss=12.57, nll_loss=7.661, mask_ins=1.583, word_ins_ml=8.68, word_reposition=1.359, kpe=0.948, ppl=6081.98, wps=6981, ups=0.35, wpb=19940.1, bsz=256, num_updates=1700, lr=0.000170066, gnorm=2.075, clip=0, loss_scale=512, train_wall=247, wall=5132
2022-07-06 16:24:17 | INFO | train_inner | epoch 002:    679 / 1122 loss=nan, nll_loss=7.588, mask_ins=1.595, word_ins_ml=8.616, word_reposition=1.36, kpe=nan, ppl=nan, wps=6950.8, ups=0.35, wpb=19889.9, bsz=256, num_updates=1800, lr=0.000180064, gnorm=1.964, clip=0, loss_scale=512, train_wall=248, wall=5418
2022-07-06 16:29:47 | INFO | train_inner | epoch 002:    779 / 1122 loss=nan, nll_loss=7.481, mask_ins=1.574, word_ins_ml=8.524, word_reposition=1.349, kpe=nan, ppl=nan, wps=6069, ups=0.3, wpb=20000.7, bsz=256, num_updates=1900, lr=0.000190062, gnorm=2.054, clip=0, loss_scale=512, train_wall=291, wall=5747
2022-07-06 16:34:33 | INFO | train_inner | epoch 002:    879 / 1122 loss=12.236, nll_loss=7.345, mask_ins=1.57, word_ins_ml=8.405, word_reposition=1.323, kpe=0.938, ppl=4823.54, wps=6923.5, ups=0.35, wpb=19821.2, bsz=256, num_updates=2000, lr=0.00020006, gnorm=2.04, clip=0, loss_scale=512, train_wall=247, wall=6034
2022-07-06 16:39:19 | INFO | train_inner | epoch 002:    979 / 1122 loss=12.045, nll_loss=7.145, mask_ins=1.555, word_ins_ml=8.229, word_reposition=1.32, kpe=0.94, ppl=4224.83, wps=6968.6, ups=0.35, wpb=19924.8, bsz=256, num_updates=2100, lr=0.000210058, gnorm=2.136, clip=0, loss_scale=783, train_wall=247, wall=6320
2022-07-06 16:44:05 | INFO | train_inner | epoch 002:   1079 / 1122 loss=11.814, nll_loss=6.828, mask_ins=1.575, word_ins_ml=7.951, word_reposition=1.34, kpe=0.947, ppl=3601.27, wps=7030.7, ups=0.35, wpb=20105.8, bsz=256, num_updates=2200, lr=0.000220056, gnorm=2.432, clip=0, loss_scale=1024, train_wall=247, wall=6606
2022-07-06 16:46:06 | INFO | train | epoch 002 | loss nan | nll_loss 7.558 | mask_ins 1.594 | word_ins_ml 8.59 | word_reposition 1.354 | kpe nan | ppl nan | wps 6717.1 | ups 0.34 | wpb 19913.7 | bsz 255.8 | num_updates 2243 | lr 0.000224355 | gnorm 2.176 | clip 0 | loss_scale 507 | train_wall 2817 | wall 6727
2022-07-06 16:47:16 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.682 | nll_loss 7.144 | mask_ins 1.675 | word_ins_ml 8.307 | word_reposition 1.327 | kpe 1.374 | ppl 6573.7 | wps 13644.2 | wpb 2279.4 | bsz 32 | num_updates 2243 | best_loss 12.682
2022-07-06 16:47:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_best.pt (epoch 2 @ 2243 updates, score 12.682) (writing took 11.010399437509477 seconds)
2022-07-06 16:50:10 | INFO | train_inner | epoch 003:     57 / 1122 loss=nan, nll_loss=6.246, mask_ins=1.559, word_ins_ml=7.443, word_reposition=1.277, kpe=nan, ppl=nan, wps=5408.9, ups=0.27, wpb=19756.3, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=2.919, clip=0, loss_scale=1024, train_wall=246, wall=6971
2022-07-06 16:54:55 | INFO | train_inner | epoch 003:    157 / 1122 loss=10.63, nll_loss=5.62, mask_ins=1.558, word_ins_ml=6.9, word_reposition=1.246, kpe=0.926, ppl=1584.43, wps=6966.4, ups=0.35, wpb=19863.1, bsz=256, num_updates=2400, lr=0.000240052, gnorm=3.021, clip=0, loss_scale=1024, train_wall=246, wall=7256
2022-07-06 16:59:41 | INFO | train_inner | epoch 003:    257 / 1122 loss=10.105, nll_loss=5.117, mask_ins=1.537, word_ins_ml=6.466, word_reposition=1.17, kpe=0.932, ppl=1101.03, wps=7003.2, ups=0.35, wpb=20001.6, bsz=256, num_updates=2500, lr=0.00025005, gnorm=3.228, clip=0, loss_scale=1024, train_wall=247, wall=7542
2022-07-06 17:04:26 | INFO | train_inner | epoch 003:    357 / 1122 loss=9.562, nll_loss=4.537, mask_ins=1.542, word_ins_ml=5.962, word_reposition=1.124, kpe=0.935, ppl=756.07, wps=6988.8, ups=0.35, wpb=19958.4, bsz=256, num_updates=2600, lr=0.000260048, gnorm=3.402, clip=0, loss_scale=1444, train_wall=247, wall=7827
2022-07-06 17:09:11 | INFO | train_inner | epoch 003:    457 / 1122 loss=9.095, nll_loss=4.131, mask_ins=1.479, word_ins_ml=5.609, word_reposition=1.064, kpe=0.943, ppl=546.76, wps=6942.7, ups=0.35, wpb=19767.7, bsz=256, num_updates=2700, lr=0.000270046, gnorm=3.447, clip=0, loss_scale=2048, train_wall=246, wall=8112
2022-07-06 17:13:57 | INFO | train_inner | epoch 003:    557 / 1122 loss=8.726, nll_loss=3.852, mask_ins=1.382, word_ins_ml=5.365, word_reposition=1.04, kpe=0.939, ppl=423.48, wps=6966.5, ups=0.35, wpb=19887.7, bsz=256, num_updates=2800, lr=0.000280044, gnorm=3.358, clip=0, loss_scale=2048, train_wall=247, wall=8397
2022-07-06 17:18:42 | INFO | train_inner | epoch 003:    657 / 1122 loss=8.482, nll_loss=3.689, mask_ins=1.3, word_ins_ml=5.222, word_reposition=1.019, kpe=0.941, ppl=357.48, wps=7007.1, ups=0.35, wpb=19967.1, bsz=256, num_updates=2900, lr=0.000290042, gnorm=3.339, clip=0, loss_scale=2048, train_wall=246, wall=8682
2022-07-06 17:23:27 | INFO | train_inner | epoch 003:    757 / 1122 loss=8.262, nll_loss=3.53, mask_ins=1.235, word_ins_ml=5.082, word_reposition=1, kpe=0.946, ppl=307.06, wps=7012.4, ups=0.35, wpb=20000.9, bsz=256, num_updates=3000, lr=0.00030004, gnorm=3.239, clip=0, loss_scale=2048, train_wall=246, wall=8968
2022-07-06 17:28:11 | INFO | train_inner | epoch 003:    857 / 1122 loss=nan, nll_loss=3.434, mask_ins=1.187, word_ins_ml=4.997, word_reposition=0.976, kpe=nan, ppl=nan, wps=7028.3, ups=0.35, wpb=19974.3, bsz=256, num_updates=3100, lr=0.000310038, gnorm=3.19, clip=0, loss_scale=2642, train_wall=246, wall=9252
2022-07-06 17:32:57 | INFO | train_inner | epoch 003:    957 / 1122 loss=8.005, nll_loss=3.38, mask_ins=1.152, word_ins_ml=4.947, word_reposition=0.967, kpe=0.94, ppl=256.97, wps=6985.1, ups=0.35, wpb=19944.3, bsz=256, num_updates=3200, lr=0.000320036, gnorm=3.158, clip=0, loss_scale=4096, train_wall=247, wall=9537
2022-07-06 17:38:35 | INFO | train_inner | epoch 003:   1057 / 1122 loss=7.975, nll_loss=3.357, mask_ins=1.145, word_ins_ml=4.926, word_reposition=0.964, kpe=0.939, ppl=251.56, wps=5887.3, ups=0.3, wpb=19933.5, bsz=256, num_updates=3300, lr=0.000330034, gnorm=3.092, clip=0, loss_scale=4096, train_wall=299, wall=9876
2022-07-06 17:41:39 | INFO | train | epoch 003 | loss nan | nll_loss 4.123 | mask_ins 1.348 | word_ins_ml 5.598 | word_reposition 1.062 | kpe nan | ppl nan | wps 6704.9 | ups 0.34 | wpb 19914.1 | bsz 255.8 | num_updates 3365 | lr 0.000336533 | gnorm 3.237 | clip 0 | loss_scale 2296 | train_wall 2816 | wall 10060
2022-07-06 17:42:49 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.483 | nll_loss 6.579 | mask_ins 1.65 | word_ins_ml 7.938 | word_reposition 1.504 | kpe 1.391 | ppl 5724.24 | wps 13696.7 | wpb 2279.4 | bsz 32 | num_updates 3365 | best_loss 12.483
2022-07-06 17:42:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_best.pt (epoch 3 @ 3365 updates, score 12.483) (writing took 10.934282182715833 seconds)
2022-07-06 17:44:39 | INFO | train_inner | epoch 004:     35 / 1122 loss=7.845, nll_loss=3.26, mask_ins=1.11, word_ins_ml=4.839, word_reposition=0.959, kpe=0.938, ppl=230, wps=5432, ups=0.27, wpb=19768.2, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=3.341, clip=0, loss_scale=4096, train_wall=245, wall=10240
2022-07-06 17:49:25 | INFO | train_inner | epoch 004:    135 / 1122 loss=7.694, nll_loss=3.166, mask_ins=1.081, word_ins_ml=4.755, word_reposition=0.942, kpe=0.917, ppl=207.08, wps=6984.9, ups=0.35, wpb=19942.7, bsz=256, num_updates=3500, lr=0.00035003, gnorm=3.145, clip=0, loss_scale=4096, train_wall=247, wall=10525
2022-07-06 17:54:10 | INFO | train_inner | epoch 004:    235 / 1122 loss=7.588, nll_loss=3.085, mask_ins=1.054, word_ins_ml=4.681, word_reposition=0.933, kpe=0.92, ppl=192.38, wps=7017.8, ups=0.35, wpb=20036.6, bsz=256, num_updates=3600, lr=0.000360028, gnorm=3.183, clip=0, loss_scale=4792, train_wall=247, wall=10811
2022-07-06 17:55:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-06 17:58:58 | INFO | train_inner | epoch 004:    336 / 1122 loss=7.571, nll_loss=3.063, mask_ins=1.056, word_ins_ml=4.66, word_reposition=0.929, kpe=0.927, ppl=190.2, wps=6872.8, ups=0.35, wpb=19802.1, bsz=256, num_updates=3700, lr=0.000370026, gnorm=3.237, clip=0, loss_scale=4948, train_wall=249, wall=11099
2022-07-06 18:03:44 | INFO | train_inner | epoch 004:    436 / 1122 loss=7.477, nll_loss=3.004, mask_ins=1.029, word_ins_ml=4.607, word_reposition=0.916, kpe=0.925, ppl=178.1, wps=7002.1, ups=0.35, wpb=19996.5, bsz=256, num_updates=3800, lr=0.000380024, gnorm=3.195, clip=0, loss_scale=4096, train_wall=247, wall=11385
2022-07-06 18:08:29 | INFO | train_inner | epoch 004:    536 / 1122 loss=7.414, nll_loss=2.932, mask_ins=1.014, word_ins_ml=4.542, word_reposition=0.922, kpe=0.935, ppl=170.56, wps=6978, ups=0.35, wpb=19874.3, bsz=256, num_updates=3900, lr=0.000390022, gnorm=3.205, clip=0, loss_scale=4096, train_wall=246, wall=11669
2022-07-06 18:13:14 | INFO | train_inner | epoch 004:    636 / 1122 loss=nan, nll_loss=2.937, mask_ins=1.02, word_ins_ml=4.545, word_reposition=0.908, kpe=nan, ppl=nan, wps=6974.1, ups=0.35, wpb=19889.4, bsz=256, num_updates=4000, lr=0.00040002, gnorm=3.278, clip=0, loss_scale=4096, train_wall=246, wall=11955
2022-07-06 18:17:59 | INFO | train_inner | epoch 004:    736 / 1122 loss=7.371, nll_loss=2.909, mask_ins=0.999, word_ins_ml=4.52, word_reposition=0.912, kpe=0.94, ppl=165.57, wps=7052, ups=0.35, wpb=20099, bsz=256, num_updates=4100, lr=0.000410018, gnorm=3.098, clip=0, loss_scale=4096, train_wall=246, wall=12240
2022-07-06 18:22:44 | INFO | train_inner | epoch 004:    836 / 1122 loss=nan, nll_loss=2.835, mask_ins=0.981, word_ins_ml=4.453, word_reposition=0.902, kpe=nan, ppl=nan, wps=6956.2, ups=0.35, wpb=19827.5, bsz=256, num_updates=4200, lr=0.000420016, gnorm=3.144, clip=0, loss_scale=6881, train_wall=246, wall=12525
2022-07-06 18:23:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-06 18:27:32 | INFO | train_inner | epoch 004:    937 / 1122 loss=7.335, nll_loss=2.868, mask_ins=0.991, word_ins_ml=4.481, word_reposition=0.915, kpe=0.949, ppl=161.47, wps=6931.7, ups=0.35, wpb=20002.5, bsz=256, num_updates=4300, lr=0.000430014, gnorm=3.177, clip=0, loss_scale=4948, train_wall=249, wall=12813
2022-07-06 18:32:18 | INFO | train_inner | epoch 004:   1037 / 1122 loss=7.346, nll_loss=2.892, mask_ins=0.99, word_ins_ml=4.501, word_reposition=0.901, kpe=0.954, ppl=162.67, wps=6943.4, ups=0.35, wpb=19822.9, bsz=256, num_updates=4400, lr=0.000440012, gnorm=3.209, clip=0, loss_scale=4096, train_wall=247, wall=13099
2022-07-06 18:36:19 | INFO | train | epoch 004 | loss nan | nll_loss 2.97 | mask_ins 1.021 | word_ins_ml 4.575 | word_reposition 0.918 | kpe nan | ppl nan | wps 6799.1 | ups 0.34 | wpb 19911.9 | bsz 255.8 | num_updates 4485 | lr 0.00044851 | gnorm 3.212 | clip 0 | loss_scale 4560 | train_wall 2766 | wall 13340
2022-07-06 18:37:29 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 13.811 | nll_loss 7.06 | mask_ins 1.978 | word_ins_ml 8.376 | word_reposition 1.72 | kpe 1.738 | ppl 14376.6 | wps 13683.3 | wpb 2279.4 | bsz 32 | num_updates 4485 | best_loss 12.483
2022-07-06 18:37:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 4 @ 4485 updates, score 13.811) (writing took 6.291195767000318 seconds)
2022-07-06 18:37:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 18:38:20 | INFO | train_inner | epoch 005:     16 / 1122 loss=7.327, nll_loss=2.881, mask_ins=0.973, word_ins_ml=4.49, word_reposition=0.898, kpe=0.965, ppl=160.52, wps=5440.6, ups=0.28, wpb=19725.1, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=3.542, clip=0, loss_scale=3772, train_wall=248, wall=13461
2022-07-06 18:43:41 | INFO | train_inner | epoch 005:    116 / 1122 loss=7.26, nll_loss=2.836, mask_ins=0.971, word_ins_ml=4.451, word_reposition=0.898, kpe=0.94, ppl=153.25, wps=6238.8, ups=0.31, wpb=20002, bsz=256, num_updates=4600, lr=0.000460008, gnorm=3.315, clip=0, loss_scale=2048, train_wall=282, wall=13782
2022-07-06 18:48:26 | INFO | train_inner | epoch 005:    216 / 1122 loss=7.201, nll_loss=2.791, mask_ins=0.965, word_ins_ml=4.408, word_reposition=0.891, kpe=0.936, ppl=147.15, wps=6992.3, ups=0.35, wpb=19936, bsz=256, num_updates=4700, lr=0.000470006, gnorm=3.219, clip=0, loss_scale=2048, train_wall=247, wall=14067
2022-07-06 18:50:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-06 18:53:15 | INFO | train_inner | epoch 005:    317 / 1122 loss=7.203, nll_loss=2.783, mask_ins=0.966, word_ins_ml=4.401, word_reposition=0.892, kpe=0.943, ppl=147.31, wps=6932.1, ups=0.35, wpb=19995.9, bsz=256, num_updates=4800, lr=0.000480004, gnorm=3.218, clip=0, loss_scale=1409, train_wall=250, wall=14355
2022-07-06 18:57:59 | INFO | train_inner | epoch 005:    417 / 1122 loss=7.255, nll_loss=2.833, mask_ins=0.959, word_ins_ml=4.444, word_reposition=0.892, kpe=0.959, ppl=152.73, wps=6978.1, ups=0.35, wpb=19872.9, bsz=256, num_updates=4900, lr=0.000490002, gnorm=3.629, clip=0, loss_scale=1024, train_wall=247, wall=14640
2022-07-06 19:02:44 | INFO | train_inner | epoch 005:    517 / 1122 loss=nan, nll_loss=2.786, mask_ins=0.964, word_ins_ml=4.402, word_reposition=0.887, kpe=nan, ppl=nan, wps=6942.1, ups=0.35, wpb=19774.3, bsz=256, num_updates=5000, lr=0.0005, gnorm=3.191, clip=0, loss_scale=1024, train_wall=247, wall=14925
2022-07-06 19:07:29 | INFO | train_inner | epoch 005:    617 / 1122 loss=7.226, nll_loss=2.816, mask_ins=0.954, word_ins_ml=4.427, word_reposition=0.891, kpe=0.955, ppl=149.74, wps=6977.7, ups=0.35, wpb=19878.6, bsz=256, num_updates=5100, lr=0.000495074, gnorm=3.409, clip=0, loss_scale=1024, train_wall=246, wall=15210
2022-07-06 19:12:14 | INFO | train_inner | epoch 005:    717 / 1122 loss=7.199, nll_loss=2.786, mask_ins=0.956, word_ins_ml=4.4, word_reposition=0.886, kpe=0.958, ppl=146.96, wps=6985.9, ups=0.35, wpb=19904.7, bsz=256, num_updates=5200, lr=0.00049029, gnorm=3.282, clip=0, loss_scale=1024, train_wall=246, wall=15495
2022-07-06 19:16:59 | INFO | train_inner | epoch 005:    817 / 1122 loss=7.162, nll_loss=2.763, mask_ins=0.939, word_ins_ml=4.377, word_reposition=0.889, kpe=0.957, ppl=143.22, wps=7033.2, ups=0.35, wpb=20018.7, bsz=256, num_updates=5300, lr=0.000485643, gnorm=3.211, clip=0, loss_scale=1546, train_wall=246, wall=15780
2022-07-06 19:18:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-06 19:21:46 | INFO | train_inner | epoch 005:    918 / 1122 loss=7.05, nll_loss=2.652, mask_ins=0.938, word_ins_ml=4.277, word_reposition=0.878, kpe=0.957, ppl=132.5, wps=6940.3, ups=0.35, wpb=19947, bsz=256, num_updates=5400, lr=0.000481125, gnorm=3.148, clip=0, loss_scale=1389, train_wall=249, wall=16067
2022-07-06 19:26:31 | INFO | train_inner | epoch 005:   1018 / 1122 loss=nan, nll_loss=2.689, mask_ins=0.924, word_ins_ml=4.31, word_reposition=0.872, kpe=nan, ppl=nan, wps=6959.4, ups=0.35, wpb=19826.1, bsz=256, num_updates=5500, lr=0.000476731, gnorm=3.188, clip=0, loss_scale=1024, train_wall=247, wall=16352
2022-07-06 19:31:17 | INFO | train_inner | epoch 005:   1118 / 1122 loss=7.08, nll_loss=2.682, mask_ins=0.935, word_ins_ml=4.302, word_reposition=0.885, kpe=0.958, ppl=135.34, wps=7016.1, ups=0.35, wpb=20030.2, bsz=256, num_updates=5600, lr=0.000472456, gnorm=3.184, clip=0, loss_scale=1024, train_wall=247, wall=16637
2022-07-06 19:31:27 | INFO | train | epoch 005 | loss nan | nll_loss 2.768 | mask_ins 0.952 | word_ins_ml 4.384 | word_reposition 0.888 | kpe nan | ppl nan | wps 6735.7 | ups 0.34 | wpb 19912.7 | bsz 255.8 | num_updates 5604 | lr 0.000472287 | gnorm 3.294 | clip 0 | loss_scale 1335 | train_wall 2802 | wall 16648
2022-07-06 19:32:37 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.926 | nll_loss 6.207 | mask_ins 1.614 | word_ins_ml 7.572 | word_reposition 1.41 | kpe 1.329 | ppl 3891.99 | wps 13699.1 | wpb 2279.4 | bsz 32 | num_updates 5604 | best_loss 11.926
2022-07-06 19:32:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_best.pt (epoch 5 @ 5604 updates, score 11.926) (writing took 12.138044694438577 seconds)
2022-07-06 19:37:22 | INFO | train_inner | epoch 006:     96 / 1122 loss=7.05, nll_loss=2.691, mask_ins=0.931, word_ins_ml=4.309, word_reposition=0.876, kpe=0.934, ppl=132.5, wps=5398.6, ups=0.27, wpb=19734.8, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=3.313, clip=0, loss_scale=1024, train_wall=245, wall=17003
2022-07-06 19:42:07 | INFO | train_inner | epoch 006:    196 / 1122 loss=6.923, nll_loss=2.593, mask_ins=0.911, word_ins_ml=4.221, word_reposition=0.867, kpe=0.924, ppl=121.38, wps=7014.1, ups=0.35, wpb=19981.3, bsz=256, num_updates=5800, lr=0.000464238, gnorm=2.927, clip=0, loss_scale=1024, train_wall=246, wall=17288
2022-07-06 19:47:39 | INFO | train_inner | epoch 006:    296 / 1122 loss=6.902, nll_loss=2.585, mask_ins=0.908, word_ins_ml=4.213, word_reposition=0.857, kpe=0.925, ppl=119.61, wps=5998.9, ups=0.3, wpb=19948.8, bsz=256, num_updates=5900, lr=0.000460287, gnorm=3.026, clip=0, loss_scale=1567, train_wall=294, wall=17620
2022-07-06 19:52:24 | INFO | train_inner | epoch 006:    396 / 1122 loss=6.871, nll_loss=2.56, mask_ins=0.902, word_ins_ml=4.189, word_reposition=0.853, kpe=0.926, ppl=117.02, wps=7011.6, ups=0.35, wpb=19948.1, bsz=256, num_updates=6000, lr=0.000456435, gnorm=2.905, clip=0, loss_scale=2048, train_wall=246, wall=17905
2022-07-06 19:57:09 | INFO | train_inner | epoch 006:    496 / 1122 loss=nan, nll_loss=2.518, mask_ins=0.901, word_ins_ml=4.151, word_reposition=0.86, kpe=nan, ppl=nan, wps=6996.6, ups=0.35, wpb=19922.3, bsz=256, num_updates=6100, lr=0.000452679, gnorm=2.865, clip=0, loss_scale=2048, train_wall=246, wall=18190
2022-07-06 20:01:54 | INFO | train_inner | epoch 006:    596 / 1122 loss=6.883, nll_loss=2.578, mask_ins=0.905, word_ins_ml=4.203, word_reposition=0.847, kpe=0.927, ppl=118.01, wps=6935.7, ups=0.35, wpb=19778.4, bsz=256, num_updates=6200, lr=0.000449013, gnorm=2.951, clip=0, loss_scale=2048, train_wall=247, wall=18475
2022-07-06 20:06:40 | INFO | train_inner | epoch 006:    696 / 1122 loss=6.898, nll_loss=2.579, mask_ins=0.905, word_ins_ml=4.204, word_reposition=0.856, kpe=0.932, ppl=119.26, wps=7000.1, ups=0.35, wpb=20029.8, bsz=256, num_updates=6300, lr=0.000445435, gnorm=3.14, clip=0, loss_scale=2048, train_wall=248, wall=18761
2022-07-06 20:11:25 | INFO | train_inner | epoch 006:    796 / 1122 loss=nan, nll_loss=2.558, mask_ins=0.898, word_ins_ml=4.184, word_reposition=0.852, kpe=nan, ppl=nan, wps=6966.1, ups=0.35, wpb=19867.4, bsz=256, num_updates=6400, lr=0.000441942, gnorm=3.009, clip=0, loss_scale=2888, train_wall=247, wall=19046
2022-07-06 20:16:09 | INFO | train_inner | epoch 006:    896 / 1122 loss=6.813, nll_loss=2.498, mask_ins=0.894, word_ins_ml=4.13, word_reposition=0.856, kpe=0.933, ppl=112.43, wps=7059.7, ups=0.35, wpb=20064.4, bsz=256, num_updates=6500, lr=0.000438529, gnorm=2.966, clip=0, loss_scale=4096, train_wall=246, wall=19330
2022-07-06 20:20:55 | INFO | train_inner | epoch 006:    996 / 1122 loss=6.821, nll_loss=2.522, mask_ins=0.888, word_ins_ml=4.15, word_reposition=0.848, kpe=0.935, ppl=113.09, wps=6983.5, ups=0.35, wpb=19914.6, bsz=256, num_updates=6600, lr=0.000435194, gnorm=3.002, clip=0, loss_scale=4096, train_wall=247, wall=19615
2022-07-06 20:25:40 | INFO | train_inner | epoch 006:   1096 / 1122 loss=6.775, nll_loss=2.49, mask_ins=0.886, word_ins_ml=4.121, word_reposition=0.842, kpe=0.926, ppl=109.55, wps=6976, ups=0.35, wpb=19884.5, bsz=256, num_updates=6700, lr=0.000431934, gnorm=2.887, clip=0, loss_scale=4096, train_wall=246, wall=19900
2022-07-06 20:26:53 | INFO | train | epoch 006 | loss nan | nll_loss 2.56 | mask_ins 0.902 | word_ins_ml 4.187 | word_reposition 0.856 | kpe nan | ppl nan | wps 6717.9 | ups 0.34 | wpb 19913.2 | bsz 255.8 | num_updates 6726 | lr 0.000431099 | gnorm 2.99 | clip 0 | loss_scale 2496 | train_wall 2813 | wall 19974
2022-07-06 20:28:02 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 12.056 | nll_loss 6.196 | mask_ins 1.645 | word_ins_ml 7.562 | word_reposition 1.39 | kpe 1.459 | ppl 4258.71 | wps 13746.7 | wpb 2279.4 | bsz 32 | num_updates 6726 | best_loss 11.926
2022-07-06 20:28:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 6 @ 6726 updates, score 12.056) (writing took 6.3636649288237095 seconds)
2022-07-06 20:31:39 | INFO | train_inner | epoch 007:     74 / 1122 loss=6.757, nll_loss=2.488, mask_ins=0.884, word_ins_ml=4.12, word_reposition=0.846, kpe=0.908, ppl=108.19, wps=5474.8, ups=0.28, wpb=19682.1, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=2.913, clip=0, loss_scale=4096, train_wall=246, wall=20260
2022-07-06 20:35:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-06 20:36:26 | INFO | train_inner | epoch 007:    175 / 1122 loss=6.686, nll_loss=2.442, mask_ins=0.874, word_ins_ml=4.077, word_reposition=0.841, kpe=0.894, ppl=102.99, wps=6951.4, ups=0.35, wpb=19950.5, bsz=256, num_updates=6900, lr=0.000425628, gnorm=2.705, clip=0, loss_scale=4177, train_wall=249, wall=20547
2022-07-06 20:41:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 20:41:14 | INFO | train_inner | epoch 007:    276 / 1122 loss=6.649, nll_loss=2.403, mask_ins=0.87, word_ins_ml=4.042, word_reposition=0.848, kpe=0.889, ppl=100.38, wps=6956, ups=0.35, wpb=20027.7, bsz=256, num_updates=7000, lr=0.000422577, gnorm=2.684, clip=0, loss_scale=4015, train_wall=249, wall=20835
2022-07-06 20:43:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-06 20:46:02 | INFO | train_inner | epoch 007:    377 / 1122 loss=6.608, nll_loss=2.372, mask_ins=0.868, word_ins_ml=4.014, word_reposition=0.83, kpe=0.896, ppl=97.55, wps=6914.9, ups=0.35, wpb=19914.6, bsz=256, num_updates=7100, lr=0.000419591, gnorm=2.705, clip=0, loss_scale=1399, train_wall=249, wall=21123
2022-07-06 20:49:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-06 20:51:29 | INFO | train_inner | epoch 007:    478 / 1122 loss=nan, nll_loss=2.408, mask_ins=0.864, word_ins_ml=4.045, word_reposition=0.825, kpe=nan, ppl=nan, wps=6091.8, ups=0.31, wpb=19908.8, bsz=256, num_updates=7200, lr=0.000416667, gnorm=2.815, clip=0, loss_scale=821, train_wall=288, wall=21450
2022-07-06 20:56:14 | INFO | train_inner | epoch 007:    578 / 1122 loss=6.614, nll_loss=2.389, mask_ins=0.862, word_ins_ml=4.028, word_reposition=0.827, kpe=0.897, ppl=97.96, wps=6966.9, ups=0.35, wpb=19829.8, bsz=256, num_updates=7300, lr=0.000413803, gnorm=2.811, clip=0, loss_scale=512, train_wall=246, wall=21734
2022-07-06 21:00:58 | INFO | train_inner | epoch 007:    678 / 1122 loss=nan, nll_loss=2.405, mask_ins=0.868, word_ins_ml=4.041, word_reposition=0.831, kpe=nan, ppl=nan, wps=7011.8, ups=0.35, wpb=19935.1, bsz=256, num_updates=7400, lr=0.000410997, gnorm=2.7, clip=0, loss_scale=512, train_wall=246, wall=22019
2022-07-06 21:05:43 | INFO | train_inner | epoch 007:    778 / 1122 loss=6.618, nll_loss=2.386, mask_ins=0.862, word_ins_ml=4.023, word_reposition=0.832, kpe=0.901, ppl=98.2, wps=7015.4, ups=0.35, wpb=19973.2, bsz=256, num_updates=7500, lr=0.000408248, gnorm=2.674, clip=0, loss_scale=512, train_wall=246, wall=22303
2022-07-06 21:10:27 | INFO | train_inner | epoch 007:    878 / 1122 loss=6.626, nll_loss=2.407, mask_ins=0.86, word_ins_ml=4.041, word_reposition=0.829, kpe=0.897, ppl=98.76, wps=7002, ups=0.35, wpb=19943.1, bsz=256, num_updates=7600, lr=0.000405554, gnorm=2.68, clip=0, loss_scale=512, train_wall=247, wall=22588
2022-07-06 21:15:13 | INFO | train_inner | epoch 007:    978 / 1122 loss=6.574, nll_loss=2.352, mask_ins=0.856, word_ins_ml=3.992, word_reposition=0.828, kpe=0.898, ppl=95.25, wps=7020.3, ups=0.35, wpb=20044.4, bsz=256, num_updates=7700, lr=0.000402911, gnorm=2.611, clip=0, loss_scale=655, train_wall=247, wall=22874
2022-07-06 21:19:57 | INFO | train_inner | epoch 007:   1078 / 1122 loss=6.624, nll_loss=2.404, mask_ins=0.859, word_ins_ml=4.036, word_reposition=0.828, kpe=0.901, ppl=98.64, wps=7007.8, ups=0.35, wpb=19926.3, bsz=256, num_updates=7800, lr=0.00040032, gnorm=2.615, clip=0, loss_scale=1024, train_wall=246, wall=23158
2022-07-06 21:22:02 | INFO | train | epoch 007 | loss nan | nll_loss 2.401 | mask_ins 0.866 | word_ins_ml 4.038 | word_reposition 0.833 | kpe nan | ppl nan | wps 6727.6 | ups 0.34 | wpb 19913.8 | bsz 255.8 | num_updates 7844 | lr 0.000399196 | gnorm 2.715 | clip 0 | loss_scale 1580 | train_wall 2804 | wall 23283
2022-07-06 21:23:12 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.606 | nll_loss 5.915 | mask_ins 1.561 | word_ins_ml 7.287 | word_reposition 1.386 | kpe 1.371 | ppl 3116.88 | wps 13724.5 | wpb 2279.4 | bsz 32 | num_updates 7844 | best_loss 11.606
2022-07-06 21:23:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_best.pt (epoch 7 @ 7844 updates, score 11.606) (writing took 11.823069053702056 seconds)
2022-07-06 21:26:03 | INFO | train_inner | epoch 008:     56 / 1122 loss=6.541, nll_loss=2.332, mask_ins=0.864, word_ins_ml=3.973, word_reposition=0.826, kpe=0.879, ppl=93.14, wps=5393, ups=0.27, wpb=19713.1, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=2.701, clip=0, loss_scale=1024, train_wall=246, wall=23524
2022-07-06 21:30:48 | INFO | train_inner | epoch 008:    156 / 1122 loss=6.494, nll_loss=2.332, mask_ins=0.844, word_ins_ml=3.972, word_reposition=0.819, kpe=0.859, ppl=90.15, wps=6978.2, ups=0.35, wpb=19918.7, bsz=256, num_updates=8000, lr=0.000395285, gnorm=2.615, clip=0, loss_scale=1024, train_wall=247, wall=23809
2022-07-06 21:35:33 | INFO | train_inner | epoch 008:    256 / 1122 loss=6.473, nll_loss=2.316, mask_ins=0.837, word_ins_ml=3.958, word_reposition=0.817, kpe=0.861, ppl=88.85, wps=6982.5, ups=0.35, wpb=19886.8, bsz=256, num_updates=8100, lr=0.000392837, gnorm=2.565, clip=0, loss_scale=1024, train_wall=246, wall=24094
2022-07-06 21:40:18 | INFO | train_inner | epoch 008:    356 / 1122 loss=6.455, nll_loss=2.292, mask_ins=0.835, word_ins_ml=3.936, word_reposition=0.818, kpe=0.865, ppl=87.74, wps=7016.2, ups=0.35, wpb=19964.5, bsz=256, num_updates=8200, lr=0.000390434, gnorm=2.617, clip=0, loss_scale=1188, train_wall=246, wall=24378
2022-07-06 21:45:02 | INFO | train_inner | epoch 008:    456 / 1122 loss=6.469, nll_loss=2.308, mask_ins=0.84, word_ins_ml=3.949, word_reposition=0.818, kpe=0.861, ppl=88.57, wps=6994.6, ups=0.35, wpb=19926.6, bsz=256, num_updates=8300, lr=0.000388075, gnorm=2.667, clip=0, loss_scale=2048, train_wall=247, wall=24663
2022-07-06 21:49:47 | INFO | train_inner | epoch 008:    556 / 1122 loss=nan, nll_loss=2.327, mask_ins=0.839, word_ins_ml=3.965, word_reposition=0.817, kpe=nan, ppl=nan, wps=7033.5, ups=0.35, wpb=20028.3, bsz=256, num_updates=8400, lr=0.000385758, gnorm=2.596, clip=0, loss_scale=2048, train_wall=247, wall=24948
2022-07-06 21:55:21 | INFO | train_inner | epoch 008:    656 / 1122 loss=6.481, nll_loss=2.322, mask_ins=0.831, word_ins_ml=3.96, word_reposition=0.819, kpe=0.872, ppl=89.35, wps=5980.2, ups=0.3, wpb=19963.9, bsz=256, num_updates=8500, lr=0.000383482, gnorm=2.631, clip=0, loss_scale=2048, train_wall=295, wall=25282
2022-07-06 22:00:06 | INFO | train_inner | epoch 008:    756 / 1122 loss=6.412, nll_loss=2.256, mask_ins=0.835, word_ins_ml=3.901, word_reposition=0.809, kpe=0.866, ppl=85.13, wps=6965, ups=0.35, wpb=19878.6, bsz=256, num_updates=8600, lr=0.000381246, gnorm=2.562, clip=0, loss_scale=2048, train_wall=247, wall=25567
2022-07-06 22:04:51 | INFO | train_inner | epoch 008:    856 / 1122 loss=nan, nll_loss=2.314, mask_ins=0.832, word_ins_ml=3.952, word_reposition=0.824, kpe=nan, ppl=nan, wps=7031, ups=0.35, wpb=19978.3, bsz=256, num_updates=8700, lr=0.000379049, gnorm=2.584, clip=0, loss_scale=2130, train_wall=246, wall=25851
2022-07-06 22:09:35 | INFO | train_inner | epoch 008:    956 / 1122 loss=6.447, nll_loss=2.286, mask_ins=0.834, word_ins_ml=3.927, word_reposition=0.812, kpe=0.873, ppl=87.25, wps=7007.4, ups=0.35, wpb=19908.9, bsz=256, num_updates=8800, lr=0.000376889, gnorm=2.535, clip=0, loss_scale=4096, train_wall=246, wall=26136
2022-07-06 22:10:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 22:14:22 | INFO | train_inner | epoch 008:   1057 / 1122 loss=6.405, nll_loss=2.259, mask_ins=0.831, word_ins_ml=3.903, word_reposition=0.803, kpe=0.868, ppl=84.71, wps=6890.3, ups=0.35, wpb=19768.9, bsz=256, num_updates=8900, lr=0.000374766, gnorm=2.472, clip=0, loss_scale=2555, train_wall=248, wall=26422
2022-07-06 22:17:26 | INFO | train | epoch 008 | loss nan | nll_loss 2.299 | mask_ins 0.836 | word_ins_ml 3.941 | word_reposition 0.815 | kpe nan | ppl nan | wps 6716.8 | ups 0.34 | wpb 19913.9 | bsz 255.8 | num_updates 8965 | lr 0.000373405 | gnorm 2.585 | clip 0 | loss_scale 1973 | train_wall 2812 | wall 26607
2022-07-06 22:18:35 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 11.624 | nll_loss 5.905 | mask_ins 1.545 | word_ins_ml 7.28 | word_reposition 1.388 | kpe 1.41 | ppl 3156.13 | wps 13668.6 | wpb 2279.4 | bsz 32 | num_updates 8965 | best_loss 11.606
2022-07-06 22:18:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 8 @ 8965 updates, score 11.624) (writing took 7.868923218920827 seconds)
2022-07-06 22:20:22 | INFO | train_inner | epoch 009:     35 / 1122 loss=6.41, nll_loss=2.266, mask_ins=0.835, word_ins_ml=3.909, word_reposition=0.809, kpe=0.857, ppl=85.03, wps=5467.8, ups=0.28, wpb=19728.5, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=2.624, clip=0, loss_scale=2048, train_wall=245, wall=26783
2022-07-06 22:25:07 | INFO | train_inner | epoch 009:    135 / 1122 loss=6.304, nll_loss=2.211, mask_ins=0.822, word_ins_ml=3.86, word_reposition=0.801, kpe=0.822, ppl=79.02, wps=7062.6, ups=0.35, wpb=20066.2, bsz=256, num_updates=9100, lr=0.000370625, gnorm=2.659, clip=0, loss_scale=2048, train_wall=246, wall=27067
2022-07-06 22:29:51 | INFO | train_inner | epoch 009:    235 / 1122 loss=6.362, nll_loss=2.264, mask_ins=0.825, word_ins_ml=3.907, word_reposition=0.805, kpe=0.825, ppl=82.23, wps=6972.1, ups=0.35, wpb=19860.6, bsz=256, num_updates=9200, lr=0.000368605, gnorm=2.613, clip=0, loss_scale=2048, train_wall=246, wall=27352
2022-07-06 22:34:36 | INFO | train_inner | epoch 009:    335 / 1122 loss=6.336, nll_loss=2.235, mask_ins=0.818, word_ins_ml=3.881, word_reposition=0.803, kpe=0.834, ppl=80.8, wps=6979.5, ups=0.35, wpb=19830, bsz=256, num_updates=9300, lr=0.000366618, gnorm=2.641, clip=0, loss_scale=2048, train_wall=246, wall=27636
2022-07-06 22:39:19 | INFO | train_inner | epoch 009:    435 / 1122 loss=nan, nll_loss=2.204, mask_ins=0.811, word_ins_ml=3.852, word_reposition=0.794, kpe=nan, ppl=nan, wps=7001.2, ups=0.35, wpb=19868.2, bsz=256, num_updates=9400, lr=0.000364662, gnorm=2.545, clip=0, loss_scale=3359, train_wall=246, wall=27920
2022-07-06 22:44:04 | INFO | train_inner | epoch 009:    535 / 1122 loss=6.275, nll_loss=2.18, mask_ins=0.81, word_ins_ml=3.831, word_reposition=0.8, kpe=0.834, ppl=77.44, wps=6994.7, ups=0.35, wpb=19911.1, bsz=256, num_updates=9500, lr=0.000362738, gnorm=2.556, clip=0, loss_scale=4096, train_wall=246, wall=28205
2022-07-06 22:48:50 | INFO | train_inner | epoch 009:    635 / 1122 loss=6.313, nll_loss=2.219, mask_ins=0.818, word_ins_ml=3.865, word_reposition=0.794, kpe=0.836, ppl=79.5, wps=6991.8, ups=0.35, wpb=19961.5, bsz=256, num_updates=9600, lr=0.000360844, gnorm=2.578, clip=0, loss_scale=4096, train_wall=247, wall=28490
2022-07-06 22:52:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 22:53:36 | INFO | train_inner | epoch 009:    736 / 1122 loss=6.234, nll_loss=2.139, mask_ins=0.817, word_ins_ml=3.794, word_reposition=0.789, kpe=0.833, ppl=75.26, wps=6932.6, ups=0.35, wpb=19892.5, bsz=256, num_updates=9700, lr=0.000358979, gnorm=2.507, clip=0, loss_scale=3711, train_wall=248, wall=28777
2022-07-06 22:58:58 | INFO | train_inner | epoch 009:    836 / 1122 loss=6.293, nll_loss=2.198, mask_ins=0.809, word_ins_ml=3.845, word_reposition=0.801, kpe=0.838, ppl=78.39, wps=6192.7, ups=0.31, wpb=19899.1, bsz=256, num_updates=9800, lr=0.000357143, gnorm=2.577, clip=0, loss_scale=2048, train_wall=282, wall=29099
2022-07-06 23:03:43 | INFO | train_inner | epoch 009:    936 / 1122 loss=6.302, nll_loss=2.212, mask_ins=0.805, word_ins_ml=3.857, word_reposition=0.799, kpe=0.841, ppl=78.88, wps=7015.8, ups=0.35, wpb=20003.8, bsz=256, num_updates=9900, lr=0.000355335, gnorm=2.507, clip=0, loss_scale=2048, train_wall=247, wall=29384
2022-07-06 23:08:28 | INFO | train_inner | epoch 009:   1036 / 1122 loss=nan, nll_loss=2.226, mask_ins=0.812, word_ins_ml=3.87, word_reposition=0.803, kpe=nan, ppl=nan, wps=7002.5, ups=0.35, wpb=19939.5, bsz=256, num_updates=10000, lr=0.000353553, gnorm=2.523, clip=0, loss_scale=2048, train_wall=246, wall=29668
2022-07-06 23:12:32 | INFO | train | epoch 009 | loss nan | nll_loss 2.209 | mask_ins 0.815 | word_ins_ml 3.856 | word_reposition 0.8 | kpe nan | ppl nan | wps 6752.3 | ups 0.34 | wpb 19913.8 | bsz 255.8 | num_updates 10086 | lr 0.000352043 | gnorm 2.576 | clip 0 | loss_scale 2680 | train_wall 2797 | wall 29913
2022-07-06 23:13:41 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 11.63 | nll_loss 5.907 | mask_ins 1.526 | word_ins_ml 7.285 | word_reposition 1.35 | kpe 1.468 | ppl 3169.35 | wps 13696.8 | wpb 2279.4 | bsz 32 | num_updates 10086 | best_loss 11.606
2022-07-06 23:13:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 9 @ 10086 updates, score 11.63) (writing took 6.460343883372843 seconds)
2022-07-06 23:14:28 | INFO | train_inner | epoch 010:     14 / 1122 loss=6.285, nll_loss=2.191, mask_ins=0.808, word_ins_ml=3.838, word_reposition=0.8, kpe=0.838, ppl=77.99, wps=5520.4, ups=0.28, wpb=19875.6, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=2.644, clip=0, loss_scale=2048, train_wall=246, wall=30029
2022-07-06 23:19:12 | INFO | train_inner | epoch 010:    114 / 1122 loss=6.193, nll_loss=2.164, mask_ins=0.805, word_ins_ml=3.814, word_reposition=0.787, kpe=0.786, ppl=73.15, wps=7017.6, ups=0.35, wpb=19939.5, bsz=256, num_updates=10200, lr=0.00035007, gnorm=2.569, clip=0, loss_scale=2191, train_wall=246, wall=30313
2022-07-06 23:22:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 23:23:59 | INFO | train_inner | epoch 010:    215 / 1122 loss=6.187, nll_loss=2.157, mask_ins=0.796, word_ins_ml=3.808, word_reposition=0.793, kpe=0.791, ppl=72.87, wps=6925.7, ups=0.35, wpb=19894.4, bsz=256, num_updates=10300, lr=0.000348367, gnorm=2.52, clip=0, loss_scale=3285, train_wall=249, wall=30600
2022-07-06 23:28:44 | INFO | train_inner | epoch 010:    315 / 1122 loss=6.216, nll_loss=2.164, mask_ins=0.807, word_ins_ml=3.813, word_reposition=0.8, kpe=0.796, ppl=74.34, wps=7019.6, ups=0.35, wpb=19981.5, bsz=256, num_updates=10400, lr=0.000346688, gnorm=2.537, clip=0, loss_scale=2048, train_wall=246, wall=30885
2022-07-06 23:33:28 | INFO | train_inner | epoch 010:    415 / 1122 loss=6.23, nll_loss=2.201, mask_ins=0.798, word_ins_ml=3.846, word_reposition=0.791, kpe=0.794, ppl=75.04, wps=7018.6, ups=0.35, wpb=19975, bsz=256, num_updates=10500, lr=0.000345033, gnorm=2.614, clip=0, loss_scale=2048, train_wall=246, wall=31169
2022-07-06 23:38:13 | INFO | train_inner | epoch 010:    515 / 1122 loss=6.186, nll_loss=2.143, mask_ins=0.803, word_ins_ml=3.794, word_reposition=0.789, kpe=0.8, ppl=72.82, wps=6982.1, ups=0.35, wpb=19880.2, bsz=256, num_updates=10600, lr=0.000343401, gnorm=2.518, clip=0, loss_scale=2048, train_wall=247, wall=31454
2022-07-06 23:42:57 | INFO | train_inner | epoch 010:    615 / 1122 loss=nan, nll_loss=2.119, mask_ins=0.794, word_ins_ml=3.773, word_reposition=0.787, kpe=nan, ppl=nan, wps=7072.7, ups=0.35, wpb=20107.7, bsz=256, num_updates=10700, lr=0.000341793, gnorm=2.57, clip=0, loss_scale=2048, train_wall=246, wall=31738
2022-07-06 23:47:42 | INFO | train_inner | epoch 010:    715 / 1122 loss=6.211, nll_loss=2.161, mask_ins=0.799, word_ins_ml=3.81, word_reposition=0.794, kpe=0.808, ppl=74.07, wps=6983.2, ups=0.35, wpb=19868.2, bsz=256, num_updates=10800, lr=0.000340207, gnorm=2.602, clip=0, loss_scale=2621, train_wall=246, wall=32023
2022-07-06 23:49:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 23:52:30 | INFO | train_inner | epoch 010:    816 / 1122 loss=nan, nll_loss=2.106, mask_ins=0.791, word_ins_ml=3.761, word_reposition=0.789, kpe=nan, ppl=nan, wps=6933.3, ups=0.35, wpb=19950.5, bsz=256, num_updates=10900, lr=0.000338643, gnorm=2.614, clip=0, loss_scale=2839, train_wall=249, wall=32310
2022-07-06 23:57:14 | INFO | train_inner | epoch 010:    916 / 1122 loss=6.184, nll_loss=2.137, mask_ins=0.798, word_ins_ml=3.788, word_reposition=0.789, kpe=0.81, ppl=72.7, wps=7008.9, ups=0.35, wpb=19944.5, bsz=256, num_updates=11000, lr=0.0003371, gnorm=2.57, clip=0, loss_scale=2048, train_wall=246, wall=32595
2022-07-07 00:02:37 | INFO | train_inner | epoch 010:   1016 / 1122 loss=6.16, nll_loss=2.118, mask_ins=0.79, word_ins_ml=3.771, word_reposition=0.79, kpe=0.809, ppl=71.5, wps=6156.4, ups=0.31, wpb=19864.5, bsz=256, num_updates=11100, lr=0.000335578, gnorm=2.477, clip=0, loss_scale=2048, train_wall=284, wall=32918
2022-07-07 00:07:22 | INFO | train_inner | epoch 010:   1116 / 1122 loss=6.163, nll_loss=2.119, mask_ins=0.792, word_ins_ml=3.771, word_reposition=0.79, kpe=0.809, ppl=71.65, wps=6940.5, ups=0.35, wpb=19779.6, bsz=256, num_updates=11200, lr=0.000334077, gnorm=2.509, clip=0, loss_scale=2048, train_wall=246, wall=33203
2022-07-07 00:07:38 | INFO | train | epoch 010 | loss nan | nll_loss 2.146 | mask_ins 0.798 | word_ins_ml 3.796 | word_reposition 0.791 | kpe nan | ppl nan | wps 6745.4 | ups 0.34 | wpb 19913.9 | bsz 255.8 | num_updates 11206 | lr 0.000333987 | gnorm 2.566 | clip 0 | loss_scale 2294 | train_wall 2802 | wall 33219
2022-07-07 00:08:48 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 11.689 | nll_loss 5.898 | mask_ins 1.539 | word_ins_ml 7.274 | word_reposition 1.395 | kpe 1.481 | ppl 3302.19 | wps 13748.7 | wpb 2279.4 | bsz 32 | num_updates 11206 | best_loss 11.606
2022-07-07 00:08:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 10 @ 11206 updates, score 11.689) (writing took 6.781526748090982 seconds)
2022-07-07 00:13:21 | INFO | train_inner | epoch 011:     94 / 1122 loss=6.078, nll_loss=2.096, mask_ins=0.785, word_ins_ml=3.751, word_reposition=0.783, kpe=0.759, ppl=67.55, wps=5500.3, ups=0.28, wpb=19765.1, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=2.742, clip=0, loss_scale=2048, train_wall=245, wall=33562
2022-07-07 00:18:05 | INFO | train_inner | epoch 011:    194 / 1122 loss=nan, nll_loss=2.058, mask_ins=0.781, word_ins_ml=3.717, word_reposition=0.776, kpe=nan, ppl=nan, wps=6990.4, ups=0.35, wpb=19850.5, bsz=256, num_updates=11400, lr=0.000331133, gnorm=2.545, clip=0, loss_scale=3072, train_wall=246, wall=33846
2022-07-07 00:19:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 00:22:52 | INFO | train_inner | epoch 011:    295 / 1122 loss=6.042, nll_loss=2.064, mask_ins=0.78, word_ins_ml=3.722, word_reposition=0.779, kpe=0.762, ppl=65.89, wps=6939.6, ups=0.35, wpb=19934.7, bsz=256, num_updates=11500, lr=0.00032969, gnorm=2.706, clip=0, loss_scale=2454, train_wall=248, wall=34133
2022-07-07 00:27:37 | INFO | train_inner | epoch 011:    395 / 1122 loss=6.044, nll_loss=2.06, mask_ins=0.785, word_ins_ml=3.718, word_reposition=0.78, kpe=0.762, ppl=65.99, wps=7015.9, ups=0.35, wpb=19955.9, bsz=256, num_updates=11600, lr=0.000328266, gnorm=2.653, clip=0, loss_scale=2048, train_wall=246, wall=34418
2022-07-07 00:32:22 | INFO | train_inner | epoch 011:    495 / 1122 loss=6.078, nll_loss=2.103, mask_ins=0.776, word_ins_ml=3.756, word_reposition=0.782, kpe=0.765, ppl=67.55, wps=7033.4, ups=0.35, wpb=20017.6, bsz=256, num_updates=11700, lr=0.00032686, gnorm=2.561, clip=0, loss_scale=2048, train_wall=246, wall=34702
2022-07-07 00:37:06 | INFO | train_inner | epoch 011:    595 / 1122 loss=6.079, nll_loss=2.089, mask_ins=0.785, word_ins_ml=3.743, word_reposition=0.78, kpe=0.77, ppl=67.58, wps=7018.4, ups=0.35, wpb=19963.6, bsz=256, num_updates=11800, lr=0.000325472, gnorm=2.621, clip=0, loss_scale=2048, train_wall=246, wall=34987
2022-07-07 00:41:50 | INFO | train_inner | epoch 011:    695 / 1122 loss=6.084, nll_loss=2.077, mask_ins=0.793, word_ins_ml=3.732, word_reposition=0.783, kpe=0.776, ppl=67.86, wps=7011.9, ups=0.35, wpb=19939.2, bsz=256, num_updates=11900, lr=0.000324102, gnorm=2.554, clip=0, loss_scale=2048, train_wall=246, wall=35271
2022-07-07 00:46:35 | INFO | train_inner | epoch 011:    795 / 1122 loss=6.08, nll_loss=2.098, mask_ins=0.781, word_ins_ml=3.75, word_reposition=0.78, kpe=0.769, ppl=67.67, wps=6960.2, ups=0.35, wpb=19798.5, bsz=256, num_updates=12000, lr=0.000322749, gnorm=2.62, clip=0, loss_scale=3461, train_wall=246, wall=35556
2022-07-07 00:51:20 | INFO | train_inner | epoch 011:    895 / 1122 loss=6.074, nll_loss=2.095, mask_ins=0.778, word_ins_ml=3.748, word_reposition=0.778, kpe=0.77, ppl=67.36, wps=7017.7, ups=0.35, wpb=19989.8, bsz=256, num_updates=12100, lr=0.000321412, gnorm=2.607, clip=0, loss_scale=4096, train_wall=247, wall=35840
2022-07-07 00:54:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 00:56:07 | INFO | train_inner | epoch 011:    996 / 1122 loss=6.078, nll_loss=2.086, mask_ins=0.788, word_ins_ml=3.739, word_reposition=0.778, kpe=0.773, ppl=67.57, wps=6906.3, ups=0.35, wpb=19860.3, bsz=256, num_updates=12200, lr=0.000320092, gnorm=2.559, clip=0, loss_scale=3265, train_wall=249, wall=36128
2022-07-07 01:00:52 | INFO | train_inner | epoch 011:   1096 / 1122 loss=nan, nll_loss=2.087, mask_ins=0.782, word_ins_ml=3.739, word_reposition=0.78, kpe=nan, ppl=nan, wps=7008.4, ups=0.35, wpb=19973.8, bsz=256, num_updates=12300, lr=0.000318788, gnorm=2.522, clip=0, loss_scale=2048, train_wall=247, wall=36413
2022-07-07 01:02:41 | INFO | train | epoch 011 | loss nan | nll_loss 2.082 | mask_ins 0.783 | word_ins_ml 3.737 | word_reposition 0.78 | kpe nan | ppl nan | wps 6752.5 | ups 0.34 | wpb 19914.8 | bsz 255.8 | num_updates 12326 | lr 0.000318452 | gnorm 2.605 | clip 0 | loss_scale 2594 | train_wall 2797 | wall 36522
2022-07-07 01:03:51 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 11.381 | nll_loss 5.694 | mask_ins 1.509 | word_ins_ml 7.075 | word_reposition 1.294 | kpe 1.502 | ppl 2666.31 | wps 13603.8 | wpb 2279.4 | bsz 32 | num_updates 12326 | best_loss 11.381
2022-07-07 01:04:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_best.pt (epoch 11 @ 12326 updates, score 11.381) (writing took 12.093872730620205 seconds)
2022-07-07 01:07:33 | INFO | train_inner | epoch 012:     74 / 1122 loss=6.009, nll_loss=2.062, mask_ins=0.779, word_ins_ml=3.718, word_reposition=0.78, kpe=0.732, ppl=64.42, wps=4946.5, ups=0.25, wpb=19822.4, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=2.627, clip=0, loss_scale=2048, train_wall=281, wall=36814
2022-07-07 01:12:14 | INFO | train_inner | epoch 012:    174 / 1122 loss=5.867, nll_loss=1.964, mask_ins=0.761, word_ins_ml=3.63, word_reposition=0.763, kpe=0.712, ppl=58.37, wps=7068.1, ups=0.36, wpb=19886.3, bsz=256, num_updates=12500, lr=0.000316228, gnorm=2.537, clip=0, loss_scale=2048, train_wall=244, wall=37095
2022-07-07 01:16:56 | INFO | train_inner | epoch 012:    274 / 1122 loss=5.943, nll_loss=2.024, mask_ins=0.772, word_ins_ml=3.683, word_reposition=0.77, kpe=0.718, ppl=61.52, wps=7070.3, ups=0.36, wpb=19907.9, bsz=256, num_updates=12600, lr=0.00031497, gnorm=2.494, clip=0, loss_scale=2048, train_wall=245, wall=37377
2022-07-07 01:21:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 01:21:40 | INFO | train_inner | epoch 012:    375 / 1122 loss=5.981, nll_loss=2.048, mask_ins=0.776, word_ins_ml=3.704, word_reposition=0.775, kpe=0.725, ppl=63.14, wps=7004.9, ups=0.35, wpb=19892.1, bsz=256, num_updates=12700, lr=0.000313728, gnorm=2.627, clip=0, loss_scale=2372, train_wall=247, wall=37661
2022-07-07 01:26:21 | INFO | train_inner | epoch 012:    475 / 1122 loss=5.943, nll_loss=2.023, mask_ins=0.753, word_ins_ml=3.682, word_reposition=0.773, kpe=0.735, ppl=61.52, wps=7071.8, ups=0.36, wpb=19879.8, bsz=256, num_updates=12800, lr=0.0003125, gnorm=2.667, clip=0, loss_scale=2048, train_wall=244, wall=37942
2022-07-07 01:31:03 | INFO | train_inner | epoch 012:    575 / 1122 loss=5.948, nll_loss=2.028, mask_ins=0.761, word_ins_ml=3.686, word_reposition=0.77, kpe=0.731, ppl=61.72, wps=7104.1, ups=0.35, wpb=20038.9, bsz=256, num_updates=12900, lr=0.000311286, gnorm=2.597, clip=0, loss_scale=2048, train_wall=245, wall=38224
2022-07-07 01:35:44 | INFO | train_inner | epoch 012:    675 / 1122 loss=5.926, nll_loss=2.012, mask_ins=0.759, word_ins_ml=3.672, word_reposition=0.764, kpe=0.731, ppl=60.81, wps=7058.5, ups=0.36, wpb=19803.2, bsz=256, num_updates=13000, lr=0.000310087, gnorm=2.61, clip=0, loss_scale=2048, train_wall=244, wall=38504
2022-07-07 01:40:25 | INFO | train_inner | epoch 012:    775 / 1122 loss=5.929, nll_loss=2.009, mask_ins=0.762, word_ins_ml=3.669, word_reposition=0.762, kpe=0.736, ppl=60.93, wps=7104, ups=0.35, wpb=20014.9, bsz=256, num_updates=13100, lr=0.000308901, gnorm=2.559, clip=0, loss_scale=2048, train_wall=245, wall=38786
2022-07-07 01:45:08 | INFO | train_inner | epoch 012:    875 / 1122 loss=5.95, nll_loss=2.02, mask_ins=0.768, word_ins_ml=3.679, word_reposition=0.768, kpe=0.735, ppl=61.81, wps=7067.8, ups=0.35, wpb=19961.4, bsz=256, num_updates=13200, lr=0.000307729, gnorm=2.642, clip=0, loss_scale=2089, train_wall=246, wall=39069
2022-07-07 01:45:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 01:49:52 | INFO | train_inner | epoch 012:    976 / 1122 loss=nan, nll_loss=2.014, mask_ins=0.761, word_ins_ml=3.672, word_reposition=0.773, kpe=nan, ppl=nan, wps=7004.5, ups=0.35, wpb=19915.6, bsz=256, num_updates=13300, lr=0.00030657, gnorm=2.702, clip=0, loss_scale=2129, train_wall=247, wall=39353
2022-07-07 01:54:34 | INFO | train_inner | epoch 012:   1076 / 1122 loss=nan, nll_loss=2.022, mask_ins=0.765, word_ins_ml=3.679, word_reposition=0.768, kpe=nan, ppl=nan, wps=7055.5, ups=0.35, wpb=19891, bsz=256, num_updates=13400, lr=0.000305424, gnorm=2.594, clip=0, loss_scale=2048, train_wall=245, wall=39635
2022-07-07 01:56:43 | INFO | train | epoch 012 | loss nan | nll_loss 2.02 | mask_ins 0.765 | word_ins_ml 3.679 | word_reposition 0.77 | kpe nan | ppl nan | wps 6880.3 | ups 0.35 | wpb 19912.2 | bsz 255.8 | num_updates 13446 | lr 0.000304901 | gnorm 2.601 | clip 0 | loss_scale 2088 | train_wall 2745 | wall 39764
2022-07-07 01:57:51 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.272 | nll_loss 5.609 | mask_ins 1.471 | word_ins_ml 6.995 | word_reposition 1.321 | kpe 1.486 | ppl 2472.71 | wps 13930.6 | wpb 2279.4 | bsz 32 | num_updates 13446 | best_loss 11.272
2022-07-07 01:58:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_best.pt (epoch 12 @ 13446 updates, score 11.272) (writing took 11.611312989145517 seconds)
2022-07-07 02:00:35 | INFO | train_inner | epoch 013:     54 / 1122 loss=5.903, nll_loss=1.987, mask_ins=0.767, word_ins_ml=3.649, word_reposition=0.774, kpe=0.712, ppl=59.83, wps=5478.8, ups=0.28, wpb=19760.6, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=2.61, clip=0, loss_scale=2048, train_wall=244, wall=39996
2022-07-07 02:05:16 | INFO | train_inner | epoch 013:    154 / 1122 loss=5.84, nll_loss=1.981, mask_ins=0.752, word_ins_ml=3.643, word_reposition=0.764, kpe=0.681, ppl=57.29, wps=7102.2, ups=0.36, wpb=19998, bsz=256, num_updates=13600, lr=0.00030317, gnorm=2.688, clip=0, loss_scale=2048, train_wall=245, wall=40277
2022-07-07 02:10:35 | INFO | train_inner | epoch 013:    254 / 1122 loss=5.861, nll_loss=1.978, mask_ins=0.767, word_ins_ml=3.64, word_reposition=0.771, kpe=0.683, ppl=58.13, wps=6239.8, ups=0.31, wpb=19869.2, bsz=256, num_updates=13700, lr=0.000302061, gnorm=2.619, clip=0, loss_scale=2048, train_wall=281, wall=40596
2022-07-07 02:14:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 02:15:20 | INFO | train_inner | epoch 013:    355 / 1122 loss=nan, nll_loss=1.983, mask_ins=0.745, word_ins_ml=3.645, word_reposition=0.757, kpe=nan, ppl=nan, wps=6990.2, ups=0.35, wpb=19934.5, bsz=256, num_updates=13800, lr=0.000300965, gnorm=2.641, clip=0, loss_scale=3285, train_wall=248, wall=40881
2022-07-07 02:20:01 | INFO | train_inner | epoch 013:    455 / 1122 loss=5.847, nll_loss=1.983, mask_ins=0.754, word_ins_ml=3.645, word_reposition=0.758, kpe=0.691, ppl=57.55, wps=7087.7, ups=0.36, wpb=19921.1, bsz=256, num_updates=13900, lr=0.00029988, gnorm=2.602, clip=0, loss_scale=2048, train_wall=244, wall=41162
2022-07-07 02:24:43 | INFO | train_inner | epoch 013:    555 / 1122 loss=5.844, nll_loss=1.972, mask_ins=0.756, word_ins_ml=3.635, word_reposition=0.759, kpe=0.695, ppl=57.46, wps=7078.3, ups=0.36, wpb=19931, bsz=256, num_updates=14000, lr=0.000298807, gnorm=2.578, clip=0, loss_scale=2048, train_wall=245, wall=41443
2022-07-07 02:29:25 | INFO | train_inner | epoch 013:    655 / 1122 loss=nan, nll_loss=1.969, mask_ins=0.763, word_ins_ml=3.632, word_reposition=0.764, kpe=nan, ppl=nan, wps=7050.4, ups=0.35, wpb=19884.6, bsz=256, num_updates=14100, lr=0.000297746, gnorm=2.743, clip=0, loss_scale=2048, train_wall=245, wall=41725
2022-07-07 02:34:07 | INFO | train_inner | epoch 013:    755 / 1122 loss=5.873, nll_loss=1.991, mask_ins=0.76, word_ins_ml=3.651, word_reposition=0.761, kpe=0.701, ppl=58.61, wps=7051.2, ups=0.35, wpb=19911.1, bsz=256, num_updates=14200, lr=0.000296695, gnorm=2.62, clip=0, loss_scale=2048, train_wall=245, wall=42008
2022-07-07 02:38:49 | INFO | train_inner | epoch 013:    855 / 1122 loss=5.894, nll_loss=2.009, mask_ins=0.759, word_ins_ml=3.667, word_reposition=0.764, kpe=0.706, ppl=59.49, wps=7069.6, ups=0.35, wpb=19926.4, bsz=256, num_updates=14300, lr=0.000295656, gnorm=2.634, clip=0, loss_scale=2314, train_wall=245, wall=42290
2022-07-07 02:43:31 | INFO | train_inner | epoch 013:    955 / 1122 loss=5.875, nll_loss=1.987, mask_ins=0.757, word_ins_ml=3.647, word_reposition=0.766, kpe=0.705, ppl=58.7, wps=7054.7, ups=0.35, wpb=19878, bsz=256, num_updates=14400, lr=0.000294628, gnorm=2.607, clip=0, loss_scale=4096, train_wall=245, wall=42571
2022-07-07 02:48:17 | INFO | train_inner | epoch 013:   1055 / 1122 loss=5.859, nll_loss=1.974, mask_ins=0.75, word_ins_ml=3.635, word_reposition=0.768, kpe=0.706, ppl=58.03, wps=6998.5, ups=0.35, wpb=20021.8, bsz=256, num_updates=14500, lr=0.00029361, gnorm=2.495, clip=0, loss_scale=4096, train_wall=249, wall=42858
2022-07-07 02:51:26 | INFO | train | epoch 013 | loss nan | nll_loss 1.982 | mask_ins 0.757 | word_ins_ml 3.643 | word_reposition 0.763 | kpe nan | ppl nan | wps 6798.9 | ups 0.34 | wpb 19913.2 | bsz 255.8 | num_updates 14567 | lr 0.000292934 | gnorm 2.631 | clip 0 | loss_scale 2670 | train_wall 2787 | wall 43047
2022-07-07 02:52:35 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 11.612 | nll_loss 5.738 | mask_ins 1.517 | word_ins_ml 7.114 | word_reposition 1.362 | kpe 1.619 | ppl 3129.66 | wps 13752.2 | wpb 2279.4 | bsz 32 | num_updates 14567 | best_loss 11.272
2022-07-07 02:52:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 13 @ 14567 updates, score 11.612) (writing took 6.597186593338847 seconds)
2022-07-07 02:54:15 | INFO | train_inner | epoch 014:     33 / 1122 loss=5.824, nll_loss=1.968, mask_ins=0.75, word_ins_ml=3.63, word_reposition=0.76, kpe=0.685, ppl=56.64, wps=5525.5, ups=0.28, wpb=19821.6, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=2.726, clip=0, loss_scale=4096, train_wall=245, wall=43216
2022-07-07 02:59:08 | INFO | train_inner | epoch 014:    133 / 1122 loss=5.75, nll_loss=1.94, mask_ins=0.751, word_ins_ml=3.605, word_reposition=0.753, kpe=0.641, ppl=53.82, wps=6802.1, ups=0.34, wpb=19886.4, bsz=256, num_updates=14700, lr=0.000291606, gnorm=2.574, clip=0, loss_scale=4096, train_wall=254, wall=43509
2022-07-07 03:00:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 03:04:02 | INFO | train_inner | epoch 014:    234 / 1122 loss=5.742, nll_loss=1.926, mask_ins=0.745, word_ins_ml=3.592, word_reposition=0.756, kpe=0.648, ppl=53.53, wps=6769.2, ups=0.34, wpb=19932.6, bsz=256, num_updates=14800, lr=0.000290619, gnorm=2.549, clip=0, loss_scale=2555, train_wall=256, wall=43803
2022-07-07 03:08:45 | INFO | train_inner | epoch 014:    334 / 1122 loss=5.762, nll_loss=1.946, mask_ins=0.747, word_ins_ml=3.61, word_reposition=0.758, kpe=0.647, ppl=54.25, wps=7038.6, ups=0.35, wpb=19928.8, bsz=256, num_updates=14900, lr=0.000289642, gnorm=2.549, clip=0, loss_scale=2048, train_wall=245, wall=44086
2022-07-07 03:14:04 | INFO | train_inner | epoch 014:    434 / 1122 loss=5.761, nll_loss=1.928, mask_ins=0.751, word_ins_ml=3.594, word_reposition=0.762, kpe=0.655, ppl=54.23, wps=6244.1, ups=0.31, wpb=19923.3, bsz=256, num_updates=15000, lr=0.000288675, gnorm=2.539, clip=0, loss_scale=2048, train_wall=282, wall=44405
2022-07-07 03:18:46 | INFO | train_inner | epoch 014:    534 / 1122 loss=5.737, nll_loss=1.914, mask_ins=0.745, word_ins_ml=3.581, word_reposition=0.755, kpe=0.656, ppl=53.35, wps=7061.9, ups=0.35, wpb=19896.6, bsz=256, num_updates=15100, lr=0.000287718, gnorm=2.586, clip=0, loss_scale=2048, train_wall=245, wall=44687
2022-07-07 03:23:28 | INFO | train_inner | epoch 014:    634 / 1122 loss=5.754, nll_loss=1.933, mask_ins=0.743, word_ins_ml=3.597, word_reposition=0.753, kpe=0.66, ppl=53.97, wps=7071.4, ups=0.36, wpb=19902, bsz=256, num_updates=15200, lr=0.00028677, gnorm=2.639, clip=0, loss_scale=2048, train_wall=245, wall=44968
2022-07-07 03:28:09 | INFO | train_inner | epoch 014:    734 / 1122 loss=5.723, nll_loss=1.908, mask_ins=0.737, word_ins_ml=3.576, word_reposition=0.751, kpe=0.659, ppl=52.8, wps=7107.6, ups=0.35, wpb=20024.5, bsz=256, num_updates=15300, lr=0.000285831, gnorm=2.476, clip=0, loss_scale=3359, train_wall=245, wall=45250
2022-07-07 03:31:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 03:32:56 | INFO | train_inner | epoch 014:    835 / 1122 loss=5.734, nll_loss=1.91, mask_ins=0.742, word_ins_ml=3.577, word_reposition=0.752, kpe=0.663, ppl=53.23, wps=6927.1, ups=0.35, wpb=19851.8, bsz=256, num_updates=15400, lr=0.000284901, gnorm=2.43, clip=0, loss_scale=3630, train_wall=249, wall=45537
2022-07-07 03:37:38 | INFO | train_inner | epoch 014:    935 / 1122 loss=5.791, nll_loss=1.951, mask_ins=0.752, word_ins_ml=3.614, word_reposition=0.756, kpe=0.669, ppl=55.37, wps=7063.2, ups=0.36, wpb=19884.4, bsz=256, num_updates=15500, lr=0.000283981, gnorm=2.49, clip=0, loss_scale=2048, train_wall=244, wall=45818
2022-07-07 03:42:20 | INFO | train_inner | epoch 014:   1035 / 1122 loss=5.771, nll_loss=1.931, mask_ins=0.744, word_ins_ml=3.595, word_reposition=0.761, kpe=0.671, ppl=54.6, wps=7056.2, ups=0.35, wpb=19934.8, bsz=256, num_updates=15600, lr=0.000283069, gnorm=2.663, clip=0, loss_scale=2048, train_wall=245, wall=46101
2022-07-07 03:46:24 | INFO | train | epoch 014 | loss nan | nll_loss 1.927 | mask_ins 0.745 | word_ins_ml 3.593 | word_reposition 0.756 | kpe nan | ppl nan | wps 6762 | ups 0.34 | wpb 19913.9 | bsz 255.8 | num_updates 15687 | lr 0.000282283 | gnorm 2.555 | clip 0 | loss_scale 2596 | train_wall 2803 | wall 46345
2022-07-07 03:47:33 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.309 | nll_loss 5.575 | mask_ins 1.496 | word_ins_ml 6.966 | word_reposition 1.3 | kpe 1.548 | ppl 2537.65 | wps 13839 | wpb 2279.4 | bsz 32 | num_updates 15687 | best_loss 11.272
2022-07-07 03:47:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 14 @ 15687 updates, score 11.309) (writing took 6.471809734590352 seconds)
2022-07-07 03:48:16 | INFO | train_inner | epoch 015:     13 / 1122 loss=nan, nll_loss=1.913, mask_ins=0.734, word_ins_ml=3.579, word_reposition=0.752, kpe=nan, ppl=nan, wps=5570.1, ups=0.28, wpb=19839, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=2.626, clip=0, loss_scale=2048, train_wall=244, wall=46457
2022-07-07 03:52:59 | INFO | train_inner | epoch 015:    113 / 1122 loss=5.649, nll_loss=1.883, mask_ins=0.737, word_ins_ml=3.553, word_reposition=0.749, kpe=0.609, ppl=50.17, wps=7079.8, ups=0.35, wpb=20011.9, bsz=256, num_updates=15800, lr=0.000281272, gnorm=2.568, clip=0, loss_scale=2048, train_wall=245, wall=46740
2022-07-07 03:57:41 | INFO | train_inner | epoch 015:    213 / 1122 loss=5.646, nll_loss=1.883, mask_ins=0.734, word_ins_ml=3.553, word_reposition=0.749, kpe=0.61, ppl=50.09, wps=7068.5, ups=0.35, wpb=19918.1, bsz=256, num_updates=15900, lr=0.000280386, gnorm=2.618, clip=0, loss_scale=2273, train_wall=245, wall=47021
2022-07-07 04:02:24 | INFO | train_inner | epoch 015:    313 / 1122 loss=nan, nll_loss=1.873, mask_ins=0.737, word_ins_ml=3.543, word_reposition=0.749, kpe=nan, ppl=nan, wps=7064.6, ups=0.35, wpb=19984.1, bsz=256, num_updates=16000, lr=0.000279508, gnorm=2.598, clip=0, loss_scale=4096, train_wall=245, wall=47304
2022-07-07 04:07:17 | INFO | train_inner | epoch 015:    413 / 1122 loss=5.657, nll_loss=1.868, mask_ins=0.742, word_ins_ml=3.539, word_reposition=0.755, kpe=0.621, ppl=50.47, wps=6795.1, ups=0.34, wpb=19948, bsz=256, num_updates=16100, lr=0.000278639, gnorm=2.696, clip=0, loss_scale=4096, train_wall=255, wall=47598
2022-07-07 04:12:06 | INFO | train_inner | epoch 015:    513 / 1122 loss=5.682, nll_loss=1.904, mask_ins=0.734, word_ins_ml=3.571, word_reposition=0.751, kpe=0.626, ppl=51.35, wps=6880, ups=0.35, wpb=19870, bsz=256, num_updates=16200, lr=0.000277778, gnorm=2.809, clip=0, loss_scale=4096, train_wall=250, wall=47887
2022-07-07 04:12:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 04:17:30 | INFO | train_inner | epoch 015:    614 / 1122 loss=5.665, nll_loss=1.881, mask_ins=0.733, word_ins_ml=3.551, word_reposition=0.749, kpe=0.632, ppl=50.73, wps=6121.1, ups=0.31, wpb=19865.5, bsz=256, num_updates=16300, lr=0.000276924, gnorm=2.586, clip=0, loss_scale=2312, train_wall=286, wall=48211
2022-07-07 04:22:13 | INFO | train_inner | epoch 015:    714 / 1122 loss=5.706, nll_loss=1.926, mask_ins=0.735, word_ins_ml=3.59, word_reposition=0.753, kpe=0.629, ppl=52.19, wps=7033.5, ups=0.35, wpb=19837.8, bsz=256, num_updates=16400, lr=0.000276079, gnorm=2.546, clip=0, loss_scale=2048, train_wall=245, wall=48493
2022-07-07 04:26:55 | INFO | train_inner | epoch 015:    814 / 1122 loss=5.685, nll_loss=1.907, mask_ins=0.732, word_ins_ml=3.572, word_reposition=0.746, kpe=0.635, ppl=51.45, wps=7086.3, ups=0.35, wpb=19990, bsz=256, num_updates=16500, lr=0.000275241, gnorm=2.569, clip=0, loss_scale=2048, train_wall=245, wall=48775
2022-07-07 04:31:36 | INFO | train_inner | epoch 015:    914 / 1122 loss=5.691, nll_loss=1.901, mask_ins=0.74, word_ins_ml=3.568, word_reposition=0.748, kpe=0.635, ppl=51.65, wps=7045.3, ups=0.36, wpb=19841.7, bsz=256, num_updates=16600, lr=0.000274411, gnorm=2.497, clip=0, loss_scale=2048, train_wall=245, wall=49057
2022-07-07 04:36:17 | INFO | train_inner | epoch 015:   1014 / 1122 loss=5.67, nll_loss=1.878, mask_ins=0.732, word_ins_ml=3.547, word_reposition=0.75, kpe=0.641, ppl=50.91, wps=7094.3, ups=0.36, wpb=19932, bsz=256, num_updates=16700, lr=0.000273588, gnorm=2.547, clip=0, loss_scale=2048, train_wall=244, wall=49338
2022-07-07 04:40:59 | INFO | train_inner | epoch 015:   1114 / 1122 loss=nan, nll_loss=1.854, mask_ins=0.729, word_ins_ml=3.525, word_reposition=0.749, kpe=nan, ppl=nan, wps=7118.3, ups=0.35, wpb=20055.6, bsz=256, num_updates=16800, lr=0.000272772, gnorm=2.56, clip=0, loss_scale=3604, train_wall=244, wall=49620
2022-07-07 04:41:20 | INFO | train | epoch 015 | loss nan | nll_loss 1.888 | mask_ins 0.735 | word_ins_ml 3.556 | word_reposition 0.75 | kpe nan | ppl nan | wps 6772.6 | ups 0.34 | wpb 19913.1 | bsz 255.8 | num_updates 16808 | lr 0.000272707 | gnorm 2.613 | clip 0 | loss_scale 2793 | train_wall 2800 | wall 49641
2022-07-07 04:42:29 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 11.693 | nll_loss 5.719 | mask_ins 1.531 | word_ins_ml 7.091 | word_reposition 1.307 | kpe 1.764 | ppl 3310.26 | wps 13844.3 | wpb 2279.4 | bsz 32 | num_updates 16808 | best_loss 11.272
2022-07-07 04:42:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 15 @ 16808 updates, score 11.693) (writing took 6.243447847664356 seconds)
2022-07-07 04:46:54 | INFO | train_inner | epoch 016:     92 / 1122 loss=nan, nll_loss=1.847, mask_ins=0.728, word_ins_ml=3.52, word_reposition=0.74, kpe=nan, ppl=nan, wps=5534.5, ups=0.28, wpb=19670.5, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=2.71, clip=0, loss_scale=4096, train_wall=244, wall=49975
2022-07-07 04:51:37 | INFO | train_inner | epoch 016:    192 / 1122 loss=5.588, nll_loss=1.865, mask_ins=0.725, word_ins_ml=3.534, word_reposition=0.745, kpe=0.583, ppl=48.1, wps=7067.8, ups=0.35, wpb=19947.5, bsz=256, num_updates=17000, lr=0.000271163, gnorm=2.594, clip=0, loss_scale=4096, train_wall=246, wall=50257
2022-07-07 04:56:18 | INFO | train_inner | epoch 016:    292 / 1122 loss=nan, nll_loss=1.836, mask_ins=0.737, word_ins_ml=3.509, word_reposition=0.742, kpe=nan, ppl=nan, wps=7102, ups=0.35, wpb=20007.5, bsz=256, num_updates=17100, lr=0.000270369, gnorm=2.582, clip=0, loss_scale=4096, train_wall=245, wall=50539
2022-07-07 05:01:00 | INFO | train_inner | epoch 016:    392 / 1122 loss=5.581, nll_loss=1.856, mask_ins=0.722, word_ins_ml=3.527, word_reposition=0.741, kpe=0.591, ppl=47.86, wps=7042.3, ups=0.35, wpb=19857.2, bsz=256, num_updates=17200, lr=0.000269582, gnorm=2.626, clip=0, loss_scale=4096, train_wall=245, wall=50821
2022-07-07 05:05:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-07 05:05:45 | INFO | train_inner | epoch 016:    493 / 1122 loss=5.561, nll_loss=1.826, mask_ins=0.726, word_ins_ml=3.5, word_reposition=0.745, kpe=0.589, ppl=47.2, wps=7017.1, ups=0.35, wpb=19971.6, bsz=256, num_updates=17300, lr=0.000268802, gnorm=2.604, clip=0, loss_scale=6164, train_wall=247, wall=51106
2022-07-07 05:10:27 | INFO | train_inner | epoch 016:    593 / 1122 loss=5.598, nll_loss=1.858, mask_ins=0.728, word_ins_ml=3.528, word_reposition=0.746, kpe=0.595, ppl=48.44, wps=7084, ups=0.35, wpb=19980.1, bsz=256, num_updates=17400, lr=0.000268028, gnorm=2.675, clip=0, loss_scale=4096, train_wall=245, wall=51388
2022-07-07 05:12:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 05:15:11 | INFO | train_inner | epoch 016:    694 / 1122 loss=5.557, nll_loss=1.831, mask_ins=0.713, word_ins_ml=3.505, word_reposition=0.739, kpe=0.6, ppl=47.07, wps=7013.5, ups=0.35, wpb=19955.3, bsz=256, num_updates=17500, lr=0.000267261, gnorm=2.59, clip=0, loss_scale=2697, train_wall=247, wall=51672
2022-07-07 05:20:44 | INFO | train_inner | epoch 016:    794 / 1122 loss=5.612, nll_loss=1.856, mask_ins=0.732, word_ins_ml=3.526, word_reposition=0.75, kpe=0.603, ppl=48.9, wps=5983.5, ups=0.3, wpb=19875, bsz=256, num_updates=17600, lr=0.000266501, gnorm=2.624, clip=0, loss_scale=2048, train_wall=295, wall=52004
2022-07-07 05:21:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 05:25:28 | INFO | train_inner | epoch 016:    895 / 1122 loss=5.58, nll_loss=1.832, mask_ins=0.719, word_ins_ml=3.505, word_reposition=0.745, kpe=0.611, ppl=47.85, wps=7021, ups=0.35, wpb=19971.8, bsz=256, num_updates=17700, lr=0.000265747, gnorm=2.779, clip=0, loss_scale=1237, train_wall=247, wall=52289
2022-07-07 05:30:10 | INFO | train_inner | epoch 016:    995 / 1122 loss=5.646, nll_loss=1.884, mask_ins=0.729, word_ins_ml=3.55, word_reposition=0.749, kpe=0.617, ppl=50.07, wps=7077.2, ups=0.36, wpb=19925.8, bsz=256, num_updates=17800, lr=0.000264999, gnorm=2.786, clip=0, loss_scale=1024, train_wall=245, wall=52570
2022-07-07 05:34:51 | INFO | train_inner | epoch 016:   1095 / 1122 loss=5.624, nll_loss=1.89, mask_ins=0.721, word_ins_ml=3.556, word_reposition=0.741, kpe=0.605, ppl=49.31, wps=7053.2, ups=0.35, wpb=19874.2, bsz=256, num_updates=17900, lr=0.000264258, gnorm=2.55, clip=0, loss_scale=1024, train_wall=245, wall=52852
2022-07-07 05:36:06 | INFO | train | epoch 016 | loss nan | nll_loss 1.854 | mask_ins 0.726 | word_ins_ml 3.525 | word_reposition 0.744 | kpe nan | ppl nan | wps 6781.9 | ups 0.34 | wpb 19915 | bsz 255.8 | num_updates 17927 | lr 0.000264059 | gnorm 2.646 | clip 0 | loss_scale 3095 | train_wall 2796 | wall 52927
2022-07-07 05:37:15 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 11.395 | nll_loss 5.547 | mask_ins 1.503 | word_ins_ml 6.93 | word_reposition 1.258 | kpe 1.704 | ppl 2692.65 | wps 13967.9 | wpb 2279.4 | bsz 32 | num_updates 17927 | best_loss 11.272
2022-07-07 05:37:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 16 @ 17927 updates, score 11.395) (writing took 6.012888427823782 seconds)
2022-07-07 05:40:46 | INFO | train_inner | epoch 017:     73 / 1122 loss=5.523, nll_loss=1.817, mask_ins=0.721, word_ins_ml=3.492, word_reposition=0.74, kpe=0.571, ppl=45.99, wps=5601, ups=0.28, wpb=19863.5, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=2.691, clip=0, loss_scale=1024, train_wall=244, wall=53207
2022-07-07 05:45:28 | INFO | train_inner | epoch 017:    173 / 1122 loss=5.516, nll_loss=1.83, mask_ins=0.727, word_ins_ml=3.502, word_reposition=0.736, kpe=0.551, ppl=45.76, wps=7088.8, ups=0.35, wpb=19973.7, bsz=256, num_updates=18100, lr=0.000262794, gnorm=2.555, clip=0, loss_scale=1024, train_wall=245, wall=53489
2022-07-07 05:50:10 | INFO | train_inner | epoch 017:    273 / 1122 loss=5.529, nll_loss=1.826, mask_ins=0.733, word_ins_ml=3.499, word_reposition=0.739, kpe=0.559, ppl=46.19, wps=7034.8, ups=0.35, wpb=19840.9, bsz=256, num_updates=18200, lr=0.000262071, gnorm=2.665, clip=0, loss_scale=1720, train_wall=245, wall=53771
2022-07-07 05:54:52 | INFO | train_inner | epoch 017:    373 / 1122 loss=nan, nll_loss=1.818, mask_ins=0.72, word_ins_ml=3.492, word_reposition=0.737, kpe=nan, ppl=nan, wps=7009.2, ups=0.35, wpb=19800.3, bsz=256, num_updates=18300, lr=0.000261354, gnorm=2.694, clip=0, loss_scale=2048, train_wall=246, wall=54053
2022-07-07 05:59:34 | INFO | train_inner | epoch 017:    473 / 1122 loss=5.507, nll_loss=1.816, mask_ins=0.716, word_ins_ml=3.49, word_reposition=0.734, kpe=0.568, ppl=45.48, wps=7054.9, ups=0.35, wpb=19882.3, bsz=256, num_updates=18400, lr=0.000260643, gnorm=2.705, clip=0, loss_scale=2048, train_wall=245, wall=54335
2022-07-07 06:04:15 | INFO | train_inner | epoch 017:    573 / 1122 loss=5.516, nll_loss=1.829, mask_ins=0.715, word_ins_ml=3.502, word_reposition=0.733, kpe=0.567, ppl=45.76, wps=7072.1, ups=0.36, wpb=19891.2, bsz=256, num_updates=18500, lr=0.000259938, gnorm=2.635, clip=0, loss_scale=2048, train_wall=245, wall=54616
2022-07-07 06:08:57 | INFO | train_inner | epoch 017:    673 / 1122 loss=5.484, nll_loss=1.775, mask_ins=0.718, word_ins_ml=3.453, word_reposition=0.739, kpe=0.574, ppl=44.74, wps=7091.4, ups=0.35, wpb=19988.6, bsz=256, num_updates=18600, lr=0.000259238, gnorm=2.643, clip=0, loss_scale=2048, train_wall=245, wall=54898
2022-07-07 06:13:39 | INFO | train_inner | epoch 017:    773 / 1122 loss=5.478, nll_loss=1.78, mask_ins=0.712, word_ins_ml=3.458, word_reposition=0.733, kpe=0.575, ppl=44.56, wps=7127.6, ups=0.36, wpb=20072.5, bsz=256, num_updates=18700, lr=0.000258544, gnorm=2.567, clip=0, loss_scale=3195, train_wall=245, wall=55180
2022-07-07 06:18:20 | INFO | train_inner | epoch 017:    873 / 1122 loss=5.498, nll_loss=1.789, mask_ins=0.72, word_ins_ml=3.465, word_reposition=0.736, kpe=0.576, ppl=45.18, wps=7065.8, ups=0.36, wpb=19879.8, bsz=256, num_updates=18800, lr=0.000257855, gnorm=2.593, clip=0, loss_scale=4096, train_wall=244, wall=55461
2022-07-07 06:23:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 06:23:51 | INFO | train_inner | epoch 017:    974 / 1122 loss=5.562, nll_loss=1.851, mask_ins=0.72, word_ins_ml=3.52, word_reposition=0.738, kpe=0.584, ppl=47.25, wps=5999.2, ups=0.3, wpb=19862.9, bsz=256, num_updates=18900, lr=0.000257172, gnorm=2.596, clip=0, loss_scale=3832, train_wall=294, wall=55792
2022-07-07 06:28:33 | INFO | train_inner | epoch 017:   1074 / 1122 loss=nan, nll_loss=1.767, mask_ins=0.714, word_ins_ml=3.445, word_reposition=0.733, kpe=nan, ppl=nan, wps=7099.5, ups=0.36, wpb=19975.6, bsz=256, num_updates=19000, lr=0.000256495, gnorm=2.624, clip=0, loss_scale=2048, train_wall=245, wall=56074
2022-07-07 06:30:47 | INFO | train | epoch 017 | loss nan | nll_loss 1.805 | mask_ins 0.719 | word_ins_ml 3.48 | word_reposition 0.736 | kpe nan | ppl nan | wps 6803.6 | ups 0.34 | wpb 19912.1 | bsz 255.8 | num_updates 19048 | lr 0.000256171 | gnorm 2.634 | clip 0 | loss_scale 2306 | train_wall 2793 | wall 56208
2022-07-07 06:31:56 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 11.472 | nll_loss 5.507 | mask_ins 1.533 | word_ins_ml 6.883 | word_reposition 1.3 | kpe 1.756 | ppl 2841.1 | wps 13926.9 | wpb 2279.4 | bsz 32 | num_updates 19048 | best_loss 11.272
2022-07-07 06:32:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 17 @ 19048 updates, score 11.472) (writing took 6.126970465295017 seconds)
2022-07-07 06:34:27 | INFO | train_inner | epoch 018:     52 / 1122 loss=5.462, nll_loss=1.778, mask_ins=0.711, word_ins_ml=3.456, word_reposition=0.737, kpe=0.559, ppl=44.09, wps=5553.8, ups=0.28, wpb=19698.5, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=2.73, clip=0, loss_scale=2048, train_wall=243, wall=56428
2022-07-07 06:39:09 | INFO | train_inner | epoch 018:    152 / 1122 loss=5.4, nll_loss=1.74, mask_ins=0.716, word_ins_ml=3.422, word_reposition=0.737, kpe=0.524, ppl=42.22, wps=7110.9, ups=0.36, wpb=19991.9, bsz=256, num_updates=19200, lr=0.000255155, gnorm=2.565, clip=0, loss_scale=2048, train_wall=244, wall=56709
2022-07-07 06:43:50 | INFO | train_inner | epoch 018:    252 / 1122 loss=5.411, nll_loss=1.761, mask_ins=0.709, word_ins_ml=3.44, word_reposition=0.728, kpe=0.533, ppl=42.54, wps=7100.2, ups=0.36, wpb=19993.6, bsz=256, num_updates=19300, lr=0.000254493, gnorm=2.641, clip=0, loss_scale=2048, train_wall=245, wall=56991
2022-07-07 06:48:31 | INFO | train_inner | epoch 018:    352 / 1122 loss=5.434, nll_loss=1.776, mask_ins=0.712, word_ins_ml=3.453, word_reposition=0.731, kpe=0.537, ppl=43.24, wps=7057.2, ups=0.36, wpb=19845.4, bsz=256, num_updates=19400, lr=0.000253837, gnorm=2.617, clip=0, loss_scale=2068, train_wall=244, wall=57272
2022-07-07 06:53:13 | INFO | train_inner | epoch 018:    452 / 1122 loss=5.437, nll_loss=1.776, mask_ins=0.71, word_ins_ml=3.454, word_reposition=0.734, kpe=0.54, ppl=43.33, wps=7087, ups=0.35, wpb=19975.5, bsz=256, num_updates=19500, lr=0.000253185, gnorm=2.635, clip=0, loss_scale=4096, train_wall=245, wall=57554
2022-07-07 06:53:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 06:54:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 06:58:00 | INFO | train_inner | epoch 018:    554 / 1122 loss=5.432, nll_loss=1.775, mask_ins=0.706, word_ins_ml=3.453, word_reposition=0.729, kpe=0.545, ppl=43.18, wps=6918.1, ups=0.35, wpb=19834.5, bsz=256, num_updates=19600, lr=0.000252538, gnorm=2.59, clip=0, loss_scale=1416, train_wall=249, wall=57841
2022-07-07 07:02:41 | INFO | train_inner | epoch 018:    654 / 1122 loss=nan, nll_loss=1.798, mask_ins=0.701, word_ins_ml=3.473, word_reposition=0.729, kpe=nan, ppl=nan, wps=7063.6, ups=0.36, wpb=19879.3, bsz=256, num_updates=19700, lr=0.000251896, gnorm=2.726, clip=0, loss_scale=1024, train_wall=245, wall=58122
2022-07-07 07:07:23 | INFO | train_inner | epoch 018:    754 / 1122 loss=nan, nll_loss=1.806, mask_ins=0.715, word_ins_ml=3.48, word_reposition=0.738, kpe=nan, ppl=nan, wps=7111.5, ups=0.36, wpb=20017.5, bsz=256, num_updates=19800, lr=0.000251259, gnorm=2.602, clip=0, loss_scale=1024, train_wall=244, wall=58404
2022-07-07 07:12:04 | INFO | train_inner | epoch 018:    854 / 1122 loss=5.468, nll_loss=1.795, mask_ins=0.714, word_ins_ml=3.47, word_reposition=0.73, kpe=0.554, ppl=44.26, wps=7125.3, ups=0.36, wpb=20049.9, bsz=256, num_updates=19900, lr=0.000250627, gnorm=2.674, clip=0, loss_scale=1024, train_wall=245, wall=58685
2022-07-07 07:16:46 | INFO | train_inner | epoch 018:    954 / 1122 loss=5.43, nll_loss=1.76, mask_ins=0.706, word_ins_ml=3.438, word_reposition=0.732, kpe=0.553, ppl=43.11, wps=7052.4, ups=0.36, wpb=19842.1, bsz=256, num_updates=20000, lr=0.00025, gnorm=2.639, clip=0, loss_scale=1024, train_wall=244, wall=58966
2022-07-07 07:21:27 | INFO | train_inner | epoch 018:   1054 / 1122 loss=5.459, nll_loss=1.79, mask_ins=0.712, word_ins_ml=3.465, word_reposition=0.728, kpe=0.554, ppl=43.99, wps=7060.2, ups=0.35, wpb=19891.7, bsz=256, num_updates=20100, lr=0.000249377, gnorm=2.628, clip=0, loss_scale=1710, train_wall=245, wall=59248
2022-07-07 07:25:15 | INFO | train | epoch 018 | loss nan | nll_loss 1.779 | mask_ins 0.711 | word_ins_ml 3.456 | word_reposition 0.732 | kpe nan | ppl nan | wps 6825 | ups 0.34 | wpb 19913.2 | bsz 255.8 | num_updates 20168 | lr 0.000248957 | gnorm 2.642 | clip 0 | loss_scale 1780 | train_wall 2780 | wall 59476
2022-07-07 07:26:23 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 11.465 | nll_loss 5.585 | mask_ins 1.492 | word_ins_ml 6.966 | word_reposition 1.274 | kpe 1.733 | ppl 2827.45 | wps 14018.7 | wpb 2279.4 | bsz 32 | num_updates 20168 | best_loss 11.272
2022-07-07 07:26:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 18 @ 20168 updates, score 11.465) (writing took 5.973454683087766 seconds)
2022-07-07 07:27:59 | INFO | train_inner | epoch 019:     32 / 1122 loss=5.454, nll_loss=1.783, mask_ins=0.719, word_ins_ml=3.459, word_reposition=0.735, kpe=0.542, ppl=43.84, wps=5032.8, ups=0.26, wpb=19709.8, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=2.695, clip=0, loss_scale=2048, train_wall=281, wall=59640
2022-07-07 07:32:41 | INFO | train_inner | epoch 019:    132 / 1122 loss=5.348, nll_loss=1.729, mask_ins=0.711, word_ins_ml=3.411, word_reposition=0.721, kpe=0.505, ppl=40.72, wps=7011.3, ups=0.36, wpb=19740.2, bsz=256, num_updates=20300, lr=0.000248146, gnorm=2.589, clip=0, loss_scale=2048, train_wall=245, wall=59921
2022-07-07 07:37:23 | INFO | train_inner | epoch 019:    232 / 1122 loss=nan, nll_loss=1.736, mask_ins=0.707, word_ins_ml=3.417, word_reposition=0.736, kpe=nan, ppl=nan, wps=7051, ups=0.35, wpb=19881.1, bsz=256, num_updates=20400, lr=0.000247537, gnorm=2.617, clip=0, loss_scale=2048, train_wall=245, wall=60203
2022-07-07 07:42:04 | INFO | train_inner | epoch 019:    332 / 1122 loss=5.398, nll_loss=1.767, mask_ins=0.713, word_ins_ml=3.444, word_reposition=0.729, kpe=0.512, ppl=42.16, wps=7070.3, ups=0.35, wpb=19933, bsz=256, num_updates=20500, lr=0.000246932, gnorm=2.546, clip=0, loss_scale=2048, train_wall=245, wall=60485
2022-07-07 07:46:47 | INFO | train_inner | epoch 019:    432 / 1122 loss=5.38, nll_loss=1.758, mask_ins=0.705, word_ins_ml=3.436, word_reposition=0.724, kpe=0.515, ppl=41.65, wps=7020, ups=0.35, wpb=19809, bsz=256, num_updates=20600, lr=0.000246332, gnorm=2.682, clip=0, loss_scale=3174, train_wall=245, wall=60767
2022-07-07 07:48:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 07:51:31 | INFO | train_inner | epoch 019:    533 / 1122 loss=5.378, nll_loss=1.749, mask_ins=0.699, word_ins_ml=3.428, word_reposition=0.734, kpe=0.517, ppl=41.57, wps=7052.3, ups=0.35, wpb=20063.2, bsz=256, num_updates=20700, lr=0.000245737, gnorm=2.596, clip=0, loss_scale=2839, train_wall=247, wall=61052
2022-07-07 07:56:14 | INFO | train_inner | epoch 019:    633 / 1122 loss=5.37, nll_loss=1.736, mask_ins=0.707, word_ins_ml=3.416, word_reposition=0.726, kpe=0.52, ppl=41.35, wps=7082.2, ups=0.35, wpb=19996, bsz=256, num_updates=20800, lr=0.000245145, gnorm=2.644, clip=0, loss_scale=2048, train_wall=245, wall=61334
2022-07-07 08:00:56 | INFO | train_inner | epoch 019:    733 / 1122 loss=5.358, nll_loss=1.73, mask_ins=0.696, word_ins_ml=3.411, word_reposition=0.727, kpe=0.524, ppl=41, wps=7107, ups=0.35, wpb=20092.1, bsz=256, num_updates=20900, lr=0.000244558, gnorm=2.739, clip=0, loss_scale=2048, train_wall=245, wall=61617
2022-07-07 08:05:38 | INFO | train_inner | epoch 019:    833 / 1122 loss=5.379, nll_loss=1.749, mask_ins=0.699, word_ins_ml=3.428, word_reposition=0.725, kpe=0.527, ppl=41.63, wps=7063.9, ups=0.35, wpb=19931, bsz=256, num_updates=21000, lr=0.000243975, gnorm=2.615, clip=0, loss_scale=2048, train_wall=245, wall=61899
2022-07-07 08:10:19 | INFO | train_inner | epoch 019:    933 / 1122 loss=nan, nll_loss=1.743, mask_ins=0.698, word_ins_ml=3.422, word_reposition=0.725, kpe=nan, ppl=nan, wps=7061, ups=0.36, wpb=19848.2, bsz=256, num_updates=21100, lr=0.000243396, gnorm=2.627, clip=0, loss_scale=2048, train_wall=244, wall=62180
2022-07-07 08:15:02 | INFO | train_inner | epoch 019:   1033 / 1122 loss=5.38, nll_loss=1.743, mask_ins=0.698, word_ins_ml=3.422, word_reposition=0.729, kpe=0.531, ppl=41.65, wps=7097, ups=0.35, wpb=20020.3, bsz=256, num_updates=21200, lr=0.000242821, gnorm=2.598, clip=0, loss_scale=3072, train_wall=245, wall=62462
2022-07-07 08:18:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 08:19:11 | INFO | train | epoch 019 | loss nan | nll_loss 1.747 | mask_ins 0.705 | word_ins_ml 3.426 | word_reposition 0.729 | kpe nan | ppl nan | wps 6891.6 | ups 0.35 | wpb 19913.7 | bsz 255.8 | num_updates 21288 | lr 0.000242319 | gnorm 2.627 | clip 0 | loss_scale 2440 | train_wall 2746 | wall 62712
2022-07-07 08:20:20 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.304 | nll_loss 5.474 | mask_ins 1.479 | word_ins_ml 6.852 | word_reposition 1.289 | kpe 1.684 | ppl 2528.48 | wps 13916.6 | wpb 2279.4 | bsz 32 | num_updates 21288 | best_loss 11.272
2022-07-07 08:20:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 19 @ 21288 updates, score 11.304) (writing took 6.1416885033249855 seconds)
2022-07-07 08:21:00 | INFO | train_inner | epoch 020:     12 / 1122 loss=5.442, nll_loss=1.769, mask_ins=0.72, word_ins_ml=3.445, word_reposition=0.741, kpe=0.536, ppl=43.47, wps=5531.3, ups=0.28, wpb=19813.9, bsz=253.8, num_updates=21300, lr=0.000242251, gnorm=2.736, clip=0, loss_scale=3488, train_wall=246, wall=62821
2022-07-07 08:22:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 08:25:45 | INFO | train_inner | epoch 020:    113 / 1122 loss=5.291, nll_loss=1.713, mask_ins=0.694, word_ins_ml=3.396, word_reposition=0.718, kpe=0.482, ppl=39.16, wps=6968.9, ups=0.35, wpb=19875.8, bsz=256, num_updates=21400, lr=0.000241684, gnorm=2.631, clip=0, loss_scale=1288, train_wall=248, wall=63106
2022-07-07 08:31:04 | INFO | train_inner | epoch 020:    213 / 1122 loss=5.325, nll_loss=1.715, mask_ins=0.708, word_ins_ml=3.398, word_reposition=0.729, kpe=0.49, ppl=40.08, wps=6255.8, ups=0.31, wpb=19941.9, bsz=256, num_updates=21500, lr=0.000241121, gnorm=2.662, clip=0, loss_scale=1024, train_wall=281, wall=63425
2022-07-07 08:35:46 | INFO | train_inner | epoch 020:    313 / 1122 loss=5.35, nll_loss=1.75, mask_ins=0.702, word_ins_ml=3.428, word_reposition=0.729, kpe=0.492, ppl=40.79, wps=7086, ups=0.35, wpb=19990.8, bsz=256, num_updates=21600, lr=0.000240563, gnorm=2.63, clip=0, loss_scale=1024, train_wall=245, wall=63707
2022-07-07 08:40:28 | INFO | train_inner | epoch 020:    413 / 1122 loss=nan, nll_loss=1.71, mask_ins=0.7, word_ins_ml=3.393, word_reposition=0.723, kpe=nan, ppl=nan, wps=7092.8, ups=0.35, wpb=19985.9, bsz=256, num_updates=21700, lr=0.000240008, gnorm=2.613, clip=0, loss_scale=1024, train_wall=245, wall=63988
2022-07-07 08:45:09 | INFO | train_inner | epoch 020:    513 / 1122 loss=5.294, nll_loss=1.708, mask_ins=0.685, word_ins_ml=3.391, word_reposition=0.721, kpe=0.496, ppl=39.22, wps=7089.2, ups=0.36, wpb=19911.4, bsz=256, num_updates=21800, lr=0.000239457, gnorm=2.641, clip=0, loss_scale=1024, train_wall=244, wall=64269
2022-07-07 08:49:50 | INFO | train_inner | epoch 020:    613 / 1122 loss=5.314, nll_loss=1.706, mask_ins=0.7, word_ins_ml=3.389, word_reposition=0.724, kpe=0.501, ppl=39.79, wps=7098.8, ups=0.36, wpb=19965.1, bsz=256, num_updates=21900, lr=0.000238909, gnorm=2.581, clip=0, loss_scale=1669, train_wall=245, wall=64551
2022-07-07 08:54:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 08:54:34 | INFO | train_inner | epoch 020:    714 / 1122 loss=5.324, nll_loss=1.718, mask_ins=0.699, word_ins_ml=3.399, word_reposition=0.725, kpe=0.501, ppl=40.07, wps=6980.8, ups=0.35, wpb=19826.2, bsz=256, num_updates=22000, lr=0.000238366, gnorm=2.636, clip=0, loss_scale=1977, train_wall=247, wall=64835
2022-07-07 08:59:15 | INFO | train_inner | epoch 020:    814 / 1122 loss=5.299, nll_loss=1.697, mask_ins=0.698, word_ins_ml=3.381, word_reposition=0.715, kpe=0.505, ppl=39.37, wps=7069.1, ups=0.36, wpb=19900, bsz=256, num_updates=22100, lr=0.000237826, gnorm=2.662, clip=0, loss_scale=1024, train_wall=245, wall=65116
2022-07-07 09:03:57 | INFO | train_inner | epoch 020:    914 / 1122 loss=nan, nll_loss=1.686, mask_ins=0.699, word_ins_ml=3.371, word_reposition=0.726, kpe=nan, ppl=nan, wps=7096.5, ups=0.35, wpb=19996.9, bsz=256, num_updates=22200, lr=0.000237289, gnorm=2.646, clip=0, loss_scale=1024, train_wall=245, wall=65398
2022-07-07 09:08:39 | INFO | train_inner | epoch 020:   1014 / 1122 loss=5.336, nll_loss=1.721, mask_ins=0.697, word_ins_ml=3.402, word_reposition=0.726, kpe=0.512, ppl=40.4, wps=7053.4, ups=0.35, wpb=19899.3, bsz=256, num_updates=22300, lr=0.000236757, gnorm=2.611, clip=0, loss_scale=1024, train_wall=245, wall=65680
2022-07-07 09:13:21 | INFO | train_inner | epoch 020:   1114 / 1122 loss=5.33, nll_loss=1.722, mask_ins=0.694, word_ins_ml=3.402, word_reposition=0.717, kpe=0.517, ppl=40.21, wps=7066.7, ups=0.35, wpb=19915.4, bsz=256, num_updates=22400, lr=0.000236228, gnorm=2.709, clip=0, loss_scale=1024, train_wall=245, wall=65962
2022-07-07 09:13:43 | INFO | train | epoch 020 | loss nan | nll_loss 1.713 | mask_ins 0.698 | word_ins_ml 3.396 | word_reposition 0.723 | kpe nan | ppl nan | wps 6817.7 | ups 0.34 | wpb 19913.8 | bsz 255.8 | num_updates 22408 | lr 0.000236186 | gnorm 2.652 | clip 0 | loss_scale 1202 | train_wall 2782 | wall 65984
2022-07-07 09:14:51 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.433 | nll_loss 5.591 | mask_ins 1.489 | word_ins_ml 6.965 | word_reposition 1.233 | kpe 1.746 | ppl 2764.05 | wps 13943.9 | wpb 2279.4 | bsz 32 | num_updates 22408 | best_loss 11.272
2022-07-07 09:14:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 20 @ 22408 updates, score 11.433) (writing took 6.2719674268737435 seconds)
2022-07-07 09:19:15 | INFO | train_inner | epoch 021:     92 / 1122 loss=5.256, nll_loss=1.682, mask_ins=0.699, word_ins_ml=3.367, word_reposition=0.719, kpe=0.471, ppl=38.2, wps=5591, ups=0.28, wpb=19816.5, bsz=253.8, num_updates=22500, lr=0.000235702, gnorm=2.844, clip=0, loss_scale=1024, train_wall=243, wall=66316
2022-07-07 09:23:57 | INFO | train_inner | epoch 021:    192 / 1122 loss=5.252, nll_loss=1.69, mask_ins=0.694, word_ins_ml=3.375, word_reposition=0.719, kpe=0.464, ppl=38.1, wps=7044.1, ups=0.35, wpb=19849.1, bsz=256, num_updates=22600, lr=0.00023518, gnorm=2.459, clip=0, loss_scale=1997, train_wall=245, wall=66598
2022-07-07 09:28:39 | INFO | train_inner | epoch 021:    292 / 1122 loss=nan, nll_loss=1.699, mask_ins=0.691, word_ins_ml=3.382, word_reposition=0.719, kpe=nan, ppl=nan, wps=7117.3, ups=0.36, wpb=20017.6, bsz=256, num_updates=22700, lr=0.000234662, gnorm=2.641, clip=0, loss_scale=2048, train_wall=244, wall=66879
2022-07-07 09:34:08 | INFO | train_inner | epoch 021:    392 / 1122 loss=5.256, nll_loss=1.681, mask_ins=0.696, word_ins_ml=3.366, word_reposition=0.718, kpe=0.476, ppl=38.21, wps=6050.4, ups=0.3, wpb=19927.7, bsz=256, num_updates=22800, lr=0.000234146, gnorm=2.535, clip=0, loss_scale=2048, train_wall=292, wall=67209
2022-07-07 09:38:50 | INFO | train_inner | epoch 021:    492 / 1122 loss=nan, nll_loss=1.713, mask_ins=0.697, word_ins_ml=3.395, word_reposition=0.724, kpe=nan, ppl=nan, wps=7074.7, ups=0.35, wpb=19984.7, bsz=256, num_updates=22900, lr=0.000233635, gnorm=2.573, clip=0, loss_scale=2048, train_wall=245, wall=67491
2022-07-07 09:43:32 | INFO | train_inner | epoch 021:    592 / 1122 loss=5.268, nll_loss=1.688, mask_ins=0.693, word_ins_ml=3.372, word_reposition=0.719, kpe=0.483, ppl=38.53, wps=7063, ups=0.35, wpb=19908.4, bsz=256, num_updates=23000, lr=0.000233126, gnorm=2.611, clip=0, loss_scale=2048, train_wall=245, wall=67773
2022-07-07 09:44:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 09:48:17 | INFO | train_inner | epoch 021:    693 / 1122 loss=5.251, nll_loss=1.684, mask_ins=0.685, word_ins_ml=3.368, word_reposition=0.718, kpe=0.48, ppl=38.07, wps=6978.6, ups=0.35, wpb=19854.6, bsz=256, num_updates=23100, lr=0.000232621, gnorm=2.66, clip=0, loss_scale=2271, train_wall=247, wall=68058
2022-07-07 09:52:59 | INFO | train_inner | epoch 021:    793 / 1122 loss=5.25, nll_loss=1.666, mask_ins=0.691, word_ins_ml=3.353, word_reposition=0.721, kpe=0.485, ppl=38.05, wps=7070.6, ups=0.35, wpb=19962.4, bsz=256, num_updates=23200, lr=0.000232119, gnorm=2.557, clip=0, loss_scale=2048, train_wall=246, wall=68340
2022-07-07 09:57:41 | INFO | train_inner | epoch 021:    893 / 1122 loss=5.315, nll_loss=1.722, mask_ins=0.702, word_ins_ml=3.402, word_reposition=0.722, kpe=0.489, ppl=39.8, wps=7031.4, ups=0.35, wpb=19852.8, bsz=256, num_updates=23300, lr=0.000231621, gnorm=2.628, clip=0, loss_scale=2048, train_wall=246, wall=68622
2022-07-07 10:02:23 | INFO | train_inner | epoch 021:    993 / 1122 loss=5.27, nll_loss=1.685, mask_ins=0.692, word_ins_ml=3.369, word_reposition=0.718, kpe=0.491, ppl=38.58, wps=7102, ups=0.36, wpb=20004.6, bsz=256, num_updates=23400, lr=0.000231125, gnorm=2.716, clip=0, loss_scale=2048, train_wall=245, wall=68904
2022-07-07 10:07:04 | INFO | train_inner | epoch 021:   1093 / 1122 loss=5.32, nll_loss=1.73, mask_ins=0.697, word_ins_ml=3.409, word_reposition=0.722, kpe=0.492, ppl=39.93, wps=7057.2, ups=0.36, wpb=19848.5, bsz=256, num_updates=23500, lr=0.000230633, gnorm=2.676, clip=0, loss_scale=2048, train_wall=244, wall=69185
2022-07-07 10:08:25 | INFO | train | epoch 021 | loss nan | nll_loss 1.692 | mask_ins 0.694 | word_ins_ml 3.376 | word_reposition 0.719 | kpe nan | ppl nan | wps 6801.4 | ups 0.34 | wpb 19914.1 | bsz 255.8 | num_updates 23529 | lr 0.000230491 | gnorm 2.621 | clip 0 | loss_scale 1980 | train_wall 2794 | wall 69266
2022-07-07 10:09:33 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.493 | nll_loss 5.518 | mask_ins 1.496 | word_ins_ml 6.893 | word_reposition 1.323 | kpe 1.78 | ppl 2881.91 | wps 13933.2 | wpb 2279.4 | bsz 32 | num_updates 23529 | best_loss 11.272
2022-07-07 10:09:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 21 @ 23529 updates, score 11.493) (writing took 5.919473681598902 seconds)
2022-07-07 10:12:59 | INFO | train_inner | epoch 022:     71 / 1122 loss=5.206, nll_loss=1.665, mask_ins=0.68, word_ins_ml=3.352, word_reposition=0.713, kpe=0.462, ppl=36.92, wps=5570.8, ups=0.28, wpb=19770.3, bsz=253.8, num_updates=23600, lr=0.000230144, gnorm=2.653, clip=0, loss_scale=3297, train_wall=244, wall=69540
2022-07-07 10:17:41 | INFO | train_inner | epoch 022:    171 / 1122 loss=5.181, nll_loss=1.651, mask_ins=0.683, word_ins_ml=3.339, word_reposition=0.709, kpe=0.45, ppl=36.27, wps=7079.9, ups=0.36, wpb=19924.7, bsz=256, num_updates=23700, lr=0.000229658, gnorm=2.627, clip=0, loss_scale=4096, train_wall=245, wall=69821
2022-07-07 10:22:23 | INFO | train_inner | epoch 022:    271 / 1122 loss=5.204, nll_loss=1.662, mask_ins=0.678, word_ins_ml=3.348, word_reposition=0.723, kpe=0.455, ppl=36.87, wps=7092.9, ups=0.35, wpb=20002.6, bsz=256, num_updates=23800, lr=0.000229175, gnorm=2.636, clip=0, loss_scale=4096, train_wall=245, wall=70104
2022-07-07 10:24:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 10:27:07 | INFO | train_inner | epoch 022:    372 / 1122 loss=5.193, nll_loss=1.66, mask_ins=0.676, word_ins_ml=3.347, word_reposition=0.712, kpe=0.459, ppl=36.58, wps=7027.7, ups=0.35, wpb=20006.1, bsz=256, num_updates=23900, lr=0.000228695, gnorm=2.721, clip=0, loss_scale=3082, train_wall=247, wall=70388
2022-07-07 10:31:49 | INFO | train_inner | epoch 022:    472 / 1122 loss=5.183, nll_loss=1.648, mask_ins=0.676, word_ins_ml=3.336, word_reposition=0.71, kpe=0.461, ppl=36.33, wps=7114, ups=0.36, wpb=20004.5, bsz=256, num_updates=24000, lr=0.000228218, gnorm=2.647, clip=0, loss_scale=2048, train_wall=244, wall=70669
2022-07-07 10:37:06 | INFO | train_inner | epoch 022:    572 / 1122 loss=5.213, nll_loss=1.674, mask_ins=0.675, word_ins_ml=3.359, word_reposition=0.718, kpe=0.461, ppl=37.1, wps=6273.1, ups=0.32, wpb=19906.3, bsz=256, num_updates=24100, lr=0.000227744, gnorm=2.65, clip=0, loss_scale=2048, train_wall=280, wall=70987
2022-07-07 10:40:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 10:41:51 | INFO | train_inner | epoch 022:    673 / 1122 loss=5.251, nll_loss=1.693, mask_ins=0.686, word_ins_ml=3.376, word_reposition=0.717, kpe=0.473, ppl=38.08, wps=7030.6, ups=0.35, wpb=20029.4, bsz=256, num_updates=24200, lr=0.000227273, gnorm=2.81, clip=0, loss_scale=1673, train_wall=248, wall=71272
2022-07-07 10:46:32 | INFO | train_inner | epoch 022:    773 / 1122 loss=5.227, nll_loss=1.674, mask_ins=0.681, word_ins_ml=3.359, word_reposition=0.713, kpe=0.475, ppl=37.46, wps=7096.7, ups=0.36, wpb=19937.5, bsz=256, num_updates=24300, lr=0.000226805, gnorm=2.942, clip=0, loss_scale=1024, train_wall=244, wall=71553
2022-07-07 10:50:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-07 10:51:16 | INFO | train_inner | epoch 022:    874 / 1122 loss=5.234, nll_loss=1.686, mask_ins=0.679, word_ins_ml=3.369, word_reposition=0.713, kpe=0.472, ppl=37.65, wps=6974.3, ups=0.35, wpb=19791.8, bsz=256, num_updates=24400, lr=0.000226339, gnorm=2.713, clip=0, loss_scale=948, train_wall=247, wall=71836
2022-07-07 10:55:57 | INFO | train_inner | epoch 022:    974 / 1122 loss=nan, nll_loss=1.663, mask_ins=0.685, word_ins_ml=3.349, word_reposition=0.714, kpe=nan, ppl=nan, wps=7078.3, ups=0.35, wpb=19955.8, bsz=256, num_updates=24500, lr=0.000225877, gnorm=2.694, clip=0, loss_scale=512, train_wall=245, wall=72118
2022-07-07 11:00:39 | INFO | train_inner | epoch 022:   1074 / 1122 loss=nan, nll_loss=1.697, mask_ins=0.69, word_ins_ml=3.379, word_reposition=0.719, kpe=nan, ppl=nan, wps=7042.1, ups=0.36, wpb=19810.1, bsz=256, num_updates=24600, lr=0.000225417, gnorm=2.685, clip=0, loss_scale=512, train_wall=245, wall=72400
2022-07-07 11:02:53 | INFO | train | epoch 022 | loss nan | nll_loss 1.671 | mask_ins 0.681 | word_ins_ml 3.357 | word_reposition 0.714 | kpe nan | ppl nan | wps 6819.1 | ups 0.34 | wpb 19915.4 | bsz 255.8 | num_updates 24648 | lr 0.000225198 | gnorm 2.713 | clip 0 | loss_scale 2054 | train_wall 2780 | wall 72534
2022-07-07 11:04:02 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 11.648 | nll_loss 5.569 | mask_ins 1.492 | word_ins_ml 6.947 | word_reposition 1.263 | kpe 1.946 | ppl 3210.01 | wps 13894.2 | wpb 2279.4 | bsz 32 | num_updates 24648 | best_loss 11.272
2022-07-07 11:04:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 22 @ 24648 updates, score 11.648) (writing took 6.288047465495765 seconds)
2022-07-07 11:06:34 | INFO | train_inner | epoch 023:     52 / 1122 loss=5.173, nll_loss=1.646, mask_ins=0.676, word_ins_ml=3.334, word_reposition=0.706, kpe=0.456, ppl=36.07, wps=5551.8, ups=0.28, wpb=19736.1, bsz=253.8, num_updates=24700, lr=0.000224961, gnorm=2.723, clip=0, loss_scale=512, train_wall=244, wall=72755
2022-07-07 11:11:16 | INFO | train_inner | epoch 023:    152 / 1122 loss=5.122, nll_loss=1.621, mask_ins=0.667, word_ins_ml=3.312, word_reposition=0.708, kpe=0.435, ppl=34.83, wps=7129, ups=0.36, wpb=20062.2, bsz=256, num_updates=24800, lr=0.000224507, gnorm=2.654, clip=0, loss_scale=512, train_wall=245, wall=73036
2022-07-07 11:15:58 | INFO | train_inner | epoch 023:    252 / 1122 loss=5.202, nll_loss=1.669, mask_ins=0.689, word_ins_ml=3.354, word_reposition=0.715, kpe=0.444, ppl=36.82, wps=7059.8, ups=0.35, wpb=19908.1, bsz=256, num_updates=24900, lr=0.000224055, gnorm=2.599, clip=0, loss_scale=527, train_wall=245, wall=73318
2022-07-07 11:20:39 | INFO | train_inner | epoch 023:    352 / 1122 loss=nan, nll_loss=1.65, mask_ins=0.682, word_ins_ml=3.337, word_reposition=0.718, kpe=nan, ppl=nan, wps=7088.2, ups=0.35, wpb=19975.2, bsz=256, num_updates=25000, lr=0.000223607, gnorm=2.723, clip=0, loss_scale=1024, train_wall=245, wall=73600
2022-07-07 11:25:21 | INFO | train_inner | epoch 023:    452 / 1122 loss=5.158, nll_loss=1.644, mask_ins=0.674, word_ins_ml=3.332, word_reposition=0.709, kpe=0.442, ppl=35.69, wps=7079.1, ups=0.36, wpb=19934.6, bsz=256, num_updates=25100, lr=0.000223161, gnorm=2.672, clip=0, loss_scale=1024, train_wall=245, wall=73882
2022-07-07 11:30:02 | INFO | train_inner | epoch 023:    552 / 1122 loss=nan, nll_loss=1.641, mask_ins=0.686, word_ins_ml=3.329, word_reposition=0.714, kpe=nan, ppl=nan, wps=7069.2, ups=0.36, wpb=19877.5, bsz=256, num_updates=25200, lr=0.000222718, gnorm=2.646, clip=0, loss_scale=1024, train_wall=244, wall=74163
2022-07-07 11:34:44 | INFO | train_inner | epoch 023:    652 / 1122 loss=5.137, nll_loss=1.616, mask_ins=0.679, word_ins_ml=3.307, word_reposition=0.705, kpe=0.446, ppl=35.2, wps=7091.4, ups=0.36, wpb=19957, bsz=256, num_updates=25300, lr=0.000222277, gnorm=2.595, clip=0, loss_scale=1024, train_wall=245, wall=74445
2022-07-07 11:40:11 | INFO | train_inner | epoch 023:    752 / 1122 loss=5.148, nll_loss=1.619, mask_ins=0.674, word_ins_ml=3.309, word_reposition=0.712, kpe=0.453, ppl=35.46, wps=6085.7, ups=0.31, wpb=19892.7, bsz=256, num_updates=25400, lr=0.000221839, gnorm=2.613, clip=0, loss_scale=1024, train_wall=290, wall=74771
2022-07-07 11:44:52 | INFO | train_inner | epoch 023:    852 / 1122 loss=5.181, nll_loss=1.637, mask_ins=0.682, word_ins_ml=3.326, word_reposition=0.717, kpe=0.456, ppl=36.27, wps=7061.8, ups=0.35, wpb=19900.6, bsz=256, num_updates=25500, lr=0.000221404, gnorm=2.66, clip=0, loss_scale=1956, train_wall=244, wall=75053
2022-07-07 11:49:34 | INFO | train_inner | epoch 023:    952 / 1122 loss=5.19, nll_loss=1.652, mask_ins=0.682, word_ins_ml=3.338, word_reposition=0.713, kpe=0.457, ppl=36.51, wps=7115.5, ups=0.36, wpb=20029.7, bsz=256, num_updates=25600, lr=0.000220971, gnorm=2.62, clip=0, loss_scale=2048, train_wall=245, wall=75335
2022-07-07 11:54:15 | INFO | train_inner | epoch 023:   1052 / 1122 loss=5.194, nll_loss=1.659, mask_ins=0.682, word_ins_ml=3.345, word_reposition=0.71, kpe=0.458, ppl=36.61, wps=7043, ups=0.36, wpb=19778.1, bsz=256, num_updates=25700, lr=0.000220541, gnorm=2.73, clip=0, loss_scale=2048, train_wall=244, wall=75616
2022-07-07 11:57:31 | INFO | train | epoch 023 | loss nan | nll_loss 1.638 | mask_ins 0.679 | word_ins_ml 3.327 | word_reposition 0.712 | kpe nan | ppl nan | wps 6816.4 | ups 0.34 | wpb 19913.5 | bsz 255.8 | num_updates 25770 | lr 0.000220241 | gnorm 2.656 | clip 0 | loss_scale 1240 | train_wall 2789 | wall 75812
2022-07-07 11:58:39 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 11.461 | nll_loss 5.5 | mask_ins 1.507 | word_ins_ml 6.883 | word_reposition 1.256 | kpe 1.815 | ppl 2819.85 | wps 13971 | wpb 2279.4 | bsz 32 | num_updates 25770 | best_loss 11.272
2022-07-07 11:58:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 23 @ 25770 updates, score 11.461) (writing took 6.302006276324391 seconds)
2022-07-07 12:00:09 | INFO | train_inner | epoch 024:     30 / 1122 loss=5.134, nll_loss=1.609, mask_ins=0.681, word_ins_ml=3.3, word_reposition=0.704, kpe=0.448, ppl=35.1, wps=5537.9, ups=0.28, wpb=19645.1, bsz=253.8, num_updates=25800, lr=0.000220113, gnorm=2.763, clip=0, loss_scale=2048, train_wall=244, wall=75970
2022-07-07 12:04:50 | INFO | train_inner | epoch 024:    130 / 1122 loss=nan, nll_loss=1.624, mask_ins=0.669, word_ins_ml=3.314, word_reposition=0.707, kpe=nan, ppl=nan, wps=7052.1, ups=0.36, wpb=19818, bsz=256, num_updates=25900, lr=0.000219687, gnorm=2.65, clip=0, loss_scale=2048, train_wall=244, wall=76251
2022-07-07 12:09:32 | INFO | train_inner | epoch 024:    230 / 1122 loss=5.103, nll_loss=1.615, mask_ins=0.672, word_ins_ml=3.306, word_reposition=0.703, kpe=0.422, ppl=34.37, wps=7068.4, ups=0.36, wpb=19899.2, bsz=256, num_updates=26000, lr=0.000219265, gnorm=2.643, clip=0, loss_scale=3666, train_wall=245, wall=76533
2022-07-07 12:14:13 | INFO | train_inner | epoch 024:    330 / 1122 loss=5.065, nll_loss=1.573, mask_ins=0.673, word_ins_ml=3.268, word_reposition=0.701, kpe=0.423, ppl=33.48, wps=7094.5, ups=0.36, wpb=19918.6, bsz=256, num_updates=26100, lr=0.000218844, gnorm=2.551, clip=0, loss_scale=4096, train_wall=244, wall=76814
2022-07-07 12:18:55 | INFO | train_inner | epoch 024:    430 / 1122 loss=nan, nll_loss=1.624, mask_ins=0.678, word_ins_ml=3.313, word_reposition=0.704, kpe=nan, ppl=nan, wps=7081.7, ups=0.35, wpb=19982.3, bsz=256, num_updates=26200, lr=0.000218426, gnorm=2.593, clip=0, loss_scale=4096, train_wall=245, wall=77096
2022-07-07 12:23:37 | INFO | train_inner | epoch 024:    530 / 1122 loss=5.103, nll_loss=1.6, mask_ins=0.672, word_ins_ml=3.291, word_reposition=0.708, kpe=0.431, ppl=34.37, wps=7091.4, ups=0.35, wpb=20004.1, bsz=256, num_updates=26300, lr=0.00021801, gnorm=2.506, clip=0, loss_scale=4096, train_wall=245, wall=77378
2022-07-07 12:28:19 | INFO | train_inner | epoch 024:    630 / 1122 loss=5.113, nll_loss=1.62, mask_ins=0.668, word_ins_ml=3.309, word_reposition=0.701, kpe=0.436, ppl=34.62, wps=7065.3, ups=0.36, wpb=19888.4, bsz=256, num_updates=26400, lr=0.000217597, gnorm=2.64, clip=0, loss_scale=4096, train_wall=245, wall=77659
2022-07-07 12:32:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-07 12:33:03 | INFO | train_inner | epoch 024:    731 / 1122 loss=5.148, nll_loss=1.652, mask_ins=0.665, word_ins_ml=3.337, word_reposition=0.709, kpe=0.436, ppl=35.45, wps=7001.4, ups=0.35, wpb=19902.7, bsz=256, num_updates=26500, lr=0.000217186, gnorm=2.614, clip=0, loss_scale=6489, train_wall=247, wall=77944
2022-07-07 12:37:44 | INFO | train_inner | epoch 024:    831 / 1122 loss=5.11, nll_loss=1.595, mask_ins=0.674, word_ins_ml=3.287, word_reposition=0.71, kpe=0.439, ppl=34.54, wps=7097.3, ups=0.36, wpb=19971, bsz=256, num_updates=26600, lr=0.000216777, gnorm=2.636, clip=0, loss_scale=4096, train_wall=245, wall=78225
2022-07-07 12:43:14 | INFO | train_inner | epoch 024:    931 / 1122 loss=5.132, nll_loss=1.615, mask_ins=0.684, word_ins_ml=3.304, word_reposition=0.703, kpe=0.44, ppl=35.06, wps=6055.4, ups=0.3, wpb=19957.1, bsz=256, num_updates=26700, lr=0.000216371, gnorm=2.583, clip=0, loss_scale=4096, train_wall=293, wall=78555
2022-07-07 12:43:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 12:47:58 | INFO | train_inner | epoch 024:   1032 / 1122 loss=5.119, nll_loss=1.599, mask_ins=0.676, word_ins_ml=3.291, word_reposition=0.709, kpe=0.444, ppl=34.76, wps=7005.5, ups=0.35, wpb=19904.5, bsz=256, num_updates=26800, lr=0.000215967, gnorm=2.622, clip=0, loss_scale=2332, train_wall=247, wall=78839
2022-07-07 12:52:11 | INFO | train | epoch 024 | loss nan | nll_loss 1.615 | mask_ins 0.674 | word_ins_ml 3.305 | word_reposition 0.706 | kpe nan | ppl nan | wps 6800 | ups 0.34 | wpb 19911.6 | bsz 255.8 | num_updates 26890 | lr 0.000215605 | gnorm 2.621 | clip 0 | loss_scale 3713 | train_wall 2790 | wall 79091
2022-07-07 12:53:19 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 11.698 | nll_loss 5.597 | mask_ins 1.46 | word_ins_ml 6.96 | word_reposition 1.286 | kpe 1.993 | ppl 3322.71 | wps 13963.5 | wpb 2279.4 | bsz 32 | num_updates 26890 | best_loss 11.272
2022-07-07 12:53:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 24 @ 26890 updates, score 11.698) (writing took 6.763051097281277 seconds)
2022-07-07 12:53:54 | INFO | train_inner | epoch 025:     10 / 1122 loss=5.175, nll_loss=1.652, mask_ins=0.679, word_ins_ml=3.338, word_reposition=0.718, kpe=0.441, ppl=36.14, wps=5583.4, ups=0.28, wpb=19865.2, bsz=253.8, num_updates=26900, lr=0.000215565, gnorm=2.8, clip=0, loss_scale=2048, train_wall=244, wall=79194
2022-07-07 12:58:34 | INFO | train_inner | epoch 025:    110 / 1122 loss=5.06, nll_loss=1.591, mask_ins=0.661, word_ins_ml=3.284, word_reposition=0.707, kpe=0.407, ppl=33.35, wps=7113.1, ups=0.36, wpb=19971.4, bsz=256, num_updates=27000, lr=0.000215166, gnorm=2.575, clip=0, loss_scale=2048, train_wall=244, wall=79475
2022-07-07 13:03:16 | INFO | train_inner | epoch 025:    210 / 1122 loss=5.046, nll_loss=1.577, mask_ins=0.66, word_ins_ml=3.271, word_reposition=0.704, kpe=0.411, ppl=33.04, wps=7074.5, ups=0.35, wpb=19932.9, bsz=256, num_updates=27100, lr=0.000214768, gnorm=2.517, clip=0, loss_scale=2048, train_wall=245, wall=79757
2022-07-07 13:07:58 | INFO | train_inner | epoch 025:    310 / 1122 loss=5.067, nll_loss=1.594, mask_ins=0.665, word_ins_ml=3.286, word_reposition=0.704, kpe=0.411, ppl=33.51, wps=7092.5, ups=0.35, wpb=19983.3, bsz=256, num_updates=27200, lr=0.000214373, gnorm=2.573, clip=0, loss_scale=2048, train_wall=245, wall=80039
2022-07-07 13:09:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 13:12:43 | INFO | train_inner | epoch 025:    411 / 1122 loss=5.073, nll_loss=1.594, mask_ins=0.669, word_ins_ml=3.286, word_reposition=0.701, kpe=0.417, ppl=33.66, wps=7033.3, ups=0.35, wpb=20039.2, bsz=256, num_updates=27300, lr=0.00021398, gnorm=2.684, clip=0, loss_scale=2271, train_wall=248, wall=80324
2022-07-07 13:17:25 | INFO | train_inner | epoch 025:    511 / 1122 loss=5.069, nll_loss=1.581, mask_ins=0.675, word_ins_ml=3.275, word_reposition=0.701, kpe=0.418, ppl=33.57, wps=7039.7, ups=0.36, wpb=19826.2, bsz=256, num_updates=27400, lr=0.000213589, gnorm=2.584, clip=0, loss_scale=2048, train_wall=245, wall=80605
2022-07-07 13:22:06 | INFO | train_inner | epoch 025:    611 / 1122 loss=nan, nll_loss=1.609, mask_ins=0.675, word_ins_ml=3.299, word_reposition=0.701, kpe=nan, ppl=nan, wps=7049.3, ups=0.36, wpb=19815.9, bsz=256, num_updates=27500, lr=0.000213201, gnorm=2.68, clip=0, loss_scale=2048, train_wall=244, wall=80886
2022-07-07 13:26:47 | INFO | train_inner | epoch 025:    711 / 1122 loss=5.082, nll_loss=1.588, mask_ins=0.671, word_ins_ml=3.281, word_reposition=0.707, kpe=0.423, ppl=33.87, wps=7069.7, ups=0.36, wpb=19871.1, bsz=256, num_updates=27600, lr=0.000212814, gnorm=2.811, clip=0, loss_scale=2048, train_wall=244, wall=81168
2022-07-07 13:30:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 13:31:30 | INFO | train_inner | epoch 025:    812 / 1122 loss=5.08, nll_loss=1.59, mask_ins=0.669, word_ins_ml=3.282, word_reposition=0.705, kpe=0.423, ppl=33.82, wps=7099.1, ups=0.35, wpb=20139.6, bsz=256, num_updates=27700, lr=0.00021243, gnorm=2.592, clip=0, loss_scale=1896, train_wall=246, wall=81451
2022-07-07 13:36:13 | INFO | train_inner | epoch 025:    912 / 1122 loss=5.071, nll_loss=1.594, mask_ins=0.668, word_ins_ml=3.286, word_reposition=0.688, kpe=0.429, ppl=33.61, wps=7015, ups=0.35, wpb=19812.4, bsz=256, num_updates=27800, lr=0.000212047, gnorm=2.721, clip=0, loss_scale=1024, train_wall=245, wall=81734
2022-07-07 13:40:54 | INFO | train_inner | epoch 025:   1012 / 1122 loss=5.096, nll_loss=1.611, mask_ins=0.669, word_ins_ml=3.3, word_reposition=0.699, kpe=0.428, ppl=34.19, wps=7049.8, ups=0.36, wpb=19839.7, bsz=256, num_updates=27900, lr=0.000211667, gnorm=2.679, clip=0, loss_scale=1024, train_wall=244, wall=82015
2022-07-07 13:41:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-07 13:46:26 | INFO | train_inner | epoch 025:   1113 / 1122 loss=nan, nll_loss=1.612, mask_ins=0.674, word_ins_ml=3.302, word_reposition=0.702, kpe=nan, ppl=nan, wps=6001.9, ups=0.3, wpb=19904.9, bsz=256, num_updates=28000, lr=0.000211289, gnorm=2.728, clip=0, loss_scale=573, train_wall=294, wall=82347
2022-07-07 13:46:50 | INFO | train | epoch 025 | loss nan | nll_loss 1.594 | mask_ins 0.669 | word_ins_ml 3.286 | word_reposition 0.702 | kpe nan | ppl nan | wps 6794.1 | ups 0.34 | wpb 19914.6 | bsz 255.8 | num_updates 28009 | lr 0.000211255 | gnorm 2.664 | clip 0 | loss_scale 1727 | train_wall 2790 | wall 82371
2022-07-07 13:47:59 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 11.757 | nll_loss 5.58 | mask_ins 1.525 | word_ins_ml 6.955 | word_reposition 1.306 | kpe 1.971 | ppl 3462.24 | wps 13900.4 | wpb 2279.4 | bsz 32 | num_updates 28009 | best_loss 11.272
2022-07-07 13:48:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 25 @ 28009 updates, score 11.757) (writing took 6.32416845113039 seconds)
2022-07-07 13:52:22 | INFO | train_inner | epoch 026:     91 / 1122 loss=5.048, nll_loss=1.579, mask_ins=0.669, word_ins_ml=3.273, word_reposition=0.705, kpe=0.401, ppl=33.08, wps=5578.9, ups=0.28, wpb=19838.4, bsz=253.8, num_updates=28100, lr=0.000210912, gnorm=2.682, clip=0, loss_scale=512, train_wall=244, wall=82702
2022-07-07 13:57:03 | INFO | train_inner | epoch 026:    191 / 1122 loss=4.993, nll_loss=1.546, mask_ins=0.661, word_ins_ml=3.243, word_reposition=0.694, kpe=0.394, ppl=31.84, wps=7088.4, ups=0.36, wpb=19929.4, bsz=256, num_updates=28200, lr=0.000210538, gnorm=2.546, clip=0, loss_scale=512, train_wall=244, wall=82983
2022-07-07 14:01:44 | INFO | train_inner | epoch 026:    291 / 1122 loss=nan, nll_loss=1.539, mask_ins=0.656, word_ins_ml=3.236, word_reposition=0.686, kpe=nan, ppl=nan, wps=7042.2, ups=0.36, wpb=19800.7, bsz=256, num_updates=28300, lr=0.000210166, gnorm=2.676, clip=0, loss_scale=512, train_wall=244, wall=83265
2022-07-07 14:06:25 | INFO | train_inner | epoch 026:    391 / 1122 loss=5.041, nll_loss=1.577, mask_ins=0.663, word_ins_ml=3.271, word_reposition=0.707, kpe=0.4, ppl=32.93, wps=7122, ups=0.36, wpb=20056.2, bsz=256, num_updates=28400, lr=0.000209795, gnorm=2.575, clip=0, loss_scale=512, train_wall=245, wall=83546
2022-07-07 14:11:07 | INFO | train_inner | epoch 026:    491 / 1122 loss=5.029, nll_loss=1.574, mask_ins=0.659, word_ins_ml=3.268, word_reposition=0.696, kpe=0.406, ppl=32.65, wps=7066.8, ups=0.36, wpb=19903.9, bsz=256, num_updates=28500, lr=0.000209427, gnorm=2.567, clip=0, loss_scale=906, train_wall=245, wall=83828
2022-07-07 14:15:48 | INFO | train_inner | epoch 026:    591 / 1122 loss=5.07, nll_loss=1.611, mask_ins=0.656, word_ins_ml=3.301, word_reposition=0.704, kpe=0.409, ppl=33.59, wps=7047.5, ups=0.36, wpb=19825.3, bsz=256, num_updates=28600, lr=0.000209061, gnorm=2.632, clip=0, loss_scale=1024, train_wall=244, wall=84109
2022-07-07 14:20:29 | INFO | train_inner | epoch 026:    691 / 1122 loss=5.06, nll_loss=1.597, mask_ins=0.666, word_ins_ml=3.288, word_reposition=0.692, kpe=0.413, ppl=33.35, wps=7119.5, ups=0.36, wpb=19971.4, bsz=256, num_updates=28700, lr=0.000208696, gnorm=2.799, clip=0, loss_scale=1024, train_wall=244, wall=84390
2022-07-07 14:25:11 | INFO | train_inner | epoch 026:    791 / 1122 loss=nan, nll_loss=1.592, mask_ins=0.666, word_ins_ml=3.283, word_reposition=0.703, kpe=nan, ppl=nan, wps=7071, ups=0.35, wpb=19924.6, bsz=256, num_updates=28800, lr=0.000208333, gnorm=2.64, clip=0, loss_scale=1024, train_wall=245, wall=84672
2022-07-07 14:29:52 | INFO | train_inner | epoch 026:    891 / 1122 loss=5.062, nll_loss=1.595, mask_ins=0.666, word_ins_ml=3.286, word_reposition=0.696, kpe=0.413, ppl=33.39, wps=7098.9, ups=0.36, wpb=19987, bsz=256, num_updates=28900, lr=0.000207973, gnorm=2.72, clip=0, loss_scale=1024, train_wall=245, wall=84953
2022-07-07 14:33:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 14:34:35 | INFO | train_inner | epoch 026:    992 / 1122 loss=5.06, nll_loss=1.59, mask_ins=0.66, word_ins_ml=3.281, word_reposition=0.703, kpe=0.416, ppl=33.36, wps=7050.6, ups=0.35, wpb=19947.2, bsz=256, num_updates=29000, lr=0.000207614, gnorm=2.602, clip=0, loss_scale=1399, train_wall=246, wall=85236
2022-07-07 14:39:16 | INFO | train_inner | epoch 026:   1092 / 1122 loss=5.051, nll_loss=1.578, mask_ins=0.661, word_ins_ml=3.271, word_reposition=0.699, kpe=0.419, ppl=33.15, wps=7092, ups=0.36, wpb=19937.1, bsz=256, num_updates=29100, lr=0.000207257, gnorm=2.699, clip=0, loss_scale=1024, train_wall=244, wall=85517
2022-07-07 14:40:40 | INFO | train | epoch 026 | loss nan | nll_loss 1.58 | mask_ins 0.662 | word_ins_ml 3.273 | word_reposition 0.699 | kpe nan | ppl nan | wps 6911.4 | ups 0.35 | wpb 19912.3 | bsz 255.8 | num_updates 29130 | lr 0.00020715 | gnorm 2.647 | clip 0 | loss_scale 869 | train_wall 2741 | wall 85601
2022-07-07 14:41:49 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 11.697 | nll_loss 5.574 | mask_ins 1.526 | word_ins_ml 6.951 | word_reposition 1.259 | kpe 1.962 | ppl 3320.69 | wps 13859.2 | wpb 2279.4 | bsz 32 | num_updates 29130 | best_loss 11.272
2022-07-07 14:41:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_uncased/checkpoint_last.pt (epoch 26 @ 29130 updates, score 11.697) (writing took 6.339963350445032 seconds)
2022-07-07 14:45:13 | INFO | train_inner | epoch 027:     70 / 1122 loss=5.008, nll_loss=1.561, mask_ins=0.66, word_ins_ml=3.255, word_reposition=0.696, kpe=0.397, ppl=32.18, wps=5536.3, ups=0.28, wpb=19758.2, bsz=253.8, num_updates=29200, lr=0.000206901, gnorm=2.701, clip=0, loss_scale=1024, train_wall=245, wall=85874
2022-07-07 14:50:45 | INFO | train_inner | epoch 027:    170 / 1122 loss=4.934, nll_loss=1.5, mask_ins=0.656, word_ins_ml=3.202, word_reposition=0.694, kpe=0.383, ppl=30.57, wps=6051.3, ups=0.3, wpb=20074.5, bsz=256, num_updates=29300, lr=0.000206548, gnorm=2.567, clip=0, loss_scale=1024, train_wall=294, wall=86206
2022-07-07 14:55:28 | INFO | train_inner | epoch 027:    270 / 1122 loss=4.976, nll_loss=1.53, mask_ins=0.66, word_ins_ml=3.229, word_reposition=0.701, kpe=0.386, ppl=31.46, wps=7083.9, ups=0.35, wpb=20053.1, bsz=256, num_updates=29400, lr=0.000206197, gnorm=2.556, clip=0, loss_scale=1024, train_wall=246, wall=86489
2022-07-07 15:00:10 | INFO | train_inner | epoch 027:    370 / 1122 loss=4.973, nll_loss=1.541, mask_ins=0.657, word_ins_ml=3.238, word_reposition=0.69, kpe=0.389, ppl=31.4, wps=7012.9, ups=0.36, wpb=19740.3, bsz=256, num_updates=29500, lr=0.000205847, gnorm=2.656, clip=0, loss_scale=1198, train_wall=244, wall=86770
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
train.sh: line 40: num: command not found
